# A DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning
# DigiRL：通过自主强化学习训练真实环境下的设备控制智能体


Hao Bai ${}^{1,2 * }$ Yifei Zhou ${}^{1 * }$ Mert Cemri ${}^{1}$ Jiayi Pan ${}^{1}$
Hao Bai ${}^{1,2 * }$ Yifei Zhou ${}^{1 * }$ Mert Cemri ${}^{1}$ Jiayi Pan ${}^{1}$


Alane Suhr ${}^{1}$ Sergey Levine ${}^{1}$ Aviral Kumar ${}^{3,4}$
Alane Suhr ${}^{1}$ Sergey Levine ${}^{1}$ Aviral Kumar ${}^{3,4}$


${}^{1}$ UC Berkeley ${}^{2}$ UIUC ${}^{3}$ CMU ${}^{4}$ Google DeepMind
${}^{1}$ 加州大学伯克利分校 ${}^{2}$ 伊利诺伊大学厄巴纳-香槟分校 ${}^{3}$ 卡内基梅隆大学 ${}^{4}$ Google DeepMind


## Abstract
## 摘要


Training corpuses for vision language models (VLMs) typically lack sufficient amounts of decision-centric data. This renders off-the-shelf VLMs sub-optimal for decision-making tasks such as in-the-wild device control through graphical user interfaces (GUIs). While training with static demonstrations has shown some promise, we show that such methods fall short for controlling real GUIs due to their failure to deal with real world stochasticity and non-stationarity not captured in static observational data. This paper introduces a novel autonomous RL approach, called DigiRL, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline RL to initialize the model, followed by offline-to-online RL. To do this, we build a scalable and parallelizable Android learning environment equipped with a VLM-based evaluator and develop a simple yet effective RL approach for learning in this domain. Our approach runs advantage-weighted RL with advantage estimators enhanced to account for stochasticity along with an automatic curriculum for deriving maximal learning signal. We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our 1.3B VLM trained with RL achieves a 49.5% absolute improvement - from 17.7 to 67.2% success rate - over supervised fine-tuning with static human demonstration data. These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V (8.3% success rate) and the 17B CogAgent trained with AitW data (38.5%), but also the prior best autonomous RL approach based on filtered behavior cloning (57.8%), thereby establishing a new state-of-the-art for digital agents for in-the-wild device control.
视觉语言模型（VLM）的训练语料库通常缺乏足够的以决策为中心的数据。这导致现成的 VLM 在处理通过图形用户界面（GUI）进行真实设备控制等决策任务时表现不佳。虽然使用静态演示进行训练已显示出一定潜力，但我们证明这类方法在控制真实 GUI 时力有不逮，因为它们无法处理静态观测数据中未涵盖的现实世界随机性和非平稳性。本文介绍了一种名为 DigiRL 的新型自主强化学习（RL）方法，通过两个阶段微调预训练 VLM 来训练真实环境下的设备控制智能体：首先利用离线 RL 初始化模型，随后进行离线到在线 RL。为此，我们构建了一个可扩展且可并行的 Android 学习环境，配备了基于 VLM 的评估器，并开发了一种简单而有效的 RL 方法。该方法运行优势加权 RL，通过增强的优势估计器来应对随机性，并配合自动课程以获取最大化的学习信号。我们在 Android-in-the-Wild (AitW) 数据集上证明了 DigiRL 的有效性：我们使用 RL 训练的 1.3B VLM 实现了 49.5% 的绝对提升——成功率从 17.7% 提高到 67.2%——超过了使用静态人类演示数据进行的监督微调。这些结果不仅显著超越了以往的最佳智能体，包括使用 GPT-4V 的 AppAgent（8.3% 成功率）和使用 AitW 数据训练的 17B CogAgent（38.5%），还超越了此前基于过滤行为克隆的最佳自主 RL 方法（57.8%），从而为真实环境下设备控制的数字智能体建立了新的技术基准。


## 1 Introduction
## 1 引言


Advances in vision-language models (VLMs), especially in regards to their remarkable commonsense, reasoning, and generalization abilities imply that realizing a fully autonomous digital AI assistant, that can simplify human life by automating day-to-day activities on computer devices via natural language interfaces, is no longer a distant aspiration [16, 45, 56]. An effective device-control AI assistant should be able to complete tasks in-the-wild through Graphical User Interfaces (GUIs) on digital devices: make travel plans; experiment with presentation designs; and operate a mobile device autonomously, all while running amidst stochasticity and distractors on the device, the Internet, and the tools it interacts with. However, enhanced reasoning or common-sense abilities do not directly transfer to intelligent assistant behavior: ultimately we want AI assistants to accomplish tasks, exhibit rational behavior, and recover from their mistakes as opposed to simply producing a plausible completion to a given observation based on the data seen during pre-training. This implies that a mechanism to channel abilities from pre-training into a deployable AI "agent" is lacking.
视觉语言模型（VLM）的进步，特别是其卓越的常识、推理和泛化能力，意味着实现全自主数字 AI 助手已不再是遥远的理想。这种助手可以通过自然语言界面自动执行计算机设备上的日常活动，从而简化人类生活 [16, 45, 56]。一个有效的设备控制 AI 助手应能通过数字设备上的图形用户界面（GUI）在真实环境下完成任务：制定旅行计划、尝试演示设计以及自主操作移动设备，同时在设备、互联网及其交互工具的随机性和干扰项中运行。然而，增强的推理或常识能力并不能直接转化为智能助手的行为：归根结底，我们希望 AI 助手能完成任务、表现出理性行为并从错误中恢复，而不仅仅是根据预训练期间看到的数据对给定观测生成合理的补全。这意味着目前缺乏一种将预训练能力转化为可部署 AI “智能体”的机制。


---



*Equal contribution, listed in alphabetical order; work done at UC Berkeley. E-mails: haob2@illinois.edu, yifei_zhou@berkeley.edu, aviralku@andrew.cmu.edu.Project page: https://digirl-agent.github.io/.Code available at https://github.com/DigiRL-agent/digirl.
*同等贡献，按字母顺序排列；工作完成于加州大学伯克利分校。电子邮件：haob2@illinois.edu, yifei_zhou@berkeley.edu, aviralku@andrew.cmu.edu。项目主页：https://digirl-agent.github.io/。代码获取地址：https://github.com/DigiRL-agent/digirl。


---



Even the strongest proprietary VLMs,such as GPT-4V [24] and Gemini 1.5 Pro [7] ${}^{2}$ ,still struggle to produce the right actions when completing tasks on devices. While general-purpose vision-language abilities help these models still make meaningful abstract deductions about novel scenes when deployed, these deductions do not transfer to accurate reasoning for control [47, 45, 55, 44]. As a result, most prior work for building device agents construct complex wrappers around proprietary VLMs by combining them with prompting, search, or tool use [47, 44, 52, 51, 45]. While building prompting or retrieval wrappers to improve decision-making performance of existing VLMs enhances their performance in the short run, without updating the weights, the effectiveness of the resulting agent is inherently limited by the capabilities of the base model [49, 3]. For example, we found that off-the-shelf VLMs make reasoning failures that derail the agent (e.g., Figure 2 and Figure 17), as direct consequences of inability of the base model to reason with low-level device-control actions. A different solution is to fine-tune the model on demonstrations via imitation learning. However, the dynamic nature of the web and device means that models trained to mimic actions in stale data can result in sub-optimalilty as the eco-system changes [26]. Agents trained in this way struggle to recover from the agents' own mistakes [8, 12].
即使是像 GPT-4V [24] 和 Gemini 1.5 Pro [7] ${}^{2}$ 这样最强大的闭源 VLM，在完成设备上的任务时仍难以生成正确的动作。虽然通用视觉语言能力有助于这些模型在部署时对新颖场景进行有意义的抽象推理，但这些推理无法转化为准确的控制推理 [47, 45, 55, 44]。因此，先前构建设备智能体的大多数工作都是通过结合提示、搜索或工具调用，围绕闭源 VLM 构建复杂的封装层 [47, 44, 52, 51, 45]。虽然构建提示或检索封装层能在短期内提高现有 VLM 的决策表现，但在不更新权重的情况下，所得智能体的有效性本质上受限于基座模型的能力 [49, 3]。例如，我们发现现成的 VLM 会出现导致智能体偏离目标的推理失败（如附图 2 和附图 17），这是基座模型无法进行底层设备控制动作推理的直接结果。另一种解决方案是通过模仿学习在演示数据上对模型进行微调。然而，网络和设备的动态特性意味着，被训练为模仿陈旧数据中动作的模型可能会随着生态系统的变化而导致次优结果 [26]。以这种方式训练的智能体也难以从自身的错误中恢复 [8, 12]。


If we can instead build an interactive approach to train a VLM to directly adapt and learn from its own experience on the device and the Internet, that can be used to build a robust and reliable device-control agent, without needing wrappers on top of proprietary models. However, this learning-based approach must satisfy some desiderata. First, it must make use of online interaction data since static demonstration data would not be representative of the task when the model is deployed: for instance, even in the setting of web navigation alone, dynamic nature of in-the-wild web-sites means that the agent will frequently encounter website versions that differ significantly from the scenarios seen during training and will need to behave reliably despite changes in visual appearance and distractions. Second, learning on-the-fly means the approach must learn from multi-turn interaction data from the model itself, a large of chunk of which would consist of failures. Proper mechanisms must be designed to automatically pick out the correct actions while filtering the wrong ones.
如果相反，我们能构建一种交互式方法来训练 VLM，使其能直接从设备和互联网上的自身经验中自适应和学习，那么就可以用来构建一个鲁棒且可靠的设备控制智能体，而无需在闭源模型之上添加封装。然而，这种基于学习的方法必须满足一些必要条件。首先，它必须利用在线交互数据，因为当模型部署时，静态演示数据将无法代表任务情况：例如，即便仅在 Web 导航设置中，野外网站的动态特性也意味着智能体将频繁遇到与训练场景显著不同的网站版本，并且需要在视觉外观改变和干扰存在的情况下表现可靠。其次，即时学习意味着该方法必须从模型自身的多轮交互数据中学习，其中很大一部分由失败组成。必须设计适当的机制来自动筛选正确动作，同时过滤错误动作。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_1.jpg?x=783&y=825&w=696&h=359&r=0"/>



Figure 1: DigiRL overview. DigiRL is built upon a VLM that has been pre-trained on extensive web data to develop fundamental skills such as common knowledge, reasoning, and visual grounding. Initially, we employ offline RL to fine-tune the VLM using stale task-specific data, which helps in eliciting goal-oriented behaviors. Subsequently, our agent engages with real-world graphical user interfaces, continuously enhancing its performance through online RL and autonomous performance evaluations.
图 1：DigiRL 概览。DigiRL 基于一个已在海量网络数据上预训练过的 VLM，以培养常识、推理和视觉定位等基本技能。最初，我们采用离线强化学习（RL），利用陈旧的任务特定数据对 VLM 进行微调，这有助于诱发目标导向的行为。随后，我们的智能体与真实的图形用户界面进行交互，通过在线 RL 和自主性能评估不断提升其表现。


To this end, our main contribution is a novel autonomous RL approach, DigiRL (i.e., RL for Digital Agents), for training device control agents, as shown in Figure 1. The resulting agent attains state-of-the-art performance on a number of Android device-control tasks. To train this agent, our approach operates in two phases: an initial offline RL phase to initialize the agent using existing data, followed by an offline-to-online RL phase, that further fine-tunes the model obtained from offline RL on online rollout data. Online RL training requires access to an environment that the agent can interact with and obtain reliable reward signals, all in a reasonable amount of wall-clock time. To do so, we build a scalable and parallelizable Android learning environment equipped with a robust VLM-based general-purpose evaluator [26] (average error rate 2.8% against human judgement) that supports running up to 64 real Android emulators at the same time to make online RL real-time. Then, to effectively learn autonomously, we develop an online RL approach that retains the simplicity of supervised learning, but incorporates several key deep RL insights to enable fast fine-tuning. Concretely, our approach is a variant of advantage-weighted regression (AWR) [28], equipped with: (i) an automatic curriculum that uses an instruction-level value function to order tasks so as to extract maximal learning signal, which is inspired by prioritized replay methods [11, 32, 23], and (ii) another step-level value function trained via effective cross-entropy loss [17, 5] to extract low-variance and less-biased learning signal amidst stochasticity and diverse tasks. This RL approach allows us to fine-tune VLMs on their own experience.
为此，我们的主要贡献是一种新颖的自主 RL 方法 DigiRL（即用于数字智能体的 RL），用于训练设备控制智能体，如图 1 所示。由此产生的智能体在多项 Android 设备控制任务上达到了 SOTA 性能。为了训练该智能体，我们的方法分为两个阶段：初始的离线 RL 阶段，使用现有数据初始化智能体；随后是离线到在线 RL 阶段，对从离线 RL 获得的模型在在线采样数据上进一步微调。在线 RL 训练需要访问一个智能体可以与之交互并获取可靠奖励信号的环境，且耗时需在合理范围内。为此，我们构建了一个可扩展且可并行的 Android 学习环境，配备了强大的基于 VLM 的通用评估器 [26]（与人类判断相比，平均错误率为 2.8%），支持同时运行多达 64 个真实 Android 模拟器，以实现实时在线 RL。接着，为了有效地自主学习，我们开发了一种在线 RL 方法，它保留了监督学习的简便性，但融入了几个关键的深度 RL 见解以实现快速微调。具体而言，我们的方法是优势加权回归（AWR）[28] 的一个变体，配备了：(i) 一个自动课程，利用指令级价值函数对任务进行排序，以提取最大的学习信号，其灵感来自优先级回放方法 [11, 32, 23]；(ii) 另一个通过有效的交叉熵损失 [17, 5] 训练的步级价值函数，用以在随机性和多样化任务中提取低方差且偏差较小的学习信号。这种 RL 方法允许我们在 VLM 自身的经验上对其进行微调。


---



${}^{2}$ We use external versions of these models as of June 11,2024. Experiments with GPT and Gemini models were performed entirely by Hao Bai, Yifei Zhou, Mert Cemri, and Jiayi Pan.
${}^{2}$ 我们使用的是截至 2024 年 6 月 11 日这些模型的外部版本。GPT 和 Gemini 模型的实验全部由 Hao Bai、Yifei Zhou、Mert Cemri 和 Jiayi Pan 完成。


---



<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_2.jpg?x=421&y=201&w=952&h=427&r=0"/>



Figure 2: Qualitative comparison between DigiRL and other approaches. AutoUI trained from static human demonstrations can easily get stuck in out-of-distribution states while GPT-4V often get on a wrong goal (searched "logitech g933bestbuy.com logitech g933" in Google instead of bestbuy.com). In contrast, DigiRL can recover from such states and complete complex instruction as requested.
图 2：DigiRL 与其他方法的定性对比。基于静态人类演示训练的 AutoUI 容易陷入分布外状态，而 GPT-4V 经常偏离目标（在 Google 中搜索了“logitech g933bestbuy.com logitech g933”而非 bestbuy.com）。相比之下，DigiRL 能从这些状态中恢复并按要求完成复杂指令。


We evaluate our agent trained with DigiRL in carrying out diverse instructions from Android in the Wild dataset [31] on real Android device emulators and find that our agent can achieve a 28.7% improvement over the existing state-of-the-art agents (from 38.5% to 67.2% success rate) 18B CogAgent [9], and over 9% improvement over the prior best autonomous learning approach based on Filtered Behavior Cloning [18, 26]. The performance of our agent also significantly surpasses wrappers on top of state-of-the-art proprietary VLMs such as GPT-4V [24] and Gemini 1.5 Pro [7] (17.7% success rate), despite using a significantly smaller model (with 1.3B parameters). To our knowledge, this is the first work to successfully build an autonomous offline-to-online RL approach to enable state-of-the-art performance on device-control problems.
我们在真实 Android 设备模拟器上评估了通过 DigiRL 训练的智能体执行 Android in the Wild 数据集 [31] 中多样化指令的表现，发现我们的智能体相比现有最先进的智能体 18B CogAgent [9] 提升了 28.7%（成功率从 38.5% 提升至 67.2%），且比之前基于过滤行为克隆 [18, 26] 的最佳自主学习方法提升了 9% 以上。尽管使用了更小的模型（1.3B 参数），我们智能体的表现也显著超过了基于 GPT-4V [24] 和 Gemini 1.5 Pro [7]（成功率 17.7%）等最先进闭源 VLM 的封装方案。据我们所知，这是首个成功构建自主“线下到线上”强化学习方法、并在设备控制问题上实现最先进性能的工作。


## 2 Related Work
## 2 相关工作


Multi-modal digital agents. In contrast to language-only agents that largely interact with both text or code inputs and outputs $\left\lbrack  {{33},{49},3,{30},{46},{20},{13}}\right\rbrack$ ,training multi-modal agents capable of controlling devices presents different challenges: first, device control is done directly at the pixel-level and in a coordinate-based action space, instead of natural language [31, 44] that LLM is most familiar with, and second, the ecosystem of a device and the Internet tends to be quite stochastic and unpredictable, which is absent with high-level planning in language only. To handle these challenges, prior work largely builds on strong proprietary VLMs [24, 7], and designs complex rule-based wrappers $\left\lbrack  {{47},{51},{45},{52}}\right\rbrack$ to enhance the visual grounding capabilities of VLMs in GUI interfaces and convert text output into pixel interactions. However, without any form of fine-tuning, this limits the room for possible performance improvement $\left\lbrack  {{44},{47},{49},3,{50}}\right\rbrack$ ,especially when pre-training corpora only present limited action-labeled data. A separate line of work fine-tunes VLMs with demonstration data $\left\lbrack  {{19},{15},9,{53}}\right\rbrack$ via imitation learning,but maximizing single-step accuracy from stale demonstrations without accounting for consequences of these actions in subsequent steps may lead to poor solutions amidst stochasticity [26], as agents trained in such ways will struggle to recover from out-of-distribution states not included in the demonstration data [8, 12]. The third category, and perhaps the closest to us, are works that run filtered imitation learning on autonomously-collected data to directly maximize the episode success rate $\left\lbrack  {{26},{18}}\right\rbrack$ . In contrast,ours is the first work to scale autonomous, offline-to-online RL for device control, producing an agent that outperforms prior agents built via imitation. Even when compared to prior work running on-policy RL in simplified web navigation settings (MiniWob++ [37, 10]), our approach is 1000x more sample efficient (around 1e3 trajectories compared to around 1e6 trajectories), and operates in real-world GUI navigation tasks.
多模态数字智能体。与主要通过文本或代码输入输出交互的纯语言智能体相比 $\left\lbrack  {{33},{49},3,{30},{46},{20},{13}}\right\rbrack$ ，训练能够控制设备的多模态智能体面临不同挑战：首先，设备控制是直接在像素级和基于坐标的动作空间内进行的，而非 LLM 最熟悉的自然语言 [31, 44]；其次，设备生态系统和互联网往往具有高度随机性和不可预测性，而纯语言的高层规划中不存在这些因素。为应对挑战，先前工作大多基于强大的闭源 VLM [24, 7]，并设计复杂的基于规则的封装器 $\left\lbrack  {{47},{51},{45},{52}}\right\rbrack$ 以增强 VLM 在 GUI 界面中的视觉定位能力，并将文本输出转换为像素交互。然而，没有任何形式的微调限制了性能提升的空间 $\left\lbrack  {{44},{47},{49},3,{50}}\right\rbrack$ ，尤其是在预训练语料库仅包含有限动作标签数据的情况下。另一类工作通过模仿学习利用演示数据微调 VLM $\left\lbrack  {{19},{15},9,{53}}\right\rbrack$ ，但在不考虑后续步骤后果的情况下最大化陈旧演示的单步准确率，在随机环境中可能会导致糟糕的方案 [26]，因为以此方式训练的智能体难以从演示数据未涵盖的分布外状态中恢复 [8, 12]。第三类（或许与我们最接近的）是在自主收集的数据上运行过滤模仿学习，以直接最大化回合成功率的工作 $\left\lbrack  {{26},{18}}\right\rbrack$ 。相比之下，我们的工作是首次将自主、线下到线上的强化学习应用于设备控制，生成的智能体性能超越了以往通过模仿构建的智能体。即使与在简化 Web 导航设置中运行在线强化学习的先前工作 (MiniWob++ [37, 10]) 相比，我们的方法样本效率高出 1000 倍（约 1e3 条轨迹对比约 1e6 条轨迹），且运行在真实世界的 GUI 导航任务中。


Environments for device control agents. Recent works have introduced simulated environments for building device control agents $\left\lbrack  {{48},{56},{16},{54},4,{44}}\right\rbrack$ . However,these environments are primarily designed for evaluation, and present only a limited range of tasks within fully deterministic and stationary settings, infeasible for acquiring a diverse repertoire of skills needed for device control. Alternatively, other works use environments with a greater diversity of tasks [48, 37], but these environments often oversimplify the task complexity, thus failing to transfer to in-the-wild settings. Coversely, our training environment utilizes autonomous evaluation [26] with Gemini 1.5 Pro [7] to support diverse, open-ended tasks on parallel actual Android devices, at full scale unlike prior environments. This also contrasts other prior works that use single-threaded Android emulators [26, 39, 19] and thus inefficient for support online RL at scale.
设备控制智能体环境。近期工作引入了用于构建设备控制智能体的模拟环境 $\left\lbrack  {{48},{56},{16},{54},4,{44}}\right\rbrack$ 。然而，这些环境主要为评估而设计，且仅在完全确定性和静态设置下提供有限的任务范围，无法获得设备控制所需的各种技能。此外，其他工作使用了任务多样性更高的环境 [48, 37]，但这些环境往往过度简化了任务复杂度，导致无法迁移到真实环境中。相反，我们的训练环境利用 Gemini 1.5 Pro [7] 进行自主评估 [26]，以支持在并行实际 Android 设备上执行多样化、开放式任务，其规模是以往环境无法比拟的。这也与以往使用单线程 Android 模拟器 [26, 39, 19] 且难以高效支持大规模在线强化学习的工作形成对比。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_3.jpg?x=440&y=204&w=919&h=404&r=0"/>



Figure 3: Environment details. Top: actions space and dynamics of the environment. Bottom: examples of the read-world non-stationarity and dynamism of the environment.
图 3：环境细节。上：环境的动作空间与动力学。下：现实世界环境的非平稳性与动态性示例。


Reinforcement learning for LLM/VLMs. The majority of prior research employing RL for foundation models concentrates on tasks that must be solved in a single turn, such as preference optimization $\left\lbrack  {{25},{58},2}\right\rbrack$ or reasoning [27]. However,optimizing for single-turn interaction from expert demonstrations may result in sub-optimal strategies for multi-step problems [57, 38, 42], especially amidst a high degree of stochasticity or non-stationarity. Therefore, we focus on building multi-turn RL algorithms that can learn from sub-optimal, online interaction data in this work. While prior works have developed value-based RL algorithms for LLMs [42, 38, 1, 57, 50], they typically require maintaining multiple models such as Q-networks, value-networks, and policy networks, along with their delayed target counterparts, and can be subjective to slow convergence and sensitivity to choices of hyper-parameters. In contrast, we focus on identifying the key design choices for instantiating a simple yet effective RL algorithm for practitioners to incorporate to substantially improve full-scale Android device control. Our approach can serve as a base model for future research.
LLM/VLM 的强化学习。先前利用 RL 优化基础模型的研究大多集中在必须单轮解决的任务上，如偏好优化 $\left\lbrack  {{25},{58},2}\right\rbrack$ 或推理 [27]。然而，针对专家演示进行单轮交互优化可能会导致在多步问题中产生次优策略 [57, 38, 42]，尤其是在高度随机或非平稳的环境中。因此，本研究专注于构建能够从次优在线交互数据中学习的多轮 RL 算法。虽然先前的工作已经为 LLM 开发了基于价值的 RL 算法 [42, 38, 1, 57, 50]，但它们通常需要维护多个模型（如 Q 网络、价值网络和策略网络）及其延迟的目标网络，且容易出现收敛缓慢和对超参数选择敏感的问题。相比之下，我们致力于为从业者确定实例化简单且高效 RL 算法的关键设计选择，以显著提升全量级安卓设备控制。我们的方法可作为未来研究的基础模型。


## 3 Problem Setup and Preliminaries
## 3 问题设置与预备知识


Problem formulation. We are interested in pixel-based interaction with virtual devices. We scope our study in the control of Android devices: this is already significantly more challenging and more general than previous learning-based environments that focus solely on web navigation $\left\lbrack  {{16},{56},4}\right\rbrack$ , where the web browser itself is merely one application within our broader environment, and link-based device controls [47, 51] are inadequate for tasks like games that do not support link inputs.
问题定义。我们关注与虚拟设备的像素级交互。我们将研究范围限定在安卓设备控制：这比以往仅关注 Web 导航 $\left\lbrack  {{16},{56},4}\right\rbrack$ 的基于学习的环境更具挑战性且更通用，因为 Web 浏览器本身仅是我们更广泛环境中的一个应用，且基于链接的设备控制 [47, 51] 无法处理如不支持链接输入的游等戏任务。


Each episode begins with the emulator initialized to the home screen. Subsequently, a task is selected from a predefined set of language instructions, some examples of which are shown in Appendix A.1. An agent is then tasked with manipulating the emulator to fulfill this instruction. At each time step, the agent receives a screenshot of the current screen as the observation. Following the action space in prior literature [31],the available actions include tapping and sliding based on normalized $\left( {x,y}\right)$ coordinates (ranging from 0 to 1 relative to the screen dimensions), typing text strings of variable length, and pressing special buttons such as HOME, BACK, and ENTER, as illustrated in Figure 3. Our train and test instructions comes from General and Web Shopping subsets in AitW [31]. These tasks consist of information-gathering tasks like "What's on the menu of In-n-Out?", and shopping tasks on the web like "Go to newegg.com, search for razer kraken, and select the first entry".
每个回合开始时，模拟器被初始化为住主屏幕。随后，从预定义的语言指令集中选择一个任务（示例见附录 A.1）。智能体的任务是操作模拟器以完成该指令。在每个时间步，智能体接收当前屏幕的截图作为观测。遵循先前文献 [31] 的动作空间，可用动作包括基于归一化 $\left( {x,y}\right)$ 坐标（相对于屏幕尺寸在 0 到 1 之间）的点击和滑动、输入变长文本字符串，以及按下 HOME、BACK 和 ENTER 等特殊按钮，如图 3 所示。我们的训练和测试指令来自 AitW [31] 中的 General 和 Web Shopping 子集。这些任务包括“In-n-Out 的菜单上有什么？”等信息获取任务，以及“前往 newegg.com，搜索 razer kraken 并选择第一个条目”等网页购物任务。


Challenges of stochasticity. Real-world device contrl presents unique challenges of stochasticity absent in simulated environments [56, 37] such as: (1) the non-stationarity of websites and applications, which undergo frequent updates, causing the online observations to be different from stale offline data, (2) various unpredictable distractors such as pop-up advertisements, login requests, and the stochastic order of search results. (3) technical challenges and glitches such as incomplete webpage loading or temporary access restrictions to certain sites. Examples of scenarios with such stochasticity from our experiments are shown in Figure 3. We observe that these stochastic elements pose significant challenges for pre-trained VLMs, including even those fine-tuned on device control data. As a concrete example, Figure 4 shows an experiment result that illustrates the necessity of continuously adapting the models to the non-stationarity of websites and applications. After obtaining a good checkpoint using our approach (DigiRL), that we will introduce in the next section, with autonomous data from June.1 to June.3, we compare the performance of a frozen policy and a continuously updating policy using fresh autonomous data from June.7 to June.11. We find that indeed the the performance of the frozen policy gradually degrades over time due to the changes on websites and applications, while continuous online updates plays a key role in preventing this degradation.
随机性的挑战。现实世界的设备控制面临模拟环境 [56, 37] 中不存在的独特随机性挑战，例如：(1) 网站和应用程序的非平稳性，频繁的更新导致在线观测与陈旧的离线数据不同；(2) 各种不可预测的干扰因素，如弹窗广告、登录请求以及搜索结果的随机排序；(3) 技术挑战与故障，如网页加载不全或对某些站点的临时访问限制。图 3 展示了我们实验中具有此类随机性的场景示例。我们观察到，这些随机元素对预训练的 VLM 构成了重大挑战，即使是那些在设备控制数据上微调过的模型也不例外。作为一个具体的例子，图 4 展示了将模型持续适应网站和应用程序非平稳性的必要性。在使用下一节介绍的 DigiRL 方法，利用 6 月 1 日至 3 日的自主数据获得良好检查点后，我们对比了冻结策略与使用 6 月 7 日至 11 日新鲜自主数据持续更新策略的性能。我们发现，由于网站和应用程序的变化，冻结策略的性能确实随时间逐渐下降，而持续的在线更新在防止这种性能退化方面起着关键作用。


Setup for reliable and scalable online RL. As autonomous RL interleaves data collection and training, to maximize learning amidst stochas-ticity, it is crucial to have a real-time data collection pipeline to collect enough experience for gradient updates. While this is not possible in single-thread Android emulator environments [26, 39] due to latency, we parallelize our Android emulator using appropriate error handling as discussed in Appendix A.1. In addition, the environment must provide a reward signal by judging whether the current observation indicates the agent has successfully completed the task. To generalize our evaluator to support a wide range of tasks, we extend Pan et al. [26]'s end-to-end autonomous evaluator that does not require accessing the internal states of the emulator or human-written rules for each task. This contrasts previous works that manually write execution functions to verify the functional completeness of each task [16, 48, 37, 44]. We adopt Gemini 1.5 Pro [6, 7] as the backbone of the autonomous evaluator. We seed this model with few-shot rollouts and the associated human-labeled success indicators to guide evaluation of novel queries. This pipeline enables a single evaluator that can evaluate all AiTW tasks. The evaluator is highly aligned with human annotations (average error rate 2.8%), validated in Figure 8.
为可靠且可扩展的在线强化学习（RL）进行设置。由于自主 RL 交替进行数据收集与训练，为在随机性中最大化学习效果，建立实时数据收集流水线以获取足够的梯度更新经验至关重要。虽然在单线程 Android 模拟器环境 [26, 39] 中受延迟限制无法实现，但我们如附录 A.1 所述，通过适当的错误处理并行化了 Android 模拟器。此外，环境必须通过判断当前观察是否表明智能体已成功完成任务来提供奖励信号。为了使评估器支持广泛的任务，我们扩展了 Pan 等人 [26] 的端到端自主评估器，该评估器无需访问模拟器内部状态或为每个任务编写人工规则。这与以往为验证每个任务的功能完整性而手动编写执行函数的工作 [16, 48, 37, 44] 形成对比。我们采用 Gemini 1.5 Pro [6, 7] 作为自主评估器的骨干模型。我们通过少量示例（few-shot）轨迹及相关的人工标注成功指标为该模型提供种子，以指导对新查询的评估。该流水线实现了可评估所有 AiTW 任务的单一评估器。如图 8 验证所示，该评估器与人工标注高度一致（平均错误率 2.8%）。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_4.jpg?x=910&y=520&w=560&h=382&r=0"/>



Figure 4: Performance of our approach (DigiRL) in different training modes on the Webshop subset. When utilizing a stale checkpoint, i.e., "frozen" (black+blue curve) performance generally begins to degrade as time evolves, whereas autonomous online training (black+red curve) via DigiRL allows us to retain performance despite non-stationarity and stochasticity.
图 4：我们的方法（DigiRL）在 Webshop 子集上不同训练模式下的表现。当使用过时的检查点（即“冻结”状态，黑+蓝曲线）时，性能通常随时间推移而下降，而通过 DigiRL 进行的自主在线训练（黑+红曲线）使我们能够在非平稳性和随机性的情况下保持性能。


## 4 DigiRL: Autonomous RL for Building a Strong Device-Control Agent
## 4 DigiRL：用于构建强设备控制智能体的自主强化学习


We now present our autonomous RL framework for training device agents. We pose the device control problem as a Markov decision process (MDP) and develop RL methods for this MDP. The core of our approach is based on a simple and scalable off-policy RL method, advantage-weighted regression (AWR) [29], but we make crucial modifications to handle stochasticity and highly-variable task difficulty, through the use of value functions trained with appropriate losses, and an automatic curriculum, induced by an instruction-level value function to maximize learning.
我们现在介绍用于训练设备智能体的自主 RL 框架。我们将设备控制问题建模为马尔可夫决策过程（MDP），并为该 MDP 开发强化学习方法。我们方法的核心基于一种简单且可扩展的离策（off-policy）强化学习方法——优势加权回归（AWR）[29]，但我们进行了关键改进，通过使用以适当损失函数训练的值函数，以及由指令级值函数引导的自动课程，来处理随机性和高度变化的任务难度，从而实现学习最大化。


Device control and GUI navigation as a MDP. We conceptualize device control guided by natural language instructions as a finite horizon Markov Decision Process (MDP) represented by $\mathcal{M} = \{ \mathcal{S},\mathcal{A},\mathcal{T},{\mu }_{0},\mathcal{R},H\}$ and run policy gradient to solve this MDP. At the beginning,an initial state ${s}_{0}$ and a natural language instruction $c$ are sampled from the initial state distribution ${\mu }_{0}$ . A reward of 1 is given at the end if the agent successfully fulfills the task per the evaluator, otherwise a reward of 0 is given. The trajectory terminates either when the agent accomplishes the task or when the maximum allowed number of interactions $H$ is exceeded. States are represented using the last two screenshots. To explain our approach in detail, we also include several standard definitions used in reinforcement learning (RL). The Q function for a policy $\pi$ represents the expected long-term return from taking a specific action at the current step and then following policy $\pi$ thereafter: ${Q}^{\pi }\left( {{s}_{h},{a}_{h},c}\right)  = {\mathbb{E}}_{\pi }\left\lbrack  {\mathop{\sum }\limits_{{t = h}}^{H}r\left( {{s}_{t},{a}_{t},c}\right) }\right\rbrack$ . The value function ${V}^{\pi }\left( {{s}_{h},c}\right)$ is calculated by averaging the Q-value, ${Q}^{\pi }\left( {{s}_{h},{a}_{h},c}\right)$ ,over actions ${a}_{h}$ drawn from the policy $\pi$ . The advantage ${A}^{\pi }\left( {{s}_{h},{a}_{h},c}\right)$ for a state-action pair is computed by subtracting the state's value under the policy from its Q-value: ${A}^{\pi }\left( {{s}_{h},{a}_{h},c}\right)  = {\bar{Q}}^{\pi }\left( {{s}_{h},{a}_{h},c}\right)  - {V}^{\pi }\left( {{s}_{h},c}\right) .$
作为 MDP 的设备控制和 GUI 导航。我们将由自然语言指令引导的设备控制概念化为由 $\mathcal{M} = \{ \mathcal{S},\mathcal{A},\mathcal{T},{\mu }_{0},\mathcal{R},H\}$ 表示的有穷马尔可夫决策过程（MDP），并运行策略梯度来解决此 MDP。开始时，从初始状态分布 ${\mu }_{0}$ 中采样初始状态 ${s}_{0}$ 和自然语言指令 $c$。如果智能体根据评估器成功完成了任务，则在结束时给予 1 的奖励，否则给予 0。当智能体完成任务或超过最大允许交互次数 $H$ 时，轨迹终止。状态使用最后两张截图表示。为了详细解释我们的方法，我们还包含了强化学习中使用的几个标准定义。策略 $\pi$ 的 Q 函数表示在当前步骤采取特定动作并随后遵循策略 $\pi$ 的预期长期回报：${Q}^{\pi }\left( {{s}_{h},{a}_{h},c}\right)  = {\mathbb{E}}_{\pi }\left\lbrack  {\mathop{\sum }\limits_{{t = h}}^{H}r\left( {{s}_{t},{a}_{t},c}\right) }\right\rbrack$。值函数 ${V}^{\pi }\left( {{s}_{h},c}\right)$ 通过对从策略 $\pi$ 中采样的动作 ${a}_{h}$ 的 Q 值 ${Q}^{\pi }\left( {{s}_{h},{a}_{h},c}\right)$ 求平均值来计算。状态-动作对的优势 ${A}^{\pi }\left( {{s}_{h},{a}_{h},c}\right)$ 通过从其 Q 值中减去该状态在策略下的价值来计算：${A}^{\pi }\left( {{s}_{h},{a}_{h},c}\right)  = {\bar{Q}}^{\pi }\left( {{s}_{h},{a}_{h},c}\right)  - {V}^{\pi }\left( {{s}_{h},c}\right) .$


### 4.1 Backbone of Our Approach: Off-Policy RL via Advantage-Weighted Regression
### 4.1 方法核心：通过优势加权回归实现的离策强化学习


The starting point we choose to build our approach on is the advantage-weighted regression (AWR) algorithm [29], which says that we can improve the policy reliably by regressing the policy towards exponentiated advantages induced by the reward function, as a proxy for optimizing the policy gradient while staying close to the previous policy $\left\lbrack  {{14},{35},{34}}\right\rbrack$ :
我们选择构建方法的基础起点是优势加权回归（AWR）算法 [29]，该算法指出，通过将策略向奖励函数诱导的指数化优势进行回归，可以可靠地改进策略，这作为在保持接近先前策略 $\left\lbrack  {{14},{35},{34}}\right\rbrack$ 的同时优化策略梯度的代理方法：


$$
\arg \mathop{\max }\limits_{\pi }{\mathbb{E}}_{\nu }\left\lbrack  {\log \pi \left( {a \mid  s,c}\right)  \cdot  \exp \left( {A\left( {s,a,c}\right) /\beta }\right) }\right\rbrack  , \tag{4.1}
$$



for some positive parameter $\beta$ and the distribution of past experience $\nu$ ,and $A\left( {s,a,c}\right)$ denotes the advantage of a state-action pair $\left( {s,a}\right)$ given a context $c$ . To avoid tuning the hyperparameter $\beta$ ,we consider an alternative that does "hard filtering" on the advantages instead of computing $\exp \left( A\right)$ , similar to prior works [22, 43]. This leads to the following loss function for fine-tuning the model:
其中 $\beta$ 为正参数，$\nu$ 为过去经验的分布，$A\left( {s,a,c}\right)$ 表示给定上下文 $c$ 下状态-动作对 $\left( {s,a}\right)$ 的优势。为了避免调整超参数 $\beta$，我们考虑一种替代方案，对优势进行“硬过滤”而非计算 $\exp \left( A\right)$，这与之前的工作 [22, 43] 类似。这为模型微调带来了如下损失函数：


$$
\mathcal{L}\left( \pi \right)  =  - {\mathbb{E}}_{\text{ filter }\left( \nu \right) }\left\lbrack  {\log \pi \left( {a \mid  s,c}\right) }\right\rbrack  . \tag{4.2}
$$



Typically, these advantages are computed by running Monte-Carlo (MC) rollouts in the environment to estimate the value of a given state-action pair, and subtracting from it an estimate of the value of the state given by a learned value estimator alone. However, this approach is likely to produce high-variance advantages given the stochasticity of the device eco-system that affects MC rollouts.
通常，这些优势是通过在环境中运行蒙特卡洛（MC）采样来估计给定状态-动作对的值，并减去由学习到的价值评估器单独给出的状态价值估计来计算的。然而，考虑到影响 MC 采样的设备生态系统的随机性，这种方法可能会产生高方差的优势。


### 4.2 Obtaining Reliable Advantage Estimates from Doubly-Robust Estimators
### 4.2 从双重稳健评估器获取可靠的优势估计


To reliably identify advantageous actions given significant environment stochasticity, we construct a per-step advantage estimator,inspired by doubly-robust estimators [40, 36]:
为了在环境具有显著随机性的情况下可靠地识别有利动作，受双重稳健评估器 [40, 36] 的启发，我们构建了一个每步优势评估器：


$$
{A}^{\text{ step }}\left( {{s}_{h},{a}_{h},c}\right)  \mathrel{\text{ := }} {\lambda }^{H - h}r\left( {{s}_{H},{a}_{H},c}\right)  + \left( {1 - {\lambda }^{H - h}r\left( {{s}_{H},{a}_{H},c}\right) }\right) \left( {{V}^{\text{ step }}\left( {{s}_{h + 1},c}\right)  + r\left( {{s}_{h},{a}_{h},c}\right)  - {V}^{\text{ step }}\left( {{s}_{h},c}\right) }\right) ,
$$



(4.3)



where $\lambda$ is a weighting hyper-parameter. This construction of the advantage estimator is a simplified version of Generalized Advantage Estimation (GAE) [36] using only the next-step advantage estimator and final-step advantage estimator as there are no intermediate rewards in our problem. This construction balances an advantage estimator with higher variance Monte-Carlo estimates ${\lambda }^{H - h}r\left( {{s}_{H},{a}_{H},c}\right)$ (due to stochasticity) and an estimator with higher bias ${V}^{\text{ step }}\left( {{s}_{h + 1},c}\right)  + r\left( {{s}_{h},{a}_{h},c}\right)  - {V}^{\text{ step }}\left( {{s}_{h},c}\right)$ (due to imperfect fitting of the value function). We observed that combining both high-variance and high-bias estimators gave us a sweet-spot in terms of performance. To implement the step-level hard filtering,we simply threshold this doubly robust estimator as ${A}^{\text{ step }}\left( {{s}_{h},{a}_{h},c}\right)  > 1/H$ to decide which actions progress towards the goal.
其中 $\lambda$ 是权重超参数。这种优势评估器的构建是广义优势估计（GAE）[36] 的简化版本，由于我们的问题中没有中间奖励，仅使用了下一步优势评估器和最终步优势评估器。这种构建平衡了具有较高方差的蒙特卡洛估计 ${\lambda }^{H - h}r\left( {{s}_{H},{a}_{H},c}\right)$（由于随机性）和具有较高偏差的评估器 ${V}^{\text{ step }}\left( {{s}_{h + 1},c}\right)  + r\left( {{s}_{h},{a}_{h},c}\right)  - {V}^{\text{ step }}\left( {{s}_{h},c}\right)$（由于价值函数拟合不完美）。我们观察到，结合高方差和高偏差评估器在性能方面达到了理想的平衡点。为了实现步骤级的硬过滤，我们只需将此双重稳健评估器设定阈值为 ${A}^{\text{ step }}\left( {{s}_{h},{a}_{h},c}\right)  > 1/H$，以决定哪些动作向目标推进。


### 4.3 Automatic Curriculum using an Instruction-Level Value Function
### 4.3 使用指令级价值函数的自动课程学习


While the AWR update (Equation 4.1) coupled with a robust advantage estimator (Equation 4.3) is likely sufficient on standard RL tasks, we did not find it to be effective enough for device control in preliminary experiments. Often this was the case because the task set presents tasks with highly-variable difficulties that collecting more data on tasks that the agent was already proficient at affected sample efficieny negatively. In contrast, maximal learning signal can be derived by experiencing the most informative tasks for the agent during training. To this end, we design an instruction-level value function ${V}^{\text{ instruct }}\left( c\right)$ to evaluate if a given rollout can provide an effective learning signal:
虽然 AWR 更新（等式 4.1）结合稳健的优势评估器（等式 4.3）在标准 RL 任务上可能已经足够，但在初步实验中，我们发现它对于设备控制不够有效。这通常是因为任务集中的任务难度差异巨大，在智能体已经熟练的任务上收集更多数据会对采样效率产生负面影响。相比之下，通过在训练期间经历对智能体信息量最大的任务，可以获得最大的学习信号。为此，我们设计了一个指令级价值函数 ${V}^{\text{ instruct }}\left( c\right)$，用以评估给定的采样是否能提供有效的学习信号：


$$
{A}^{\text{ instruct }}\left( {{s}_{h},{a}_{h},c}\right)  \mathrel{\text{ := }} \mathop{\sum }\limits_{{t = h}}^{H}r\left( {{s}_{t},{a}_{t},c}\right)  - {V}^{\text{ instruct }}\left( c\right)  = r\left( {{s}_{H},{a}_{H},c}\right)  - {V}^{\text{ instruct }}\left( c\right) , \tag{4.4}
$$



where $\mathop{\sum }\limits_{{t = h}}^{H}r\left( {{s}_{t},{a}_{t},c}\right)$ is a Monte-Carlo estimator of $Q\left( {{s}_{h},{a}_{h},c}\right)$ . The equality holds because the MDP formulation only provides rewards at the end of a rollout. Intuitively, if a rollout attains a high value of ${A}^{\text{ instruct }}\left( {{s}_{h},{a}_{h},c}\right)$ ,it means the value function ${V}^{\text{ instruct }}$ is small. Therefore,this rollout represents a valuable experience of the agent accomplishing a difficult task, and thus should be prioritized, akin to ideas pertaining to prioritized experience [32] or level replay [11]. When training the actor with a buffer of historical off-policy data, we first perform a filtering step to identify the top- $p$ datapoints with highest ${A}^{\text{ instruct }}\left( {{s}_{h},{a}_{h},c}\right)$ . Then,we use it for AWR (Equation 4.1) with the doubly-robust advantage estimator (Equation 4.3).
其中 $\mathop{\sum }\limits_{{t = h}}^{H}r\left( {{s}_{t},{a}_{t},c}\right)$ 是 $Q\left( {{s}_{h},{a}_{h},c}\right)$ 的蒙特卡洛估计量。该等式成立是因为 MDP 公式仅在 rollout 结束时提供奖励。直观地说，如果一个 rollout 获得了较高的 ${A}^{\text{ instruct }}\left( {{s}_{h},{a}_{h},c}\right)$ 值，这意味着价值函数 ${V}^{\text{ instruct }}$ 较小。因此，该 rollout 代表了智能体完成一项困难任务的宝贵经验，应当被优先考虑，这类似于优先经验重放 [32] 或关卡重放 [11] 的思想。当使用历史离策数据缓冲区训练 Actor 时，我们首先执行过滤步骤，以识别具有最高 ${A}^{\text{ instruct }}\left( {{s}_{h},{a}_{h},c}\right)$ 的前 $p$ 个数据点。然后，我们将其用于 AWR（等式 4.1）以及双重稳健优势估计器（等式 4.3）。


Implementation details. Inspired by the findings in some recent works [5, 17] that modern deep learning architectures like transformers [41] are better trained with cross-entropy losses instead of mean-squared losses, we utilize a cross-entropy objective based on the Monte-Carlo estimate of the trajectory reward for training both of our value functions:
实现细节。受近期一些工作 [5, 17] 的启发，即 Transformer [41] 等现代深度学习架构使用交叉熵损失比均方损失训练效果更好，我们利用基于轨迹奖励蒙特卡洛估计的交叉熵目标来训练我们的两个价值函数：


$$
\mathcal{L}\left( {V}^{\text{ traj }}\right)  =  - {\mathbb{E}}_{\nu }\left\lbrack  {r\left( {{s}_{H},{a}_{H},c}\right) \log {V}^{\text{ traj }}\left( c\right)  + \left( {1 - r\left( {{s}_{H},{a}_{H},c}\right) }\right) \log \left( {1 - {V}^{\text{ traj }}\left( c\right) }\right) }\right\rbrack  , \tag{4.5}
$$



$$
\mathcal{L}\left( {V}^{\text{ step }}\right)  =  - {\mathbb{E}}_{\nu }\left\lbrack  {r\left( {{s}_{H},{a}_{H},c}\right) \log {V}^{\text{ step }}\left( {{s}_{h},{a}_{h},c}\right)  + \left( {1 - r\left( {{s}_{H},{a}_{H},c}\right) }\right) \log \left( {1 - {V}^{\text{ step }}\left( {{s}_{h},{a}_{h},c}\right) }\right) }\right\rbrack  . \tag{4.6}
$$



<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_6.jpg?x=421&y=201&w=960&h=439&r=0"/>



Figure 5: Algorithm visualization. The two value function are first trained with original distribution of collected trajectories according to Equation (4.5) and Equation (4.6), then used to filter the trajectories for training the actor. We use the MLE loss (Maximum Likelihood Estimation loss) to train the actor.
图 5：算法可视化。首先根据等式 (4.5) 和等式 (4.6) 使用收集到的轨迹原始分布训练两个价值函数，然后用于过滤轨迹以训练 Actor。我们使用 MLE 损失（极大似然估计损失）来训练 Actor。


Final algorithm. The final practical algorithm is shown in Figure 5. The instruction-level value function estimates the values of the trajectories, which is trained with loss shown in Equation (4.5). The step-level value function estimates the values of states, which is trained with loss shown in Equation (4.6). When training the actor, we first filter out trajectories and states using the value functions as shown in Equation (4.4) and Equation (4.3), then train the actor with the MLE loss shown in Equation (4.2) on the filtered data.
最终算法。最终的实际算法如图 5 所示。指令级价值函数估计轨迹的价值，使用等式 (4.5) 所示的损失进行训练。步骤级价值函数估计状态的价值，使用等式 (4.6) 所示的损失进行训练。在训练 Actor 时，我们首先根据等式 (4.4) 和等式 (4.3) 使用价值函数过滤出轨迹和状态，然后在过滤后的数据上使用等式 (4.2) 所示的 MLE 损失训练 Actor。


## 5 Experimental Evaluation
## 5 实验评估


The goal of our experiments is to evaluate the performance of DigiRL on challenging Android device control problems. Specifically, we are interested in understanding if DigiRL can produce agents that can effectively learn from autonomous interaction, while still being able to utilize offline data for learning. To this end, we perform a comparative analysis of DigiRL against several prior approaches, including state-of-the-art agents in Section 5.1. We also perform several ablation experiments to understand the necessity and sufficiency of various components of our approach in Section 5.2.
我们实验的目的是评估 DigiRL 在具有挑战性的 Android 设备控制问题上的性能。具体而言，我们有兴趣了解 DigiRL 是否能够产生既能有效地从自主交互中学习，同时仍能利用离线数据进行学习的智能体。为此，我们在第 5.1 节中对 DigiRL 与包括最先进智能体在内的几种先前方法进行了对比分析。我们还在第 5.2 节中进行了几项消融实验，以了解我们方法中各种组件的必要性和充分性。


Baselines and comparisons. We compare DigiRL with: (a) state-of-the-art agents built around proprietary VLMs, with the use of several prompting and retrieval-style techniques; (b) running imitation learning on static human demonstrations with the same instruction distribution, and (c)a filtered BC approach [26]. For proprietary VLMs, we evaluate GPT-4V [24] and Gemini 1.5 Pro [7] both zero-shot and when augmented with carefully-designed prompts. For the zero-shot setting, we use the prompt from Yang et al. [47] and augment the observation with Set-of-Marks [55]. Set-of-Marks overlays a number for each interactable element over the screenshot, so that a VLM can directly output the number of the element to interact with in plain text instead of attempting to calculate pixel coordinates, which is typically significantly harder. We also compare with AppAgent [47], which first prompts the VLM to explore the environment, and appends the experience collected to the test-time prompt. We also compare with two state-of-the-art fine-tuning methods for Android device control: AutoUI (specifically AutoUI-Base [53]) and CogAgent [9]. AutoUI-Base uses an LM with 200M parameters, and a a vision encoder with 1.1B parameters. CogAgent has 11B parameters for its vision encoder and 7B for its LM. The supervised training corpus for both AutoUI-Base and CogAgent contains AitW, including the instruction set and the emulator configuration we use.
基准测试与比较。我们将 DigiRL 与以下各项进行比较：(a) 基于商用 VLM 并结合多种提示和检索技术的先进智能体；(b) 在具有相同指令分布的静态人类演示上运行模仿学习；以及 (c) 过滤式 BC 方法 [26]。对于商用 VLM，我们评估了零样本设置下以及通过精心设计提示增强后的 GPT-4V [24] 和 Gemini 1.5 Pro [7]。在零样本设置中，我们使用来自 Yang 等人 [47] 的提示，并使用 Set-of-Marks [55] 增强观测。Set-of-Marks 在屏幕截图上为每个可交互元素叠加一个数字，以便 VLM 可以直接以纯文本形式输出要交互的元素编号，而不是尝试计算通常难度大得多的像素坐标。我们还与 AppAgent [47] 进行了比较，该方法首先提示 VLM 探索环境，并将收集到的经验附加到测试时的提示中。我们还与两种最先进的 Android 设备控制微调方法进行了比较：AutoUI（特别是 AutoUI-Base [53]）和 CogAgent [9]。AutoUI-Base 使用一个具有 2 亿参数的 LM 和一个具有 11 亿参数的视觉编码器。CogAgent 的视觉编码器拥有 110 亿参数，其 LM 拥有 70 亿参数。AutoUI-Base 和 CogAgent 的监督训练语料库均包含 AitW，包括我们使用的指令集和模拟器配置。


Base VLM and offline dataset. Both Filtered BC and DigiRL use trained AutoUI-Base checkpoints with the image encoder frozen. The instruction and step-level value functions for DigiRL employ this same frozen image encoder. The visual features output from the encoder are concatenated with instruction features derived from RoBERTa [21]. A two-layer MLP is then used to predict the value function. In the offline phase, the offline dataset is collected by rolling out the initial AutoUI-Base supervised trained checkpoint as policy. For fair comparisons, we keep the number of offline data collected in the pure offline training roughly the same as the total number of data collected in the offline-to-online training. Due to the dynamic nature of the Internet-device eco-system, our offline data was stale by the time we were able to run our offline-to-online experiments, and this presented additional challenge in offline-to-online learning. In both General and Web Shopping subsets, offline experiments make use of around 1500 trajectories while offline-to-online experiments start with
基础VLM与离线数据集。Filtered BC和DigiRL均使用冻结图像编码器的已训练AutoUI-Base检查点。DigiRL的指令和步骤级价值函数也采用相同的冻结图像编码器。编码器输出的视觉特征与源自RoBERTa [21]的指令特征连接。随后使用双层MLP预测价值函数。在离线阶段，通过运行初始AutoUI-Base监督训练检查点作为策略来收集离线数据集。为了公平比较，我们保持纯离线训练中收集的离线数据数量与离线转在线训练中收集的总数据量大致相同。由于互联网设备生态系统的动态特性，当我们能够运行离线转在线实验时，我们的离线数据已经过时，这为离线转在线学习带来了额外挑战。在通用和网络购物子集中，离线实验使用约1500条轨迹，而离线转在线实验则从以下规模开始


<table><tr><td colspan="2" rowspan="2"></td><td rowspan="2"></td><td colspan="2">AitW General</td><td colspan="2">AitW Web Shopping</td></tr><tr><td>Train</td><td>Test</td><td>Train</td><td>Test</td></tr><tr><td rowspan="4">Prompting</td><td rowspan="2">SET-OF-MARKS</td><td>GPT-4V</td><td>5.2</td><td>13.5</td><td>3.1</td><td>8.3</td></tr><tr><td>Gemini 1.5 Pro</td><td>32.3</td><td>16.7</td><td>6.3</td><td>11.5</td></tr><tr><td rowspan="2">APPAGENT</td><td>GPT-4V</td><td>13.5</td><td>17.7</td><td>12.5</td><td>8.3</td></tr><tr><td>Gemini 1.5 Pro</td><td>14.6</td><td>16.7</td><td>5.2</td><td>8.3</td></tr><tr><td rowspan="6">Learning</td><td>Supervised</td><td>CogAgent</td><td>25.0</td><td>25.0</td><td>31.3</td><td>38.5</td></tr><tr><td>TRAINING</td><td>AutoUI</td><td>12.5</td><td>14.6</td><td>14.6</td><td>17.7</td></tr><tr><td rowspan="2">OFFLINE</td><td>Filtered BC</td><td>51.7 ± 5.4</td><td>${50.7} \pm  {1.8}$</td><td>44.7 ± 1.6</td><td>${45.8} \pm  {0.9}$</td></tr><tr><td>Ours</td><td>46.9 ± 5.6</td><td>62.8±1.0</td><td>39.3 ± 6.0</td><td>${45.8} \pm  {6.6}$</td></tr><tr><td rowspan="2">OFF-TO-ON</td><td>Filtered BC</td><td>53.5 ± 0.8</td><td>61.5±1.1</td><td>53.6 ± 4.7</td><td>${57.8} \pm  {2.6}$</td></tr><tr><td>Ours</td><td>63.5±0.0</td><td>71.9±1.1</td><td>68.2 ± 6.8</td><td>$\mathbf{{67.2} \pm  {1.5}}$</td></tr></table>
<table><tbody><tr><td colspan="2" rowspan="2"></td><td rowspan="2"></td><td colspan="2">AitW 通用</td><td colspan="2">AitW 网页购物</td></tr><tr><td>训练集</td><td>测试集</td><td>训练集</td><td>测试集</td></tr><tr><td rowspan="4">提示词</td><td rowspan="2">标记集 (SoM)</td><td>GPT-4V</td><td>5.2</td><td>13.5</td><td>3.1</td><td>8.3</td></tr><tr><td>Gemini 1.5 Pro</td><td>32.3</td><td>16.7</td><td>6.3</td><td>11.5</td></tr><tr><td rowspan="2">APPAGENT</td><td>GPT-4V</td><td>13.5</td><td>17.7</td><td>12.5</td><td>8.3</td></tr><tr><td>Gemini 1.5 Pro</td><td>14.6</td><td>16.7</td><td>5.2</td><td>8.3</td></tr><tr><td rowspan="6">学习</td><td>监督式</td><td>CogAgent</td><td>25.0</td><td>25.0</td><td>31.3</td><td>38.5</td></tr><tr><td>训练</td><td>AutoUI</td><td>12.5</td><td>14.6</td><td>14.6</td><td>17.7</td></tr><tr><td rowspan="2">离线</td><td>过滤后的 BC</td><td>51.7 ± 5.4</td><td>${50.7} \pm  {1.8}$</td><td>44.7 ± 1.6</td><td>${45.8} \pm  {0.9}$</td></tr><tr><td>我们的</td><td>46.9 ± 5.6</td><td>62.8±1.0</td><td>39.3 ± 6.0</td><td>${45.8} \pm  {6.6}$</td></tr><tr><td rowspan="2">离线转在线</td><td>过滤后的 BC</td><td>53.5 ± 0.8</td><td>61.5±1.1</td><td>53.6 ± 4.7</td><td>${57.8} \pm  {2.6}$</td></tr><tr><td>我们的</td><td>63.5±0.0</td><td>71.9±1.1</td><td>68.2 ± 6.8</td><td>$\mathbf{{67.2} \pm  {1.5}}$</td></tr></tbody></table>


Table 1: Main comparisons of different agents across various settings. Each offline experiment is repeated three times and the mean and standard deviation are reported. Each online experiment is repeated two times. Results are evaluated with our autonomous evaluator with the first 96 instructions in the train and test set. Correlation of our correlation and human judgements can be found in Figure 8.
表 1：不同智能体在各种设置下的主要对比。每个离线实验重复三次，并报告均值和标准差。每个在线实验重复两次。结果通过我们的自主评估器对训练集和测试集中的前 96 条指令进行评估。我们的评估与人类判断的相关性见图 8。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_7.jpg?x=327&y=843&w=1147&h=367&r=0"/>



Figure 6: Offline-to-online training curves for Filtered BC and DigiRL. Curves are smoothed with exponential weighting over the x-axis. Left: AitW General. Right: AitW Web Shopping. Two runs for each model are started on two different dates with at least two days apart. Observe that DigiRL is able to improve faster with a fewer number of samples. Since the data collection frequency is the bottleneck, these performance trends directly reflect performance trends against wall-clock time as well.
图 6：Filtered BC 和 DigiRL 的离线到在线训练曲线。曲线经过沿 x 轴的指数加权平滑。左图：AitW General。右图：AitW Web Shopping。每个模型的两次运行分别在相隔至少两天的两个不同日期开始。观察到 DigiRL 能够以更少的样本量实现更快的提升。由于数据采集频率是瓶颈，这些性能趋势也直接反映了随实际耗时变化的性能趋势。


around 500 offline trajectories and update with another 1000 online trajectories. In the offline phase, DigiRL skips instruction-level filtering and instead trains the actor with all successful trajectories to make full use of the offline data. See a detailed breakdown of our dataset in Appendix A.1.
约 500 条离线轨迹，并使用另外 1000 条在线轨迹进行更新。在离线阶段，DigiRL 跳过指令级过滤，而是使用所有成功的轨迹训练 actor，以充分利用离线数据。数据集的详细细分见附录 A.1。


### 5.1 Main Results
### 5.1 主要结果


Our main results are summarized in Table 1 and Figure 6. We find that on both AitW General and AitW Web Shopping subsets, the agent trained via DigiRL significantly outperforms prior state-of-the-art methods based on prompting and retrieval (AppAgent + GPT-4V/Gemini 1.5 Pro) or training on static demonstrations (CogAgent and AutoUI), by a large margin with more than 49.5% absolute improvement (from 17.7% to 71.9% on the General subset and from 17.7% to 67.2% on the Web Shopping subset). Notably, this improvement from DigiRL is realized fully autonomously without making use of human supervision (e.g. manually labeled rollouts or hand-written verifiers).
我们的主要结果总结在表 1 和图 6 中。我们发现，在 AitW General 和 AitW Web Shopping 子集上，通过 DigiRL 训练的智能体显著优于以往基于提示和检索（AppAgent + GPT-4V/Gemini 1.5 Pro）或基于静态演示训练（CogAgent 和 AutoUI）的一流方法，具有超过 49.5% 的绝对提升（General 子集从 17.7% 提升至 71.9%，Web Shopping 子集从 17.7% 提升至 67.2%）。值得注意的是，DigiRL 的这种提升是完全自主实现的，无需使用人类监督（例如手动标记的展开过程或手工编写的验证器）。


Are inference-time prompting and retrieval techniques or supervised training enough for device control? Delving into Table 1, we observe that off-the-shelf proprietary VLMs, even when supplemented with the set-of-marks mechanism, do not attain satisfactory performance: both GPT-4V and Gemini 1.5 Pro achieve success rates under ${20}\%$ . One possible cause could be the underrepresentation of Android device data in the pre-training data. Moreover, inference-time adaptation strategies such as AppAgent [47] show minimal improvement, with gains not exceeding 5% for either model. All this evidence suggests a limited scope for improvement without fine-tuning of some sort. As illustrated in Figure 7, the primary failures of these VLMs stem from hallucinatory reasoning that lead the VLMs to land on a relevant but wrong page. This suggests that while state-of-the-art VLMs excel at reasoning problems in code and math, their reliability in less-familiar domains, such as device control, remains inadequate. For example, for the instruction "Go to newegg.com, search for alienware area 51, and select the first entry", a GPT-4V based agent erroneously searched "alien area 51 ebay" in Google.com and decided that it had made progress towards the task (Figure 17).
推理时提示和检索技术或监督训练是否足以进行设备控制？深入研究表 1，我们观察到现成的商业 VLM 即使辅以 set-of-marks 机制，也无法获得令人满意的性能：GPT-4V 和 Gemini 1.5 Pro 的成功率均在 ${20}\%$ 以下。一个可能的原因可能是预训练数据中 Android 设备数据的代表性不足。此外，AppAgent [47] 等推理时适配策略显示出的改进极小，两种模型的增益均不超过 5%。所有这些证据表明，如果不进行某种形式的微调，改进空间有限。如图 7 所示，这些 VLM 的主要失败源于幻觉推理，导致 VLM 停留在相关但错误的页面上。这表明，虽然最先进的 VLM 擅长代码和数学中的推理问题，但它们在设备控制等不太熟悉的领域的可靠性仍然不足。例如，对于指令“去 newegg.com，搜索 alienware area 51，并选择第一个条目”，基于 GPT-4V 的智能体错误地在 Google.com 中搜索了“alien area 51 ebay”，并判定其在任务上取得了进展（图 17）。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_8.jpg?x=426&y=201&w=946&h=330&r=0"/>



Figure 7: Failure modes for each approach on both the AiTW General and Web Shopping subsets. We found that the failure mode RL training is most effective at reducing compared to model supervised trained on human data is "Fail to recover from mistakes". A more fine-grained decomposition can be found in Appendix D.
图 7：AiTW General 和 Web Shopping 子集上每种方法的失败模式。我们发现，与在人类数据上进行监督训练的模型相比，RL 训练在减少“无法从错误中恢复”这一失败模式方面最为有效。更细粒度的分解见附录 D。


Training on domain-specific human demonstrations, however, does boost performance, allowing the smaller, specialized VLM, AutoUI with 1.5 billion parameters, to match or surpass the larger, generalist VLMs like GPT-4V and Gemini 1.5 Pro. Nonetheless, this supervised imitation learning approach still fall short, with success rates on both subsets remaining below 20%. This shortcoming is not fundamentally addressed via enhancements in model scale or architecture, as evidenced by CogAgent [9], with 18 billion parameters still achieving performances below 40% success rate. As depicted in Figure 7, a predominant failure mode for these agents is an inability to rectify their own errors. An example trajectory that we observed is that for the instruction "what's on the menu of In-n-Out", the agent accidentally activated the voice input button, and failed to quit that page until the step limit. In contrast, DigiRL is able to recover from the errors more efficiently( Appendix C.2).
然而，在特定领域的辅助下进行人类演示训练确实能提升性能，使拥有 15 亿参数的小型专用 VLM AutoUI 能够达到或超过 GPT-4V 和 Gemini 1.5 Pro 等大型通用 VLM。尽管如此，这种监督模仿学习方法仍然力有不逮，两个子集的成功率均保持在 20% 以下。这一缺陷并未通过模型规模或架构的增强得到根本解决，CogAgent [9] 拥有 180 亿参数，其成功率仍低于 40%。如图 7 所示，这些智能体的一种主要失败模式是无法纠正自身的错误。我们观察到的一个轨迹示例是：对于指令“In-n-Out 的菜单上有什么”，智能体误触了语音输入按钮，直到步数限制都未能退出该页面。相比之下，DigiRL 能够更有效地从错误中恢复（附录 C.2）。


Comparison of different RL approaches. In Table 1 and Figure 6, we present a comparative analysis of various autonomous approaches. Notably, both offline and offline-to-online configurations demonstrate that our RL approach, when augmented with a continuous stream of autonomous interaction data and reward feedback, substantially improves performance. This improvement is evident from an increase in the success rate from under ${20}\%$ to over ${40}\%$ ,as the agent learns to adapt to stochastic and non-stationary device interfaces. Moreover, although the total sample sizes for offline and offline-to-online settings are equivalent, the top-performing offline-to-online algorithm markedly surpasses its offline counterpart (75% versus 62.8% on the General subset). This highlights the efficacy of autonomous environment interaction, and establishes the efficacy of DigiRL in learning from such uncurated, sub-optimal data. Lastly, DigiRL consistently outperforms the state-of-the-art alternative, Filtered BC, across both the General and Web Shopping subsets, improving from 61.5% to 71.9% and 57.8% to 61.4%, respectively, highlighting DigiRL's performance and efficiency.
不同RL方法的比较。在表1和图6中，我们对各种自主方法进行了对比分析。值得注意的是，离线和离线到在线配置均表明，当我们的RL方法辅以持续的自主交互数据流和奖励反馈时，性能得到了显著提升。随着智能体学会适应随机且非平稳的设备界面，成功率从低于 ${20}\%$ 提升至 ${40}\%$ 以上，这一进步显而易见。此外，尽管离线和离线到在线设置的总样本量相等，但表现最佳的离线到在线算法显著超越了其离线对应版本（在General子集上为75%对62.8%）。这突显了自主环境交互的有效性，并确立了DigiRL从这类未经筛选的次优数据中学习的能力。最后，DigiRL在General和Web Shopping子集上的表现始终优于最先进的替代方案Filtered BC，分别从61.5%提升至71.9%以及从57.8%提升至61.4%，彰显了DigiRL的性能与效率。


### 5.2 Analysis and Ablations
### 5.2 分析与消融研究


Failure modes analysis. We conduct an additional user study to annotate the failure modes for each agent as shown in Figure 7, and a more fine-grained breakdown can be found in Appendix D. At a high level, we classify the major failure modes of all agents into the following three categories: (1) Failure to recover from mistakes refers to the scenario where the agent made a mistake that led it to states from which it failed to quickly recover and resume the task, such as a wrong search page. (2) Getting stuck midway refers to the failure mode where the agent gets distracted on the right track to completing the instruction and as a result fails to accomplish the task. For example, failing to click on the right link or failing to search after typing the key words. (3) Arriving at wrong goal refers to the failure mode where the agent arrives at a wrong page and mistakenly thinks that it had completed the task. For e.g, the agent finds a macbook on costco.com instead of finding a macbook on ebay.com.
失败模式分析。我们进行了额外的用户研究，以标注如图7所示的每个智能体的失败模式，更详细的细分可以在附录D中找到。从宏观层面，我们将所有智能体的主要失败模式分为以下三类：(1) 无法从错误中恢复：指智能体犯错导致其进入无法快速恢复并继续任务的状态（如错误的搜索页面）。(2) 中途卡住：指智能体在完成指令的正确路径上分心，导致未能完成任务。例如，键入关键词后未能点击正确的链接或未能进行搜索。(3) 到达错误目标：指智能体到达了错误的页面并误以为已完成任务。例如，智能体在costco.com而非ebay.com上找到了macbook。


While all the types of failure modes benefit from offline and offline-to-online RL training as shown in Figure 7, the most consistent and significant reduction is probably for the failure mode of failing to recover from mistakes. This is because while pre-trained models, generating plausible future tokens, can get distracted by the dynamic nature of the environment and, as a result, encounter at never-before-seen states. With no clue of how to escape such states, these methods are unable to recover and fail to solve the task. In contrast, by training on autonomously-collected rollouts, our agent DigiRL is able to learn from its own mistakes and reduces failures to recover over training.
如图7所示，虽然所有类型的失败模式都从离线和离线到在线RL训练中获益，但最持续且显著的减少可能是“无法从错误中恢复”这一模式。这是因为预训练模型虽然能生成合理的后续Token，但容易被环境的动态特性所干扰，从而进入从未见过的状态。由于不知道如何脱离这些状态，这些方法无法恢复并导致任务失败。相比之下，通过在自主采集的Rollout上进行训练，我们的DigiRL智能体能够从自身的错误中学习，并在训练过程中减少恢复失败的情况。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_9.jpg?x=421&y=203&w=945&h=287&r=0"/>



Figure 8: Correlation between our autonomous evaluator and human judgements for all policy models on General and Web Shopping subsets. For repeated offline and online runs, we report the correlation results for the run with the highest autonomous evaluation success rate.
图8：在General和Web Shopping子集上，所有策略模型的自主评估器与人类评判之间的相关性。对于重复的离线和在线运行，我们报告了具有最高自主评估成功率的运行相关性结果。


Ablation study of each component in DigiRL. We conduct an ablation study on different components of DigiRL in Figure 9. We find that all the components used by our approach are necessary: (1) using cross-entropy for training the value functions boosts performance by around ${12}\%$ (compare Ours and Ours w/ Regression); (2) using step-level advantages improves efficiency by ${12}\%$ (comparing Ours and Ours w/o step-level advantage); (3) the use of automatic curriculum improves the speed of learning by around ${25}\%$ (comparing Ours w/o step-level advantage and Filtered BC); (4) Ours outperforms vanilla AWR that does not employ a doubly-robust advantage estimator or curriculum.
DigiRL各组件的消融研究。我们在图9中对DigiRL的不同组件进行了消融研究。我们发现本方法所使用的所有组件都是必要的：(1) 使用交叉熵训练价值函数使性能提升了约 ${12}\%$（对比Ours与Ours w/ Regression）；(2) 使用步级优势（step-level advantages）将效率提升了 ${12}\%$（对比Ours与Ours w/o step-level advantage）；(3) 使用自动课程学习将学习速度提升了约 ${25}\%$（对比Ours w/o step-level advantage与Filtered BC）；(4) Ours的性能超过了不采用双重稳健优势估计器或课程学习的原生AWR。


Additionally, we also observe no degradation in performance as a result of "hard-filtering", as show by nearly comparable performance of our approach and the best run of exponential filtering obtained via an extensive tuning of the temperature hyperparameter $\tau$ in naïve AWR (comparing Ours and Ours w/ vanilla AWR reweight-ing), despite simplicity of implementation in the hard filtering approach. Putting together, these choices result in a new state-of-the-art RL approach for device control.
此外，我们也观察到“硬过滤”并未导致性能下降，如我们的方法与通过对原生AWR中的温度超参数 $\tau$ 进行大量调优而获得的指数过滤最佳运行结果性能相当（对比Ours与Ours w/ vanilla AWR reweighting），尽管硬过滤方法的实现更为简单。综合而言，这些选择构成了一种全新的、处于领先水平的设备控制RL方法。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_9.jpg?x=910&y=875&w=561&h=396&r=0"/>



Figure 9: Ablation study results on the AitW Web Shopping subset.
图9：AitW Web Shopping子集上的消融研究结果。


Evaluation of our autonomous evaluator. In Figure 8, we present the findings from a user study aimed at assessing the accuracy of our autonomous evaluator. Our results indicate that the success rates reported by our automatic evaluator are remarkably consistent with those assessed by human evaluators across almost all models, with differences less than 3%. Furthermore, we observed that evaluations on the Web Shopping subset are more precise compared to those on the General subset. This increased accuracy likely stems from the fact that tasks in the General subset are formulated in free-form language, which can introduce ambiguity, whereas the Web Shopping subset features a narrower range of language expressions, reducing potential variability.
对我们自主评估器的评估。在图8中，我们展示了一项旨在评估自主评估器准确性的用户研究结果。结果表明，我们的自动评估器报告的成功率与人类评估者的评估结果在几乎所有模型上都高度一致，差异小于3%。此外，我们观察到Web Shopping子集的评估比General子集更精确。这种准确性的提升可能源于General子集的任务采用自由语言表述，可能引入歧义；而Web Shopping子集的语言表达范围较窄，减少了潜在的可变性。


## 6 Discussion and Limitations
## 6 讨论与局限性


In this paper, we propose a novel autonomous RL approach, DigiRL, for training in-the-wild, multimodal, device-control agents that establish a new state-of-the-art performance on a number of Android control tasks from Android-in-the-Wild dataset [31]. To achieve this, we first build a scalable and parallelizable Android environment with a robust VLM-based general-purpose evaluator that supports fast online data collection. We then develop a system for offline RL pre-training, followed by autonomous RL fine-tuning to learn via interaction, admist the stochasticity of the real-world Internet and device eco-system. Our agent achieves a 280% improvement over the previous state-of-the-art agents (from 17.7% to 68.2% in terms of task success rate), including AppAgent based on GPT-4V and Gemini 1.5 Pro, and supervised trained models such as AutoUI and CogAgent.
在本文中，我们提出了一种新型自主强化学习方法DigiRL，用于训练在真实环境中运行的多模态设备控制智能体。该方法在Android-in-the-Wild数据集[31]的多个安卓控制任务上刷新了SOTA性能。为此，我们首先构建了一个可扩展且可并行的安卓环境，并配备了基于VLM的强大通用评估器以支持快速在线数据采集。随后，我们开发了一套离线强化学习预训练系统，并结合自主强化学习微调，以在真实互联网和设备生态系统的随机性中通过交互进行学习。我们的智能体相比之前的SOTA智能体实现了280%的提升（任务成功率从17.7%提升至68.2%），这些智能体包括基于GPT-4V和Gemini 1.5 Pro的AppAgent，以及AutoUI和CogAgent等监督训练模型。


Due to computational limitations, and despite the fact that the parallel emulator and autonomous evaluator can be easily extended to complicated tasks, our agent is trained only with tasks from AitW instead of a all possible tasks on the device. Our design of the DigiRL algorithm aims for maximal implementation simplicity, so we hope that our approach to serve as a base algorithm for future research to build on, including algorithmic research as well as expanding the space of tasks.
受计算资源限制，尽管并行模拟器和自主评估器可以轻松扩展到复杂任务，我们的智能体目前仅针对AitW任务而非设备上所有可能的任务进行训练。DigiRL算法的设计旨在最大化实现简易性，因此我们希望该方法能作为后续研究的基础算法，包括算法研究以及任务空间的扩展。


## Acknowledgements
## 致谢


We thank Yi Su, Izzedin Gur, Xinyang Geng, and Sandra Faust for feedback on an earlier version of this paper and for informative discussions. This work is supported by NSF IIS-2246811 and ONR N00014-21-1-2838, and Gemini 1.5 Pro credit donations for academic use and cloud resources from Google Cloud.
感谢Yi Su、Izzedin Gur、Xinyang Geng和Sandra Faust对本文早期版本的反馈及富有成效的讨论。本研究由NSF IIS-2246811和ONR N00014-21-1-2838资助，并获得了谷歌云提供的学术用途Gemini 1.5 Pro额度捐赠及云资源支持。


## References
## 参考文献


[1] Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models, 2023.
[1] Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models, 2023.


[2] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem B1yk, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback, 2023.
[2] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem B1yk, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback, 2023.


[3] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. ArXiv, abs/2310.05915, 2023. URL https: //api.semanticscholar.org/CorpusID: 263829338.
[3] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. ArXiv, abs/2310.05915, 2023. URL https: //api.semanticscholar.org/CorpusID: 263829338.


[4] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. Workarena: How capable are web agents at solving common knowledge work tasks?, 2024.
[4] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. Workarena: How capable are web agents at solving common knowledge work tasks?, 2024.


[5] Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Taïga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, Aviral Kumar, and Rishabh Agarwal. Stop regressing: Training value functions via classification for scalable deep rl, 2024.
[5] Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Taïga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, Aviral Kumar, and Rishabh Agarwal. Stop regressing: Training value functions via classification for scalable deep rl, 2024.


[6] 2023 Gemini Team. Gemini: A family of highly capable multimodal models, 2024.
[6] 2023 Gemini Team. Gemini: A family of highly capable multimodal models, 2024.


[7] 2024 Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024.
[7] 2024 Gemini Team. Gemini 1.5: 在数百万 token 上下文中解锁多模态理解, 2024.


[8] Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey Levine. Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability. NeurIPS, 2021.
[8] Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey Levine. 强化学习中泛化为何困难：认识论 POMDPs 与隐式部分可观测性. NeurIPS, 2021.


[9] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2023.
[9] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: 用于 GUI 智能体的视觉语言模型, 2023.


[10] Peter C Humphreys, David Raposo, Toby Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Alex Goldin, Adam Santoro, and Timothy Lillicrap. A data-driven approach for learning to control computers, 2022.
[10] Peter C Humphreys, David Raposo, Toby Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Alex Goldin, Adam Santoro, and Timothy Lillicrap. 一种用于学习控制计算机的数据驱动方法, 2022.


[11] Minqi Jiang, Edward Grefenstette, and Tim Rocktäschel. Prioritized level replay. CoRR, abs/2010.03934, 2020. URL https://arxiv.org/abs/2010.03934.
[11] Minqi Jiang, Edward Grefenstette, and Tim Rocktäschel. 优先级别重放. CoRR, abs/2010.03934, 2020. URL https://arxiv.org/abs/2010.03934.


[12] Yiding Jiang, J Zico Kolter, and Roberta Raileanu. On the importance of exploration for generalization in reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.
[12] Yiding Jiang, J Zico Kolter, and Roberta Raileanu. 论探索对强化学习泛化的重要性. Advances in Neural Information Processing Systems, 36, 2024.


[13] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024.
[13] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: 语言模型能否解决现实世界的 GitHub 问题？, 2024.


[14] Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning. In International Conference on Machine Learning, 2002. URL https://api.semanticscholar.org/CorpusID:31442909.
[14] Sham M. Kakade and John Langford. 近似最优的近似强化学习. In International Conference on Machine Learning, 2002. URL https://api.semanticscholar.org/CorpusID:31442909.


[15] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikih, and Ruslan Salakhutdinov. Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web, 2024.
[15] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikih, and Ruslan Salakhutdinov. Omniact: 用于支持桌面和 Web 多模态全才自主智能体的数据集与基准, 2024.


[16] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024.
[16] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: 在现实视觉 Web 任务上评估多模态智能体. arXiv preprint arXiv:2401.13649, 2024.


[17] Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. Offline q-learning on diverse multi-task data both scales and generalizes, 2023.
[17] Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. 针对多样化多任务数据的离线 Q 学习兼具扩展性与泛化性, 2023.


[18] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: Bootstrap and reinforce a large language model-based web navigating agent, 2024.
[18] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: 自举并增强基于大语言模型的 Web 导航智能体, 2024.


[19] Juyong Lee, Taywon Min, Minyong An, Changyeon Kim, and Kimin Lee. Benchmarking mobile device control agents across diverse configurations, 2024.
[19] Juyong Lee, Taywon Min, Minyong An, Changyeon Kim, and Kimin Lee. 跨多样化配置评估移动设备控制智能体的基准测试, 2024.


[20] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents, 2023.
[20] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: 评估大语言模型作为智能体的表现, 2023.


[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.
[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: 一种强力优化的 BERT 预训练方法. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.


[22] Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement learning with offline datasets. CoRR, abs/2006.09359, 2020. URL https: //arxiv.org/abs/2006.09359.
[22] Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. 利用离线数据集加速在线强化学习. CoRR, abs/2006.09359, 2020. URL https: //arxiv.org/abs/2006.09359.


[23] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik's cube with a robot hand, 2019.
[23] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. 用机器人手解魔方, 2019.


[24] 2023 OpenAI Team. Gpt-4 technical report, 2023.
[24] 2023 OpenAI Team. GPT-4 技术报告, 2023.


[25] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022. URL https://api.semanticscholar.org/ CorpusID: 246426909.
[25] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 训练语言模型以遵循人类反馈指令. ArXiv, abs/2203.02155, 2022. URL https://api.semanticscholar.org/ CorpusID: 246426909.


[26] Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024.
[26] Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. 数字智能体的自主评估与优化. arXiv preprint arXiv:2404.06474, 2024.


[27] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization, 2024.
[27] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. 迭代推理偏好优化, 2024.


[28] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. CoRR, abs/1910.00177, 2019. URL http://arxiv.org/abs/1910.00177.
[28] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. 优势加权回归：简单且可扩展的离策略强化学习. CoRR, abs/1910.00177, 2019. URL http://arxiv.org/abs/1910.00177.


[29] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning, 2019.
[29] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. 优势加权回归：简单且可扩展的离策略强化学习, 2019.


[30] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023.
[30] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. ToolLLM：促进大语言模型掌握 16000 多个真实世界 API, 2023.


[31] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: A large-scale dataset for android device control. arXiv preprint arXiv:2307.10088, 2023.
[31] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 野外 Android：用于 Android 设备控制的大规模数据集. arXiv preprint arXiv:2307.10088, 2023.


[32] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay, 2016.
[32] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. 优先级经验重放, 2016.


[33] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools, 2023.
[33] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom. Toolformer：语言模型可以学会自发使用工具, 2023.


[34] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015. URL http://arxiv.org/abs/ 1502.05477.
[34] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. 置信域策略优化. CoRR, abs/1502.05477, 2015. URL http://arxiv.org/abs/ 1502.05477.


[35] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347.
[35] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 近端策略优化算法. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347.


[36] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation, 2018.
[36] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 使用广义优势估计的高维连续控制, 2018.


[37] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3135-3144. PMLR, 06-11 Aug 2017. URL https://proceedings.mlr.press/v70/shi17a.html.
[37] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: 一个基于 Web 智能体的开放域平台。收录于 Doina Precup 和 Yee Whye Teh 编辑的《第 34 届国际机器学习大会论文集》，机器学习研究论文集第 70 卷，第 3135-3144 页。PMLR，2017 年 8 月 6-11 日。URL https://proceedings.mlr.press/v70/shi17a.html。


[38] Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language generation with implicit language q learning, 2023.
[38] Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. 利用隐式语言 Q 学习进行自然语言生成的离线强化学习，2023。


[39] Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: A reinforcement learning platform for android. arXiv preprint arXiv:2105.13231, 2021.
[39] Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv：一个用于 Android 的强化学习平台。arXiv 预印本 arXiv:2105.13231, 2021。


[40] Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. CoRR, abs/1509.06461, 2015. URL http://arxiv.org/abs/1509.06461.
[40] Hado van Hasselt, Arthur Guez, and David Silver. 结合双 Q 学习的深度强化学习。CoRR, abs/1509.06461, 2015。URL http://arxiv.org/abs/1509.06461。


[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.
[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need，2023。


[42] Siddharth Verma, Justin Fu, Mengjiao Yang, and Sergey Levine. Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning, 2022.
[42] Siddharth Verma, Justin Fu, Mengjiao Yang, and Sergey Levine. Chai：一个基于离线强化学习的任务导向型对话聊天机器人 AI，2022。


[43] Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, and Nando de Freitas. Critic regularized regression, 2021.
[43] Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, and Nando de Freitas. 评论者正则化回归，2021。


[44] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972, 2024.
[44] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld：在真实计算机环境中评估开放式任务的多模态智能体基准。arXiv 预印本 arXiv:2404.07972, 2024。


[45] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, Zicheng Liu, and Lijuan Wang. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation, 2023.
[45] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, Zicheng Liu, and Lijuan Wang. GPT-4V 漫游仙境：用于零样本智能手机 GUI 导航的大型多模态模型，2023。


[46] John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback, 2023.
[46] John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode：通过执行反馈实现交互式编程的标准化与基准测试，2023。


[47] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023.
[47] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent：作为智能手机用户的多模态智能体。arXiv 预印本 arXiv:2312.13771, 2023。


[48] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents, 2023.
[48] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop：通过落地语言智能体实现可扩展的真实世界 Web 交互，2023。


[49] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms, 2023.
[49] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning：为大语言模型开启泛化智能体能力，2023。


[50] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. arXiv preprint arXiv:2405.10292, 2024.
[50] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. 通过强化学习微调大型视觉语言模型作为决策智能体。arXiv 预印本 arXiv:2405.10292, 2024。


[51] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: A ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024.
[51] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. UFO：一个专注于 Windows 操作系统交互的 UI 智能体。arXiv 预印本 arXiv:2402.07939, 2024。


[52] Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents, 2024.
[52] Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents, 2024.


[53] Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents, 2023.
[53] Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents, 2023.


[54] Ziniu Zhang, Shulin Tian, Liangyu Chen, and Ziwei Liu. Mmina: Benchmarking multihop multimodal internet agents. arXiv preprint arXiv:2404.09992, 2024.
[54] Ziniu Zhang, Shulin Tian, Liangyu Chen, and Ziwei Liu. Mmina: Benchmarking multihop multimodal internet agents. arXiv preprint arXiv:2404.09992, 2024.


[55] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is a generalist web agent, if grounded, 2024.
[55] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is a generalist web agent, if grounded, 2024.


[56] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment for building autonomous agents. ArXiv, abs/2307.13854, 2023. URL https: //api.semanticscholar.org/CorpusID:260164780.
[56] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment for building autonomous agents. ArXiv, abs/2307.13854, 2023. URL https: //api.semanticscholar.org/CorpusID:260164780.


[57] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.
[57] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.


[58] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. CoRR, abs/1909.08593, 2019. URL http://arxiv.org/abs/1909.08593.
[58] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. CoRR, abs/1909.08593, 2019. URL http://arxiv.org/abs/1909.08593.


## Appendices
## 附录


## A Environment details
## A 环境详情


### A.1 Post-processing of AitW
### A.1 AitW 的后处理


The Android in the Wild (AiTW) task set is a large-scale dataset for android device control, containing five subsets: GoogleApps, Install, Web Shopping, General, and Single, where we select the General and Web Shopping subsets. Single subset is not considered here because all tasks in Single can be completed within one step and thus this subset fails to examine the multi-step challenges that we are interested in this paper. Install and GoogleApps are not considered due to security reasons as those tasks require an active Google account and parallel emulations can flag security concerns.
Android in the Wild (AiTW) 任务集是一个大规模的安卓设备控制数据集，包含五个子集：GoogleApps、Install、Web Shopping、General 和 Single。我们选择了 General 和 Web Shopping 子集。此处未考虑 Single 子集，因为该子集中的所有任务均可在一步内完成，因此无法考察本文所关注的多步挑战。出于安全原因，未考虑 Install 和 GoogleApps 子集，因为这些任务需要活跃的 Google 账号，且并行模拟可能会触发安全警报。


General. The General set focuses on searching for information and basic application usage. For example, it contains searching for latest news in Chile, search for flights from NYC to Sydney, opening Gmail, etc. We use all 545 tasks in the training set for training and the first 96 tasks in the test set for testing due to computational and budget constraints. The maximum allowed number of steps for this subset is 10. Offline data is collected by rolling our the initial AutoUI policy with tasks from the training set. The offline data used for the offline-to-online setting contains 608 trajectories while the offline data used for the offline setting contains 1552 trajectories. Some task examples are shown in Table 3.
General。General 集合侧重于信息查询和基础应用使用。例如，它包含搜索智利最新新闻、搜索从纽约到悉尼的航班、打开 Gmail 等。由于计算和预算限制，我们使用训练集中的全部 545 个任务进行训练，使用测试集中的前 96 个任务进行测试。该子集允许的最大步数为 10。离线数据是通过在训练集任务上运行初始 AutoUI 策略收集的。用于“离线转在线”设置的离线数据包含 608 条轨迹，而用于“离线”设置的离线数据包含 1552 条轨迹。表 3 展示了一些任务示例。


Task Example
任务示例


How do I get to the nearest Verizon Store?
我该如何前往最近的 Verizon 商店？


How much does a 2 bedroom apartment rent for in Denver?
丹佛的两居室公寓租金是多少？


Search for flights from Barcelona to Boston
搜索从巴塞罗那到波士顿的航班


What's a good restaurant in New York?
纽约有什么好的餐厅？


What's on the menu at Burger King?
汉堡王（Burger King）的菜单上有什么？


Table 2: Examples of task descriptions in the AiTW General task set.
表 2：AiTW 通用任务集中的任务描述示例。


Web Shopping. The Web Shopping subset comprises search instructions on various shopping websites, like searching for razer blader on ebay. As some websites (e.g. Amazon) and operations (e.g. adding items to cart) frequently require captcha verifications, we post-process the Web Shopping subset to exclude such operations and websites and also make the task easy to evaluate for our autonomous evaluator. The resulting task set involves navigating through five websites (costco.com, bestbuy.com, target.com, walmart.com, newegg.com) and three basic operations (go to website, search in the website, and select items from the searched results). Our post-processed training set contains 438 tasks and our testing set contains 96 tasks. Example tasks after post-processing can be found in Table 3. The maximum allowed number of steps for this subset is 20. Offline data is collected by rolling our the initial AutoUI policy with tasks from the training set. The offline data used for the offline-to-online setting contains 528 trajectories while the offline data used for the offline setting contains 1296 trajectories.
网络购物。网络购物子集包含各种购物网站上的搜索指令，例如在 ebay 上搜索 razer blader。由于某些网站（如 Amazon）和操作（如将商品添加到购物车）经常需要验证码验证，我们对网络购物子集进行了后处理，以排除此类操作和网站，并使任务更易于我们的自主评估器进行评估。处理后的任务集涉及五个网站（costco.com, bestbuy.com, target.com, walmart.com, newegg.com）和三种基本操作（前往网站、在网站内搜索、从搜索结果中选择商品）。后处理后的训练集包含 438 个任务，测试集包含 96 个任务。后处理后的示例任务见表 3。该子集允许的最大步数为 20。离线数据是通过使用训练集中的任务运行初始 AutoUI 策略收集的。用于离线到在线设置的离线数据包含 528 条轨迹，而用于离线设置的离线数据包含 1296 条轨迹。


<table><tr><td>Difficulty</td><td>Task Example</td></tr><tr><td>1</td><td>Go to costco.com <br> Go to walmart.com</td></tr><tr><td>2</td><td>Go to costco.com, search for "bose soundsport free" <br> Go to walmart.com, search for "logitech g910"</td></tr><tr><td>3</td><td>Go to costco.com, search for "bose soundsport free" and select the first entry <br> Go to walmart.com, search for "logitech g910" and select the first entry</td></tr></table>
<table><tbody><tr><td>难度</td><td>任务示例</td></tr><tr><td>1</td><td>访问 costco.com <br/> 访问 walmart.com</td></tr><tr><td>2</td><td>访问 costco.com，搜索“bose soundsport free” <br/> 访问 walmart.com，搜索“logitech g910”</td></tr><tr><td>3</td><td>访问 costco.com，搜索“bose soundsport free”并选择第一项 <br/> 访问 walmart.com，搜索“logitech g910”并选择第一项</td></tr></tbody></table>


Table 3: Examples of task descriptions in the AiTW Webshopping task set.
表 3：AiTW Webshopping 任务集中任务描述的示例。


<table><tr><td rowspan="2"></td><td colspan="2">AitW General</td><td colspan="2">AitW Web Shopping</td></tr><tr><td>All Trajectories</td><td>Successful Trajectories</td><td>All Trajectories</td><td>Successful Trajectories</td></tr><tr><td>DigiRL Run1</td><td>6.31</td><td>4.40</td><td>11.35</td><td>7.23</td></tr><tr><td>DigiRL Run2</td><td>6.64</td><td>5.04</td><td>10.86</td><td>6.55</td></tr><tr><td>Filtered BC Run1</td><td>8.08</td><td>6.56</td><td>12.05</td><td>6.88</td></tr><tr><td>Filtered BC Run2</td><td>7.36</td><td>6.13</td><td>14.72</td><td>9.62</td></tr></table>
<table><tbody><tr><td rowspan="2"></td><td colspan="2">AitW 通用</td><td colspan="2">AitW 网页购物</td></tr><tr><td>所有轨迹</td><td>成功轨迹</td><td>所有轨迹</td><td>成功轨迹</td></tr><tr><td>DigiRL 运行1</td><td>6.31</td><td>4.40</td><td>11.35</td><td>7.23</td></tr><tr><td>DigiRL 运行2</td><td>6.64</td><td>5.04</td><td>10.86</td><td>6.55</td></tr><tr><td>过滤后 BC 运行1</td><td>8.08</td><td>6.56</td><td>12.05</td><td>6.88</td></tr><tr><td>过滤后 BC 运行2</td><td>7.36</td><td>6.13</td><td>14.72</td><td>9.62</td></tr></tbody></table>


Table 4: Average rollout length of the DigiRL agent compared to filtered BC. Darker green means shorter rollout length. On both AitW General and AitW Web Shopping test subsets, we find that DigiRL consistently produces shorter length rollouts than filtered BC.
表 4：DigiRL 智能体与过滤后的 BC 的平均 rollout 长度对比。深绿色表示更短的 rollout 长度。在 AitW General 和 AitW Web Shopping 测试子集上，我们发现 DigiRL 产生的 rollout 长度始终短于过滤后的 BC。


## B Other Quantitative Experiments
## B 其他定量实验


### B.1 Curriculum Learning
### B.1 课程学习


When running experiments on the AitW Web Shopping subset, we find solving easier tasks helps improve solving harder tasks, where the difficulty is identified in Table 3. By specifying the difficulty DigiRL-Run1 in Figure 6, we empirically show the success rates of each difficulty across the online learning process in Figure 12, we observe that a significant increase of success rate of tasks of difficulty 1 leads to increasing success rate of difficulty 2, and the same pattern for difficulty 2 and 3, demonstrating effective curriculum learning.
在 AitW Web Shopping 子集上进行实验时，我们发现解决较简单的任务有助于提高解决较难任务的能力，其难度在表 3 中定义。通过图 6 中指定的难度 DigiRL-Run1，我们在图 12 中凭经验展示了在线学习过程中每个难度的成功率，观察到难度 1 任务成功率的显著提高带动了难度 2 成功率的提升，难度 2 与 3 之间也呈现相同模式，证明了有效的课程学习。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_15.jpg?x=354&y=961&w=1090&h=373&r=0"/>



Figure 10: Left: Success rate under different difficulties for the AiTW Webshopping task set. Right: Success rate under different methods with different horizon length $\left( {H \in  \{ {10},{20}\} }\right)$ on the AiTW Google Search task set.
图 10：左：AiTW Webshopping 任务集在不同难度下的成功率。右：AiTW Google Search 任务集在不同步长限制 $\left( {H \in  \{ {10},{20}\} }\right)$ 下不同方法的成功率。


### B.2 Learning Method
### B.2 学习方法


We ablate on the learning method, i.e. online learning or offline-to-online learning. We find that offline-to-online learning converges faster than online learning, and is not necessarily worse than online learning in terms of final performance, as shown in Figure 11.
我们对学习方法进行了消融研究，即在线学习或离线到在线学习。如图 11 所示，我们发现离线到在线学习比在线学习收敛更快，且最终性能不一定比在线学习差。


### B.3 Horizon Limit
### B.3 步长限制


We investigate the horizon limit of filtered BC and DigiRL on the AitW General subset. As most tasks can be effectively solved within 10 steps, we specify two horizon limits: a sufficient horizont $H = {10}$ ,and a redundant horizon $H = {20}$ . Results in Figure 12 show that a redundant horizon introduces significantly faster learning speed for both filtered BC and DigiRL, presumbaly because longer horizon means more opportunity to try in a single trajectory. In both horizon settings, we observe the DigiRL offers a significant speedup of around 100 trajectories over Filtered BC.
我们在 AitW General 子集上研究了过滤后的 BC 和 DigiRL 的步长限制。由于大多数任务可在 10 步内有效解决，我们指定了两个步长限制：充足步长 $H = {10}$ 和冗余步长 $H = {20}$ 。图 12 的结果显示，冗余步长显著加快了过滤后的 BC 和 DigiRL 的学习速度，推测是因为更长的步长意味着在单个轨迹中有更多尝试机会。在两种步长设置下，我们都观察到 DigiRL 比过滤后的 BC 显著提速了约 100 条轨迹。


### B.4 Trajectory Length
### B.4 轨迹长度


We investigate the rollout length of DigiRL compared to filtered BC. Results in Table 4 demonstrate that DigiRL consistently achieves shorter average rollout lengths compared to filtered BC across both subsets. This observation holds true whether considering all rollouts for computing this correlation or only investigating this correlation on rollouts that eventually succeed. This indicates the capability of DigiRL to solve tasks in a more efficient and directed manner. Qualitative examples can be found in Figure 16.
我们对比了 DigiRL 与过滤后的 BC 的 rollout 长度。表 4 的结果表明，在两个子集上，DigiRL 的平均 rollout 长度始终短于过滤后的 BC。无论是计算所有 rollout 的相关性，还是仅研究最终成功轨迹的相关性，这一观察结果均成立。这表明 DigiRL 能够以更高效、更有针对性的方式解决任务。定性示例见图 16。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_16.jpg?x=633&y=210&w=534&h=485&r=0"/>



Figure 11: Success rate with pure online learning or offline-to-online learning w.r.t. the number of online trajectories trained on the AitW General dataset. The starting points of curves in this figure look different from the main results figure because the starting points of the main results figure is smoothed at the average performance of the offline trajectories collected for the offline-to-online learning.
图 11：在 AitW General 数据集上，纯在线学习或离线到在线学习的成功率相对于在线训练轨迹数量的关系。此图中曲线的起点与主结果图不同，因为主结果图的起点是针对离线到在线学习所收集的离线轨迹的平均性能进行了平滑处理。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_16.jpg?x=629&y=913&w=539&h=369&r=0"/>



Figure 12: Success rate with different horizon length $\left( {H \in  \{ {10},{20}\} }\right)$ under different methods on the AiTW Google Search task set.
图 12：AiTW Google Search 任务集在不同步长限制 $\left( {H \in  \{ {10},{20}\} }\right)$ 下不同方法的成功率。


## C Qualitative Examples
## C 定性示例


### C.1 Random sample of trajectories for different agents
### C.1 不同智能体轨迹的随机采样


In Figures 13 and 14, we provide trajectories of DigiRL, AutoUI, and GPT-4V randomly sampled from our test set to offer a qualitative understanding of the agents' performance. As shown in these examples, DigiRLcan efficiently carry out in-the-wild device control tasks and less likely to get stuck or get to a wrong page compared to AutoUI and GPT-4V.
在图 13 和 14 中，我们提供了从测试集中随机采样的 DigiRL、AutoUI 和 GPT-4V 轨迹，以提供对智能体性能的定性理解。如这些示例所示，与 AutoUI 和 GPT-4V 相比，DigiRL 能高效执行野外设备控制任务，且更不容易陷入停滞或进入错误页面。


### C.2 Error Recovery
### C.2 错误恢复


We observe that DigiRL is able to recover from its own mistakes. As shown in Figure 15, we find that DigiRL explores ways to get back to the original screen in order to perform a search. As a comparison, AutoUI fails to reset to the original screen and gets stuck at the diverged screen. Under the hood, we find DigiRL trying to maximize the state value, which usually induces it to reset to the original screen (that has a large value to success).
我们观察到 DigiRL 能够从自身错误中恢复。如图 15 所示，我们发现 DigiRL 会探索返回原始屏幕的方法以执行搜索。相比之下，AutoUI 无法重置到原始屏幕，并卡在偏离的屏幕上。在底层，我们发现 DigiRL 试图最大化状态值，这通常会促使其重置到原始屏幕（该屏幕具有较高的成功价值）。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_17.jpg?x=451&y=229&w=899&h=1803&r=0"/>



Figure 13: Agents' trajectory on two randomly sampled tasks on the General split of AitW.
图 13：智能体在 AitW General 拆分集中两个随机采样任务上的轨迹。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_18.jpg?x=448&y=228&w=902&h=1804&r=0"/>



Figure 14: Agents' trajectory on two randomly sampled tasks on the WebShop split of AitW.
图 14：智能体在 AitW WebShop 拆分集中两个随机采样任务上的轨迹。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_19.jpg?x=336&y=206&w=1127&h=810&r=0"/>



Figure 15: Error recovery cases. In bestbuy.com, we systematically find DigiRL able to recover from its own mistakes, while AutoUI fails to do so.
图 15：错误恢复案例。在 bestbuy.com 中，我们系统性地发现 DigiRL 能够从自身错误中恢复，而 AutoUI 则无法做到。


### C.3 Trajectory Length
### C.3 轨迹长度


Qualitative example on the number of steps in trajectories of DigiRL and filtered BC are shown in Figure 16. We find consistent cases where DigiRL has shorter trajectory length than filtere BC.
图 16 显示了 DigiRL 和经过过滤的 BC 轨迹步数的定性示例。我们发现，在一致的情况下，DigiRL 的轨迹长度比过滤后的 BC 更短。


### C.4 Reasoning failure of GPT-4V
### C.4 GPT-4V 的推理失败


The performance of GPT-4V failed on AiTW tasks predominantly due to not being able to carry out control actions as it plans on a high level, and then not being able to recover from these mistakes. Moreover, one of the main reasons why it is not able to recover from a mistake is that it might hallucinate and make itself believe that it is a wrong app or website. Indeed, GPT-4V constructs a plan of further actions when provided a task from either Web Shopping or General dataset of AiTW. Then, when it makes a misclick and fails to successfully proceed in an intermediate step, it might think that it actually solved that intermediate step and is in the correct app or website to execute further actions, causing the overall trajectory to fail. An example of this is provided in Figure 17. Here, we ask the model to search for an item in a webshopping website, in particular in "newegg.com". However, the model fails to proceed to that website due to not being able to precisely locating the search button. Then, instead of trying to go to that website again, the model thinks it is already in that webshopping website, and mistakes the search bar of Google with the search bar of "newegg.com". Hence, the rest of the trajectory also fails. Another slightly different phenomenon is illustrated in Figure 18. Here, the model is able to proceed to the correct website and search for an item, but this time it fails to tap on the search button on the website and clicks to an advertisement instead. Consequently, the model fools itself to think it successfully searched the item, and scrolls the page hoping to find that item, but it cannot do so because in reality it views the results of the advertisement. The primary reason of these failures is the challenge of grounding the control actions in GUI interfaces to realize the intermediary goals laid out by GPT-4V model's thoughts. As an example, we provide an illustration of trying to set up an alarm task in Figure 19. Here, in the last frame, it fails to execute the precise movements in the necessary amount of rounds to correctly set up the alarm to the desired time, and in the last frame we see that the action taken does not align with the thought process of the model.
GPT-4V 在 AiTW 任务上的性能失败，主要归因于无法在进行高层规划时执行控制动作，且随后无法从这些错误中恢复。此外，无法从错误中恢复的主要原因之一是它可能会产生幻觉，并误认为自己处于错误的应用程序或网站中。事实上，当提供来自 AiTW 的 Web Shopping 或 General 数据集的任务时，GPT-4V 会构建后续动作计划。然而，当它发生误点并导致中间步骤无法成功进行时，它可能会认为自己实际上已经解决了该中间步骤，并处于正确的应用程序或网站中来执行后续动作，从而导致整个轨迹失败。图 17 提供了一个此类示例。在这里，我们要求模型在购物网站（特别是“newegg.com”）中搜索商品。然而，由于无法精确定位搜索按钮，模型无法进入该网站。接着，模型并没有尝试再次访问该网站，而是认为自己已经处于该购物网站中，并将 Google 的搜索栏误认为是“newegg.com”的搜索栏。因此，轨迹的其余部分也失败了。图 18 插图说明了另一个略有不同的现象。在这里，模型能够进入正确的网站并搜索商品，但这次它未能点击网站上的搜索按钮，而是点击了一个广告。结果，模型欺骗自己认为已成功搜索到商品，并滚动页面希望找到该商品，但由于实际上看到的是广告结果而无法做到。这些失败的主要原因是在 GUI 界面中落实控制动作以实现 GPT-4V 模型思路所设定的中间目标存在挑战。作为示例，我们在图 19 中提供了一个尝试设置闹钟任务的插图。在这里，在最后一帧中，它未能通过必要的轮次执行精确移动来将闹钟正确设置为所需时间，并且我们在最后一帧中看到，所采取的动作与模型的思考过程并不一致。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_20.jpg?x=336&y=201&w=1127&h=888&r=0"/>



Figure 16: Examples where DigiRL has shorter trajectory length than online filtered BC.
图 16：DigiRL 的轨迹长度比在线过滤 BC 短的示例。


## D Fine-grained failure modes
## D 细粒度失败模式


In Figure 20, we present a more fine-grained breakdown for all six failure modes provided in the user study. Those failure modes include:
在图 20 中，我们展示了用户研究中提供的所有六种失败模式的更细粒度分类。这些失败模式包括：


- Failure to recover from mistakes refers to the scenario where the agent made a mistake that led it to states from which it failed to quickly recover and resume the task, such as a wrong google search page.
- 无法从错误中恢复：指智能体犯下错误并进入某些状态，随后无法从中快速恢复并继续执行任务的情况，例如错误的 Google 搜索页面。


- Failure to click on the right link or failure to click refers to the failure mode where the agent either fails to locate the element that it tries to click on and keeps clicking on the nearby region, or fails to start typing in the string when it is supposed to do so.
- 未能点击正确链接或点击失败：指智能体要么无法定位尝试点击的元素并持续点击附近区域，要么在应该开始输入字符串时未能执行输入操作的失败模式。


- Failure to take reasonable attempts at all refers to the failure mode where there is no clear reason that the agent fails to complete the task and does not seem to be on the right track throughout the trajectory.
- 完全未能进行合理的尝试：指没有明显原因导致智能体无法完成任务，且在整个轨迹中似乎都没有处于正确轨道上的失败模式。


- Quit or press HOME early refers to the failure mode where the agent decided to finish the task or press HOME to start over before the task is actually finished.
- 提早退出或按 HOME 键：指智能体在任务实际完成前就决定结束任务或按 HOME 键重新开始的失败模式。


- Stops at wrong but relevant page refers to the failure mode where the agent arrives at a wrong page and mistakenly thinks that it had completed the task. For example, the agent finds a macbook on costco.com while the instruction asked it to find a macbook on ebay.com.
“停止在错误但相关的页面”是指智能体到达了错误的页面，并误认为已完成任务的故障模式。例如，指令要求在 ebay.com 查找 macbook，但智能体在 costco.com 上找到了它。


- Technical issues refer to the failure mode that either the task is impossible (e.g. the tasks asks to open Amazon app but this app is not installed) or the agent is temporarily blocked from a certain website due to frequent visits.
“技术问题”是指任务无法完成（例如任务要求打开 Amazon 应用但未安装）或智能体因频繁访问而被某网站暂时封禁的故障模式。


The translation between fine-grained failure modes and coarse-grained failure modes is presented in Table 5.
细粒度故障模式与粗粒度故障模式之间的对应关系如表 5 所示。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_21.jpg?x=311&y=214&w=1180&h=762&r=0"/>



Figure 17: Failure of GPT-4V, with its thoughts and link-based actions given. A typical cause of failure is that it cannot tap on the correct "search" button after entering a query and mistakenly tapped onto the "x" symbol in the search bar as the "search" button. Here the goal is: Go to newegg.com, search for "alienware area 51" and select the first entry. As seen in red emboldened actions, it fails to press search button and deletes the query instead. Also, as seen in red highlighted parts in thoughts, it thinks it is in "newegg.com" website even though it is not.
图 17：GPT-4V 故障案例及其思维过程和基于链接的操作。一个典型的失败原因是它在输入查询后无法点击正确的“搜索”按钮，误将搜索栏中的“x”符号当成“搜索”按钮。此处目标是：访问 newegg.com，搜索“alienware area 51”并选择第一条。如红色加粗操作所示，它未能按下搜索按钮，而是删除了查询。此外，如思维过程中红色高亮部分所示，尽管它并不在“newegg.com”网站上，却认为自己就在该网站。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_21.jpg?x=313&y=1229&w=1173&h=703&r=0"/>



Figure 18: Failure of GPT-4V, with its thoughts and link-based actions given. This time the reason for failure is misclick on the wrong button. The task is "Go to costco.com, search for "acer predator", and select the first entry". Notice that up until the fourth frame in this Figure, the trajectory goes correct. But then it clicks on the generic advertisements on the Costco.com website, and it cannot recover back. It continues to scroll the page and takes wrong actions thereafter.
图 18：GPT-4V 故障案例及其思维过程和基于链接的操作。此次失败原因是误触。任务是“访问 costco.com，搜索‘acer predator’并选择第一条”。注意直到本图第四帧为止，轨迹都是正确的。但随后它点击了 Costco.com 网站上的通用广告且无法恢复。此后它继续滚动页面并采取错误操作。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_22.jpg?x=314&y=241&w=1193&h=778&r=0"/>



Figure 19: Failure of GPT-4V, with an example task on the AiTW general test set. The task is "Set an alarm for 4pm". Here, GPT-4V is able to successfully navigate to the clock app, and the alarm settings of that app. However, it cannot take the correct precise actions to set the alarm quickly enough, and it fails due to maximum rounds reached. In the last round, notice that the action of tap(1) contradict with its own thought process of setting minutes to "00".
图 19：GPT-4V 在 AiTW 通用测试集任务中的故障案例。任务是“设置下午 4 点的闹钟”。此处 GPT-4V 成功导航至时钟应用及其闹钟设置。然而，它无法采取正确的精确操作来快速设置闹钟，因达到最大轮数而失败。在最后一轮中，注意 tap(1) 的操作与其将分钟设为“00”的思维过程相矛盾。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_22.jpg?x=309&y=1279&w=1179&h=269&r=0"/>



Figure 20: Failure modes decomposition for each policy model for both General and Web Shopping subsets.
图 20：通用和网页购物子集中各策略模型的故障模式分解。


<table><tr><td>Fine-Grained Failure</td><td>Coarse-Grained Failure</td></tr><tr><td>Fail to recover from mistakes</td><td>Fail to recover from mistakes</td></tr><tr><td>Fail to click on the right link or fail to type</td><td>Get stuck midway</td></tr><tr><td>Fail to take reasonable attempts at all</td><td>Get stuck midway</td></tr><tr><td>Quit or Press HOME early</td><td>Arrive at wrong goal</td></tr><tr><td>Stops at wrong but relevant page</td><td>Arrive at wrong goal</td></tr><tr><td>Technical Issues</td><td>None</td></tr></table>
<table><tbody><tr><td>细粒度失败</td><td>粗粒度失败</td></tr><tr><td>未能从错误中恢复</td><td>未能从错误中恢复</td></tr><tr><td>未能点击正确链接或未能输入</td><td>中途卡顿</td></tr><tr><td>完全没有进行合理的尝试</td><td>中途卡顿</td></tr><tr><td>过早退出或按主页键</td><td>到达错误目标</td></tr><tr><td>停在错误但相关的页面</td><td>到达错误目标</td></tr><tr><td>技术问题</td><td>无</td></tr></tbody></table>


Table 5: Examples of task descriptions in the AiTW Webshopping task set.
表 5：AiTW Webshopping 任务集中的任务描述示例。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_23.jpg?x=322&y=194&w=1155&h=412&r=0"/>



Figure 21: Multi-machine parallel emulator execution. The host machine is equipped with GPU accelerators and the worker machines are equipped only with CPUs. The policy update is executed on the worker machine and the trajectory collections are executed distributedly on the worker machines and aggregated by the host machine.
图 21：多机并行模拟器执行。宿主机配备 GPU 加速器，工作机仅配备 CPU。策略更新在工作机上执行，轨迹采集在各工作机上分布式执行并由宿主机汇总。


## E Experiment machines
## E 实验机器


Our main experiments are conducted on VM instances from Google Cloud Platform. Each VM instance comes with 1x Tesla T4 GPU and 16x Intel(R) Xeon(R) CPU.
我们的主要实验在 Google Cloud Platform 的 VM 实例上进行。每个 VM 实例配备 1 个 Tesla T4 GPU 和 16 个 Intel(R) Xeon(R) CPU。


## F Setup for parallel environment
## F 并行环境配置


Running multiple emulators in parallel can be challenging due to the inefficiency in thread synchronization and frequent fault propagation when one emulator runs into an unknown error. To address this challenge, we set up a server-client system where all emulator processes are running in independent server processes. Each emulator process communicates with the main training process through different UIAutomotor servers. The main training process sends high-level instructions to UIAutomotor servers (such as reset and step), while UIAutomotor servers parse high-level instructions into low-level UI commands (such as typing a character and tapping at a coordinate) and such UI commands are executed by the emulator processes. When an exception is thrown in the emulator, the UIAutomotor examines if it is recoverable (e.g. an UI command takes too long to execute in the emulator) and reset the emulator process if it is not. When an exception is thrown in the UIAutomotor server, the main training process stops and resets the UIAutomotor server to ensure data correctness.
由于线程同步效率低下以及单个模拟器遇到未知错误时频繁的故障传播，并行运行多个模拟器具有挑战性。为了应对这一挑战，我们构建了一个服务端-客户端系统，其中所有模拟器进程都运行在独立的服务器进程中。每个模拟器进程通过不同的 UIAutomotor 服务器与主训练进程通信。主训练进程向 UIAutomotor 服务器发送高层指令（如 reset 和 step），而 UIAutomotor 服务器将高层指令解析为底层 UI 命令（如输入字符和点击坐标），并由模拟器进程执行这些 UI 命令。当模拟器抛出异常时，UIAutomotor 会检查其是否可恢复（例如 UI 命令在模拟器中执行时间过长），若不可恢复则重置模拟器进程。当 UIAutomotor 服务器抛出异常时，主训练进程会停止并重置 UIAutomotor 服务器以确保数据正确性。


This design can easily be scaled up to a multi-machine setting. As illustrated in Figure 21, one host machine equipped with GPU accelerator has a local copy of the current policy ${\pi }_{t}$ ,and distributes the policy to all worker machines equipped with only one GPU and multiple CPUs. Each worker machine will then collect trajectories of different tasks using ${\pi }_{t}$ . After all collection processes are synchronized,the host machine gathers all the trajectories together to update the policy to ${\pi }_{t + 1}$ . This process keeps iterating until the policy converges.
该设计可以轻松扩展到多机环境。如图 21 所示，一台配备 GPU 加速器的宿主机持有当前策略 ${\pi }_{t}$ 的本地副本，并将该策略分发给所有仅配备一个 GPU 和多个 CPU 的工作机。随后，每台工作机将使用 ${\pi }_{t}$ 采集不同任务的轨迹。在所有采集进程同步后，宿主机汇总所有轨迹以将策略更新为 ${\pi }_{t + 1}$ 。此过程不断迭代直至策略收敛。


Speedup of emulation parallel. The performance boost with respect to the number of worker machines is nearly linear, as demonstrated in Figure 22 (right), where we conduct experiments that examine the scaling performance of our parallel emulator. Our distributed emulator that runs emulations across multiple servers can reliably collect data with up to 64 parallel emulators on 128 CPUs with near-linear speedup. In contrast, a naive baseline that runs all parallel emulations on the same server achieves much inferior performance (0.74 compared to 1.74 trajs/min using 64 CPUs).
模拟并行加速。如图 22（右）所示，性能提升随工作机数量呈近线性增长，我们在该实验中考察了并行模拟器的扩展性能。我们的分布式模拟器跨多台服务器运行模拟，能在 128 个 CPU 上使用多达 64 个并行模拟器可靠地采集数据，并实现近线性加速。相比之下，在同一台服务器上运行所有并行模拟器的原生基准测试性能要差得多（使用 64 个 CPU 时为 0.74 trajs/min，而本方案为 1.74 trajs/min）。


## G Autonomous evaluator details
## G 自主评估器细节


Our autonomous evaluator gives a reward to each observation we get. The observation is composed of the current screenshot of device and the task. The evaluator gives a reward of 1 if the screenshot shows a completion of the task, and will terminate the POMDP as a result result.
我们的自主评估器为获得的每个观测值提供奖励。观测值由设备当前截图和任务组成。如果截图显示任务已完成，评估器将给出奖励 1，并因此终止 POMDP。


The optimized prompt is shown in Figure 23 and Figure 24 for General and Web Shopping subsets respectively.
针对通用（General）和网页购物（Web Shopping）子集的优化提示词分别如图 23 和图 24 所示。


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_24.jpg?x=713&y=206&w=370&h=361&r=0"/>



Figure 22: Emulation speed w.r.t number of CPUs used. The upper bound can only achieved when there is no communication and error handling cost. Our design of distributed emulator can significantly improve the efficiency of emulation compaared to the vanilla method of running all emulations over the same instance.
图 22：模拟速度随 CPU 使用数量的变化。上限仅在无通信和错误处理成本时才能达到。与在同一实例上运行所有模拟的常规方法相比，我们设计的分布式模拟器能显著提高模拟效率。


## H Zero-shot Baseline Details
## H 零样本基准细节


Figure 25 shows the prompt that we used for testing the Set-of-Marks performance for GPT-4V and Gemini 1.5 Pro. This prompt is directly taken from Yang et al. [47].
图 25 展示了我们用于测试 GPT-4V 和 Gemini 1.5 Pro 的 Set-of-Marks 性能的提示词。该提示词直接取自 Yang 等人 [47]。


## I Hyperparameters
## I 超参数


Hyperparameters for both Filtered BC and DigiRL are carefully tuned through binary search on the training set of General and Web Shopping subsets. The final choice of hyperparameters for both methods can be found in Table 6. As shown in the table, the only hyperparameters introduced by DigiRL are supervised training hyperparameters for the value function and instruction value function (including number of iterations and learning rate) and GAE $\lambda$ .
Filtered BC 和 DigiRL 的超参数均通过在通用和网页购物子集的训练集上进行二分查找经过仔细调优。两种方法的最终超参数选择见表 6。如表所示，DigiRL 引入的唯一超参数是价值函数和指令价值函数的监督训练超参数（包括迭代次数和学习率）以及 GAE $\lambda$ 。


## Prompt
## 提示词


You're an expert in evaluating whether the Screenshot successfully completes the Task.
你是一位评估“截图是否成功完成任务”的专家。


## =====Examples===
## =====示例===


Screenshot: \{train_1.png\}
截图：\{train_1.png\}


Task: Open the settings.
任务：打开设置。


Q: What should I expect to see on the screenshot if I've opened the settings?
问：如果我打开了设置，我期望在截图中看到什么？


A: I should expect to see I'm in the settings app. The screenshot shows the home screen of a mobile device, with various app icons displayed, including the settings app icon, but the settings app is not opened.
答：我期望看到我正处于设置应用中。截图显示的是移动设备的桌面，展示了各种应用图标（包括设置应用图标），但并未打开设置应用。


Status: failure
状态：失败


Screenshot: \{train_2.png\}
截图：\{train_2.png\}


Task: Find hotels in washington dc
任务：查找华盛顿特区的酒店


Q: What should I expect to see on the screenshot if I've searched for hotels in Washington, DC?
问：如果我搜索了华盛顿特区的酒店，我期望在截图中看到什么？


A: I should expect to see I'm in a search results page for hotels in Washington, DC. The screenshot shows a Google search page with the search field populated with the query "hotels in washington dc" and a list of suggested searches related to hotels in Washington, DC, but it does not show any search results for hotels in Washington, DC.
答：我期望看到我处于华盛顿特区酒店的搜索结果页面。截图显示的是谷歌搜索页面，搜索框中输入了“hotels in washington dc”，并显示了一系列相关的搜索建议，但并未显示任何酒店搜索结果。


Status: failure
状态：失败


Screenshot: \{train_3.png\}
截图：\{train_3.png\}


Task: What's a good restaurant in Portland?
任务：波特兰有哪些好餐厅？


Q: What should I expect to see on the screenshot if I've searched for a good restaurant in Portland?
问：如果我搜索了波特兰的好餐厅，我期望在截图中看到什么？


A: I should expect to see I'm in a search results page for a good restaurant in Portland. The screenshot shows a
A：我应该会看到波特兰一家优秀餐厅的搜索结果页面。截图显示了一个


Google search page with a search input field for "good restaurant in portland" and a map results preview showing
Google搜索页面，搜索框内输入了“good restaurant in portland”，并显示了一个地图结果预览，其中包含


business locations near Portland, like "Li Pigeon", "Portland City Grill", and "Higgins",
波特兰附近的商家位置，例如“Li Pigeon”、“Portland City Grill”和“Higgins”，


Status: success
状态：成功


... (more cases)
... (更多案例)


## =====Your Turn====
## =====轮到你了====


Screenshot: \{test.png\}
截图：\{test.png\}


Task: \{task_this_traj\}
任务：\{task_this_traj\}


Respond in this format:
请按此格式回答：


Q: What should I expect to see on the screenshot if I've <repeat the task>?
Q：如果我已<重复任务>，我应该在截图中看到什么？


A: I should expect to see <first expectation, then what's in the given screenshot.>
A：我应该看到<首先是预期，然后是给定截图中的内容。>


Status: success or failure (don't return anything else)
状态：成功或失败（请勿返回任何其他内容）


Start with "Q:".
以“Q:”开头。


## Response
## 响应


Q: What should I expect to see on the screenshot if I've searched for the price of a 12' ladder at Home Depot? A: I should expect to see the price of a 12' ladder at Home Depot; the screenshot shows a search result page for the p rice of a 12' ladder, with some product advertisements showing prices from Home Depot.
Q：如果我已搜索 Home Depot 12英尺梯子的价格，我应该在截图中看到什么？A：我应该看到 Home Depot 12英尺梯子的价格；截图显示了一个关于12英尺梯子价格的搜索结果页面，其中一些产品广告显示了来自 Home Depot 的价格。


Status: success
状态：成功


## Image Sources
## 图像来源


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_25.jpg?x=410&y=1521&w=1027&h=491&r=0"/>



Figure 23: Prompt for our autonomous evaluator for tasks in AitW General subset.
图 23：针对 AitW General 子集任务的自主评估器提示词。


## Prompt
## 提示词


You're an expert in evaluating whether the Screenshot successfully completes the Task.
你是一位评估“截图”是否成功完成“任务”的专家。


## =====Examples===
## =====示例===


Screenshot: \{train_1.png\}
截图：\{train_1.png\}


Task: Go to bestbuy.com
任务：访问 bestbuy.com


Q: What should I expect to see on the screenshot if I've gone to bestbuy.com?
问：如果我访问了 bestbuy.com，我预期会在截图中看到什么？


A: I should expect to see I'm in the Best Buy website, which usually shows the best buy logo with some featured products the same for the SM data of the ucts and categories. The screenshot shows I'm searching for "bestbuy.com" in the Google search (with some search s uggestions) instead of being in the Best Buy website.
答：我预期会看到我处于百思买（Best Buy）网站中，通常会显示百思买的标志以及一些特色产品及类别。截图显示我正在 Google 搜索中搜索“bestbuy.com”（带有一些搜索建议），而不是在百思买网站内。


Status: failure
状态：失败


Screenshot: \{train_2.png\}
截图：\{train_2.png\}


Task: Go to ebay.com, search for "corsair k70"
任务：访问 ebay.com，搜索“corsair k70”


Q: What should I expect to see on the screenshot if I've gone to ebay.com AND searched for "corsair k70"?
问：如果我访问了 ebay.com 并且搜索了“corsair k70”，我预期会在截图中看到什么？


A: I should expect to see I'm in the eBay website and search results for "corsair k70". The screenshot shows I'm in the eBay ebay website with some search suggestions for "corsair k70", but it does not show search results of the product, which usually includes price and the product details.
答：我预期会看到我处于 eBay 网站中，以及“corsair k70”的搜索结果。截图显示我处于 eBay 网站，带有一些“corsair k70”的搜索建议，但并未显示产品的搜索结果（通常包含价格和产品详情）。


Status: failure
状态：失败


Screenshot: \{train_3.png\}
截图：\{train_3.png\}


Task: Go to ebay.com, search for "lenovo thinkpad"
任务：访问 ebay.com，搜索“lenovo thinkpad”


Q: What should I expect to see on the screenshot if I've gone to ebay.com AND searched for "lenovo thinkpad"? A: I should expect to see I'm in the eBay website and search results for "lenovo thinkpad". The screenshot shows I'm in the eBay website and have several search results for "lenovo thinkpad".
问：如果我访问了 ebay.com 并搜索了“lenovo thinkpad”，我应该在截图中看到什么？答：我应该看到自己处于 eBay 网站中，以及“lenovo thinkpad”的搜索结果。截图显示我正在 eBay 网站，并且有多个“lenovo thinkpad”的搜索结果。


Status: success
状态：成功


... (more cases)
...（更多案例）


=====Your Turn===
=====轮到你了===


Screenshot: \{test.png\}
截图：\{test.png\}


Task: \{task_this_traj\}
任务：\{task_this_traj\}


Respond in this format:
以此格式回答：


Q: What should I expect to see on the screenshot if I've <repeat the task>?
问：如果我已经&lt;重复任务&gt;，我应该在截图中看到什么？


A: I should expect to see <first expectation, then what's in the given screenshot.>
答：我应该看到&lt;首先是预期目标，然后是当前截图中的内容&gt;。


Status: success or failure (don't return anything else)
状态：成功或失败（请勿返回任何其他内容）


Start with "Q:".
以“问：”开头。


## Response
## 回复


Q: What should I expect to see on the screenshot if I've searched for the price of a 12' ladder at Home Depot? A: I should expect to see the price of a 12' ladder at Home Depot; the screenshot shows a search result page for the p rice of a 12' ladder, with some product advertisements showing prices from Home Depot.
问：如果我搜索了 Home Depot 中 12 英尺梯子的价格，我应该在截图中看到什么？答：我应该看到 Home Depot 中 12 英尺梯子的价格；截图显示了一个关于 12 英尺梯子价格的搜索结果页面，其中一些产品广告显示了来自 Home Depot 的价格。


Status: success
状态：成功


## Image Sources
## 图像来源


<img src="https://cdn.noedgeai.com/bo_d5v21l3ef24c73bqgf20_26.jpg?x=406&y=1527&w=1054&h=479&r=0"/>



Figure 24: Prompt for our autonomous evaluator for tasks in AitW Web Shopping subset.
图 24：针对 AitW 网络购物子集任务的自主评估器提示词。


## Prompt
## 提示词


"You are an agent that is trained to perform some basic tasks on a smartphone. You will be given a \\nsmartphone screenshot. The interactive UI elements on the screenshot are labeled with numeric tags starting from 1 . The \\\\nuumeric tag of each interactive element is located in the center of the element.\\n\\nYou can call the following functions to control the smartphone:\\n\\n1. tap(element: int)\\nThis function is used to tap an UI element shown on the smartphone screen.\\n\\"element\\" is a numeric tag assigned to an UI element shown on the smartphone screen. \\nA simple use case can be tap(5), which taps the UI element labeled with the number 5.\\n\\n2. text(text_input: str)\\nThis function is used to insert text input in an input field/box. text_input is the string you want to insert and must \\nbe wrapped with double quotation marks. A simple use case can be text(\\"Hello, world!\\"), which inserts the string $\smallsetminus  n \smallsetminus$ "Hello,world!\\" into the input area on the smartphone screen. This function is usually callable when you see a keyboard $\smallsetminus$ nshowing in the lower half of the screen. $\smallsetminus   \smallsetminus$ na. long_press(element: int) $\smallsetminus$ nThis function is used to long press an UI element shown on the smartphone screen. shown on the smartphone screen.\\nA simple use case can be long_press(5), which long presses the UI element labeled with the number 5.\\n\\n4. swipe(element: int, direction: str, dist: str)\\nThis function is used to swipe an UI element shown on the smartphone screen, usually a scroll view or a slide bar.\\n\\"lement\\" is a numeric tag assigned to an UI element shown on the smartphone screen. "direction\\" is a string that \\\\represents one of the four directions: up, down, left, right. \\"direction\\" must be wrapped with double quotation \\\\marks. \\"dist\\" determines the distance of the swipe and can be one of the three options: short, medium, long. You should \\nchoose the appropriate distance option according to your need.\\nA simple use case can be swipe(21, \\"up\\", \\"medium\\"), which swipes up the UI element labeled with the number 21 for a \\\\medium distance.\\n\\n5. grid()\\nYou should call this function when you find the element you want to interact with is not labeled with a numeric tag and \\\\nother elements with numeric tags cannot help with the task. The function will bring up a grid overlay to divide the \\nsmartphone screen into small areas and this will give you more freedom to choose any part of the screen to tap, long \\npress, or swipe.
"你是一个受过训练，能在智能手机上执行基础任务的智能体。你会得到一张\\n手机截图。截图上的交互式 UI 元素都标记了从 1 开始的数字标签。每个交互元素的\\\\n数字标签位于该元素的中心。\\n\\n你可以调用以下函数来控制手机：\\n\\n1. tap(element: int)\\n此函数用于点击手机屏幕上显示的 UI 元素。\\n\"element\" 是分配给屏幕上显示的 UI 元素的数字标签。\\n一个简单的用例是 tap(5)，即点击标记为数字 5 的 UI 元素。\\n\\n2. text(text_input: str)\\n此函数用于在输入字段/框中插入文本输入。text_input 是你想要插入的字符串，且必须\\n用双引号括起来。一个简单的用例是 text(\"Hello, world!\")，即在手机屏幕的输入区域插入字符串 $\smallsetminus  n \smallsetminus$ \"Hello,world!\"。当你看到屏幕下半部分显示键盘 $\smallsetminus$ n 时，通常可以调用此函数。$\smallsetminus   \smallsetminus$ na. long_press(element: int) $\smallsetminus$ n此函数用于长按手机屏幕上显示的 UI 元素。\\n一个简单的用例是 long_press(5)，即长按标记为数字 5 的 UI 元素。\\n\\n4. swipe(element: int, direction: str, dist: str)\\n此函数用于滑动手机屏幕上显示的 UI 元素，通常是滚动视图或滑动条。\\n\"element\" 是分配给屏幕上显示的 UI 元素的数字标签。\"direction\" 是表示四个方向之一的字符串：\\\\nup, down, left, right。\"direction\" 必须用双引号括起来。\\\\n\"dist\" 决定滑动的距离，可以是以下三个选项之一：short, medium, long。你应该\\n根据需要选择合适的距离选项。\\n一个简单的用例是 swipe(21, \"up\", \"medium\")，即向上滑动标记为数字 21 的 UI 元素\\\\n中等距离。\\n\\n5. grid()\\n当你发现想要交互的元素没有标记数字标签，且\\\\n其他带有数字标签的元素无法帮助完成任务时，你应该调用此函数。该函数将调出网格覆盖层，将\\n手机屏幕划分为小区域，这会让你有更多自由来选择屏幕的任何部分进行点击、长按或滑动。


The task you need to complete is to How much does a 2 bedroom apartment rent for in Denver?.
你需要完成的任务是：丹佛的两居室公寓租金是多少？。


Your past actions to proceed with this task are summarized as follows: None
你之前执行此任务的操作总结如下：无


Now, given the documentation and the following labeled screenshot, you need to think and call the function needed to proceed with the task. Your output should include three parts in the given format:
现在，根据文档和以下带有标签的截图，你需要思考并调用执行任务所需的函数。你的输出应包含给定格式的三个部分：


Observation: <Describe what you observe in the image>
Observation: <描述你在图像中观察到的内容>


Thought: <To complete the given task, what is the next step I should do>
Thought: <为了完成给定任务，我下一步应该做什么>


Action: <The function call with the correct parameters to proceed with the task. When you are certain that the task is successfully done and the goal is reached as of the current observation, you should output FINISH. You cannot output anything else except a function call or FINISH \\nin this field.>
Action: <带正确参数的函数调用以继续执行任务。当你根据当前观察确信任务已成功完成且目标已达成时，你应该输出 FINISH。在此字段中，除了函数调用或 FINISH \\n之外，你不能输出任何其他内容。>


Summary: <Summarize your past actions along with your latest action in one or two sentences. Do not include the numeric \\ntag in your summary>\\nYou can only take one action at a time, so please directly call the function."
Summary: <用一两句话总结你过去的操作以及你最新的操作。摘要中不要包含数字\\\\n标签>\\n你一次只能执行一个操作，所以请直接调用函数。"


Figure 25: Set-of-Marks prompting. The boldened inputs can be changed according to our goal. The task changes for every different task. The past actions change as we take actions (it is None now since this is the prompt for the first round).
图 25：标记集（Set-of-Marks）提示。加粗的输入可以根据我们的目标进行更改。任务随每个不同的任务而变化。过去的操作随着我们执行操作而变化（由于这是第一轮的提示，现在为“无”）。


## NeurIPS Paper Checklist
## NeurIPS 论文自查表


## 1. Claims
## 1. 核心主张


Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
问题：摘要和引言中提出的主要主张是否准确反映了论文的贡献和范围？


## Answer: [Yes]
## 回答：[是]


Justification: The main claims in the abstract and introduction explicitly state the contributions of the paper.
理由：摘要和引言中的主要主张明确阐述了论文的贡献。


Guidelines:
指南：


- The answer NA means that the abstract and introduction do not include the claims made in the paper.
- 回答 NA 表示摘要和引言未包含论文中所作的陈述。


- The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
- 摘要和/或引言应清晰阐明所作陈述，包括论文的贡献以及重要的假设与局限性。对此问题回答 No 或 NA 会给评审人员留下负面印象。


- The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
- 所作陈述应与理论和实验结果相符，并反映出结果在多大程度上可推广至其他设置。


Table 6: Hyperparameters for All Experiments
表 6：所有实验的超参数


<table><tr><td>Method</td><td>Hyperparameter</td><td>Offline</td><td>Offline-to-Online</td></tr><tr><td rowspan="8">Filtered <br> BC</td><td>actor lr</td><td>3e-3</td><td>3e-3</td></tr><tr><td>batch size</td><td>128</td><td>128</td></tr><tr><td>rollout trajectories</td><td>-</td><td>16</td></tr><tr><td>replay buffer size</td><td>-</td><td>5000</td></tr><tr><td>rollout temperature</td><td>-</td><td>1.0</td></tr><tr><td>maximum gradient norm</td><td>0.01</td><td>0.01</td></tr><tr><td>actor updates per iteration</td><td>20</td><td>20</td></tr><tr><td>number of iterations for offline actor updates</td><td>10</td><td>10</td></tr><tr><td rowspan="16">DigiRL</td><td>actor lr</td><td>3e-3</td><td>3e-3</td></tr><tr><td>value function lr</td><td>3e-3</td><td>3e-3</td></tr><tr><td>instruction value function lr</td><td>3e-3</td><td>3e-3</td></tr><tr><td>instruction value function lr</td><td>3e-3</td><td>3e-3</td></tr><tr><td>batch size</td><td>128</td><td>128</td></tr><tr><td>rollout trajectories</td><td>-</td><td>16</td></tr><tr><td>replay buffer size</td><td>-</td><td>5000</td></tr><tr><td>rollout temperature</td><td>-</td><td>1.0</td></tr><tr><td>maximum gradient norm</td><td>0.01</td><td>0.01</td></tr><tr><td>GAE $\lambda$</td><td>0.5</td><td>0.5</td></tr><tr><td>actor updates per iteration</td><td>20</td><td>20</td></tr><tr><td>value function updates per iteration</td><td>5</td><td>5</td></tr><tr><td>instruction value function updates per iteration</td><td>-</td><td>5</td></tr><tr><td>number of iterations for offline actor updates</td><td>10</td><td>10</td></tr><tr><td>number of iterations for offline value function updates</td><td>20</td><td>20</td></tr><tr><td>number of iterations for offline instruction value function updates</td><td>-</td><td>20</td></tr></table>
<table><tbody><tr><td>方法</td><td>超参数</td><td>离线</td><td>离线转在线</td></tr><tr><td rowspan="8">Filtered <br/> BC</td><td>策略网络学习率</td><td>3e-3</td><td>3e-3</td></tr><tr><td>批大小</td><td>128</td><td>128</td></tr><tr><td>采样轨迹数</td><td>-</td><td>16</td></tr><tr><td>重放池大小</td><td>-</td><td>5000</td></tr><tr><td>采样温度</td><td>-</td><td>1.0</td></tr><tr><td>最大梯度范数</td><td>0.01</td><td>0.01</td></tr><tr><td>单次迭代策略网络更新次数</td><td>20</td><td>20</td></tr><tr><td>离线策略网络更新迭代数</td><td>10</td><td>10</td></tr><tr><td rowspan="16">DigiRL</td><td>策略网络学习率</td><td>3e-3</td><td>3e-3</td></tr><tr><td>价值函数学习率</td><td>3e-3</td><td>3e-3</td></tr><tr><td>指令价值函数学习率</td><td>3e-3</td><td>3e-3</td></tr><tr><td>指令价值函数学习率</td><td>3e-3</td><td>3e-3</td></tr><tr><td>批大小</td><td>128</td><td>128</td></tr><tr><td>采样轨迹数</td><td>-</td><td>16</td></tr><tr><td>重放池大小</td><td>-</td><td>5000</td></tr><tr><td>采样温度</td><td>-</td><td>1.0</td></tr><tr><td>最大梯度范数</td><td>0.01</td><td>0.01</td></tr><tr><td>GAE $\lambda$</td><td>0.5</td><td>0.5</td></tr><tr><td>单次迭代策略网络更新次数</td><td>20</td><td>20</td></tr><tr><td>单次迭代价值函数更新次数</td><td>5</td><td>5</td></tr><tr><td>单次迭代指令价值函数更新次数</td><td>-</td><td>5</td></tr><tr><td>离线策略网络更新迭代数</td><td>10</td><td>10</td></tr><tr><td>离线价值函数更新迭代数</td><td>20</td><td>20</td></tr><tr><td>离线指令价值函数更新迭代数</td><td>-</td><td>20</td></tr></tbody></table>


Table 7: Hyperparameters for DigiRL and Filtered BC on both General and Web Shopping subset of AitW..
表 7：DigiRL 和 Filtered BC 在 AitW 通用及网络购物子集上的超参数。


- It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
- 可以将远大目标作为动机，只要明确本文并未实现这些目标即可。


## 2. Limitations
## 2. 局限性


Question: Does the paper discuss the limitations of the work performed by the authors?
问题：论文是否讨论了作者所做工作的局限性？


Answer: [Yes]
回答：[是]


Justification: Limitations are discussed in the last section of the paper.
理由：局限性在论文最后一部分进行了讨论。


Guidelines:
指南：


- The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
- 回答 NA 意味着论文没有局限性，而回答“否”意味着论文有局限性但在文中未讨论。


- The authors are encouraged to create a separate "Limitations" section in their paper.
- 鼓励作者在论文中创建一个独立的“局限性”章节。


- The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
- 论文应指出任何强假设，以及结果在这些假设被违背时的稳健性（例如独立性假设、无噪声设置、模型规范正确性、仅在局部成立的渐近近似）。作者应思考这些假设在实践中可能如何被违背及其影响。


- The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
- 作者应反思所提主张的范围，例如该方法是否仅在少量数据集或少量运行次数上进行了测试。通常，实证结果往往依赖于隐含假设，这些假设应当被阐明。


- The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
- 作者应反思影响方法性能的因素。例如，人脸识别算法在图像分辨率低或光线暗时可能表现不佳。或者，语音转文字系统可能因无法处理专业术语而无法可靠地用于提供在线讲座的字幕。


- The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
- 作者应讨论所提算法的计算效率及其随数据集规模的扩展性。


- If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
- 如适用，作者应讨论其方法在解决隐私和公平性问题时可能存在的局限性。


- While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
- 虽然作者可能担心完全坦诚局限性会被审稿人作为拒稿理由，但更糟的结果是审稿人发现了文中未承认的局限性。作者应运用其最佳判断，并意识到支持透明度的个人行为在形成维护社区诚信的规范中起着重要作用。审稿人将被明确指示不得惩罚关于局限性的诚实行为。


## 3. Theory Assumptions and Proofs
## 3. 理论假设与证明


Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
问题：对于每个理论结果，论文是否提供了完整的假设集和完整（且正确）的证明？


## Answer: [NA]
## 回答：[不适用]


Justification: This paper does not provide theoretical results.
理由：本论文未提供理论结果。


Guidelines:
指南：


- The answer NA means that the paper does not include theoretical results.
- 回答“不适用”意味着论文不包含理论结果。


- All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
- 论文中所有的定理、公式和证明都应编号并进行交叉引用。


- All assumptions should be clearly stated or referenced in the statement of any theorems.
- 所有假设应在任何定理的陈述中清晰说明或引用。


- The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
- 证明可以出现在正文或补充材料中；若在补充材料中，鼓励作者提供简短的证明思路以提供直观理解。


- Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
- 相反，正文中提供的任何非正式证明都应由附录或补充材料中的正式证明作为补充。


- Theorems and Lemmas that the proof relies upon should be properly referenced.
- 证明所依赖的定理和引理应得到妥善引用。


## 4. Experimental Result Reproducibility
## 4. 实验结果可复现性


Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
问题：论文是否充分披露了复现主要实验结果所需的所有信息，且其披露程度足以影响论文的主要主张和/或结论（无论是否提供代码和数据）？


## Answer: [Yes]
## 回答：[是]


Justification: All loss functions and implementation details are provided in Section 4.
理由：所有损失函数和实现细节均在第 4 节中提供。


Guidelines:
指南：


- The answer NA means that the paper does not include experiments.
- 回答“不适用”意味着论文不包含实验。


- If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
- 如果论文包含实验，对该问题的“否”回答将不会给评审人员留下好印象：无论是否提供代码和数据，使论文具有可重现性都很重要。


- If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
- 如果贡献是数据集和/或模型，作者应描述为使其结果可重现或可验证而采取的步骤。


- Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
- 根据贡献的不同，可以通过多种方式实现可重现性。例如，如果贡献是新颖的架构，完整描述该架构可能就足够了；或者如果贡献是特定模型和经验评估，则可能有必要让他人能够使用相同的数据集复制该模型，或提供该模型的访问权限。通常，发布代码和数据是实现这一目标的一种好方法，但可重现性也可以通过以下方式提供：有关如何复制结果的详细说明、访问托管模型（例如在大语言模型的情况下）、发布模型检查点，或其他适合所执行研究的方法。


- While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
- 虽然 NeurIPS 不要求发布代码，但会议要求所有提交的内容都提供某种合理的重现途径，这可能取决于贡献的性质。例如


(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
(a) 如果贡献主要是一种新算法，论文应明确如何重现该算法。


(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
(b) 如果贡献主要是一种新的模型架构，论文应清晰完整地描述该架构。


(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
(c) 如果贡献是一个新模型（例如大语言模型），则应提供访问该模型以重现结果的方法，或者提供重现该模型的方法（例如使用开源数据集或构建该数据集的说明）。


(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
(d) 我们认识到在某些情况下重现性可能很棘手，在这种情况下，欢迎作者描述他们提供重现性的特定方式。对于闭源模型，访问模型可能会受到某种限制（例如仅限注册用户），但其他研究人员应该有某种路径来重现或验证结果。


## 5. Open access to data and code
## 5. 数据和代码的开放获取


Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
问题：论文是否按照补充材料中的描述，提供了数据和代码的开放获取，并附有足以忠实重现主要实验结果的说明？


## Answer: [No]
## 回答：[否]


Justification: We are still actively cleaning the code and make the environment more accessible to a broader audience. Once we are done with that, we will open-source the code along with the release of the paper.
理由：我们仍在积极清理代码，并使环境更易于更广泛的受众使用。一旦完成，我们将在论文发布时同步开源代码。


Guidelines:
指南：


- The answer NA means that paper does not include experiments requiring code.
- 回答 NA 意味着论文不包含需要代码的实验。


- Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.
- 请参阅 NeurIPS 代码和数据提交指南 (https://nips.cc/ public/guides/CodeSubmissionPolicy) 了解更多详情。


- While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
- 虽然我们鼓励发布代码和数据，但我们理解这可能无法实现，因此“否”是一个可以接受的回答。论文不能仅因未包含代码而被拒绝，除非这是贡献的核心（例如对于新的开源基准测试）。


- The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.
- 指南应包含复现结果所需的准确命令和运行环境。详情请参阅 NeurIPS 代码和数据提交指南 (https: //nips.cc/public/guides/CodeSubmissionPolicy)。


- The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
- 作者应提供有关数据访问和准备的指南，包括如何访问原始数据、预处理数据、中间数据以及生成的数据等。


- The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
- 作者应提供脚本以复现新提出的方法和基准测试的所有实验结果。如果只有部分实验可复现，应说明脚本中省略了哪些实验及其原因。


- At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
- 在提交时，为了保持匿名性，作者应发布匿名版本（如适用）。


- Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
- 建议在补充材料（附在论文后）中提供尽可能多的信息，但允许包含指向数据和代码的 URL。


## 6. Experimental Setting/Details
## 6. 实验设置/详情


Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
问题：论文是否具体说明了理解结果所必需的所有训练和测试详情（例如：数据拆分、超参数及其选择方式、优化器类型等）？


## Answer: [Yes]
## 回答：[是]


Justification: Dataset details are provided in Appendix A. 1 and hyperparameters are provided in Appendix I.
理由：数据集详情见附录 A.1，超参数详情见附录 I。


Guidelines:
指南：


- The answer NA means that the paper does not include experiments.
- 回答 NA 意味着论文不包含实验。


- The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
- 实验设置应在论文核心部分呈现，其详细程度应足以让人理解并领会实验结果。


- The full details can be provided either with the code, in appendix, or as supplemental material.
- 完整详情可在代码、附录或补充材料中提供。


## 7. Experiment Statistical Significance
## 7. 实验统计显著性


Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
问题：论文是否报告了定义恰当且正确的误差棒，或其他关于实验统计显著性的适当信息？


## Answer: [Yes]
## 回答：[是]


Justification: Repeated experiments are carried out with their means and standard deviations reported in Table 1.
理由：进行了重复实验，其均值和标准差已在表 1 中报告。


Guidelines:
指南：


- The answer NA means that the paper does not include experiments.
- 回答 NA 表示论文不包含实验。


- The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
- 如果结果附带误差条、置信区间或统计显著性检验，至少对于支持论文主要观点的实验，作者应回答“Yes”。


- The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
- 应明确说明误差条捕捉的变异因素（例如，训练/测试集划分、初始化、某些参数的随机抽取或特定实验条件下的整体运行）。


- The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
- 应说明误差条的计算方法（闭式公式、调用库函数、自助法等）。


- The assumptions made should be given (e.g., Normally distributed errors).
- 应给出所做的假设（例如，正态分布误差）。


- It should be clear whether the error bar is the standard deviation or the standard error of the mean.
- 应明确误差条是标准差还是均值标准误差。


- It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a ${96}\% \mathrm{{CI}}$ ,if the hypothesis of Normality of errors is not verified.
- 报告 1 倍标准差误差条是可以接受的，但应予以说明。如果误差正态性假设未经验证，作者应优先报告 2 倍标准差误差条，而非说明其具有 ${96}\% \mathrm{{CI}}$。


- For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
- 对于非对称分布，作者应注意不要在表格或图中显示可能导致结果超出范围（例如负错误率）的对称误差条。


- If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
- 如果在表格或图中报告了误差条，作者应在正文中解释其计算方式，并在正文中引用相应的图表。


## 8. Experiments Compute Resources
## 8. 实验计算资源


Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
问题：对于每项实验，论文是否提供了复现实验所需的计算资源（计算节点类型、内存、运行时间）的充分信息？


## Answer: [Yes]
## 回答：[Yes]


Justification: This information is provided in Appendix E.
理由：这些信息在附录 E 中提供。


Guidelines:
指南：


- The answer NA means that the paper does not include experiments.
- 回答 NA 表示论文不包含实验。


- The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
- 论文应指明计算节点的类型（CPU 或 GPU）、内部集群或云服务提供商，以及相关的内存和存储。


- The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
- 论文应提供单次实验运行所需的计算量，并估算总计算量。


- The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
- 论文应披露整个研究项目所需的计算量是否超过了论文中报告的实验（例如，未纳入论文的初步实验或失败实验）。


## 9. Code Of Ethics
## 9. 伦理准则


Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
问题：论文中进行的研究是否在各方面均符合 NeurIPS 伦理准则 https://neurips.cc/public/EthicsGuidelines？


## Answer: [Yes]
## 回答：[是]


Justification: The research conducted in the paper conform, in every respect, with the NeuIPS code of Etics.
理由：论文中进行的研究在各方面均符合 NeurIPS 伦理准则。


Guidelines:
指南：


- The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
- 回答 NA 表示作者未审阅 NeurIPS 伦理准则。


- If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
- 如果作者回答“否”，则应说明需要偏离伦理准则的特殊情况。


- The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
- 作者应确保保持匿名（例如，如果由于其所在辖区的法律或法规而有特殊考量）。


## 10. Broader Impacts
## 10. 更广泛的影响


Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
问题：论文是否讨论了所做工作可能产生的正面及负面社会影响？


## Answer: [Yes]
## 回答：[是]


Justification: The positive societal impacts are discussed in the Introduction while the negative societal impacts are discussed in Section 6.
理由：正面社会影响在引言中讨论，负面社会影响在第 6 节中讨论。


Guidelines:
指南：


- The answer NA means that there is no societal impact of the work performed.
- 回答 NA 表示所开展的工作没有社会影响。


- If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
- 如果作者回答 NA 或 No，应解释其工作为何没有社会影响，或为何论文未涉及社会影响。


- Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
- 负面社会影响的例子包括潜在的恶意或意外用途（如虚假信息、生成假个人资料、监视）、公平性考量（如部署可能做出不公平影响特定群体决策的技术）、隐私考量和安全考量。


- The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
- 会议预期许多论文属于基础研究，不与特定应用挂钩，更不用说部署。然而，如果存在通往任何负面应用的直接路径，作者应予以指出。例如，指出生成模型质量的提升可能被用于生成虚假信息的深度伪造（Deepfakes）是合理的；另一方面，则无需指出用于优化神经网络的通用算法可能会使人们更快地训练出生成深度伪造的模型。


- The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
- 作者应考虑技术在按预期使用且功能正常时可能产生的危害、按预期使用但给出错误结果时可能产生的危害，以及因（有意或无意）误用技术而产生的危害。


- If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
- 如果存在负面社会影响，作者还可以讨论可能的缓解策略（如模型的受控发布、在提供攻击手段的同时提供防御措施、监测误用的机制、监测系统如何随时间从反馈中学习的机制、提高机器学习的效率和可访问性）。


## 11. Safeguards
## 11. 安全保障


Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
问题：论文是否描述了为负责任地发布具有高误用风险的数据或模型（如预训练语言模型、图像生成器或爬取的数据集）而采取的安全保障措施？


---



Answer: [NA]
回答：[NA]


---



Justification: The capability of the model that we will be releasing is limited to simple tasks in Android in the Wild dataset, and therefore does not have a high risk for misuse.
理由：我们将发布的模型能力仅限于 Android in the Wild 数据集中的简单任务，因此不具有高误用风险。


Guidelines:
指南：


- The answer NA means that the paper poses no such risks.
- 回答 NA 表示论文不存在此类风险。


- Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
- 具有高误用风险或双重用途风险的已发布模型应带有必要的安全保障措施，以允许对模型进行受控使用，例如要求用户遵守使用指南或访问限制，或实施安全过滤器。


- Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
- 从互联网爬取的数据集可能存在安全风险。作者应描述他们如何避免发布不安全图像。


- We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
- 我们认识到提供有效的安全保障具有挑战性，且许多论文不需要这样做，但我们鼓励作者考虑到这一点并尽最大努力。


## 12. Licenses for existing assets
## 12. 现有资产的许可


Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
问题：论文中使用的资产（如代码、数据、模型）的制作者或原始所有者是否得到了适当的致谢，许可和使用条款是否明确提及并得到妥善遵守？


## Answer: [Yes]
## 回答：[是]


Justification: We have properly cited the assets that we are using.
理由：我们已对所使用的资产进行了妥善引用。


Guidelines:
指南：


- The answer NA means that the paper does not use existing assets.
- 回答 NA 表示论文未使用现有资产。


- The authors should cite the original paper that produced the code package or dataset.
- 作者应引用产出该代码包或数据集的原始论文。


- The authors should state which version of the asset is used and, if possible, include a URL.
- 作者应说明所用资产的版本，并尽可能提供 URL。


- The name of the license (e.g., CC-BY 4.0) should be included for each asset.
- 应包含每个资产的许可名称（如 CC-BY 4.0）。


- For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
- 对于从特定来源（如网站）抓取的数据，应提供该来源的版权和版权服务条款。


- If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
- 如果发布了资产，应提供包中的许可、版权信息和使用条款。对于流行数据集，paperswithcode.com/datasets 收录了部分数据集的许可。其许可指南可帮助确定数据集的许可。


- For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
- 对于重新打包的现有数据集，应同时提供原始许可和衍生资产的许可（如果已更改）。


- If this information is not available online, the authors are encouraged to reach out to the asset's creators.
- 如果无法在线获取此类信息，鼓励作者联系资产制作者。


### 13.New Assets
### 13. 新资产


Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
问题：论文中引入的新资产是否文档齐全，且文档是否随资产一同提供？


## Answer: [NA]
## 回答：[不适用]


Justification: This submission does not include new assets. New assets including open-sourced code, model checkpoints, and model trajectories will be released with documentation when we release the paper.
理由：本次提交不包含新资产。包括开源代码、模型权重和模型轨迹在内的新资产将随论文发布及相关文档一同公开。


Guidelines:
指南：


- The answer NA means that the paper does not release new assets.
- 回答 NA 表示论文未发布新资产。


- Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
- 研究人员应在提交时通过结构化模板沟通数据集/代码/模型的细节。这包括训练、许可、局限性等详情。


- The paper should discuss whether and how consent was obtained from people whose asset is used.
- 论文应讨论是否以及如何从所用资产的所有者处获得许可。


- At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
- 提交时，请记得对您的资产进行匿名化（如果适用）。您可以创建匿名 URL 或包含匿名 zip 文件。


## 14. Crowdsourcing and Research with Human Subjects
## 14. 众包与人类受试者研究


Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?
问题：对于众包实验和人类受试者研究，论文是否包含给予参与者的完整指示文本和截图（如果适用），以及报酬详情（如果有）？


## Answer: [NA]
## 回答：[NA]


Justification: This research does not involve crowdsourcing or human subjects. Annotations of trajectories in Figure 7 and Figure 8 are carried out by authors alone.
理由：本研究不涉及众包或人类受试者。图 7 和图 8 中的轨迹标注仅由作者完成。


Guidelines:
指南：


- The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
- 回答 NA 表示论文不涉及众包或人类受试者研究。


- Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
- 将这些信息包含在补充材料中是可以的，但如果论文的主要贡献涉及人类受试者，则应在正文中包含尽可能详尽的细节。


- According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
- 根据 NeurIPS 伦理准则，参与数据收集、清理或其他劳动的工人之薪酬应至少不低于数据收集者所在国家的最低工资。


## 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects
## 15. 人类受试者研究的机构审查委员会 (IRB) 批准或同等证明


Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
问题：论文是否描述了研究参与者可能承担的风险、这些风险是否已向受试者告知，以及是否获得了机构审查委员会 (IRB) 的批准（或根据您所在国家或机构要求的同等批准/审查）？


Answer: [NA]
回答：[NA]


Justification: This research does not involve crowdsourcing or human subjects.
理由：本研究不涉及众包或人类受试者。


Guidelines:
准则：


- The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
- 回答 NA 意味着论文不涉及众包，也不涉及以人类为受试者的研究。


- Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
- 根据开展研究所在国家/地区的不同，任何人类受试者研究可能都需要 IRB 批准（或同等批准）。如果您获得了 IRB 批准，应在论文中明确说明。


- We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
- 我们认识到相关程序在不同机构和地点之间可能存在显著差异，我们期望作者遵守 NeurIPS 道德准则及其所属机构的指南。


- For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
- 对于初次提交，请勿包含任何可能破坏匿名性的信息（如果适用），例如执行审查的机构。