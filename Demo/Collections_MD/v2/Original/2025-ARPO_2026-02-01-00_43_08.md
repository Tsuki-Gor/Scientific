# ARPO: End-to-End Policy Optimization for GUI Agents with Experience Replay
# ARPO: 结合经验回放的 GUI 智能体端到端策略优化


Fanbin Lu ${}^{1}$ Zhisheng Zhong ${}^{1}$ Shu Liu ${}^{2}$ Chi-Wing Fu ${}^{1}$ Jiaya Jia ${}^{2,3}$
Fanbin Lu ${}^{1}$ Zhisheng Zhong ${}^{1}$ Shu Liu ${}^{2}$ Chi-Wing Fu ${}^{1}$ Jiaya Jia ${}^{2,3}$


${}^{1}$ The Chinese University of Hong Kong ${}^{2}$ SmartMore
${}^{1}$ 香港中文大学 ${}^{2}$ 思谋科技


${}^{3}$ Hong Kong University of Science and Technology
${}^{3}$ 香港科技大学


<img src="https://cdn.noedgeai.com/bo_d5v20ljef24c73bqgee0_0.jpg?x=316&y=708&w=1158&h=369&r=0"/>



Figure 1: Agentic Replay Policy Optimization (ARPO) enables effective end-to-end policy optimization for GUI agents. (a) Our vision-language agent processes long-horizon visual observations and interaction histories to generate sequential actions and receive policy gradients from sparse, delayed rewards. (b) ARPO significantly boosts in-domain task success rates compared to baseline and GRPO-only training. (c) Average training reward steadily increases, demonstrating improved policy learning and sample efficiency in complex GUI environments.
图 1：智能体回放策略优化 (ARPO) 实现了 GUI 智能体有效的端到端策略优化。(a) 我们的视觉语言智能体处理长时程视觉观测和交互历史，生成序贯动作，并从稀疏、延迟的奖励中获取策略梯度。(b) 与基准和仅使用 GRPO 训练相比，ARPO 显著提升了域内任务成功率。(c) 平均训练奖励稳步增长，证明了在复杂 GUI 环境中改进的策略学习和样本效率。


## Abstract
## 摘要


Training large language models (LLMs) as interactive agents for controlling graphical user interfaces (GUIs) presents a unique challenge to optimize long-horizon action sequences with multimodal feedback from complex environments. While recent works have advanced multi-turn reinforcement learning (RL) for reasoning and tool-using capabilities in LLMs, their application to GUI-based agents remains relatively underexplored due to the difficulty of sparse rewards, delayed feedback, and high rollout costs. In this paper, we investigate end-to-end policy optimization for vision-language-based GUI agents with the aim of improving performance on complex, long-horizon computer tasks. We propose Agentic Replay Policy Optimization (ARPO), an end-to-end RL approach that augments Group Relative Policy Optimization (GRPO) with a replay buffer to reuse the successful experience across training iterations. To further stabilize the training process, we propose a task selection strategy that filters tasks based on baseline agent performance, allowing the agent to focus on learning from informative interactions. Additionally, we compare ARPO with offline preference optimization approaches, highlighting the advantages of policy-based methods in GUI environments. Experiments on the OSWorld benchmark demonstrate that ARPO achieves competitive results, establishing a new performance baseline for LLM-based GUI agents trained via reinforcement learning. Our findings underscore the effectiveness of reinforcement learning for training multi-turn, vision-language GUI agents capable of managing complex real-world UI interactions. Codes and models:https://github.com/dvlab-research/ARPO.git.
将大语言模型 (LLM) 训练为控制图形用户界面 (GUI) 的交互式智能体，对于如何利用来自复杂环境的多模态反馈来优化长时程动作序列提出了独特挑战。虽然近期工作在提升 LLM 的推理和工具使用能力的推理强化学习 (RL) 方面取得了进展，但由于奖励稀疏、反馈延迟和采样成本高等困难，其在基于 GUI 的智能体上的应用仍处于相对初步的探索阶段。在本文中，我们研究了基于视觉语言的 GUI 智能体端到端策略优化，旨在提高其在复杂、长时程计算机任务上的性能。我们提出了智能体回放策略优化 (ARPO)，这是一种端到端的 RL 方法，它通过引入回放缓存来增强群组相对策略优化 (GRPO)，从而在训练迭代中复用成功经验。为了进一步稳定训练过程，我们提出了一种任务选择策略，根据基准智能体的表现过滤任务，使智能体能够专注于从信息丰富的交互中学习。此外，我们将 ARPO 与离线偏好优化方法进行了比较，突出了在 GUI 环境中基于策略的方法的优势。在 OSWorld 基准测试上的实验表明，ARPO 取得了具有竞争力的结果，为通过强化学习训练的基于 LLM 的 GUI 智能体建立了新的性能基准。我们的研究结果强调了强化学习对于训练能够处理复杂现实世界 UI 交互的多轮视觉语言 GUI 智能体的有效性。代码与模型：https://github.com/dvlab-research/ARPO.git。


## 1 Introduction
## 1 引言


Among various agent types, GUI agents that interact with the computer screen through vision-based perception and action have been of long-standing interest $\left\lbrack  {2,{16},4}\right\rbrack$ . Most prior work relies on supervised fine-tuning (SFT) on large-scale trajectory data. These agents are typically trained through SFT on large-scale trajectory datasets, where the model learns to imitate human behavior by predicting the next action based on the current screenshot and interaction history. However, these agents lack the ability to self-correct and suffer from error accumulation in the working trajectory.
在各种类型的智能体中，通过基于视觉的感知和动作与计算机屏幕交互的 GUI 智能体长期以来备受关注 $\left\lbrack  {2,{16},4}\right\rbrack$。大多数先前的工作依赖于在大规模轨迹数据上进行监督微调 (SFT)。这些智能体通常在 SFT 下进行训练，模型通过根据当前截图和交互历史预测下一个动作来学习模仿人类行为。然而，这些智能体缺乏自我修正能力，并且在执行轨迹中容易受到误差累积的影响。


To address these limitations, we explore reinforcement learning (RL) in GUI agent training. In contrast to single-turn RL or static reward optimization, we adopt Group Relative Policy Optimization (GRPO) [5], a recent variant of PPO [18] that eliminates the need for a value function and estimates token-level advantages from group-wise reward normalization. GRPO has demonstrated promising results in mathematical reasoning [19] and tool-use agent [15]. It is a natural fit for training vision-language agents due to its ability to handle long sequences and multiple modalities with improved efficiency.
为了解决这些局限性，我们探索了在 GUI 智能体训练中使用强化学习 (RL)。与单轮 RL 或静态奖励优化不同，我们采用了群组相对策略优化 (GRPO) [5]，这是 PPO [18] 的最新变体，它消除了对价值函数的依赖，并通过群组奖励归一化来估计 Token 级别的优势。GRPO 在数学推理 [19] 和工具使用智能体 [15] 方面已展现出显著成果。由于其能够以更高的效率处理长序列和多种模态，它非常适合用于训练视觉语言智能体。


This paper tackles the challenge of end-to-end policy optimization for GUI agents, with a particular focus on multi-turn, multi-modal agent design, see Fig. 1a. Our goal is to maximize the rule-based reward from the environment over entire trajectories using GRPO. However, GUI environments typically offer sparse and delayed reward signals: agents receive feedback only upon task completion, and many complex tasks may yield no reward at all during early training phases. Moreover, the cost of rollouts in real desktop environments is non-trivial. GUI interaction involves operating system-level delays, which significantly slow down the data collection process. To overcome these obstacles, we develop a scalable distributed rollout system that enables parallel interaction with real desktop environments, such as OSWorld [25]. By batching inference across environments, we reduce latency and make efficient use of GPU resources, thus facilitating rollout collection at scale.
本文解决了 GUI 智能体端到端策略优化的挑战，特别关注多轮、多模态智能体设计，见图 1a。我们的目标是使用 GRPO 在整个轨迹上最大化来自环境的基于规则的奖励。然而，GUI 环境通常提供稀疏且延迟的奖励信号：智能体仅在任务完成时收到反馈，且许多复杂任务在训练早期可能完全没有奖励。此外，在真实桌面环境中的采样成本极高。GUI 交互涉及操作系统级别的延迟，这显著降低了数据收集的速度。为了克服这些障碍，我们开发了一个可扩展的分布式采样系统，能够与 OSWorld [25] 等真实桌面环境进行并行交互。通过跨环境的批量推理，我们降低了延迟并高效利用了 GPU 资源，从而促进了大规模的采样收集。


To further enhance training stability and sample efficiency, we introduce a task selection strategy that filters for those capable of producing successful rollouts under baseline agents. This curated subset enhances signal quality during early training and accelerates convergence. We further introduce an experience replay buffer tailored to GUI agent learning. This buffer stores successful trajectories on a per-task basis and dynamically injects them into GRPO training groups when all sampled rollouts in a group fail. The inclusion of at least one high-reward trajectory within each group ensures meaningful reward variance, which is critical for computing token-level advantages.
为了进一步增强训练稳定性和样本效率，我们引入了一种任务选择策略，筛选出那些能够在基准智能体下产生成功交互的任务。这一精选子集增强了训练早期的信号质量并加速了收敛。我们还进一步引入了专为GUI智能体学习定制的经验回放缓存。该缓存按任务存储成功的轨迹，并在某组采样交互全部失败时，将其动态注入GRPO训练组。每个组内至少包含一条高奖励轨迹，确保了有意义的奖励方差，这对于计算词元级优势至关重要。


We conduct extensive evaluations on the OSWorld benchmark and observe that reinforcement learning effectively improves agent performance. We also find an interesting fact that RL training delivers strong gains on in-domain tasks, but hardly benefits out-of-domain agentic tasks.
我们在OSWorld基准测试上进行了广泛评估，观察到强化学习有效地提升了智能体性能。我们还发现一个有趣的事实：RL训练在域内任务上带来了强劲增益，但在域外智能体任务上几乎没有收益。


Our contributions are summarized as follows:
我们的贡献总结如下：


- We propose an end-to-end policy optimization approach for training a GUI agent in challenging multi-turn, multi-modal environments using GRPO.
- 我们提出了一种端到端的策略优化方法，使用GRPO在具有挑战性的多轮、多模态环境中训练GUI智能体。


- We demonstrate that careful selection of training tasks is critical for maintaining reward diversity and ensuring stable policy optimization.
- 我们证明了精心选择训练任务对于维持奖励多样性和确保稳定的策略优化至关重要。


- We propose an experience replay buffer that retains successful trajectories, enhancing sample efficiency and stabilizing training in sparse-reward settings.
- 我们提出了一种保留成功轨迹的经验回放缓存，在稀疏奖励设置下提高了样本效率并稳定了训练。


- We find that reinforcement learning substantially improves agent performance on in-domain tasks, while offering moderate generalization improvements to out-of-domain agentic tasks.
- 我们发现强化学习显著提升了智能体在域内任务上的性能，同时对域外智能体任务提供了适度的泛化改进。


## 2 Related Works
## 2 相关工作


GUI Agents. Recent advances in multimodal models have led to significant progress in GUI and web-based automation. SeeClick [2] and ScreenAgent [14] utilize large vision-language models (VLMs) with visual input processing to perform interactive tasks on user interfaces. Building on this, OmniAct [9] introduces a benchmark that focuses on generating executable actions from visually grounded natural language instructions. CogAgent [6] and UI-Tars [16] extend pretraining with large-scale web and desktop interface data, enhancing screen understanding and agent behavior. GUI-R1 [24] explores reinforcement learning to improve UI grounding in VLM-based agents. However, directly optimizing policy models for GUI agents in an end-to-end policy optimization way remains unexplored in current research.
GUI智能体。多模态模型的最新进展推动了GUI和基于Web自动化的显著进步。SeeClick [2] 和 ScreenAgent [14] 利用具有视觉输入处理能力的大型视觉语言模型（VLM）在用户界面上执行交互任务。在此基础上，OmniAct [9] 引入了一个基准测试，专注于从视觉定位的自然语言指令中生成可执行操作。CogAgent [6] 和 UI-Tars [16] 通过大规模Web和桌面界面数据扩展了预训练，增强了屏幕理解和智能体行为。GUI-R1 [24] 探索了通过强化学习来改善VLM智能体中的UI定位。然而，在当前研究中，以端到端策略优化的方式直接为GUI智能体优化策略模型仍处于空白。


Reinforcement Learning for Agents. Rule-based reinforcement learning (RL) has proven effective in fine-tuning large language models (LLMs) across a range of domains. OpenAI's o1 [8] and DeepSeek-R1 [5] demonstrate strong performance in tasks such as mathematical reasoning [19], code generation [11], and multi-modal inference [7, 12] through structured reward signals. ToolRL [15] extends this paradigm by introducing RL-based training for LLM agents that interact with external tools. Sweet-RL [29] introduces a multi-turn DPO framework to enhance long-horizon language agent behaviors. RAGEN [21] further advances multi-turn RL by applying it in live, rule-based environments for self-evolving agent training.
智能体强化学习。基于规则的强化学习（RL）已被证明在微调跨多个领域的大语言模型（LLM）方面非常有效。OpenAI的o1 [8] 和 DeepSeek-R1 [5] 通过结构化奖励信号，在数学推理 [19]、代码生成 [11] 和多模态推理 [7, 12] 等任务中展现了强大的性能。ToolRL [15] 通过为与外部工具交互的LLM智能体引入基于RL的训练，扩展了这一范式。Sweet-RL [29] 引入了一个多轮DPO框架来增强长程语言智能体的行为。RAGEN [21] 通过在实时、基于规则的环境中应用多轮RL进行自我演化智能体训练，进一步推动了该领域的发展。


Despite these advancements, most existing work focuses on symbolic tasks or static tool use. Applying reinforcement learning to vision-language agents operating in dynamic, multimodal GUI environments remains a challenging task. In particular, this work aims to leverage rule-based rewards from live desktop environments for end-to-end policy optimization in multi-turn GUI agents.
尽管取得了这些进展，现有的大多数工作仍集中在符号任务或静态工具使用上。将强化学习应用于在动态、多模态GUI环境中运行的视觉语言智能体仍然是一项具有挑战性的任务。特别是，本工作旨在利用来自实时桌面环境的基于规则的奖励，对多轮GUI智能体进行端到端策略优化。


## 3 Method
## 3 方法


In this section, we first provide a brief introduction to the preliminaries of Group Relative Policy Optimization (GRPO). Then, we describe the architecture and training procedure of our GUI agent. The agent builds upon vision-language models (VLMs), enhanced with longer context windows and longer image, action chains. These modifications are essential for training complex GUI tasks with end-to-end reinforcement learning algorithms like GRPO.
在本节中，我们首先简要介绍组相对策略优化（GRPO）的预备知识。然后，我们描述GUI智能体的架构和训练流程。该智能体基于视觉语言模型（VLM）构建，并增强了更长的上下文窗口以及更长的图像和动作链。这些改进对于使用GRPO等端到端强化学习算法训练复杂的GUI任务至关重要。


### 3.1 Preliminary
### 3.1 预备知识


Group Relative Policy Optimization (GRPO) [19] is a reinforcement learning algorithm designed to optimize language models efficiently without requiring an explicit value function or critic. GRPO modifies the standard Proximal Policy Optimization (PPO) objective by computing token-level advantages based on group-normalized rewards, making it particularly suitable for LLMs.
组相对策略优化（GRPO）[19] 是一种强化学习算法，旨在高效优化语言模型，而无需显式的价值函数或判别器（critic）。GRPO通过计算基于组归一化奖励的词元级优势，修改了标准的近端策略优化（PPO）目标，使其特别适用于LLM。


Given a batch of $G$ responses ${\left\{  {o}_{i}\right\}  }_{i = 1}^{G}$ from a query $q$ ,each consisting of a sequence of tokens ${o}_{i} = \left( {{o}_{i}\left( 1\right) ,\ldots ,{o}_{i}\left( T\right) }\right)$ ,the GRPO objective is defined as:
给定来自查询 $q$ 的一批 $G$ 响应 ${\left\{  {o}_{i}\right\}  }_{i = 1}^{G}$，每个响应由词元序列 ${o}_{i} = \left( {{o}_{i}\left( 1\right) ,\ldots ,{o}_{i}\left( T\right) }\right)$ 组成，GRPO目标定义为：


$$
{J}_{\mathrm{{GRPO}}}\left( \theta \right)  = \frac{1}{G}\mathop{\sum }\limits_{{i = 1}}^{G}\frac{1}{\left| {o}_{i}\right| }\mathop{\sum }\limits_{{t = 1}}^{\left| {o}_{i}\right| }\min \left\lbrack  {\frac{{\pi }_{\theta }\left( {{o}_{i}\left( t\right)  \mid  {o}_{i, < t}}\right) }{{\pi }_{\mathrm{{old}}}\left( {{o}_{i}\left( t\right)  \mid  {o}_{i, < t}}\right) }{\widehat{A}}_{i,t},\operatorname{clip}\left( {\frac{{\pi }_{\theta }\left( {{o}_{i}\left( t\right)  \mid  {o}_{i, < t}}\right) }{{\pi }_{\mathrm{{old}}}\left( {{o}_{i}\left( t\right)  \mid  {o}_{i, < t}}\right) },1 - \varepsilon ,1 + \varepsilon }\right) {\widehat{A}}_{i,t}}\right\rbrack  ,
$$



where ${\widehat{A}}_{i,t}$ is the group-normalized advantage for token $t$ in response ${o}_{i}$ ,computed as:
其中 ${\widehat{A}}_{i,t}$ 是响应 ${o}_{i}$ 中标记 $t$ 的组归一化优势值，计算公式为：


$$
{\widehat{A}}_{i,t} = \frac{{r}_{i} - \mu }{\sigma },\;\text{ with }{r}_{i}\text{ the total reward of }{o}_{i},
$$



and $\mu ,\sigma$ ,the mean and standard deviation of rewards in the group.
以及 $\mu ,\sigma$ ，即组内奖励的均值和标准差。


### 3.2 Multi-turn GUI Agent
### 3.2 多轮 GUI 智能体


Unlike single-turn reinforcement learning, GUI agents are required to perform multi-turn reasoning and decision-making, interacting with dynamic environments that provide visual feedback. We adopt a Markov Decision Process (MDP) framework, where each agent trajectory comprises a sequence of screenshot observations ${s}_{t}$ ,mouse and keyboard actions ${a}_{t}$ ,and a scalar reward $r$ in the end of the trajectory. The agent policy ${\pi }_{\theta }$ is optimized to maximize the rewards:
与单轮强化学习不同，GUI 智能体需要进行多轮推理和决策，与提供视觉反馈的动态环境交互。我们采用马尔可夫决策过程（MDP）框架，其中每个智能体轨迹包含一系列截图观测 ${s}_{t}$ 、鼠标和键盘动作 ${a}_{t}$ ，以及轨迹末端的标量奖励 $r$ 。优化智能体策略 ${\pi }_{\theta }$ 以最大化奖励：


$$
\tau  = {\left\{  {s}_{t},{a}_{t}\right\}  }_{t = 0,1,\cdots ,T - 1}\text{ ,where }{a}_{t} \sim  {P}_{\theta }\left( {\left\{  {s}_{i},{a}_{i}\right\}  }_{i < t}\right) \text{ . } \tag{1}
$$



Our GUI agent is built upon the UI-Tars [16] framework and the Qwen2.5-VL architecture [1]. To predict the next action ${a}_{t}$ the model tokenizes the entire history of screenshots and corresponding actions into the input context of the VLM model.
我们的 GUI 智能体基于 UI-Tars [16] 框架和 Qwen2.5-VL 架构 [1] 构建。为了预测下一步动作 ${a}_{t}$ ，模型将完整的截图历史和相应动作进行标记化，并输入到 VLM 模型的上下文空间中。


Our design results in a VLM model with at most 15 images input and ${64K}$ model context length to correctly process an entire GUI trajectory with ${1080}\mathrm{P}$ resolution. Unlike prior short-context
我们的设计使 VLM 模型最多可输入 15 张图像并具备 ${64K}$ 模型上下文长度，从而能够以 ${1080}\mathrm{P}$ 分辨率正确处理完整的 GUI 轨迹。与以往的短上下文


<img src="https://cdn.noedgeai.com/bo_d5v20ljef24c73bqgee0_3.jpg?x=349&y=199&w=1121&h=470&r=0"/>



Figure 2: Illustration of the reinforcement learning procedure for our multi-turn GUI agent. For a single task,we use $n$ parallel environments and perform rollouts to collect trajectories and rewards ${\left\{  {\tau }_{i},{r}_{i}\right\}  }_{i = 0,1,\cdots ,n - 1}$ in the environments. If all the rewards are zero,we fetch a positive trajectory ${\tau }^{ + }$ from the replay buffer to avoid gradient vanishing.
图 2：多轮 GUI 智能体强化学习流程图。对于单个任务，我们使用 $n$ 个并行环境进行采样，以收集环境中的轨迹和奖励 ${\left\{  {\tau }_{i},{r}_{i}\right\}  }_{i = 0,1,\cdots ,n - 1}$ 。如果所有奖励均为零，我们将从回放缓存中获取一条正向轨迹 ${\tau }^{ + }$ 以避免梯度消失。


GUI agents $\left\lbrack  {2,4,{26}}\right\rbrack$ ,which truncate the trajectory and only process the most recent one or two screenshots, our approach leverages the full trajectory history, enabling the model to reason over long-term dependencies and optimize performance across the entire interaction sequence.
GUI 智能体 $\left\lbrack  {2,4,{26}}\right\rbrack$ 不同（这类智能体会截断轨迹且仅处理最近的一两张截图），我们的方法利用了完整的轨迹历史，使模型能够进行长期依赖推理，并优化整个交互序列的性能。


## CoT for GUI Agents
## GUI 智能体的思维链（CoT）


To enhance the reasoning capabilities of VLM agents, we integrate the Chain-of-Thought (CoT) prompting technique [22] into our action generation process. Each action ${a}_{t}$ is composed of:
为了增强 VLM 智能体的推理能力，我们将思维链（CoT）提示技术 [22] 整合到动作生成过程中。每个动作 ${a}_{t}$ 由以下部分组成：


- A thinking part, which represents the agent's internal reasoning.
- 思考部分：代表智能体的内部推理。


- A solution part, which executes the resulting action.
- 方案部分：执行最终的动作。


This design allows the agent to perform more accurate and interpretable decision-making.
这种设计使智能体能够做出更准确且可解释的决策。


## Action Space Definition
## 动作空间定义


Our GUI agent adopts the action space defined in UI-Tars [16], including the following primitive operations:
我们的 GUI 智能体采用 UI-Tars [16] 中定义的动作空间，包括以下原子操作：


- LEFT_CLICK, RIGHT_CLICK, SCROLL
- 左键点击、右键点击、滚动


- TYPE_TEXT, PRESS_HOTKEY
- TYPE_TEXT, PRESS_HOTKEY


In addition to these, several meta-actions are used to manage the agent's workflow:
除这些以外，还有几种用于管理智能体工作流的元动作：


- WAIT: Pause and observe the environment.
- WAIT：暂停并观察环境。


- FINISH: Successfully complete the task.
- FINISH：成功完成任务。


- FAIL: Indicate task failure.
- FAIL：表示任务失败。


- CALL_USER: Request human intervention.
- CALL_USER：请求人工干预。


### 3.3 Distributed Trajectory Rollout
### 3.3 分布式轨迹展开


Training GUI agents through reinforcement learning requires scalable and efficient trajectory collection across rich, interactive desktop environments. To meet this need, we design a distributed trajectory rollout strategy tailored for parallel interaction with live environments, such as OSWorld [25].
通过强化学习训练 GUI 智能体需要在丰富且交互式的桌面环境中进行可扩展且高效的轨迹收集。为了满足这一需求，我们设计了一种分布式轨迹展开策略，专为与 OSWorld [25] 等实时环境进行并行交互而定制。


We establish a set of rollout workers. Each worker consists of an interactive environment paired with a GUI agent that maintains a history of screenshots and corresponding actions,denoted as $\left( {{s}_{t},{a}_{t}}\right)$ . Each rollout worker continuously captures screenshots of the current GUI environment and transmits them to a centralized language model inference server powered by VLLM [10]. The policy model processes these batched visual observations in parallel, predicting the next action for all environments simultaneously.
我们建立了一组展开工作器。每个工作器包含一个交互式环境，并配有一个维护屏幕截图历史和对应动作（记为 $\left( {{s}_{t},{a}_{t}}\right)$）的 GUI 智能体。每个展开工作器持续捕获当前 GUI 环境的截图，并将其传输到由 VLLM [10] 提供支持的集中式语言模型推理服务器。策略模型并行处理这些批量的视觉观测结果，同时为所有环境预测下一步动作。


Unlike math [19] or tool-use [15] environments, interaction with live GUIs like OSWorld incurs non-trivial latency due to OS-level delays. Parallel trajectory rollout allows for efficient utilization of GPU resources on the inference server and minimizes the per-step decision latency.
与数学 [19] 或工具使用 [15] 环境不同，与 OSWorld 等实时 GUI 的交互会因操作系统级延迟而产生明显的滞后。并行轨迹展开可以高效利用推理服务器上的 GPU 资源，并将每步决策延迟降至最低。


### 3.4 End-to-End Policy Optimization with GRPO
### 3.4 使用 GRPO 进行端到端策略优化


We adopt GRPO [19] as our reinforcement learning algorithm to train vision-based GUI agents. GRPO eliminates the need for a value function by leveraging group-wise reward normalization to compute token-level advantages. This property makes it well-suited for training VLM agents with multiple image inputs and extended context length.
我们采用 GRPO [19] 作为强化学习算法来训练基于视觉的 GUI 智能体。GRPO 通过利用组内奖励归一化来计算 Token 级优势，从而消除了对价值函数的需求。这一特性使其非常适合训练具有多个图像输入和较长上下文长度的 VLM 智能体。


Reward Design. To effectively guide policy optimization, we design a structured reward function incorporating both task-level success and action format correctness.
奖励设计。为了有效引导策略优化，我们设计了一个结构化奖励函数，结合了任务级成功和动作格式正确性。


- Trajectory Reward: For each task,we have a scalar trajectory-level reward ${r}_{t}$ ,based on task completion. A reward of ${r}_{t} = 1$ is assigned if the agent successfully completes the task as defined by the OSWorld [25],and ${r}_{i} = 0$ otherwise. This binary reward provides a high-level training signal to encourage successful multi-turn planning and execution.
- 轨迹奖励：对于每个任务，我们根据任务完成情况设定一个标量轨迹级奖励 ${r}_{t}$。如果智能体成功完成了 OSWorld [25] 所定义的任务，则分配奖励 ${r}_{t} = 1$，否则分配 ${r}_{i} = 0$。这种二值奖励提供了一个高层训练信号，以鼓励成功的跨轮次规划与执行。


- Action Format Reward: During rollout, each response from the VLM agent is parsed into discrete actions. If a response fails to conform to the required action schema and cannot be parsed, we assign a penalty of ${r}_{f} =  - 1$ . This encourages the model to generate syntactically valid and executable actions.
- 动作格式奖励：在展开过程中，来自 VLM 智能体的每个响应都会被解析为离散动作。如果响应不符合要求的动作架构且无法解析，我们将分配惩罚值 ${r}_{f} =  - 1$。这鼓励模型生成语法有效且可执行的动作。


Training Objective. We treat GUI interaction as a multi-turn MDP, where the agent observes a sequence of screenshots ${s}_{t}$ and generates actions ${a}_{t}$ to complete a task instruction $x \in  \mathcal{D}$ . The trajectory $\tau  = \left( {{s}_{0},{a}_{0},\ldots ,{a}_{n}}\right)$ is encoded by a VLM agent with extended context,enabling long-horizon reasoning over multiple steps and observations. Our training objective is to maximize the expected reward over tasks and trajectories:
训练目标。我们将 GUI 交互视为一个多轮 MDP，其中智能体观察一系列截图 ${s}_{t}$ 并生成动作 ${a}_{t}$ 以完成任务指令 $x \in  \mathcal{D}$。轨迹 $\tau  = \left( {{s}_{0},{a}_{0},\ldots ,{a}_{n}}\right)$ 由具有扩展上下文的 VLM 智能体进行编码，从而实现对多个步骤和观测结果的长跨度推理。我们的训练目标是最大化任务和轨迹上的期望奖励：


$$
\mathop{\max }\limits_{\theta }{\mathbb{E}}_{x \sim  \mathcal{D},\tau  \sim  {\pi }_{\theta }}\left\lbrack  {{r}_{t}\left( {x,\tau }\right)  + {r}_{f}\left( {x,\tau }\right) }\right\rbrack  . \tag{2}
$$



We optimize this objective using GRPO, which estimates token-level advantages via group-normalized trajectory rewards, allowing efficient and scalable training without a value function.
我们使用 GRPO 优化这一目标，它通过分组归一化的轨迹奖励来估计 token 级优势，从而在无需价值函数的情况下实现高效且可扩展的训练。


Valuable Tasks Selection for GRPO. Despite recent progress in the variants of GRPO [28], the task of training GUI agents remains difficult, particularly due to the sparse reward signals associated with complex desktop environments like OSWorld [25]. Many tasks in this benchmark are not reliably solvable by current state-of-the-art agents $\left\lbrack  {{16},4}\right\rbrack$ ,even when given multiple attempts. As a result, these tasks generate limited feedback during rollouts, which can hinder effective policy optimization training for GRPO.
为 GRPO 选择有价值的任务。尽管 GRPO 的变体 [28] 最近取得了进展，但训练 GUI 智能体仍然困难，特别是由于 OSWorld [25] 等复杂桌面环境相关的奖励信号稀疏。该基准测试中的许多任务即使经过多次尝试，目前的先进智能体 $\left\lbrack  {{16},4}\right\rbrack$ 也无法可靠地解决。因此，这些任务在采样过程中产生的反馈有限，会阻碍 GRPO 有效的策略优化训练。


To improve the sampling efficiency, we introduce a task filtering procedure to identify a subset of "valuable" tasks, those capable of producing successful trajectories under a baseline agent. Specifically, we evaluate each task in OSWorld using the UI-Tars-1.5 model, performing 16 rollouts per task. A task is retained in the GRPO training set if the agent completes it successfully in at least one of these attempts. This method yields a curated set of 128 tasks that are more amenable to early-stage learning, allowing the policy optimization to benefit from informative reward signals.
为了提高采样效率，我们引入了一种任务过滤程序，以识别“有价值”的任务子集，即那些能够在基准智能体下产生成功轨迹的任务。具体而言，我们使用 UI-Tars-1.5 模型评估 OSWorld 中的每个任务，每个任务进行 16 次采样。如果智能体在这些尝试中至少成功完成一次，则该任务将保留在 GRPO 训练集中。该方法产生了一组包含 128 个任务的精选集，这些任务更适合早期学习，使策略优化能够从信息丰富的奖励信号中获益。


### 3.5 Experience Replay Buffer
### 3.5 经验回放池


Dynamic Sampling [28] has been proposed to improve the sample efficiency of GRPO by removing training groups in which all rewards are uniform. In such cases, the computed token-level advantages are zero across the group, resulting in vanishing gradients and slowed convergence. However, this strategy becomes less effective in GUI interaction settings due to two primary challenges: the high cost of obtaining trajectories and the infrequency of successful rollouts.
动态采样 [28] 被提出用于通过移除所有奖励均相同的训练组来提高 GRPO 的采样效率。在这种情况下，计算出的组内 token 级优势全为零，导致梯度消失并减慢收敛。然而，由于两个主要挑战：获取轨迹的高昂成本以及成功采样的低频率，这种策略在 GUI 交互设置中变得不那么有效。


Unlike mathematical reasoning tasks, which typically follow well-defined logical chains, GUI-based tasks sometimes require a certain amount of exploratory interactions with the environment, resulting in sparse reward signals. Therefore, successful trajectories are rare but especially informative. Preserving and reusing them is critical for the training progress.
与通常遵循明确定义的逻辑链的数学推理任务不同，基于 GUI 的任务有时需要与环境进行一定量的探索性交互，从而导致奖励信号稀疏。因此，成功的轨迹虽然罕见，但信息量特别大。保留并重复使用它们对于训练进度至关重要。


To address this, we introduce an experience replay buffer that caches successful trajectories on a per-task basis. During training, if an entire GRPO training group consists of only failed trajectories (i.e., all with zero reward), we randomly replace one of them with a previously stored successful trajectory from the buffer for the corresponding task. This guarantees that, as long as the agent has successfully completed a task once, its training group in the later training process will include at least one rollout with a non-zero reward signal, as illustrated in Fig. 2. The buffer is updated dynamically during rollout. To prevent the stored samples from diverging too significantly from the current policy, we impose a fixed-size limit on the buffer and evict the oldest entries when full.
为了解决这个问题，我们引入了一个经验回放池，按任务缓存成功的轨迹。在训练过程中，如果整个 GRPO 训练组仅由失败轨迹组成（即奖励全部为零），我们会随机将其中一条替换为该任务此前存储在回放池中的成功轨迹。如图 2 所示，这确保了只要智能体成功完成过一次任务，其后续训练过程中的训练组将包含至少一个具有非零奖励信号的采样。回放池在采样期间动态更新。为了防止存储的样本与当前策略偏离过大，我们对回放池施加了固定大小的限制，并在存满时剔除最早的条目。


## 4 Experiments
## 4 实验


### 4.1 Implementation Details
### 4.1 实现细节


Training Details. We use the 7B UI-Tars-1.5 model [16] as the base and conduct training using the VERL framework [20]. For trajectory rollout, we set up 256 parallel virtual environments and the rollout number for each task is 8. A total of 128 tasks are sampled from the OSWorld benchmark, according to the strategy described in Sec. 3.4, and training is performed over 15 epochs. Rollouts are conducted with a batch size of 32 and a temperature of 1.0 to encourage exploration. For policy optimization,we use the AdamW optimizer [13] with a learning rate of $1 \times  {10}^{-6}$ and a mini-batch size of 8 per device. The gradient accumulation number is 4. Following DAPO [28], we set the clipping parameters to ${\epsilon }_{\text{ low }} = {0.2}$ and ${\epsilon }_{\text{ high }} = {0.3}$ to balance exploration and exploitation. During evaluation, the temperature is lowered to 0.6 for more stable performance. We remove the KL divergence loss to remove the need for the reference model.
训练细节。我们使用 7B UI-Tars-1.5 模型 [16] 作为基础模型，并使用 VERL 框架 [20] 进行训练。对于轨迹采样，我们设置了 256 个并行虚拟环境，每个任务的采样次数为 8。根据 3.4 节描述的策略，从 OSWorld 基准测试中采样了总计 128 个任务，并在 15 个 epoch 上进行训练。采样时的 batch size 为 32，温度为 1.0 以鼓励探索。对于策略优化，我们使用 AdamW 优化器 [13]，学习率为 $1 \times  {10}^{-6}$，每台设备的 mini-batch 大小为 8。梯度累积次数为 4。参考 DAPO [28]，我们将裁剪参数设置为 ${\epsilon }_{\text{ low }} = {0.2}$ 和 ${\epsilon }_{\text{ high }} = {0.3}$ 以平衡探索与利用。在评估期间，温度降至 0.6 以获得更稳定的性能。我们移除了 KL 散度损失，从而无需参考模型。


Datasets and Benchmarks. We evaluate our method on the OSWorld benchmark [25], a recently proposed real-computer environment designed for evaluating multimodal agents on open-ended GUI tasks. OSWorld contains 369 tasks across diverse domains such as office productivity, web browsing, system management, and multi-app workflows. Each task is executed within virtual machines using real applications and evaluated via execution-based scripts. The benchmark supports full GUI interaction with mouse and keyboard actions, enabling rigorous assessment of multi-turn vision-based agents in realistic desktop environments.
数据集与基准测试。我们在 OSWorld 基准测试 [25] 上评估我们的方法，这是一个最近提出的真实计算机环境，旨在评估多模态智能体在开放式 GUI 任务上的表现。OSWorld 包含 369 个任务，涵盖办公效率、网页浏览、系统管理和多应用工作流等不同领域。每个任务都在使用真实应用程序的虚拟机中执行，并通过基于执行的脚本进行评估。该基准测试支持完整的 GUI 交互（包括鼠标和键盘操作），能够对真实桌面环境中的多轮视觉智能体进行严格评估。


Evaluation Metrics. We follow the standard rule-based evaluation protocol defined in OS-World [25]. Each agent trajectory receives a scalar reward between 0 and 1.0 from the environment. We notice that previous works [16] replace the last action with a FAIL action when the maximum step number is reached in a rollout. While this approach may prevent unstable or endlessly running behaviors during evaluation, it will hack the rewards of the real impossible tasks defined in the benchmark. To provide a more accurate assessment of agent capabilities for RL, we introduce a stricter evaluation protocol that prohibits final action replacement, denoted as OSWorld Hard.
评估指标。我们遵循 OS-World [25] 中定义的标准基于规则的评估协议。每个智能体轨迹从环境中获得 0 到 1.0 之间的标量奖励。我们注意到，之前的工作 [16] 在 rollout 达到最大步数限制时会将最后一步操作替换为 FAIL。虽然这种方法可以防止评估过程中出现不稳定或无休止运行的行为，但它会干扰基准测试中定义的真实不可完成任务的奖励。为了更准确地评估强化学习（RL）智能体的能力，我们引入了一种更严格的评估协议，禁止替换最后一步操作，称之为 OSWorld Hard。


### 4.2 Experimental Results
### 4.2 实验结果


We evaluate the performance of our ARPO method on the OSWorld benchmark [25], comparing it against several recent GUI agents. As shown in Table 1, our approach achieves the highest success rates across both evaluation settings. Specifically, applying ARPO to the UI-Tars-1.5 base model results in a success rate of 29.9% on the standard OSWorld setting and 23.8% on the stricter OSWorld Hard variant-improving upon the original UI-Tars-1.5 model by 6.4% and 5.6%, respectively. These results highlight the effectiveness of reinforcement learning with GRPO and structured experience replay in enhancing multi-turn GUI decision-making. Additionally, ARPO shows consistent gains across earlier model versions; for example, UI-Tars-7B-DPO improves from 15.6% to 20.4% with ARPO. All the models are tested with a maximum step number limit of 15 for a single trajectory.
我们在 OSWorld 基准测试 [25] 上评估了 ARPO 方法的性能，并将其与近期几种 GUI 智能体进行了对比。如表 1 所示，我们的方法在两种评估设置下均获得了最高的成功率。具体而言，将 ARPO 应用于 UI-Tars-1.5 基座模型后，在标准 OSWorld 设置下的成功率达到 29.9%，在更严格的 OSWorld Hard 变体下达到 23.8%，分别比原始 UI-Tars-1.5 模型提升了 6.4% 和 5.6%。这些结果突显了结合 GRPO 和结构化经验回放的强化学习在增强多轮 GUI 决策方面的有效性。此外，ARPO 在早期模型版本中也表现出一致的提升；例如，UI-Tars-7B-DPO 在应用 ARPO 后从 15.6% 提升至 20.4%。所有模型在单条轨迹下的最大步数限制均为 15 步。


Table 1: OSWorld evaluation performance for GUI Agents. All models are evaluated at a maximum execution length of 15. We provide numerical results for two metrics: OSWorld and OSWorld Hard
表 1：GUI 智能体的 OSWorld 评估性能。所有模型均在最大执行长度为 15 的条件下进行评估。我们提供了两项指标的数值结果：OSWorld 和 OSWorld Hard


<table><tr><td>Model</td><td>GPT-40</td><td>OSWorld</td><td>OSWorld Hard</td></tr><tr><td>Aria-UI [27]</td><td>✓</td><td>15.2%</td><td>-</td></tr><tr><td>Aguvis-7B [26]</td><td>✓</td><td>14.8%</td><td>-</td></tr><tr><td>Aguvis-72B [26]</td><td>✓</td><td>17.0%</td><td>-</td></tr><tr><td>OS-Atlas-7B [23]</td><td>✓</td><td>14.6%</td><td>-</td></tr><tr><td>UI-Tars-7B-DPO</td><td></td><td>15.6%</td><td>11.3%</td></tr><tr><td>UI-Tars-7B-DPO + GRPO</td><td></td><td>18.3% (+2.7%)</td><td>16.4% (+5.1%)</td></tr><tr><td>UI-Tars-7B-DPO + ARPO</td><td></td><td>20.4% (+4.8%)</td><td>18.0% (+6.7%)</td></tr><tr><td>UI-Tars-7B-1.5</td><td></td><td>23.5%</td><td>18.2%</td></tr><tr><td>UI-Tars-7B-1.5 + GPRO</td><td></td><td>26.0% (+2.5%)</td><td>20.9% (+2.7%)</td></tr><tr><td>UI-Tars-7B-1.5 + ARPO</td><td></td><td>29.9% (+6.4%)</td><td>23.8% (+5.6%)</td></tr></table>
<table><tbody><tr><td>模型</td><td>GPT-4o</td><td>OSWorld</td><td>OSWorld 困难</td></tr><tr><td>Aria-UI [27]</td><td>✓</td><td>15.2%</td><td>-</td></tr><tr><td>Aguvis-7B [26]</td><td>✓</td><td>14.8%</td><td>-</td></tr><tr><td>Aguvis-72B [26]</td><td>✓</td><td>17.0%</td><td>-</td></tr><tr><td>OS-Atlas-7B [23]</td><td>✓</td><td>14.6%</td><td>-</td></tr><tr><td>UI-Tars-7B-DPO</td><td></td><td>15.6%</td><td>11.3%</td></tr><tr><td>UI-Tars-7B-DPO + GRPO</td><td></td><td>18.3% (+2.7%)</td><td>16.4% (+5.1%)</td></tr><tr><td>UI-Tars-7B-DPO + ARPO</td><td></td><td>20.4% (+4.8%)</td><td>18.0% (+6.7%)</td></tr><tr><td>UI-Tars-7B-1.5</td><td></td><td>23.5%</td><td>18.2%</td></tr><tr><td>UI-Tars-7B-1.5 + GPRO</td><td></td><td>26.0% (+2.5%)</td><td>20.9% (+2.7%)</td></tr><tr><td>UI-Tars-7B-1.5 + ARPO</td><td></td><td>29.9% (+6.4%)</td><td>23.8% (+5.6%)</td></tr></tbody></table>


<img src="https://cdn.noedgeai.com/bo_d5v20ljef24c73bqgee0_6.jpg?x=312&y=744&w=535&h=353&r=0"/>



Figure 3: Ablation study of the replay buffer.
图 3：回放缓冲区的消融研究。


<img src="https://cdn.noedgeai.com/bo_d5v20ljef24c73bqgee0_6.jpg?x=874&y=738&w=614&h=356&r=0"/>



Figure 4: Ablation study for GRPO and ARPO in-domain and out-of-domain RL training tasks.
图 4：GRPO 和 ARPO 在域内和域外 RL 训练任务中的消融研究。


### 4.3 Ablation on the Replay Buffer
### 4.3 回放缓冲区的消融研究


We evaluate the impact of the experience replay buffer by comparing training trajectories with and without its use, as shown in Fig. 3. The model equipped with the replay buffer begins to outperform the baseline around Step 30 and maintains a consistent advantage throughout the remainder of training. This improvement is attributed to the buffer's ability to retain successful trajectories. This gain is largely due to the buffer's ability to retain high-reward trajectories from earlier stages, which serve as strong learning signals in later updates. By maintaining reward diversity within GRPO groups and non-zero advantages, the replay buffer supports more stable optimization and accelerates convergence. By the end of training, the model with replay achieves a higher average trajectory reward (0.75 vs. 0.65), demonstrating that leveraging past successes substantially improves both sample efficiency and overall policy performance in sparse-reward GUI environments.
如图 3 所示，我们通过对比使用与不使用经验回放缓冲区的训练轨迹来评估其影响。配备回放缓冲区的模型在第 30 步左右开始超越基线，并在剩余训练过程中保持一致优势。这种提升归功于缓冲区保留成功轨迹的能力。这种增益主要源于缓冲区能够保留早期阶段的高奖励轨迹，这些轨迹在后期更新中充当了强大的学习信号。通过维持 GRPO 组内的奖励多样性和非零优势，回放缓冲区支持更稳定的优化并加速收敛。到训练结束时，带有回放的模型实现了更高的平均轨迹奖励（0.75 vs. 0.65），证明利用过去的成功实质性地提高了稀疏奖励 GUI 环境下的样本效率和整体策略性能。


The benefits of the replay buffer extend beyond reward curves. As shown in Fig. 4, the in-domain task success rate climbs from 68.8% with GRPO to 81.25% with ARPO, a 12.5% absolute improvement. This substantial gain highlights the replay buffer's critical role in enhancing policy generalization and downstream performance.
回放缓冲区的益处延伸到了奖励曲线之外。如图 4 所示，域内任务成功率从 GRPO 的 68.8% 上升到 ARPO 的 81.25%，实现了 12.5% 的绝对提升。这一显著增益突显了回放缓冲区在增强策略泛化和下游性能方面的关键作用。


### 4.4 Does RL training generalize well to OOD GUI agent tasks?
### 4.4 RL 训练是否能很好地泛化到 OOD GUI 智能体任务？


To assess the generalization ability of RL training, we evaluate model performance on both in-domain and out-of-domain (OOD) tasks. Specifically, we select 32 tasks from the training task set for reinforcement learning, using the remaining 96 as OOD tasks. As shown in Fig. 4, reinforcement learning substantially improves in-domain accuracy: GRPO achieves 68.8% and ARPO reaches 81.25%, compared to 43.8% for the base UI-Tars-1.5 model. However, on OOD tasks, gains are more modest. UI-Tars-1.5 achieves 55.2%, while GRPO slightly underperforms at 52.08%. ARPO, however, recovers generalization capability, scoring 56.3%, slightly above the base model, indicating that structured trajectory grouping and replay mitigate overfitting. Overall, while reinforcement learning effectively improves the in-domain success rate of VLM agents, strong generalization still depends on broader task diversity, carefully designed reward signals, and larger-scale training compute.
为了评估 RL 训练的泛化能力，我们评估了模型在域内和域外 (OOD) 任务上的表现。具体而言，我们从训练任务集中选择 32 个任务进行强化学习，其余 96 个作为 OOD 任务。如图 4 所示，强化学习显著提高了域内准确率：相比基础 UI-Tars-1.5 模型的 43.8%，GRPO 达到 68.8%，ARPO 达到 81.25%。然而，在 OOD 任务上，增益较为有限。UI-Tars-1.5 达到 55.2%，而 GRPO 的表现略低，为 52.08%。不过，ARPO 恢复了泛化能力，得分为 56.3%，略高于基础模型，表明结构化轨迹分组和回放减轻了过拟合。总的来说，虽然强化学习有效提高了 VLM 智能体的域内成功率，但强大的泛化仍取决于更广泛的任务多样性、精心设计的奖励信号以及更大规模的训练算力。


<img src="https://cdn.noedgeai.com/bo_d5v20ljef24c73bqgee0_7.jpg?x=315&y=202&w=1164&h=461&r=0"/>



Figure 5: Training performance comparison for RL training with selected subset and full set.
图 5：选定子集与全集 RL 训练的训练性能对比。


### 4.5 Valuable Task Selection for GRPO Training
### 4.5 GRPO 训练的有价值任务选择


As outlined in Sec. 3.4, we adopt a task selection strategy for GRPO training by filtering out tasks that consistently fail to provide meaningful reward signals. To evaluate the impact of this approach, we conduct an ablation study comparing GRPO performance when trained on a curated subset of 128 valuable tasks versus the full task set. As illustrated in Fig. 5a, training on the selected subset leads to significantly higher average trajectory rewards and faster convergence speed from the early stages of training.
如 3.4 节所述，我们对 GRPO 训练采取了任务选择策略，过滤掉那些始终无法提供有意义奖励信号的任务。为了评估该方法的影响，我们进行了一项消融研究，对比了在 128 个精选有价值任务子集上训练与在全任务集上训练的 GRPO 性能。如图 5a 所示，在选定子集上训练从训练早期阶段就带来了显著更高的平均轨迹奖励和更快的收敛速度。


Fig. 5b shows that the standard deviation of rewards within GRPO groups is consistently higher when training on the curated task set. This increased variance is critical for GRPO, which relies on within-group reward diversity to compute token-level advantages. In contrast, training on the full task set results in flatter reward distributions with reduced variance.
图 5b 显示，在精选任务集上训练时，GRPO 组内的奖励标准差始终更高。这种增加的方差对于依赖组内奖励多样性来计算 token 级优势的 GRPO 至关重要。相比之下，在全任务集上训练会导致奖励分布更平坦，方差更小。


### 4.6 Comparison with Offline Preference Optimization
### 4.6 与离线偏好优化的对比


In Fig. 6, we compare the performance between GRPO and offline preference optimization algorithms. For a fair comparison, all methods are trained on the same task set with an equal number of rollouts. We compare GRPO with reject sampling, DPO [17], and KTO [3]. For reject sampling, we take only the positive trajectory for SFT training. For DPO, we randomly sample a positive and a negative trajectory per task to create paired training data. For KTO, we threshold the scalar rewards at 0.5 to generate binary labels for training.
在图 6 中，我们对比了 GRPO 与离线偏好优化算法的性能。为了公平对比，所有方法均在相同任务集上以相同数量的采样进行训练。我们将 GRPO 与拒绝采样、DPO [17] 和 KTO [3] 进行了对比。对于拒绝采样，我们仅选取正向轨迹进行 SFT 训练。对于 DPO，我们为每个任务随机采样一个正向和负向轨迹以构建配对训练数据。对于 KTO，我们将标量奖励的阈值设为 0.5 以生成二进制标签进行训练。


ARPO achieves the highest score (27.3%), followed by GRPO (26.0%), both outperforming preference-based methods by a significant margin. Among the baselines, KTO performs best (24.6%), while DPO and Reject Sampling lag behind at 22.4% and 21.8%, respectively. These results suggest that direct trajectory-level optimization with rule-based rewards provides stronger learning signals than offline preference modeling. The added experience replay in ARPO further enhances stability and sample efficiency in sparse-reward GUI settings.
ARPO 获得了最高分 (27.3%)，其次是 GRPO (26.0%)，两者均大幅领先于基于偏好的方法。在基线中，KTO 表现最好 (24.6%)，而 DPO 和拒绝采样分别落后于 22.4% 和 21.8%。这些结果表明，利用基于规则的奖励进行直接轨迹级优化比离线偏好建模提供了更强的学习信号。ARPO 中增加的经验回放进一步增强了稀疏奖励 GUI 环境下的稳定性和样本效率。


### 4.7 Rollout Efficiency Analysis
### 4.7 采样效率分析


Fig. 7 shows that increasing the number of parallel environments significantly improves training efficiency. We show the rollout time for a single batch of trajectories (in minutes) and the total time to sample all trajectories in an epoch (in hours), respectively. As the batch size increases, although the rollout time per batch grows from 3 minutes (8 environments) to 19 minutes (256 environments), the total time per epoch drops sharply from over 6 hours to just around 1.2 hours.
图 7 显示，增加并行环境的数量显著提高了训练效率。我们分别展示了单批次轨迹的展开时间（以分钟为单位）和每个 epoch 采样所有轨迹的总时间（以小时为单位）。随着批次大小的增加，虽然每批次的展开时间从 3 分钟（8 个环境）增长到 19 分钟（256 个环境），但每个 epoch 的总时间从超过 6 小时骤降至仅约 1.2 小时。


<img src="https://cdn.noedgeai.com/bo_d5v20ljef24c73bqgee0_8.jpg?x=312&y=206&w=576&h=344&r=0"/>



Effect of Parallel Environments on Rollout Speed
并行环境对展开速度的影响


Rollout time per batch (min)
每批次展开时间 (min)


Time per epoch (h)
每 epoch 时间 (h)


17.5



ollout Time (min) 15.0
展开时间 (min) 15.0


10.0



7.5



2.5



32 64 128 256



Figure 6: Comparisons with offline preference optimization methods. Figure 7: Rollout Efficiency.
图 6：与离线偏好优化方法的比较。图 7：展开效率。


Thought: I noticed that there is a subscript feature in the toolbar, which is exactly what I need. To change the "2" in $\mathrm{H}2\mathrm{O}$ to a subscript, I first need to select that "2." So now, I will use my mouse to click and drag from the position of the first "H" all the way to where the "O" is located. This way, I can accurately select just that middle number. Task: Help me change the 2 in "H2O" to a subscript.
思考：我注意到工具栏中有一个下标功能，这正是我需要的。为了将 $\mathrm{H}2\mathrm{O}$ 中的“2”更改为下标，我首先需要选择那个“2”。所以现在，我将使用鼠标从第一个“H”的位置一直点击并拖动到“O”所在的位置。这样，我就可以准确地只选中中间那个数字。任务：帮我将“H2O”中的 2 更改为下标。


<img src="https://cdn.noedgeai.com/bo_d5v20ljef24c73bqgee0_8.jpg?x=309&y=751&w=600&h=342&r=0"/>



Action #1: drag(start_box='(689,493)', end_box='(710,495)')
动作 #1：drag(start_box='(689,493)', end_box='(710,495)')


Thought: I just selected the "2" in H2O, and now I need to locate the subscript button on the toolbar. It should be in that row of formatting tools, and clicking it will change this number into a subscript.
思考：我刚刚选中了 H2O 中的“2”，现在我需要找到工具栏上的下标按钮。它应该在那一排格式工具中，点击它将把这个数字变成下标。


Action #2: click(start_box='(863,149)')
动作 #2：click(start_box='(863,149)')


Thought: I just accidentally clicked the wrong button and changed the "2" in H2O to a superscript, which is not what we wanted. Let me press Ctrl+Z to undo that mistake and start over. Action #3: hotkey(key='ctrl z')
思考：我刚才不小心点错了按钮，把 H2O 中的“2”变成了上标，这不是我们想要的。让我按 Ctrl+Z 撤销那个错误并重新开始。动作 #3：hotkey(key='ctrl z')


Figure 8: Trajectory showing the ARPO-trained agent correcting a misclick error.
图 8：展示经 ARPO 训练的智能体纠正误点错误的轨迹。


This speedup stems from two main factors: (1) Larger batches allow the VLLM server to perform more efficient GPU inference, and (2) OS-level delays in GUI environments are overlapped for all parallel environments. As a result, scaling to 256 environments enables high-throughput rollouts, making RL training in real desktop settings much more practical.
这种加速源于两个主要因素：(1) 更大的批次允许 VLLM 服务器执行更高效的 GPU 推理，以及 (2) GUI 环境中的操作系统级延迟在所有并行环境中发生了重叠。因此，扩展到 256 个环境实现了高吞吐量的展开，使真实桌面设置中的强化学习训练变得更加实用。


### 4.8 Qualitative Analysis: Self-Correction Behavior in GUI Agent
### 4.8 定性分析：GUI 智能体的自我纠错行为


Figure 8 illustrates a trajectory where the ARPO-trained agent demonstrates self-corrective behavior. Initially, it selects the superscript button instead of the subscript button. It realizes the mistake by observing the current screen and decides to use the Ctrl-Z hotkey to revert the previous operation. Notably, the success rate for the specific before and after ARPO are 25% vs. 62.5%.
图 8 展示了一条轨迹，其中经 ARPO 训练的智能体表现出了自我纠错行为。最初，它选择了上标按钮而不是下标按钮。它通过观察当前屏幕意识到了错误，并决定使用 Ctrl-Z 热键撤销之前的操作。值得注意的是，ARPO 训练前后的特定成功率分别为 25% 和 62.5%。


## 5 Conclusion
## 5 结论


In this work, we present a reinforcement learning approach for training GUI agents using vision-language models enhanced with longer input context and multi-turn, multi-modal screenshot processing. By introducing ARPO, a variant of GRPO tailored for GUI agents, we demonstrate that rule-based reward signals can effectively guide end-to-end policy optimization in complex GUI environments. Our experiments show that careful task selection significantly improves learning stability and reward variance.
本研究提出了一种利用视觉语言模型训练GUI智能体的强化学习方法，该模型增强了长输入上下文以及多轮多模态截图处理能力。通过引入ARPO（一种专为GUI智能体定制的GRPO变体），我们证明了基于规则的奖励信号可以有效引导复杂GUI环境下的端到端策略优化。实验表明，谨慎的任务选择能显著提升学习稳定性并降低奖励方差。


This study highlights the potential of combining multimodal understanding with reinforcement learning to enable more adaptive and capable GUI agents. Future directions include expanding the task set to cover a broader range of real-world applications, extending the context length of agents further to support more sophisticated trial-and-error behaviors, and investigating the use of learned reward models to autonomously evaluate trajectories, reducing reliance on manually crafted reward functions.
本研究强调了结合多模态理解与强化学习以实现更具自适应能力和更强大GUI智能体的潜力。未来方向包括：扩展任务集以覆盖更广泛的真实世界应用；进一步延长智能体的上下文长度以支持更复杂的试错行为；以及研究利用训练好的奖励模型自动评估轨迹，从而减少对人工设定奖励函数的依赖。


## References
## 参考文献


[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3
[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3


[2] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Secelick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. 2,4
[2] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Secelick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. 2,4


[3] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. 8
[3] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. 8


[4] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. 2, 4, 5
[4] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. 2, 4, 5


[5] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 3
[5] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 3


[6] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14281-14290, 2024. 2
[6] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14281-14290, 2024. 2


[7] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 3
[7] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 3


[8] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Hel-yar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 3
[8] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Hel-yar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 3


[9] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. arXiv preprint arXiv:2402.17553, 2024. 2
[9] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. arXiv preprint arXiv:2402.17553, 2024. 2


[10] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611-626, 2023. 5
[10] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611-626, 2023. 5


[11] Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. arXiv preprint arXiv:2503.18470, 2025. 3
[11] Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. arXiv preprint arXiv:2503.18470, 2025. 3


[12] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. 3
[12] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, 和 Jiaya Jia. Seg-zero: 通过认知强化实现推理链引导的分割. arXiv 预印本 arXiv:2503.06520, 2025. 3


[13] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6
[13] Ilya Loshchilov 和 Frank Hutter. 解耦权重衰减正则化. arXiv 预印本 arXiv:1711.05101, 2017. 6


[14] Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan Leng, He Kong, Yi Chang, and Qi Wang. Screenagent: A vision language model-driven computer control agent. arXiv preprint arXiv:2402.07945, 2024. 2
[14] Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan Leng, He Kong, Yi Chang, 和 Qi Wang. Screenagent: 视觉语言模型驱动的计算机控制智能体. arXiv 预印本 arXiv:2402.07945, 2024. 2


[15] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. 2, 3, 5
[15] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, 和 Heng Ji. Toolrl: 奖励是工具学习所需的一切. arXiv 预印本 arXiv:2504.13958, 2025. 2, 3, 5


[16] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. 2, 3, 4, 5, 6
[16] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, 等. Ui-tars: 探索原生智能体自动化 GUI 交互. arXiv 预印本 arXiv:2501.12326, 2025. 2, 3, 4, 5, 6


[17] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36:53728-53741, 2023. 8
[17] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, 和 Chelsea Finn. 直接偏好优化：你的语言模型秘密地是一个奖励模型. 神经信息处理系统进展, 36:53728-53741, 2023. 8


[18] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 2
[18] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, 和 Oleg Klimov. 近端策略优化算法. arXiv 预印本 arXiv:1707.06347, 2017. 2


[19] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300.2, 3, 5
[19] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, 等. Deepseekmath: 挑战开源语言模型数学推理的极限, 2024. URL https://arxiv.org/abs/2402.03300.2, 3, 5


[20] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. 6
[20] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, 和 Chuan Wu. Hybridflow: 一个灵活高效的 RLHF 框架. arXiv 预印本 arXiv:2409.19256, 2024. 6


[21] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025. 3
[21] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, 等. Ragen: 通过多轮强化学习理解 LLM 智能体的自我进化. arXiv 预印本 arXiv:2504.20073, 2025. 3


[22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022. 4
[22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, 等. 思维链提示在大型语言模型中激发推理. 神经信息处理系统进展, 35:24824-24837, 2022. 4


[23] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: A foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. 7
[23] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, 等. Os-atlas: 通用 GUI 智能体基础动作模型. arXiv 预印本 arXiv:2410.23218, 2024. 7


[24] Xiaobo Xia and Run Luo. Gui-r1: A generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. 2
[24] Xiaobo Xia 和 Run Luo. Gui-r1: 适用于 GUI 智能体的通用 R1 式视觉语言动作模型. arXiv 预印本 arXiv:2504.10458, 2025. 2


[25] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:52040-52094, 2024. 2, 4, 5, 6
[25] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: 真实计算机环境下开放式任务的多模态智能体基准测试. Advances in Neural Information Processing Systems, 37:52040-52094, 2024. 2, 4, 5, 6


[26] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. 4, 7
[26] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: 用于自主 GUI 交互的统一纯视觉智能体. arXiv preprint arXiv:2412.04454, 2024. 4, 7


[27] Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions. arXiv preprint arXiv:2412.16256, 2024. 7
[27] Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: GUI 指令的视觉定位. arXiv preprint arXiv:2412.16256, 2024. 7


[28] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 5, 6
[28] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: 一个大规模开源 LLM 强化学习系统. arXiv preprint arXiv:2503.14476, 2025. 5, 6


[29] Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li. Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks. arXiv preprint arXiv:2503.15478, 2025. 3
[29] Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li. Sweet-rl: 在协作推理任务上训练多轮 LLM 智能体. arXiv preprint arXiv:2503.15478, 2025. 3