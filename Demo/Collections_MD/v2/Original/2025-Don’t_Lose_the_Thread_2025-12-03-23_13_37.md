# DON'T LOSE THE THREAD: EMPOWERING LONG- HORIZON LLM AGENTS WITH COGNITIVE RESOURCE SELF-ALLOCATION
# 别让线索断裂：赋能长时域 LLM 代理的认知资源自我分配


Anonymous authors
匿名作者


Paper under double-blind review
论文正处于双盲评审


## ABSTRACT
## 摘要


Agents powered by large language models (LLMs) have demonstrated remarkable progress in solving complex reasoning tasks. However, LLM agents often falter on long-horizon tasks due to cognitive overload, as their working memory becomes cluttered with expanding and irrelevant information, which dilutes their attention and hinders effective planning and reasoning. To mitigate this challenge, we introduce COgnitive Resource Self-ALlocation (CORAL), a novel reasoning paradigm that empowers agents to proactively optimize their context. Implemented as an agent-callable working memory management toolset, CORAL allows an agent to maintain crucial checkpoints of its progress within its working memory and adaptively initiate a new problem-solving episode by purging cluttered working memory and resuming its reasoning from the most recent checkpoint, effectively reallocating agentic cognitive resources by implicitly sharpening their attention on the checkpoints. We further enhance the agent's checkpoint capabilities using a Multi-episode Agentic Reinforced Policy Optimization algorithm. On several long-horizon task benchmarks, CORAL significantly outperforms standard LLM agent methods. Notably, analysis of the LLMs' attention distribution reveals that CORAL substantially optimizes agentic RL dynamics, which in turn ensures agents maintain a focused cognitive resource allocation, thereby continuously amplifying performance gains.
由大型语言模型（LLM）驱动的代理在解决复杂推理任务上取得了显著进展。然而，LLM 代理在长时域任务中常因认知过载而失效——其工作记忆被不断扩展的无关信息填满，分散注意力并阻碍有效的规划与推理。为缓解此问题，我们提出了认知资源自我分配（COgnitive Resource Self-ALlocation，CORAL），一种使代理主动优化其上下文的新型推理范式。作为可被代理调用的工作记忆管理工具集实现，CORAL 允许代理在工作记忆中保留关键进展检查点，并在必要时通过清理混乱的工作记忆并从最近的检查点恢复推理来自适应启动新的问题解决回合，从而通过隐式聚焦于检查点来重新分配代理的认知资源。我们进一步通过多回合代理强化策略优化算法增强了代理的检查点能力。在若干长时域任务基准上，CORAL 显著优于标准 LLM 代理方法。值得注意的是，对 LLM 注意力分布的分析表明，CORAL 实质性地优化了代理的 RL 动力学，进而确保代理保持聚焦的认知资源分配，从而持续放大性能提升。


## 1 INTRODUCTION
## 1 引言


RecentIt, LLM-driven agents represent a powerful paradigm that extends the capabilities of Large Language Models (LLMs) through the integration of external tools (OpenAI, 2025b; Gemini, 2025) Liu et al. 2025; Li et al. 2025b), substantially outperforming methods reliant on single-turn inference. To address long-horizon tasks, these agents operate on a THOUGHT-ACTION-OBSERVATION cycle (Yao et al. 2022), engaging in multiple cycles of planning, environmental interaction, and reasoning (Erdogan et al. 2025; Qiao et al. 2024; Huang et al. 2024). A critical challenge arises as each cycle populates LLM agents' context with verbose environmental feedback and a history of failed attempts (Wu et al. 2025b, Shinn et al. 2023). This escalating contextual noise progressively degrades the model's planning and reasoning faculties (Yang et al. 2025), a phenomenon comparable to the cognitive overload that impairs human problem-solving when working memory becomes saturated.
近来，LLM 驱动的代理作为一种强大范式，通过集成外部工具扩展了大型语言模型（LLMs）的能力（OpenAI, 2025b；Gemini, 2025；Liu et al. 2025；Li et al. 2025b），在多轮推理上远超依赖单次推断的方法。为应对长时域任务，这些代理沿用 THOUGHT-ACTION-OBSERVATION 循环（Yao et al. 2022），在多个规划、环境交互与推理回合中运作（Erdogan et al. 2025；Qiao et al. 2024；Huang et al. 2024）。关键挑战在于每一回合都会向 LLM 代理的上下文填入冗长的环境反馈和失败尝试的历史（Wu et al. 2025b；Shinn et al. 2023），这种逐渐增长的上下文噪声会逐步削弱模型的规划与推理能力（Yang et al. 2025），类似于当工作记忆饱和时对人类问题解决的认知过载影响。


Current paradigms for context optimization in LLM agents seek to prevent this contextual bloat by pruning messages or distilling salient information. The activation of these methods is generally governed by rule-based heuristics, such as fixed intervals (Zhou et al. 2025b; Yu et al. 2025a) or the imminent saturation of the context window (Wu et al. 2025c). The underlying mechanism for optimization typically involves either truncating the context directly (Luo et al. 2025) or utilizing external models to achieve compression (Wu et al. 2025c).
当前 LLM 代理的上下文优化范式旨在通过修剪消息或提取要点来防止这种上下文膨胀。这些方法通常由基于规则的启发式触发，例如固定间隔（Zhou et al. 2025b；Yu et al. 2025a）或接近上下文窗口饱和时（Wu et al. 2025c）。优化的底层机制通常要么直接截断上下文（Luo et al. 2025），要么使用外部模型进行压缩（Wu et al. 2025c）。


Fundamentally, this redundant context is a direct consequence of the agent's imperfect planning and reasoning. Suboptimal tool use generates a high volume of irrelevant environmental feedback (Wang et al. 2025), which in turn clutters the context and further degrades the agent's reasoning, creating a vicious cycle. To address this, one line of research employs agentic reinforcement learning (RL), using algorithms such as GRPO (Shao et al., 2024b) and DAPO (Yu et al., 2025b) with carefully designed reward functions to optimize the agent's tool-use policy (Qian et al., 2025; Zhang et al. 2025; Jin et al., 2025). However, in long-horizon tasks involving multi-step interactions, these RL methods face significant challenges with reward sparsity. Relying solely on final outcomes makes it difficult to assign credit to intermediate actions, leading to unstable and inefficient training dynamics. While methods like estimated step-level credit assignment can mitigate this (Feng et al., 2025) Xia et al. 2025; Chandrahasan et al. 2025), proactive context optimization presents a powerful, orthogonal method of improving RL training dynamics (Kimi, 2025). Wu et al. 2025c).
从根本上讲，这类冗余上下文直接源于代理不完美的规划与推理。次优的工具使用会产生大量无关的环境反馈（Wang et al. 2025），进而使上下文混乱并进一步削弱代理的推理，形成恶性循环。为此，一类研究采用代理化强化学习（RL），使用诸如 GRPO（Shao et al., 2024b）和 DAPO（Yu et al., 2025b）等算法，并通过精心设计的奖励函数来优化代理的工具使用策略（Qian et al., 2025；Zhang et al., 2025；Jin et al., 2025）。然而，在涉及多步交互的长时域任务中，这些 RL 方法面临奖励稀疏的重大挑战。仅依赖最终结果使得中间行动难以归因，从而导致训练动态不稳定且低效。尽管诸如估计步级归因的方法可以缓解这一问题（Feng et al., 2025；Xia et al. 2025；Chandrahasan et al. 2025），主动的上下文优化仍是一种强有力且正交的改进 RL 训练动态的方法（Kimi, 2025；Wu et al. 2025c）。


<img src="https://cdn.noedgeai.com/bo_d4ng05f7aajc73frsb80_1.jpg?x=320&y=228&w=1159&h=754&r=0"/>



Figure 1: Comparison of the ReAct and CORAL frameworks. CORAL enhances the standard ReAct loop by incorporating two key components: a Memory Management (MM) tool and a Context Optimization (CO) tool. The memory is designed to store two categories of information: task progress and verified facts. The CO tool periodically resets the model's context, which segments a complete trajectory into a series of independent units termed episodes.
图 1：ReAct 与 CORAL 框架比较。CORAL 在标准 ReAct 循环上引入两项关键组件：记忆管理（MM）工具和上下文优化（CO）工具。记忆用于存储两类信息：任务进展与已验证事实。CO 工具周期性地重置模型上下文，将完整轨迹划分为一系列称为回合的独立单元。


To address these challenges, we introduce the COgnitive Resource Self-ALlocation (CORAL) framework. CORAL extends existing agentic architectures with a callable toolset for working memory management, empowering an agent to dynamically optimize its context and sustain high-level planning and reasoning throughout long-horizon tasks. Specifically, the agent can autonomously invoke memory tools to create checkpoints of its progress and verified facts. The periodic insertion of these checkpoints along the task trajectory systematically refocuses the agent's attention on its most current state, preventing cognitive resources from being squandered on obsolete information, such as prior environmental feedback or failed attempts. This process facilitates an implicit yet effective self-allocation of cognitive resources. Furthermore, CORAL allows the agent to adaptively initiate new problem-solving episodes by purging its working memory and resuming its reasoning from the latest checkpoint. We initially enhance the crucial checkpointing ability, the capacity to accurately distill key task information through Supervised Fine-Tuning (SFT).
为应对这些挑战，我们提出了认知资源自我分配（COgnitive Resource Self-ALlocation，CORAL）框架。CORAL 在现有的智能体架构上扩展了可调用的工作记忆管理工具集，使智能体能够动态优化其上下文并在长时程任务中维持高级规划与推理能力。具体而言，智能体可以自主调用记忆工具来创建关于进展与已验证事实的检查点。沿任务轨迹定期插入这些检查点，会系统性地将智能体的注意力重新聚焦到其最新状态，防止认知资源被过时信息（如早期环境反馈或失败尝试）浪费。该过程促成了一种隐含但有效的认知资源自我分配。此外，CORAL 允许智能体通过清空工作记忆并从最新检查点恢复推理，来自适应地发起新的问题求解回合。我们首先通过监督微调（Supervised Fine-Tuning，SFT）强化关键的检查点能力，即准确提炼任务关键信息的能力。


To further enable the model to discover optimal checkpointing strategies without additional trajectory data, we introduce a Multi-episode Agentic Reinforced Policy Optimization (Multi-episode ARPO) algorithm. This approach not only refines the agent's checkpointing policy but also significantly improves the overall agentic RL dynamics. We validate CORAL's effectiveness on the GAIA benchmark, where it substantially outperforms existing LLM agent methods on long-horizon tasks (Levels 2 and 3). Analysis of the action-level attention distribution reveals the source of this success: CORAL enables the agent to efficiently allocate its cognitive resources throughout the entire reasoning process.
为了在不增加轨迹数据的前提下让模型发现最优的检查点策略，我们引入了多回合智能体强化策略优化（Multi-episode Agentic Reinforced Policy Optimization，Multi-episode ARPO）算法。该方法不仅细化了智能体的检查点策略，还显著改善了整体的智能体强化学习动态。我们在 GAIA 基准上验证了 CORAL 的有效性，CORAL 在长时程任务（Level 2 与 Level 3）上大幅优于现有的 LLM 智能体方法。对动作级注意力分布的分析揭示了这一成功的根源：CORAL 使智能体能够在整个推理过程中高效分配认知资源。


In summary, the key contributions of this work are as follows:
总之，本工作的主要贡献如下：


- We introduce COgnitive Resource Self-ALlocation (CORAL), a framework that empowers agents to manage their working memory through a callable toolset. By dynamically optimizing its own context, an agent using CORAL can maintain robust planning and reasoning capabilities on long-horizon tasks.
- 我们提出了认知资源自我分配（CORAL）框架，赋能智能体通过可调用工具集管理其工作记忆。通过动态优化自身上下文，采用 CORAL 的智能体能够在长时程任务中保持稳健的规划与推理能力。


- We use Supervised Fine-Tuning (SFT) to instill core checkpointing skills and then leverage a multi-episode agentic reinforced policy optimization (Multi-episode ARPO) algorithm to allow the agent to discover optimal checkpointing strategies.
- 我们先通过监督微调（SFT）灌输核心的检查点技能，然后利用多回合智能体强化策略优化（Multi-episode ARPO）算法让智能体发现最优的检查点策略。


- On the GAIA benchmark, CORAL significantly outperforms existing approaches on complex long-horizon tasks (Level-2 and Level-3). An analysis of action-level attention distributions confirms that CORAL's success stems from its ability to effectively allocate the agent's cognitive resources during reasoning.
- 在 GAIA 基准上，CORAL 在复杂的长时程任务（Level-2 与 Level-3）上显著优于现有方法。对动作级注意力分布的分析证实，CORAL 的成功源于其在推理过程中有效分配智能体认知资源的能力。


## 2 PRELIMINARIES
## 2 预备知识


### 2.1 PROBLEM FORMULATION
### 2.1 问题表述


We consider a general large language model (LLM)-based agent. Upon receiving a problem specification, the agent is capable of interacting with its environment and executing a sequence of reasoning and action steps to progressively derive a solution. Following the ReAct (Yao et al. 2022) framework, these steps can be formalized as iterations of Thought-Action-Observation. Specifically,given a question $q \in  p\left( Q\right)$ ,the LLM agent ${\pi }_{\theta }$ at time step $t$ generates Thought ${r}_{t} \sim  {\pi }_{\theta }\left( {\cdot  \mid  {c}_{t}}\right)$ and a textual Action ${a}_{t} \sim  {\pi }_{\theta }\left( {\cdot  \mid  {c}_{t},{r}_{t}}\right)$ . The ${c}_{t}$ denotes the context in the time step $t : {c}_{t} = \left( {q,{r}_{1},{a}_{1},{o}_{1},\ldots ,{r}_{t - 1},{a}_{t - 1},{o}_{t - 1}}\right)$ . Then the environment gives the feedback as the Observation ${o}_{t}$ . The loop ends when the agent solves the question or reaches the max steps. Therefore, the final episode with $M$ steps can be defined as:
我们考虑一种通用的大型语言模型（LLM）驱动的智能体。接到问题描述后，智能体能够与环境交互并执行一系列推理与动作步骤，逐步导出解答。按照 ReAct（Yao et al. 2022）框架，这些步骤可形式化为思考-动作-观察的迭代。具体地，给定问题 $q \in  p\left( Q\right)$，在时间步 $t$ 时 LLM 智能体 ${\pi }_{\theta }$ 生成思考 ${r}_{t} \sim  {\pi }_{\theta }\left( {\cdot  \mid  {c}_{t}}\right)$ 与文本动作 ${a}_{t} \sim  {\pi }_{\theta }\left( {\cdot  \mid  {c}_{t},{r}_{t}}\right)$。${c}_{t}$ 表示时间步 $t : {c}_{t} = \left( {q,{r}_{1},{a}_{1},{o}_{1},\ldots ,{r}_{t - 1},{a}_{t - 1},{o}_{t - 1}}\right)$ 的上下文。随后环境给予反馈作为观察 ${o}_{t}$。当智能体解决问题或达到最大步数时循环结束。因此，具有 $M$ 步的最终回合可定义为：


$$
{e}_{\text{ terminated }} = \left( {q,{r}_{1},{a}_{1},{o}_{1},\ldots ,{r}_{M},{a}_{M},{o}_{M}}\right) \tag{1}
$$



$$
{e}_{\text{ completed }} = \left( {q,{r}_{1},{a}_{1},{o}_{1},\ldots ,{r}_{M}}\right) \tag{2}
$$



Note that in the completed episode,the Thought in the final round $\left( {r}_{M}\right)$ contains the answer to the question, and the episode stops immediately.
注意在完成的回合中，最终回合的思考 $\left( {r}_{M}\right)$ 包含问题的答案，回合随即终止。


### 2.2 WEB SEARCH AGENTIC TOOL DESIGN
### 2.2 网络搜索型智能体工具设计


At each time step $t$ ,the LLM-based agent generates a textual Action ${a}_{t} \in  \mathcal{A}$ ,where $\mathcal{A}$ denotes the predefined action space. In this work, we focus on an LLM-based tool-use agent, in which the action space $\mathcal{A}$ comprises a set of specialized tool-use commands and interaction primitives that the agent can execute to accomplish complex tasks. To operationalize this action space, we design two purpose-built tools that collectively support web search and webpage browse. These tools are described as follows:
在每个时间步 $t$ ，基于大模型的代理会生成一个文本动作 ${a}_{t} \in  \mathcal{A}$ ，其中 $\mathcal{A}$ 表示预定义的动作空间。在本工作中，我们关注基于大模型的工具使用代理，其动作空间 $\mathcal{A}$ 包含一组专门的工具使用命令和交互原语，代理可执行这些命令以完成复杂任务。为使该动作空间可操作化，我们设计了两种专用工具，共同支持网页搜索与浏览。以下对这些工具进行描述：


- Web Search. Enables the agent to issue multiple search queries in parallel via a search engine, retrieve and format the results, and present them in a structured manner.
- 网络搜索。使代理能够通过搜索引擎并行发出多个搜索查询，检索并格式化结果，并以结构化方式呈现。


- Web Browse. Allows the agent to intelligently retrieve and analyze content from specified web pages according to a user-defined goal, extract relevant information, summarize key findings, and identify useful external links for further exploration.
- 网页浏览。允许代理根据用户定义的目标智能检索并分析指定网页内容，提取相关信息，概述关键发现，并识别可供进一步探索的有用外部链接。


## 3 COGNITIVE RESOURCE SELF-ALLOCATION (CORAL)
## 3 认知资源自我分配（CORAL）


Inspired from cognitive resource theory, which posits that effective problem-solving in humans relies on the strategic management of finite cognitive resources like attention and working memory, we draw a parallel to the operational challenges faced by LLM agents. The agent's context serves as its working memory. On long-horizon tasks, this "memory" becomes progressively cluttered with intermediate steps (thoughts, actions, and observations), leading to cognitive overload. To address this, we introduce COgnitive Resource Self-ALlocation (CORAL), a paradigm that empowers the agent to proactively manage its own cognitive load. CORAL allows the agent to mimic the human process of consolidating progress and refocusing attention by creating checkpoints and purging irrelevant context.
受认知资源理论启发，该理论认为人类有效解决问题依赖于对注意力和工作记忆等有限认知资源的战略性管理，我们将其类比于大模型代理面临的运行挑战。代理的上下文相当于其工作记忆。在长时程任务中，这种“记忆”会被中间步骤（思考、动作和观测）逐渐淤塞，导致认知过载。为此，我们提出认知资源自我分配（CORAL），一种使代理主动管理自身认知负荷的范式。CORAL 允许代理模拟人类通过创建检查点和清理无关上下文来整合进展并重新聚焦注意力的过程。


### 3.1 WORKING MEMORY MANAGEMENT TOOLSET
### 3.1 工作记忆管理工具集


We operationalize this paradigm through the following working memory management toolset:
我们通过以下工作记忆管理工具集将该范式具体化：


- Memory Management. Assists the agent in managing its working memory by adding or removing knowledge units, thereby retaining essential information across context resets while discarding outdated or irrelevant data to maintain clarity and task continuity.
- 记忆管理。帮助代理通过添加或移除知识单元来管理其工作记忆，从而在上下文重置时保留必要信息，同时丢弃过时或无关的数据以保持清晰和任务连续性。


- Context Optimization. Performs a hard reset of the conversational context to mitigate token bloat and sustain performance. It clears all conversational history except for essential components-such as working memory, system prompt, and the original user request-ensuring that critical information is preserved while resetting the token count and removing accumulated tool outputs.
- 上下文优化。执行对话上下文的硬重置以缓解 token 膨胀并维持性能。它清除除必要组成部分之外的所有对话历史——例如工作记忆、系统提示和原始用户请求——以确保关键信息被保留，同时重置 token 计数并移除累计的工具输出。


Specifically,in the time step $t$ ,the context is $\left( {q,{r}_{1},{a}_{1},{o}_{1},\ldots ,{r}_{t - 1},{a}_{t - 1},{o}_{t - 1}}\right)$ ,the LLM-based agent call the Context Optimization tool,i.e. ${a}_{t} = {a}_{CO}$ ,the tool response ${o}_{t}$ will be the next round’s context $c$ . Therefore,in the next round,the episode will begin like $\left( {c,{r}_{1},{a}_{1},{o}_{1},\ldots }\right)$ .
具体地，在时间步 $t$ ，上下文为 $\left( {q,{r}_{1},{a}_{1},{o}_{1},\ldots ,{r}_{t - 1},{a}_{t - 1},{o}_{t - 1}}\right)$ ，基于大模型的代理调用上下文优化工具，即 ${a}_{t} = {a}_{CO}$ ，工具响应 ${o}_{t}$ 将成为下一轮的上下文 $c$ 。因此，在下一轮，回合将以 $\left( {c,{r}_{1},{a}_{1},{o}_{1},\ldots }\right)$ 开始。


Multi-episode trajectory. The context optimization tool, as described, performs a hard reset of the conversational context. This reset operation effectively segments what would otherwise be a single continuous reasoning process into multiple shorter episodes, each starting with a refreshed context while retaining only essential information. To capture this behavior, we extend the single-episode formulation to a multi-episode trajectory. We assume that there are $N$ episodes in total,then the i-th episode (with ${M}_{i}$ iterations) can be formulated as:
多回合轨迹。正如所述，上下文优化工具对话上下文执行硬重置。该重置操作有效地将本来是单次连续推理过程的内容切分为多个较短的回合，每回合以刷新后的上下文开始但仅保留必要信息。为反映此行为，我们将单回合的表述扩展为多回合轨迹。我们假设总共有 $N$ 个回合，则第 i 个回合（具有 ${M}_{i}$ 次迭代）可以表述为：


$$
{e}_{i} = \left\{  \begin{array}{ll} \left( {{c}_{i},{r}_{i,1},{a}_{i,1},{o}_{i,1},\ldots ,{r}_{i,{M}_{i}},{a}_{i,{M}_{i}},{o}_{i,{M}_{i}}}\right) , & i < N \\  \left( {{c}_{i},{r}_{i,1},{a}_{i,1},{o}_{i,1},\ldots ,{r}_{i,{M}_{i}}}\right) , & i = N \end{array}\right. \tag{3}
$$



$$
\text{ where }{c}_{i} = \left\{  \begin{array}{ll} q, & i = 1 \\  {o}_{i - 1,{M}_{i - 1}}, & i > 1 \end{array}\right. \tag{4}
$$



Notice that in the final episode,the thought of the last iteration ${r}_{N,{M}_{N}}$ contains the answer,then the episode ends immediately. ${c}_{i}$ is the initial context of each episode. In the first episode,it is the question $q \in  p\left( Q\right)$ . While in the following episodes,it is the optimized context ${o}_{i - 1,{M}_{i - 1}}$ from the last Context Optimization tool. Then a complete trajectory with $N$ episodes can be defined as:
注意在最后一回合中，最后一次迭代的思考 ${r}_{N,{M}_{N}}$ 包含了答案，则该回合立即结束。${c}_{i}$ 是每回合的初始上下文。在第一回合，它是问题 $q \in  p\left( Q\right)$ 。而在随后各回合，它是来自上一次上下文优化工具的优化上下文 ${o}_{i - 1,{M}_{i - 1}}$ 。随后，一个包含 $N$ 个回合的完整轨迹可以定义为：


$$
\mathcal{T} = \left( {{e}_{1},{e}_{2},\ldots ,{e}_{N}}\right) \tag{5}
$$



<img src="https://cdn.noedgeai.com/bo_d4ng05f7aajc73frsb80_3.jpg?x=339&y=1606&w=1124&h=241&r=0"/>



Figure 2: Multi-episode DAPO. The reward of a multi-episode trajectory is computed using the last episode, which contains the answer. Then the reward is broadcasted to all previous episodes in the same trajectory.
图 2：多回合 DAPO。多回合轨迹的奖励使用包含答案的最后一回合来计算。然后该奖励广播到同一轨迹中的所有先前回合。


### 3.2 CORAL FRAMEWORK
### 3.2 CORAL 框架


We propose the COgnitive Resource Self-ALlocation (CORAL) framework, a novel reasoning paradigm designed to empower LLM agents to overcome cognitive overload in long-horizon tasks.
我们提出了认知资源自我分配（CORAL）框架，一种新颖的推理范式，旨在使大模型代理在长时程任务中克服认知过载。


The framework is built on the principle that agents should be able to proactively manage their own context, much like humans manage their working memory. As illustrated in Figure 1, we implement this capability by augmenting the traditional ReAct framework with a specialized working memory management toolset. This toolset, comprising two distinct tools, provides the agent with the explicit mechanisms needed to self-regulate its cognitive load. Specifically, the Memory Management tool enables the agent to consolidate its progress and focus on planning, while the Context Optimization tool acts as a reset mechanism, allowing it to strategically purge irrelevant information from its context.
该框架基于这样的原则：代理应能主动管理自身上下文，类似人类管理工作记忆。如图1所示，我们通过在传统 ReAct 框架上增添专门的工作记忆管理工具集来实现此能力。该工具集由两种不同工具组成，为代理提供自我调节认知负荷的明确机制。具体而言，内存管理工具使代理能够整合进展并专注于规划，而上下文优化工具则作为重置机制，允许其有策略地清除上下文中无关的信息。


### 3.3 FURTHER ENHANCEMENT METHODS
### 3.3 进一步增强方法


While the CORAL framework can be implemented in a prompting-only fashion, we explore dedicated training methods to further enhance its capabilities. Behavior Cloning. To endow the agent with basic function call ability, we apply behavior cloning through supervised fine-tuning (SFT) on curated, high-quality trajectories. From Equation 5 we know that a trajectory is consist of multiple context independent episodes, therefore, we split the trajectory into episodes, and fine-tune the model using batches of episodes. For each episode described in Equation 3 and Equation 4 we compute the loss using the following loss function:
尽管 CORAL 框架可以仅通过提示实现，我们仍探索专门的训练方法以进一步提升其能力。行为克隆。为了赋予代理基本的函数调用能力，我们在精心挑选的高质量轨迹上通过监督微调（SFT）进行行为克隆。由等式5可知，一条轨迹由多个相互独立的 episode 组成，因此我们将轨迹拆分成若干 episode，并以 episode 批次对模型进行微调。对于等式3 和等式4 中描述的每个 episode，我们使用以下损失函数计算损失：


$$
L =  - \frac{1}{\mathop{\sum }\limits_{{i = 1}}^{\left| e\right| }\mathbb{I}\left( {{x}_{i} \neq  o}\right) }\mathop{\sum }\limits_{{i = 1}}^{\left| e\right| }\mathbb{I}\left( {{x}_{i} \neq  o}\right)  \cdot  \log {\pi }_{\theta }\left( {{x}_{i} \mid  {x}_{ < i}}\right) \tag{6}
$$



where $\mathbb{I}\left( \cdot \right)$ is the indicator function. Here $\mathbb{I}\left( {{x}_{i} \neq  o}\right)$ masks out the loss from observation tokens, ensuring the loss is computed over the agent's own generated outputs, such as its reasoning steps (thoughts) and function calls (actions). By doing so, we only supervise the model on the behaviors it is expected to learn, rather than penalizing it for failing to predict external information from the environment.
其中 $\mathbb{I}\left( \cdot \right)$ 是指示函数。此处 $\mathbb{I}\left( {{x}_{i} \neq  o}\right)$ 屏蔽了来自观测标记的损失，确保损失仅在代理自身生成的输出上计算，例如其推理步骤（思考）和函数调用（动作）。通过如此做，我们仅监督模型应学会的行为，而不是因未能预测来自环境的外部信息而惩罚模型。


Multi-episode Agentic Reinforced Policy Optimization. The classic DAPO optimization objective in Agent Reinforcement Learning (Wu et al. 2025a):
多 episode 的 Agentic 强化策略优化。经典的 Agent 强化学习中的 DAPO 优化目标（Wu 等，2025a）：


$$
{\mathcal{J}}_{\text{ DAPO }}\left( \theta \right)  = {\mathbb{E}}_{\left( {q,a}\right)  \sim  \mathcal{D},{\left\{  {o}_{i}\right\}  }_{i = 1}^{G} \sim  {\pi }_{{\theta }_{\text{ old }}}\left( {\cdot  \mid  \text{ context }}\right) }
$$



$$
\left\lbrack  {\frac{1}{\mathop{\sum }\limits_{{i = 1}}^{G}\left| {o}_{i}\right| }\mathop{\sum }\limits_{{i = 1}}^{G}\mathop{\sum }\limits_{{t = 1}}^{\left| {o}_{i}\right| }\min \left( {{r}_{i,t}\left( \theta \right) {\widehat{A}}_{i,t},\operatorname{clip}\left( {{r}_{i,t}\left( \theta \right) ,1 - {\varepsilon }_{\text{ low }},1 + {\varepsilon }_{\text{ high }}}\right) {\widehat{A}}_{i,t}}\right) }\right\rbrack \tag{7}
$$



$$
\text{ s.t. }0 < \left| \left\{  {{o}_{i} \mid  \text{ is\_equivalent }\left( {a,{o}_{i}}\right) }\right\}  \right|  < G\text{ , }
$$



where
其中


$$
{r}_{i,t}\left( \theta \right)  = \frac{{\pi }_{\theta }\left( {{o}_{i,t} \mid  q,{o}_{i, < t}}\right) }{{\pi }_{{\theta }_{\text{ old }}}\left( {{o}_{i,t} \mid  q,{o}_{i, < t}}\right) },\;{\widehat{A}}_{i,t} = \frac{{R}_{i} - \operatorname{mean}\left( {\left\{  {R}_{i}\right\}  }_{i = 1}^{G}\right) }{\operatorname{std}\left( {\left\{  {R}_{i}\right\}  }_{i = 1}^{G}\right) }. \tag{8}
$$



Noted that agentic execution ${o}_{i}$ refers solely to the tokens generated by models,excluding any tool responses. It means the optimization is applied only to the model-generated tokens.
注意，agentic 执行中的 ${o}_{i}$ 仅指模型生成的标记，不包括任何工具响应。这意味着优化仅作用于模型生成的标记。


In this work, the trajectory consists of multiple episodes, as mentioned in Equation 5 We further extend the Agentic DAPO algorithm to handle multi-episode trajectories by treating each episode as a separate optimization unit while maintaining trajectory-level coherence. Figure 2 illustrates our main idea. For a multi-episode trajectory ${\mathcal{T}}_{i} = \left( {{e}_{i}^{1},{e}_{i}^{2},\ldots ,{e}_{i}^{N}}\right)$ ,we use the last episode to compute the reward. And all previous episodes in the same trajectory share this reward: ${R}_{i}^{j} = {R}_{i}^{{n}_{i}}$ for $1 \leq  j < {n}_{i}$ . Then all episodes participate in the group computation to get an advantage.
在本工作中，轨迹由多个 episode 组成，如等式5 所述。我们进一步将 Agentic DAPO 算法扩展为处理多 episode 轨迹，将每个 episode 视为独立的优化单元，同时保持轨迹级一致性。图2 说明了我们的主要思路。对于多 episode 轨迹 ${\mathcal{T}}_{i} = \left( {{e}_{i}^{1},{e}_{i}^{2},\ldots ,{e}_{i}^{N}}\right)$ ，我们使用最后一个 episode 来计算奖励。同一轨迹中的所有先前 episode 共享该奖励：${R}_{i}^{j} = {R}_{i}^{{n}_{i}}$ 对于 $1 \leq  j < {n}_{i}$ 。然后所有 episode 参与组内计算以获得优势值。


Reward Design. We design a simple reward function that consist of format reward ${R}_{i}^{\text{ format }}$ and answer reward ${R}_{i}^{\text{ answer }}$ . The format reward verifies whether the whole trajectory follows the predefined format, and all the tool call in the json format is valid. The answer reward uses a LLM as a judge to determine whether the final answer is correct.
奖励设计。我们设计了一个简单的奖励函数，由格式奖励 ${R}_{i}^{\text{ format }}$ 和答案奖励 ${R}_{i}^{\text{ answer }}$ 组成。格式奖励用于验证整个轨迹是否遵循预定格式，以及所有以 JSON 格式的工具调用是否合法。答案奖励使用一个大模型作为裁判来判定最终答案是否正确。


$$
{R}_{i} = {R}_{i}^{\text{ format }} \times  {R}_{i}^{\text{ answer }} \tag{9}
$$



Table 1: Main results on GAIA. We boldface the best performance and underline the second best performance. Models with size 7 or 8B and models larger than 32B are marked separately. "-" means results that are not reported.
表1：GAIA 上的主要结果。我们将最佳表现加粗，第二佳表现下划线。7B 或 8B 大小的模型与大于 32B 的模型分别标注。“-”表示未报告的结果。


<table><tr><td>Model</td><td>Level 1</td><td>Level 2</td><td>Level 3</td><td>Average</td></tr><tr><td colspan="5">DIRECT INFERENCE</td></tr><tr><td>GPT-40</td><td>23.1</td><td>15.4</td><td>8.3</td><td>17.5</td></tr><tr><td>DeepSeek-R1</td><td>43.6</td><td>26.9</td><td>8.3</td><td>31.1</td></tr><tr><td>Claude-4.0-Sonnet</td><td>38.5</td><td>36.5</td><td>8.3</td><td>34.0</td></tr><tr><td colspan="5">AGENTIC INFERENCE</td></tr><tr><td>R1-Searcher-7B</td><td>28.2</td><td>19.2</td><td>8.3</td><td>20.4</td></tr><tr><td>WebDancer-7B</td><td>41.0</td><td>30.7</td><td>0.0</td><td>31.0</td></tr><tr><td>WebSailor-7B</td><td>-</td><td>-</td><td>-</td><td>37.9</td></tr><tr><td>CK-Pro-8B</td><td>56.4</td><td>42.3</td><td>8.3</td><td>43.7</td></tr><tr><td>WebDancer-32B</td><td>46.1</td><td>44.2</td><td>8.3</td><td>40.7</td></tr><tr><td>WebThinker-32B-RL</td><td>56.4</td><td>50.0</td><td>16.7</td><td>48.5</td></tr><tr><td>WebSailor-72B</td><td>-</td><td>-</td><td>-</td><td>55.4</td></tr><tr><td>WebShaper-72B</td><td>-</td><td>-</td><td>-</td><td>60.1</td></tr><tr><td>OpenAI DR</td><td>74.3</td><td>69.1</td><td>47.6</td><td>67.4</td></tr><tr><td colspan="5">CONTEXT OPTIM</td></tr><tr><td>ReAct A</td><td>-</td><td>-</td><td>-</td><td>60.0</td></tr><tr><td>+HARD OPTIM</td><td>-</td><td>-</td><td>-</td><td>66.0</td></tr><tr><td>ReAct 六</td><td>33.3</td><td>11.5</td><td>8.3</td><td>19.4</td></tr><tr><td>+HARD OPTIM</td><td>28.2</td><td>19.2</td><td>0.0</td><td>20.4</td></tr><tr><td>+SFT</td><td>41.0</td><td>40.4</td><td>11.1</td><td>37.2</td></tr><tr><td>+RL</td><td>41.0</td><td>44.2</td><td>25.0</td><td>40.9</td></tr></table>
<table><tbody><tr><td>模型</td><td>一级</td><td>二级</td><td>三级</td><td>平均</td></tr><tr><td colspan="5">直接推断</td></tr><tr><td>GPT-40</td><td>23.1</td><td>15.4</td><td>8.3</td><td>17.5</td></tr><tr><td>DeepSeek-R1</td><td>43.6</td><td>26.9</td><td>8.3</td><td>31.1</td></tr><tr><td>Claude-4.0-Sonnet</td><td>38.5</td><td>36.5</td><td>8.3</td><td>34.0</td></tr><tr><td colspan="5">代理推断</td></tr><tr><td>R1-Searcher-7B</td><td>28.2</td><td>19.2</td><td>8.3</td><td>20.4</td></tr><tr><td>WebDancer-7B</td><td>41.0</td><td>30.7</td><td>0.0</td><td>31.0</td></tr><tr><td>WebSailor-7B</td><td>-</td><td>-</td><td>-</td><td>37.9</td></tr><tr><td>CK-Pro-8B</td><td>56.4</td><td>42.3</td><td>8.3</td><td>43.7</td></tr><tr><td>WebDancer-32B</td><td>46.1</td><td>44.2</td><td>8.3</td><td>40.7</td></tr><tr><td>WebThinker-32B-RL</td><td>56.4</td><td>50.0</td><td>16.7</td><td>48.5</td></tr><tr><td>WebSailor-72B</td><td>-</td><td>-</td><td>-</td><td>55.4</td></tr><tr><td>WebShaper-72B</td><td>-</td><td>-</td><td>-</td><td>60.1</td></tr><tr><td>OpenAI DR</td><td>74.3</td><td>69.1</td><td>47.6</td><td>67.4</td></tr><tr><td colspan="5">上下文优化</td></tr><tr><td>ReAct A</td><td>-</td><td>-</td><td>-</td><td>60.0</td></tr><tr><td>+硬优化</td><td>-</td><td>-</td><td>-</td><td>66.0</td></tr><tr><td>ReAct 六</td><td>33.3</td><td>11.5</td><td>8.3</td><td>19.4</td></tr><tr><td>+硬优化</td><td>28.2</td><td>19.2</td><td>0.0</td><td>20.4</td></tr><tr><td>+SFT</td><td>41.0</td><td>40.4</td><td>11.1</td><td>37.2</td></tr><tr><td>+RL</td><td>41.0</td><td>44.2</td><td>25.0</td><td>40.9</td></tr></tbody></table>


## 4 EXPERIMENTS
## 4 实验


### 4.1 EXPERIMENTAL SETTINGS
### 4.1 实验设置


Baselines. We compare our method with against three representative paradigms.
基线。我们将方法与三类代表性范式进行比较。


- Direct Inference: GPT-4.1 (OpenAI, 2025a), DeepSeek-R1 (Guo et al., 2025), Claude-4.0- Sonnet
- 直接推理：GPT-4.1 (OpenAI, 2025a), DeepSeek-R1 (Guo et al., 2025), Claude-4.0- Sonnet


- Agentic Inference: R1-Searcher (Song et al., 2025), WebDancer (Wu et al., 2025a), Web-Thinker (Li et al. 2025a), WebSailor, WebShaper, OpenAI Deep research (OpenAI, 2025b)
- 代理式推理：R1-Searcher (Song et al., 2025), WebDancer (Wu et al., 2025a), Web-Thinker (Li et al. 2025a), WebSailor, WebShaper, OpenAI Deep research (OpenAI, 2025b)


- ReAct. Classic ReAct diagram using web search and web browse tools.
- ReAct。经典 ReAct 流程，使用网页搜索与浏览工具。


Benchmarks. We use GAIA (Mialon et al. 2023) as the evaluation benchmark. We follow existing works by using the 103-sample text-only validation subset. Questions are categorized into three difficulty levels, with Level 3 representing the most challenging long-horizon tasks requiring extensive reasoning chains.
基准。我们使用 GAIA (Mialon et al. 2023) 作为评估基准。遵循既有工作，采用 103 条仅文本的验证子集。问题按三档难度分类，Level 3 表示最具挑战性、需长程推理链的任务。


Dataset. We follow the data construction pipeline of WebShaper (Tao et al., 2025) to construct high quality questions with controllable difficulty. We use commercial models to synthesize the interaction trajectories. Ultimately, this process yielded a dataset of 1115 trajectories, of which approximately 55% successfully lead to the correct answer.
数据集。我们沿用 WebShaper (Tao et al., 2025) 的数据构建流程以生成可控难度的高质量问题，使用商业模型合成交互轨迹。最终得到 1115 条轨迹，其中约 55% 成功得到正确答案。


### 4.2 OVERALL PERFORMANCE
### 4.2 整体性能


Table 1 resents our main experimental results. CORAL demonstrates substantial improvements over existing methods, particularly excelling on the most challenging long-horizon tasks (Level 2 and Level 3) that require extended reasoning chains. When applied to a powerful proprietary model (Claude-4-Sonnet), our prompting-only CORAL achieves an average score of 66.0, comparable with OpenAI DR (67.4). This highlights the effectiveness of our approach even when enhancing already capable models.
表 1 给出主要实验结果。CORAL 在现有方法上展现显著提升，尤其在最具挑战性的长程任务（Level 2 和 Level 3）上表现突出。当应用于强大的专有模型（Claude-4-Sonnet）时，仅通过提示的 CORAL 达到 66.0 的平均分，接近 OpenAI DR 的 67.4，表明该方法即便用于已有高性能模型仍能获得效果提升。


However, when applied to Qwen3-8B, the prompting-only CORAL shows only marginal improvements. This can be attributed to a discrepancy between the sophistication of CORAL's working memory management tools and the limited agentic capabilities of the base model. The model frequently fails to adhere to the required format or makes errors during tool calling, which negates the potential benefits of the framework.
然而，当应用于 Qwen3-8B 时，仅提示的 CORAL 仅带来有限改进。这主要归因于 CORAL 的工作记忆管理工具与基础模型的代理能力不匹配。该模型常不能遵守所需格式或在调用工具时出错，从而抵消了框架的潜在收益。


This phenomenon can be mitigated through behavior cloning, i.e., by performing Supervised Fine-Tuning (SFT) on high-quality trajectories. Remarkably, our experiments demonstrate that SFT on a small dataset of just 1115 trajectories is sufficient for the model to master this operational pattern and achieve superior performance on GAIA. This is achieved even though ${45}\%$ of the trajectories in the training data culminate in an incorrect final answer, suggesting the model is effectively learning the reasoning process itself. The subsequent application of Reinforcement Learning (RL) further enhances performance, with the advantages being most pronounced on long-horizon tasks (Level 2 and Level 3).
该现象可通过行为克隆缓解，即对高质量轨迹进行监督微调（SFT）。值得注意的是，我们的实验表明，仅用 1115 条轨迹的小数据集进行 SFT 即足以让模型掌握该操作模式并在 GAIA 上取得优异表现。尽管训练数据中有 ${45}\%$ 条轨迹最终给出错误答案，模型仍能有效学习推理过程本身。随后应用的强化学习（RL）进一步提升了性能，且在长程任务（Level 2 和 Level 3）上的增益最为明显。


### 4.3 ABLATION STUDY
### 4.3 消融研究


Does the trajectory with wrong answer degrade model's performance? To investigate this question, we fine-tuned a base model exclusively on trajectories from our dataset that resulted in correct answers. This model achieved a score of 31.1% on the GAIA text-only subset, a result substantially lower than that of our model fine-tuned on the complete dataset (which includes both correct and incorrect trajectories).
错误答案的轨迹会降低模型性能吗？为探究此问题，我们仅用数据集中导致正确答案的轨迹对基础模型进行微调。该模型在 GAIA 仅文本子集上取得了 31.1% 的得分，远低于用包含正确与错误轨迹的完整数据集微调的模型。


This finding indicates that including trajectories with incorrect answers is not only harmless but is, in fact, beneficial. This aligns with our hypothesis that the primary goal of Supervised Fine-Tuning (SFT) is to "clone behavior", where the value gained from learning a high-quality reasoning process outweighs the negative signal of an incorrect final answer. Therefore, even high-quality reasoning paths that conclude with an incorrect answer can positively contribute to the model's overall reasoning capabilities. However, whether this conclusion remains valid when the dataset is scaled up significantly requires further investigation.
这一发现表明，包含错误答案的轨迹不仅无害，反而有益。这与我们的假设一致：监督微调的主要目标是“克隆行为”，从高质量推理过程中获得的价值超过了错误最终答案带来的负面信号。因此，即便以错误答案结尾的高质量推理路径也能积极提升模型的整体推理能力。然而，当数据集显著扩增时该结论是否仍然成立尚需进一步研究。


### 4.4 ATTENTION ANALYSIS
### 4.4 注意力分析


CORAL has shown significant improvements over baseline methods, particularly on challenging long-horizon tasks requiring extended reasoning chains.
CORAL 在基线方法上表现出显著提升，尤以需要长时间推理链的困难长程任务为最。


In this section, we move beyond a macro-level evaluation of the CORAL framework's performance to a micro-level analysis of the underlying mechanisms driving its success. The central hypothesis is that the CORAL framework implicitly facilitates a more efficient and effective reallocation of model's cognitive resources. This analysis uses attention mechanisms as a lens to investigate how the model learns to prioritize critical information and steer its problem-solving trajectory.
本节从宏观评估转向对促成 CORAL 成功的底层机制进行微观分析。核心假设是 CORAL 隐式促成了模型认知资源的更高效、更有效的再分配。本分析以注意力机制为切入点，研究模型如何学会优先处理关键信息并引导其解题轨迹。


Receiver heads. Previous work (Bogdan et al. 2025) in attention analysis has identified "important sentences" that receive heightened attention from downstream sentences, a phenomenon known as attention aggregation. Inspired by this, we also try to find important parts in the context that might get higher attention values and thus have a greater impact on the model's behavior. In our multiturn conversation setting, we shift the unit of analysis from tokens or sentences to messages, aiming to discover which messages are more important. Following (Bogdan et al., 2025), we refer to attention heads that narrow attention toward specific messages as "receiver heads". We first identify the receiver heads (details in Appendix A.2), then analysis the attention distribution through these heads.
接收头。先前关于注意力分析的工作（Bogdan et al. 2025）识别出被下游句子高度关注的“重要句子”，即注意力聚合现象。受此启发，我们尝试在上下文中找出可能获得更高注意力值、从而对模型行为产生更大影响的重要部分。在我们的多轮对话设置中，我们将分析单元从 token 或句子转为消息，旨在发现哪些消息更重要。参照（Bogdan et al., 2025），我们将将注意力收敛到特定消息的注意力头称为“接收头”。我们先识别接收头（详见附录 A.2），然后通过这些头分析注意力分布。


Case study: Sharpening attention on checkpoints. In Figure 3 we show a case of message-level attention from the base model and fine-tuned model. The attention map clearly shows that, after fine-tuning, the model pays more attention to previous checkpoint (memory management tool response) when calling memory management tool, while other part of the context show a relatively lower attention value. We also find that,
案例研究：在检查点处聚焦注意力。图 3 展示了基础模型与微调模型的消息级注意力示例。注意力图清楚表明，微调后模型在调用内存管理工具时对先前检查点（内存管理工具响应）给予了更多关注，而上下文的其他部分则显示出相对较低的注意力值。我们还发现，


<img src="https://cdn.noedgeai.com/bo_d4ng05f7aajc73frsb80_7.jpg?x=315&y=329&w=1176&h=525&r=0"/>



Figure 3: Comparison of attention at a checkpoint between the base model and the fine-tuned model in CORAL diagram. Left: Message-level attention map from the Qwen3-8B base model. Right: Message-level attention map from our fine-tuned Qwen3-8B. Red box: Attention corresponding to two consecutive memory management tool calls.
图 3：CORAL 示意图中基础模型与微调模型在检查点处注意力的比较。左：Qwen3-8B 基础模型的消息级注意力图。右：我们微调后的 Qwen3-8B 的消息级注意力图。红框：对应两次连续内存管理工具调用的注意力。


## 5 RELATED WORK
## 5 相关工作


Reinforcement learning for LLM agents. Reinforcement learning (RL) is a crucial methodology for empowering Large Language Model (LLM) agents to operate effectively within dynamic and open-ended environments. Compared to supervised fine-tuning which relies on pre-collected expert data, RL-based methods allow agents to learn directly from their interactions with an environment. The application of RL to LLM agents has evolved significantly over time. Initial efforts utilized classical algorithms like DQN for training agents in text-based games (Narasimhan et al. 2015). Subsequently, more advanced value-based methods, such as PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024a), were employed in a broader array of interactive settings, including embodied AI tasks like ALFWorld (Shridhar et al. 2021), information seeking tasks (Mialon et al. 2023; Wei et al. 2025; Zhou et al. 2025a; Xbench-Team 2025), and strategic card games (Brock-man et al. 2016).
面向 LLM 代理的强化学习。强化学习（RL）是赋能大型语言模型（LLM）代理在动态且开放环境中有效运行的关键方法。与依赖预先收集的专家数据的监督微调不同，基于 RL 的方法允许代理直接从与环境的交互中学习。将 RL 应用于 LLM 代理的研究随着时间显著发展。早期工作在基于文本的游戏中采用经典算法如 DQN 训练代理（Narasimhan et al. 2015）。随后，更先进的基于价值的方法，如 PPO（Schulman et al., 2017）和 GRPO（Shao et al., 2024a），被用于更广泛的交互场景，包括具身 AI 任务如 ALFWorld（Shridhar et al. 2021）、信息检索任务（Mialon et al. 2023; Wei et al. 2025; Zhou et al. 2025a; Xbench-Team 2025）以及策略卡牌游戏（Brock-man et al. 2016）。


Context Engineering in LLM Agents. Managing context effectively is a critical challenge in developing LLM-based agents, particularly as these systems become more sophisticated and operate over extended interactions. Recent research has explored various approaches to address the limitations of context windows and maintain relevant information throughout agent execution. One prominent approach involves breaking down complex tasks into smaller, manageable subtasks to better utilize limited context windows (Luo et al., 2025; Schroeder et al., 2024). Another line of research focuses on employ context compression after each function call (Zhou et al. 2025b). While this approach can effectively manage context size, it may suffer from information loss and difficulties in maintaining high-level planning coherence across extended agent interactions. Some systems have begun to incrementally read context by splitting it into chunks (Yu et al. 2025a). However, they have only considered scenarios with fixed contexts, while dynamic contexts involving function calling remain unexplored.
LLM 代理中的上下文工程。有效管理上下文是开发基于 LLM 的代理时的一项关键挑战，尤其当这些系统变得更复杂并在更长的交互中运行时。近期研究探索了多种方法以应对上下文窗口的限制并在代理执行过程中保持相关信息。一种重要方法是将复杂任务拆解为更小的子任务，以更好地利用有限的上下文窗口（Luo et al., 2025; Schroeder et al., 2024）。另一条研究线关注在每次函数调用后进行上下文压缩（Zhou et al. 2025b）。该方法虽能有效管理上下文大小，但可能导致信息丢失并难以在长时间交互中维持高层规划一致性。一些系统开始通过将上下文分块来增量读取（Yu et al. 2025a）。然而，它们仅考虑了固定上下文的场景，而涉及函数调用的动态上下文尚未被充分探讨。


## 6 CONCLUSION
## 6 结论


In conclusion, we address the critical challenge of contextual bloat in LLM-driven agents, where the accumulation of environmental feedback and intermediate reasoning steps degrades performance on long-horizon tasks. We introduce the COgnitive Resource Self-ALlocation (CORAL) framework, a novel paradigm that empowers agents with a callable toolset to actively manage their own working memory. By learning to create checkpoints and strategically reset its context, an agent equipped with CORAL can mitigate cognitive overload and sustain high-level reasoning throughout a task. Our two-stage training approach, which combines Supervised Fine-Tuning to instill core skills with a novel Multi-episode Agentic Reinforced Policy Optimization (Multi-episode ARPO) algorithm, enables the agent to discover effective, adaptive memory management policies. On the challenging Level 2 and Level 3 tasks of the GAIA benchmark, CORAL substantially outperforms existing methods. Our analysis of action-level attention distributions confirms that this performance gain is directly attributable to the agent's improved ability to allocate its cognitive resources, focusing on salient information while discarding obsolete context.
总之，我们解决了由大模型驱动的智能体中情境膨胀的关键问题：环境反馈和中间推理步骤的积累会在长时程任务中降低性能。我们提出了认知资源自我分配（COgnitive Resource Self-ALlocation，CORAL）框架，这一新范式赋予智能体一组可调用工具以主动管理其工作记忆。通过学习创建检查点并有策略地重置上下文，配备 CORAL 的智能体能够缓解认知过载并在整个任务中维持高阶推理。我们采用两阶段训练方法：先用监督微调灌输核心技能，再用一种新颖的多回合主体性强化策略优化（Multi-episode Agentic Reinforced Policy Optimization，Multi-episode ARPO）算法，使智能体发现有效且自适应的记忆管理策略。在 GAIA 基准的挑战性 Level 2 和 Level 3 任务上，CORAL 显著优于现有方法。我们对动作级注意力分布的分析证实，这一性能提升直接归因于智能体改进的认知资源分配能力：聚焦关键信息并丢弃过时上下文。


## REFERENCES
## 参考文献


Paul C. Bogdan, Uzay Macar, Neel Nanda, and Arthur Conmy. Thought anchors: Which llm reasoning steps matter? arXiv preprint arXiv:2506.19143, 2025.
Paul C. Bogdan, Uzay Macar, Neel Nanda, and Arthur Conmy. Thought anchors: Which llm reasoning steps matter? arXiv preprint arXiv:2506.19143, 2025.


Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.


Prahaladh Chandrahasan, Jiahe Jin, Zhihan Zhang, Tevin Wang, Andy Tang, Lucy Mo, Morteza Ziyadi, Leonardo FR Ribeiro, Zimeng Qiu, Markus Dreyer, et al. Deep research comparator: A platform for fine-grained human annotations of deep research agents. arXiv preprint arXiv:2507.05495, 2025.
Prahaladh Chandrahasan, Jiahe Jin, Zhihan Zhang, Tevin Wang, Andy Tang, Lucy Mo, Morteza Ziyadi, Leonardo FR Ribeiro, Zimeng Qiu, Markus Dreyer, et al. Deep research comparator: A platform for fine-grained human annotations of deep research agents. arXiv preprint arXiv:2507.05495, 2025.


Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anu-manchipalli, Kurt Keutzer, and Amir Gholami. Plan-and-act: Improving planning of agents for long-horizon tasks. arXiv preprint arXiv:2503.09572, 2025.
Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anu-manchipalli, Kurt Keutzer, and Amir Gholami. Plan-and-act: Improving planning of agents for long-horizon tasks. arXiv preprint arXiv:2503.09572, 2025.


Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025.
Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025.


Gemini. Gemini deep research. https://gemini.google/overview/deep-research.2025.
Gemini. Gemini deep research. https://gemini.google/overview/deep-research.2025.


Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.


Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey. arXiv preprint arXiv:2402.02716, 2024.
Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey. arXiv preprint arXiv:2402.02716, 2024.


Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025.
Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025.


Kimi. Kimi-researcher: End-to-end rl training for emerging agentic capabilities. https: //moonshotai.github.io/Kimi-Researcher/ 2025.
Kimi. Kimi-researcher: End-to-end rl training for emerging agentic capabilities. https: //moonshotai.github.io/Kimi-Researcher/ 2025.


Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025a.
Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025a.


Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025b.
Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025b.


Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, et al. Scalecua: Scaling open-source computer use agents with cross-platform data. arXiv preprint arXiv:2509.15221, 2025.
Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, et al. Scalecua: Scaling open-source computer use agents with cross-platform data. arXiv preprint arXiv:2509.15221, 2025.


Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack O'Brien, and James Glass. Beyond context limits: Subconscious threads for long-horizon reasoning. arXiv preprint arXiv:2507.16784, 2025.
Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack O'Brien, and James Glass. Beyond context limits: Subconscious threads for long-horizon reasoning. arXiv preprint arXiv:2507.16784, 2025.


Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023.
Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023.


Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015.
Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015.


OpenAI. Introducing openai gpt-4.1, 2025a. URL https://openai.com/index/ gpt-4-1/
OpenAI. Introducing openai gpt-4.1, 2025a. URL https://openai.com/index/ gpt-4-1/


OpenAI. Deep research system card, 2025b. URL https://cdn.openai.com/ deep-research-system-card.pdf
OpenAI. Deep research system card, 2025b. URL https://cdn.openai.com/ deep-research-system-card.pdf


Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025.
Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025.


Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Agent planning with world knowledge model. Advances in Neural Information Processing Systems, 37:114843-114871, 2024.
Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Agent planning with world knowledge model. Advances in Neural Information Processing Systems, 37:114843-114871, 2024.


Philip Schroeder, Nathaniel Morgan, Hongyin Luo, and James Glass. Thread: Thinking deeper with recursive spawning. arXiv preprint arXiv:2405.17402, 2024.
Philip Schroeder, Nathaniel Morgan, Hongyin Luo, and James Glass. Thread: Thinking deeper with recursive spawning. arXiv preprint arXiv:2405.17402, 2024.


John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.


Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024a.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024a.


Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024b.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024b.


Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:8634-8652, 2023.
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:8634-8652, 2023.


Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.
Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.


Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025.
Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025.


Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, et al. Webshaper: Agentically data synthesizing via information-seeking formalization. arXiv preprint arXiv:2507.15061, 2025.
郑伟陶, 吴佳龙, 尹文彪, 张俊凯, 李柏轩, 沈海洋, 李宽, 张立文, 王新宇, 蒋勇, 等. Webshaper: 通过寻求信息的形式化以代理方式合成数据. arXiv 预印本 arXiv:2507.15061, 2025.


Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, and Heng Ji. Otc: Optimal tool calls via reinforcement learning. arXiv e-prints, pp. arXiv-2504, 2025.
王鸿儒, 钱成, 钟万君, 陈秀思, 邱家豪, 黄世爵, 金博文, 王梦笛, 王锦辉, 纪恒. Otc: 通过强化学习实现最优工具调用. arXiv 电子稿, 页码 arXiv-2504, 2025.


Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: A simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025.
Jason Wei, 孙志庆, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, 和 Amelia Glaese. Browsecomp: 一个简单却具挑战性的浏览代理基准. arXiv 预印本 arXiv:2504.12516, 2025.


Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025a.
吴佳龙, 李柏轩, 方润南, 尹文彪, 张立文, 郑伟陶, 张定初, 席泽坤, 蒋勇, 谢鹏军, 等. Webdancer: 迈向自主信息寻求代理. arXiv 预印本 arXiv:2505.22648, 2025a.


Penghao Wu, Shengnan Ma, Bo Wang, Jiaheng Yu, Lewei Lu, and Ziwei Liu. Gui-reflection: Empowering multimodal gui models with self-reflection behavior. arXiv preprint arXiv:2506.08012, 2025b.
吴鹏浩, 马胜楠, 王博, 余家恒, 卢乐为, 刘子炜. Gui-reflection: 赋能多模态 GUI 模型以具备自我反思行为. arXiv 预印本 arXiv:2506.08012, 2025b.


Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, et al. Resum: Unlocking long-horizon search intelligence via context summarization. arXiv preprint arXiv:2509.13313, 2025c.
吴喜喜, 李宽, 赵一达, 张立文, 欧力图, 尹慧峰, 张中旺, 蒋勇, 谢鹏军, 黄飞, 等. Resum: 通过上下文摘要解锁长时域搜索智能. arXiv 预印本 arXiv:2509.13313, 2025c.


Xbench-Team. Xbench-deepsearch, 2025. URL https://xbench.org/agi/aisearch
Xbench-Team. Xbench-deepsearch, 2025. 网址 https://xbench.org/agi/aisearch


Yu Xia, Jingru Fan, Weize Chen, Siyu Yan, Xin Cong, Zhong Zhang, Yaxi Lu, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Agentrm: Enhancing agent generalization with reward modeling. arXiv preprint arXiv:2502.18407, 2025.
夏煜, 樊婧茹, 陈伟泽, 阎思宇, 丛鑫, 张中, 逯雅惜, 林彦凯, 刘志远, 孙茂松. Agentrm: 通过奖励建模增强代理泛化能力. arXiv 预印本 arXiv:2502.18407, 2025.


Minglai Yang, Ethan Huang, Liang Zhang, Mihai Surdeanu, William Wang, and Liangming Pan. How is llm reasoning distracted by irrelevant context? an analysis using a controlled benchmark. arXiv preprint arXiv:2505.18761, 2025.
杨明来, Ethan Huang, 张亮, Mihai Surdeanu, William Wang, 潘良明. 无关上下文如何干扰大模型推理？基于受控基准的分析. arXiv 预印本 arXiv:2505.18761, 2025.


Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.
姚顺宇, Jeffrey Zhao, 余典, 杜楠, Izhak Shafran, Karthik Narasimhan, 曹元. React: 在语言模型中协同推理与行动. arXiv 预印本 arXiv:2210.03629, 2022.


Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, Wei-Ying Ma, Jingjing Liu, Mingxuan Wang, et al. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259, 2025a.
余宏立, 陈庭鸿, 冯江涛, 陈江杰, 戴维楠, 于启英, 张亚勤, 马维英, 刘晶晶, 王明轩, 等. Memagent: 用基于多轮对话强化学习的记忆代理重塑长上下文大模型. arXiv 预印本 arXiv:2507.02259, 2025a.


Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025b.
于启英, 张征, 朱若飞, 袁玉峰, 左晓晨, 岳宇, 戴维楠, 樊甜甜, 刘高宏, 刘凌君, 等. Dapo: 一个规模化的开源大模型强化学习系统. arXiv 预印本 arXiv:2503.14476, 2025b.


Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and Guilin Liu. Nemotron-research-tool-n1: Tool-using language models with reinforced reasoning. arXiv preprint arXiv:2505.00024, 2025.
张少坤, 董毅, 张洁瑜, Jan Kautz, Bryan Catanzaro, Andrew Tao, 吴庆云, 余志定, 刘桂林. Nemotron-research-tool-n1: 使用增强推理的工具化语言模型. arXiv 预印本 arXiv:2505.00024, 2025.


Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314, 2025a.
周沛霖, Bruce Leon, 应翔, 张灿, 邵一帆, 叶祺晨, 冲大定, 金志岭, 谢陈轩, 曹猛, 等. Browsecomp-zh: 用中文评测大语言模型的网页浏览能力. arXiv 预印本 arXiv:2504.19314, 2025a.


Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents. arXiv preprint arXiv:2506.15841, 2025b.
Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents. arXiv preprint arXiv:2506.15841, 2025b.


## A APPENDIX
## A 附录


### A.1 The Use of LARGE LANGUAGE MODELS (LLMS)
### A.1 使用大型语言模型（LLMs）


We used Large Language Models (LLMs) to aid in polishing the language of this manuscript. Their role was confined to improving grammar, clarity, and sentence structure. The intellectual content, including all ideas and findings, is entirely the work of the human authors, who reviewed and approved the final text.
我们使用大型语言模型（LLMs）对本文稿的语言进行了润色，其作用仅限于改进语法、清晰度和句子结构。知识性内容，包括所有思想和发现，完全由人工作者负责并已由他们审阅和批准最终文本。


### A.2 THE IDENTIFICATION OF RECEIVE HEADS
### A.2 接收头的识别


Receive heads refers to the attention heads which consistently narrow attention toward specific messages. Following (Bogdan et al., 2025), we plot the vertical attention scores for each message by the 32 different heads in 36 different layers. From Figure 4. We find that in later layers (layer 35) shows a clear difference in attention values between different attention heads. In this case, the receive heads are head 9 and head 22 in layer 35.
“接收头”指那些持续将注意力集中到特定消息上的注意力头。遵循 (Bogdan et al., 2025) 的方法，我们绘制了 36 层中 32 个不同头对每条消息的垂直注意力得分。从图 4 我们发现在较后层（第 35 层）不同注意力头之间的注意力值差异明显。在这种情况下，第 35 层的第 9 号头和第 22 号头是接收头。


We take a look at these two head's message level attention map, find that these two attention really show a relatively high attention value (see Figure 5 and Figure 6). And narrow attention toward specific messages, such as the 6th message in head 22.
我们查看了这两个头的消息级注意力图，发现这两个注意力头确实表现出相对较高的注意力值（见图 5 和图 6），并将注意力聚焦于特定消息，例如第 22 号头的第 6 条消息。


<img src="https://cdn.noedgeai.com/bo_d4ng05f7aajc73frsb80_11.jpg?x=351&y=356&w=1098&h=255&r=0"/>



Figure 4: Vertical attention scores for each message by 32 different heads in layer 9, 21, 35 respectively. The backbone of the tested model is Qwen3-8B.
图 4：第 9、21、35 层中 32 个不同头对每条消息的垂直注意力得分。所测模型的骨干为 Qwen3-8B。


<img src="https://cdn.noedgeai.com/bo_d4ng05f7aajc73frsb80_11.jpg?x=294&y=970&w=1202&h=992&r=0"/>



Figure 5: Message level attention map for head 9 layer 35 and its neighbors.
图 5：第 35 层第 9 号头及其相邻头的消息级注意力图。


<img src="https://cdn.noedgeai.com/bo_d4ng05f7aajc73frsb80_12.jpg?x=306&y=662&w=1190&h=994&r=0"/>



Figure 6: Message level attention map for head 22 layer 35 and its neighbors.
图 6：第 35 层第 22 号头及其相邻头的消息级注意力图。