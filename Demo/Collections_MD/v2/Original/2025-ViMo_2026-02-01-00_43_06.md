# A Generative Visual GUI World Model for App Agents
# 一种面向应用智能体的生成式视觉 GUI 世界模型


Dezhao Luo ${}^{1, * }$ , Bohan Tang ${}^{2, * }$ , Kang Li ${}^{2}$ , Georgios Papoudakis ${}^{3}$ , Jifei Song ${}^{3}$ , Shaogang Gong ${}^{1}$ , Jianye Hao ${}^{3}$ , Jun Wang ${}^{4}$ , Kun Shao ${}^{3, \dagger  }$
罗德召 ${}^{1, * }$ , 唐博瀚 ${}^{2, * }$ , 李康 ${}^{2}$ , Georgios Papoudakis ${}^{3}$ , 宋继斐 ${}^{3}$ , 龚少刚 ${}^{1}$ , 郝建业 ${}^{3}$ , 汪军 ${}^{4}$ , 邵坤 ${}^{3, \dagger  }$


${}^{1}$ Queen Mary University of London, ${}^{2}$ University of Oxford, ${}^{3}$ Huawei Noah’s Ark Lab,
${}^{1}$ 伦敦玛丽女王大学, ${}^{2}$ 牛津大学, ${}^{3}$ 华为诺亚方舟实验室,


${}^{4}$ University College London
${}^{4}$ 伦敦大学学院


## Abstract
## 摘要


App agents, which autonomously operate mobile Apps through Graphical User Interfaces (GUIs), have gained significant interest in real-world applications. Yet, they often struggle with long-horizon planning, failing to find the optimal actions for complex tasks with longer steps. To address this, world models are used to predict the next GUI observation based on user actions, enabling more effective agent planning. However, existing world models primarily focus on generating only textual descriptions, lacking essential visual details. To fill this gap, we propose ViMo, the first Visual world Model designed to generate future App observations as images. For the challenge of generating text in image patches, where even minor pixel errors can distort readability, we decompose GUI generation into graphic and text content generation. We propose a novel data representation, the Symbolic Text Representation (STR), to overlay text content with symbolic placeholders while preserving graphics. With this design, ViMo employs a STR Predictor to predict future GUIs' graphics and a GUI-text Predictor for generating the corresponding text. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the outcome of different action options. Experiments show ViMo's ability to generate visually plausible and functionally effective GUIs that enable App agents to make more informed decisions. Project link: https://ai-agents-2030.github io/ViMo/
应用智能体通过图形用户界面（GUI）自主操作移动应用，在现实应用中引起了广泛关注。然而，它们在长程规划方面仍面临挑战，难以在步骤较多的复杂任务中找到最优动作。为此，世界模型被用于根据用户动作预测下一帧 GUI 观测，从而实现更有效的智能体规划。然而，现有的世界模型主要侧重于仅生成文本描述，缺乏必要的视觉细节。为了弥补这一空白，我们提出了 ViMo，这是首个旨在将未来应用观测生成为图像的视觉世界模型。针对图像块中生成文本的挑战（即使极小的像素误差也会导致可读性受损），我们将 GUI 生成分解为图形和文本内容的生成。我们提出了一种新颖的数据表示方式——符号文本表示（STR），在保留图形的同时用符号占位符覆盖文本内容。基于此设计，ViMo 采用 STR 预测器预测未来 GUI 的图形，并使用 GUI 文本预测器生成对应的文本。此外，我们部署 ViMo 通过预测不同动作选项的结果来增强智能体任务。实验表明，ViMo 能够生成视觉上合理且功能上有效的 GUI，使应用智能体能够做出更明智的决策。项目链接：https://ai-agents-2030.github.io/ViMo/


## 1 Introduction
## 1 引言


Recent advancements in Large Language Models (LLMs) have unlocked new possibilities for deploying AI agents across diverse fields [2, 3, 4]. A notable application is the smartphone application (App) agents [5, 6], designed to directly interact with Graphical User Interfaces (GUIs) to perform tasks autonomously and efficiently in a mobile operating system.
大语言模型（LLMs）的最新进展为在不同领域部署 AI 智能体开启了新的可能性 [2, 3, 4]。一个显著的应用是智能手机应用（App）智能体 [5, 6]，旨在直接与图形用户界面（GUIs）交互，以在移动操作系统中自主高效地执行任务。


However, existing agents struggle with making decisions for tasks requiring longer steps [7]. To address this "long-horizon" limitation, an increasing number of studies have introduced world models, which predict how GUIs evolve in response to user actions [8]. Yet, these models typically rely on language to describe future observations. These language-based descriptions often fail to capture the intricate visual details, such as the location and colour of GUI elements, necessary for a precise representation [7]. There is a need for a visual GUI world model capable of comprehensively representing observations in visual modality. Addressing this gap serves as the primary motivation for this work.
然而，现有智能体在处理需要较多步骤的任务决策时表现不佳 [7]。为了解决这一“长程”局限性，越来越多的研究引入了世界模型，用以预测 GUI 如何随用户动作而演变 [8]。然而，这些模型通常依赖语言来描述未来的观测结果。这些基于语言的描述往往无法捕捉到精确表示所需的复杂视觉细节，例如 GUI 元素的具体位置和颜色 [7]。因此，需要一种能够以视觉模态全面表示观测结果的视觉 GUI 世界模型。填补这一空白是本工作的主要动机。


To build a visual GUI world model capable of generating plausible future GUI observations that are visually consistent with user actions, a straightforward approach involves generating each pixel of a
为了构建一个能够生成与用户动作在视觉上保持一致且合理的未来 GUI 观测的视觉 GUI 世界模型，一种直接的方法是使用图像生成技术来生成 GUI 的每个像素 [11, 12]。


---



* Equal Contribution, † Corresponding author: shaokun2@huawei.com
* 等同贡献，† 通讯作者：shaokun2@huawei.com


${}^{1}$ By LLMs,we refer to the concept of foundation models that accept various input modalities (e.g.,visual language models (VLMs), multimodal LLMs (MLLMs)) while producing textual sequences [1].
${}^{1}$ 我们所指的 LLMs 是指能够接受多种输入模态（例如视觉语言模型 (VLMs)、多模态 LLMs (MLLMs)）并生成文本序列的基础模型概念 [1]。


---



<img src="https://cdn.noedgeai.com/bo_d5v247n7aajc73870dfg_1.jpg?x=318&y=215&w=1173&h=208&r=0"/>



Figure 1: GUIs generated by image-based methods (UI-Diffuser [9], TextDiffuser-2 [10], and IP2P [11] fine-tuned on GUI dataset, denoted as IP2P*). Images are cropped for space efficiency.
图 1：由基于图像的方法（在 GUI 数据集上微调的 UI-Diffuser [9]、TextDiffuser-2 [10] 和 IP2P [11]，记作 IP2P*）生成的 GUI。为节省空间，图像经过裁剪。


GUI using image generations [11, 12]. Although these methods demonstrate promising results, such as the GUI graphic generation on the location, style, and colour of GUI elements [9], or scene-text generation in a style that aligns the visual context [10, 13], they still display distortions in the text rendering, particularly for small-sized text where each pixel is critical for accurately identifying and representing the text (see Fig. 1 for an illustration).
虽然这些方法展示了具有前景的结果，例如在 GUI 元素的位置、样式和颜色上的 GUI 图形生成 [9]，或以符合视觉上下文的样式生成场景文本 [10, 13]，但在文本渲染中仍存在失真，特别是在每个像素对于准确识别和表示文本都至关重要的小尺寸文本上（如图 1 所示）。


To address the challenges of accurately generating high-fidelity text content within a GUI, we propose ViMo, the first visual GUI world model. ViMo decouples the generation of graphic and text content into distinct processes, using a novel data representation named Symbolic Text Representation (STR). In STR, each text content is replaced (overlayed) with a text symbol, a rectangle-shaped placeholder with a defined border and fill colours, functioning as a special GUI element. Thus, we simplify the task of text content generation to text symbol generation, which reframes the problem to the localisation of the text within a GUI. Based on STR, ViMo employs a STR Predictor and a GUI-text Predictor to generate the graphic and the text content respectively. Specifically, the STR predictor is implemented as a diffusion model, taking the current STR, extracted from the given GUI, and a user action as inputs to generate the STR of the next GUI. Meanwhile, the GUI-text predictor, implemented based on an LLM, leverages the STR generated by the STR predictor to produce the corresponding text for each text symbol. Finally, the predicted STR and the generated text are combined to produce the next GUI.
为了解决在GUI中准确生成高保真文本内容的挑战，我们提出了ViMo，这是首个视觉GUI世界模型。ViMo利用名为符号文本表示（STR）的新型数据表示，将图形和文本内容的生成解耦为不同的过程。在STR中，每个文本内容都被替换（覆盖）为文本符号——一种具有定义边界和填充颜色的矩形占位符，作为特殊的GUI元素。因此，我们将文本内容生成任务简化为文本符号生成，从而将问题转化为GUI内的文本定位。基于STR，ViMo采用STR预测器和GUI-文本预测器分别生成图形和文本内容。具体而言，STR预测器实现为扩散模型，以从给定GUI中提取的当前STR和用户操作作为输入，生成下一帧GUI的STR。同时，基于LLM实现的GUI-文本预测器利用STR预测器生成的STR，为每个文本符号生成相应的文本。最后，将预测的STR和生成的文本结合，产生下一帧GUI。


We evaluated ViMo in three distinct scenarios to comprehensively demonstrate its effectiveness. First, we assessed its world model capability, where the quality of the generated GUIs was measured using visual similarity, instructional accuracy, and action readiness scores. Each score was examined through both automatic metrics and user studies. These assessments provided a robust and holistic evaluation of how visually precise and contextually plausible the generated GUIs were. Second, we tested ViMo in an agent-focused task to evaluate its benefits for existing App agents and its superiority over other world models. In this setup, given a goal and the current App observation, the agent selected optimal actions to achieve the goal [6]. By accurately predicting the next GUI based on the current observation and an action, ViMo enabled the agent to better anticipate action outcomes and make more informed decisions. This experiment demonstrated the model's effectiveness in enhancing decision-making for App agents. Finally, we evaluated ViMo's real-world applicability under two settings: online navigation and zero-shot generalisation. These scenarios assessed the model's ability to perform in real-time interactions and to generalise to previously unseen Apps, further demonstrating its generalisation capabilities and practical value in dynamic environments.
我们在三种不同场景下对 ViMo 进行了评估，以全面展示其有效性。首先，我们评估了其世界模型能力，通过视觉相似度、指令准确性和操作就绪得分来衡量生成 GUI 的质量。每项得分都通过自动指标和用户研究进行了考察。这些评估对生成的 GUI 在视觉精确度和上下文合理性方面提供了稳健且全面的评估。其次，我们在以智能体为中心的任务中测试了 ViMo，以评估其对现有 App 智能体的益处及其相较于其他世界模型的优越性。在此设置下，给定目标和当前 App 观测值，智能体选择最优操作以实现目标 [6]。通过基于当前观测和操作准确预测下一个 GUI，ViMo 使智能体能够更好地预判操作结果并做出更明智的决策。该实验证明了该模型在增强 App 智能体决策能力方面的有效性。最后，我们在在线导航和零样本泛化两种设置下评估了 ViMo 的实际应用能力。这些场景评估了模型在实时交互中的表现以及对前所未见的 App 的泛化能力，进一步展示了其在动态环境中的泛化能力和实用价值。


Our main contributions are summarised as follows:
我们的主要贡献总结如下：


- We propose ViMo, the first generative visual GUI world model that predicts App observations in a visual modality, capable of more realistic and concrete visual GUI predictions compared to contemporary language-based methods.
- 我们提出了ViMo，这是首个以视觉模态预测App观测的生成式视觉GUI世界模型，与当代的基于语言的方法相比，能够实现更真实、具体的视觉GUI预测。


- To address the challenge of strict pixel-level accuracy required to avoid distorted or blurred text generation in a GUI, we propose a Symbolic Text Representation (STR), overlaying text with uniform text symbols (placeholders) to simplify text content generation to text location generation. Then ViMo leverages an LLM to generate the corresponding text content for each text symbol.
- 为了解决避免GUI中生成扭曲或模糊文本所需的严格像素级准确性挑战，我们提出了符号文本表示（STR），用统一的文本符号（占位符）覆盖文本，将文本内容生成简化为文本位置生成。随后，ViMo利用LLM为每个文本符号生成相应的文本内容。


- Extensive experiments demonstrated the effectiveness of ViMo in both world model evaluation and agent-focused tasks. Specifically, ViMo achieved an average 29.14% and 182.74% relative improvement over existing world models in terms of automatic metrics and user studies, respectively. Moreover, ViMo boosted the step-wise action prediction accuracy of App agents, achieving a 14.07% relative performance gain. In the online navigation setup, ViMo increased the task completion rate from 33.19% to 40.95%, yielding a substantial improvement of 7.76%.
- 广泛的实验证明了ViMo在世界模型评估和以智能体为中心的任务中的有效性。具体而言，ViMo在自动指标和用户研究方面分别比现有世界模型实现了29.14%和182.74%的平均相对提升。此外，ViMo提高了App智能体的逐步操作预测准确率，实现了14.07%的相对性能增益。在在线导航设置中，ViMo将任务完成率从33.19%提升至40.95%，带来了7.76%的显著提升。


## 2 Related Works
## 2 相关工作


### 2.1 App Agent
### 2.1 App智能体


App agents, powered by LLMs, have made significant strides in automating tasks on mobile Apps [14] 15, 16, 17, 18, 19]. These agents interact with GUIs by emulating human actions such as tapping, typing, and swiping. This enables them to accomplish various user goals, such as searching for products or setting alarms. Approaches in this domain are broadly divided into language-based and multi-modality-based methods. Language-based methods rely on textual description of the App observation and the user goal to generate appropriate actions [5, 20, 21], while multi-modality-based methods enhance this capability by incorporating GUIs for a more comprehensive understanding of the interface [5, 6, 22, 23]. Despite the promising results, these approaches struggle with long-horizon tasks that require multiple interdependent actions and a deep understanding of dynamic environments [7]. For this challenge, recent works [24, 25] introduce the use of real-world emulators to simulate GUI changes resulting from agent actions, enabling App agents to better navigate complex, multi-step scenarios and improve decision-making accuracy. However, the use of real-world emulators faces significant drawbacks, including the high computational costs of setting up emulators for specific mobile systems and the safety risks involved in executing real-world interactions, such as repeatedly sending messages or making purchases. To overcome these, world models have attracted increasing attention as a more efficient alternative [7, 8].
由大语言模型驱动的应用程序智能体在实现移动应用程序任务自动化方面取得了显著进展 [14, 15, 16, 17, 18, 19]。这些智能体通过模拟人类的点击、输入和滑动等操作与图形用户界面（GUI）进行交互。这使它们能够完成各种用户目标，如搜索产品或设置闹钟。该领域的方法大致分为基于语言的方法和基于多模态的方法。基于语言的方法依靠对应用程序观察结果和用户目标的文本描述来生成适当的操作 [5, 20, 21]，而基于多模态的方法则通过结合 GUI 以更全面地理解界面来增强这种能力 [5, 6, 22, 23]。尽管取得了有希望的成果，但这些方法在处理需要多个相互依赖的操作以及深入理解动态环境的长周期任务时仍面临困难 [7]。针对这一挑战，近期的研究 [24, 25] 引入了使用真实世界模拟器来模拟智能体操作导致的 GUI 变化，使应用程序智能体能够更好地应对复杂的多步骤场景并提高决策准确性。然而，使用真实世界模拟器存在显著缺点，包括为特定移动系统设置模拟器的高计算成本，以及执行真实世界交互（如反复发送消息或进行购买）所涉及的安全风险。为克服这些问题，世界模型作为一种更高效的替代方案受到了越来越多的关注 [7, 8]。


### 2.2 World Model
### 2.2 世界模型


By observing the real world, world models can predict how the environment evolves in response to an action [26, 27]. For instance, video generation models such as Sora [28] simulate future observations by predicting videos given a video observation and GameNGen [29] predicts how a game system will respond to user actions. Notably, the ability to anticipate potential outcomes of actions has proven to be highly beneficial in informing decision-making processes [30,31,32,33]. Inspired by their success, world models have emerged to predict the next observation on websites. These models [7, 8, 34] typically take a website observation and an action as inputs to generate a textual description of the next observation. While websites provide multiple sources of information, including the actual site and their CSS or HTML source files, mobiles present a more limited context, as only the GUIs are typically accessible. Moreover, such text-only descriptions often lack the precise visual details required for accurately predicting future observations, highlighting the need for a visual world model capable of generating high-fidelity future GUI images.
通过观察现实世界，世界模型可以预测环境如何响应动作而演变 [26, 27]。例如，Sora [28] 等视频生成模型通过在给定视频观察的情况下预测视频来模拟未来的观察结果，而 GameNGen [29] 则预测游戏系统将如何响应用户动作。值得注意的是，预判动作潜在结果的能力已被证明对决策过程极具价值 [30,31,32,33]。受其成功的启发，旨在预测网站下一阶段观察结果的世界模型应运而生。这些模型 [7, 8, 34] 通常以网站观察和动作作为输入，生成下一阶段观察的文本描述。虽然网站提供了包括实际站点及其 CSS 或 HTML 源文件在内的多种信息源，但移动端呈现的上下文更为有限，因为通常只能访问 GUI。此外，这种纯文本描述往往缺乏准确预测未来观察所需的精确视觉细节，这凸显了对能够生成高保真未来 GUI 图像的视觉世界模型的需求。


### 2.3 GUI Generation
### 2.3 GUI 生成


With the rapid advancements in image generation techniques, such as diffusion models [12, 35, 36], generating realistic images has become increasingly feasible. Building on these developments, previous methods have explored generating GUI directly in pixel space. For instance, layout generation methods generate the location of GUI elements [37, 38, 39, 40], scene-text generation methods generate text that aligns with the visual context [10, 13, 41, 42], UI-diffuser [9] fine-tune a stable diffusion model to generate mobile GUIs conditioned on text prompts. For the next GUI generation conditioned on current GUI observation and a user action, it seems straightforward to resort to an image-and-text-conditioned approach [11]. However, we find that pixel-based image generation struggles with rendering text accurately, as even minor pixel prediction errors can lead to distortions, particularly for small-sized text (see Fig. 1 for examples).
随着扩散模型 [12, 35, 36] 等图像生成技术的快速进步，生成逼真图像已变得日益可行。基于这些进展，此前的方法已经探索了直接在像素空间生成 GUI。例如，布局生成方法生成 GUI 元素的位置 [37, 38, 39, 40]，场景文本生成方法生成与视觉上下文一致的文本 [10, 13, 41, 42]，UI-diffuser [9] 则微调稳定扩散模型以根据文本提示生成移动端 GUI。对于基于当前 GUI 观察和用户动作的下一阶段 GUI 生成，采用图像与文本条件的方案 [11] 似乎显而易见。然而，我们发现基于像素的图像生成在准确渲染文本方面存在困难，因为即使是微小的像素预测误差也可能导致失真，尤其是对于小尺寸文本（示例见图 1）。


In this work, we advance beyond existing approaches that generate GUI entirely (graphic and text) at the pixel level. Instead, we render graphics as image pixels and generate text as language tokens, enabling a more accurate method for GUI generation.
在这项工作中，我们超越了现有的完全在像素层面生成 GUI（图形和文本）的方法。相反，我们将图形渲染为图像像素，并将文本生成为语言标记，从而实现了一种更精确的 GUI 生成方法。


<img src="https://cdn.noedgeai.com/bo_d5v247n7aajc73870dfg_3.jpg?x=316&y=199&w=1166&h=310&r=0"/>



Figure 2: Framework of our ViMo. We first detect text content (actual words) in the current GUI $\left( {x}_{k}\right)$ and overlay it with text symbols (rectangle-shaped placeholders with a black border and white fill),to create ${\mathrm{{STR}}}_{{x}_{k}}$ . Then ${\mathrm{{STR}}}_{{x}_{k}}$ and the user action $\left( a\right)$ are input to the STR predictor to generate the STR of the next GUI ( ${\mathrm{{STR}}}_{{x}_{k + 1}^{a}}$ ). Next,text symbols within ${\mathrm{{STR}}}_{{x}_{k + 1}^{a}}$ are located and assigned unique ID token. Then the LLM predicts the text content corresponding to each token. Finally, the next GUI image is constructed by overlaying the predicted text into the STR.
图 2：我们的 ViMo 框架。我们首先检测当前 GUI $\left( {x}_{k}\right)$ 中的文本内容（实际词汇），并用文本符号（带黑边和白色填充的矩形占位符）覆盖它，以创建 ${\mathrm{{STR}}}_{{x}_{k}}$。然后，将 ${\mathrm{{STR}}}_{{x}_{k}}$ 和用户动作 $\left( a\right)$ 输入到 STR 预测器中，生成下一阶段 GUI 的 STR ( ${\mathrm{{STR}}}_{{x}_{k + 1}^{a}}$ )。接着，定位 ${\mathrm{{STR}}}_{{x}_{k + 1}^{a}}$ 内的文本符号并分配唯一的 ID 标记。然后，LLM 预测每个标记对应的文本内容。最后，通过将预测文本覆盖到 STR 中来构建下一阶段 GUI 图像。


## 3 Method
## 3 方法


In this section, we first define our setup in Subsection 3.1 Then, we explain our motivation and introduce our ViMo in Subection 3.2. Finally, we demonstrate how ViMo can be applied to enhance existing App agents in real-world scenarios (Subsection 3.3). All the prompts in this section are listed in the Appendix.
在本节中，我们首先在 3.1 小节定义我们的设置。然后，在 3.2 小节解释我们的动机并介绍 ViMo。最后，我们展示如何将 ViMo 应用于增强现实场景中现有的 App 智能体（3.3 小节）。本节中的所有提示词均列于附录中。


### 3.1 Problem Setup
### 3.1 问题设置


In general,a GUI world model processes a given GUI Image ${x}_{k}$ at step $k$ ,and user action $a$ ,to predict the effect of $a$ on ${x}_{k}$ and simulate the next GUI. Formally,this can be expressed as:
通常，GUI 世界模型处理步骤 $k$ 给定的 GUI 图像 ${x}_{k}$ 和用户动作 $a$，以预测 $a$ 对 ${x}_{k}$ 的影响并模拟下一阶段 GUI。形式上，这可以表示为：


$$
{x}_{k + 1}^{a} = f\left( {{x}_{k},a}\right) , \tag{1}
$$



where $f\left( \cdot \right)$ represents the world model,and ${x}_{k + 1}^{a}$ denotes the predicted next $\left( {k + 1}\right)$ GUI image after applying $a$ to ${x}_{k}$ . In the following,we explain in detail of our world model.
其中 $f\left( \cdot \right)$ 代表世界模型，${x}_{k + 1}^{a}$ 表示在对 ${x}_{k}$ 执行 $a$ 后预测的下一阶段 $\left( {k + 1}\right)$ GUI 图像。接下来，我们将详细解释我们的世界模型。


### 3.2 ViMo: Generative Visual GUI World Model
### 3.2 ViMo：生成式视觉 GUI 世界模型


To tackle the limitation of existing methods [9, 10, 11] in generating visually plausible text for a GUI, as shown in Fig. 1, we propose ViMo, a novel generative visual GUI world model that decouples the graphic and text content generation. As shown in Fig. 2, we first detect and remove all the text in the GUI by overlaying it with a text symbol to create the Symbolic Text Representation (STR). Then a STR predictor is leveraged for determining the STR representation of the next GUI with a pixel-based diffusion process. Finally, a GUI-text predictor is proposed to generate the text content for each symbol using an LLM, followed by a handcrafted design to overplay the text into the STR image to create the next GUI. Their details are specified in the following.
为了解决现有方法 [9, 10, 11] 在生成 GUI 视觉合理文本方面的局限性（如图 1 所示），我们提出了 ViMo，一种将图形与文本内容生成解耦的新型生成式视觉 GUI 世界模型。如图 2 所示，我们首先通过覆盖文本符号来检测并移除 GUI 中的所有文本，以创建符号化文本表示 (STR)。然后，利用 STR 预测器通过基于像素的扩散过程确定下一帧 GUI 的 STR 表示。最后，提出一种 GUI 文本预测器，利用 LLM 为每个符号生成文本内容，随后通过手工设计将文本叠加到 STR 图像中以创建下一帧 GUI。其详细信息如下所述。


#### 3.2.1 STR: Symbolic Text Representation
#### 3.2.1 STR：符号化文本表示


To develop a GUI prediction model that eliminates the need to generate specific text content, we propose the Symbolic Text Representation (STR), where all the text content (actual words) within the GUI image is symbolised (overlayed) with uniform text symbols (placeholders). To be specific, we create an STR representation from a given GUI image with three steps: 1) using an OCR model [43, 44] to detect text within the GUI; 2) masking the detected text by overlaying it with a box filled with white and bordered in black; 3) we leverage an LLM to filter out static text displayed on static GUI elements and preserve it in the image, as it does not involve any semantic evolution or dynamic changes and remains unchanged as part of specific elements such as a keyboard or a clock face. Additionally, we empirically find that predicting this static text with complex spatial patterns poses significant challenges for the LLM.
为了开发一种无需生成特定文本内容的 GUI 预测模型，我们提出了符号化文本表示 (STR)，将 GUI 图像中的所有文本内容（实际单词）用统一的文本符号（占位符）进行符号化（覆盖）。具体而言，我们通过三个步骤从给定的 GUI 图像创建 STR 表示：1) 使用 OCR 模型 [43, 44] 检测 GUI 内的文本；2) 通过覆盖白底黑框的矩形来遮罩检测到的文本；3) 利用 LLM 过滤掉显示在静态 GUI 元素上的静态文本并将其保留在图像中，因为这些文本不涉及任何语义演变或动态变化，作为特定元素（如键盘或表盘）的一部分保持不变。此外，我们根据经验发现，预测具有复杂空间模式的静态文本对 LLM 构成了重大挑战。


Algorithm 1 Enhancing App Agent with Generative Visual GUI World Model
算法 1 利用生成式视觉 GUI 世界模型增强 App 智能体


---



Input: Current GUI Observation ${x}_{k}$ ,A goal $g$ ,A visual world model ViMo,A selection model
输入：当前 GUI 观测 ${x}_{k}$，目标 $g$，视觉世界模型 ViMo，选择模型


$S\left( \cdot \right)$ .



Output: Action to be applied on ${x}_{k}$ to achieve $g$ .
输出：作用于 ${x}_{k}$ 以实现 $g$ 的动作。


Generate action options $\mathcal{A}$ with $n$ actions $\left\{  {a}^{i}\right\}$ (Eq. 4)).
生成具有 $n$ 个动作 $\left\{  {a}^{i}\right\}$ 的动作选项 $\mathcal{A}$（公式 4）。


for $i = 1$ to $n$ do
对于 $i = 1$ 到 $n$ 执行


&nbsp;&nbsp;&nbsp;&nbsp;Leverage ViMo to synthesis the next GUI observation conditioned on ${a}^{i}$ and ${x}_{k}$ ,denoted as
&nbsp;&nbsp;&nbsp;&nbsp;利用 ViMo 以 ${a}^{i}$ 和 ${x}_{k}$ 为条件合成下一帧 GUI 观测，记作


&nbsp;&nbsp;&nbsp;&nbsp;${x}_{k + 1}^{{a}^{i}}$ (Eq. 5).
&nbsp;&nbsp;&nbsp;&nbsp;${x}_{k + 1}^{{a}^{i}}$（公式 5）。


end for
结束循环


Use $S\left( \cdot \right)$ to identify the optimal action with predicted observation (Eq. 6)).
使用 $S\left( \cdot \right)$ 通过预测的观测值识别最优动作（公式 6）。


---



Through the above process, GUI images are transformed into the STR representation, where the text content is abstracted into a text symbol, relaxing the task of generating semantic text content into predicting text symbols that indicate the location and size.
通过上述过程，GUI 图像被转换为 STR 表示，其中文本内容被抽象为文本符号，从而将生成语义文本内容的任务简化为预测指示位置和大小的文本符号。


#### 3.2.2 STR Predictor
#### 3.2.2 STR 预测器


Building on the powerful generative capability of diffusion-based models [12], we introduce a STR predictor specifically trained to understand a given STR and a user action, enabling it to generate the corresponding next STR effectively. In particular, we fine-tune a pre-trained stable diffusion model [12] to predict the next STR, conditioned on the STR of the current GUI and the user action. Given a STR representation $\left( {\mathrm{{STR}}}_{{x}_{k}}\right)$ extracted from GUI $\left( {x}_{k}\right)$ ,the process starts with the encoding of ${\operatorname{STR}}_{{x}_{k}}$ into a latent representation [45]: $z = \mathcal{E}\left( {\operatorname{STR}}_{{x}_{k}}\right)$ . Gaussian noise is then added to this representation to create ${z}_{t}$ at timestep $t$ . A denoising autoencoder is subsequently trained to predict the Gaussian noise in the latent representation, aiming to reverse the noise addition. The objective is defined as:
基于扩散模型 [12] 强大的生成能力，我们引入了专门训练的 STR 预测器，用于理解给定的 STR 和用户动作，使其能够有效地生成相应的下一帧 STR。具体而言，我们微调了一个预训练的稳定扩散模型 [12]，以当前 GUI 的 STR 和用户动作为条件预测下一帧 STR。给定从 GUI $\left( {x}_{k}\right)$ 提取的 STR 表示 $\left( {\mathrm{{STR}}}_{{x}_{k}}\right)$，该过程首先将 ${\operatorname{STR}}_{{x}_{k}}$ 编码为潜表示 [45]：$z = \mathcal{E}\left( {\operatorname{STR}}_{{x}_{k}}\right)$。然后在该表示中加入高斯噪声，在时间步 $t$ 创建 ${z}_{t}$。随后训练一个去噪自编码器来预测潜表示中的高斯噪声，旨在逆转加噪过程。目标函数定义为：


$$
L = {\mathbb{E}}_{\mathcal{E}\left( {\mathrm{{STR}}}_{x}\right) ,\epsilon  \sim  \mathcal{N}\left( {0,I}\right) ,t}\left\lbrack  {\begin{Vmatrix}\epsilon  - {\epsilon }_{\theta }\left( {z}_{t},\mathcal{E}\left( {\mathrm{{STR}}}_{{x}_{k}}\right) ,t,a\right) \end{Vmatrix}}_{2}^{2}\right\rbrack  , \tag{2}
$$



where ${\epsilon }_{\theta }$ is a U-Net [46] architecture conditioned on a timestep $t$ ,a text prompt $a$ (action),the visual input ${z}_{t}$ and the image condition ${\operatorname{STR}}_{{x}_{k}}$ . To support the condition on images,we follow IP2P [11] to add additional input channels to the first convolutional layer, concatenating the image condition $\mathcal{E}\left( {\mathrm{{STR}}}_{{x}_{k}}\right)$ with the noised latent ${z}_{t}$ . After training,our STR predictor is capable of synthesising the next STR $\left( {\mathrm{{STR}}}_{{x}_{k + 1}^{a}}\right)$ for given ${\mathrm{{STR}}}_{{x}_{k}}$ with action instruction $a$ .
其中 ${\epsilon }_{\theta }$ 是一个以时间步 $t$、文本提示 $a$（动作）、视觉输入 ${z}_{t}$ 和图像条件 ${\operatorname{STR}}_{{x}_{k}}$ 为条件的 U-Net [46] 架构。为了支持图像条件，我们遵循 IP2P [11]，在第一个卷积层添加额外的输入通道，将图像条件 $\mathcal{E}\left( {\mathrm{{STR}}}_{{x}_{k}}\right)$ 与加噪潜变量 ${z}_{t}$ 拼接。训练后，我们的 STR 预测器能够根据给定的 ${\mathrm{{STR}}}_{{x}_{k}}$ 和动作指令 $a$ 合成下一个 STR $\left( {\mathrm{{STR}}}_{{x}_{k + 1}^{a}}\right)$。


#### 3.2.3 GUI-Text Predictor
#### 3.2.3 GUI 文本预测器


Given a STR representation generated by our STR predictor, we design a GUI-text predictor to generate plausible text for the text symbols in the STR based on its graphics. Specifically, we first locate the text symbols in the STR by colour matching and boundary detection. This outputs the location of text symbols,along with their unique ID tokens assigned via enumeration,denoted as $\mathcal{T}$ . Then we leverage the image processing and task understanding ability of LLM to predict the text content based on its context in STR. This process can be formulated as:
给定由我们的 STR 预测器生成的 STR 表示，我们设计了一个 GUI 文本预测器，根据 STR 的图形为其文本符号生成合理的文本。具体而言，我们首先通过颜色匹配和边界检测定位 STR 中的文本符号。这将输出文本符号的位置，以及通过枚举分配的唯一 ID 令牌，记作 $\mathcal{T}$。然后，我们利用大语言模型（LLM）的图像处理和任务理解能力，根据 STR 中的上下文预测文本内容。该过程可公式化为：


$$
{\operatorname{text}}_{{x}_{k + 1}^{a}} = \operatorname{LLM}\left( {{\mathrm{{STR}}}_{{x}_{k + 1}^{a}},{x}_{k},a,\mathcal{T}}\right) , \tag{3}
$$



where ${\operatorname{STR}}_{{x}_{k + 1}^{a}}$ denotes the STR representation of ${x}_{k + 1}^{a}$ . text ${x}_{{x}_{k + 1}^{a}}$ contains the predicted text content for each text symbol, associated with its ID token. This design ensures flexible and accurate text generation tailored to the predicted GUI STRs as the context. Finally, we overlay each text content $\left( {\operatorname{text}}_{{x}_{k + 1}^{a}}\right)$ to ${\operatorname{STR}}_{{x}_{k + 1}^{a}}$ to reconstruct the predicted GUI image $\left( {x}_{k + 1}^{a}\right)$ . To be specific, text symbols are replaced with the corresponding text based on coordinates, with dynamic styling determined by the symbol's size and surrounding colours. More details are provided in the Appendix.
其中 ${\operatorname{STR}}_{{x}_{k + 1}^{a}}$ 表示 ${x}_{k + 1}^{a}$ 的 STR 表示。文本 ${x}_{{x}_{k + 1}^{a}}$ 包含每个文本符号预测的文本内容及其关联的 ID 令牌。这种设计确保了能够针对预测的 GUI STR 上下文进行灵活且准确的文本生成。最后，我们将每个文本内容 $\left( {\operatorname{text}}_{{x}_{k + 1}^{a}}\right)$ 叠加到 ${\operatorname{STR}}_{{x}_{k + 1}^{a}}$ 上，以重建预测的 GUI 图像 $\left( {x}_{k + 1}^{a}\right)$。具体来说，文本符号将根据坐标被替换为相应的文本，并根据符号大小和周围颜色确定动态样式。更多细节见附录。


### 3.3 ViMo Enhanced App Agent
### 3.3 ViMo 增强型 App 智能体


Motivated by that App agents usually face limitations in long-horizon planning to make optimal decisions on action selection [7], we leverage the proposed ViMo to enhance the decision-making of App agents.
考虑到 App 智能体通常在长程规划方面面临限制，难以在动作选择上做出最优决策 [7]，我们利用所提出的 ViMo 来增强 App 智能体的决策能力。


Table 1: GUI quality evaluation. ${s}_{gc}$ indicates the GUI consistency, ${s}_{ia}$ instructional accuracy and ${s}_{ar}$ action readiness score. ${s}_{h}$ indicates the harmonic average between the 3 metrics. $\Delta {s}_{h}$ is the relative performance gains of our ViMo over other methods. IP2P* denotes finetuing of IP2P on our dataset.
表 1：GUI 质量评估。${s}_{gc}$ 表示 GUI 一致性，${s}_{ia}$ 表示指令准确度，${s}_{ar}$ 表示动作就绪分数。${s}_{h}$ 表示这 3 项指标的调和平均值。$\Delta {s}_{h}$ 是我们的 ViMo 相较于其他方法的相对性能提升。IP2P* 表示在我们的数据集上微调后的 IP2P。


<table><tr><td rowspan="2">Method</td><td colspan="5">Automatic Metric</td><td colspan="5">User Study</td></tr><tr><td>${S}_{qc}$</td><td>Sia</td><td>${s}_{ar}$</td><td>${s}_{h}$</td><td>$\Delta {s}_{h}$</td><td>${S}_{qc}$</td><td>${s}_{ia}$</td><td>${s}_{ar}$</td><td>${s}_{h}$</td><td>$\Delta {s}_{h}$</td></tr><tr><td>HTML-vision</td><td>0.70</td><td>85.77</td><td>62.79</td><td>0.72</td><td>5.39%</td><td>0.31</td><td>11.32</td><td>9.01</td><td>0.23</td><td>282.61%</td></tr><tr><td>IP2P*</td><td>0.74</td><td>63.57</td><td>70.15</td><td>0.69</td><td>10.20%</td><td>0.82</td><td>58.92</td><td>52.81</td><td>0.63</td><td>39.68%</td></tr><tr><td>UI-diffuser</td><td>0.60</td><td>39.61</td><td>38.75</td><td>0.44</td><td>71.82%</td><td>0.36</td><td>14.32</td><td>8.56</td><td>0.27</td><td>225.93%</td></tr><tr><td>ViMo (Ours)</td><td>0.74</td><td>75.39</td><td>78.68</td><td>0.76</td><td>-</td><td>0.89</td><td>91.12</td><td>84.71</td><td>0.88</td><td>-</td></tr></table>
<table><tbody><tr><td rowspan="2">方法</td><td colspan="5">自动指标</td><td colspan="5">用户研究</td></tr><tr><td>${S}_{qc}$</td><td>Sia</td><td>${s}_{ar}$</td><td>${s}_{h}$</td><td>$\Delta {s}_{h}$</td><td>${S}_{qc}$</td><td>${s}_{ia}$</td><td>${s}_{ar}$</td><td>${s}_{h}$</td><td>$\Delta {s}_{h}$</td></tr><tr><td>HTML-vision</td><td>0.70</td><td>85.77</td><td>62.79</td><td>0.72</td><td>5.39%</td><td>0.31</td><td>11.32</td><td>9.01</td><td>0.23</td><td>282.61%</td></tr><tr><td>IP2P*</td><td>0.74</td><td>63.57</td><td>70.15</td><td>0.69</td><td>10.20%</td><td>0.82</td><td>58.92</td><td>52.81</td><td>0.63</td><td>39.68%</td></tr><tr><td>UI-diffuser</td><td>0.60</td><td>39.61</td><td>38.75</td><td>0.44</td><td>71.82%</td><td>0.36</td><td>14.32</td><td>8.56</td><td>0.27</td><td>225.93%</td></tr><tr><td>ViMo (Ours)</td><td>0.74</td><td>75.39</td><td>78.68</td><td>0.76</td><td>-</td><td>0.89</td><td>91.12</td><td>84.71</td><td>0.88</td><td>-</td></tr></tbody></table>


To be specific, we break down the process into three steps: action option generation, action outcome synthesis,and action selection. In the first step,the App agent generates $n$ action options,as follows:
具体而言，我们将该过程分解为三个步骤：动作选项生成、动作结果合成以及动作选择。在第一步中，App智能体生成 $n$ 动作选项，如下所示：


$$
\mathcal{A} = \operatorname{Agent}\left( {{x}_{k},g}\right) , \tag{4}
$$



where $\mathcal{A} = \left\{  {{a}^{1},{a}^{2},\cdots ,{a}^{n}}\right\}$ denotes the action option set, ${x}_{k}$ is the current GUI image at step $k$ ,and $g$ is the given user goal. With these action options,our world model ViMo is leveraged to synthesise the outcome (next GUI) of these actions as follows:
其中 $\mathcal{A} = \left\{  {{a}^{1},{a}^{2},\cdots ,{a}^{n}}\right\}$ 表示动作选项集，${x}_{k}$ 是步骤 $k$ 的当前 GUI 图像，而 $g$ 是给定的用户目标。利用这些动作选项，我们的世界模型 ViMo 被用于合成这些动作的结果（下一帧 GUI），如下所示：


$$
{x}_{k + 1}^{{a}^{i}} = \operatorname{ViMo}\left( {{x}_{k},{a}^{i}}\right) , \tag{5}
$$



where ${x}_{k + 1}^{{a}^{i}}$ denotes the synthesised next GUI of applying action ${a}^{i}$ on ${x}_{k}$ . Finally,each action ${a}^{i}$ and its corresponding predicted outcome ${x}_{k + 1}^{{a}^{i}}$ are fed into an LLM-based selection model,which identifies the optimal action based on the generated GUIs. This process can be formulated as:
其中 ${x}_{k + 1}^{{a}^{i}}$ 表示在 ${x}_{k}$ 上执行动作 ${a}^{i}$ 后合成的下一帧 GUI。最后，每个动作 ${a}^{i}$ 及其对应的预测结果 ${x}_{k + 1}^{{a}^{i}}$ 被输入到基于大语言模型（LLM）的选择模型中，该模型根据生成的 GUI 识别出最佳动作。该过程可以公式化为：


$$
{a}_{se} = S\left( {\left\{  \left( {a}^{i},{x}_{k + 1}^{{a}^{i}}\right) \right\}  }_{i = 1}^{n}\right) , \tag{6}
$$



where ${a}_{se}$ denotes the selected action,and $S\left( \cdot \right)$ is the selection model. This procedure is outlined in Algorithm 1 By predicting the next GUI, ViMo provides the agent with the potential outcome of an action, enabling it to make more informed decisions. We build the selection model to identify the best action in two steps. First, we query an LLM to evaluate all the action candidates, providing a judgment-either valid or invalid-and a confidence score for each action. Second, we query the LLM again to select the best action from the two highest-scoring actions. This process is motivated by our observation that, in over 70% of tasks, the difference between the top two scores is equal to or less than 0.1, indicating that both are likely optimal. By explicitly prompting the LLM to compare the top candidates, we go beyond coarse scoring and enable more detailed decision-making.
其中 ${a}_{se}$ 表示选定的动作，$S\left( \cdot \right)$ 是选择模型。该流程在算法 1 中列出。通过预测下一帧 GUI，ViMo 为智能体提供了动作的潜在结果，使其能够做出更明智的决策。我们分两步构建选择模型以识别最佳动作。首先，我们查询 LLM 以评估所有候选动作，并为每个动作提供判断（有效或无效）以及置信度分数。其次，我们再次查询 LLM，从得分最高的两个动作中选出最佳动作。这一做法源于我们的观察：在超过 70% 的任务中，前两名分数之差等于或小于 0.1，表明两者都可能是最优的。通过显式提示 LLM 比较顶级候选动作，我们超越了粗略评分，实现了更精细的决策。


## 4 Experiments
## 4 实验


In this section, we begin by summarising our proposed STR dataset discussed in Subsection 3.2.1 Next, we tested the core capability of our ViMo, focusing on its GUI generation ability. Building on this, we demonstrated how the powerful GUI generation capability of ViMo can enhance the decision-making of App agents. Then, we studied our effectiveness in real-world App navigation tasks. Finally, we carried out the ablation study to validate the effectiveness of our model design. Specific setups and experiment details are elaborated in subsequent sections. Unless explicitly stated otherwise, GPT-40 [47] was employed as the default LLM in the following sections.
在本节中，我们首先总结 3.2.1 节中讨论的 STR 数据集。接着，我们测试了 ViMo 的核心能力，重点关注其 GUI 生成能力。在此基础上，我们展示了 ViMo 强大的 GUI 生成能力如何增强 App 智能体的决策。随后，我们研究了在真实世界 App 导航任务中的有效性。最后，我们进行了消融研究，以验证模型设计的有效性。具体的设置和实验细节在后续章节中详细阐述。除非另有明确说明，以下章节中默认使用的 LLM 为 GPT-4o [47]。


### 4.1 Dataset Summarisation
### 4.1 数据集总结


Our STR dataset was constructed using data from two widely recognised and large-scale sources: Android Control [21] and Android in the Wild (AITW) [4]. From these sources, we respectively sampled 12 and 7 Apps, selecting those with rich data samples while filtering out noise. Android Control provides two types of user actions: 1) action commands: predefined actions (e.g., click, scroll) accompanied by specific parameters such as coordinates (x, y); 2) action instructions: actions described in natural languages, such as "click the plus icon". We used action instructions as conditions for our world model as this approach was more concrete and better utilised the pre-trained model in understanding natural language. For AITW, action commands were converted into action instructions using GPT-40 [47]. In total, we collected 19 Apps with 3,550 episodes, 23,620 images and 18,450 actions. To ensure both time-efficient and cost-efficient experiments, we followed prior App agent [4] on partial split evaluation, instead of evaluating the model on the entire testing set. Specifically, we randomly sampled 57 episodes across 19 distinct Apps. Details including dataset collection, split summarisation and full-split experiments are provided in the Appendix.
我们的 STR 数据集是使用来自两个广泛认可的大规模来源构建的：Android Control [21] 和 Android in the Wild (AITW) [4]。我们分别从这些来源中采样了 12 个和 7 个 App，选择了数据样本丰富的 App 并过滤了噪声。Android Control 提供两类用户动作：1) 动作命令：带有特定参数（如坐标 x, y）的预定义动作（如点击、滚动）；2) 动作指令：以自然语言描述的动作，例如“点击加号图标”。我们使用动作指令作为世界模型的条件，因为这种方法更具体，且能更好地利用预训练模型理解自然语言。对于 AITW，动作命令使用 GPT-4o [47] 转换为动作指令。我们总共收集了 19 个 App，包含 3,550 个序列、23,620 张图像和 18,450 个动作。为确保实验的时间和成本效益，我们遵循先前 App 智能体 [4] 的部分划分评估方法，而不是在整个测试集上评估模型。具体而言，我们从 19 个不同的 App 中随机采样了 57 个序列。包括数据集收集、划分总结和全划分实验在内的细节见附录。


<img src="https://cdn.noedgeai.com/bo_d5v247n7aajc73870dfg_6.jpg?x=333&y=203&w=1138&h=728&r=0"/>



Figure 3: GUI generation comparison in graphic generation (Top) and text generation (Bottom).
图 3：图形生成（上）与文本生成（下）中的 GUI 生成对比。


### 4.2 World Model Ability
### 4.2 世界模型能力


We evaluated the GUI generation capability of ViMo by GUI quality evaluation. We included IP2P [11] and UI-diffuser [9], both originally designed for image editing and GUI generation. For IP2P, we fine-tuned their diffusion model on our dataset to generate everything of the GUI, including the text content and the graphic, denoted as IP2P*. We also leveraged an LLM to predict App observations in an HTML format, which were rendered into images, denoted as HTML-vision.
我们通过 GUI 质量评估来评价 ViMo 的 GUI 生成能力。我们纳入了原本为图像编辑和 GUI 生成设计的 IP2P [11] 和 UI-diffuser [9]。对于 IP2P，我们在我们的数据集上微调了其扩散模型，以生成 GUI 的全部内容（包括文本内容和图形），记作 IP2P*。我们还利用 LLM 以 HTML 格式预测 App 观测值，并将其渲染为图像，记作 HTML-vision。


We leveraged 3 evaluation metrics: The GUI consistency score $\left( {s}_{gc}\right)$ assessed the visual similarity between the ground truth and the generated next GUI; Instructional accuracy score $\left( {s}_{ia}\right)$ determined whether the generated GUI adheres to the user action; Action readiness score $\left( {s}_{ar}\right)$ evaluated whether the generated GUI retains valid elements essential for subsequent actions required to achieve the user goal. We conducted both automatic evaluations and user preference studies to assess our methods. For the automatic evaluation, we used DINO [48] as the visual encoder to compute ${s}_{gc}$ ,and an LLM to evaluate ${s}_{ia}$ and ${s}_{ar}$ . The prompts used for the LLM-based evaluations are provided in the Appendix. For the user study, we invited 70 voluntary participants to complete questionnaires based on 80 GUI samples, generated by all 4 compared methods. For each sample, participants were asked three questions-one each for evaluating ${s}_{gc},{s}_{ia}$ ,and ${s}_{ar}$ . The full instructions provided to the participants are provided in the Appendix.
我们采用了 3 项评估指标：GUI 一致性评分 $\left( {s}_{gc}\right)$ 用于评估地面真值与生成的下一界面之间的视觉相似度；指令准确性评分 $\left( {s}_{ia}\right)$ 用于确定生成的界面是否遵循用户操作；操作就绪评分 $\left( {s}_{ar}\right)$ 用于评估生成的界面是否保留了实现用户目标后续操作所需的有效元素。我们进行了自动评估和用户偏好研究来评估我们的方法。在自动评估中，我们使用 DINO [48] 作为视觉编码器来计算 ${s}_{gc}$，并使用 LLM 评估 ${s}_{ia}$ 和 ${s}_{ar}$。基于 LLM 评估所使用的提示词见附录。在用户研究中，我们邀请了 70 名志愿者根据 4 种对比方法生成的 80 个 GUI 样本填写问卷。对于每个样本，参与者被问及三个问题，分别用于评估 ${s}_{gc},{s}_{ia}$ 和 ${s}_{ar}$。提供给参与者的完整指南见附录。


As shown in Table 1, ViMo achieved the highest score on the harmonic average of the three automatic metrics, surpassing other methods with an average relative performance improvement of 29.14%. The results of the user study were consistent with the automatic evaluations, where ViMo demonstrated the best performance. Notably, HTML-vision and UI-diffuser performed significantly worse in human evaluation compared to their scores in the LLM-based assessment. This discrepancy likely arose because human evaluators perceived the outputs of these methods as visually unrealistic or functionally incoherent,leading to lower subjective scores in ${s}_{gc},{s}_{ia}$ and ${s}_{h}$ .
如表 1 所示，ViMo 在三项自动指标的调和平均值上获得最高分，超越其他方法，平均相对性能提升了 29.14%。用户研究的结果与自动评估一致，ViMo 表现出最佳性能。值得注意的是，HTML-vision 和 UI-diffuser 在人工评估中的表现明显低于其在基于 LLM 评估中的得分。这种差异可能是因为人工评估者认为这些方法的输出在视觉上不真实或在功能上不连贯，导致 ${s}_{gc},{s}_{ia}$ 和 ${s}_{h}$ 的主观评分较低。


Qualitative comparisons are presented in Fig. 3, under two scenarios: GUI graphic changes (Top) and text generation (Bottom, cropped for space efficiency). Experiments revealed that while the
定性对比见图 3，包含两种场景：GUI 图形变化（上）和文本生成（下，为节省空间已裁剪）。实验表明，虽然


Table 2: Decision optimisation comparison with App agents.
表 2：与 App 智能体的决策优化对比。


<table><tr><td>Agent Type</td><td>App Agent</td><td>Leisure</td><td>Work</td><td>System</td><td>Overall</td></tr><tr><td rowspan="4">Language-Based</td><td>ER 21</td><td>31.76</td><td>46.15</td><td>34.13</td><td>34.50</td></tr><tr><td>AutoDroid 20</td><td>35.81</td><td>46.15</td><td>31.75</td><td>35.46</td></tr><tr><td>T3A [5]</td><td>41.22</td><td>51.28</td><td>42.86</td><td>43.13</td></tr><tr><td>T3A + ViMo (Ours)</td><td>50.00</td><td>58.97</td><td>45.24</td><td>49.20</td></tr><tr><td rowspan="4">Multi-Modality-Based</td><td>APP-Agent [49</td><td>43.24</td><td>51.28</td><td>39.68</td><td>42.81</td></tr><tr><td>Mobile-Agent-v2 [6]</td><td>43.92</td><td>53.85</td><td>39.68</td><td>43.45</td></tr><tr><td>M3A [5]</td><td>46.62</td><td>51.28</td><td>43.65</td><td>46.01</td></tr><tr><td>M3A + ViMo (Ours)</td><td>53.38</td><td>53.85</td><td>45.24</td><td>50.16</td></tr></table>
<table><tbody><tr><td>智能体类型</td><td>应用智能体</td><td>休闲</td><td>工作</td><td>系统</td><td>总体</td></tr><tr><td rowspan="4">基于语言</td><td>ER 21</td><td>31.76</td><td>46.15</td><td>34.13</td><td>34.50</td></tr><tr><td>AutoDroid 20</td><td>35.81</td><td>46.15</td><td>31.75</td><td>35.46</td></tr><tr><td>T3A [5]</td><td>41.22</td><td>51.28</td><td>42.86</td><td>43.13</td></tr><tr><td>T3A + ViMo (本研究)</td><td>50.00</td><td>58.97</td><td>45.24</td><td>49.20</td></tr><tr><td rowspan="4">基于多模态</td><td>APP-Agent [49</td><td>43.24</td><td>51.28</td><td>39.68</td><td>42.81</td></tr><tr><td>Mobile-Agent-v2 [6]</td><td>43.92</td><td>53.85</td><td>39.68</td><td>43.45</td></tr><tr><td>M3A [5]</td><td>46.62</td><td>51.28</td><td>43.65</td><td>46.01</td></tr><tr><td>M3A + ViMo (本研究)</td><td>53.38</td><td>53.85</td><td>45.24</td><td>50.16</td></tr></tbody></table>


Table 3: Result on M3A Agent.
表 3：M3A Agent 上的结果。


<table><tr><td>Modality</td><td>World Model</td><td>Step Acc.</td></tr><tr><td>w/o WM</td><td>w/o WM</td><td>46.01</td></tr><tr><td rowspan="2">Language</td><td>Change-text</td><td>47.28</td></tr><tr><td>HTML-text</td><td>46.65</td></tr><tr><td rowspan="4">Vision</td><td>HTML-vision</td><td>48.89</td></tr><tr><td>UI-diffuser</td><td>47.60</td></tr><tr><td>IP2P*</td><td>48.56</td></tr><tr><td>ViMo (Ours)</td><td>50.16</td></tr></table>
<table><tbody><tr><td>模态</td><td>世界模型</td><td>单步准确率</td></tr><tr><td>无世界模型</td><td>无世界模型</td><td>46.01</td></tr><tr><td rowspan="2">语言</td><td>文本变更</td><td>47.28</td></tr><tr><td>HTML文本</td><td>46.65</td></tr><tr><td rowspan="4">视觉</td><td>HTML视觉</td><td>48.89</td></tr><tr><td>UI-diffuser</td><td>47.60</td></tr><tr><td>IP2P*</td><td>48.56</td></tr><tr><td>ViMo (本文方法)</td><td>50.16</td></tr></tbody></table>


Table 4: Zero-shot Evaluation.
表 4：零样本评估。


<table><tr><td>App Agent</td><td>LLM</td><td>Step Acc.</td></tr><tr><td>SeeAct</td><td>GPT-4-Turbo</td><td>33.9</td></tr><tr><td>M3A</td><td>GPT-4-Turbo</td><td>42.1</td></tr><tr><td>ER</td><td>Gemini 1.5 Pro</td><td>24.4</td></tr><tr><td>T3A</td><td>Gemini-2.0-Flash</td><td>41.4</td></tr><tr><td>T3A+ViMo</td><td>Gemini-2.0-Flash</td><td>46.8</td></tr><tr><td>M3A</td><td>Gemini-2.0-Flash</td><td>44.2</td></tr><tr><td>M3A+ViMo</td><td>Gemini-2.0-Flash</td><td>47.6</td></tr></table>
<table><tbody><tr><td>应用智能体</td><td>大语言模型</td><td>步骤准确率</td></tr><tr><td>SeeAct</td><td>GPT-4-Turbo</td><td>33.9</td></tr><tr><td>M3A</td><td>GPT-4-Turbo</td><td>42.1</td></tr><tr><td>ER</td><td>Gemini 1.5 Pro</td><td>24.4</td></tr><tr><td>T3A</td><td>Gemini-2.0-Flash</td><td>41.4</td></tr><tr><td>T3A+ViMo</td><td>Gemini-2.0-Flash</td><td>46.8</td></tr><tr><td>M3A</td><td>Gemini-2.0-Flash</td><td>44.2</td></tr><tr><td>M3A+ViMo</td><td>Gemini-2.0-Flash</td><td>47.6</td></tr></tbody></table>


Table 5: Online Evaluation.
表 5：在线评估。


<table><tr><td>App Agent</td><td>LLM</td><td>Task Acc.</td></tr><tr><td>SeeAct</td><td>GPT-4-Turbo</td><td>15.50</td></tr><tr><td>M3A</td><td>GPT-4-Turbo</td><td>25.40</td></tr><tr><td>M3A</td><td>Gemini-1.5-Pro</td><td>22.80</td></tr><tr><td>T3A</td><td>GPT-4-Turbo</td><td>30.60</td></tr><tr><td>T3A</td><td>Gemini-1.5-Pro</td><td>19.40</td></tr><tr><td>T3A</td><td>Gemini-2.0-Flash</td><td>33.19</td></tr><tr><td>T3A + ViMo</td><td>Gemini-2.0-Flash</td><td>40.95</td></tr></table>
<table><tbody><tr><td>应用智能体</td><td>大语言模型</td><td>任务准确率</td></tr><tr><td>SeeAct</td><td>GPT-4-Turbo</td><td>15.50</td></tr><tr><td>M3A</td><td>GPT-4-Turbo</td><td>25.40</td></tr><tr><td>M3A</td><td>Gemini-1.5-Pro</td><td>22.80</td></tr><tr><td>T3A</td><td>GPT-4-Turbo</td><td>30.60</td></tr><tr><td>T3A</td><td>Gemini-1.5-Pro</td><td>19.40</td></tr><tr><td>T3A</td><td>Gemini-2.0-Flash</td><td>33.19</td></tr><tr><td>T3A + ViMo</td><td>Gemini-2.0-Flash</td><td>40.95</td></tr></tbody></table>


HTML-vision method exhibited greater flexibility in responding to user actions (as shown in the bottom examples), it failed to produce concrete details necessary for future actions (top). Conversely, IP2P* generated plausible GUI graphics but lacked flexibility in text content generation (also reflected by ${s}_{gc}$ and ${s}_{ia}$ in Table 1). This trade-off highlighted the superior balance of ViMo.
HTML-vision方法在响应用户操作方面表现出更大的灵活性（如底部示例所示），但未能产生未来操作所需的具体细节（顶部）。相反，IP2P*生成了合理的GUI图形，但在文本内容生成方面缺乏灵活性（这也反映在表1中的${s}_{gc}$和${s}_{ia}$）。这种权衡凸显了ViMo卓越的平衡性。


### 4.3 World Model Enhanced App Agent
### 4.3 世界模型增强的App智能体


This section demonstrates that: 1) ViMo enhanced the performance of App agents in decision-making; 2) ViMo outperformed other world models in enabling App agents to make more accurate decisions.
本节证明了：1) ViMo增强了App智能体在决策中的性能；2) 在赋能App智能体做出更准确决策方面，ViMo优于其他世界模型。


Comparison with App Agents. In this experiment, we collected 6 LLM-based App agents, which included three language-based methods: ER, AutoDroid, and T3A, as well as three multi-modality-based methods: APP-Agent, Mobile-Agent-v2, and M3A. We applied our ViMo into M3A and T3A following the process in Subsection 3.3 Moreover, we followed the previous works [4, 21] to use the step accuracy (the number of correct actions divided by the number of overall actions) to quantify the model performance. To provide more detailed results, we categorised the Apps into three groups: "Leisure", "Work" and "System". Table 2 demonstrates that ViMo was beneficial to the App agent, achieving a relative performance gain of 9.01% for M3A and 14.07% for T3A. These findings highlighted the effectiveness of our proposed world model in providing App agents with enhanced decision-making capability. Additional information about the categorisation and experiments with more App agents are provided in the Appendix.
与App智能体的比较。在本实验中，我们收集了6个基于LLM的App智能体，包括三种基于语言的方法：ER、AutoDroid和T3A，以及三种基于多模态的方法：APP-Agent、Mobile-Agent-v2和M3A。我们按照3.3节的流程将ViMo应用于M3A和T3A。此外，我们遵循前人的工作[4, 21]，使用步准确率（正确操作数除以总操作数）来量化模型性能。为了提供更详细的结果，我们将App分为三组：“休闲”、“工作”和“系统”。表2显示，ViMo对App智能体有益，使M3A的相对性能提升了9.01%，T3A提升了14.07%。这些发现突显了我们提出的世界模型在为App智能体提供增强决策能力方面的有效性。关于分类和更多App智能体实验的补充信息见附录。


Comparison with World Models. We further evaluated the ability of ViMo to enhance App agent decision-making by comparing it against existing world models. In addition to vision-based world models discussed in Subsection 4.2 we also incorporated two language-based world models where we adapted website world models [8] to App setups, utilising Change-text to generate textual descriptions capturing differences between consecutive observations and HTML-text to predict App observations in an HTML format.Then, we applied all the world models on M3A App agents. Table 3 illustrates that our proposed world model outperformed other world models, achieving a step accuracy (Step Acc.) of 50.16%. These findings underscored the effectiveness of our world model in enhancing step-level decision-making. Experiments with more App agents are provided in the Appendix.
与世界模型的比较。我们通过与现有世界模型的对比，进一步评估了ViMo增强App智能体决策的能力。除了4.2节讨论的基于视觉的世界模型外，我们还引入了两种基于语言的世界模型，我们将网页世界模型[8]适配于App设置，利用Change-text生成捕捉连续观测差异的文本描述，并利用HTML-text以HTML格式预测App观测。然后，我们将所有世界模型应用于M3A App智能体。表3说明我们提出的世界模型优于其他世界模型，步准确率（Step Acc.）达到50.16%。这些发现强调了我们的世界模型在增强步级决策方面的有效性。更多App智能体的实验见附录。


### 4.4 Real-world Applications
### 4.4 现实应用


Practical Deployment. ViMo was designed to be lightweight and easily deployable. The minimum requirement for deployment is a GPU with 16 GB of memory. Moreover, ViMo was implemented as a plug-and-play API that required only a single function call, making integration straightforward. Inference time on V100 GPU is 8 seconds on a STR image generation and 30 seconds on GUI-text prediction. We collected and compared the inference time with existing methods in the Appendix.
实际部署。ViMo设计轻巧且易于部署。部署的最低要求是16 GB显存的GPU。此外，ViMo被实现为仅需单次函数调用的即插即用API，使集成变得简单。在V100 GPU上，STR图像生成的推理时间为8秒，GUI文本预测为30秒。我们在附录中收集并对比了现有方法的推理时间。


Generalisation to New Apps. Generalisation is a crucial capability for real-world applications. To assess the generalisation performance of our method on new Apps that were unseen during training, we conducted a zero-shot evaluation using data from the Android Control dataset [21], explicitly excluding Apps encountered during training. As shown in Table 4 our approach substantially outperformed the baseline and achieved 47.6%, underscoring its robustness and adaptability to novel App environments. Additional visualisations of unseen scenarios are provided in the Appendix.
对新App的泛化。泛化是现实应用的关键能力。为了评估我们的方法在训练期间未见的新App上的泛化性能，我们使用Android Control数据集[21]的数据进行了零样本评估，明确排除了训练期间遇到的App。如表4所示，我们的方法显著优于基线并达到47.6%，突显了其对新App环境的鲁棒性和适应性。未见场景的更多可视化结果见附录。


Online Evaluation. To further demonstrate the effectiveness of ViMo in realistic App navigation scenarios, we conducted an online evaluation using the AndroidWorld dataset [5], which comprises 116 distinct navigation tasks. Performance was measured using the task success rate (Task Acc.). As illustrated in Table 5. ViMo achieved a notable improvement of 7.76% over the baseline method, highlighting its effectiveness and reliability in real-world settings.
在线评估。为了进一步展示ViMo在真实App导航场景中的有效性，我们使用包含116个不同导航任务的AndroidWorld数据集[5]进行了在线评估。性能通过任务成功率（Task Acc.）衡量。如表5所示，ViMo比基线方法实现了7.76%的显著提升，突显了其在现实环境中的有效性和可靠性。


### 4.5 Ablation Study
### 4.5 消融研究


In this section, we ablated on three key components of ViMo: 1) preserving static text within the image to simplify the text generation task; 2) using action instructions instead of action commands as the conditioning input for ViMo; and 3) varying the number of iterations, where each iteration corresponds to one roll-out step into the future during GUI prediction.
在本节中，我们对ViMo的三个关键组件进行了消融：1) 在图像中保留静态文本以简化文本生成任务；2) 使用操作指令而非操作命令作为ViMo的条件输入；3) 改变迭代次数，其中每次迭代对应GUI预测中向未来推演的一步。


Table 6: Ablations on preserving static text and using action instructions.
表6：关于保留静态文本和使用操作指令的消融实验。


<table><tr><td rowspan="2">Static Text</td><td rowspan="2">Action Instr.</td><td colspan="2">App Agent</td></tr><tr><td>T3A</td><td>M3A</td></tr><tr><td>N/A</td><td>N/A</td><td>43.13</td><td>46.01</td></tr><tr><td>✓</td><td>-</td><td>42.81</td><td>45.05</td></tr><tr><td>-</td><td>✓</td><td>47.28</td><td>48.88</td></tr><tr><td>✓</td><td>✓</td><td>49.20</td><td>50.16</td></tr></table>
<table><tbody><tr><td rowspan="2">静态文本</td><td rowspan="2">动作指令</td><td colspan="2">应用代理</td></tr><tr><td>T3A</td><td>M3A</td></tr><tr><td>不适用</td><td>不适用</td><td>43.13</td><td>46.01</td></tr><tr><td>✓</td><td>-</td><td>42.81</td><td>45.05</td></tr><tr><td>-</td><td>✓</td><td>47.28</td><td>48.88</td></tr><tr><td>✓</td><td>✓</td><td>49.20</td><td>50.16</td></tr></tbody></table>


Firstly, for the challenge of predicting static text from specific GUI elements, such as keyboard, number pad or clock face, which typically did not involve text changes and exhibited complex spatial patterns, we retained static text within the image (Subsection 3.2.1). This approach eliminated the need for the LLM to generate such static text while generating in pixels instead. Secondly, we proposed conditioning STR prediction on action instruction rather than action commands (Subsection 4.1). Ablation results are presented in Table 6, where "Static Text" indicates whether static text was retained in the images, and "Action Instr." denotes whether natural language instructions ("√") or abstract action commands ("-") were used as conditioning input to ViMo. The first row indicates the baseline where ViMo was not applied. The table shows that both components contributed significantly to performance improvements across the two App agents, highlighting their critical roles in enabling ViMo to generate high-quality GUIs. Visual comparison examples are provided in the Appendix for further illustration.
首先，针对预测特定GUI元素（如键盘、数字键盘或钟面）静态文本的挑战，这些元素通常不涉及文本变化且呈现复杂的空间模式，我们将静态文本保留在图像中（3.2.1节）。这种方法消除了LLM生成此类静态文本的需求，转而以像素形式生成。其次，我们建议基于动作指令而非动作命令进行STR预测（4.1节）。消融结果如表6所示，其中“Static Text”表示图像中是否保留了静态文本，“Action Instr.”表示是否使用自然语言指令（“√”）或抽象动作命令（“-”）作为ViMo的条件输入。第一行表示未应用ViMo的基线。表格显示，这两个组件都对两个App代理的性能提升做出了显著贡献，突显了它们在使ViMo生成高质量GUI方面的关键作用。附录中提供了视觉对比示例以作进一步说明。


Table 7: Ablation on the number of iterations.
表 7：关于迭代次数的消融研究。


<table><tr><td>Method</td><td>Iterations</td><td>Step Acc. (%)</td></tr><tr><td>T3A</td><td>N/A</td><td>39.94</td></tr><tr><td rowspan="3">T3A+ViMo</td><td>1</td><td>46.06</td></tr><tr><td>2</td><td>46.65</td></tr><tr><td>3</td><td>45.05</td></tr></table>
<table><tbody><tr><td>方法</td><td>迭代次数</td><td>步准确率 (%)</td></tr><tr><td>T3A</td><td>不适用</td><td>39.94</td></tr><tr><td rowspan="3">T3A+ViMo</td><td>1</td><td>46.06</td></tr><tr><td>2</td><td>46.65</td></tr><tr><td>3</td><td>45.05</td></tr></tbody></table>


Our ViMo predicted future GUI observations, which could be recursively fed back as input to simulate further into the future. In this ablation study, we varied the iteration number to evaluate how extended roll-outs impact prediction accuracy. We took Gemini-2.0-Flash [50] as the LLM in this study. As shown in Table 7, performing two iterations yielded the highest accuracy. However, this also led to increased computational cost. Therefore, we selected one step as a practical trade-off between performance and efficiency. We also observed a slight decline in performance at iteration 3 relative to iterations 1 and 2, indicating that extending the prediction horizon did not necessarily improve agent behaviour. This was likely due to that longer horizons introduced not only additional foresight but also a greater accumulation of prediction errors, whose detrimental effect could outweigh the potential benefits. Further analysis of error accumulation across iterations, along with comparisons with various world models and implementation details, is provided in the Appendix.
我们的 ViMo 能够预测未来的 GUI 观测结果，这些结果可以递归地作为输入反馈，以模拟更遥远的未来。在此项消融研究中，我们通过改变迭代次数来评估延长推演（roll-outs）对预测准确性的影响。本研究采用 Gemini-2.0-Flash [50] 作为大语言模型（LLM）。如表 7 所示，进行两次迭代的准确率最高。然而，这也导致了计算成本的增加。因此，我们在性能与效率之间进行权衡，选择了单步迭代。我们还观察到，第 3 次迭代的性能较第 1 次和第 2 次略有下降，这表明延长预测视界并不一定能改善智能体的行为。这可能是由于较长的视界不仅引入了额外的远见，还导致了预测误差的更大累积，其负面影响可能超过了潜在收益。关于各迭代间误差累积的进一步分析，以及与各种世界模型的对比和实现细节，请参阅附录。


## 5 Conclusion
## 5 结论


In this work, we introduced ViMo, a novel generative visual GUI world model designed to predict App observations in a visual modality, providing a more realistic and concrete approach compared to contemporary language-based models. To address the unique challenges of GUI generation, ViMo was equipped with the Symbolic Text Representation (STR) to simplify text content generation to text location prediction by overlaying text content with placeholders and delegating content generation to LLM. This innovation ensured high visual fidelity and avoided artefacts like distorted or blurred text. Through extensive experiments, we demonstrated that ViMo generated both visually plausible and functionally effective GUIs. Notably, ViMo boosted step-wise action prediction accuracy by a relative performance gain of 14.07%, underscoring its potential to enhance decision-making of App agents. Furthermore, real-world experiments demonstrated the strong generalisation ability of ViMo to unseen Apps, along with its robust performance in online navigation tasks under real-time environment interaction. These results highlighted the value of ViMo for advancing GUI world models and set a new benchmark for future research.
在这项工作中，我们推出了 ViMo，这是一种新型的生成式视觉 GUI 世界模型，旨在预测视觉模态下的 App 观测结果，与当下的基于语言的模型相比，它提供了一种更真实、更具体的方法。为了应对 GUI 生成的独特挑战，ViMo 配备了符号文本表示（STR），通过将文本内容覆盖为占位符并将内容生成委派给 LLM，将文本内容生成简化为文本位置预测。这一创新确保了极高的视觉保真度，并避免了文本扭曲或模糊等伪影。通过广泛的实验，我们证明了 ViMo 能够生成视觉上合理且功能上有效的 GUI。值得注意的是，ViMo 将步进式动作预测准确率相对提升了 14.07%，强调了其增强 App 智能体决策能力的潜力。此外，现实世界实验证明了 ViMo 对未知 App 具有强大的泛化能力，以及在实时环境交互的在线导航任务中的稳健表现。这些结果凸显了 ViMo 对推动 GUI 世界模型发展的价值，并为未来的研究树立了新标杆。


## References
## 参考文献


[1] W. contributors. Large language model - Wikipedia, the free encyclopedia. [Online]. Available: urlhttps://en.wikipedia.org/wiki/Large_language_model, 2024. Accessed: 2024-11-25.
[1] W. contributors. Large language model - Wikipedia, the free encyclopedia. [Online]. Available: urlhttps://en.wikipedia.org/wiki/Large_language_model, 2024. Accessed: 2024-11-25.


[2] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large language model society. NeurIPS, 36:51991-52008, 2023.
[2] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large language model society. NeurIPS, 36:51991-52008, 2023.


[3] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. Tora: A tool-integrated reasoning agent for mathematical problem solving. In ICLR, 2023.
[3] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. Tora: A tool-integrated reasoning agent for mathematical problem solving. In ICLR, 2023.


[4] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: A large-scale dataset for android device control. NeurIPS, 36, 2024.
[4] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: A large-scale dataset for android device control. NeurIPS, 36, 2024.


[5] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeh Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: A dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024.
[5] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeh Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: A dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024.


[6] Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. arXiv preprint arXiv:2406.01014, 2024.
[6] Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. arXiv preprint arXiv:2406.01014, 2024.


[7] Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sungh-wan Kim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. arXiv preprint arXiv:2410.13232, 2024.
[7] Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sungh-wan Kim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. arXiv preprint arXiv:2410.13232, 2024.


[8] Yu Gu, Boyuan Zheng, Boyu Gou, Kai Zhang, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, and Yu Su. Is your llm secretly a world model of the internet? model-based planning for web agents. arXiv preprint arXiv:2411.06559, 2024.
[8] Yu Gu, Boyuan Zheng, Boyu Gou, Kai Zhang, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, and Yu Su. Is your llm secretly a world model of the internet? model-based planning for web agents. arXiv preprint arXiv:2411.06559, 2024.


[9] Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Gérard Dray, and Walid Maalej. On ai-inspired ui-design. arXiv preprint arXiv:2406.13631, 2024.
[9] Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Gérard Dray, and Walid Maalej. 论受AI启发的UI设计. arXiv preprint arXiv:2406.13631, 2024.


[10] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser-2: Unleashing the power of language models for text rendering. In ECCV, pages 386-402. Springer, 2024.
[10] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser-2: 释放语言模型在文本渲染中的力量. In ECCV, pages 386-402. Springer, 2024.


[11] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, pages 18392-18402, 2023.
[11] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: 学习遵循图像编辑指令. In CVPR, pages 18392-18402, 2023.


[12] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684-10695, 2022.
[12] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 基于潜在扩散模型的高分辨率图像合成. In CVPR, pages 10684-10695, 2022.


[13] Lingjun Zhang, Xinyuan Chen, Yaohui Wang, Yue Lu, and Yu Qiao. Brush your text: Synthesize any scene text on images via diffusion model. In AAAI, volume 38, pages 7215-7223, 2024.
[13] Lingjun Zhang, Xinyuan Chen, Yaohui Wang, Yue Lu, and Yu Qiao. Brush your text: 通过扩散模型在图像上合成任意场景文本. In AAAI, volume 38, pages 7215-7223, 2024.


[14] Hao Wen, Shizuo Tian, Borislav Pavlov, Wenjie Du, Yixuan Li, Ge Chang, Shanhui Zhao, Jiacheng Liu, Yunxin Liu, Ya-Qin Zhang, et al. Autodroid-v2: Boosting slm-based gui agents via code generation. arXiv preprint arXiv:2412.18116, 2024.
[14] Hao Wen, Shizuo Tian, Borislav Pavlov, Wenjie Du, Yixuan Li, Ge Chang, Shanhui Zhao, Jiacheng Liu, Yunxin Liu, Ya-Qin Zhang, et al. Autodroid-v2: 通过代码生成增强基于SLM的GUI智能体. arXiv preprint arXiv:2412.18116, 2024.


[15] Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, et al. Spa-bench: A comprehensive benchmark for smartphone agent evaluation. In NeurIPS 2024 Workshop on Open-World Agents, 2024.
[15] Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, et al. Spa-bench: 智能手机智能体评估的综合基准. In NeurIPS 2024 Workshop on Open-World Agents, 2024.


[16] Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, et al. Large language model-brained gui agents: A survey. arXiv preprint arXiv:2411.18279, 2024.
[16] Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, et al. 以大语言模型为大脑的GUI智能体：综述. arXiv preprint arXiv:2411.18279, 2024.


[17] Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents. arXiv preprint arXiv:2403.02713, 2024.
[17] Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: 用于GUI智能体的动作思维链. arXiv preprint arXiv:2403.02713, 2024.


[18] Sunjae Lee, Junyoung Choi, Jungjae Lee, Munim Hasan Wasi, Hojun Choi, Steven Y Ko, Sangeun Oh, and Insik Shin. Explore, select, derive, and recall: Augmenting llm with human-like memory for mobile task automation. arXiv preprint arXiv:2312.03003, 2023.
[18] Sunjae Lee, Junyoung Choi, Jungjae Lee, Munim Hasan Wasi, Hojun Choi, Steven Y Ko, Sangeun Oh, and Insik Shin. 探索、选择、派生与召回：为移动任务自动化增强类人记忆的LLM. arXiv preprint arXiv:2312.03003, 2023.


[19] Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. Gui agents: A survey. arXiv preprint arXiv:2412.13501, 2024.
[19] Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. GUI智能体：综述. arXiv preprint arXiv:2412.13501, 2024.


[20] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Autodroid: Llm-powered task automation in android. In MobiCom, pages 543-557, 2024.
[20] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Autodroid: Android中由LLM驱动的任务自动化. In MobiCom, pages 543-557, 2024.


[21] Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents. arXiv preprint arXiv:2406.03679, 2024.
[21] Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. 论数据规模对计算机控制智能体的影响. arXiv preprint arXiv:2406.03679, 2024.


[22] Filippos Christianos, Georgios Papoudakis, Thomas Coste, Jianye Hao, Jun Wang, and Kun Shao. Lightweight neural app control. arXiv preprint arXiv:2410.17883, 2024.
[22] Filippos Christianos, Georgios Papoudakis, Thomas Coste, Jianye Hao, Jun Wang, and Kun Shao. 轻量级神经应用控制. arXiv preprint arXiv:2410.17883, 2024.


[23] Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, and Kun Shao. Distrl: An asynchronous distributed reinforcement learning framework for on-device control agents. arXiv preprint arXiv:2410.14803, 2024.
[23] Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, and Kun Shao. Distrl: 一种用于端侧控制智能体的异步分布式强化学习框架. arXiv preprint arXiv:2410.14803, 2024.


[24] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024.
[24] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: 自主人工智能智能体的高级推理与学习. arXiv preprint arXiv:2408.07199, 2024.


[25] Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents. arXiv preprint arXiv:2407.01476, 2024.
[25] Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. 语言模型智能体的树搜索. arXiv preprint arXiv:2407.01476, 2024.


[26] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):1-62, 2022.
[26] Yann LeCun. 迈向自主机器智能之路 0.9. 2 版, 2022-06-27. Open Review, 62(1):1-62, 2022.


[27] Jingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Zefang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li, Nicholas Sukiennik, et al. Understanding world or predicting future? a comprehensive survey of world models. arXiv preprint arXiv:2411.14499, 2024.
[27] Jingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Zefang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li, Nicholas Sukiennik, et al. 理解世界还是预测未来？世界模型综合调查. arXiv preprint arXiv:2411.14499, 2024.


[28] OpenAI. Sora: Creating video from text., 2024.
[28] OpenAI. Sora: 从文本创建视频., 2024.


[29] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024.
[29] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. 扩散模型即实时游戏引擎. arXiv preprint arXiv:2408.14837, 2024.


[30] Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, Sebastien Racanière, David Reichert, Théophane Weber, Daan Wierstra, and Peter Battaglia. Learning model-based planning from scratch. arXiv preprint arXiv:1707.06170, 2017.
[30] Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, Sebastien Racanière, David Reichert, Théophane Weber, Daan Wierstra, and Peter Battaglia. 从零开始学习基于模型的规划. arXiv preprint arXiv:1707.06170, 2017.


[31] Chang Yang, Xinrun Wang, Junzhe Jiang, Qinggang Zhang, and Xiao Huang. Evaluating world models with llm for decision making. arXiv preprint arXiv:2411.08794, 2024.
[31] Chang Yang, Xinrun Wang, Junzhe Jiang, Qinggang Zhang, and Xiao Huang. 利用大语言模型评估用于决策的世界模型. arXiv preprint arXiv:2411.08794, 2024.


[32] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.
[32] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. 通过学习模型规划掌握雅达利、围棋、象棋和将棋. Nature, 588(7839):604-609, 2020.


[33] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.
[33] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. 掌控梦境：通过潜在想象学习行为. arXiv preprint arXiv:1912.01603, 2019.


[34] Anthony Liu, Lajanugen Logeswaran, Sungryull Sohn, and Honglak Lee. A picture is worth a thousand words: Language models plan from pixels. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 16450-16459, 2023.
[34] Anthony Liu, Lajanugen Logeswaran, Sungryull Sohn, and Honglak Lee. 一图胜千言：语言模型从像素进行规划. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 16450-16459, 2023.


[35] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In CVPR, pages 1931-1941, 2023.
[35] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 文本到图像扩散的多概念定制. In CVPR, pages 1931-1941, 2023.


[36] Yu Cao and Shaogang Gong. Few-shot image generation by conditional relaxing diffusion inversion. In ECCV, pages 20-37. Springer, 2024.
[36] Yu Cao and Shaogang Gong. 通过条件松弛扩散反演进行少样本图像生成. In ECCV, pages 20-37. Springer, 2024.


[37] Yuwen Lu, Ziang Tong, Qinyi Zhao, Chengzhi Zhang, and Toby Jia-Jun Li. Ui layout generation with llms guided by ui grammar. arXiv preprint arXiv:2310.15455, 2023.
[37] Yuwen Lu, Ziang Tong, Qinyi Zhao, Chengzhi Zhang, and Toby Jia-Jun Li. 由 UI 语法引导的大语言模型 UI 布局生成. arXiv preprint arXiv:2310.15455, 2023.


[38] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In CVPR, pages 22490-22499, 2023.
[38] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: 用于布局到图像生成的可控扩散模型. In CVPR, pages 22490-22499, 2023.


[39] Andrey Sobolevsky, Guillaume-Alexandre Bilodeau, Jinghui Cheng, and Jin LC Guo. Guilget: Gui layout generation with transformer. arXiv preprint arXiv:2304.09012, 2023.
[39] Andrey Sobolevsky, Guillaume-Alexandre Bilodeau, Jinghui Cheng, and Jin LC Guo. Guilget: 基于 Transformer 的 GUI 布局生成. arXiv 预印本 arXiv:2304.09012, 2023.


[40] Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image generation from layout. In CVPR, pages 8584-8593, 2019.
[40] Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. 基于布局的图像生成. In CVPR, pages 8584-8593, 2019.


[41] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. NeurIPS, 36, 2024.
[41] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: 作为文本绘制器的扩散模型. NeurIPS, 36, 2024.


[42] Weichao Zeng, Yan Shu, Zhenhang Li, Dongbao Yang, and Yu Zhou. Textctrl: Diffusion-based scene text editing with prior guidance control. arXiv preprint arXiv:2410.10133, 2024.
[42] Weichao Zeng, Yan Shu, Zhenhang Li, Dongbao Yang, and Yu Zhou. Textctrl: 基于先验引导控制的扩散模型场景文本编辑. arXiv 预印本 arXiv:2410.10133, 2024.


[43] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. IEEE transactions on pattern analysis and machine intelligence, 39(11):2298-2304, 2016.
[43] Baoguang Shi, Xiang Bai, and Cong Yao. 一种用于基于图像序列识别的端到端可训练神经网络及其在场景文本识别中的应用. IEEE transactions on pattern analysis and machine intelligence, 39(11):2298-2304, 2016.


[44] Zhi Qiao, Yu Zhou, Dongbao Yang, Yucan Zhou, and Weiping Wang. Seed: Semantics enhanced encoder-decoder framework for scene text recognition. In CVPR, pages 13528-13537, 2020.
[44] Zhi Qiao, Yu Zhou, Dongbao Yang, Yucan Zhou, and Weiping Wang. Seed: 用于场景文本识别的语义增强编码器-解码器框架. In CVPR, pages 13528-13537, 2020.


[45] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
[45] Diederik P Kingma and Max Welling. 自动编码变分贝叶斯. arXiv 预印本 arXiv:1312.6114, 2013.


[46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234-241. Springer, 2015.
[46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: 用于生物医学图像分割的卷积网络. In Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234-241. Springer, 2015.


[47] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welhinda, Alan Hayes, Alec Radford, et al. Gpt-40 system card. arXiv preprint arXiv:2410.21276, 2024.
[47] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welhinda, Alan Hayes, Alec Radford, et al. Gpt-40 系统卡. arXiv 预印本 arXiv:2410.21276, 2024.


[48] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In CVPR, pages 9650-9660, 2021.
[48] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 自监督视觉 Transformer 中的新兴特性. In CVPR, pages 9650-9660, 2021.


[49] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023.
[49] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: 作为智能手机用户的多模态智能体. arXiv 预印本 arXiv:2312.13771, 2023.


[50] Demis Hassabis and Koray Kavukcuoglu. Introducing gemini 2.0: our new ai model for the agentic era, December 2024. Accessed: 2025-05-04.
[50] Demis Hassabis and Koray Kavukcuoglu. 介绍 Gemini 2.0：我们面向智能体时代的新 AI 模型, 2024年12月. 访问日期: 2025-05-04.


[51] Yisroel Mirsky and Wenke Lee. The creation and detection of deepfakes: A survey. ACM computing surveys (CSUR), 54(1):1-41, 2021.
[51] Yisroel Mirsky and Wenke Lee. Deepfake 的创建与检测：综述. ACM computing surveys (CSUR), 54(1):1-41, 2021.


In this Appendix, we first provide detailed explanations, including prompts related to our methods, descriptions of our STR dataset, and evaluation details. Then, we present additional experimental results. Finally, we present additional visualisations of our proposed ViMo for GUI generation.
在本附录中，我们首先提供详细说明，包括与方法相关的提示词、STR 数据集的描述以及评估细节。随后，我们展示额外的实验结果。最后，我们呈现所提 ViMo 在 GUI 生成方面的补充可视化结果。


## A Experimental Details
## A 实验详情


### A.1 GUI-text Predictor
### A.1 GUI 文本预测器


This subsection elaborates on the design and functionality of the GUI-text predictor, summarising its key components and providing a detailed explanation of its underlying processes.
本小节详细阐述了 GUI 文本预测器的设计与功能，总结了其核心组件并对其底层过程进行了详细解释。


Given a STR prediction, the GUI-text predictor starts by locating the text symbols. To be specific, we first detect black borders by identifying black pixels in the BGR colour space, generating a binary mask that indicates whether a pixel is black or not. A pixel is classified as black if its BGR values fall within the range $\left\lbrack  {0,0,0}\right\rbrack$ to $\left\lbrack  {{50},{50},{50}}\right\rbrack$ . Next,we identify rectangular regions within this mask by computing the ratio of the actual contour area to its corresponding bounding rectangle area. If this ratio exceeds 0.8 , the region is considered a valid rectangle, allowing us to extract rectangles with black borders. For these detected regions, we further analyse their internal colour distribution to determine whether they contain the desired white colour. Specifically, we define white pixels as those with BGR values within the range [200,200,200] to [255,255,255]. If more than 50% of the pixels within a region fall within this range, the region is classified as a text symbol. Thus, the locations of text symbols are extracted, and we assign a unique identifier (ID) to each symbol through enumeration.
给定 STR 预测后，GUI 文本预测器首先定位文本符号。具体而言，我们首先通过识别 BGR 颜色空间中的黑色像素来检测黑色边框，生成指示像素是否为黑色的二值掩码。若像素的 BGR 值落在 $\left\lbrack  {0,0,0}\right\rbrack$ 到 $\left\lbrack  {{50},{50},{50}}\right\rbrack$ 范围内，则被归类为黑色。接着，我们通过计算实际轮廓面积与其对应外接矩形面积的比值来识别该掩码中的矩形区域。若该比值超过 0.8，则该区域被视为有效矩形，从而使我们能够提取具有黑色边框的矩形。对于这些检测到的区域，我们进一步分析其内部颜色分布，以确定其是否包含所需的白色。具体地，我们将白色像素定义为 BGR 值在 [200,200,200] 到 [255,255,255] 之间的像素。若一个区域内超过 50% 的像素落在此范围内，则该区域被归类为文本符号。至此，文本符号的位置被提取出来，我们通过枚举为每个符号分配一个唯一标识符 (ID)。


Building on this,we take as inputs the current GUI image ${x}_{k}$ ,an action $a$ to be applied to this image, the predicted STR ( ${\operatorname{STR}}_{{x}_{k + 1}^{a}}$ ),the location and unique ID token of the text symbols in the STR $\mathcal{T}$ . Then we leverage an LLM to predict the text content for each text symbol. The process begins with preprocessing the STR by overlaying the ID token for each text symbol to the corresponding position in the STR image,resulting in a modified representation denoted as ${\operatorname{STR}}_{k + 1}^{ID}$ . Next,we prompt an LLM to identify which text symbols will remain unchanged after the action $a$ (see the prompt in Subsection G.1). These symbols are determined to not be affected by the action and have content identical to the previous GUI ${x}_{k}$ . Based on the resulting ID list,we retrieve the corresponding pixels from the previous GUI ${x}_{k}$ based on their location and update the STR representation. The updated image is still referred to as ${\mathrm{{STR}}}_{k + 1}^{ID}$ for simplicity.
在此基础上，我们将当前 GUI 图像 ${x}_{k}$、要应用于该图像的动作 $a$、预测的 STR (${\operatorname{STR}}_{{x}_{k + 1}^{a}}$)、以及 STR 中文本符号的位置和唯一 ID 令牌 $\mathcal{T}$ 作为输入。然后，我们利用 LLM 来预测每个文本符号的文本内容。该过程首先对 STR 进行预处理，将每个文本符号的 ID 令牌叠加到 STR 图像中的相应位置，从而得到表示为 ${\operatorname{STR}}_{k + 1}^{ID}$ 的修改后表示。接着，我们提示 LLM 识别哪些文本符号在动作 $a$ 后将保持不变（参见 G.1 小节中的提示词）。这些符号被确定为不受动作影响，且内容与之前的 GUI ${x}_{k}$ 相同。根据生成的 ID 列表，我们根据位置从之前的 GUI ${x}_{k}$ 中检索相应的像素并更新 STR 表示。为简化起见，更新后的图像仍称为 ${\mathrm{{STR}}}_{k + 1}^{ID}$。


Subsequently, the LLM is prompted to determine the semantic role of each text symbol by analysing its context (see the prompt in Subsection G.2). This semantic information,combined with ${\operatorname{STR}}_{k + 1}^{ID}$ , is then used to predict the exact text content of each symbol (see the prompt in Subsection G.3).
随后，提示 LLM 通过分析上下文来确定每个文本符号的语义角色（参见 G.2 小节中的提示词）。该语义信息结合 ${\operatorname{STR}}_{k + 1}^{ID}$，随后被用于预测每个符号的确切文本内容（参见 G.3 小节中的提示词）。


Finally, to overlay a symbol with its actual text content, we perform the following steps: 1) For a given text symbol's location and corresponding text, the average background colour is computed by the average colour of the area on the edge of text symbol's coordinates; 2) The text colour is set to either white or black to ensure optimal contrast with the background colour, for better visibility; 3) The font size is calculated as the maximum size that allows the text to fit entirely within the boundaries of the text symbol, ensuring optimal use of space and readability.
最后，为了将实际文本内容叠加到符号上，我们执行以下步骤：1) 对于给定的文本符号位置和相应文本，通过文本符号坐标边缘区域的平均颜色计算出平均背景颜色；2) 将文本颜色设置为白色或黑色，以确保与背景颜色的最佳对比度，从而提高可见性；3) 字体大小计算为允许文本完全容纳在文本符号边界内的最大尺寸，以确保空间利用和可读性的最优化。


### A.2 Action Selection
### A.2 动作选择


In practice, our selection model, described in Section 3.3, identifies the best action in two steps. First, we query an LLM to evaluate all the action options, providing a judgment -either valid or invalid-and a confidence score for each action (see the prompt in Subsection G.4). These judgments are transformed into scores: if an action is judged valid, its score equals the confidence; if judged invalid, its score is the confidence multiplied by -1 . This scoring reflects that higher confidence in a valid action yields a higher score, while higher confidence in an invalid action results in a lower (negative) score. Second, we query the LLM again to select the best action from the two highest-scoring actions (see the prompt in Subsection G.5). This step is motivated by our observation that, in over 70% of tasks, the difference between the top two scores is equal to or less than 0.1, indicating that both are likely optimal. By allowing the LLM to choose between them, we refine the selection beyond simply picking the action with the highest score.
在实践中，我们 3.3 节所述的选择模型分两步确定最佳动作。首先，我们查询 LLM 以评估所有动作选项，并为每个动作提供判断（有效或无效）及置信度分数（参见 G.4 小节中的提示词）。这些判断被转化为分数：如果一个动作被判定为有效，其分数等于置信度；如果被判定为无效，其分数为置信度乘以 -1。这种评分方式反映出，有效动作的置信度越高，分数越高；而无效动作的置信度越高，分数越低（负值）。其次，我们再次查询 LLM，从得分最高的两个动作中选出最佳动作（参见 G.5 小节中的提示词）。这一步骤源于我们的观察：在超过 70% 的任务中，前两名分数之间的差异等于或小于 0.1，表明两者都可能是最优的。通过让 LLM 在两者之间做出选择，我们实现了比简单选取最高分动作更精细的选择。


Table 8: Summarisation of our STR dataset.
表 8：我们的 STR 数据集摘要。


<table><tr><td>Split</td><td>App</td><td>Episode</td><td>Image</td><td>Instrucion</td></tr><tr><td>Train</td><td>19</td><td>2853</td><td>19010</td><td>14852</td></tr><tr><td>Val</td><td>19</td><td>349</td><td>2290</td><td>1774</td></tr><tr><td>Test</td><td>19</td><td>348</td><td>2320</td><td>1824</td></tr><tr><td>All</td><td>19</td><td>3550</td><td>23620</td><td>18450</td></tr></table>
<table><tbody><tr><td>拆分</td><td>应用</td><td>剧集</td><td>图像</td><td>指令</td></tr><tr><td>训练集</td><td>19</td><td>2853</td><td>19010</td><td>14852</td></tr><tr><td>验证集</td><td>19</td><td>349</td><td>2290</td><td>1774</td></tr><tr><td>测试集</td><td>19</td><td>348</td><td>2320</td><td>1824</td></tr><tr><td>全部</td><td>19</td><td>3550</td><td>23620</td><td>18450</td></tr></tbody></table>


Table 9: Decision optimisation comparisons on APP agent performance. Apps are categorised into "Leisure", "Work", and "System".
表 9：APP 智能体性能的决策优化对比。应用分为“休闲”、“工作”和“系统”三类。


<table><tr><td>App Agent</td><td>World Model Modality</td><td>World Model</td><td>Leisure</td><td>Work</td><td>System</td><td>Overall</td></tr><tr><td rowspan="7">T3A</td><td>w/o world model</td><td>w/o world model</td><td>41.22</td><td>51.28</td><td>42.86</td><td>43.13</td></tr><tr><td rowspan="2">Langugae</td><td>Change-text</td><td>49.32</td><td>51.28</td><td>42.06</td><td>46.65</td></tr><tr><td>HTML-text</td><td>47.30</td><td>48.72</td><td>43.65</td><td>46.01</td></tr><tr><td rowspan="4">Vision</td><td>HTML-vision</td><td>50.68</td><td>53.85</td><td>43.65</td><td>48.24</td></tr><tr><td>UI-diffuser</td><td>48.65</td><td>53.85</td><td>43.65</td><td>47.28</td></tr><tr><td>IP2P</td><td>48.65</td><td>53.85</td><td>45.24</td><td>47.92</td></tr><tr><td>ViMo (Ours)</td><td>50.00</td><td>58.97</td><td>45.24</td><td>49.20</td></tr><tr><td rowspan="7">APP-Agnet</td><td>w/o world model</td><td>w/o world model</td><td>43.24</td><td>51.28</td><td>39.68</td><td>42.81</td></tr><tr><td rowspan="2">Langugae</td><td>Change-text</td><td>45.96</td><td>56.41</td><td>45.24</td><td>46.96</td></tr><tr><td>HTML-text</td><td>44.59</td><td>56.41</td><td>45.24</td><td>46.33</td></tr><tr><td rowspan="4">Vision</td><td>HTML-vision</td><td>47.97</td><td>56.41</td><td>46.03</td><td>48.24</td></tr><tr><td>UI-diffuser</td><td>47.30</td><td>56.41</td><td>44.44</td><td>47.28</td></tr><tr><td>IP2P</td><td>47.30</td><td>58.97</td><td>45.24</td><td>47.92</td></tr><tr><td>ViMo (Ours)</td><td>50.68</td><td>58.97</td><td>43.65</td><td>48.89</td></tr><tr><td rowspan="7">Mobile-Agent-v2</td><td>w/o world model</td><td>w/o world model</td><td>43.92</td><td>53.85</td><td>39.68</td><td>43.45</td></tr><tr><td rowspan="2">Langugae</td><td>Change-text</td><td>47.30</td><td>66.67</td><td>41.27</td><td>47.28</td></tr><tr><td>HTML-text</td><td>47.30</td><td>66.67</td><td>38.89</td><td>46.33</td></tr><tr><td rowspan="4">Vision</td><td>HTML-vision</td><td>50.00</td><td>66.67</td><td>41.27</td><td>48.56</td></tr><tr><td>UI-diffuser</td><td>49.32</td><td>61.54</td><td>41.27</td><td>47.60</td></tr><tr><td>IP2P</td><td>46.62</td><td>66.67</td><td>45.24</td><td>48.56</td></tr><tr><td>ViMo (Ours)</td><td>50.00</td><td>66.67</td><td>44.44</td><td>49.84</td></tr><tr><td rowspan="7">M3A</td><td>w/o world model</td><td>w/o world model</td><td>46.62</td><td>51.28</td><td>43.65</td><td>46.01</td></tr><tr><td rowspan="2">Langugae</td><td>Change-text</td><td>51.35</td><td>51.28</td><td>41.27</td><td>47.28</td></tr><tr><td>HTML-text</td><td>50.68</td><td>51.28</td><td>40.48</td><td>46.65</td></tr><tr><td rowspan="4">Vision</td><td>HTML-vision</td><td>52.03</td><td>48.72</td><td>45.24</td><td>48.89</td></tr><tr><td>UI-diffuser</td><td>50.00</td><td>48.72</td><td>44.44</td><td>47.60</td></tr><tr><td>IP2P</td><td>52.03</td><td>48.72</td><td>44.44</td><td>48.56</td></tr><tr><td>ViMo (Ours)</td><td>53.38</td><td>53.85</td><td>45.24</td><td>50.16</td></tr></table>
<table><tbody><tr><td>App 智能体</td><td>世界模型模态</td><td>世界模型</td><td>休闲</td><td>工作</td><td>系统</td><td>综合</td></tr><tr><td rowspan="7">T3A</td><td>无世界模型</td><td>无世界模型</td><td>41.22</td><td>51.28</td><td>42.86</td><td>43.13</td></tr><tr><td rowspan="2">语言</td><td>文本变更</td><td>49.32</td><td>51.28</td><td>42.06</td><td>46.65</td></tr><tr><td>HTML 文本</td><td>47.30</td><td>48.72</td><td>43.65</td><td>46.01</td></tr><tr><td rowspan="4">视觉</td><td>HTML 视觉</td><td>50.68</td><td>53.85</td><td>43.65</td><td>48.24</td></tr><tr><td>UI-diffuser</td><td>48.65</td><td>53.85</td><td>43.65</td><td>47.28</td></tr><tr><td>IP2P</td><td>48.65</td><td>53.85</td><td>45.24</td><td>47.92</td></tr><tr><td>ViMo (本文)</td><td>50.00</td><td>58.97</td><td>45.24</td><td>49.20</td></tr><tr><td rowspan="7">APP-Agnet</td><td>无世界模型</td><td>无世界模型</td><td>43.24</td><td>51.28</td><td>39.68</td><td>42.81</td></tr><tr><td rowspan="2">语言</td><td>文本变更</td><td>45.96</td><td>56.41</td><td>45.24</td><td>46.96</td></tr><tr><td>HTML 文本</td><td>44.59</td><td>56.41</td><td>45.24</td><td>46.33</td></tr><tr><td rowspan="4">视觉</td><td>HTML 视觉</td><td>47.97</td><td>56.41</td><td>46.03</td><td>48.24</td></tr><tr><td>UI-diffuser</td><td>47.30</td><td>56.41</td><td>44.44</td><td>47.28</td></tr><tr><td>IP2P</td><td>47.30</td><td>58.97</td><td>45.24</td><td>47.92</td></tr><tr><td>ViMo (本文)</td><td>50.68</td><td>58.97</td><td>43.65</td><td>48.89</td></tr><tr><td rowspan="7">Mobile-Agent-v2</td><td>无世界模型</td><td>无世界模型</td><td>43.92</td><td>53.85</td><td>39.68</td><td>43.45</td></tr><tr><td rowspan="2">语言</td><td>文本变更</td><td>47.30</td><td>66.67</td><td>41.27</td><td>47.28</td></tr><tr><td>HTML 文本</td><td>47.30</td><td>66.67</td><td>38.89</td><td>46.33</td></tr><tr><td rowspan="4">视觉</td><td>HTML 视觉</td><td>50.00</td><td>66.67</td><td>41.27</td><td>48.56</td></tr><tr><td>UI-diffuser</td><td>49.32</td><td>61.54</td><td>41.27</td><td>47.60</td></tr><tr><td>IP2P</td><td>46.62</td><td>66.67</td><td>45.24</td><td>48.56</td></tr><tr><td>ViMo (本文)</td><td>50.00</td><td>66.67</td><td>44.44</td><td>49.84</td></tr><tr><td rowspan="7">M3A</td><td>无世界模型</td><td>无世界模型</td><td>46.62</td><td>51.28</td><td>43.65</td><td>46.01</td></tr><tr><td rowspan="2">语言</td><td>文本变更</td><td>51.35</td><td>51.28</td><td>41.27</td><td>47.28</td></tr><tr><td>HTML 文本</td><td>50.68</td><td>51.28</td><td>40.48</td><td>46.65</td></tr><tr><td rowspan="4">视觉</td><td>HTML 视觉</td><td>52.03</td><td>48.72</td><td>45.24</td><td>48.89</td></tr><tr><td>UI-diffuser</td><td>50.00</td><td>48.72</td><td>44.44</td><td>47.60</td></tr><tr><td>IP2P</td><td>52.03</td><td>48.72</td><td>44.44</td><td>48.56</td></tr><tr><td>ViMo (本文)</td><td>53.38</td><td>53.85</td><td>45.24</td><td>50.16</td></tr></tbody></table>


### A.3 Data Collection
### A.3 数据收集


To ensure the quality and diversity of data samples for each App, while minimising noise, we collected App information from both Android Control [21] and Android in the Wild dataset (AITW) [4] datasets. To be specific, out of 15,274 episodes in the Android Control, only 5,697 episodes include the "open_app" action. From these episodes, we extracted their "app_name", identifying 758 unique applications. However, only 13 of these Apps had more than 50 samples. To enrich the dataset, we manually collected additional samples for these 13 Apps from the rest of the dataset. For AITW, we extracted App names by using the package name listed under the "current activity" field. After filtering out the noisy, 11 valid Apps remained. By combining the overlapping applications from both datasets, we obtained a total of 19 unique Apps. We split our dataset into "Train", "Validation" and "Test" splits, and we summarise our dataset under each split in Table 8
为了确保每个 App 数据样本的质量和多样性，同时尽量减少噪声，我们从 Android Control [21] 和 Android in the Wild (AITW) [4] 数据集中收集了 App 信息。具体而言，在 Android Control 的 15,274 个片段中，仅有 5,697 个包含 "open_app" 动作。我们从这些片段中提取了 "app_name"，识别出 758 个唯一的应用程序。然而，其中只有 13 个 App 的样本数超过 50 个。为了丰富数据集，我们从该数据集的其余部分中手动为这 13 个 App 收集了额外样本。对于 AITW，我们通过使用 "current activity" 字段下的包名来提取 App 名称。在过滤掉噪声后，剩余 11 个有效 App。通过合并两个数据集中重叠的应用，我们总共获得了 19 个唯一的 App。我们将数据集划分为“训练集”、“验证集”和“测试集”，并在表 8 中总结了各划分下的数据集情况。


Furthermore, we converted action commands into action instructions for AITW with specific prompts in Subsection G.6. We use Paddleocr [43] for STR generation.
此外，我们根据 G.6 小节中的特定提示，将 AITW 的动作命令转换为动作指令。我们使用 Paddleocr [43] 进行 STR 生成。


### A.4 Evaluation
### A.4 评估


World Model Ability. For the results under automatic metrics presented in Table 1 we prompt LLM for the instructional accuracy score ${s}_{ia}$ and action readiness score ${s}_{ar}$ ,as shown in Subsection G. 7 and Subsection G.8 respectively. A generation is considered successful if "success" appears under "Status" for ${s}_{ia}$ and "yes" under "ready for action" for ${s}_{ar}$ . For the user study,we collected 80 generated samples—20 from each of the four world models. We then asked 70 participants to answer three questions on each sample designed to reflect the ${s}_{ia},{s}_{gc}$ and ${s}_{ar}$ scores,as detailed in
世界模型能力。对于表 1 中自动指标的结果，我们提示 LLM 获取指令准确率得分 ${s}_{ia}$ 和动作就绪得分 ${s}_{ar}$，分别如 G.7 和 G.8 小节所示。如果 ${s}_{ia}$ 的 "Status" 下出现 "success" 且 ${s}_{ar}$ 的 "ready for action" 下出现 "yes"，则认为生成成功。对于用户研究，我们收集了 80 个生成样本——四个世界模型各 20 个。随后，我们要求 70 名参与者针对每个样本回答三个问题，这些问题旨在反映 ${s}_{ia},{s}_{gc}$ 和 ${s}_{ar}$ 得分，详见


Table 10: Trajectory synthesis evaluation. "T+L" denotes the accuracy of the whole trajectory with length L.
表 10：轨迹合成评估。“T+L”表示长度为 L 的整个轨迹的准确率。


<table><tr><td>World Model</td><td>T+1</td><td>T+2</td><td>T+3</td><td>T+4</td></tr><tr><td>w/o world model</td><td>22.81</td><td>14.04</td><td>7.02</td><td>0</td></tr><tr><td>Change-text</td><td>52.63</td><td>26.32</td><td>10.53</td><td>5.26</td></tr><tr><td>HTML-text</td><td>38.60</td><td>14.04</td><td>12.28</td><td>7.02</td></tr><tr><td>HTML-vision</td><td>43.86</td><td>19.30</td><td>10.53</td><td>10.53</td></tr><tr><td>UI-diffuser</td><td>52.63</td><td>29.82</td><td>12.28</td><td>5.26</td></tr><tr><td>IP2P*</td><td>56.14</td><td>21.05</td><td>10.53</td><td>7.02</td></tr><tr><td>ViMo (Ours)</td><td>57.89</td><td>36.84</td><td>14.03</td><td>12.28</td></tr></table>
<table><tbody><tr><td>世界模型</td><td>T+1</td><td>T+2</td><td>T+3</td><td>T+4</td></tr><tr><td>无世界模型</td><td>22.81</td><td>14.04</td><td>7.02</td><td>0</td></tr><tr><td>文本修改</td><td>52.63</td><td>26.32</td><td>10.53</td><td>5.26</td></tr><tr><td>HTML文本</td><td>38.60</td><td>14.04</td><td>12.28</td><td>7.02</td></tr><tr><td>HTML视觉</td><td>43.86</td><td>19.30</td><td>10.53</td><td>10.53</td></tr><tr><td>UI扩散器</td><td>52.63</td><td>29.82</td><td>12.28</td><td>5.26</td></tr><tr><td>IP2P*</td><td>56.14</td><td>21.05</td><td>10.53</td><td>7.02</td></tr><tr><td>ViMo (本文方法)</td><td>57.89</td><td>36.84</td><td>14.03</td><td>12.28</td></tr></tbody></table>


Table 11: Evaluation on randomness by running the experiment 3 times (r1-r3) on our sampled test split. "All" denotes the evaluation of the full test split. ${s}_{qc},{s}_{ia}$ and ${s}_{ar}$ are the metrics same with Table 1. ${s}_{h}$ denotes their harmonic score. STD denote the standard deviation from r1 to r3.
表 11：在采样测试集上运行 3 次实验（r1-r3）的随机性评估。“All”表示完整测试集的评估结果。${s}_{qc},{s}_{ia}$ 与 ${s}_{ar}$ 为与表 1 相同的指标。${s}_{h}$ 表示其调和分。STD 表示 r1 到 r3 的标准差。


<table><tr><td>World Model</td><td>${s}_{gc}$</td><td>Sia</td><td>Sar</td><td>${s}_{h}$</td></tr><tr><td>r1</td><td>0.7421</td><td>75.08</td><td>78.29</td><td>0.7582</td></tr><tr><td>r2</td><td>0.7323</td><td>75.63</td><td>77.64</td><td>0.7546</td></tr><tr><td>r3</td><td>0.7423</td><td>75.39</td><td>78.68</td><td>0.7605</td></tr><tr><td>STD</td><td>0.0057</td><td>0.23</td><td>0.42</td><td>0.0025</td></tr><tr><td>ALL</td><td>0.7389</td><td>75.37</td><td>78.20</td><td>0.7578</td></tr></table>
<table><tbody><tr><td>世界模型</td><td>${s}_{gc}$</td><td>希雅</td><td>萨尔</td><td>${s}_{h}$</td></tr><tr><td>r1</td><td>0.7421</td><td>75.08</td><td>78.29</td><td>0.7582</td></tr><tr><td>r2</td><td>0.7323</td><td>75.63</td><td>77.64</td><td>0.7546</td></tr><tr><td>r3</td><td>0.7423</td><td>75.39</td><td>78.68</td><td>0.7605</td></tr><tr><td>标准</td><td>0.0057</td><td>0.23</td><td>0.42</td><td>0.0025</td></tr><tr><td>全部</td><td>0.7389</td><td>75.37</td><td>78.20</td><td>0.7578</td></tr></tbody></table>


Subsection G.9 For the ${s}_{gc}$ ,participants are asked to rate on a scale from 1 to 5 . These scores were then normalised to the $\left\lbrack  {0,1}\right\rbrack$ range in Table 1
分节 G.9 对于 ${s}_{gc}$，参与者被要求按 1 到 5 的等级评分。这些得分随后被归一化到表 1 中的 $\left\lbrack  {0,1}\right\rbrack$ 范围


World Model Enhanced App Agent. In Table 2, we categorised APPs based on their primary functions into three groups: Leisure, Work, and System. The Leisure category includes APPs commonly used for relaxation and entertainment, such as Decathlon, eBay, Flipkart, Amazon, Adidas, Kitchen Stories, Booking.com, YouTube, and Vimeo. The Work category comprises APPs typically associated with professional or productivity-related activities, including Gmail, Drive, and Chrome. Lastly, the System category encompasses APPs pre-installed in the Android operating system, such as com.android.contacts, com.google.android.dialer, com.google.android.googlequicksearchbox, com.android.settings, com.google.android.APPs.maps, and com.android.vending.
世界模型增强的 App Agent。在表 2 中，我们根据主要功能将 APP 分为三组：休闲、工作和系统。休闲类包括通常用于放松和娱乐的 APP，如 Decathlon、eBay、Flipkart、Amazon、Adidas、Kitchen Stories、Booking.com、YouTube 和 Vimeo。工作类包含通常与职业或生产力活动相关的 APP，包括 Gmail、Drive 和 Chrome。最后，系统类涵盖了 Android 操作系统中预装的 APP，如 com.android.contacts、com.google.android.dialer、com.google.android.googlequicksearchbox、com.android.settings、com.google.android.APPs.maps 和 com.android.vending。


Ablation on Iteration Numbers. ViMo predicts future GUI observations, which can be recursively fed back as input to simulate further into the future. Taking the generative GUI as the current GUI, an agent was prompted to generate the action instructions based on the user goal (see the prompt in Subsection G.10). Then the action instruction and the GUI were fed into ViMo to generate the next GUI. In this study, we defined the iteration number as the number of times ViMo was called. We only use the final output as the signals during the candidate action selection phase, guiding the final selection among potential actions.
迭代次数消融实验。ViMo 预测未来的 GUI 观测结果，这些结果可以递归地作为输入反馈，以模拟更远的未来。将生成的 GUI 作为当前 GUI，提示智能体根据用户目标生成操作指令（参见 G.10 节中的提示词）。然后将操作指令和 GUI 输入 ViMo 以生成下一个 GUI。在本研究中，我们将迭代次数定义为调用 ViMo 的次数。我们仅将最终输出作为候选操作选择阶段的信号，引导从潜在操作中做出最终选择。


## B Additional Experimental Results
## B 额外实验结果


Comparison with World Models. Table 3 compares our ViMo with existing world models under M3A App agent. To further highlight our superiority, Table 9 presents additional results of ViMo applied to T3A, APP-Agent, and Mobile-Agent-V2. One can see from the results that our method consistently outperforms existing world models, clearly demonstrating its superiority.
与世界模型的比较。表 3 将我们的 ViMo 与 M3A App agent 下的现有世界模型进行了比较。为了进一步突出我们的优势，表 9 展示了 ViMo 应用于 T3A、APP-Agent 和 Mobile-Agent-V2 的额外结果。从结果中可以看出，我们的方法一致优于现有的世界模型，清楚地证明了其优越性。


Generation Error Analysis. As discussed in Subsec. 4.5, our method ViMo can iteratively generate future GUIs. However, as the number of iterations increases, the accumulated error also grows. In addition to the evidence presented in Table 7, we conduct further experiments to analyse this iteration error and compare our approach with existing world models.
生成误差分析。如 4.5 节所述，我们的方法 ViMo 可以迭代生成未来的 GUI。然而，随着迭代次数的增加，累积误差也会增长。除了表 7 中呈现的证据外，我们还进行了进一步的实验来分析这种迭代误差，并将我们的方法与现有的世界模型进行比较。


To this end, we design a trajectory synthesis evaluation to assess how well the GUIs generated by ViMo align with those observations in real-world environments over longer iterations. In this setup, the generated GUI is leveraged as the input to an App agent to generate the subsequent action, with higher-quality trajectories indicating a GUI more aligned with the real-world environment. Specifically, the GUIs generated by ViMo serve as the observation input for the App agent, which generates actions aimed at achieving the user's goal. These output actions are then evaluated to reflect whether the GUI representations offer concrete and reliable information for action prediction. This process is repeated for $\mathrm{L}$ steps,and we calculate the success rate of the entire $\mathrm{L}$ -step trajectory.
为此，我们设计了一个轨迹合成评估，以衡量 ViMo 生成的 GUI 在较长迭代中与现实环境观测结果的吻合程度。在这种设置中，生成的 GUI 被用作 App agent 的输入以生成随后的操作，更高质量的轨迹表明 GUI 与真实环境更加一致。具体而言，ViMo 生成的 GUI 作为 App agent 的观测输入，由其生成旨在实现用户目标的操作。然后评估这些输出操作，以反映 GUI 表征是否为操作预测提供了具体且可靠的信息。此过程重复 $\mathrm{L}$ 步，我们计算整个 $\mathrm{L}$ 步轨迹的成功率。


<img src="https://cdn.noedgeai.com/bo_d5v247n7aajc73870dfg_15.jpg?x=308&y=196&w=1171&h=389&r=0"/>



Figure 4: Qualitative ablation studies. Left: Static text generation. (a) Generating static text via an LLM; (b) Preserving the original text in the image by rendering it as image pixels. Right: STR generation under two input formats—(a) action command and (b) action instruction.
图 4：定性消融研究。左：静态文本生成。(a) 通过 LLM 生成静态文本；(b) 通过将其渲染为图像像素来保留图像中的原始文本。右：两种输入格式下的 STR 生成——(a) 操作命令和 (b) 操作指令。


<img src="https://cdn.noedgeai.com/bo_d5v247n7aajc73870dfg_15.jpg?x=541&y=745&w=717&h=428&r=0"/>



Figure 5: GUI generation conditioned on a novel combination of current GUI observation and user action.
图 5：以当前 GUI 观测和用户操作的新颖组合为条件的 GUI 生成。


We employ an LLM as a judge to assess the alignment between the agent's simulated actions and the ground truth actions within a given trajectory. Specifically, an agent was prompted to generate the action instructions based on the given GUI and the user goal (see the prompt in Subsection G.10) and the LLM evaluated whether the simulated action lead to the same outcome as the ground truth action (see the prompt in Subsection G.11), a "yes" of the "Status" is calculated as a match.
我们聘请 LLM 作为裁判，评估在给定轨迹内智能体的模拟操作与真实操作（ground truth）之间的一致性。具体而言，提示智能体根据给定的 GUI 和用户目标生成操作指令（见 G.10 节中的提示词），由 LLM 评估模拟操作是否导致与真实操作相同的结果（见 G.11 节中的提示词），“状态（Status）”为“是”则计为匹配。


As shown in Table 10, we compared ViMo against both visual- and language-based world models and demonstrated that while performance decreases across all world models with more iterations, our model significantly outperformed the other methods by providing more accurate and reliable information. This was reflected in higher trajectory prediction accuracy, underscoring the ability of our model to generate GUIs that aligned with the real-world environment.
如表 10 所示，我们将 ViMo 与基于视觉和语言的世界模型进行了比较，并证明尽管所有世界模型的性能都随着迭代次数的增加而下降，但我们的模型通过提供更准确和可靠的信息显著优于其他方法。这体现在更高的轨迹预测准确率上，强调了我们的模型生成与真实环境一致的 GUI 的能力。


Randomness Study and Evaluation on Full Test Split. ViMo involves random factors, particularly from the use of LLMs. To evaluate their influence, we conducted the experiment three times, as summarised in Table 11 (r1-r3). The results demonstrate that the randomness does not significantly impact the performance or consistency of our method. Additionally, we focused on a randomly selected subset of examples for evaluation, with results from the full test set also included to illustrate that the observed differences are minor, as shown in Table 11 (compare ALL to r1-r3). We consider the subset results to provide an accurate and reliable approximation for our analysis.
全测试集上的随机性研究与评估。ViMo 涉及随机因素，特别是由于使用了 LLM。为了评估其影响，我们进行了三次实验，结果汇总于表 11 (r1-r3)。结果表明，随机性并未显著影响我们方法的性能或一致性。此外，我们重点对随机选择的样本子集进行了评估，并包含全测试集的结果以说明观察到的差异微乎其微，如表 11 所示（对比 ALL 与 r1-r3）。我们认为子集结果为分析提供了准确且可靠的近似。


Table 12: Inference time and step accuracy comparison across models.
表 12：各模型间的推理时间与步骤准确率对比。


<table><tr><td>Model</td><td>Inference Time</td><td>Step Accuracy (%)</td></tr><tr><td>Baseline (T3A)</td><td>$\sim  4$ minutes</td><td>43.13</td></tr><tr><td>Change-text</td><td>~5 seconds</td><td>46.64</td></tr><tr><td>IP2P*</td><td>~1.5 minutes</td><td>47.92</td></tr><tr><td>ViMo (Ours)</td><td>~2 minutes</td><td>49.20</td></tr></table>
<table><tbody><tr><td>模型</td><td>推理时间</td><td>单步准确率 (%)</td></tr><tr><td>基线 (T3A)</td><td>$\sim  4$ 分钟</td><td>43.13</td></tr><tr><td>文本修改</td><td>~5 秒</td><td>46.64</td></tr><tr><td>IP2P*</td><td>~1.5 分钟</td><td>47.92</td></tr><tr><td>ViMo (本文方法)</td><td>~2 分钟</td><td>49.20</td></tr></tbody></table>


<img src="https://cdn.noedgeai.com/bo_d5v247n7aajc73870dfg_16.jpg?x=306&y=523&w=1186&h=508&r=0"/>



Figure 6: False examples where the text symbols are incorrectly represented, making them unrecognizable to indicate the location of text.
图 6：文本符号表示错误的失败案例，导致其无法被识别为文本位置。


Qualitative Ablation Analysis. In addition to the quantitative ablation results presented in Table 6 we also provide qualitative comparisons. Fig. 4 (left) illustrates the challenges faced by the LLM in predicting static text under complex spatial layouts. Fig. 4 (right) displays the STR generation of the same user intent but with different action types. It demonstrated that models learned with action commands failed to predict STR that aligns with the user's intent, whereas action instructions offered a more concrete description, enabling the model to better capture the intent.
定性消融分析。除了表 6 中的定量消融结果外，我们还提供了定性对比。图 4（左）展示了 LLM 在复杂空间布局下预测静态文本所面临的挑战。图 4（右）显示了相同用户意图但在不同操作类型下的 STR 生成结果。这表明，使用操作命令学习的模型无法预测与用户意图一致的 STR，而操作指令提供了更具体的描述，使模型能够更好地捕捉意图。


Qualitative Generalisation Study. We studied the generalisation of ViMo in Fig. 5 by providing user actions that were not typically encountered within the App's standard context. For example, in the Clock App, a user action to "add a file" generated a Drive-style file selection window while retaining the Clock interface. Similarly, in the Kitchen Store App, ViMo can generate content corresponding to the action. These results emphasised ViMo's generalisation ability facing novel combinations of App observations and user actions.
定性泛化研究。我们在图 5 中通过提供 App 标准上下文中不常见的用户操作来研究 ViMo 的泛化能力。例如，在时钟 App 中，“添加文件”的操作生成了 Drive 风格的文件选择窗口，同时保留了时钟界面。同样，在厨房商店 App 中，ViMo 也能生成与操作对应的内容。这些结果强调了 ViMo 在面对 App 观测与用户操作的新颖组合时的泛化能力。


## C Practical Deployment
## C 实际部署


In this section, we report the computational efficiency of our method to demonstrate its practicality in real-world applications. The minimum hardware requirement is a GPU with 16 GB of memory. On a V100 GPU, STR image generation takes approximately 8 seconds, and GUI-text prediction takes around 30 seconds. In our setup, the total inference time per request is about 2 minutes, including model loading and communication overhead. Each request involves predicting future GUIs for three different user actions. Table 12 compares the inference time and step accuracy of ViMo with other world models. With an additional 2 minutes of inference time, ViMo achieves a notable accuracy improvement of 6.07%.
在本节中，我们报告了该方法的计算效率，以证明其在实际应用中的实用性。最低硬件要求是 16 GB 显存的 GPU。在 V100 GPU 上，STR 图像生成约需 8 秒，GUI 文本预测约需 30 秒。在我们的设置中，每个请求的总推理时间约为 2 分钟，包括模型加载和通信开销。每个请求涉及为三个不同的用户操作预测未来的 GUI。表 12 对比了 ViMo 与其他世界模型的推理时间及步准确率。在额外增加 2 分钟推理时间的情况下，ViMo 实现了 6.07% 的显著准确率提升。


<img src="https://cdn.noedgeai.com/bo_d5v247n7aajc73870dfg_17.jpg?x=302&y=206&w=1197&h=1386&r=0"/>



Figure 7: Visualisation of ViMo in generating the next GUI. For each example, the action is displayed at the top, with the current GUI shown on the left and the generated GUI on the right.
图 7：ViMo 生成下一帧 GUI 的可视化。对于每个示例，操作显示在顶部，当前 GUI 显示在左侧，生成的 GUI 显示在右侧。


## D Limitation
## D 局限性


Fig. 6 illustrates failure cases where text symbols are not represented as our rectangle-shaped placeholders with a black border and white fill, making them unrecognisable as text symbols. Improving the representation of text symbols remains a potential direction for future work.
图 6 展示了失败案例，其中文本符号未表示为我们带有黑框和白色填充的矩形占位符，导致其无法被识别为文本符号。改进文本符号的表示仍是未来工作的一个潜在方向。


## E Additional Visualisation
## E 补充可视化


Diverse visualisations are presented in Fig. 7 and Fig. 8 These examples illustrate how our method effectively generates the next GUI based on the given action and current GUI observation, showcasing its ability to produce visually coherent and contextually accurate GUI simulations. Moreover,
图 7 和图 8 展示了多样的可视化结果。这些示例说明了我们的方法如何根据给定操作和当前 GUI 观测有效生成下一帧 GUI，展示了其产生视觉连贯且上下文准确的 GUI 模拟的能力。此外，


<img src="https://cdn.noedgeai.com/bo_d5v247n7aajc73870dfg_18.jpg?x=301&y=207&w=1199&h=1200&r=0"/>



Figure 8: Visualisation of ViMo in generating the next GUI. For each example, the action is displayed at the top, with the current GUI shown on the left and the generated GUI on the right.
图 8：ViMo 生成下一帧 GUI 的可视化。对于每个示例，操作显示在顶部，当前 GUI 显示在左侧，生成的 GUI 显示在右侧。


Fig. 9 showcases results generated from a single current GUI paired with different actions, further highlighting the versatility of our approach.
图 9 展示了从单个当前 GUI 结合不同操作生成的结果，进一步突显了我们方法的多样性。


## F Social Impact and Safeguards
## F 社会影响与保护措施


Our work, which focuses on predicting future GUI states, is aligned with general advances in the field of machine learning. As such, it carries potential societal implications similar to those associated with generative technologies, such as deepfakes [51], including concerns around misuse, misinformation, or user manipulation. To prevent the high risk of misuse of the proposed method, an additional user commitment will be required for accessing the checkpoint in our forthcoming open release, through which we hope to alleviate the potential misuse while benefiting further research.
我们的工作专注于预测未来的 GUI 状态，这与机器学习领域的整体进步相一致。因此，它具有与 Deepfakes [51] 等生成技术相似的潜在社会影响，包括对滥用、虚假信息或用户操纵的担忧。为了防止该方法被滥用的高风险，在我们即将发布的开源版本中，访问权重文件将需要额外的用户承诺，我们希望借此减轻潜在的滥用，同时造福进一步的研究。


<img src="https://cdn.noedgeai.com/bo_d5v247n7aajc73870dfg_19.jpg?x=305&y=212&w=1186&h=537&r=0"/>



Figure 9: Visualisation of ViMo in generating GUIs given a single current GUI paired with different actions.
图 9：ViMo 在给定单个当前 GUI 并结合不同操作时生成 GUI 的可视化。


## G Prompts
## G 提示词


### G.1 Prompt to decide the text symbols to remain unchanged after the action
### G.1 确定操作后保持不变的文本符号的提示词


---



You are a professional UI/UX analyst and your goal is to compare the two UI screenshots and return
你是一位专业的 UI/UX 分析师，目标是对比两张 UI 截图并返回


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;their overlapping layout.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;它们重叠的布局。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###Inputs:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###### 输入：


&nbsp;&nbsp;&nbsp;&nbsp;1. **Current Screenshot**: The current mobile UI as an image.
&nbsp;&nbsp;&nbsp;&nbsp;1. **当前截图**：当前移动端 UI 图像。


2. **Next UI Layout Screenshot**:
2. **下一 UI 布局截图**：


- An image of the next mobile UI layout with all text replaced by white boxes.
- 下一移动端 UI 布局的图像，所有文本均被白框替换。


&nbsp;&nbsp;&nbsp;&nbsp;- Each box has a unique red ID label.
&nbsp;&nbsp;&nbsp;&nbsp;- - 每个框都有一个唯一的红色 ID 标签。


&nbsp;&nbsp;&nbsp;&nbsp;3. Use action: a user action described by language
&nbsp;&nbsp;&nbsp;&nbsp;3. 执行操作：用语言描述的用户操作


&nbsp;&nbsp;&nbsp;&nbsp;Next UI Layout Screenshot is a result of a user action on the current screenshot, but the text
&nbsp;&nbsp;&nbsp;&nbsp;下一 UI 布局截图是用户在当前截图上执行操作后的结果，但文本


&nbsp;&nbsp;&nbsp;&nbsp;elements are masked.
&nbsp;&nbsp;&nbsp;&nbsp;元素已被遮盖。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Please help me identify those layouts that are located in the same position, so I can predict their
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;请帮我识别位于相同位置的布局，以便我能直接从当前截图中


&nbsp;&nbsp;&nbsp;&nbsp;text directly from the current screenshot.
&nbsp;&nbsp;&nbsp;&nbsp;预测它们的文本。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Usually, the system bar information should be included. Exclude elements from the result if:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通常应包含系统栏信息。如果满足以下情况，请从结果中排除该元素：


&nbsp;&nbsp;&nbsp;&nbsp;The content (text) changes as a result of the user action, even if the element exists in both
&nbsp;&nbsp;&nbsp;&nbsp;即使元素在两张截图中都存在，但其内容（文本）因用户操作而发生了


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;screenshots.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;变化。


&nbsp;&nbsp;&nbsp;&nbsp;Please be very very cautious about putting an ID on the list, which means you are very very
&nbsp;&nbsp;&nbsp;&nbsp;将 ID 放入列表时请务必极其谨慎，这意味着你对此任务非常有


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;confident with this task. if you are unsure about some elements, please ignore them and do not
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;把握。如果你对某些元素不确定，请忽略它们，不要


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;put them on the list.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将它们放入列表。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###Output the list of existing elements : Return the results in the following JSON format:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;######输出现有元素列表：请按以下 JSON 格式返回结果：


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;['id1','id2',...]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;['id1','id2',...]


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###Notes:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;######注意：


- Ensure the detected elements appear in both UI screenshots, which means their surrounding context
- 确保检测到的元素在两个 UI 截图均出现，即它们的周边上下文


&nbsp;&nbsp;&nbsp;&nbsp;is the same.
&nbsp;&nbsp;&nbsp;&nbsp;一致。


- Ensure identify those elements that their text will change by the user action and exclude them
- 确保识别出文本会随用户操作而变化的元素，并将其


from your response.
从响应中排除。


- Ensure identify those elements that share a similar context layout, but their absolute are not
- 确保识别出上下文布局相似但绝对位置


the same, and them from your response.
不同的元素，并将其从响应中排除。


- Ensure only reply in pure JSON format, with no placeholders or comments.
- 确保仅以纯 JSON 格式回复，不包含占位符或注释。


---



### G.2 Prompt to determine the semantic role of each text symbol
### G.2 用于确定每个文本符号语义角色的提示词


You are a professional UI/UX analyst assigned to structure and analyse the semantics of mobile UI screenshots.
你是一名专业的 UI/UX 分析师，负责对移动端 UI 截图的语义进行结构化分析。


Your goal is to segment the UI and annotate box elements in a way that enhances understanding of their roles and relationships within the interface. Inputs:
你的目标是分割 UI 并标注框选元素，以此增强对其在界面中的角色和关系的理解。输入：


- Current Screenshot: A visual representation of the mobile UI.
- 当前截图：移动端 UI 的视觉呈现。


- Next UI layout screenshot: A visual representation of the next UI layout with all the text masked with a white box. Each box has an ID number on it in red colour. - User Action: An action put on the current UI will result in the next UI.
- 下一 UI 布局截图：下一 UI 布局的视觉呈现，所有文本均被白框遮盖。每个框上都有红色的 ID 编号。- 用户操作：对当前 UI 执行的操作将导致下一 UI 的产生。


- Box locations: a list of box locations to better help you to locate the boxes in the format of 'id': id, 'Location': [x1,y1, width, height]. ID indicates their ID number in the UI screenshot.
- 框位置：框位置列表，以 'id': id, 'Location': [x1, y1, width, height] 格式辅助定位。ID 对应 UI 截图中的编号。


- UI_size: the width/height of the input images. They are the same size. The image you received might be resized. Please scale it back for the locations.
- UI_size：输入图像的宽/高。尺寸一致。你收到的图像可能经过缩放，请在定位时将其还原。


Task:
任务：


Structure the boxes in the Next UI layout screenshot with semantics based on the visual input by following these steps:
按照以下步骤，根据视觉输入为下一 UI 布局截图中的框构建语义结构：


1 , Divide the UI into Semantic Windows Group the UI into functional sections with a specific name (e.g., "Header Windows," "Time Selector Panel").
1. 划分语义窗口：将 UI 划分为具有特定名称的功能区块（例如“页眉窗口”、“时间选择器面板”）。


2. Structure Text Elements in Each Semantic Window.
2. 结构化每个语义窗口内的文本元素。


- Assign box elements to windows based on logical, visual relationships or semantic roles.
- 根据逻辑、视觉关系或语义角色将框元素分配至窗口。


- For every element, structure output as :
- 对于每个元素，按如下格式构建输出：


**id: corresponding box retrieved from the box list and the Next UI layout screenshot.
**id：从框列表和下一 UI 布局截图中提取的对应框 ID。


**Role: A brief explanation of the role of this box. You should consider their [x1,y1] to indicate their location, [w,h] to indicate their size to decide the role. It is important to consider the context for the role prediction. The role should be in detail to distinguish it from other items in the same category.
**Role（角色）：该框角色的简要说明。应结合 [x1, y1] 位置信息和 [w, h] 尺寸信息来判定。预测角色时必须考虑上下文。角色描述应足够详细，以便与同类项区分。


Output Format: \{ "Window Name": "Category Name": [ "id":id, "Role": "Role", "id":id, "Role": "Role", ... ], "Category Name": [ "id":id, "Role": "Role", ... ] , ... \} Key Guidelines:
输出格式：\{ "窗口名称": "类别名称": [ "id":id, "Role": "角色", "id":id, "Role": "角色", ... ], "类别名称": [ "id":id, "Role": "角色", ... ] , ... \} 关键指南：


- Ensure to retrieve id from the given screenshot and box list.
- 确保从给定的截图和框列表中提取 id。


- Avoid duplicating or omitting IDs.
- 避免重复或遗漏 ID。


- Every box element in the box location list must be included in the structured output.
- 框位置列表中的每个框元素都必须包含在结构化输出中。


- Ensure there is no additional formatting, code blocks or placeholders in your response; return only a clean JSON without any comments.
确保响应中没有额外的格式、代码块或占位符；仅返回不含任何注释的干净 JSON。


### G.3 Prompt to predict the exact text content for each symbol
### G.3 预测每个符号准确文本内容的提示词


---



Task: Plan the content for the next UI screen based on the provided inputs and instructions.
任务：根据提供的输入和指令，规划下一个 UI 屏幕的内容。


&nbsp;&nbsp;&nbsp;&nbsp;Inputs:
&nbsp;&nbsp;&nbsp;&nbsp;输入：


&nbsp;&nbsp;&nbsp;&nbsp;Current Screenshot: A visual representation of the mobile UI.
&nbsp;&nbsp;&nbsp;&nbsp;当前截图：移动端 UI 的视觉呈现。


Next UI layout screenshot: A visual representation of the next UI layout with all light yellow
下一 UI 布局截图：下一 UI 布局的视觉呈现，所有淡黄色


boxes indicating a text place. Each box has an ID number on it.
方框表示文本位置。每个方框上都有一个 ID 编号。


User Instruction: A specific action or command that transitions the current UI to the next UI
用户指令：将当前 UI 转换至下一 UI


state.
状态的特定操作或命令。


&nbsp;&nbsp;&nbsp;&nbsp;Semantics for the masks in Next UI screenshot: A structured map.
&nbsp;&nbsp;&nbsp;&nbsp;下一 UI 截图中的遮罩语义：结构化映射表。


Goal:
目标：


Predict the content (text) for each masked area in the next UI layout screenshot based on the
根据以下步骤，预测下一 UI 布局截图中每个遮罩区域的内容（文本）：


following steps:
将受影响的元素映射到下一 UI。


Map Affected Elements to the Next UI.
将受影响的元素与下一 UI 上的黄色方框坐标对齐。


&nbsp;&nbsp;&nbsp;&nbsp;Align the affected elements with the yellow box coordinates on the next UI.
&nbsp;&nbsp;&nbsp;&nbsp;根据用户指令和当前上下文，预测每个黄色方框的文本。


&nbsp;&nbsp;&nbsp;&nbsp;Predict the text for each yellow box based on the user instruction and the context of the current
&nbsp;&nbsp;&nbsp;&nbsp;根据用户指令和当前上下文，预测每个黄色方框的文本。


UI.



If you can not find any information about the text, predict a plausible text based on its context.
如果无法找到有关文本的任何信息，请根据上下文预测合理的文本。


Ensure to use the semantics to help you understand the layout and predict the text. If you think
确保利用语义来帮助理解布局并预测文本。如果你认为


the semantics is wrong, please modify it in your
语义有误，请在你的


&nbsp;&nbsp;&nbsp;&nbsp;Output:
&nbsp;&nbsp;&nbsp;&nbsp;输出中进行修改：


Return the predictions in JSON format with the structure: \{"Window Name ": "Category Name ": [
以 JSON 格式返回预测结果，结构如下：\{"Window Name ": "Category Name ": [


&nbsp;&nbsp;&nbsp;&nbsp;"id ": id, "text ": "text", "role ": "role", "id ": "id", "text": "text", "role": "role "
&nbsp;&nbsp;&nbsp;&nbsp;"id ": id, "text ": "text", "role ": "role", "id ": "id", "text": "text", "role": "role "


], , ... \}



&nbsp;&nbsp;&nbsp;&nbsp;Ensure to predict text based on the context.
&nbsp;&nbsp;&nbsp;&nbsp;确保根据上下文预测文本。


Do not include any special characters.
不要包含任何特殊字符。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ensure there is no additional formatting, code blocks or placeholders in your response; return only
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;确保响应中没有额外的格式、代码块或占位符；仅返回


&nbsp;&nbsp;&nbsp;&nbsp;a clean JSON without any comments.
&nbsp;&nbsp;&nbsp;&nbsp;不含任何注释的干净 JSON。


---



### G.4 Prompt to evaluate actions with a confidence score
### G.4 带有置信度评分的动作评估提示词


You are an agent who can operate an Android phone on behalf of a user. When given a user request, you will try to complete it step by step. At each step, a list of descriptions for most UI elements on the current screen will be given to you (each element can be specified by an index), together with a history of what you have done in previous steps. Based on these pieces of information and the goal, you must choose to perform one of the actions in the following list (action description followed by the JSON format) by outputting the action in the correct JSON format: action options from the dataset
你是一个可以代表用户操作 Android 手机的智能体。收到用户请求后，你将尝试逐步完成它。在每一步中，你都会获得当前屏幕上大多数 UI 元素的描述列表（每个元素可通过索引指定），以及此前步骤的操作历史。根据这些信息和目标，你必须从以下列表中选择执行一个操作（操作描述后跟 JSON 格式），并以正确的 JSON 格式输出该操作：来自数据集的操作选项


The overall user goal/request is: \{goal\}
用户的总体目标/请求是：\{goal\}


Here is a history of what you have done so far:\{history\} This is the action you picked in the latest step: \{action\}, whose semantic description is: \{sum\}
这是你目前的操作历史：\{history\} 这是你在最后一步选择的操作：\{action\}，其语义描述为：\{sum\}


Your goal is to judge **whether the action you picked in the latest step is on the right track to the successful execution of the overall user goal/request**.
你的目标是判断**你在最后一步选择的操作是否正处于成功执行用户总体目标/请求的正确路径上**。


You will be given the screenshots before and after you perform the action
你将获得执行操作前后的屏幕截图


- The first screenshot corresponds to the UI state before you performed the action.
- 第一张截图对应您执行操作前的 UI 状态。


- The second screenshot corresponds to the UI state after you performed the action.
- 第二张截图对应您执行操作后的 UI 状态。


Also here is the list of detailed information for some UI elements in the before screenshot: \{before_elements\}
这里还列出了“操作前”截图中部分 UI 元素的详细信息：\{before_elements\}


Note that, the "after" screenshot is generated by the agent's world model. As such, it may not faithfully represent the real UI. For instance: Some UI elements in the simulated "after" screenshot may not exist in a real UI. Your evaluation should consider the reliability of the UI predictions. If the "after" screenshot contains unreasonable elements, this likely indicates a failure.
请注意，“操作后”截图是由智能体的世界模型生成的。因此，它可能无法忠实地反映真实 UI。例如：模拟的“操作后”截图中某些 UI 元素在真实 UI 中可能并不存在。您的评估应考虑 UI 预测的可靠性。如果“操作后”截图包含不合理的元素，这很可能表示操作失败。


Now provide your judgment on the selected action in JSON format. Your response must include: Reason: A detailed explanation of why the action is valid or invalid.
现在请以 JSON 格式对所选操作给出您的判断。您的回复必须包含：Reason（原因）：关于该操作有效或无效的详细解释。


Judgment: Your judgment must be either "valid" or "invalid".
Judgment（判断）：您的判断必须是“valid”（有效）或“invalid”（无效）。


Confidence: A confidence score between 0.0 and 1.0, reflects how likely your judgment is correct. You must follow this structure exactly:
Confidence（置信度）：介于 0.0 到 1.0 之间的置信度分数，反映您判断正确的可能性。您必须严格遵守以下结构：


\{Reason: ..., Judgement: "valid" or "invalid", Confidence: ...\}
\{Reason: ..., Judgement: "valid" or "invalid", Confidence: ...\}


Your Answer:
您的回答：


### G.5 Prompt to select the optimal actions among two highest-scoring actions
### G.5 从两个最高分操作中选择最佳操作的提示词


---



You are an agent who can operate an Android phone on behalf of a user. When given a user
您是一个可以代表用户操作 Android 手机的智能体。当收到用户


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;request, you will try to complete it step by step. At each step, a list of descriptions for
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;请求时，您将尝试逐步完成它。在每一步中，都会为您提供当前屏幕上


&nbsp;&nbsp;&nbsp;&nbsp;most UI elements on the current screen will be given to you (each element can be specified by an
&nbsp;&nbsp;&nbsp;&nbsp;大多数 UI 元素的描述列表（每个元素都可以由一个


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;index), together with a history of what you have done in previous steps. Based on these pieces
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;索引指定），以及您在之前步骤中所做操作的历史记录。基于这些


&nbsp;&nbsp;&nbsp;&nbsp;of information and the goal, you must choose to perform one of the actions in the following list
&nbsp;&nbsp;&nbsp;&nbsp;信息和目标，您必须从以下列表中选择执行其中一个操作


(action description followed by the JSON format) by outputting the action in the correct JSON
（操作描述后跟 JSON 格式），通过以正确的 JSON 格式输出该操作


&nbsp;&nbsp;&nbsp;&nbsp;format action options from the dataset
&nbsp;&nbsp;&nbsp;&nbsp;从数据集中格式化操作选项


&nbsp;&nbsp;&nbsp;&nbsp;The overall user goal/request is: \{goal\}
&nbsp;&nbsp;&nbsp;&nbsp;用户的总体目标/请求是：\{goal\}


Here is a history of what you have done so far:\{history\}
这是你目前为止的操作历史：\{history\}


&nbsp;&nbsp;&nbsp;&nbsp;Here is a list of descriptions for some UI elements on the current screen:\{before_elements\}
&nbsp;&nbsp;&nbsp;&nbsp;这是当前屏幕上部分 UI 元素的描述列表：\{before_elements\}


&nbsp;&nbsp;&nbsp;&nbsp;Here are two candidate actions:
&nbsp;&nbsp;&nbsp;&nbsp;这里有两个候选操作：


Action 1: \{action_0\}, described semantically as \{sum_0\}. The rationale for this action is:
操作 1：\{action_0\}，语义描述为 \{sum_0\}。该操作的理由是：


\{act_re_0\}
\{act_re_0\}


Action 2: \{action_1\}, described semantically as \{sum_1\}. The rationale for this action is:
操作 2：\{action_1\}，语义描述为 \{sum_1\}。该操作的理由是：


\{act_re_1\}
\{act_re_1\}


Hints for making your decision: \{GUIDANCE\}
决策提示：\{GUIDANCE\}


- Both "more options" buttons and scrolling actions may reveal new content. Evaluate which is more
- “更多选项”按钮和滚动操作都可能展示新内容。请评估哪种方式更


suitable for the goal.
符合目标。


- Consider the history of previous actions. If prior steps involved repeated "scroll down" actions,
- 考虑历史操作。如果之前的步骤包含重复的“向下滚动”，


it is more likely that "scroll down" is the correct next step.
那么“向下滚动”极有可能是正确的下一步。


- If the user goal involves viewing reviews or similar tasks and the current screen already
- 如果用户目标涉及查看评论或类似任务，且当前屏幕已


displays such content, "scroll down" may reveal more information.
显示此类内容，则“向下滚动”可能会展示更多信息。


Your task is to choose the best action from the two provided.
你的任务是从提供的两个动作中选择最佳的一个。


Now, provide your judgment in JSON format with the following structure:
现在，请按以下 JSON 格式提供你的判断：


Reason: A detailed explanation of your choice, considering the hints above.
Reason：结合上述提示，详细解释你的选择。


Choice: Action 1 or Action 2.
Choice：动作 1 或动作 2。


&nbsp;&nbsp;&nbsp;&nbsp;Your output must exactly match this format:
&nbsp;&nbsp;&nbsp;&nbsp;你的输出必须完全符合此格式：


\{Reason: ..., Choice: Action 1 or Action 2\}
\{Reason: ..., Choice: Action 1 or Action 2\}


---



### G.6 Prompt to convert action commands into action instructions
### G.6 将动作命令转换为动作指令的提示词


You are a professional UI/UX analyst specializing in identifying the semantics of dual point actions between mobile UI screenshots.
你是一位专业的 UI/UX 分析师，擅长识别移动端 UI 截图之间双点动作的语义。


Inputs:
输入：


Current Screenshot: A visual representation of the mobile UI.
当前截图：移动端 UI 的视觉呈现。


Next Screenshot: A visual representation of the NEXT mobile UI.
下一张截图：下一刻移动端 UI 的视觉呈现。


Goal: A user intent on this Mobile interface.
目标：用户在此移动界面上的意图。


touch_xy: the x,y coordinates for the touch point, as a percentage of the image dimensions.
touch_xy：触碰点的 x,y 坐标，以图像尺寸的百分比表示。


lift_xy: the x,y coordinates for the lift point, as a percentage of the image dimensions.
lift_xy：抬起点的 x,y 坐标，以图像尺寸的百分比表示。


Your task is to analyse these elements describe the precise user action in plain language and
你的任务是分析这些要素，用通俗的语言描述精确的用户动作，并且


return your answer in plain string (e.g., "click the + icon", "scroll up").
以纯字符串形式返回答案（例如，“点击 + 图标”、“向上滚动”）。


If the two screenshots are identical, please return an empty string as "".
如果两张截图完全相同，请返回空字符串 ""。


If the Next Screenshot does not seem to be one step away from the Current Screenshot, return an empty string as "". One step means only one interaction with the cell phone.
如果“下一张截图”看起来与“当前截图”并非一步之遥，请返回空字符串 ""。“一步”指对手机仅进行一次交互。


Ensure there is no additional formatting, code blocks or placeholders in your response; return only a clean string without any comments
确保响应中没有额外的格式、代码块或占位符；仅返回不含任何注释的纯字符串


### G.7 Prompt for instructional accuracy score $\left( {s}_{ia}\right)$
### G.7 指令准确度评分提示词 $\left( {s}_{ia}\right)$


---



You are an expert in evaluating the performance of a mobile emulator. The mobile emulator is
你是评估移动模拟器性能的专家。该移动模拟器的


designed to navigate the UI change based on human instruction.
设计目标是根据人工指令进行 UI 导航变更。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Inputs:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;输入：


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Current UI Screenshot: The present state of the cellphone's user interface.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当前 UI 截图：手机用户界面的当前状态。


Next UI Screenshot: The mobile emulator generated UI indicating the next state of the cellphone's
下一张 UI 截图：由移动模拟器生成的 UI，表示根据人工指令执行后的手机


&nbsp;&nbsp;&nbsp;&nbsp;user interface based on human instruction.
&nbsp;&nbsp;&nbsp;&nbsp;用户界面下一状态。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Human instruction: The action applied on the current UI screenshot.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;人工指令：在当前 UI 截图上执行的操作。


Your goal is to determine whether the mobile emulator successfully predicts the next UI image with
你的目标是根据当前 UI 和用户操作，判定移动模拟器是否利用当前信息和布局


&nbsp;&nbsp;&nbsp;&nbsp;current information and layout based on the current UI and the user action.
&nbsp;&nbsp;&nbsp;&nbsp;成功预测了下一张 UI 图像。


&nbsp;&nbsp;&nbsp;&nbsp;*IMPORTANT*
&nbsp;&nbsp;&nbsp;&nbsp;*重要提示*


&nbsp;&nbsp;&nbsp;&nbsp;Format your response into a JSON map as shown below:
&nbsp;&nbsp;&nbsp;&nbsp;请将你的响应格式化为如下所示的 JSON 映射：


\{



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Thoughts": <your thoughts and reasoning process>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Thoughts": <你的想法和推理过程>，


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Status": "success" or "failure",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Status": "success" 或 "failure",


\}



---



### G.8 Prompt for action readiness accuracy score $\left( {s}_{ar}\right)$
### G.8 操作就绪准确度评分提示词 $\left( {s}_{ar}\right)$


---



You are an expert in evaluating the performance of a mobile emulator. The mobile emulator is
你是评估移动模拟器性能的专家。该移动模拟器


designed to navigate the UI change based on human instruction.
旨在根据人类指令进行 UI 导航变更。


&nbsp;&nbsp;&nbsp;&nbsp;Inputs:
&nbsp;&nbsp;&nbsp;&nbsp;输入：


&nbsp;&nbsp;&nbsp;&nbsp;UI Screenshot: The mobile emulator generated UI indicating the state of the cellphone's user
&nbsp;&nbsp;&nbsp;&nbsp;UI 截图：移动模拟器生成的 UI，指示手机用户


&nbsp;&nbsp;&nbsp;&nbsp;interface.
&nbsp;&nbsp;&nbsp;&nbsp;界面的状态。


User intent: The user goal to achieve.
用户意图：要实现的用户目标。


&nbsp;&nbsp;&nbsp;&nbsp;Next action: the action will be applied to this UI.
&nbsp;&nbsp;&nbsp;&nbsp;下一步操作：将应用于此 UI 的操作。


Your goal is to determine whether the next action is validated on the UI Screenshot.
你的目标是判断下一步操作在 UI 截图上是否有效。


&nbsp;&nbsp;&nbsp;&nbsp;Please also indicate if it is still in the right App according to the goal.
&nbsp;&nbsp;&nbsp;&nbsp;请同时说明根据目标判断，当前是否仍处于正确的应用中。


*IMPORTANT* Format your response into a JSON map as shown below:
*重要* 请按如下所示的 JSON 映射格式进行回答：


\{



&nbsp;&nbsp;&nbsp;&nbsp;"Thoughts": <your thoughts and reasoning process>,
&nbsp;&nbsp;&nbsp;&nbsp;"Thoughts": <你的思考和推理过程>,


&nbsp;&nbsp;&nbsp;&nbsp;"In the right App": "yes" or "no"
&nbsp;&nbsp;&nbsp;&nbsp;"In the right App": "yes" 或 "no"


"ready for action": "yes" or "yes",
"ready for action": "yes" 或 "no",


\}



---



### G.9 Instructions for User Study
### G.9 用户研究指南


The following prompt provides the instructions for the user study. An example screenshot is shown in Fig. 10
以下提示词提供了用户研究的说明。图 10 展示了一个示例截图


<img src="https://cdn.noedgeai.com/bo_d5v247n7aajc73870dfg_23.jpg?x=312&y=199&w=1179&h=411&r=0"/>



Figure 10: Screenshot of user study example.
图 10：用户研究示例截图。


Question 1: You are presented with a mobile screen (Current GUI).
问题 1：你将看到一个移动端屏幕（当前 GUI）。


Your task is to evaluate whether the generated GUI correctly reflects the result of the user action applied on the current GUI. Answer "Yes" or "No" to each sample. Question 2:
你的任务是评估生成的 GUI 是否正确反映了在当前 GUI 上执行用户操作后的结果。对每个样本回答“是”或“否”。问题 2：


You are presented with Ground Truth Next GUI, the correct screens that should appear after the user performs the given action.
你将看到 Ground Truth 下一屏 GUI，即用户执行给定操作后应当出现的正确屏幕。


In this task, you will evaluate the visual similarity between the Ground Truth Next GUI and AI-generated GUI, scoring from 1-5.
在此任务中，你将评估 Ground Truth 下一屏 GUI 与 AI 生成的 GUI 之间的视觉相似度，评分范围为 1-5。


Question 3:
问题 3：


In this task, you will validate whether a specific user action is valid on the generated GUI.
在此任务中，你将验证特定的用户操作在生成的 GUI 上是否有效。


An action is considered valid if the required GUI element is visually presented on the screen. Answer "Yes" or "No" to each sample.
如果所需的 GUI 元素在屏幕上可见，则认为该操作有效。对每个样本回答“是”或“否”。


### G.10 Prompt to generate the action instruction based on the given GUI and the user goal
### G.10 基于给定 GUI 和用户目标生成操作指令的提示词


---



You are an autonomous intelligent agent tasked with navigating a cell phone to accomplish specific
你是一个自主智能体，任务是操作手机以完成特定


tasks. You will be provided with the following information:
任务。你将获得以下信息：


1. Initial UI screenshot: A visual representation of the initial state of the cell phone's
1. 初始 UI 截图：手机界面


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;interface.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;初始状态的视觉呈现。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2. User Objective: This is the task you are trying to complete.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2. 用户目标：这是你尝试完成的任务。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. Previous Action: An action sequence performed on the initial UI.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. 先前操作：在初始 UI 上执行的操作序列。


&nbsp;&nbsp;&nbsp;&nbsp;4, Current UI states: A visual representation of the current state of the cell phone's interface,
&nbsp;&nbsp;&nbsp;&nbsp;4, 当前 UI 状态：手机界面的当前状态可视化表示，


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;generated by a simulated environment.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由模拟环境生成。


The initial image is the screenshot before actually performing all the previous actions.
初始图像是实际执行所有先前操作之前的截图。


The current cell phone UI is generated by applying previous actions on the initial screenshot.
当前手机 UI 是通过在初始截图上应用先前操作生成的。


Your Task: Please predict a single next step action to complete the given task based on current
你的任务：请根据当前的


&nbsp;&nbsp;&nbsp;&nbsp;vision states.
&nbsp;&nbsp;&nbsp;&nbsp;视觉状态，预测完成给定任务的下一个操作步骤。


&nbsp;&nbsp;&nbsp;&nbsp;To be successful, it is very important to follow the following rules:
&nbsp;&nbsp;&nbsp;&nbsp;为了成功完成任务，遵循以下规则至关重要：


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. You should only issue one action that is valid based on the current UI states.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. 你应当仅发布一个基于当前 UI 状态的有效操作。


2. You should only issue one action at a time. Avoid issuing multiple actions like "do A and do
2. 你应当一次仅发布一个操作。避免发布多个操作，如“执行 A 并执行


&nbsp;&nbsp;&nbsp;&nbsp;B".



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. Generate the action in plain text. For example, Scroll down to set the minute as 15.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. 以纯文本形式生成操作。例如：向下滑动将分钟设为 15。


4. Issue "Stop." if you think the action is already completed. Ensure you only return the action,
4. 如果你认为操作已完成，请发布“Stop.”。确保仅返回操作，


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;not other formats, comments or placeholders
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;而非其他格式、注释或占位符


---



### G.11 Prompt to evaluates whether the simulated action leads to the same outcome as the ground truth action
### G.11 评估模拟操作是否导致与真实路径操作相同结果的提示词


---



You are an expert in evaluating the performance of a cell phone navigation agent. The agent is
你是评估手机导航代理性能的专家。该代理


&nbsp;&nbsp;&nbsp;&nbsp;designed to help a human user navigate a cellphone to complete a task.
&nbsp;&nbsp;&nbsp;&nbsp;旨在帮助人类用户操作手机以完成任务。


&nbsp;&nbsp;&nbsp;&nbsp;Inputs:
&nbsp;&nbsp;&nbsp;&nbsp;输入：


&nbsp;&nbsp;&nbsp;&nbsp;Current UI Screenshot: The present state of the cellphone's user interface.
&nbsp;&nbsp;&nbsp;&nbsp;当前 UI 截图：手机用户界面的当前状态。


&nbsp;&nbsp;&nbsp;&nbsp;User Intent: The goal the human user aims to achieve.
&nbsp;&nbsp;&nbsp;&nbsp;用户意图：人类用户想要达成的目标。


&nbsp;&nbsp;&nbsp;&nbsp;Action History: The sequence of actions taken so far for you to track the progress.
&nbsp;&nbsp;&nbsp;&nbsp;操作历史：迄今为止执行的操作序列，供你追踪进度。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Agent Simulated Action: The action suggested by the agent to achieve the user's intent.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;智能体模拟操作：智能体为实现用户意图建议的操作。


&nbsp;&nbsp;&nbsp;&nbsp;Ground Truth Action: The correct action is needed to achieve the user's intent.
&nbsp;&nbsp;&nbsp;&nbsp;标准答案操作：实现用户意图所需的正确操作。


&nbsp;&nbsp;&nbsp;&nbsp;Your goal is to determine whether the agent's simulated action leads to the same outcome as the
&nbsp;&nbsp;&nbsp;&nbsp;你的目标是判断智能体的模拟操作是否能产生与


ground truth action.
标准答案操作相同的结果。


&nbsp;&nbsp;&nbsp;&nbsp;Additionally, if the simulated action does not exactly match the ground truth action but is still
&nbsp;&nbsp;&nbsp;&nbsp;此外，如果模拟操作与标准答案操作不完全一致，但仍


progressing toward the correct outcome to achieve user intent, indicating that the action is "on
在朝着实现用户意图的正确结果推进，则表明该操作“处于


&nbsp;&nbsp;&nbsp;&nbsp;the right track."
&nbsp;&nbsp;&nbsp;&nbsp;正确轨道上”。


&nbsp;&nbsp;&nbsp;&nbsp;*IMPORTANT*
&nbsp;&nbsp;&nbsp;&nbsp;*重要提示*


&nbsp;&nbsp;&nbsp;&nbsp;Format your response into a JSON map as shown below:
&nbsp;&nbsp;&nbsp;&nbsp;请将你的回答格式化为如下所示的 JSON 映射：


\{



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Thoughts": <your thoughts and reasoning process>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Thoughts": <你的想法和推理过程>,


&nbsp;&nbsp;&nbsp;&nbsp;"Status": "success" or "failure",
&nbsp;&nbsp;&nbsp;&nbsp;"Status": "success" 或 "failure",


"On the right track to success": "yes" or "no"
"On the right track to success": "yes" 或 "no"


\}



---
