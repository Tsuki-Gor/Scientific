# PAL-UI: PLANNING WITH ACTIVE LOOK-BACK FOR VISION-BASED GUI AGENTS
# PAL-UI：用于基于视觉的 GUI 代理的带主动回顾的规划


Zikang Liu ${}^{1}$ , Junyi Li ${}^{2}$ , Wayne Xin Zhao ${}^{1\infty }$ , Dawei Gao ${}^{3}$ , Yaliang Li ${}^{3}$ , Ji-Rong Wen ${}^{1}$
刘子康 ${}^{1}$，李君怡 ${}^{2}$，赵新文 ${}^{1\infty }$，高大为 ${}^{3}$，李亚良 ${}^{3}$，文继荣 ${}^{1}$


${}^{1}$ Gaoling School of Artificial Intelligence,Renmin University of China.
${}^{1}$ 中国人民大学高瓴人工智能学院。


${}^{2}$ Department of Data Science,City University of Hong Kong. ${}^{3}$ Alibaba Group.
${}^{2}$ 香港城市大学数据科学系。 ${}^{3}$ 阿里巴巴集团。


\{jasonlaw8121,batmanfly\}@gmail.com, junyili@cityu.edu.hk \{gaodawei.gdw, yaliang.li\}@alibaba-inc.com, jrwen@ruc.edu.cn
\{jasonlaw8121,batmanfly\}@gmail.com, junyili@cityu.edu.hk \{gaodawei.gdw, yaliang.li\}@alibaba-inc.com, jrwen@ruc.edu.cn


## ABSTRACT
## 摘要


Graphical User Interface (GUI) agents powered by Multimodal Large Language Models (MLLMs) promise human-like interaction with software applications, yet long-horizon tasks remain challenging due to memory limitations. Existing approaches either truncate history or rely on simple textual summaries, which risk losing critical information when past visual details become necessary for future decisions. In this paper, we propose PAL-UI (Planning with Active Look-back), a novel framework that enables GUI agents to adaptively retrieve past observations when required. PAL-UI combines a dual-level summarization agent, capturing both observation-level cues and action-level outcomes, with a dedicated retrieval tool that allows the agent to recall specific historical screenshots during planning. We curate a step-level instruction dataset of 8.6K samples from mobile GUI navigation trajectories and train PAL-UI-3B and PAL-UI-7B models based on Qwen2.5-VL. Extensive experiments demonstrate that PAL-UI significantly outperforms baseline models and prior methods in mobile GUI navigation tasks, even under data-efficient settings. Moreover, PAL-UI exhibits strong cross-domain generalization, achieving notable improvements in web navigation without additional training. Our work highlights the potential of active memory retrieval for long-horizon planning capabilities of vision-based GUI agents.
由多模态大模型（MLLMs）驱动的图形用户界面（GUI）代理承诺能像人类一样与软件交互，但由于记忆限制，长时程任务仍然具有挑战性。现有方法要么截断历史，要么依赖简单的文本摘要，当未来决策需要回溯过去的视觉细节时，这些做法可能丢失关键信息。本文提出 PAL-UI（带主动回顾的规划），一种新的框架，使 GUI 代理在需要时自适应检索过去的观测。PAL-UI 将捕获观测层线索与动作层结果的双层摘要代理与一个专门的检索工具相结合，使代理在规划时能回忆特定历史截图。我们从移动 GUI 导航轨迹中整理了 8.6K 条步级指令数据，并基于 Qwen2.5-VL 训练了 PAL-UI-3B 与 PAL-UI-7B 模型。大量实验表明，PAL-UI 在移动 GUI 导航任务上显著优于基线模型和先前方法，即便在数据高效设置下亦然。此外，PAL-UI 在跨域泛化上表现强劲，在未额外训练的情况下对网页导航也取得了显著改进。我们的工作强调了主动记忆检索对于基于视觉的 GUI 代理实现长时程规划能力的潜力。


## 1 INTRODUCTION
## 1 引言


Large Language Models (LLMs) have dramatically advanced the capabilities of AI systems in recent years (Brown et al. 2020; Zhao et al. 2023). This progress has spurred the development of GUI agents, i.e., autonomous agents that perform tasks via graphical user interfaces (GUI) (Wang et al. 2024, Hong et al. 2024). Early paradigms for LLM-driven GUI agents typically relied on converting visual interface information into textual form (e.g., reading an app's accessibility tree or metadata) so that a language model could process it (Nakano et al. 2021; Zhang & Zhang, 2023). However, such text-based representations often require external modules and inject a large number of additional tokens into the context, limiting efficiency and fidelity. With the emergence of Multimodal LLMs (MLLMs) that can directly handle images as input (Liu et al. 2023), a new vision-based GUI agent paradigm has surfaced (Gou et al. 2024; Xu et al. 2024). These agents perceive raw screenshots of the interface and simulate human-like operations on the GUI, enabling end-to-end interaction without intermediate text conversions.
近年来，大型语言模型（LLMs）显著提升了人工智能系统能力（Brown 等，2020；Zhao 等，2023）。这一进展推动了 GUI 代理的发展，即通过图形用户界面执行任务的自主代理（Wang 等，2024；Hong 等，2024）。早期基于 LLM 的 GUI 代理范式通常依赖将视觉界面信息转换为文本形式（例如读取应用的无障碍树或元数据），以便语言模型处理（Nakano 等，2021；Zhang & Zhang，2023）。然而，这类基于文本的表示常常需要外部模块并向上下文注入大量额外 token，限制了效率和保真度。随着能直接处理图像输入的多模态 LLM（MLLMs）的出现（Liu 等，2023），一种新的基于视觉的 GUI 代理范式出现了（Gou 等，2024；Xu 等，2024）。这些代理感知界面的原始截图并在 GUI 上模拟类人操作，实现了端到端交互而无需中间文本转换。


A central challenge for long-horizon GUI tasks is how to incorporate memory of past observations and actions into the agent's planning. In traditional text-based agents, a common strategy is to append the entire interaction history to the input context for planning (Yao et al., 2023). Unfortunately, directly extending this to vision-based agents is infeasible. Visual observations are much heavier than text, i.e., each image input to an MLLM is encoded into a large number of tokens (Bai et al. 2025), quickly exhausting the model's context length. Moreover, current MLLMs struggle to reason effectively over many images at once; recent studies show that their capability to handle multiple simultaneous images remains quite limited (Zhao et al., 2024). Simply providing all past screenshots as input can overwhelm the model with redundant information and obscure the relevant details. As a result, existing vision-enabled GUI agents resort to very restrictive memory usage: some only feed the most recent action or a brief textual summary of it into the context (Chen et al. 2024; Xu et al. 2024), while others retain only a few of the last screenshots as visual memory (Qin et al. 2025). These ad-hoc strategies risk losing critical information from earlier steps and ultimately constrain the agent's performance on complex, multi-step tasks.
对于长时程 GUI 任务，一项核心挑战是如何将过去的观测与动作记忆纳入代理的规划。在传统的基于文本的代理中，常见策略是将整个交互历史附加到输入上下文中以进行规划（Yao 等，2023）。不幸的是，将此直接扩展到基于视觉的代理不可行。视觉观测比文本更“重”，即每张输入给 MLLM 的图像都会被编码为大量 token（Bai 等，2025），迅速耗尽模型的上下文长度。此外，当前的 MLLM 对同时处理多张图像的推理能力有限；最近研究表明它们同时处理多图像的能力仍然相当有限（Zhao 等，2024）。简单地将所有过去截图作为输入会用冗余信息淹没模型并掩盖相关细节。因此，现有的视觉使能 GUI 代理采用了非常受限的记忆使用策略：有些只将最近一次动作或其简短文本摘要放入上下文（Chen 等，2024；Xu 等，2024），而另一些仅保留最后几张截图作为视觉记忆（Qin 等，2025）。这些临时策略可能丢失早期步骤的关键信息，最终限制代理在复杂多步任务中的表现。


---



C Corresponding author.
C 通讯作者。


---



To address the above limitations, we propose a new paradigm called Planning with Active Look-back (PAL-UI). The key idea is to empower the GUI agent to actively retrieve and consult pertinent details from its history when needed, rather than carrying the full burden of the past at every step. Concretely, we equip the agent with a tool interface that can fetch a specific historical observation (a screenshot from a previous step) on demand during the planning process. The agent operates in an iterative loop: it maintains a compressed memory of the task so far (i.e., a concise textual summary of past interactions), which provides a lightweight context for the language model. While this summary covers the high-level history, the agent can look back using the tool to retrieve detailed visual information from any prior step it deems important for the current decision. This active look-back mechanism mimics how a human might momentarily glance at a past screen or recall a specific detail when uncertain about the next action. By integrating tool-based retrieval into the planning loop, our agent can leverage extensive historical information when necessary, without suffering from the context explosion or distraction issues of naively large visual contexts.
为了解决上述局限性，我们提出了一种名为“带主动回顾的规划（Planning with Active Look-back，PAL-UI）”的新范式。其核心思想是让 GUI 代理在需要时主动从历史中检索并参考相关细节，而不是在每一步都背负全部过去信息。具体而言，我们为代理配备一个工具接口，能够在规划过程中按需获取特定的历史观测（先前步骤的截图）。代理在一个迭代循环中运行：它维护一份迄今为止任务的压缩记忆（即对过去交互的简明文本摘要），为语言模型提供轻量上下文。尽管该摘要覆盖了高层历史，代理仍可使用该工具回溯以检索其认为对当前决策重要的任意先前步骤的详细视觉信息。这种主动回顾机制模拟了人在不确定下一步动作时短暂查看过去屏幕或回忆特定细节的行为。通过将基于工具的检索整合到规划循环中，我们的代理能在必要时利用广泛的历史信息，而不会像天真地把大量视觉上下文全部输入那样引发上下文爆炸或干扰问题。


Training an agent to perform active look-back introduces unique challenges, as standard demonstration data lack both tool-use annotations and explicit reasoning traces. To bridge this gap, we construct a synthetic instruction-tuning dataset that augments raw trajectories from AndroidControl (Li et al. 2024) with tool-calling behavior. Concretely, we propose a four-stage deliberated look-back framework that guides a stronger teacher model to simulate when and how an agent should retrieve past observations. From these curated trajectories, we filter for correctness, rebalance samples to prevent retrieval recency bias, supplement with non-retrieval cases to avoid overfitting, and finally standardize into a structured dialogue format. The resulting dataset comprises 8.6K high-quality, step-level trajectories. we fine-tune a Qwen2.5-VL (Bai et al. 2025) backbone on this dataset, obtaining our PAL-UI agents in two sizes (PAL-UI-3B and PAL-UI-7B), both of which acquire the ability to reflect, decide when to look back, and act effectively in long-horizon GUI tasks.
训练一个能执行主动回顾的代理带来了独特挑战，因为标准示范数据既缺少工具使用标注，也缺少明确的推理轨迹。为弥补这一差距，我们构建了一个合成的指令微调数据集，将 AndroidControl（Li et al. 2024）的原始轨迹增强为包含工具调用行为。具体而言，我们提出了一个四阶段的审慎回顾框架，引导更强的教师模型模拟代理应何时以及如何检索过去观测。基于这些精心制作的轨迹，我们筛选正确样本、重新平衡以防检索近因偏差、补充非检索案例以避免过拟合，最终标准化为结构化对话格式。得到的数据集包含 8.6K 条高质量的步骤级轨迹。我们在该数据集上对 Qwen2.5-VL（Bai et al. 2025）骨干进行了微调，得到两种规模的 PAL-UI 代理（PAL-UI-3B 和 PAL-UI-7B），它们均获得了反思、决定何时回顾并在长时序 GUI 任务中有效行动的能力。


We evaluate our PAL-UI agent on a broad set of GUI navigation benchmarks. Experimental results demonstrate that PAL-UI significantly outperforms the base MLLM (without active look-back) on long-horizon mobile UI tasks, achieving new state-of-the-art performance under comparable training data settings. Notably, by effectively leveraging historical context, our method yields higher success rates than prior methods even with far fewer training examples. Moreover, although our training data and design focused on mobile app environments, the PAL-UI agent exhibits strong zero-shot transfer to other domains, such as web browser interfaces. It substantially improves task success on web-based GUI benchmarks compared to baselines, highlighting the generality of our approach.
我们在一系列 GUI 导航基准上评估了 PAL-UI 代理。实验结果表明，在长时序移动端 UI 任务上，PAL-UI 显著优于不带主动回顾的基础 MLLM，在相当的训练数据条件下取得了新的最先进性能。值得注意的是，通过有效利用历史上下文，我们的方法即使在训练样本远少于以往方法时也能获得更高的成功率。此外，尽管我们的训练数据与设计侧重于移动应用环境，PAL-UI 代理在零样本迁移到其他领域（如网页浏览器界面）时表现出强泛化性。在基于网页的 GUI 基准上，它相比基线大幅提升了任务成功率，凸显了我们方法的通用性。


## 2 RELATED WORK
## 2 相关工作


### 2.1 GUI AGENTS
### 2.1 GUI 代理


With the success of large language models (LLM) and multimodal large language models (MLLM), GUI Agents (Gou et al. 2024), Qin et al. 2025) have achieved significant advancement across various GUI platforms. Early GUI Agents (Zhang & Zhang 2023; Zheng et al. 2024a) usually rely on structured information such as HTML or accessibility trees for element localization, making them difficult to generalize across different platforms. Consequently, researches shift toward vision-based GUI Agents (Gou et al. 2024; Xu et al. 2024), which simply take screenshots as observations and interact with interfaces through human-like mouse and keyboard actions. Such paradigms enable end-to-end automation for cross-platform tasks, making progress toward broader applicability.
随着大型语言模型（LLM）和多模态大型语言模型（MLLM）的成功，GUI 代理（Gou et al. 2024；Qin et al. 2025）在各类 GUI 平台上取得了显著进展。早期的 GUI 代理（Zhang & Zhang 2023；Zheng et al. 2024a）通常依赖于结构化信息如 HTML 或无障碍树来定位元素，难以跨平台泛化。因此，研究逐渐转向基于视觉的 GUI 代理（Gou et al. 2024；Xu et al. 2024），它们直接将截图作为观测，并通过类人鼠标和键盘操作与界面交互。这类范式实现了跨平台任务的端到端自动化，推动了更广泛的适用性。


### 2.2 MEMORY MANAGEMENT FOR GUI AGENTS
### 2.2 GUI 代理的记忆管理


Previous LLM-based agent (Yao et al. 2023) systems usually manage memory simply by appending history information (observations and actions) directly to the input context. However, such paradigm introduces several technical challenges for vision-based GUI Agents. First, the observations for GUI agents exist in the form of screenshot images (Cheng et al. 2024). Storing all history screenshots can lead to high token costs as images consume a substantial number of tokens (Bai et al., 2025). Second, such approach will introduce multi-image inputs for the agent model, potentially impairing the model's reasoning capability (Zhao et al., 2024). Consequently, most existing approaches (Chen et al. 2024; Xu et al. 2024) retain only past actions or the few most recent past observations as memory (Qin et al., 2025). To address the limitation, we propose PAL-UI, a paradigm that allows agents to plan with active look-back. In this way, the agent can actively retrieve detailed information from history during inference, thereby mitigating the potential information loss in previous paradigms.
以往基于 LLM 的代理（Yao et al. 2023）系统通常通过将历史信息（观测和动作）直接附加到输入上下文中来管理记忆。然而，这种范式对基于视觉的 GUI 代理引入了若干技术挑战。首先，GUI 代理的观测以截图图像形式存在（Cheng et al. 2024）。存储全部历史截图会导致高代币成本，因为图像消耗大量代币（Bai et al., 2025）。其次，这种方法会引入多图像输入，可能损害模型的推理能力（Zhao et al., 2024）。因此，大多数现有方法（Chen et al. 2024；Xu et al. 2024）仅保留过去的动作或最近的少量观测作为记忆（Qin et al., 2025）。为了解决该限制，我们提出 PAL-UI，一种允许代理带主动回顾进行规划的范式。通过这种方式，代理可以在推理期间主动从历史中检索详细信息，从而缓解以往范式可能导致的信息丢失问题。


### 2.3 MLLM-BASED TOOL-USE AGENTS
### 2.3 基于 MLLM 的工具使用代理


Enhancing MLLMs through tool calling has recently emerged as a popular direction, as external tools enable MLLMs to transcend their capability bottlenecks and improve their performance. Early approaches (Wu et al. 2023, Yang et al. 2023c) often employed training-free prompting methods to invoke tools and enhance the model's visual perception abilities. Subsequent work, such as LLaVA-Plus Liu et al. (2024) and GPT4tools (Yang et al. 2023b), further strengthened MLLMs' tool-use capabilities by synthesizing high-quality tool-calling trajectories and performing supervised fine-tuning. With the development of slow-thinking reasoning paradigms (Jaech et al., 2024; Guo et al. 2025), more recent studies have shifted toward a "Thinking with images" approach (Su et al. 2025) Zhou et al. 2025), where tools are invoked during the reasoning process to edit input images, leading to significant improvements in the model's reasoning performance. As for MLLM-based GUI agents, early methods (Zheng et al. 2024a) often relied on Set-of-Marks (SoM) (Yang et al. 2023a) or accessibility trees to provide additional on-screen information. Recently, FOCUS (Tang et al. 2025) proposed a dual-system framework comprising fast and slow prediction mechanisms to enhance GUI grounding. To the best of our knowledge, we are the first to leverage external tools to improve long-horizon planning ability in GUI agents through an active look-back mechanism.
通过调用外部工具来增强 MLLM 最近成为热门方向，因外部工具可使 MLLM 超越能力瓶颈并提升表现。早期方法（Wu et al. 2023, Yang et al. 2023c）常用无训练的提示策略来调用工具并增强模型的视觉感知能力。随后工作，如 LLaVA-Plus Liu et al. (2024) 和 GPT4tools (Yang et al. 2023b)，通过合成高质量的调用轨迹并进行监督微调，进一步强化了 MLLM 的工具使用能力。随着慢思考推理范式的发展（Jaech et al., 2024; Guo et al. 2025），近期研究更多转向“与图像一起思考”方法（Su et al. 2025）Zhou et al. 2025），在推理过程中调用工具以编辑输入图像，从而显著提升模型的推理性能。关于基于 MLLM 的 GUI 代理，早期方法（Zheng et al. 2024a）常依赖 Set-of-Marks (SoM) (Yang et al. 2023a) 或无障碍树以提供额外的屏幕信息。最近，FOCUS (Tang et al. 2025) 提出一个包含快速与慢速预测机制的双系统框架以增强 GUI 定位。据我们所知，我们是首个通过主动回溯机制利用外部工具来提升 GUI 代理长远规划能力的工作。


## 3 PROBLEM FORMULATION
## 3 问题表述


We consider a GUI-based sequential decision process where an agent must achieve a specified goal by interacting with a user interface. Formally,at each time step $i$ ,the agent receives an observation ${o}_{i}$ of the current GUI state (e.g.,a screenshot or UI view) and selects an action ${a}_{i}$ from the set of possible interface actions (such as clicking a button,entering text,or scrolling). Executing action ${a}_{i}$ changes the interface state,leading to a new observation ${o}_{i + 1}$ . The process continues until the agent accomplishes the global goal $G$ ,which is given as part of the task (typically a high-level instruction or target outcome), or until a maximum number of steps is reached.
我们考虑一个基于 GUI 的序列决策过程，其中代理必须通过与用户界面交互来完成指定目标。形式化地，在每个时间步 $i$，代理接收当前 GUI 状态的观测 ${o}_{i}$（例如截图或界面视图），并从可选界面操作集合中选择一个动作 ${a}_{i}$（如点击按钮、输入文本或滚动）。执行动作 ${a}_{i}$ 会改变界面状态，产生新的观测 ${o}_{i + 1}$。该过程持续直到代理完成作为任务一部分给定的全局目标 $G$（通常是高级指令或目标结果），或直到达到最大步数。


Each task thus forms a trajectory $\tau  = \left( {{o}_{0},{a}_{1},{o}_{1},{a}_{2},{o}_{2},\ldots ,{a}_{T},{o}_{T}}\right)$ ,where ${o}_{0}$ is the initial observation and $T$ is the total number of steps. The core challenge in this setting lies in long-horizon planning and memory management: as $T$ grows,the agent accumulates a large history of observations (each potentially high-dimensional, such as images with text) and actions. A naive strategy of feeding the entire raw history into a vision-based agent quickly becomes impractical due to context length limits and redundant information. In fact, without a mechanism to compress and retrieve relevant information from past observations, an LLM-based planner may suffer performance declines when important details from earlier steps get lost in a sea of tokens. Our goal is to enable the agent to retain critical information from its interaction history and actively look back to past states when necessary, all while staying within feasible context lengths.
因此每个任务形成一条轨迹 $\tau  = \left( {{o}_{0},{a}_{1},{o}_{1},{a}_{2},{o}_{2},\ldots ,{a}_{T},{o}_{T}}\right)$，其中 ${o}_{0}$ 是初始观测，$T$ 是总步数。本设置的核心挑战在于长时程规划与记忆管理：随着 $T$ 增长，代理会积累大量历史观测（每条可能是高维的，例如带文本的图像）和动作。将整个原始历史喂入基于视觉的代理的简单策略很快因上下文长度限制和冗余信息而变得不可行。事实上，如果没有将过去观测中相关信息压缩并检索的机制，基于 LLM 的规划器在早期步骤的重要细节在大量 token 中丢失时可能出现性能下降。我们的目标是使代理能够保留交互历史中的关键信息，并在必要时主动回溯到过去状态，同时保持在可行的上下文长度内。


We follow previous studies (Bai et al. 2025, Qin et al., 2025) to adopt a unified action space for interacting with the GUI interface. Specifically, we introduce the following three types of actions:
我们沿用先前研究（Bai et al. 2025, Qin et al., 2025）采用统一的与 GUI 交互的动作空间。具体地，我们引入以下三类动作：


- General actions across all platforms: Click, Type, Scroll, Drag, Wait, and Finished;
- 跨平台通用动作：Click、Type、Scroll、Drag、Wait 和 Finished；


- Actions for mobile platform: LongPress,OpenApp, PressHome, and PressBack;
- 移动平台动作：LongPress、OpenApp、PressHome 和 PressBack；


- Actions for web platform: Hotkey, LeftDouble, and RightSingle.
- Web 平台动作：Hotkey、LeftDouble 和 RightSingle。


Note that some actions may require an action-related content (e.g., coordinates for Click and textual string for Type). During inference, we prompt the GUI agent to generate a reasoning process and a predicted action. Following Lu et al. (2025), the reasoning process and predicted action are encircled in <think></think> and <tool_use></tool_use> tokens, respectively.
注意，某些动作可能需要与动作相关的内容（例如 Click 的坐标和 Type 的文本字符串）。在推理时，我们提示 GUI 代理生成推理过程和预测动作。遵循 Lu et al. (2025)，推理过程与预测动作分别用 <think></think> 与 <tool_use></tool_use> 标记包裹。


<img src="https://cdn.noedgeai.com/bo_d4nfubv7aajc73frsadg_3.jpg?x=320&y=225&w=1159&h=584&r=0"/>



Figure 1: The illustration of our proposed PAL-UI agent. We utilize an observation-level captioner and an action-level validator for memory compression. Then, equipped with a retrieval tool, the agent is able to actively recall detailed visual information from past memory at inference time.
图 1：我们提出的 PAL-UI 代理示意。我们利用观测级标题生成器与动作级验证器对记忆进行压缩。随后，配备检索工具的代理能够在推理时主动从过去记忆中召回详细的视觉信息。


## 4 APPROACH
## 4 方法


We propose PAL-UI (Planning with Active Look-back), a framework to enhance the long-horizon planning capabilities of a GUI agent by combining dual-level summarization of the history with an active retrieval mechanism. Figure 1 gives an overview of our approach. In particular, PAL-UI compresses the interaction history into a succinct textual memory and equips the agent with a special tool to fetch detailed visual information from past steps on demand. The agent is trained via supervised fine-tuning on a dataset of expert trajectories augmented with tool-use demonstrations. We organize our approach as follows: first, we introduce the construction of the summarization agent and the retrieval tool (Section 4.1); second, we explain how the agent plans with active look-back at inference time (Section 4.2); finally, we describe the training data generation pipeline used to impart these capabilities to the agent (Section 4.3).
我们提出 PAL-UI（带主动回溯的规划），一种通过将双层历史摘要与主动检索机制结合来增强 GUI 代理长远规划能力的框架。图 1 展示了我们方法的概览。具体而言，PAL-UI 将交互历史压缩为简洁的文本记忆，并为代理配备了一个专用工具，以便在需要时从过去步骤中按需获取详细的视觉信息。代理通过在包含工具使用示范的专家轨迹数据集上进行有监督微调来训练。我们按如下方式组织本文：首先介绍摘要代理与检索工具的构建（第 4.1 节）；其次解释代理在推理时如何进行带主动回溯的规划（第 4.2 节）；最后描述用于将这些能力传授给代理的训练数据生成管道（第 4.3 节）。


### 4.1 Tool Construction and Summary Agent
### 4.1 工具构建与摘要代理


To prevent the agent's context from being overwhelmed by redundant visual tokens, we compress the history at two levels, i.e., observations and actions, using a dual-level summary agent. Meanwhile, we introduce a memory retrieval tool that the agent can invoke to recover raw observations from any past step when detailed information is needed.
为防止多余的视觉 token 使代理上下文超负荷，我们通过双层摘要代理在观察和动作两个层面压缩历史。同时，我们引入一个记忆检索工具，代理可在需要详细信息时调用该工具以恢复任意过去步骤的原始观测。


Observation-level Captioner. We first summarize each new visual observation ${o}_{i}$ to capture the key information relevant to the task. Even though a GUI screenshot may contain crucial cues for the long-term goal, these cues are often hidden among numerous UI elements. Simply appending every raw image $\left\{  {{o}_{1},{o}_{2},\ldots ,{o}_{i - 1}}\right\}$ to the prompt would rapidly exhaust the context window as $i$ grows, and the model could struggle to locate important details in the noise. Therefore, for each step we generate a concise observation caption that highlights salient information in ${o}_{i}$ (especially text on the screen or unique interface changes) in relation to the global goal $G$ . We employ the large vision-language model Qwen-2.5-VL (Bai et al. 2025) as our captioning module, due to its strong OCR capability and instruction-following performance. The captioner is prompted with the current screenshot ${o}_{i}$ and the task goal $G$ to produce a brief description focusing on the most relevant UI content (e.g., widget labels, displayed messages, or enabled/disabled states).
观察层描述器。我们首先对每个新的视觉观测 ${o}_{i}$ 进行摘要，以捕捉与任务相关的关键信息。尽管 GUI 截图可能包含对长期目标至关重要的线索，这些线索常常被大量 UI 元素掩盖。简单地将每个原始图像 $\left\{  {{o}_{1},{o}_{2},\ldots ,{o}_{i - 1}}\right\}$ 附加到提示中会随着 $i$ 增长快速耗尽上下文窗口，而且模型可能难以在噪声中定位重要细节。因此，对于每一步我们生成简洁的观测说明，突出与全局目标 $G$ 相关的显著信息（尤其是屏幕上的文本或独特的界面变化）。我们采用具有强大 OCR 能力和指令遵循性能的大型视觉-语言模型 Qwen-2.5-VL（Bai 等，2025）作为描述模块。描述器以当前截图 ${o}_{i}$ 和任务目标 $G$ 作为提示，生成聚焦于最相关 UI 内容的简短描述（例如组件标签、显示信息或启用/禁用状态）。


Action-level Validator. In addition to summarizing observations, we also summarize the outcome of each action to form an action memory. Prior works on GUI agents often record only the action sequence as history (Chen et al. 2024; Xu et al. 2024), but without the surrounding state context, a plain list of actions provides limited insight into the task progress. Moreover, those approaches typically do not verify if an action succeeded, leaving the agent prone to repeating failed actions or getting stuck in loops. To address this, we introduce an action-level validator, which produces a compact description of the agent’s last action ${a}_{i}$ and evaluates its execution result by comparing the interface before and after the action. Using the same Qwen-2.5-VL model, we prompt it with the triplet: current step’s action ${a}_{i}$ ,the pre-action observation ${o}_{i}$ ,and the post-action observation ${o}_{i + 1}$ . The validator then generates a brief assessment,for example,"The user typed "football" into the search bar, and the search results for related content are displayed.". This summary serves two purposes: it clarifies the intent of ${a}_{i}$ in natural language (which can be easier for the LLM to reason with than raw action code) and it confirms whether ${a}_{i}$ achieved the expected effect on the GUI state.
动作层验证器。除对观测进行摘要外，我们还对每个动作的结果进行摘要以形成动作记忆。先前关于 GUI 代理的工作常仅记录动作序列作为历史（Chen 等，2024；Xu 等，2024），但缺乏周围状态上下文时，单纯的动作列表对任务进展提供的洞见有限。此外，那些方法通常不验证动作是否成功，导致代理易于重复失败动作或陷入循环。为此，我们引入动作层验证器，它生成对代理上一次动作 ${a}_{i}$ 的精简描述，并通过比较动作前后的界面来评估其执行结果。我们使用相同的 Qwen-2.5-VL 模型，以三元组作为提示：当前步骤的动作 ${a}_{i}$、动作前的观测 ${o}_{i}$ 和动作后的观测 ${o}_{i + 1}$。验证器随后生成简短评估，例如，“用户在搜索栏中输入了 "football"，并显示了相关内容的搜索结果。”该摘要有两个用途：以自然语言阐明 ${a}_{i}$ 的意图（这比原始动作代码更便于 LLM 推理）并确认 ${a}_{i}$ 是否对 GUI 状态产生了预期效果。


Memory Retrieval Tool. While the dual-level summaries dramatically condense the history, converting rich visual observations into text can inevitably lose some details, e.g., the exact appearance of a screen or an unseen UI element that later becomes relevant. To allow the agent to recover such details when needed, we implement a Retrieve tool (Retrieve) for active look-back. The Retrieve tool can be invoked with a past step index $j$ and returns the observation ${o}_{j}$ (e.g.,the screenshot image at step $j\left( {j < i}\right)$ ) to the agent’s context. We add this tool to the agent’s action space,so it can decide at any point to retrieve a past screen instead of executing a normal GUI action. The retrieved image ${o}_{j}$ is then appended to the agent’s current context,enabling the model to re-inspect that state before continuing the plan. In essence, the agent has the choice to temporarily step back and "refresh its memory" of a previous interface state.
记忆检索工具。尽管双层摘要能大幅压缩历史，将丰富的视觉观察转为文本不可避免地会丢失一些细节，例如屏幕的精确外观或后来变得相关但未被看到的界面元素。为使代理在需要时能恢复这些细节，我们实现了一个用于主动回溯的检索工具（Retrieve）。检索工具可使用过去步骤索引 $j$ 调用，并将观测结果 ${o}_{j}$（例如第 $j\left( {j < i}\right)$ 步的截图图像）返回到代理的上下文。我们将此工具加入代理的动作空间，使其可以在任何时刻决定检索过去的屏幕而不是执行常规 GUI 动作。检索到的图像 ${o}_{j}$ 随后被追加到代理的当前上下文中，使模型在继续计划前能够重新检查该状态。本质上，代理可以选择暂时回退并“刷新记忆”以回顾先前的界面状态。


### 4.2 ACTIVE LOOK-BACK PLANNING PROCESS
### 4.2 主动回溯规划过程


With the ability to summarize history and retrieve past observations, the agent plans actions in a think-act loop that actively looks back when necessary. At each step $i$ ,the agent’s context includes: (1) the compressed memory ${m}_{i}$ up to step $i$ (consisting of relevant summaries of past observations and actions),(2) the current raw observation ${o}_{i}$ (the latest screenshot of the GUI),and (3) the task goal $G$ . Using this context,the agent generates either a concrete GUI action ${a}_{i}$ or a retrieval query. There are two possible outcomes for the planning at step $i$ :
在具备历史摘要与检索过去观测的能力后，代理在一个思考—执行循环中规划动作，必要时主动回溯。在每个步骤 $i$，代理的上下文包括：(1) 到步 $i$ 的压缩记忆 ${m}_{i}$（由过去观测和动作的相关摘要组成），(2) 当前的原始观测 ${o}_{i}$（最新的 GUI 截图），以及 (3) 任务目标 $G$。基于此上下文，代理生成具体的 GUI 动作 ${a}_{i}$ 或检索查询。步骤 $i$ 的规划有两种可能的结果：


- Direct Action Prediction: In the typical case,the agent uses the context $\left\{  {{m}_{i},{o}_{i},G}\right\}$ to directly predict the next GUI action ${a}_{i}$ (discussed in Section 3). This prediction is informed by the distilled knowledge in the summaries such as what has been done so far and what key information is on the current screen, allowing the agent to decide the best next step toward the goal.
- 直接动作预测：在典型情况下，代理使用上下文 $\left\{  {{m}_{i},{o}_{i},G}\right\}$ 直接预测下一个 GUI 动作 ${a}_{i}$（在第 3 节讨论）。该预测受摘要中提炼出的知识指导，如已完成的操作和当前屏幕上的关键信息，从而使代理决定朝目标前进的最佳下一步。


- Active Retrieval then Action: If the agent is uncertain or needs more detail from a previous state, it can choose to invoke the Retrieve action instead of immediately outputting a GUI action. For example, the model might decide it needs to "look back at the login screen (step 2) to recall the exact error message" before deciding how to proceed. When the retrieve tool is called, the specified past screenshot ${o}_{j}$ is fetched and added to the context. The planning then continues with the augmented context $\left\{  {{m}_{i},{o}_{i},{o}_{j},G}\right\}$ ,and the agent produces the final action ${a}_{i}$ based on both the current state and the newly recalled information from step $j$ . By integrating this look-back step into the reasoning process,the agent can significantly improve its prediction accuracy for ${a}_{i}$ ,especially in situations where subtle details from earlier in the task are needed to choose the correct action.
- 先主动检索然后动作：如果代理不确定或需要从先前状态获取更多细节，它可以选择调用 Retrieve 动作，而不是立即输出 GUI 动作。例如，模型可能决定需要“回看登录屏（步骤 2）以回忆确切的错误信息”，然后再决定下一步。当调用检索工具时，指定的过去截图 ${o}_{j}$ 被取回并加入上下文。随后规划在增强后的上下文 $\left\{  {{m}_{i},{o}_{i},{o}_{j},G}\right\}$ 中继续，代理基于当前状态和从步骤 $j$ 新召回的信息生成最终动作 ${a}_{i}$。通过将此回溯步骤纳入推理流程，代理在需要早期任务中的细微细节以选择正确动作的情况下，可以显著提高对 ${a}_{i}$ 的预测准确性。


Overall, this planning with active look-back mechanism enables the agent to dynamically balance remembering (through compact summaries) and recalling (through targeted retrieval) as it navigates toward the global task goal.
总体而言，这种带主动回溯的规划机制使代理在朝向全局任务目标航行时，能动态地在记忆（通过紧凑摘要）与回忆（通过有针对性的检索）之间取得平衡。


### 4.3 TRAINING DATA CONSTRUCTION
### 4.3 训练数据构建


Equipping the agent with the above capabilities requires appropriate training data that demonstrates when and how to use the summarization context and retrieval tool. However, standard human demonstration data do not contain examples of tool usage or reasoning traces. We therefore create a synthesized fine-tuning dataset of tool-augmented trajectories via a combination of distillation from a stronger model and careful data curation. The process consists of four main steps:
赋予代理上述能力需要合适的训练数据来示范何时以及如何使用摘要上下文和检索工具。然而，标准的人类示范数据不包含工具使用或推理轨迹的示例。因此，我们通过从更强模型的蒸馏和仔细的数据策划相结合，创建了一个合成的带工具增强轨迹的微调数据集。该过程包括四个主要步骤：


Seed Trajectory Collection. We begin with a large set of human demonstration trajectories from the AndroidControl (Li et al. 2024) training dataset, each consisting of a sequence of GUI screen-shots $\left\{  {{o}_{0},{o}_{1},\ldots ,{o}_{T}}\right\}$ ,human actions $\left\{  {{a}_{1},\ldots ,{a}_{T}}\right\}$ ,and a global task goal $G$ . These serve as the ground-truth action sequences that our agent should ideally replicate. For each step in each trajectory, we run our dual-level summary agent on the ground-truth trajectory to generate the compressed memory ${m}_{i}$ summarizing all history up to step $i$ (from 0 to $T$ ).
种子轨迹收集。我们从 AndroidControl（Li 等，2024）训练数据集中开始，收集大量人类示范轨迹，每条包含一序列 GUI 截图 $\left\{  {{o}_{0},{o}_{1},\ldots ,{o}_{T}}\right\}$、人类动作 $\left\{  {{a}_{1},\ldots ,{a}_{T}}\right\}$ 和全局任务目标 $G$。这些作为代理理想上应复制的真实动作序列。对于每条轨迹中的每一步，我们在真实轨迹上运行我们的双层摘要代理，以生成到步 $i$ 为止总结所有历史的压缩记忆 ${m}_{i}$（从 0 到 $T$）。


Tool-Use Calling Curation. Given that seed trajectories consist only of observations and ground-truth actions, they lack explicit tool-use steps. To construct training trajectories that include retrieval behavior, we require the teacher model to simulate when and how an agent would invoke the Retrieve tool. However, prompting the teacher model directly to produce tool calls proved ineffective, as the base model was not pretrained on such interactions. To address this gap, we design a four-stage deliberated look-back framework that gradually guides the teacher to integrate retrieval into its reasoning. At each step, the teacher model is prompted to perform the following stages:
工具调用示例生成。由于种子轨迹仅包含观察和真实动作，缺少明确的工具使用步骤。为构建包含检索行为的训练轨迹，我们要求教师模型模拟代理何时以及如何调用 Retrieve 工具。然而直接提示教师模型生成工具调用效果不佳，因为基础模型未在此类交互上预训练。为弥补这一差距，我们设计了一个四阶段的回顾式引导框架，逐步引导教师将检索融入推理。每一步，教师模型被提示执行以下阶段：


- History Revision: The model reviews the compressed memory ${m}_{i}$ to assess task progress toward the global goal $G$ . This ensures a clear understanding of what has been achieved so far.
- 历史修订：模型回顾压缩记忆 ${m}_{i}$ 以评估相对于全局目标 $G$ 的任务进展，确保清楚理解迄今已完成的内容。


- Candidate Proposals: The model proposes several plausible next actions based on the current observation ${o}_{i}$ ,encouraging it to enumerate possible strategies rather than committing prematurely.
- 候选提案：模型基于当前观察 ${o}_{i}$ 提出若干合理的下一步动作，鼓励枚举可能策略而非过早确定。


- Confidence Evaluation: The model reflects on how confident it is in each proposed action and whether it feels uncertain enough to warrant checking something in the past. If the model determines that a specific detail from an earlier step is needed, it should invoke the Retrieve tool.
- 置信度评估：模型反思对每个提议动作的置信度，以及是否不确定到需要检查过去某些信息。如果模型判断需要早期步骤的具体细节，就应调用 Retrieve 工具。


- Tool-Use Action Prediction: Depending on the previous stage, the model will receive the retrieved observation ${o}_{j}$ (from step $j$ ) if retrieval was requested and then outputs ${a}_{i}$ with the additional context.
- 工具使用动作预测：视前一阶段而定，若请求检索模型将收到检索到的观察 ${o}_{j}$（来自步骤 $j$），然后在附加上下文下输出 ${a}_{i}$。


Data Filtering and Balancing. After synthesizing a large set of trajectories with tool calling, we then filter and balance this data to focus on high-quality examples. First, we discard any sample where the teacher’s final predicted action ${a}_{i}$ does not match the ground-truth human action for that step, ensuring our training data only contains correct predictions. Among the remaining samples, we identify those in which the teacher actually utilized the retrieval tool and succeeded in choosing the correct action afterward. We found about 4.3K such high-quality tool-use cases. Since the teacher model tended to prefer looking at very recent steps, which might bias the agent to only look back one step,we increased the sampling weight of samples where the retrieved step $j$ was further back in history. Finally, to prevent the agent from overusing the tool when it's not necessary, we also include 4.3K high-quality samples where the teacher solved the step without any retrieval (direct action with correct outcome). This yields a balanced dataset of roughly 8.6K step-level samples, half with tool use and half without, all with correct decisions.
数据过滤与平衡。在合成大量包含工具调用的轨迹后，我们对这些数据进行过滤和平衡以聚焦高质量样本。首先删除任何教师最终预测动作 ${a}_{i}$ 与该步骤的人类真实动作不匹配的样本，确保训练数据只包含正确预测。在剩余样本中，我们识别出教师实际使用检索工具并随后成功选择正确动作的样本。我们发现约有 4.3K 个此类高质量工具使用案例。由于教师模型倾向于查看非常近期的步骤，这可能导致代理只回看一步的偏差，我们提高了检索步骤 $j$ 更远历史样本的采样权重。最后，为防止代理在无需检索时过度使用工具，我们还包含了 4.3K 个教师在无需检索即可解决且结果正确的高质量样本（直接动作且结果正确）。最终得到约 8.6K 个步级样本的均衡数据集，一半使用工具一半不使用，均为正确决策。


Compilation and Formatting. After obtaining data samples, we compile them into the Qwen2.5- VL format for supervised fine-tuning (SFT) (Bai et al. 2025). At each step, the input consists of the system prompt,compressed memory ${m}_{i}$ ,and the current observation ${o}_{i}$ ,while the output is composed of the reasoning process and actions. To make a coherent reasoning, we first employ a strong LLM, Qwen3-32B (Yang et al. 2025), to synthesize all model responses from each stage of the above deliberated look-back framework into a logical natural language explanation. This explanation will serve as the thinking process, enclosed in <think></think> tokens. Finally, the action (Retrieval or other types of actions in Section 3) is encircled by <tool_use></tool_use> tokens and the retrieved screenshot will be injected into the next step. This standardized format directly teaches the agent how to reflect, when to look back, and how to act effectively.
汇编与格式化。在获得数据样本后，我们将其编译为 Qwen2.5-VL 格式用于监督微调 (SFT) (Bai 等，2025)。每个步骤的输入由系统提示、压缩记忆 ${m}_{i}$ 和当前观察 ${o}_{i}$ 组成，输出由推理过程与动作构成。为形成连贯的推理，我们首先使用强大的大模型 Qwen3-32B (Yang 等，2025) 将上述回顾式引导框架各阶段的模型回应合成为合乎逻辑的自然语言解释。该解释作为思考过程，置于 <think></think> 标记内。最终，动作（检索或第 3 节中的其他类型动作）用 <tool_use></tool_use> 标记包裹，检索到的截图将注入下一步。此标准化格式直接教会代理如何反思、何时回顾以及如何有效行动。


## 5 EXPERIMENTS
## 5 实验


### 5.1 EVALUATION BENCHMARKS
### 5.1 评估基准


We evaluate our PAL-UI agent on two high-level GUI planning tasks: AndroidControl-High (Li et al., 2024) and GUI-Odyssey (Lu et al., 2024). For all tasks, we adopt a subtask-level evaluation paradigm where the model predicts the next action based on the global task goal, current screenshot, and historical memory. Following prior work (Gou et al., 2024; Xu et al., 2024), we report the type match score (Type), grounding accuracy (GR), and step success rate (SR) for the two benchmarks.
我们在两项高级 GUI 规划任务上评估 PAL-UI 代理：AndroidControl-High (Li 等，2024) 与 GUI-Odyssey (Lu 等，2024)。对于所有任务，我们采用子任务级评估范式，模型基于全局任务目标、当前截图与历史记忆预测下一步动作。遵循先前工作（Gou 等，2024；Xu 等，2024），我们报告类型匹配得分（Type）、定位准确率（GR）和步成功率（SR）。


Table 1: Results on two mobile GUI datasets. "Method" indicates the training and inference method. "ZS", "SFT" and "RFT" are short for zero-shot, supervised fine-tuning, and reinforcement fine-tuning, respectively. Bold and underline denote the best and second best results.
表 1：两套移动 GUI 数据集上的结果。“Method” 表示训练与推理方法。“ZS”、“SFT” 与 “RFT” 分别代表零-shot、监督微调与强化微调。加粗与下划线分别表示最佳与次佳结果。


<table><tr><td rowspan="2">Model</td><td rowspan="2">Method</td><td colspan="3">AndroidControl-High</td><td colspan="3">GUI-Odyssey</td><td rowspan="2">Overall</td></tr><tr><td>Type</td><td>GR</td><td>SR</td><td>Type</td><td>GR</td><td>SR</td></tr><tr><td>GPT-40</td><td>ZS</td><td>63.1</td><td>30.9</td><td>21.2</td><td>37.5</td><td>14.2</td><td>5.4</td><td>28.7</td></tr><tr><td>Qwen2.5-VL-3B</td><td>ZS</td><td>47.8</td><td>46.5</td><td>38.9</td><td>37.4</td><td>26.5</td><td>26.7</td><td>37.3</td></tr><tr><td>Qwen2.5-VL-7B</td><td>ZS</td><td>68.7</td><td>59.7</td><td>47.1</td><td>55.6</td><td>37.8</td><td>34.4</td><td>50.6</td></tr><tr><td>OS-Atlas-4B</td><td>SFT</td><td>49.0</td><td>49.5</td><td>22.8</td><td>49.6</td><td>34.6</td><td>20.3</td><td>37.6</td></tr><tr><td>OS-Atlas-7B</td><td>SFT</td><td>57.4</td><td>54.9</td><td>29.8</td><td>60.4</td><td>39.7</td><td>27.0</td><td>44.8</td></tr><tr><td>NaviMaster-7B</td><td>SFT</td><td>72.9</td><td>-</td><td>54.0</td><td>64.4</td><td>-</td><td>36.9</td><td>-</td></tr><tr><td>UI-R1-3B</td><td>RFT</td><td>57.8</td><td>55.7</td><td>45.4</td><td>52.2</td><td>34.5</td><td>32.5</td><td>46.4</td></tr><tr><td>GUI-R1-3B</td><td>RFT</td><td>58.0</td><td>56.2</td><td>46.5</td><td>54.8</td><td>41.5</td><td>41.3</td><td>49.8</td></tr><tr><td>GUI-R1-7B</td><td>RFT</td><td>71.6</td><td>65.6</td><td>51.7</td><td>65.5</td><td>43.6</td><td>38.8</td><td>56.1</td></tr><tr><td>PAL-UI-3B</td><td>SFT</td><td>60.4</td><td>58.7</td><td>49.3</td><td>56.7</td><td>36.9</td><td>34.6</td><td>49.4</td></tr><tr><td>PAL-UI-7B</td><td>SFT</td><td>71.3</td><td>70.5</td><td>57.8</td><td>65.1</td><td>46.8</td><td>41.7</td><td>58.9</td></tr></table>
<table><tbody><tr><td rowspan="2">模型</td><td rowspan="2">方法</td><td colspan="3">AndroidControl-高</td><td colspan="3">GUI-Odyssey</td><td rowspan="2">总体</td></tr><tr><td>类型</td><td>GR</td><td>SR</td><td>类型</td><td>GR</td><td>SR</td></tr><tr><td>GPT-40</td><td>ZS</td><td>63.1</td><td>30.9</td><td>21.2</td><td>37.5</td><td>14.2</td><td>5.4</td><td>28.7</td></tr><tr><td>Qwen2.5-VL-3B</td><td>ZS</td><td>47.8</td><td>46.5</td><td>38.9</td><td>37.4</td><td>26.5</td><td>26.7</td><td>37.3</td></tr><tr><td>Qwen2.5-VL-7B</td><td>ZS</td><td>68.7</td><td>59.7</td><td>47.1</td><td>55.6</td><td>37.8</td><td>34.4</td><td>50.6</td></tr><tr><td>OS-Atlas-4B</td><td>SFT</td><td>49.0</td><td>49.5</td><td>22.8</td><td>49.6</td><td>34.6</td><td>20.3</td><td>37.6</td></tr><tr><td>OS-Atlas-7B</td><td>SFT</td><td>57.4</td><td>54.9</td><td>29.8</td><td>60.4</td><td>39.7</td><td>27.0</td><td>44.8</td></tr><tr><td>NaviMaster-7B</td><td>SFT</td><td>72.9</td><td>-</td><td>54.0</td><td>64.4</td><td>-</td><td>36.9</td><td>-</td></tr><tr><td>UI-R1-3B</td><td>RFT</td><td>57.8</td><td>55.7</td><td>45.4</td><td>52.2</td><td>34.5</td><td>32.5</td><td>46.4</td></tr><tr><td>GUI-R1-3B</td><td>RFT</td><td>58.0</td><td>56.2</td><td>46.5</td><td>54.8</td><td>41.5</td><td>41.3</td><td>49.8</td></tr><tr><td>GUI-R1-7B</td><td>RFT</td><td>71.6</td><td>65.6</td><td>51.7</td><td>65.5</td><td>43.6</td><td>38.8</td><td>56.1</td></tr><tr><td>PAL-UI-3B</td><td>SFT</td><td>60.4</td><td>58.7</td><td>49.3</td><td>56.7</td><td>36.9</td><td>34.6</td><td>49.4</td></tr><tr><td>PAL-UI-7B</td><td>SFT</td><td>71.3</td><td>70.5</td><td>57.8</td><td>65.1</td><td>46.8</td><td>41.7</td><td>58.9</td></tr></tbody></table>


### 5.2 IMPLEMENTATION DETAILS
### 5.2 实施细节


Following existing studies (Luo et al. 2025), we employ Qwen2.5-VL (Bai et al. 2025) as the backbone model and use Qwen2.5-VL-7B as the summary agent for its potential in basic GUI scene understanding. All training and evaluation processes are conducted on 8 NVIDIA A100-80G GPUs. For SFT training, we follow previous studies and utilize LLaMA-Factory framework (Zheng et al. 2024b). We set the global batch size to 8 and learning rate to ${1e} - 5$ . During inference,we utilize the same prompt with unified tool-calling method across all experiments to ensure a fair comparison.
遵循现有研究（Luo et al. 2025），我们采用 Qwen2.5-VL（Bai et al. 2025）作为骨干模型，并选用 Qwen2.5-VL-7B 作为摘要代理，因其在基础 GUI 场景理解上的潜力。所有训练与评估均在 8 块 NVIDIA A100-80G GPU 上进行。对于 SFT 训练，我们沿用先前工作并使用 LLaMA-Factory 框架（Zheng et al. 2024b）。将全局批量大小设为 8，学习率为 ${1e} - 5$。推理时，我们在所有实验中使用相同的提示和统一的工具调用方法以确保公平比较。


### 5.3 RESULTS
### 5.3 结果


Table 1 presents the comprehensive comparison with state-of-the-art methods in low-data setting, including zero-shot (ZS), supervised fine-tuning (SFT), and reinforcement fine-tuning (RFT).
表 1 展示了在低数据设置下与最先进方法的综合比较，包括零样本（ZS）、监督微调（SFT）和强化微调（RFT）。


Comparison with Base Models. Compared to the base models Qwen2.5-VL-3B and Qwen2.5- VL-7B under the zero-shot setting, our PAL-UI agents achieve substantial gains across all metrics. PAL-UI-7B reaches an overall score of 58.9%, an absolute improvement of 8.3% over Qwen2.5- VL-7B (50.6%). The boost is especially clear in Success Rate (SR): PAL-UI-7B achieves 57.8% on AndroidControl-High, surpassing the base model by 10.7%. Despite being trained only on Android-Control, PAL-UI also generalizes well to the out-of-domain GUI-Odyssey benchmark, where PAL-UI-7B improves SR to 41.7%, a 7.3% gain. These consistent improvements across both in-domain and out-of-domain tasks highlight the robustness and generalization capacity of our approach.
与基础模型的比较。与零样本设定下的基础模型 Qwen2.5-VL-3B 与 Qwen2.5-VL-7B 相比，我们的 PAL-UI 代理在所有指标上均取得了显著提升。PAL-UI-7B 的整体得分为 58.9%，比 Qwen2.5-VL-7B（50.6%）绝对提升 8.3%。提升在成功率（SR）上尤为明显：PAL-UI-7B 在 AndroidControl-High 上达到 57.8%，比基础模型高出 10.7%。尽管仅在 Android-Control 上训练，PAL-UI 在跨域的 GUI-Odyssey 基准上也表现良好，PAL-UI-7B 将 SR 提升至 41.7%，增加 7.3%。这些在域内与域外任务上的一致提升突显了我们方法的鲁棒性与泛化能力。


Comparison with State-of-the-Art Methods. PAL-UI further outperforms existing SFT and RFT baselines. PAL-UI-7B achieves the best overall score of 58.9%, surpassing GUI-R1-7B (56.1%) by 2.8% and outperforming the strongest SFT method NaviMaster-7B by a large margin. It establishes new state-of-the-art results in multiple metrics, including GR (70.5% on AndroidControl-High and 46.8% on GUI-Odyssey) and SR (57.8% and 41.7%, respectively). Notably, PAL-UI-7B outperforms GUI-R1-7B despite relying only on SFT rather than reinforcement learning, showing that our approach yields stronger results with lower training complexity. Finally, scaling from PAL-UI-3B to PAL-UI-7B consistently improves performance, validating the scalability of our method.
与最先进方法的比较。PAL-UI 进一步优于现有的 SFT 与 RFT 基线。PAL-UI-7B 取得最佳整体得分 58.9%，比 GUI-R1-7B（56.1%）高 2.8%，并大幅超越最强的 SFT 方法 NaviMaster-7B。它在多项指标上建立了新的最先进结果，包括 GR（在 AndroidControl-High 上为 70.5%，在 GUI-Odyssey 上为 46.8%）和 SR（分别为 57.8% 和 41.7%）。值得注意的是，PAL-UI-7B 在仅依赖 SFT 而非强化学习的情况下仍优于 GUI-R1-7B，表明我们的方法以更低的训练复杂度获得了更强的结果。最后，从 PAL-UI-3B 扩展到 PAL-UI-7B 一致地提升了性能，验证了方法的可扩展性。


## 6 FURTHER ANALYSIS
## 6 进一步分析


Ablation Study. We investigate the contributions of different components in the PAL-UI framework, and the results are summarized in Table 2. Several key findings emerge: First, the dual-level summary agent (SA) consistently improves performance under both zero-shot and supervised fine-tuning settings. For example, on the out-of-domain GUI-Odyssey benchmark, equipping the base model with SA boosts SR from 34.4 to 38.9, confirming its effectiveness in mitigating the information loss inherent in action-only memory. Second, while SFT alone yields clear in-domain gains (SR on AC-High improves from 47.1 to 53.2), its effect on generalization is limited, with only a minor increase on GUI-Odyssey (34.4 to 36.4). This highlights the difficulty of transferring knowledge without a richer memory mechanism. Third, we observe that without memory planning with active look-back (PAL) hampers PAL-UI's effectiveness: as its performance drops on AC-High (SR from 53.2 to 52.3). This indicates that without rich historical summaries, the agent struggles to judge the relevance of retrieved information to make correct actions. Finally, the full PAL-UI framework, integrating SFT, SA, and PAL, achieves the strongest results across both benchmarks (SR 57.8 on AC-High and 41.7 on GUI-Odyssey). These findings show that the synergy of summarization and active retrieval is essential for robust long-horizon planning and cross-domain generalization.
消融研究。我们考察了 PAL-UI 框架中不同组件的贡献，结果汇总于表 2。若干关键发现如下：首先，双层摘要代理（SA）在零样本和监督微调设置下均持续提升性能。例如，在跨域的 GUI-Odyssey 基准上，为基础模型配备 SA 将 SR 从 34.4 提升到 38.9，证明其在缓解仅动作记忆带来的信息丢失方面的有效性。其次，单纯的 SFT 在域内带来明显提升（AC-High 上 SR 从 47.1 提升到 53.2），但其对泛化的作用有限，在 GUI-Odyssey 上仅有小幅增加（34.4 至 36.4），凸显在缺乏更丰富记忆机制时知识迁移的困难。第三，我们观察到，缺少带有主动回溯的记忆规划（PAL）会削弱 PAL-UI 的效果：其在 AC-High 上的表现下降（SR 从 53.2 降至 52.3）。这表明在没有丰富历史摘要的情况下，代理难以判断检索信息的相关性以做出正确动作。最后，完整的 PAL-UI 框架（整合 SFT、SA 与 PAL）在两个基准上取得最强结果（AC-High SR 为 57.8，GUI-Odyssey 为 41.7）。这些发现表明，摘要与主动检索的协同对稳健的长程规划与跨域泛化至关重要。


Table 2: Ablation results. "SA" indicates summary agents. "PAL" indicates planning with active look-back mechanism. "Full" indicates our full approach, where we train the model on our constructed dataset and leverage SA and PAL during inference.
表 2：消融结果。“SA”表示摘要代理。“PAL”表示带主动回溯的规划机制。“Full”表示我们的完整方法：在构建的数据集上训练模型，并在推理时利用 SA 与 PAL。


<table><tr><td rowspan="2">Setting</td><td colspan="2">AC-High</td><td colspan="2">GUI-Odessey</td></tr><tr><td>Type</td><td>SR</td><td>Type</td><td>SR</td></tr><tr><td>Zero-Shot</td><td>68.7</td><td>47.1</td><td>55.6</td><td>34.4</td></tr><tr><td>+ SA</td><td>70.9</td><td>54.6</td><td>62.8</td><td>38.9</td></tr><tr><td>+ SFT</td><td>73.4</td><td>53.2</td><td>56.3</td><td>36.4</td></tr><tr><td>+ SFT & SA</td><td>72.9</td><td>56.3</td><td>59.4</td><td>38.4</td></tr><tr><td>+ SFT & PAL</td><td>67.4</td><td>52.3</td><td>56.6</td><td>37.1</td></tr><tr><td>+ Full</td><td>71.3</td><td>57.8</td><td>65.1</td><td>41.7</td></tr></table>
<table><tbody><tr><td rowspan="2">设置</td><td colspan="2">交流-高</td><td colspan="2">图形界面-奥德赛</td></tr><tr><td>类型</td><td>SR</td><td>类型</td><td>SR</td></tr><tr><td>零样本</td><td>68.7</td><td>47.1</td><td>55.6</td><td>34.4</td></tr><tr><td>+ SA</td><td>70.9</td><td>54.6</td><td>62.8</td><td>38.9</td></tr><tr><td>+ SFT</td><td>73.4</td><td>53.2</td><td>56.3</td><td>36.4</td></tr><tr><td>+ SFT & SA</td><td>72.9</td><td>56.3</td><td>59.4</td><td>38.4</td></tr><tr><td>+ SFT & PAL</td><td>67.4</td><td>52.3</td><td>56.6</td><td>37.1</td></tr><tr><td>+ 完整</td><td>71.3</td><td>57.8</td><td>65.1</td><td>41.7</td></tr></tbody></table>


Table 3: Comparison of context length and performance for different paradigm. None: not using memory; $+ A$ : using only actions as memory; $+ {5O}$ and $+ {AO}$ : using recent 5 screenshot or all historical screenshots as memory; +SA: using summary agent for memory processing; + PAL: using the full approach.
表 3：不同范式下上下文长度与性能的比较。None：不使用记忆；$+ A$：仅将动作作为记忆；$+ {5O}$ 和 $+ {AO}$：将最近 5 张截图或所有历史截图作为记忆；+SA：使用摘要代理处理记忆；+ PAL：使用完整方法。


<table><tr><td rowspan="2">Setting</td><td rowspan="2">Len.</td><td colspan="2">AC-High</td><td colspan="2">GUI-Odessey</td></tr><tr><td>Type</td><td>SR</td><td>Type</td><td>SR</td></tr><tr><td>None</td><td>4307.6</td><td>65.4</td><td>45.8</td><td>52.6</td><td>34.6</td></tr><tr><td>+ A</td><td>4371.7</td><td>68.7</td><td>47.1</td><td>55.6</td><td>34.4</td></tr><tr><td>+ 5O</td><td>12630.0</td><td>69.1</td><td>48.3</td><td>56.8</td><td>36.4</td></tr><tr><td>+ AO</td><td>16383.8</td><td>67.8</td><td>45.5</td><td>54.6</td><td>34.9</td></tr><tr><td>+ SA</td><td>4965.4</td><td>70.9</td><td>54.6</td><td>62.8</td><td>38.9</td></tr><tr><td>+ PAL</td><td>7330.2</td><td>71.3</td><td>57.8</td><td>65.1</td><td>41.7</td></tr></table>
<table><tbody><tr><td rowspan="2">设置</td><td rowspan="2">长。</td><td colspan="2">交流-高</td><td colspan="2">GUI-奥德赛</td></tr><tr><td>类型</td><td>采样率</td><td>类型</td><td>采样率</td></tr><tr><td>无</td><td>4307.6</td><td>65.4</td><td>45.8</td><td>52.6</td><td>34.6</td></tr><tr><td>+ A</td><td>4371.7</td><td>68.7</td><td>47.1</td><td>55.6</td><td>34.4</td></tr><tr><td>+ 5O</td><td>12630.0</td><td>69.1</td><td>48.3</td><td>56.8</td><td>36.4</td></tr><tr><td>+ AO</td><td>16383.8</td><td>67.8</td><td>45.5</td><td>54.6</td><td>34.9</td></tr><tr><td>+ SA</td><td>4965.4</td><td>70.9</td><td>54.6</td><td>62.8</td><td>38.9</td></tr><tr><td>+ PAL</td><td>7330.2</td><td>71.3</td><td>57.8</td><td>65.1</td><td>41.7</td></tr></tbody></table>


Table 4: Cross-platform results of PAL-UI on Multimodal-Mind2web. We report the model performance on three benchmark splits: cross-task, cross-website, and cross-domain.
表4：PAL-UI 在 Multimodal-Mind2web 上的跨平台结果。我们报告模型在三种基准划分上的性能：跨任务、跨网站和跨领域。


<table><tr><td rowspan="2">Model</td><td colspan="3">Cross-Task</td><td colspan="3">Cross-Website</td><td colspan="3">Cross-Domain</td></tr><tr><td>Op.F1</td><td>Ele.Acc</td><td>SR</td><td>Op.F1</td><td>Ele.Acc</td><td>SR</td><td>Op.F1</td><td>Ele.Acc</td><td>SR</td></tr><tr><td>Qwen2.5-VL-3B</td><td>53.8</td><td>26.5</td><td>25.9</td><td>50.3</td><td>25.3</td><td>23.4</td><td>53.5</td><td>30.4</td><td>28.3</td></tr><tr><td>Qwen2.5-VL-7B</td><td>61.3</td><td>33.1</td><td>32.3</td><td>57.1</td><td>31.7</td><td>29.8</td><td>61.0</td><td>36.8</td><td>34.4</td></tr><tr><td>PAL-UI-3B</td><td>54.5</td><td>27.9</td><td>27.6</td><td>54.6</td><td>25.9</td><td>25.1</td><td>57.1</td><td>33.3</td><td>31.9</td></tr><tr><td>PAL-UI-7B</td><td>68.7</td><td>36.0</td><td>35.0</td><td>69.2</td><td>36.4</td><td>35.2</td><td>69.6</td><td>39.6</td><td>37.9</td></tr></table>
<table><tbody><tr><td rowspan="2">模型</td><td colspan="3">跨任务</td><td colspan="3">跨网站</td><td colspan="3">跨领域</td></tr><tr><td>操作.F1</td><td>元素.准确率</td><td>召回率</td><td>操作.F1</td><td>元素.准确率</td><td>召回率</td><td>操作.F1</td><td>元素.准确率</td><td>召回率</td></tr><tr><td>Qwen2.5-VL-3B</td><td>53.8</td><td>26.5</td><td>25.9</td><td>50.3</td><td>25.3</td><td>23.4</td><td>53.5</td><td>30.4</td><td>28.3</td></tr><tr><td>Qwen2.5-VL-7B</td><td>61.3</td><td>33.1</td><td>32.3</td><td>57.1</td><td>31.7</td><td>29.8</td><td>61.0</td><td>36.8</td><td>34.4</td></tr><tr><td>PAL-UI-3B</td><td>54.5</td><td>27.9</td><td>27.6</td><td>54.6</td><td>25.9</td><td>25.1</td><td>57.1</td><td>33.3</td><td>31.9</td></tr><tr><td>PAL-UI-7B</td><td>68.7</td><td>36.0</td><td>35.0</td><td>69.2</td><td>36.4</td><td>35.2</td><td>69.6</td><td>39.6</td><td>37.9</td></tr></tbody></table>


Cross-Platform Performance. Although PAL-UI is trained exclusively on mobile data and primarily evaluated on mobile benchmarks, we further examine its ability to generalize across platforms. Specifically, we test PAL-UI on Multimodal-Mind2Web (Deng et al. 2023), an web navigation benchmark, following the official evaluation protocol and reporting Operation F1 (Op.F1), Element Accuracy (Ele.Acc), and Step Success Rate (SR). Results are shown in Table 4 Despite being trained solely on mobile trajectories, PAL-UI achieves consistent improvements on web tasks, with average gains of +3.8 and +2.4 points for the 7B and 3B models, respectively. While these gains are smaller than those observed in mobile domain, they still demonstrate that PAL-UI effectively transfers its planning capability to a different platform. This ability to maintain performance across different environments highlights the robustness and generalization potential of our approach.
跨平台性能。尽管 PAL-UI 仅在移动端数据上训练且主要在移动端基准上评估，我们进一步考察其跨平台泛化能力。具体地，按照官方评估协议，我们在 Multimodal-Mind2Web (Deng et al. 2023) 网页导航基准上测试 PAL-UI，并报告操作 F1 (Op.F1)、元素准确率 (Ele.Acc) 和步骤成功率 (SR)。结果见表 4。尽管仅用移动轨迹训练，PAL-UI 在网页任务上仍取得稳定提升，7B 和 3B 模型的平均增益分别为 +3.8 和 +2.4 点。虽然这些增益小于在移动领域观察到的，但仍表明 PAL-UI 能有效将其规划能力迁移到不同平台。这种在不同环境中保持性能的能力凸显了我们方法的鲁棒性和泛化潜力。


Context Length Comparison. We compare the trade-off between context length and performance across three paradigms: PAL-based reasoning, action-only memory, and full screenshot memory. For evaluation, we select 50 trajectories from the AndroidControl test set and measure the input token length under each setting (Table 3). The results show that the screenshot-based method consumes the largest number of tokens yet provides little benefit, as redundant visual information increases both computational cost and prediction errors. By contrast, the action-only method requires the fewest tokens but yields clearly inferior performance due to the loss of critical historical details. Our PAL-UI strikes a balance between the two extremes: it adds only a modest number of tokens yet delivers significant improvements. Notably, PAL-UI achieves a 6% average performance gain while using just 44% of the tokens required by the screenshot-based approach, highlighting both the efficiency and effectiveness of our method.
上下文长度比较。我们比较三种范式之间在上下文长度与性能上的权衡：基于 PAL 的推理、仅动作记忆和完整截图记忆。为评估，我们从 AndroidControl 测试集选取 50 条轨迹，并在每种设置下测量输入 token 长度（表 3）。结果显示，基于截图的方法消耗的 token 最多但收益甚微，因为冗余的视觉信息既增加了计算成本又提升了预测错误。相较之下，仅动作方法所需 token 最少但因丢失关键历史细节而性能明显较差。我们的 PAL-UI 在两者之间取得平衡：仅增加少量 token 却带来显著改进。值得注意的是，PAL-UI 在仅使用截图法所需 token 的 44% 的情况下实现了平均 6% 的性能提升，突出了我们方法的高效性与有效性。


<img src="https://cdn.noedgeai.com/bo_d4nfubv7aajc73frsadg_8.jpg?x=904&y=232&w=520&h=394&r=0"/>



Table 5: Results of PAL-UI with different summarization models.
表 5：使用不同摘要模型的 PAL-UI 结果。


<table><tr><td rowspan="2">Summarization Model</td><td colspan="2">AC-High</td><td colspan="2">GUI-Odessey</td></tr><tr><td>Type</td><td>SR</td><td>Type</td><td>SR</td></tr><tr><td>Owen2.5-VL-3B</td><td>70.9</td><td>57.4</td><td>63.8</td><td>41.0</td></tr><tr><td>Qwen2.5-VL-7B</td><td>71.3</td><td>57.8</td><td>65.1</td><td>41.7</td></tr><tr><td>InternVL3-2B</td><td>69.4</td><td>56.2</td><td>64.2</td><td>39.8</td></tr><tr><td>InternVL3-8B</td><td>72.5</td><td>58.4</td><td>64.0</td><td>41.3</td></tr></table>
<table><tbody><tr><td rowspan="2">摘要模型</td><td colspan="2">交流高压</td><td colspan="2">GUI-奥德赛</td></tr><tr><td>类型</td><td>采样率</td><td>类型</td><td>采样率</td></tr><tr><td>Owen2.5-VL-3B</td><td>70.9</td><td>57.4</td><td>63.8</td><td>41.0</td></tr><tr><td>Qwen2.5-VL-7B</td><td>71.3</td><td>57.8</td><td>65.1</td><td>41.7</td></tr><tr><td>InternVL3-2B</td><td>69.4</td><td>56.2</td><td>64.2</td><td>39.8</td></tr><tr><td>InternVL3-8B</td><td>72.5</td><td>58.4</td><td>64.0</td><td>41.3</td></tr></tbody></table>


Figure 2: Behavior of active look-back.
图 2：主动回溯行为。


Effect of Summarization Models. We conduct extensive experiments to investigate the impact of different summarization models on agent performance. Specifically, we leverage Qwen2.5-VL (Bai et al. 2025) and InternVL-3 (Zhu et al. 2025) series models for memory summarization. The results are presented in Table 5 . As we observe, different summarization models do not exert a significant impact on overall performance. The reason might be that current MLLMs already possess strong OCR and visual comprehension capabilities, enabling them to accurately identify and extract key information in GUI screenshots and discerning action intentions and execution states across consecutive screenshots. As a result, we can employ a relatively compact model for memory compression to enhance agent performance while introducing minimal time overhead.
摘要模型的影响。我们进行了大量实验以研究不同摘要模型对代理性能的影响。具体地，我们采用 Qwen2.5-VL (Bai et al. 2025) 和 InternVL-3 (Zhu et al. 2025) 系列模型进行记忆摘要。结果见表 5。正如我们观察到的，不同的摘要模型对整体性能并无显著影响。原因可能在于当前的 MLLM 已具备强大的 OCR 与视觉理解能力，能够准确识别并提取 GUI 截图中的关键信息，判断连续截图中的操作意图与执行状态。因此，我们可以采用相对紧凑的模型进行记忆压缩，以提升代理性能且只带来极小的时间开销。


Analysis of Retrieval Behavior. We further analyze how the agent employs the retrieve tool during planning. Concretely, we sample 100 agent responses from the AndroidControl test set in which the retrieval tool is invoked, and record the distance between the retrieved step and the current step (Table 2). The results show that most retrievals occur within five steps of the current step, with the maximum distance extending to eight steps. This tendency likely reflects the relative simplicity of current navigation benchmarks: tasks usually involve short, localized interactions (e.g., simple GUI manipulations) that demand only limited long-range memory. We believe that in more complex or semantically demanding GUI environments, where long-distance dependencies play a larger role, the benefits of active look-back may become even more pronounced.
检索行为分析。我们进一步分析代理在规划过程中如何使用 retrieve 工具。具体地，我们从 AndroidControl 测试集中采样 100 条调用了检索工具的代理响应，并记录检索步骤与当前步骤之间的距离（表 2）。结果显示大多数检索发生在当前步骤前后五步内，最大距离延伸到八步。这一趋势可能反映了当前导航基准的相对简单性：任务通常涉及短小的局部交互（如简单的 GUI 操作），仅需有限的长程记忆。我们认为在更复杂或语义要求更高的 GUI 环境中，长距离依赖的作用更大，主动回溯的收益可能会更明显。


## 7 CONCLUSION
## 7 结论


We presented PAL-UI (Planning with Active Look-back), a framework that empowers GUI agents to selectively retrieve past observations during planning rather than carrying the full history at every step. By combining dual-level summaries with an active retrieval tool, PAL-UI effectively balances efficiency and completeness, enabling agents to handle long-horizon tasks with improved accuracy and reduced context overhead. To support this paradigm, we introduced a deliberated look-back framework for constructing tool-augmented trajectories, yielding 8.6K high-quality instruction-tuning samples. Extensive experiments demonstrate that PAL-UI significantly outperforms both base MLLMs and state-of-the-art baselines on mobile navigation benchmarks, while also generalizing well to out-of-domain web environments. These results underscore the importance of active memory retrieval for robust GUI planning. Future work will explore extending PAL-UI to more complex tasks and environments, integrating reinforcement learning objectives, and broadening its applicability to real-world interactive systems.
我们提出了 PAL-UI（带主动回溯的规划），一个使 GUI 代理在规划时选择性检索过去观测而非在每一步携带完整历史的框架。通过结合双层摘要与主动检索工具，PAL-UI 有效在效率与完整性之间取得平衡，使代理能在减少上下文负担的同时更准确地处理长时程任务。为支持该范式，我们引入了一个审慎回溯框架以构建工具增强轨迹，产出 8.6K 高质量指令调优样本。大量实验表明，PAL-UI 在移动导航基准上显著优于基础 MLLM 与最先进基线，并能较好泛化到域外网页环境。这些结果强调了主动记忆检索在稳健 GUI 规划中的重要性。未来工作将探索将 PAL-UI 扩展到更复杂的任务与环境，融合强化学习目标，并拓展其在真实交互系统中的适用性。


## 8 ETHICS STATEMENT
## 8 伦理声明


In the course of this research, all procedures were conducted in strict compliance with established academic norms and ethical principles. The experimental data utilized fully adhere to ethical requirements, containing no personal private information, no material inconsistent with human values, and no biased or offensive content. The objective of this work is to enhance the capabilities of autonomous agents, with the ultimate aim of advancing AI technologies that can effectively benefit all of humanity and contribute positively to society and human welfare. In the writing process, LLMs were employed solely for the purpose of checking and correcting grammatical errors in the manuscript. All AI-generated content has been carefully reviewed by the authors to ensure the accuracy and rigor of the paper.
在本研究过程中，所有程序均严格遵循既定的学术规范与伦理原则。所用实验数据完全符合伦理要求，不含个人隐私信息、不含与人类价值观不符的材料，也不包含偏见或冒犯性内容。本工作的目标是增强自主代理的能力，旨在推进能够有效造福全人类并为社会与人类福祉带来积极贡献的 AI 技术。在写作过程中，LLM 仅用于检查和纠正稿件中的语法错误。所有 AI 生成内容均由作者仔细审阅，以确保论文的准确性与严谨性。


## 9 REPRODUCIBILITY STATEMENT
## 9 可复现性声明


To ensure the reproducibility of our work, we provide a detailed description of our approach in Section 4. Moreover, we present the details about the implementation of our experiments in Section 5 including the detailed training and evaluation setting. Furthermore, we provide the code for evaluation in our supplementary material.
为确保工作可复现性，我们在第 4 节提供了方法的详细描述。此外，我们在第 5 节给出了实验实现的细节，包括详细的训练与评估设置。此外，我们在补充材料中提供了用于评估的代码。


## REFERENCES
## 参考文献


Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl 技术报告。arXiv preprint arXiv:2502.13923, 2025。


Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners。Advances in neural information processing systems, 33:1877-1901, 2020。


Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024.
Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents。arXiv preprint arXiv:2406.11317, 2024。


Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiy-ong Wu. Secelick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024.
Kanzhi Cheng、Qiushi Sun、Yougang Chu、Fangzhi Xu、Yantao Li、Jianbing Zhang 和 Zhiy-ong Wu。Secelick：利用 GUI 定位构建先进视觉 GUI 代理。arXiv 预印本 arXiv:2401.10935，2024。


Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 36:28091-28114, 2023.
Xiang Deng、Yu Gu、Boyuan Zheng、Shijie Chen、Sam Stevens、Boshi Wang、Huan Sun 和 Yu Su。Mind2web：迈向通用的网页代理。Advances in Neural Information Processing Systems, 36:28091-28114，2023。


Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024.
Boyu Gou、Ruohan Wang、Boyuan Zheng、Yanan Xie、Cheng Chang、Yiheng Shu、Huan Sun 和 Yu Su。像人类一样导航数字世界：用于 GUI 代理的通用视觉定位。arXiv 预印本 arXiv:2410.05243，2024。


Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-rl: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.
Daya Guo、Dejian Yang、Haowei Zhang、Junxiao Song、Ruoyu Zhang、Runxin Xu、Qihao Zhu、Shirong Ma、Peiyi Wang、Xiao Bi 等。Deepseek-rl：通过强化学习激励大型语言模型的推理能力。arXiv 预印本 arXiv:2501.12948，2025。


Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14281-14290, 2024.
Wenyi Hong、Weihan Wang、Qingsong Lv、Jiazheng Xu、Wenmeng Yu、Junhui Ji、Yan Wang、Zihan Wang、Yuxiao Dong、Ming Ding 等。Cogagent：面向 GUI 代理的视觉语言模型。收入 IEEE/CVF 计算机视觉与模式识别会议论文集，页码 14281-14290，2024。


Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024.
Aaron Jaech、Adam Kalai、Adam Lerer、Adam Richardson、Ahmed El-Kishky、Aiden Low、Alec Helyar、Aleksander Madry、Alex Beutel、Alex Carney 等。Openai o1 系统说明。arXiv 预印本 arXiv:2412.16720，2024。


Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents. arXiv e-prints, pp. arXiv-2406, 2024.
Wei Li、William Bishop、Alice Li、Chris Rawles、Folawiyo Campbell-Ajala、Divya Tyamagundlu 和 Oriana Riva。数据规模对计算机控制代理影响的研究。arXiv e-prints，页码 arXiv-2406，2024。


Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:34892-34916, 2023.
Haotian Liu、Chunyuan Li、Qingyang Wu 和 Yong Jae Lee。视觉指令微调。Advances in neural information processing systems, 36:34892-34916，2023。


Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. In European conference on computer vision, pp. 126-142. Springer, 2024.
Shilong Liu、Hao Cheng、Haotian Liu、Hao Zhang、Feng Li、Tianhe Ren、Xueyan Zou、Jianwei Yang、Hang Su、Jun Zhu 等。Llava-plus：学习使用工具以创建多模态代理。收入欧洲计算机视觉会议，页码 126-142。Springer，2024。


Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: A comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024.
Quanfeng Lu、Wenqi Shao、Zitao Liu、Fanqing Meng、Boxuan Li、Botong Chen、Siyuan Huang、Kaipeng Zhang、Yu Qiao 和 Ping Luo。Gui odyssey：用于移动设备跨应用 GUI 导航的综合数据集。arXiv 预印本 arXiv:2406.08451，2024。


Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025.
Zhengxi Lu、Yuxiang Chai、Yaxuan Guo、Xi Yin、Liang Liu、Hao Wang、Han Xiao、Shuai Ren、Guanjing Xiong 和 Hongsheng Li。Ui-r1：通过强化学习增强 GUI 代理的高效动作预测。arXiv 预印本 arXiv:2503.21620，2025。


Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: A generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025.
Run Luo、Lu Wang、Wanwei He 和 Xiaobo Xia。Gui-r1：面向 GUI 代理的通用 R1 风格视觉-语言动作模型。arXiv 预印本 arXiv:2504.10458，2025。


Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
Reiichiro Nakano、Jacob Hilton、Suchir Balaji、Jeff Wu、Long Ouyang、Christina Kim、Christopher Hesse、Shantanu Jain、Vineet Kosaraju、William Saunders 等。Webgpt：结合人类反馈的浏览器辅助问答。arXiv 预印本 arXiv:2112.09332，2021。


Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025.
Yujia Qin、Yining Ye、Junjie Fang、Haoming Wang、Shihao Liang、Shizuo Tian、Junda Zhang、Jiahao Li、Yunxin Li、Shijue Huang 等。Ui-tars：开创性地以原生代理实现自动化 GUI 交互。arXiv 预印本 arXiv:2501.12326，2025。


Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025.
赵辰�?（Zhaochen Su）、李林杰（Linjie Li）、宋明阳（Mingyang Song）、郝云卓（Yunzhuo Hao）、杨正远（Zhengyuan Yang）、张军（Jun Zhang）、陈冠杰（Guanjie Chen）、顾嘉伟（Jiawei Gu）、李军涛（Juntao Li）、曲晓烨（Xiaoye Qu）等. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025.


Fei Tang, Yongliang Shen, Hang Zhang, Siqi Chen, Guiyang Hou, Wenqi Zhang, Wenqiao Zhang, Kaitao Song, Weiming Lu, and Yueting Zhuang. Think twice, click once: Enhancing gui grounding via fast and slow systems. arXiv preprint arXiv:2503.06470, 2025.
唐飞（Fei Tang）、申永亮（Yongliang Shen）、张航（Hang Zhang）、陈思琪（Siqi Chen）、侯贵阳（Guiyang Hou）、张文琦（Wenqi Zhang）、张文樵（Wenqiao Zhang）、宋凯涛（Kaitao Song）、卢伟明（Weiming Lu）、庄跃庭（Yueting Zhuang）. Think twice, click once: Enhancing gui grounding via fast and slow systems. arXiv preprint arXiv:2503.06470, 2025.


Shuai Wang, Weiwen Liu, Jingxuan Chen, Yuqi Zhou, Weinan Gan, Xingshan Zeng, Yuhan Che, Shuai Yu, Xinlong Hao, Kun Shao, et al. Gui agents with foundation models: A comprehensive survey. arXiv preprint arXiv:2411.04890, 2024.
王帅（Shuai Wang）、刘维文（Weiwen Liu）、陈景轩（Jingxuan Chen）、周雨琦（Yuqi Zhou）、甘伟南（Weinan Gan）、曾行珊（Xingshan Zeng）、车宇涵（Yuhan Che）、于帅（Shuai Yu）、郝新龙（Xinlong Hao）、邵坤（Kun Shao）等. Gui agents with foundation models: A comprehensive survey. arXiv preprint arXiv:2411.04890, 2024.


Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.
吴晨飞（Chenfei Wu）、尹盛明（Shengming Yin）、祁维臻（Weizhen Qi）、王晓东（Xiaodong Wang）、唐泽诚（Zecheng Tang）、段楠（Nan Duan）. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.


Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024.
许怡恒（Yiheng Xu）、王泽坤（Zekun Wang）、王俊立（Junli Wang）、卢敦杰（Dunjie Lu）、谢天宝（Tianbao Xie）、Amrita Saha、Doyen Sahoo、余涛（Tao Yu）、熊才明（Caiming Xiong）. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024.


An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.
杨安（An Yang）、李安丰（Anfeng Li）、杨宝松（Baosong Yang）、张北辰（Beichen Zhang）、惠斌元（Binyuan Hui）、郑波（Bo Zheng）、于博文（Bowen Yu）、高畅（Chang Gao）、黄承恩（Chengen Huang）、吕晨旭（Chenxu Lv）等. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.


Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023a.
杨建伟（Jianwei Yang）、张浩（Hao Zhang）、李锋（Feng Li）、邹雪艳（Xueyan Zou）、李春元（Chunyuan Li）、高建峰（Jianfeng Gao）. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023a.


Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction. Advances in Neural Information Processing Systems, 36:71995-72007, 2023b.
杨锐（Rui Yang）、宋林（Lin Song）、李彦伟（Yanwei Li）、赵思杰（Sijie Zhao）、葛一肖（Yixiao Ge）、李秀（Xiu Li）、单英山（Ying Shan）. Gpt4tools: Teaching large language model to use tools via self-instruction. Advances in Neural Information Processing Systems, 36:71995-72007, 2023b.


Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023c.
杨正远（Zhengyuan Yang）、李林杰（Linjie Li）、王建峰（Jianfeng Wang）、林凯文（Kevin Lin）、Ehsan Azarnasab、Faisal Ahmed、刘子成（Zicheng Liu）、刘策（Ce Liu）、曾迈克（Michael Zeng）、王丽娟（Lijuan Wang）. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023c.


Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.
姚舜宇（Shunyu Yao）、赵杰弗里（Jeffrey Zhao）、俞典（Dian Yu）、段楠（Nan Du）、Izhak Shafran、Karthik Narasimhan、曹元（Yuan Cao）. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.


Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436, 2023.
张卓胜（Zhuosheng Zhang）与Aston Zhang. You only look at screens: Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436, 2023.


Bingchen Zhao, Yongshuo Zong, Letian Zhang, and Timothy Hospedales. Benchmarking multi-image understanding in vision and language models: Perception, knowledge, reasoning, and multi-hop reasoning. arXiv preprint arXiv:2406.12742, 2024.
赵炳晨（Bingchen Zhao）、宗永硕（Yongshuo Zong）、张乐天（Letian Zhang）、Timothy Hospedales. Benchmarking multi-image understanding in vision and language models: Perception, knowledge, reasoning, and multi-hop reasoning. arXiv preprint arXiv:2406.12742, 2024.


Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023.
赵鑫（Wayne Xin Zhao）、周坤（Kun Zhou）、李俊毅（Junyi Li）、唐天毅（Tianyi Tang）、王晓磊（Xiaolei Wang）、侯玉鹏（Yupeng Hou）、闵盈谦（Yingqian Min）、张北辰（Beichen Zhang）、张俊杰（Junjie Zhang）、董子灿（Zican Dong）等. A survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023.


Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is a generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024a.
郑博源（Boyuan Zheng）、苟博宇（Boyu Gou）、Kil Jihyung（Jihyung Kil）、孙欢（Huan Sun）、苏宇（Yu Su）. Gpt-4v (ision) is a generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024a.


Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024b.
郑耀维, 张日充, 张君豪, 叶彦翰, 罗哲妍, 冯章驰, 和 马永强. Llamafactory：统一高效地对 100+ 语言模型进行微调. arXiv 预印本 arXiv:2403.13372, 2024b.


Yiyang Zhou, Yangfan He, Yaofeng Su, Siwei Han, Joel Jang, Gedas Bertasius, Mohit Bansal, and Huaxiu Yao. Reagent-v: A reward-driven multi-agent framework for video understanding. arXiv preprint arXiv:2506.01300, 2025.
周亦阳, 何杨帆, 苏耀锋, 韩思为, Joel Jang, Gedas Bertasius, Mohit Bansal, 和 姚华修. Reagent-v：一个以奖励为驱动的视频理解多智能体框架. arXiv 预印本 arXiv:2506.01300, 2025.


Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025.
朱金国, 王维云, 陈哲, 刘朝阳, 叶升龙, 顾立新, 田浩, 段誉晨, 苏伟杰, 邵杰, 等. Internvl3：探索开源多模态模型的高级训练与测试时方案. arXiv 预印本 arXiv:2504.10479, 2025.