# SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving
# SEPT：用于自动驾驶的标准定义地图增强场景感知与拓扑推理


Muleilan Pei, Jiayao Shan, Peiliang Li, Jieqi Shi, Jing Huo, Yang Gao, and Shaojie Shen
裴穆磊岚，单佳尧，李沛良，石杰琦，霍晶，高扬，沈少杰


Abstract-Online scene perception and topology reasoning are critical for autonomous vehicles to understand their driving environments, particularly for mapless driving systems that endeavor to reduce reliance on costly High-Definition (HD) maps. However, recent advances in online scene understanding still face limitations, especially in long-range or occluded scenarios, due to the inherent constraints of onboard sensors. To address this challenge, we propose a Standard-Definition (SD) Map Enhanced scene Perception and Topology reasoning (SEPT) framework, which explores how to effectively incorporate the SD map as prior knowledge into existing perception and reasoning pipelines. Specifically, we introduce a novel hybrid feature fusion strategy that combines SD maps with Bird's-Eye-View (BEV) features, considering both rasterized and vectorized representations, while mitigating potential misalignment between SD maps and BEV feature spaces. Additionally, we leverage the SD map characteristics to design an auxiliary intersection-aware keypoint detection task, which further enhances the overall scene understanding performance. Experimental results on the large-scale OpenLane-V2 dataset demonstrate that by effectively integrating SD map priors, our framework significantly improves both scene perception and topology reasoning, outperforming existing methods by a substantial margin.
摘要-在线场景感知与拓扑推理对于自动驾驶车辆理解其驾驶环境至关重要，尤其对于旨在减少对昂贵高精度（HD）地图依赖的无地图驾驶系统。然而，近期在线场景理解的进展仍面临限制，特别是在远距离或遮挡场景中，因车载传感器的固有限制。为解决此挑战，我们提出了标准定义（SD）地图增强的场景感知与拓扑推理（SEPT）框架，探讨如何将SD地图作为先验知识有效融入现有感知与推理流程。具体而言，我们引入了一种新颖的混合特征融合策略，结合SD地图与鸟瞰视图（BEV）特征，兼顾栅格化与矢量化表示，同时缓解SD地图与BEV特征空间之间的潜在错位。此外，我们利用SD地图特性设计了辅助的路口感知关键点检测任务，进一步提升整体场景理解性能。在大规模OpenLane-V2数据集上的实验结果表明，通过有效整合SD地图先验，我们的框架显著提升了场景感知与拓扑推理能力，较现有方法有大幅度优势。


Index Terms-Computer vision for transportation, deep learning for visual perception, intelligent transportation systems.
关键词-交通计算机视觉，视觉感知深度学习，智能交通系统。


## I. INTRODUCTION
## 一、引言


SCENE understanding is essential for autonomous vehicles, facilitating critical downstream tasks such as accurate motion prediction and decision-making. High-Definition (HD) maps play a pivotal role in this process, providing rich geometric and semantic information, as well as topology relationships. However, HD maps present significant challenges, including high annotation costs, scalability limitations, and ongoing maintenance demands [1], which underscore the increasing need for online scene perception and topology reasoning [2].
场景理解对于自动驾驶车辆至关重要，支持准确的运动预测与决策等关键下游任务。高精度（HD）地图在此过程中发挥核心作用，提供丰富的几何与语义信息及拓扑关系。然而，HD地图存在高昂标注成本、扩展性受限及持续维护需求等重大挑战[1]，凸显了在线场景感知与拓扑推理日益增长的需求[2]。


In recent years, vision-centric mapless driving approaches (i.e., driving without HD maps) have made significant strides [3], [4], especially within advanced driver assistance systems. These methods aim to reduce the heavy reliance on HD maps by leveraging onboard sensors to perceive the complex scene structure of driving environments in real time. Specifically, with multi-view images as input, a variety of tasks need to be addressed, including lane segment detection, traffic element recognition, and scene topology reasoning [5], [6].
近年来，以视觉为核心的无地图驾驶方法（即不依赖HD地图驾驶）取得显著进展[3]，[4]，尤其在高级驾驶辅助系统中。这些方法旨在通过车载传感器实时感知复杂驾驶环境的场景结构，减少对HD地图的高度依赖。具体而言，以多视角图像为输入，需要解决多项任务，包括车道段检测、交通元素识别及场景拓扑推理[5]，[6]。


<!-- Media -->



<!-- figureText: Multi-View Images Ours GT w/ SD Map Baseline -->



<img src="https://cdn.noedgeai.com/bo_d3vjg1jef24c73d2mnk0_0.jpg?x=912&y=474&w=741&h=888&r=0"/>



Fig. 1. Illustration of the SD map prior for enhancing online scene understanding in long-distance and occlusion scenarios. In this example, the front view is severely obstructed by a bus at the intersection (highlighted in the red box in the front-view image), and the left-back zone is distant. The baseline (LaneSegNet [6]) fails to correctly perceive the road structure (indicated by the red boxes in the top and bottom of the middle visualization), whereas our SEPT framework accurately predicts the road layout with the augmentation of the SD map prior. The Ground Truth (GT) of lane segments is shown in the left figure, with the green line representing the SD map.
图1. 标准定义地图先验用于增强远距离及遮挡场景在线理解的示意图。示例中，路口前视被一辆公交车严重遮挡（前视图红框标注），左后方区域较远。基线方法（LaneSegNet [6]）未能正确感知道路结构（中间可视化上下红框所示），而我们的SEPT框架在SD地图先验增强下准确预测了道路布局。车道段真实标注（GT）见左图，绿色线条表示SD地图。


<!-- Media -->



Nevertheless, due to the inherent limitations of onboard sensors, such as constrained perception range and restricted field of view, fully mapless driving systems often struggle to accurately reconstruct far-seeing or occluded road conditions. Given that human drivers typically perceive the surrounding scenarios by combining observations with navigation maps, also known as Standard-Definition (SD) maps [7], integrating SD maps as additional prior knowledge of road structures offers a promising solution to complement onboard sensory inputs. In general, the SD map provides a centerline skeleton of road networks without detailed and high-precision annotations [8], making it lightweight, scalable, easily accessible, and low-cost [9]. This basic geographic and road-level topological information can effectively augment online sensing capabilities, thereby enhancing scene perception and topology reasoning, particularly in long-distance or occlusion scenarios, as demonstrated in Fig. 1.
然而，由于车载传感器固有限制，如感知范围受限和视野狭窄，纯无地图驾驶系统常难以准确重建远距离或遮挡的道路状况。鉴于人类驾驶员通常结合导航地图（即标准定义（SD）地图[7]）与观察感知周围场景，将SD地图作为道路结构的额外先验知识整合，成为补充车载传感输入的有力方案。一般而言，SD地图提供道路网络的中心线骨架，缺乏详细高精度标注[8]，因而轻量、可扩展、易获取且成本低廉[9]。这种基础的地理及道路级拓扑信息能有效增强在线感知能力，提升场景感知与拓扑推理，尤其在远距离或遮挡场景中，如图1所示。


---

<!-- Footnote -->



This work was supported in part by the Hong Kong PhD Fellowship Scheme, and in part by the HKUST-DJI Joint Innovation Laboratory. (Corresponding author: Jieqi Shi.)
本工作部分由香港博士研究生奖学金计划资助，部分由香港科技大学-大疆联合创新实验室支持。（通讯作者：石杰琦）


Muleilan Pei and Shaojie Shen are with the Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China (email: mpei@ust.hk; eeshaojie@ust.hk).
裴穆磊岚与沈少杰隶属于中国香港科技大学电子与计算机工程系（邮箱：mpei@ust.hk；eeshaojie@ust.hk）。


Jiayao Shan and Peiliang Li are with Zhuoyu Technology, Shenzhen, China (email: jiayao.shan@zyt.com; peiliang.li@zyt.com)
单佳尧与李沛良隶属于中国深圳卓宇科技（邮箱：jiayao.shan@zyt.com；peiliang.li@zyt.com）。


Jieqi Shi, Jing Huo, and Yang Gao are with State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China (email: isjieqi@nju.edu.cn; huojing@nju.edu.cn; gaoy@nju.edu.cn).
石杰琦、霍晶与高扬隶属于中国南京大学新型软件技术国家重点实验室（邮箱：isjieqi@nju.edu.cn；huojing@nju.edu.cn；gaoy@nju.edu.cn）。


<!-- Footnote -->

---



Despite the substantial potential benefits of SD maps, the effective integration of such map priors into current online perception and reasoning paradigms remains an ongoing challenge. Existing approaches typically rely on relatively simple encoding strategies to represent SD maps, either in a rasterized [8] or vectorized [10] format. Each representation has distinct advantages: dense rasterization preserves spatial positional information and fine-grained local details, while sparse vectorization captures complex geometry and topology more efficiently. However, most methods either focus on one representation or combine the two in a simplistic manner [11], which limits effective feature extraction and results in suboptimal utilization or information loss from the SD map. To address this gap, we encode the SD map using a hybrid representation and propose a lightweight yet effective fusion module to augment the Bird's-Eye-View (BEV) features with SD map priors. Additionally, inherent inaccuracies in GPS signals often cause weak spatial misalignment between the SD map and BEV space [12]. While previous works tend to neglect this artifact or dismiss it as noise, we introduce a feature alignment mechanism to resolve this issue. Specifically, for rasterization, we design a feature transformation network that dynamically modulates the features through predicting a scaling factor and bias term for each feature channel; for vectorization, we adopt a cross-attention mechanism [13] that adaptively attends to corresponding features, ensuring better alignment with the BEV feature space.
尽管SD地图具有显著的潜在优势，但将此类地图先验有效整合到当前的在线感知与推理范式中仍然是一个持续的挑战。现有方法通常依赖相对简单的编码策略来表示SD地图，采用栅格化[8]或矢量化[10]格式。每种表示各有优势：密集栅格化保留空间位置信息和细粒度局部细节，而稀疏矢量化则更高效地捕捉复杂的几何形状和拓扑结构。然而，大多数方法要么专注于单一表示，要么以简单方式结合两者[11]，这限制了有效特征提取，导致SD地图信息利用不足或信息丢失。为弥补这一不足，我们采用混合表示对SD地图进行编码，并提出一个轻量且高效的融合模块，以SD地图先验增强鸟瞰图（BEV）特征。此外，GPS信号固有的不准确性常导致SD地图与BEV空间之间存在轻微的空间错位[12]。以往工作往往忽视这一现象或将其视为噪声，而我们引入了特征对齐机制来解决该问题。具体而言，对于栅格化，我们设计了一个特征变换网络，通过预测每个特征通道的缩放因子和偏置项动态调节特征；对于矢量化，我们采用了交叉注意力机制[13]，自适应地关注对应特征，确保与BEV特征空间更好地对齐。


Moreover, existing approaches overlook the importance of topological road structures in driving scenes. For example, intersections, including cross, merge, or diverge nodes, serve as critical topological attributes that signify changes in road networks. Such keypoints can be effectively identified from SD map priors, which provide valuable characteristics about road structures. To leverage this information, we introduce an auxiliary task focused on recognizing the distribution of road intersections derived from SD maps. This task enables BEV features to capture crucial road topology, thereby enhancing overall driving scene understanding.
此外，现有方法忽视了驾驶场景中道路拓扑结构的重要性。例如，交叉口，包括交叉、汇合或分岔节点，是标志道路网络变化的关键拓扑属性。这些关键点可通过SD地图先验有效识别，提供有关道路结构的宝贵特征。为利用这些信息，我们引入了一个辅助任务，专注于识别源自SD地图的道路交叉口分布。该任务使BEV特征能够捕捉关键的道路拓扑，从而提升整体驾驶场景理解能力。


In summary, the primary contributions of this letter are as follows: (1) We propose a novel hybrid fusion strategy for SD maps that combines both rasterized and vectorized representations, ensuring effective alignment with BEV features for improved synergy. (2) We introduce an auxiliary Intersection-aware KeyPoint Detection (IKPD) task conditioned on the SD map prior, further enhancing scene understanding capabilities. (3) Extensive experiments on the large-scale OpenLane-V2 dataset demonstrate that our SD map-enhanced framework, termed SEPT, significantly improves both scene perception and topology reasoning performance.
综上所述，本信的主要贡献包括：（1）提出了一种结合栅格化与矢量化表示的SD地图新型混合融合策略，确保与BEV特征的有效对齐以提升协同效果；（2）引入了基于SD地图先验的辅助交叉口感知关键点检测（IKPD）任务，进一步增强场景理解能力；（3）在大规模OpenLane-V2数据集上的大量实验表明，我们的SD地图增强框架SEPT显著提升了场景感知和拓扑推理性能。


## II. RELATED WORK
## II. 相关工作


## A. Online Scene Perception
## A. 在线场景感知


Online HD map construction relies on the accurate perception of scene elements. Pioneering efforts have focused on laneline detection [14], [15] to capture road geometry, or centerline perception [2], [16] to recognize lane connectivity. Given the intertwined nature of these two representations, a comprehensive mapping format, lane segment [6], has been proposed to seamlessly integrate both geometric 3D lanelines and topological 3D lane centerlines, along with areas defined by road boundaries and pedestrian crossings. Additionally, traffic element recognition has also been extensively explored in the literature [17], [18] for driving scene understanding, including the detection of traffic lights, road signs, and their associated semantic attributes. Despite advances in detecting these map elements, current online scene perception systems still struggle with occlusions and long-range scenarios. To address these limitations, our work leverages SD map priors, serving as essential complementary prompts with the potential to improve performance in these challenging conditions.
在线高清地图构建依赖于对场景元素的准确感知。早期工作主要聚焦于车道线检测[14]，[15]以捕捉道路几何，或中心线感知[2]，[16]以识别车道连通性。鉴于这两种表示的紧密关联，提出了综合映射格式——车道段[6]，无缝整合几何三维车道线和拓扑三维车道中心线，以及由道路边界和人行横道定义的区域。此外，文献中也广泛探讨了交通元素识别[17]，[18]，包括交通信号灯、道路标志及其相关语义属性的检测，以促进驾驶场景理解。尽管在检测这些地图元素方面取得进展，当前在线场景感知系统仍面临遮挡和远距离场景的挑战。为克服这些限制，我们的工作利用SD地图先验，作为重要的补充提示，有望提升在这些复杂条件下的性能。


## B. Scene Topology Reasoning
## B. 场景拓扑推理


Scene topology information is significant for downstream trajectory prediction [19] and behavior planning [20] tasks, as it provides the topological relationships among lanes and between lanes and traffic elements. Nevertheless, research on topology reasoning has been limited until the emergence of the OpenLane-V2 benchmark [5], which utilizes adjacency matrices to characterize topological connectivity. Most existing methods rely on Multi-Layer Perceptrons (MLPs) [21] or Graph Neural Networks (GNNs) [2] to learn these connection relationships, or incorporate spatial position encoding [22] to enhance reasoning capabilities. These methods, however, are prone to disruption by endpoint shift issues. To address this, the calculation of geometric distance and semantic similarity [23] has been proposed to mitigate such effects. Moreover, since SD maps inherently contain the topological structure of driving scenes, recent works [24] have explored leveraging this prior knowledge to further improve topology reasoning.
场景拓扑信息对于下游轨迹预测[19]和行为规划[20]任务至关重要，因为它提供了车道之间及车道与交通元素之间的拓扑关系。然而，拓扑推理的研究较为有限，直到OpenLane-V2基准[5]的出现，该基准利用邻接矩阵表征拓扑连通性。大多数现有方法依赖多层感知机（MLP）[21]或图神经网络（GNN）[2]学习这些连接关系，或结合空间位置编码[22]以增强推理能力。但这些方法易受端点偏移问题干扰。为此，提出了几何距离和语义相似度计算[23]以缓解该影响。此外，由于SD地图本质上包含驾驶场景的拓扑结构，近期工作[24]探索利用该先验知识进一步提升拓扑推理。


### C.SD Map Prior for Autonomous Driving
### C. 自动驾驶的SD地图先验


SD maps, such as Google Maps, are widely used for urban navigation and have recently garnered increasing attention in autonomous driving tasks. Previous studies have primarily concentrated on leveraging SD map priors to enhance online map construction, particularly in long-range scenarios [25]. These methods typically involve rasterizing SD maps [7] and employing Convolutional Neural Networks (CNNs) to extract features. However, the intrinsic weak alignment between SD maps and BEV features remains a challenge [1], leading to the adoption of attention mechanisms [8]. Recent advances in topology reasoning have also incorporated SD maps by vectorizing them into polylines and using Transformer [10] or GNN [9] architectures to improve online lane topology understanding. To fully exploit both representations, a concurrent approach [11] combines these two distinct streams; however, its fusion strategy remains overly simplistic, limiting effectiveness. Considering the existing constraints in SD map utilization, our work further explores their potential by developing a powerful hybrid fusion module and introducing an auxiliary intersection-aware keypoint forecasting task.
SD地图，如谷歌地图，广泛应用于城市导航，近年来在自动驾驶任务中受到越来越多的关注。以往研究主要集中于利用SD地图先验来提升在线地图构建，尤其是在远距离场景中[25]。这些方法通常涉及将SD地图栅格化[7]，并采用卷积神经网络（CNN）提取特征。然而，SD地图与鸟瞰视图（BEV）特征之间的内在弱对齐仍然是一个挑战[1]，因此引入了注意力机制[8]。最近在拓扑推理方面的进展也通过将SD地图矢量化为折线，并使用Transformer[10]或图神经网络（GNN）[9]架构来提升在线车道拓扑理解。为了充分利用这两种表示形式，一种并行方法[11]结合了这两条不同的路径；但其融合策略过于简单，限制了效果。鉴于现有SD地图利用的局限性，我们的工作通过开发强大的混合融合模块并引入辅助的路口感知关键点预测任务，进一步挖掘其潜力。


<!-- Media -->



<!-- figureText: PV2BEV BEV Constructor Decoder LaneSeg Area Cross HHDG Topology Keypoint Attention ${\mathcal{F}}_{B}^{ \vee  }$ Feature Transform ${\mathcal{F}}_{B}^{\mathrm{R}}$ Perception Result Encoder ${\mathcal{F}}_{B}$ Vector Multi-View Images Encoder ${\mathcal{F}}_{SD}^{\mathrm{V}}$ Raster Encoder SD Map -->



<img src="https://cdn.noedgeai.com/bo_d3vjg1jef24c73d2mnk0_2.jpg?x=148&y=156&w=1507&h=406&r=0"/>



Fig. 2. Overview of the SEPT architecture, demonstrating how it enhances the existing perception and reasoning model for online scene understanding through the integration of the SD map prior.
图2. SEPT架构概览，展示了如何通过整合SD地图先验，增强现有的感知与推理模型，实现在线场景理解。


<!-- Media -->



## III. METHODOLOGY
## 三、方法论


## A. Task Statement
## A. 任务描述


The online driving scene understanding task involves both scene perception and topology reasoning, using multi-view images and the corresponding SD map priors as inputs. Scene perception includes detecting lane segments, drivable areas, and traffic elements. To be specific, lane segments comprise directed lane centerlines, left and right lane boundaries, and their associated line types (e.g., non-visible, solid, dashed). Drivable areas are represented by undirected curves or closed polygons corresponding to road boundaries and pedestrian crossings. Traffic elements encompass traffic lights and road signs visible in the front view, together with their relevant attributes. For topology reasoning, the goal is to infer the topological relationships among lane segments and between lane segments and traffic elements. This topological information is typically modeled as a lane graph, where nodes represent lane segments or traffic elements, and edges signify connectivity relationships. An adjacency matrix is employed to characterize the lane graph.
在线驾驶场景理解任务包括场景感知和拓扑推理，输入为多视角图像及对应的SD地图先验。场景感知涵盖车道段、可行驶区域及交通元素的检测。具体而言，车道段包括有向的车道中心线、左右车道边界及其对应的线型（如不可见、实线、虚线）。可行驶区域由无向曲线或闭合多边形表示，对应道路边界和人行横道。交通元素包括前视图中可见的交通信号灯和道路标志及其相关属性。拓扑推理的目标是推断车道段之间以及车道段与交通元素之间的拓扑关系。该拓扑信息通常建模为车道图，节点代表车道段或交通元素，边表示连接关系。邻接矩阵用于描述车道图。


## B. Framework Overview
## B. 框架概述


The overall pipeline of our SEPT framework is illustrated in Fig. 2, which improves the baseline model by incorporating SD map priors. Specifically, given multi-view images, the PV2BEV encoder first extracts visual information via the image backbone and then transforms the Perspective-View (PV) features into the BEV feature,denoted as ${\mathcal{F}}_{B}$ ,by view transformation. Additionally, the SD map prior is encoded in two distinct formats: rasterized features ${\mathcal{F}}_{SD}^{\mathrm{R}}$ and vectorized features ${\mathcal{F}}_{SD}^{\mathrm{V}}$ ,through a hybrid SD map encoding approach. These two representations are then leveraged to augment the BEV feature through a Feature Transformation (FT) module and a cross-attention network, respectively, producing the enhanced BEV features ${\mathcal{F}}_{B}^{\mathrm{R}}$ and ${\mathcal{F}}_{B}^{\mathrm{V}}$ . A lightweight yet effective Dual Gated Feature Fusion (DGFF) module is employed to fuse these two augmented features, generating the final enhanced BEV feature ${\mathcal{F}}_{B}^{\mathrm{{SD}}}$ . This feature is consequently decoded to address various subtasks by different heads, such as the lane segment head, area head, topology head, etc. Notably, we also introduce an additional keypoint head for an auxiliary task, which detects road intersections from SD maps, further enhancing scene understanding capabilities.
我们SEPT框架的整体流程如图2所示，通过引入SD地图先验提升基线模型。具体地，给定多视角图像，PV2BEV编码器首先通过图像主干网络提取视觉信息，然后通过视角变换将透视视图（PV）特征转换为鸟瞰视图（BEV）特征，记为${\mathcal{F}}_{B}$。此外，SD地图先验通过混合SD地图编码方法被编码为两种不同格式：栅格化特征${\mathcal{F}}_{SD}^{\mathrm{R}}$和矢量化特征${\mathcal{F}}_{SD}^{\mathrm{V}}$。这两种表示随后分别通过特征变换（FT）模块和交叉注意力网络增强BEV特征，生成增强的BEV特征${\mathcal{F}}_{B}^{\mathrm{R}}$和${\mathcal{F}}_{B}^{\mathrm{V}}$。采用轻量且高效的双门控特征融合（DGFF）模块融合这两种增强特征，生成最终的增强BEV特征${\mathcal{F}}_{B}^{\mathrm{{SD}}}$。该特征随后由不同的头部解码以完成各子任务，如车道段头、区域头、拓扑头等。值得注意的是，我们还引入了一个额外的关键点头作为辅助任务，用于从SD地图检测路口，进一步提升场景理解能力。


## C. Hybrid SD Map Encoding and Fusion
## C. 混合SD地图编码与融合


To fully leverage SD map priors, we introduce a hybrid encoding approach, utilizing both rasterized and vectorized formats. These two representations are incorporated to enhance the BEV feature while ensuring implicit alignment between them. In addition, we design an efficient and effective fusion strategy to seamlessly integrate these features, thereby improving overall performance. Herein, let the BEV feature be represented as ${\mathcal{F}}_{B} \in  {\mathbb{R}}^{H \times  W \times  C}$ ,where $H$ and $W$ correspond to the spatial dimensions of the BEV perception range,and $C$ denotes the feature dimension.
为充分利用SD地图先验，我们引入了一种混合编码方法，结合栅格化和矢量化两种格式。这两种表示被用来增强BEV特征，同时确保它们之间的隐式对齐。此外，我们设计了一种高效且有效的融合策略，实现这些特征的无缝整合，从而提升整体性能。这里，设BEV特征表示为${\mathcal{F}}_{B} \in  {\mathbb{R}}^{H \times  W \times  C}$，其中$H$和$W$对应BEV感知范围的空间维度，$C$表示特征维度。


1) Vectorized SD Map Encoding: Given raw polylines of SD maps, we begin by uniformly resampling these sequences to obtain $M$ segments. For each segment,we further evenly sample a fixed number of points. Following the structure of the classical vectorized method, SMERF [10], we then vectorize the SD map and extract the initial vectorized feature ${\mathcal{F}}_{SD}^{\mathrm{V}} \in  {\mathbb{R}}^{M \times  C}$ using a Transformer-based encoder model. In this paradigm, spatial misalignment between the SD map tokens and the BEV space can be mitigated through a multihead cross-attention mechanism. Here, the BEV feature acts as query tokens, while the SD map tokens serve as keys and values. This enables the BEV queries to adaptively aggregate relevant SD map tokens conditioned on a learnable attention distribution. As a result, we obtain implicitly aligned BEV features ${\mathcal{F}}_{B}^{\mathrm{V}} \in  {\mathbb{R}}^{H \times  W \times  C}$ ,complemented by the vectorized SD map priors.
1) 向量化SD地图编码：给定原始的SD地图折线，我们首先对这些序列进行均匀重采样以获得$M$段。对于每个段，我们进一步均匀采样固定数量的点。遵循经典向量化方法SMERF [10]的结构，随后我们对SD地图进行向量化，并使用基于Transformer的编码器模型提取初始向量化特征${\mathcal{F}}_{SD}^{\mathrm{V}} \in  {\mathbb{R}}^{M \times  C}$。在此范式中，SD地图标记与BEV空间之间的空间错位可以通过多头交叉注意力机制得到缓解。这里，BEV特征作为查询标记，而SD地图标记作为键和值。这使得BEV查询能够根据可学习的注意力分布自适应地聚合相关的SD地图标记。因此，我们获得了隐式对齐的BEV特征${\mathcal{F}}_{B}^{\mathrm{V}} \in  {\mathbb{R}}^{H \times  W \times  C}$，并辅以向量化的SD地图先验。


2) Rasterized SD Map Encoding: We first rasterize the SD map into an $H \times  W$ canvas with a binary representation, where each grid cell is assigned a value of 1 if occupied by a polyline, and 0 otherwise. Different road types, such as crosswalks and sidewalks, are encoded as separate channels. The original SD map features are then extracted using CNNs, yielding the rasterized feature ${\mathcal{F}}_{SD}^{\mathrm{R}} \in  {\mathbb{R}}^{H \times  W \times  C}$ . Note that this feature may be weakly misaligned with the BEV space. To address this, motivated by the T-Net in PointNet [26], we introduce a Feature Transformation (FT) module to align ${\mathcal{F}}_{SD}^{\mathrm{R}}$ with ${\mathcal{F}}_{B}^{\mathrm{V}}$ at the feature level. Specifically,we first project both features along the channel dimension and compute their feature difference ${\mathcal{F}}_{\Delta } \in  {\mathbb{R}}^{H \times  W \times  C}$ ,which represents a form of calibration error. We then apply a max-pooling operation on ${\mathcal{F}}_{\Delta }$ to obtain the global context vector ${\mathcal{F}}_{\Delta }^{\text{Global }} \in  {\mathbb{R}}^{C}$ . Next, we leverage Feature-wise Linear Modulation (FiLM) [27] to predict the scaling factor $\gamma  \in  {\mathbb{R}}^{C}$ and the bias term $\beta  \in  {\mathbb{R}}^{C}$ for each feature channel. Finally, these transformation parameters are applied to the rasterized feature ${\mathcal{F}}_{SD}^{\mathrm{R}}$ ,resulting in the enhanced BEV feature ${\mathcal{F}}_{B}^{\mathrm{R}} \in  {\mathbb{R}}^{H \times  W \times  C}$ ,with implicit spatial alignment, as follows:
2) 光栅化SD地图编码：我们首先将SD地图光栅化为一个$H \times  W$画布，采用二值表示，其中每个网格单元若被折线占据则赋值为1，否则为0。不同的道路类型，如人行横道和人行道，被编码为独立的通道。随后使用卷积神经网络（CNN）提取原始SD地图特征，得到光栅化特征${\mathcal{F}}_{SD}^{\mathrm{R}} \in  {\mathbb{R}}^{H \times  W \times  C}$。注意，该特征可能与BEV空间存在弱错位。为解决此问题，受PointNet [26]中T-Net的启发，我们引入特征变换（FT）模块，在特征层面对齐${\mathcal{F}}_{SD}^{\mathrm{R}}$与${\mathcal{F}}_{B}^{\mathrm{V}}$。具体地，我们首先沿通道维度投影两者并计算它们的特征差异${\mathcal{F}}_{\Delta } \in  {\mathbb{R}}^{H \times  W \times  C}$，该差异代表一种校准误差。然后对${\mathcal{F}}_{\Delta }$进行最大池化操作以获得全局上下文向量${\mathcal{F}}_{\Delta }^{\text{Global }} \in  {\mathbb{R}}^{C}$。接着，我们利用特征线性调制（FiLM）[27]预测每个特征通道的缩放因子$\gamma  \in  {\mathbb{R}}^{C}$和偏置项$\beta  \in  {\mathbb{R}}^{C}$。最后，将这些变换参数应用于光栅化特征${\mathcal{F}}_{SD}^{\mathrm{R}}$，得到隐式空间对齐的增强BEV特征${\mathcal{F}}_{B}^{\mathrm{R}} \in  {\mathbb{R}}^{H \times  W \times  C}$，具体如下：


$$
{\mathcal{F}}_{B}^{\mathrm{R}} = \gamma  \odot  {\mathcal{F}}_{SD}^{\mathrm{R}} + \beta , \tag{1}
$$



<!-- Media -->



<!-- figureText: ${\mathcal{F}}_{B}^{ \vee  }$ FFN ${\mathcal{F}}_{B}^{\mathrm{{SD}}}$ $\sigma$ ${\mathcal{F}}_{B}^{\mathrm{R}}$ -->



<img src="https://cdn.noedgeai.com/bo_d3vjg1jef24c73d2mnk0_3.jpg?x=135&y=153&w=746&h=261&r=0"/>



Fig. 3. The hybrid feature fusion process of the DGFF module.
图3. DGFF模块的混合特征融合过程。


<!-- Media -->



where $\odot$ denotes the Hadamard (element-wise) product,and all operations follow the broadcasting mechanism.
其中$\odot$表示Hadamard（元素级）乘积，所有操作均遵循广播机制。


3) Dual Gated Feature Fusion: After obtaining the two BEV features augmented by rasterized and vectorized SD map features, it is essential to design an effective fusion strategy to combine these distinct features, as the characteristics of the two branches may differ significantly. To this end, we propose a lightweight yet powerful fusion network called the Dual Gated Feature Fusion (DGFF) module, which leverages the gated attention mechanism to aggregate the dual-branch features. As depicted in Fig. 3,the two features ${\mathcal{F}}_{B}^{\mathrm{R}}$ and ${\mathcal{F}}_{B}^{\mathrm{V}}$ are first concatenated along the feature dimension and passed through a feed-forward network to produce a fused feature ${\mathcal{F}}_{B}^{\mathrm{R} \boxplus  \mathrm{V}} \in  {\mathbb{R}}^{H \times  W \times  C}$ ,as follows:
3) 双门控特征融合：在获得由光栅化和向量化SD地图特征增强的两种BEV特征后，设计有效的融合策略以结合这两种不同特征至关重要，因为这两条分支的特性可能存在显著差异。为此，我们提出了一种轻量且强大的融合网络——双门控特征融合（DGFF）模块，该模块利用门控注意力机制聚合双分支特征。如图3所示，两个特征${\mathcal{F}}_{B}^{\mathrm{R}}$和${\mathcal{F}}_{B}^{\mathrm{V}}$首先在特征维度上拼接，并通过前馈网络生成融合特征${\mathcal{F}}_{B}^{\mathrm{R} \boxplus  \mathrm{V}} \in  {\mathbb{R}}^{H \times  W \times  C}$，具体如下：


$$
{\mathcal{F}}_{B}^{\mathrm{R} \boxplus  \mathrm{V}} = \mathrm{{FFN}}\left( {{\mathcal{F}}_{B}^{\mathrm{R}} \boxplus  {\mathcal{F}}_{B}^{\mathrm{V}}}\right) , \tag{2}
$$



where $\boxplus$ denotes concatenation along the feature dimension, and $\operatorname{FFN}\left( \cdot \right)$ represents the feed-forward network. Next,an element-wise gating mechanism is performed on each input stream using the sigmoid function, enabling the model to adaptively weight the contributions of rasterized and vectorized features. This is expressed as:
其中$\boxplus$表示沿特征维度的拼接，$\operatorname{FFN}\left( \cdot \right)$代表前馈网络。接下来，使用sigmoid函数对每个输入流执行逐元素门控机制，使模型能够自适应地权衡栅格化和矢量化特征的贡献。表达式如下：


$$
{w}_{\mathrm{R}} = \sigma \left( {\mathcal{F}}_{B}^{\mathrm{R}}\right) ,\;{w}_{\mathrm{V}} = \sigma \left( {\mathcal{F}}_{B}^{\mathrm{V}}\right) , \tag{3}
$$



where $\sigma \left( \cdot \right)$ is the element-wise sigmoid function,producing attention weights for each input stream. Although simpler, the gating mechanism introduces nonlinearity and adaptability, allowing the model to capture richer feature interactions without increasing the number of learnable parameters. This strikes a balance between representational capacity and efficiency. Finally, two parallel projection networks refine each gated feature before merging them via a weighted averaging operation, generating the final enhanced BEV feature ${\mathcal{F}}_{B}^{\mathrm{{SD}}} \in  {\mathbb{R}}^{H \times  W \times  C}$ . This process can be formulated by the following expression:
其中$\sigma \left( \cdot \right)$是逐元素sigmoid函数，为每个输入流生成注意力权重。尽管结构更简单，门控机制引入了非线性和适应性，使模型能够捕捉更丰富的特征交互，而无需增加可学习参数数量。这在表示能力和效率之间取得了平衡。最后，两个并行投影网络对每个门控特征进行细化，然后通过加权平均操作合并，生成最终增强的BEV特征${\mathcal{F}}_{B}^{\mathrm{{SD}}} \in  {\mathbb{R}}^{H \times  W \times  C}$。该过程可用以下表达式表示：


$$
{\mathcal{F}}_{B}^{\mathrm{{SD}}} = \mu  \cdot  {\operatorname{Proj}}_{\mathrm{R}}\left( {{w}_{\mathrm{R}} \odot  {\mathcal{F}}_{B}^{\mathrm{R} \boxplus  \mathrm{V}}}\right)  + \nu  \cdot  {\operatorname{Proj}}_{\mathrm{V}}\left( {{w}_{\mathrm{V}} \odot  {\mathcal{F}}_{B}^{\mathrm{R} \boxplus  \mathrm{V}}}\right) , \tag{4}
$$



where ${\operatorname{Proj}}_{\mathrm{R}}$ and ${\operatorname{Proj}}_{\mathrm{V}}$ are the parallel projection networks. $\mu$ and $\nu$ are hyperparameters for balancing each term.
其中${\operatorname{Proj}}_{\mathrm{R}}$和${\operatorname{Proj}}_{\mathrm{V}}$是并行投影网络。$\mu$和$\nu$是用于平衡各项的超参数。


Overall, with the support of the DGFF module, our hybrid SD map encoding and fusion strategy can adaptively fuse heterogeneous feature representations, empowering the model to emphasize the most informative components from each branch. This substantially boosts the representation capabilities of both rasterized and vectorized SD map priors, while maintaining efficiency and expressiveness.
总体而言，在DGFF模块的支持下，我们的混合SD地图编码与融合策略能够自适应融合异构特征表示，使模型能够强调每个分支中最具信息量的部分。这显著提升了栅格化和矢量化SD地图先验的表示能力，同时保持了效率和表现力。


## D. Intersection-Aware Keypoint Detection
## D. 路口感知关键点检测


To further enhance the BEV feature representation and improve understanding of road topology and geometry, we introduce an Intersection-Aware Keypoint Detection (IKPD) task. This auxiliary task helps the model capture road interaction patterns by detecting the road interaction distribution.
为了进一步增强BEV特征表示并提升对道路拓扑和几何结构的理解，我们引入了路口感知关键点检测（IKPD）任务。该辅助任务通过检测道路交互分布，帮助模型捕捉道路交互模式。


1) Intersection Generation: The first step in implementing the IKPD task involves identifying the intersection points of roads from SD maps, which will serve as the ground truth for supervision. Since the SD map prior provides the essential skeleton of road networks, intersection locations (e.g., merging, diverging, and crossing points) can be easily extracted. However, intersection points are typically sparsely distributed across the scene, and directly using these points as supervision can lead to class imbalance during training. Additionally, due to intrinsic positional biases relative to the ground-truth HD maps in certain scenarios, the intersection points derived from the SD map may not perfectly align with the finer details of the road structure. To mitigate this issue, we represent the keypoints as Gaussian distributions, similar to the approach used in confidence-based keypoint detection [28], [29]. Specifically, for each scene, we construct a heatmap $\mathcal{H} \in  {\mathbb{R}}^{H \times  W \times  1}$ to model the ground-truth distribution of road intersections. Each intersection is represented as a Gaussian distribution centered at its location, with a certain radius reflecting the spatial uncertainty.
1) 路口生成：实现IKPD任务的第一步是从SD地图中识别道路的路口点，这些点将作为监督的真实标签。由于SD地图先验提供了道路网络的基本骨架，路口位置（如汇合、分叉和交叉点）可以轻松提取。然而，路口点通常在场景中分布稀疏，直接使用这些点作为监督会导致训练中的类别不平衡。此外，由于某些场景中相对于真实HD地图存在固有的位置信偏差，SD地图导出的路口点可能无法与道路结构的细节完全对齐。为缓解此问题，我们将关键点表示为高斯分布，类似于基于置信度的关键点检测方法[28]，[29]。具体而言，对于每个场景，我们构建一个热图$\mathcal{H} \in  {\mathbb{R}}^{H \times  W \times  1}$来模拟道路路口的真实分布。每个路口以其位置为中心，采用一定半径的高斯分布，反映空间不确定性。


2) IKPD Head: Given the augmented BEV feature ${\mathcal{F}}_{B}^{\mathrm{{SD}}}$ , we aim to design a lightweight network capable of effectively decoding the road intersection heatmap, thereby enriching the BEV feature with crucial geometric and topological information about the road structure. The IKPD head follows an efficient design paradigm that emphasizes both local feature extraction and global context reasoning. Specifically, the BEV feature is first passed through a series of CNN blocks with residual connections. Each residual block comprises depthwise separable convolutions (i.e., a depthwise convolution followed by a pointwise convolution) [30], which decouple spatial and channel-wise operations for computational efficiency. Dilated convolutions are also incorporated for capturing broader spatial context information. After each convolution, the output feature is refined with the Squeeze-and-Excitation (SE) [31] attention, which recalibrates the channel-wise features by computing global statistics and adaptively weighting the importance of each channel. This allows the IKPD head to prioritize the most relevant features for keypoint detection, improving its ability to focus on critical patterns. Consequently, the final output is produced through a $1 \times  1$ convolution followed by a sigmoid activation function, generating a heatmap that represents the distribution of road intersections.
2) IKPD头：给定增强的BEV特征${\mathcal{F}}_{B}^{\mathrm{{SD}}}$，我们旨在设计一个轻量级网络，有效解码道路路口热图，从而丰富BEV特征中关于道路结构的关键几何和拓扑信息。IKPD头遵循高效设计范式，强调局部特征提取与全局上下文推理。具体来说，BEV特征首先通过一系列带残差连接的卷积神经网络（CNN）模块。每个残差模块包含深度可分离卷积（即先深度卷积后逐点卷积）[30]，该结构将空间和通道操作解耦以提高计算效率。还引入了空洞卷积以捕获更广泛的空间上下文信息。每次卷积后，输出特征通过Squeeze-and-Excitation（SE）[31]注意力机制进行细化，该机制通过计算全局统计量并自适应加权各通道的重要性，重新校准通道特征。这使IKPD头能够优先关注关键特征，提升关键点检测的能力。最终输出通过$1 \times  1$卷积和sigmoid激活函数生成，形成表示道路路口分布的热图。


## E. Training Objectives
## E. 训练目标


Following the baseline approaches [2], [6], the supervision is applied to each head to optimize distinct training objectives, including detection losses for lane segments, areas, and traffic elements,denoted as ${\mathcal{L}}_{\mathrm{{DET}}}$ ,and topology reasoning losses, denoted as ${\mathcal{L}}_{\text{TOP }}$ . Our proposed SEPT framework does not modify the baseline loss functions but introduces an additional loss term for the auxiliary IKPD head,denoted as ${\mathcal{L}}_{\text{IKPD }}$ . Given the road intersection distribution is sparse and imbalanced, we employ focal loss [28] to supervise the keypoint heatmap training. The overall loss $\mathcal{L}$ for SEPT is formulated as:
继基线方法[2]，[6]之后，监督信号被应用于每个头部以优化不同的训练目标，包括车道段、区域和交通元素的检测损失，记为${\mathcal{L}}_{\mathrm{{DET}}}$，以及拓扑推理损失，记为${\mathcal{L}}_{\text{TOP }}$。我们提出的SEPT框架并未修改基线损失函数，而是为辅助IKPD头引入了额外的损失项，记为${\mathcal{L}}_{\text{IKPD }}$。鉴于路口分布稀疏且不平衡，我们采用焦点损失（focal loss）[28]来监督关键点热图的训练。SEPT的总体损失$\mathcal{L}$定义如下：


$$
\mathcal{L} = {\mathcal{L}}_{\mathrm{{DET}}} + {\mathcal{L}}_{\mathrm{{TOP}}} + {\mathcal{L}}_{\mathrm{{IKPD}}}. \tag{5}
$$



<!-- Media -->



TABLE I



Quantitative results on the OLV2 validation split, benchmarked using $\mathtt{{OLS}}$ . All metrics follow the higher-the-better criterion. The official ranking metric is shaded in gray, and the best results are indicated in bold. A "-" denotes the absence of relevant DATA.
在OLV2验证集上的定量结果，基于$\mathtt{{OLS}}$进行基准测试。所有指标均遵循越高越好的原则。官方排名指标以灰色阴影标出，最佳结果以粗体显示。“-”表示缺少相关数据。


<table><tr><td rowspan="2">Method</td><td rowspan="2">${\mathrm{{DET}}}_{l} \uparrow$</td><td rowspan="2">${\mathrm{{DET}}}_{t} \uparrow$</td><td colspan="3">v1.0</td><td colspan="3">v1.1</td><td rowspan="2">Params</td></tr><tr><td>${\mathrm{{TOP}}}_{ll} \uparrow$</td><td>${\operatorname{TOP}}_{lt} \uparrow$</td><td>OLS↑</td><td>${\mathrm{{TOP}}}_{ll} \uparrow$</td><td>${\operatorname{TOP}}_{lt} \uparrow$</td><td>OLS↑</td></tr><tr><td>TopoNet [2]</td><td>28.6</td><td>48.6</td><td>4.1</td><td>20.3</td><td>35.6</td><td>10.9</td><td>23.8</td><td>39.8</td><td>62.6M</td></tr><tr><td>w/ OLV2 [9]</td><td>27.9</td><td>48.1</td><td>5.1</td><td>20.9</td><td>36.1</td><td>-</td><td>-</td><td>-</td><td>75.9M</td></tr><tr><td>w/ OSMG [9]</td><td>30.0</td><td>47.6</td><td>5.4</td><td>21.3</td><td>36.7</td><td>-</td><td>-</td><td>-</td><td>64.6M</td></tr><tr><td>w/ OSMR [9]</td><td>30.6</td><td>44.6</td><td>7.7</td><td>22.9</td><td>37.7</td><td>-</td><td>-</td><td>-</td><td>75.9M</td></tr><tr><td>w/ SMERF [10]</td><td>33.4</td><td>48.6</td><td>7.5</td><td>23.4</td><td>39.4</td><td>15.4</td><td>25.4</td><td>42.9</td><td>65.8M</td></tr><tr><td>w/ SEPT (Ours)</td><td>34.2 (+5.6)</td><td>49.8 (+1.2)</td><td>8.3 (+4.2)</td><td>23.8 (+3.5)</td><td>40.4 (+4.8)</td><td>19.5 (+8.6)</td><td>27.5 (+3.7)</td><td>45.2 (+5.4)</td><td>70.4M</td></tr><tr><td>TopoLogic [23]</td><td>29.2</td><td>46.5</td><td>18.0</td><td>20.6</td><td>40.9</td><td>23.6</td><td>24.2</td><td>43.4</td><td>61.8M</td></tr><tr><td>w/ SMERF [10]</td><td>31.0</td><td>48.7</td><td>21.2</td><td>22.4</td><td>43.3</td><td>26.9</td><td>26.2</td><td>45.7</td><td>65.1M</td></tr><tr><td>w/ SEPT (Ours)</td><td>34.3 (+5.1)</td><td>48.9 (+2.4)</td><td>25.1 (+7.1)</td><td>25.1 (+4.5)</td><td>45.8 (+4.9)</td><td>31.2 (+7.6)</td><td>29.7 (+5.5)</td><td>48.4 (+5.0)</td><td>69.6M</td></tr></table>
<table><tbody><tr><td rowspan="2">方法</td><td rowspan="2">${\mathrm{{DET}}}_{l} \uparrow$</td><td rowspan="2">${\mathrm{{DET}}}_{t} \uparrow$</td><td colspan="3">v1.0</td><td colspan="3">v1.1</td><td rowspan="2">参数</td></tr><tr><td>${\mathrm{{TOP}}}_{ll} \uparrow$</td><td>${\operatorname{TOP}}_{lt} \uparrow$</td><td>OLS↑</td><td>${\mathrm{{TOP}}}_{ll} \uparrow$</td><td>${\operatorname{TOP}}_{lt} \uparrow$</td><td>OLS↑</td></tr><tr><td>TopoNet [2]</td><td>28.6</td><td>48.6</td><td>4.1</td><td>20.3</td><td>35.6</td><td>10.9</td><td>23.8</td><td>39.8</td><td>62.6M</td></tr><tr><td>使用 OLV2 [9]</td><td>27.9</td><td>48.1</td><td>5.1</td><td>20.9</td><td>36.1</td><td>-</td><td>-</td><td>-</td><td>75.9M</td></tr><tr><td>使用 OSMG [9]</td><td>30.0</td><td>47.6</td><td>5.4</td><td>21.3</td><td>36.7</td><td>-</td><td>-</td><td>-</td><td>64.6M</td></tr><tr><td>使用 OSMR [9]</td><td>30.6</td><td>44.6</td><td>7.7</td><td>22.9</td><td>37.7</td><td>-</td><td>-</td><td>-</td><td>75.9M</td></tr><tr><td>使用 SMERF [10]</td><td>33.4</td><td>48.6</td><td>7.5</td><td>23.4</td><td>39.4</td><td>15.4</td><td>25.4</td><td>42.9</td><td>65.8M</td></tr><tr><td>使用 SEPT（本方法）</td><td>34.2 (+5.6)</td><td>49.8 (+1.2)</td><td>8.3 (+4.2)</td><td>23.8 (+3.5)</td><td>40.4 (+4.8)</td><td>19.5 (+8.6)</td><td>27.5 (+3.7)</td><td>45.2 (+5.4)</td><td>70.4M</td></tr><tr><td>TopoLogic [23]</td><td>29.2</td><td>46.5</td><td>18.0</td><td>20.6</td><td>40.9</td><td>23.6</td><td>24.2</td><td>43.4</td><td>61.8M</td></tr><tr><td>使用 SMERF [10]</td><td>31.0</td><td>48.7</td><td>21.2</td><td>22.4</td><td>43.3</td><td>26.9</td><td>26.2</td><td>45.7</td><td>65.1M</td></tr><tr><td>使用 SEPT（本方法）</td><td>34.3 (+5.1)</td><td>48.9 (+2.4)</td><td>25.1 (+7.1)</td><td>25.1 (+4.5)</td><td>45.8 (+4.9)</td><td>31.2 (+7.6)</td><td>29.7 (+5.5)</td><td>48.4 (+5.0)</td><td>69.6M</td></tr></tbody></table>


<!-- Media -->



## IV. EXPERIMENTS AND RESULTS
## 四、实验与结果


## A. Experiment Setups
## A. 实验设置


1) Real-World Dataset: We train and evaluate our proposed approach on the large-scale OpenLane-V2 (OLV2) dataset [5], which, to the best of our knowledge, is currently the only benchmark for both scene perception and topology reasoning in autonomous driving. All experiments in our work are conducted on the primary subset of OLV2, subset_A, built upon the Argoverse 2 [32] dataset with additional annotations for lane segments, traffic elements, and lane topology, etc. The subset $A$ comprises $1\mathrm{\;k}$ scenes collected from six cities, with $2\mathrm{{Hz}}$ multi-view images and optional SD map information (including three categories: roads, crosswalks, and sidewalks) extracted from the OpenStreetMap (OSM) [33]. The training set consists of approximately ${27}\mathrm{\;k}$ frames,while the validation set contains around ${4.8}\mathrm{k}$ frames.
1) 真实世界数据集：我们在大规模OpenLane-V2（OLV2）数据集[5]上训练和评估所提出的方法。据我们所知，该数据集目前是自动驾驶中场景感知和拓扑推理的唯一基准。我们所有的实验均在OLV2的主子集subset_A上进行，该子集基于Argoverse 2 [32]数据集构建，附加了车道段、交通元素和车道拓扑等标注。子集$A$包含从六个城市采集的$1\mathrm{\;k}$个场景，拥有$2\mathrm{{Hz}}$多视角图像和可选的SD地图信息（包括道路、人行横道和人行道三类），这些信息来自OpenStreetMap（OSM）[33]。训练集约有${27}\mathrm{\;k}$帧，验证集约有${4.8}\mathrm{k}$帧。


2) Evaluation Metrics: We evaluate the performance of perception and reasoning using the official metrics provided by OLV2 [5]. There are two primary benchmark categories, each with distinct evaluation metrics: OLV2 Score (OLS) and OLV2 UniScore (OLUS). Both scores are averages derived from various metrics across different subtasks. The main distinction between them is that OLS focuses exclusively on lane centerline perception, while OLUS emphasizes lane segment perception. Specifically,OLS includes four sub-metrics: ${\mathrm{{DET}}}_{l}$ , ${\mathrm{{DET}}}_{t},{\mathrm{{TOP}}}_{ll}$ ,and ${\mathrm{{TOP}}}_{lt}$ . ${\mathrm{{DET}}}_{l}$ measures the mean average precision (mAP) for lane centerline detection, based on the Fréchet distance with match thresholds of 1.0, 2.0, and 3.0. ${\mathrm{{DET}}}_{t}$ represents the mAP for traffic element recognition, conditioned on the average Intersection over Union (IoU) with a match threshold of 0.75 across various traffic attributes. ${\mathrm{{TOP}}}_{ll}$ and ${\mathrm{{TOP}}}_{lt}$ measure mAP for topology among lane centerlines and between lane centerlines and traffic elements, using the adjacency matrix. Notably, there are two versions for calculating TOP scores: V1.0 and V1.1, with the V1.0 calculation containing a potential loophole issue [22]. The OLS score is computed as the average of these four metrics, given by:
2) 评估指标：我们使用OLV2官方提供的指标[5]评估感知和推理性能。主要有两类基准指标，分别是OLV2分数（OLS）和OLV2统一分数（OLUS），两者均为不同子任务多项指标的平均值。主要区别在于OLS专注于车道中心线感知，而OLUS强调车道段感知。具体而言，OLS包含四个子指标：${\mathrm{{DET}}}_{l}$、${\mathrm{{DET}}}_{t},{\mathrm{{TOP}}}_{ll}$和${\mathrm{{TOP}}}_{lt}$。${\mathrm{{DET}}}_{l}$衡量基于Fréchet距离（匹配阈值为1.0、2.0和3.0）的车道中心线检测的平均精度均值（mAP）。${\mathrm{{DET}}}_{t}$表示基于平均交并比（IoU）且匹配阈值为0.75的多种交通属性的交通元素识别mAP。${\mathrm{{TOP}}}_{ll}$和${\mathrm{{TOP}}}_{lt}$分别衡量车道中心线之间及车道中心线与交通元素之间的拓扑mAP，采用邻接矩阵表示。值得注意的是，TOP分数的计算有两个版本：V1.0和V1.1，其中V1.0版本存在潜在漏洞[22]。OLS分数为这四个指标的平均值，计算公式为：


$$
\mathrm{{OLS}} = \frac{1}{4}\left\lbrack  {{\mathrm{{DET}}}_{l} + {\mathrm{{DET}}}_{t} + f\left( {\mathrm{{TOP}}}_{ll}\right)  + f\left( {\mathrm{{TOP}}}_{lt}\right) }\right\rbrack  , \tag{6}
$$



where $f$ represents the square root function.
其中$f$表示平方根函数。


In contrast,OLUS encompasses five sub-metrics: ${\mathrm{{DET}}}_{ls}$ , ${\mathrm{{DET}}}_{a},{\mathrm{{DET}}}_{te},{\mathrm{{TOP}}}_{lsls}$ ,and ${\mathrm{{TOP}}}_{lste}$ ,covering detection for lane segments, areas, and traffic elements, as well as topology reasoning among lane segments and between lane segments and traffic elements. These metrics follow a similar calculation procedure to OLS,with the addition of ${\mathrm{{DET}}}_{a}$ ,which is measured using Chamfer distance.
相比之下，OLUS包含五个子指标：${\mathrm{{DET}}}_{ls}$、${\mathrm{{DET}}}_{a},{\mathrm{{DET}}}_{te},{\mathrm{{TOP}}}_{lsls}$和${\mathrm{{TOP}}}_{lste}$，涵盖车道段、区域和交通元素的检测，以及车道段之间和车道段与交通元素之间的拓扑推理。这些指标的计算方法与OLS类似，额外包含使用Chamfer距离测量的${\mathrm{{DET}}}_{a}$。


3) Implementation Details: We select two representative high-performance models as baselines: TopoNet [2] for OLS and LaneSegNet [6] for OLUS. To ensure a fair comparison, we retain the official implementations of both baseline models, incorporating only the modules designed specifically for the SD map prior into the codebase. All models employ the default ResNet-50 backbone,with BEV feature dimensions set to $H =$ 200 and $W = {100}$ . We train our model on eight GPUs with a total batch size of 8 . The training configuration, including the learning rate and optimizer, remains consistent with baseline settings, and all models share the same hyperparameters.
3) 实现细节：我们选择两个代表性的高性能模型作为基线：TopoNet [2]用于OLS，LaneSegNet [6]用于OLUS。为保证公平比较，我们保留两个基线模型的官方实现，仅将专门设计的SD地图先验模块整合进代码库。所有模型均采用默认的ResNet-50主干网络，BEV特征维度设置为$H =$ 200和$W = {100}$。我们在八块GPU上训练模型，总批量大小为8。训练配置（包括学习率和优化器）与基线保持一致，所有模型共享相同的超参数。


## B. Comparison with State-of-the-Art
## B. 与最先进方法的比较


We compare our SEPT framework with other state-of-the-art methods that incorporate SD maps as input on the OLV2 benchmark, using the OLS evaluation metric. TopoNet [2] serves as the baseline for centerline perception. TopoNet with OLV2, OSMG, and OSMR [9] represent approaches utilizing rasterized SD maps from OLV2, augmented with full OSM attributes (e.g., stop signs, speed limits), and graph-based SD maps with OSM augmentation, respectively. SMERF is a classical method that enhances lane-topology understanding with SD maps in a vectorized representation. We present results for both v1.0 and v1.1 metrics to provide a comprehensive comparison, as shown in the upper group of Tab. I. Since the v1.1 metric is a recent update, the results for TopoNet and SMERF on v1.1 have been reevaluated using their official checkpoints, while others are not available. Compared to the baseline, our method significantly improves performance across all subtasks by effectively integrating SD map priors without introducing excessive parameters. Specifically, we achieve a 4.8 OLS improvement for v1.0 and a 5.4 OLS improvement for $\mathrm{v}{1.1}$ ,with a notable ${8.6}{\mathrm{{TOP}}}_{ll}$ increase in topology reasoning, outperforming other existing methods augmented with SD maps. Moreover, we also apply our SEPT framework to the recent model, TopoLogic [23], which is designed to enhance lane topology reasoning. As shown in the lower group of Tab. I, our SEPT consistently improves performance across all subtasks, achieving a 4.9 OLS gain for v1.0 and a 5.0 OLS gain for v1.1.
我们在OLV2基准上，使用OLS评估指标，将我们的SEPT框架与其他将SD地图作为输入的先进方法进行了比较。TopoNet [2]作为中心线感知的基线方法。TopoNet结合OLV2、OSMG和OSMR [9]分别代表利用OLV2栅格化SD地图、增强了完整OSM属性（如停车标志、限速）以及基于图的SD地图并进行OSM增强的方法。SMERF是一种经典方法，利用矢量化表示的SD地图增强车道拓扑理解。我们展示了v1.0和v1.1两个版本的指标结果，以提供全面比较，如表I上半部分所示。由于v1.1指标是近期更新，TopoNet和SMERF在v1.1上的结果已使用其官方检查点重新评估，其他方法则无此数据。与基线相比，我们的方法通过有效整合SD地图先验，在所有子任务上显著提升性能，且未引入过多参数。具体而言，我们在v1.0上实现了4.8的OLS提升，在$\mathrm{v}{1.1}$上实现了5.4的OLS提升，拓扑推理方面有显著的${8.6}{\mathrm{{TOP}}}_{ll}$提升，优于其他基于SD地图增强的现有方法。此外，我们还将SEPT框架应用于近期设计用于增强车道拓扑推理的模型TopoLogic [23]。如表I下半部分所示，我们的SEPT在所有子任务上持续提升性能，v1.0获得4.9的OLS增益，v1.1获得5.0的OLS增益。


<!-- Media -->



TABLE II



Quantitative results on the OLV2 validation split with map element buckets, benethmarked using OLUS.
在OLV2验证集上，基于地图元素分桶，使用OLUS进行基准测试的定量结果。


<table><tr><td>Method</td><td>Raster</td><td>Vector</td><td>IKPD</td><td>${\mathrm{{DET}}}_{ls} \uparrow$</td><td>${\mathrm{{DET}}}_{a} \uparrow$</td><td>${\text{DET}}_{te} \uparrow$</td><td>${\mathrm{{TOP}}}_{lsls} \uparrow$</td><td>${\mathrm{{TOP}}}_{\text{lste }} \uparrow$</td><td>OLUS $\uparrow$</td><td>Params</td></tr><tr><td>LaneSegNet [6]</td><td/><td/><td/><td>30.9</td><td>20.0</td><td>36.7</td><td>25.6</td><td>20.8</td><td>36.7</td><td>61.8M</td></tr><tr><td>w/ Raster Only</td><td>✓</td><td/><td/><td>33.8 (+2.9)</td><td>${28.1}\left( {+{8.1}}\right)$</td><td>${38.1}\left( {+{1.4}}\right)$</td><td>27.5 (+1.9)</td><td>21.8 (+1.0)</td><td>39.9 (+3.2)</td><td>65.5M</td></tr><tr><td>w/ Vector Only</td><td/><td>✓</td><td/><td>35.3 (+4.4)</td><td>${22.3}\left( {+{2.5}}\right)$</td><td>39.2 (+2.3)</td><td>30.2 (+4.6)</td><td>${22.6}\left( {+{1.8}}\right)$</td><td>39.9 (+3.2)</td><td>65.8M</td></tr><tr><td>w/ Hybrid Fusion</td><td>✓</td><td>✓</td><td/><td>35.8 (+4.9)</td><td>${28.2}\left( {+{8.2}}\right)$</td><td>39.6 (+2.9)</td><td>31.0 (+5.4)</td><td>${22.7}\left( {+{1.9}}\right)$</td><td>41.4 (+4.7)</td><td>69.8M</td></tr><tr><td>w/ SEPT</td><td>✓</td><td>✓</td><td>✓</td><td>38.4 (+7.5)</td><td>29.0 (+9.0)</td><td>40.0 (+3.3)</td><td>32.2 (+6.6)</td><td>23.8 (+3.0)</td><td>42.6 (+5.9)</td><td>70.4M</td></tr></table>
<table><tbody><tr><td>方法</td><td>栅格</td><td>矢量</td><td>IKPD</td><td>${\mathrm{{DET}}}_{ls} \uparrow$</td><td>${\mathrm{{DET}}}_{a} \uparrow$</td><td>${\text{DET}}_{te} \uparrow$</td><td>${\mathrm{{TOP}}}_{lsls} \uparrow$</td><td>${\mathrm{{TOP}}}_{\text{lste }} \uparrow$</td><td>OLUS $\uparrow$</td><td>参数</td></tr><tr><td>LaneSegNet [6]</td><td></td><td></td><td></td><td>30.9</td><td>20.0</td><td>36.7</td><td>25.6</td><td>20.8</td><td>36.7</td><td>61.8M</td></tr><tr><td>仅使用栅格</td><td>✓</td><td></td><td></td><td>33.8 (+2.9)</td><td>${28.1}\left( {+{8.1}}\right)$</td><td>${38.1}\left( {+{1.4}}\right)$</td><td>27.5 (+1.9)</td><td>21.8 (+1.0)</td><td>39.9 (+3.2)</td><td>65.5M</td></tr><tr><td>仅使用矢量</td><td></td><td>✓</td><td></td><td>35.3 (+4.4)</td><td>${22.3}\left( {+{2.5}}\right)$</td><td>39.2 (+2.3)</td><td>30.2 (+4.6)</td><td>${22.6}\left( {+{1.8}}\right)$</td><td>39.9 (+3.2)</td><td>65.8M</td></tr><tr><td>混合融合</td><td>✓</td><td>✓</td><td></td><td>35.8 (+4.9)</td><td>${28.2}\left( {+{8.2}}\right)$</td><td>39.6 (+2.9)</td><td>31.0 (+5.4)</td><td>${22.7}\left( {+{1.9}}\right)$</td><td>41.4 (+4.7)</td><td>69.8M</td></tr><tr><td>使用SEPT</td><td>✓</td><td>✓</td><td>✓</td><td>38.4 (+7.5)</td><td>29.0 (+9.0)</td><td>40.0 (+3.3)</td><td>32.2 (+6.6)</td><td>23.8 (+3.0)</td><td>42.6 (+5.9)</td><td>70.4M</td></tr></tbody></table>


TABLE III



EFFECT OF FT MODULE ON RASTERIZED SD MAP ENCODING.
FT模块对光栅化SD地图编码的影响。


<table><tr><td>FT</td><td>${\mathrm{{DET}}}_{ls}$</td><td>${\mathrm{{DET}}}_{a}$</td><td>${\mathrm{{DET}}}_{te}$</td><td>${\mathrm{{TOP}}}_{lsls}$</td><td>${\mathrm{{TOP}}}_{\text{lste}}$</td><td>OLUS</td></tr><tr><td>✘</td><td>32.2</td><td>24.5</td><td>36.7</td><td>26.7</td><td>21.2</td><td>38.2</td></tr><tr><td>✓</td><td>33.8</td><td>28.1</td><td>38.1</td><td>27.5</td><td>21.8</td><td>39.9</td></tr></table>
<table><tbody><tr><td>傅里叶变换</td><td>${\mathrm{{DET}}}_{ls}$</td><td>${\mathrm{{DET}}}_{a}$</td><td>${\mathrm{{DET}}}_{te}$</td><td>${\mathrm{{TOP}}}_{lsls}$</td><td>${\mathrm{{TOP}}}_{\text{lste}}$</td><td>奥卢斯</td></tr><tr><td>✘</td><td>32.2</td><td>24.5</td><td>36.7</td><td>26.7</td><td>21.2</td><td>38.2</td></tr><tr><td>✓</td><td>33.8</td><td>28.1</td><td>38.1</td><td>27.5</td><td>21.8</td><td>39.9</td></tr></tbody></table>


<!-- Media -->



In addition, we further evaluate our approach using the latest and more challenging OLUS evaluation metric, which focuses on lane segment perception and provides a more comprehensive assessment of the map element bucket. We directly apply our SEPT framework to LaneSegNet [6], a leading method for driving scene topology. As shown in Tab. II, even without further adaptation, our framework significantly enhances the baseline performance across all five subtasks, resulting in an overall improvement of 5.9 OLUS. This underscores the effectiveness and generalizability of our proposed framework.
此外，我们进一步使用最新且更具挑战性的OLUS评估指标对我们的方法进行评估，该指标侧重于车道段感知，并提供对地图元素桶的更全面评估。我们直接将SEPT框架应用于LaneSegNet [6]，这是一种领先的驾驶场景拓扑方法。如表II所示，即使没有进一步的适配，我们的框架也显著提升了基线模型在所有五个子任务上的表现，总体提升了5.9 OLUS分数。这凸显了我们所提框架的有效性和泛化能力。


## C. Ablation Study
## C. 消融研究


We conduct ablation studies to validate the effectiveness of each proposed component of SEPT as well as FT and DGFF modules using the OLV2 validation split, employing the OLUS evaluation metric for a comprehensive assessment. LaneSegNet [6] serves as the baseline model.
我们使用OLV2验证集划分，采用OLUS评估指标进行全面评估，开展消融研究以验证SEPT各个提出组件以及FT和DGFF模块的有效性。LaneSegNet [6]作为基线模型。


1) Component Study of SEPT: We conduct an in-depth analysis of the contributions of each proposed component, as shown in Tab. II.
1) SEPT组件研究：我们对各个提出组件的贡献进行了深入分析，如表II所示。


Hybrid Fusion. Compared to the baseline, integrating the SD map prior, whether in rasterized or vectorized representation, improves performance across all metrics, with both formats yielding comparable results. Specifically, the rasterized SD map significantly enhances area detection by ${8.1}{\mathrm{{DET}}}_{a}$ ,as rasterization captures more spatial information. In contrast, the vectorized format improves the lane segment detection and topology reasoning between lane segments by ${4.4}{\mathrm{{DET}}}_{ls}$ and ${4.6}{\mathrm{{TOP}}}_{\text{lals }}$ ,respectively,as vectorization better preserves the geometry and topology of the road structure. This highlights that both representations have distinct advantages and should complement each other,underscoring the superiority of employing our hybrid fusion strategy.
混合融合。与基线相比，整合SD地图先验，无论是栅格化还是矢量化表示，都提升了所有指标的性能，且两种格式的结果相当。具体而言，栅格化SD地图显著提升了区域检测能力${8.1}{\mathrm{{DET}}}_{a}$，因为栅格化捕捉了更多的空间信息。相反，矢量化格式分别提升了车道段检测和车道段间拓扑推理能力${4.4}{\mathrm{{DET}}}_{ls}$和${4.6}{\mathrm{{TOP}}}_{\text{lals }}$，因为矢量化更好地保留了道路结构的几何形状和拓扑关系。这表明两种表示各有优势，应相辅相成，凸显了采用我们混合融合策略的优越性。


<!-- Media -->



TABLE IV



COMPARISON OF DIFFERENT FEATURE FUSION STRATEGIES.
不同特征融合策略的比较。


<table><tr><td>Fusion Strategy</td><td>${\mathrm{{DET}}}_{ls}$</td><td>${\mathrm{{DET}}}_{a}$</td><td>${\mathrm{{DET}}}_{te}$</td><td>${\mathrm{{TOP}}}_{lsls}$</td><td>${\mathrm{{TOP}}}_{\text{lste}}$</td><td>OLUS</td></tr><tr><td>Addition</td><td>35.4</td><td>23.9</td><td>36.8</td><td>29.5</td><td>21.7</td><td>39.4</td></tr><tr><td>Concatenation</td><td>35.7</td><td>24.3</td><td>37.7</td><td>29.9</td><td>22.0</td><td>39.9</td></tr><tr><td>Cross-Attention</td><td>35.5</td><td>25.5</td><td>38.3</td><td>30.0</td><td>22.5</td><td>40.3</td></tr><tr><td>DGFF (Ours)</td><td>35.8</td><td>28.2</td><td>39.6</td><td>31.0</td><td>22.7</td><td>41.4</td></tr></table>
<table><tbody><tr><td>融合策略</td><td>${\mathrm{{DET}}}_{ls}$</td><td>${\mathrm{{DET}}}_{a}$</td><td>${\mathrm{{DET}}}_{te}$</td><td>${\mathrm{{TOP}}}_{lsls}$</td><td>${\mathrm{{TOP}}}_{\text{lste}}$</td><td>OLUS</td></tr><tr><td>加法</td><td>35.4</td><td>23.9</td><td>36.8</td><td>29.5</td><td>21.7</td><td>39.4</td></tr><tr><td>拼接</td><td>35.7</td><td>24.3</td><td>37.7</td><td>29.9</td><td>22.0</td><td>39.9</td></tr><tr><td>交叉注意力</td><td>35.5</td><td>25.5</td><td>38.3</td><td>30.0</td><td>22.5</td><td>40.3</td></tr><tr><td>DGFF（我们的方法）</td><td>35.8</td><td>28.2</td><td>39.6</td><td>31.0</td><td>22.7</td><td>41.4</td></tr></tbody></table>


TABLE V



COMPARISON OF DIFFERENT FUSION WEIGHTS.
不同融合权重的比较。


<table><tr><td>Raster</td><td>Vector</td><td>${\mathrm{{DET}}}_{ls}$</td><td>${\mathrm{{DET}}}_{a}$</td><td>${\mathrm{{DET}}}_{te}$</td><td>${\mathrm{{TOP}}}_{lsls}$</td><td>${\text{TOP}}_{lste}$</td><td>OLUS</td></tr><tr><td>0.2</td><td>0.8</td><td>35.5</td><td>27.1</td><td>39.2</td><td>30.1</td><td>22.4</td><td>40.8</td></tr><tr><td>0.5</td><td>0.5</td><td>35.8</td><td>28.2</td><td>39.6</td><td>31.0</td><td>22.7</td><td>41.4</td></tr><tr><td>0.8</td><td>0.2</td><td>35.2</td><td>27.5</td><td>38.9</td><td>30.1</td><td>22.3</td><td>40.7</td></tr></table>
<table><tbody><tr><td>光栅</td><td>矢量</td><td>${\mathrm{{DET}}}_{ls}$</td><td>${\mathrm{{DET}}}_{a}$</td><td>${\mathrm{{DET}}}_{te}$</td><td>${\mathrm{{TOP}}}_{lsls}$</td><td>${\text{TOP}}_{lste}$</td><td>OLUS</td></tr><tr><td>0.2</td><td>0.8</td><td>35.5</td><td>27.1</td><td>39.2</td><td>30.1</td><td>22.4</td><td>40.8</td></tr><tr><td>0.5</td><td>0.5</td><td>35.8</td><td>28.2</td><td>39.6</td><td>31.0</td><td>22.7</td><td>41.4</td></tr><tr><td>0.8</td><td>0.2</td><td>35.2</td><td>27.5</td><td>38.9</td><td>30.1</td><td>22.3</td><td>40.7</td></tr></tbody></table>


<!-- Media -->



IKPD. As shown in the last two rows of Tab. II, incorporating the auxiliary IKPD task enables our method to fully exploit the potential of the SD map prior, leading to further improvements across all subtasks in scene perception and topology reasoning.
IKPD。如表II最后两行所示，加入辅助IKPD任务使我们的方法能够充分利用SD地图先验的潜力，从而在场景感知和拓扑推理的所有子任务中均取得进一步提升。


2) Effect of the FT Module: We first investigate the impact of spatial alignment in the rasterized SD map encoding process. In the rasterized SD map-only configuration, we remove the proposed FT module from the pipeline. As shown in Tab. III, the model with the FT module consistently outperforms its counterpart across all metrics. This highlights the importance of aligning the SD map and BEV feature space, with feature-level alignment effectively mitigating associated challenges.
2) FT模块的作用：我们首先研究了栅格化SD地图编码过程中空间对齐的影响。在仅使用栅格化SD地图的配置中，我们从流程中移除了所提的FT模块。如表III所示，带有FT模块的模型在所有指标上均持续优于其对应版本。这凸显了对齐SD地图与BEV特征空间的重要性，特征级对齐有效缓解了相关挑战。


3) Effect of the DGFF Module: We further evaluate the fusion capability of the proposed DGFF module. Given the two BEV features ${\mathcal{F}}_{B}^{\mathrm{R}}$ and ${\mathcal{F}}_{B}^{\mathrm{V}}$ ,enhanced by rasterized and vectorized SD maps, respectively, we replace the DGFF with various fusion strategies, including element-wise addition, concatenation (followed by FFN), and cross-attention. As shown in Tab. IV, simple combinations of these features either hinder overall performance or fail to achieve a synergistic effect. While cross-attention improves performance, it introduces significant computational overhead. In contrast, our DGFF module is both more efficient and effective, utilizing a gated attention mechanism to integrate the two features and deliver superior performance. Additionally, we also explore the impact of different combination weights (i.e., $\mu$ and $\nu$ in Eq. 4) for the gated rasterized and vectorized features. From Tab. V, we observe that a balanced combination of the two features yields the best performance, with the rasterized term slightly outperforming in area detection and the vectorized term excelling in lane segment detection. These results are consistent with the findings discussed in Section IV-C1.
3) DGFF模块的作用：我们进一步评估了所提DGFF模块的融合能力。给定由栅格化和矢量化SD地图分别增强的两个BEV特征${\mathcal{F}}_{B}^{\mathrm{R}}$和${\mathcal{F}}_{B}^{\mathrm{V}}$，我们用多种融合策略替代DGFF，包括逐元素相加、拼接（后接前馈网络）和交叉注意力。如表IV所示，这些特征的简单组合要么阻碍整体性能，要么未能实现协同效应。虽然交叉注意力提升了性能，但带来了显著的计算开销。相比之下，我们的DGFF模块更高效且更有效，利用门控注意力机制融合两种特征，表现更优。此外，我们还探讨了门控栅格化和矢量化特征的不同组合权重（即公式4中的$\mu$和$\nu$）的影响。从表V观察到，两种特征的均衡组合效果最佳，其中栅格化项在区域检测中略优，矢量化项在车道段检测中表现更佳。这些结果与第四章C1节的发现一致。


<!-- Media -->



<!-- figureText: Multi-View Images GT (Centerline) TopoNet TopoNet + SEPT GT (Lane Segment) LaneSegNet LaneSegNet + SEPT -->



<img src="https://cdn.noedgeai.com/bo_d3vjg1jef24c73d2mnk0_6.jpg?x=128&y=141&w=1550&h=1692&r=0"/>



Fig. 4. Qualitative comparisons between the baselines with and without our SEPT module on the OLV2 validation split. From left to right, the figure presents the multi-view images, the ground truth (GT) for centerline perception and topology, the results of the baseline (TopoNet) with and without SEPT, the GT for lane segment perception and topology, and the results of the baseline (LaneSegNet) with and without SEPT. The green line indicates the corresponding SD map prior.
图4. 在OLV2验证集上，带有和不带SEPT模块的基线模型的定性比较。图中从左至右依次展示多视角图像、车道中心线感知与拓扑的真实标注（GT）、带有和不带SEPT的基线（TopoNet）结果、车道段感知与拓扑的真实标注（GT）以及带有和不带SEPT的基线（LaneSegNet）结果。绿色线条表示对应的SD地图先验。


<!-- Media -->



## D. Qualitative Results
## D. 定性结果


We present visualizations from the OLV2 validation split to demonstrate the improvements brought by our proposed SEPT framework over two baseline models: TopoNet [2] for lane centerline perception and LaneSegNet [6] for lane segment perception. As illustrated in Fig. 4, we highlight several representative cases. In the first row, we demonstrate that in long-range scenarios, the baseline models either fail to detect or inaccurately predict the left turn (highlighted in purple boxes), whereas our model successfully identifies it. In the second row, due to occlusion from a front vehicle, our approach notably enhances the prediction of the occluded road structure at the intersection. The third row illustrates how effectively incorporating the SD map improves lane topology reasoning. The final row presents an interesting case where the SD map provides outdated information, resulting in incorrect prior knowledge. However, our SEPT framework demonstrates the ability to prioritize online perception, yielding a correct prediction that aligns with current observations. This shows that our model strikes an effective balance between onboard sensing and SD map priors. Overall, our SEPT framework significantly improves both scene perception and topology reasoning. More qualitative results can be found in our supplementary video.
我们展示了OLV2验证集上的可视化结果，以证明所提SEPT框架相较于两个基线模型的改进效果：TopoNet [2]用于车道中心线感知，LaneSegNet [6]用于车道段感知。如图4所示，我们突出展示了若干典型案例。第一行展示了在远距离场景中，基线模型要么未能检测到左转（紫色框标注），要么预测不准确，而我们的模型成功识别。第二行中，由于前车遮挡，我们的方法显著提升了交叉口处被遮挡路况的预测。第三行展示了有效融合SD地图如何改善车道拓扑推理。最后一行呈现了一个有趣案例，SD地图提供了过时信息，导致先验知识错误，但我们的SEPT框架能够优先考虑在线感知，给出与当前观测一致的正确预测。这表明我们的模型在车载感知与SD地图先验之间实现了有效平衡。总体而言，SEPT框架显著提升了场景感知和拓扑推理能力。更多定性结果可见补充视频。


## V. CONCLUSION
## V. 结论


In this letter, we present SEPT, a novel framework that integrates SD map priors into existing perception and reasoning models to enhance online scene understanding. SEPT effectively mitigates the misalignment issue in both rasterized and vectorized SD map representations and leverages the DGFF module to fuse these features for synergistic improvement. To further exploit the potential of SD maps, we introduce the auxiliary IKPD task, which enhances the model in capturing road interaction patterns. We apply our SEPT to two baseline methods and validate it on the large-scale OLV2 benchmark. The significant performance gains demonstrate the superiority of our framework. Future work will focus on incorporating additional SD map prior information, such as lane numbers and road directions, to further enhance scene perception and topology reasoning for mapless driving. REFERENCES
本文提出了SEPT，一种将SD地图先验整合进现有感知与推理模型以增强在线场景理解的新框架。SEPT有效缓解了栅格化和矢量化SD地图表示中的错位问题，并利用DGFF模块融合这些特征，实现协同提升。为进一步挖掘SD地图潜力，我们引入了辅助IKPD任务，增强模型捕捉道路交互模式的能力。我们将SEPT应用于两个基线方法，并在大规模OLV2基准上验证，显著的性能提升证明了框架的优越性。未来工作将聚焦于引入更多SD地图先验信息，如车道数量和道路方向，以进一步提升无地图驾驶的场景感知与拓扑推理能力。参考文献


[1] J. Li, P. Jia, J. Chen, J. Liu, and L. He, "Local map construction methods with sd map: A novel survey," arXiv preprint arXiv:2409.02415, 2024.
[1] J. Li, P. Jia, J. Chen, J. Liu, 和 L. He, “基于SD地图的局部地图构建方法：一项新颖综述,” arXiv预印本 arXiv:2409.02415, 2024.


[2] T. Li, L. Chen, H. Wang, Y. Li, J. Yang, X. Geng, S. Jiang, Y. Wang, H. Xu, C. Xu et al., "Graph-based topology reasoning for driving scenes," arXiv preprint arXiv:2304.05277, 2023.
[2] T. Li, L. Chen, H. Wang, Y. Li, J. Yang, X. Geng, S. Jiang, Y. Wang, H. Xu, C. Xu 等，“基于图的驾驶场景拓扑推理”，arXiv 预印本 arXiv:2304.05277，2023年。


[3] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang et al., "Planning-oriented autonomous driving," in CVPR, 2023, pp. 17853-17862.
[3] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang 等，“面向规划的自动驾驶”，发表于 CVPR，2023年，第17853-17862页。


[4] B. Jiang, S. Chen, Q. Xu, B. Liao, J. Chen, H. Zhou, Q. Zhang, W. Liu, C. Huang, and X. Wang, "Vad: Vectorized scene representation for efficient autonomous driving," in Proc. of the IEEE Intl. Conf. Comput. Vis. (ICCV), 2023, pp. 8340-8350.
[4] B. Jiang, S. Chen, Q. Xu, B. Liao, J. Chen, H. Zhou, Q. Zhang, W. Liu, C. Huang, 和 X. Wang，“VAD：用于高效自动驾驶的矢量化场景表示”，发表于 IEEE 国际计算机视觉会议（ICCV）论文集，2023年，第8340-8350页。


[5] H. Wang, T. Li, Y. Li, L. Chen, C. Sima, Z. Liu, B. Wang, P. Jia, Y. Wang, S. Jiang et al., "Openlane-v2: A topology reasoning benchmark for unified 3d hd mapping," Advances in Neural Information Processing Systems, vol. 36, 2024.
[5] H. Wang, T. Li, Y. Li, L. Chen, C. Sima, Z. Liu, B. Wang, P. Jia, Y. Wang, S. Jiang 等，“OpenLane-v2：统一三维高清地图的拓扑推理基准”，《神经信息处理系统进展》（Advances in Neural Information Processing Systems），第36卷，2024年。


[6] T. Li, P. Jia, B. Wang, L. Chen, K. Jiang, J. Yan, and H. Li, "Lanesegnet: Map learning with lane segment perception for autonomous driving," arXiv preprint arXiv:2312.16108, 2023.
[6] T. Li, P. Jia, B. Wang, L. Chen, K. Jiang, J. Yan, 和 H. Li，“LaneSegNet：基于车道段感知的地图学习用于自动驾驶”，arXiv 预印本 arXiv:2312.16108，2023年。


[7] T. Ort, J. M. Walls, S. A. Parkison, I. Gilitschenski, and D. Rus, "Maplite 2.0: Online hd map inference using a prior sd map," IEEE Robotics and Automation Letters, vol. 7, no. 3, pp. 8355-8362, 2022.
[7] T. Ort, J. M. Walls, S. A. Parkison, I. Gilitschenski, 和 D. Rus，“MapLite 2.0：利用先验低精度地图的在线高清地图推断”，《IEEE机器人与自动化快报》，第7卷第3期，第8355-8362页，2022年。


[8] Z. Jiang, Z. Zhu, P. Li, H.-a. Gao, T. Yuan, Y. Shi, H. Zhao, and H. Zhao, "P-mapnet: Far-seeing map generator enhanced by both sdmap and hdmap priors," IEEE Robotics and Automation Letters, vol. 9, pp. 8539-8546, 2024.
[8] Z. Jiang, Z. Zhu, P. Li, H.-a. Gao, T. Yuan, Y. Shi, H. Zhao, 和 H. Zhao，“P-MapNet：结合低精度地图和高清地图先验的远视地图生成器”，《IEEE机器人与自动化快报》，第9卷，第8539-8546页，2024年。


[9] H. Zhang, D. Paz, y. Guo, A. Das, X. Huang, K. Haug, H. Christensen, and L. Ren, "Enhancing online road network perception and reasoning with standard definition maps," in Proc. of the IEEE/RSJ Intl. Conf. on Intell. Robots Syst.(IROS), 2024.
[9] H. Zhang, D. Paz, Y. Guo, A. Das, X. Huang, K. Haug, H. Christensen, 和 L. Ren，“利用低精度地图增强在线道路网络感知与推理”，发表于 IEEE/RSJ 国际智能机器人系统会议（IROS），2024年。


[10] K. Z. Luo, X. Weng, Y. Wang, S. Wu, J. Li, K. Q. Weinberger, Y. Wang, and M. Pavone, "Augmenting lane perception and topology understanding with standard definition navigation maps," in Proc. of the IEEE Intl. Conf. on Robot. and Autom. (ICRA), 2024, pp. 4029-4035.
[10] K. Z. Luo, X. Weng, Y. Wang, S. Wu, J. Li, K. Q. Weinberger, Y. Wang, 和 M. Pavone，“利用低精度导航地图增强车道感知与拓扑理解”，发表于 IEEE 国际机器人与自动化会议（ICRA），2024年，第4029-4035页。


[11] S. Yang, M. Jiang, Z. Fan, X. Xie, X. Tan, Y. Li, E. Ding, L. Wang, and J. Wang, "Toposd: Topology-enhanced lane segment perception with sdmap prior," arXiv preprint arXiv:2411.14751, 2024.
[11] S. Yang, M. Jiang, Z. Fan, X. Xie, X. Tan, Y. Li, E. Ding, L. Wang, 和 J. Wang，“TopoSD：结合低精度地图先验的拓扑增强车道段感知”，arXiv 预印本 arXiv:2411.14751，2024年。


[12] H. Wu, Z. Zhang, S. Lin, X. Mu, Q. Zhao, M. Yang, and T. Qin, "Maplocnet: Coarse-to-fine feature registration for visual re-localization in navigation maps," in Proc. of the IEEE/RSJ Intl. Conf. on Intell. Robots Syst.(IROS). IEEE, 2024, pp. 13 198-13 205.
[12] H. Wu, Z. Zhang, S. Lin, X. Mu, Q. Zhao, M. Yang, 和 T. Qin，“MapLocNet：用于导航地图视觉重定位的粗到细特征配准”，发表于 IEEE/RSJ 国际智能机器人系统会议（IROS），IEEE，2024年，第13198-13205页。


[13] A. Vaswani, "Attention is all you need," Advances in Neural Information Processing Systems, 2017.
[13] A. Vaswani，“Attention is all you need（注意力机制即一切）”，《神经信息处理系统进展》，2017年。


[14] B. Liao, S. Chen, X. Wang et al., "Maptr: Structured modeling and learning for online vectorized hd map construction," in International Conference on Learning Representations, 2023.
[14] B. Liao, S. Chen, X. Wang 等，“MapTR：用于在线矢量化高清地图构建的结构化建模与学习”，发表于国际学习表征会议，2023年。


[15] Q. Li, Y. Wang, Y. Wang, and H. Zhao, "Hdmapnet: An online hd map construction and evaluation framework," in Proc. of the IEEE Intl. Conf. on Robot. and Autom. (ICRA), 2022, pp. 4628-4634.
[15] Q. Li, Y. Wang, Y. Wang, 和 H. Zhao, “Hdmapnet：一个在线高清地图构建与评估框架，”发表于IEEE国际机器人与自动化会议（ICRA），2022年，第4628-4634页。


[16] Z. Xu, Y. Liu, Y. Sun, M. Liu, and L. Wang, "Centerlinedet: Centerline graph detection for road lanes with vehicle-mounted sensors by transformer for hd map generation," in Proc. of the IEEE Intl. Conf. on Robot. and Autom. (ICRA), 2023, pp. 3553-3559.
[16] Z. Xu, Y. Liu, Y. Sun, M. Liu, 和 L. Wang, “Centerlinedet：基于变换器的车载传感器道路车道中心线图检测，用于高清地图生成，”发表于IEEE国际机器人与自动化会议（ICRA），2023年，第3553-3559页。


[17] T. Langenberg, T. Lüddecke, and F. Wörgötter, "Deep metadata fusion for traffic light to lane assignment," IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. 973-980, 2019.
[17] T. Langenberg, T. Lüddecke, 和 F. Wörgötter, “用于交通灯到车道分配的深度元数据融合，”IEEE机器人与自动化快报，卷4，第2期，2019年，第973-980页。


[18] Y. Liu, T. Wang, X. Zhang, and J. Sun, "Petr: Position embedding transformation for multi-view 3d object detection," in European Conference on Computer Vision. Springer, 2022, pp. 531-548.
[18] Y. Liu, T. Wang, X. Zhang, 和 J. Sun, “Petr：多视角三维目标检测的位置嵌入变换，”发表于欧洲计算机视觉会议，Springer出版社，2022年，第531-548页。


[19] M. Liang, B. Yang, R. Hu, Y. Chen, R. Liao, S. Feng, and R. Urtasun, "Learning lane graph representations for motion forecasting," in ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16. Springer, 2020, pp. 541-556.
[19] M. Liang, B. Yang, R. Hu, Y. Chen, R. Liao, S. Feng, 和 R. Urtasun, “用于运动预测的车道图表示学习，”发表于2020年第16届欧洲计算机视觉会议（ECCV），英国格拉斯哥，2020年8月23-28日，会议论文集第二部分，Springer出版社，2020年，第541-556页。


[20] H. Liu, L. Chen, Y. Qiao, C. Lv, and H. Li, "Reasoning multi-agent behavioral topology for interactive autonomous driving," in Annual Conference on Neural Information Processing Systems, 2024.
[20] H. Liu, L. Chen, Y. Qiao, C. Lv, 和 H. Li, “多智能体行为拓扑推理用于交互式自动驾驶，”发表于神经信息处理系统年会，2024年。


[21] Y. B. Can, A. Liniger, D. P. Paudel, and L. Van Gool, "Structured bird's-eye-view traffic scene understanding from onboard images," in Proc. of the IEEE Intl. Conf. Comput. Vis. (ICCV), 2021, pp. 15661-15670.
[21] Y. B. Can, A. Liniger, D. P. Paudel, 和 L. Van Gool, “基于车载图像的结构化鸟瞰交通场景理解，”发表于IEEE国际计算机视觉会议（ICCV），2021年，第15661-15670页。


[22] D. Wu, J. Chang, F. Jia, Y. Liu, T. Wang, and J. Shen, "Topomlp: A simple yet strong pipeline for driving topology reasoning," in The Twelfth International Conference on Learning Representations, 2024.
[22] D. Wu, J. Chang, F. Jia, Y. Liu, T. Wang, 和 J. Shen, “Topomlp：一个简单而强大的驾驶拓扑推理流程，”发表于第十二届国际表征学习会议，2024年。


[23] Y. Fu, W. Liao, X. Liu, H. Xu, Y. Ma, Y. Zhang, and F. Dai, "Topologic: An interpretable pipeline for lane topology reasoning on driving scenes," Advances in Neural Information Processing Systems, vol. 37, pp. 61658-61676, 2024.
[23] Y. Fu, W. Liao, X. Liu, H. Xu, Y. Ma, Y. Zhang, 和 F. Dai, “Topologic：一个可解释的驾驶场景车道拓扑推理流程，”神经信息处理系统进展，卷37，2024年，第61658-61676页。


[24] Z. Ma, S. Liang, Y. Wen, W. Lu, and G. Wan, "Roadpainter: Points are ideal navigators for topology transformer," in European Conference on Computer Vision. Springer, 2025, pp. 179-195.
[24] Z. Ma, S. Liang, Y. Wen, W. Lu, 和 G. Wan, “Roadpainter：点是拓扑变换器的理想导航者，”发表于欧洲计算机视觉会议，Springer出版社，2025年，第179-195页。


[25] H. Wu, Z. Zhang, S. Lin, T. Qin, J. Pan, Q. Zhao, C. Xu, and M. Yang, "Blos-bev: Navigation map enhanced lane segmentation network, beyond line of sight," in 2024 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2024, pp. 3212-3219.
[25] H. Wu, Z. Zhang, S. Lin, T. Qin, J. Pan, Q. Zhao, C. Xu, 和 M. Yang, “Blos-bev：导航地图增强的车道分割网络，超越视线范围，”发表于2024年IEEE智能车辆研讨会（IV），IEEE，2024年，第3212-3219页。


[26] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, "Pointnet: Deep learning on point sets for $3\mathrm{\;d}$ classification and segmentation," in CVPR,2017,pp. 652-660.
[26] C. R. Qi, H. Su, K. Mo, 和 L. J. Guibas, “Pointnet：基于点集的深度学习用于分类和分割，”发表于CVPR，2017年，第652-660页。


[27] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville, "Film: Visual reasoning with a general conditioning layer," in Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, 2018.
[27] E. Perez, F. Strub, H. De Vries, V. Dumoulin, 和 A. Courville, “Film：具有通用条件层的视觉推理，”发表于AAAI人工智能会议论文集，卷32，第1期，2018年。


[28] H. Law and J. Deng, "Cornernet: Detecting objects as paired keypoints," in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 734-750.
[28] H. Law 和 J. Deng, “Cornernet：将目标检测为成对关键点，”发表于欧洲计算机视觉会议（ECCV），2018年，第734-750页。


[29] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian, "Centernet: Keypoint triplets for object detection," in CVPR, 2019, pp. 6569-6578.
[29] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, 和 Q. Tian, “Centernet：用于目标检测的关键点三元组，”发表于CVPR，2019年，第6569-6578页。


[30] F. Chollet, "Xception: Deep learning with depthwise separable convolutions," in CVPR, 2017, pp. 1251-1258.
[30] F. Chollet, “Xception：基于深度可分离卷积的深度学习，”发表于CVPR，2017年，第1251-1258页。


[31] J. Hu, L. Shen, and G. Sun, "Squeeze-and-excitation networks," in CVPR, 2018, pp. 7132-7141.
[31] J. Hu, L. Shen, 和 G. Sun，“挤压与激励网络（Squeeze-and-excitation networks），”发表于CVPR，2018年，第7132-7141页。


[32] B. Wilson, W. Qi, T. Agarwal et al., "Argoverse 2: Next generation datasets for self-driving perception and forecasting," arXiv preprint arXiv:2301.00493, 2023.
[32] B. Wilson, W. Qi, T. Agarwal 等，“Argoverse 2：面向自动驾驶感知与预测的下一代数据集，”arXiv预印本 arXiv:2301.00493，2023年。


[33] M. Haklay and P. Weber, "Openstreetmap: User-generated street maps," IEEE Pervasive computing, vol. 7, no. 4, pp. 12-18, 2008.
[33] M. Haklay 和 P. Weber，“OpenStreetMap：用户生成的街道地图，”IEEE普适计算，卷7，第4期，第12-18页，2008年。