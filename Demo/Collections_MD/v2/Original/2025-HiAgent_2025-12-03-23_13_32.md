# HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model
# HiAgent：用于解决长时程代理任务的分层工作记忆管理，基于大型语言模型


Mengkang Hu* Tianxing Chen ${}^{\spadesuit }$ Qiguang Chen ${}^{\heartsuit }$ Yao Mu*
胡梦康* 陈天行 ${}^{\spadesuit }$ 陈其光 ${}^{\heartsuit }$ 穆尧*


Wenqi Shao ${}^{\diamondsuit }$ Ping Luo ${}^{\spadesuit }$ *
邵文琦 ${}^{\diamondsuit }$ 罗平 ${}^{\spadesuit }$ *


The University of Hong Kong ${}^{\heartsuit }$ Harbin Institution of Technology
香港大学 ${}^{\heartsuit }$ 哈尔滨工业大学


♢ Shanghai Artificial Intelligence Laboratory
♢ 上海人工智能实验室


https://github.com/HiAgent2024/HiAgent



## Abstract
## 摘要


Large Language Model (LLM)-based agents exhibit significant potential across various domains, operating as interactive systems that process environmental observations to generate executable actions for target tasks. The effectiveness of these agents is significantly influenced by their memory mechanism, which records historical experiences as sequences of action-observation pairs. We categorize memory into two types: cross-trial memory, accumulated across multiple attempts, and in-trial memory (working memory), accumulated within a single attempt. While considerable research has optimized performance through cross-trial memory, the enhancement of agent performance through improved working memory utilization remains underexplored. Instead, existing approaches often involve directly inputting entire historical action-observation pairs into LLMs, leading to redundancy in long-horizon tasks. Inspired by human problem-solving strategies, this paper introduces HIAGENT, a framework that leverages subgoals as memory chunks to manage the working memory of LLM-based agents hierarchically. Specifically, HIAGENT prompts LLMs to formulate subgoals before generating executable actions and enables LLMs to decide proactively to replace previous subgoals with summarized observations, retaining only the action-observation pairs relevant to the current subgoal. Experimental results across five long-horizon tasks demonstrate that HIAGENT achieves a twofold increase in success rate and reduces the average number of steps required by 3.8. Additionally, our analysis shows that HIAGENT consistently improves performance across various steps, highlighting its robustness and generalizability.
基于大型语言模型（LLM）的代理在多个领域表现出显著潜力，作为交互系统处理环境观测并生成可执行动作以完成目标任务。这些代理的效果在很大程度上受其记忆机制影响，记忆以动作-观测对序列记录历史经验。我们将记忆分为两类：跨试记忆（在多次尝试中累计）和试内记忆（工作记忆，在单次尝试中累计）。尽管大量研究通过跨试记忆优化性能，但通过改进工作记忆利用来提升代理性能的探索仍然不足。现有方法常直接将整个历史动作-观测对输入到LLM，导致在长时程任务中产生冗余。受人类解决问题策略启发，本文提出 HIAGENT 框架，利用子目标作为记忆块分层管理基于LLM代理的工作记忆。具体地，HIAGENT 促使LLM在生成可执行动作前先制定子目标，并允许LLM主动决定用摘要观测替换先前子目标，仅保留与当前子目标相关的动作-观测对。五个长时程任务的实验结果表明，HIAGENT 将成功率提高了两倍，平均所需步骤减少了3.8步。此外，我们的分析显示 HIAGENT 在不同步数下均能持续提升性能，凸显其鲁棒性和泛化能力。


## 1 Introduction
## 1 引言


Owing to the development of powerful reasoning capabilities of Large Language Models (LLMs) in recent years (OpenAI, 2022, 2023; Meta AI, 2024; Touvron et al., 2023; Jiang et al., 2023), LLM-based agents have demonstrated significant potential in various applications (Xie et al., 2023; Wang et al., 2024; Xi et al., 2023), such as software development (Hong et al., 2023; Bairi et al., 2024), robotic planning (Yao et al., 2022b; Puig et al., 2018; Singh et al., 2023; Huang et al., 2022a), simulating human behavior (Park et al., 2023), etc. Typically, an LLM-based agent refers to an interactive system that processes environmental observations, maintains context across multiple rounds of dialogue, and outputs executable actions tailored to completing a given task. Memory is one of the critical components of LLM-based agents, involving how agents store and utilize past experiences. When handling a specific task, an agent's memory can be divided into cross-trial and in-trial memory (also as known as working memory). Cross-trial memory typically consists of the historical trajectory information accumulated across multiple attempts at the current task. In contrast, in-trial memory pertains to the information relevant to the current trial. While many papers have explored leveraging cross-trial memory to optimize agent performance (Shinn et al., 2024; Zhao et al., 2024; Guo et al., 2023), few have investigated ways to better utilize working memory. Existing LLM-based agent literature primarily employs the STANDARD strategy illustrated in Figure 1, where all action-observation pairs in working memory are directly incorporated into the context when prompting LLMs (Liu et al., 2023c; Ma et al., 2024; Yao et al., 2022b). Although this approach transmits the historical information to the LLM as comprehensively as possible, it encounters issues in long-horizon agent tasks. Such tasks typically require the agent to perform numerous actions to complete the task, resulting in an extensive working memory. This lengthy working memory creates a redundant context, hindering LLMs from maintaining coherent strategies and making accurate predictions over extended periods.
近年来大型语言模型（LLM）推理能力的快速发展（OpenAI, 2022, 2023；Meta AI, 2024；Touvron 等, 2023；Jiang 等, 2023），使得基于LLM的代理在多种应用中展现出巨大潜力（Xie 等, 2023；Wang 等, 2024；Xi 等, 2023），如软件开发（Hong 等, 2023；Bairi 等, 2024）、机器人规划（Yao 等, 2022b；Puig 等, 2018；Singh 等, 2023；Huang 等, 2022a）、模拟人类行为（Park 等, 2023）等。通常，基于LLM的代理指的是处理环境观测、在多轮对话中维护上下文并输出可执行动作以完成给定任务的交互系统。记忆是LLM代理的关键组成部分之一，涉及代理如何存储与利用过去经验。在处理特定任务时，代理的记忆可分为跨试记忆和试内记忆（即工作记忆）。跨试记忆通常由多次尝试中累积的历史轨迹信息构成，而试内记忆则与当前试次相关的信息有关。尽管许多工作探索了利用跨试记忆来优化代理性能（Shinn 等, 2024；Zhao 等, 2024；Guo 等, 2023），但关于如何更好地利用工作记忆的研究很少。现有基于LLM的代理文献主要采用图1所示的标准策略 STANDARD，将工作记忆中所有动作-观测对在提示LLM时直接并入上下文（Liu 等, 2023c；Ma 等, 2024；Yao 等, 2022b）。虽然该方法尽可能全面地将历史信息传递给LLM，但在长时程代理任务中遇到问题。这类任务通常要求代理执行大量动作以完成任务，导致工作记忆庞大。冗长的工作记忆产生冗余上下文，妨碍LLM在较长时间尺度上维持一致策略并做出准确预测。


<img src="https://cdn.noedgeai.com/bo_d4nfq3f7aajc73frs8h0_0.jpg?x=849&y=602&w=607&h=565&r=0"/>



Figure 1: Top right: A commonly adopted paradigm STANDARD for LLM-based agents includes: i) prompts LLMs to generate one action; ii) executes the generated action and then append the obtained observation to the LLM's context (working memory); and iii) generates the next action. Bottom: Instead of incorporating all historical action-observation pairs into the working memory, HIAGENT leverage subgoals as memory chunks, with a summarized observation as the observation for each memory chunk. HIAGENT achieves an average success rate improvement of twofold (42 vs. 21) across five long-horizon tasks.
图1：右上：常用范式 STANDARD 用于基于LLM的代理，包括：i) 提示LLM生成一个动作；ii) 执行该动作并将获得的观测追加到LLM的上下文（工作记忆）；iii) 生成下一个动作。下方：HIAGENT 不将所有历史动作-观测对并入工作记忆，而是将子目标作为记忆块，每个记忆块的观测以摘要观测表示。HIAGENT 在五个长时程任务上取得了平均成功率两倍的提升（42 vs. 21）。


---



*Corresponding Author (pluo.lhi@gmail.com).
*通信作者 (pluo.lhi@gmail.com)。


---



Drawing on principles of cognitive science (Newell et al., 1972; Anderson, 2013), humans typically decompose a complex problem into multiple subproblems, addressing each individually. Each subproblem is treated as a memory "chunk," thereby reducing the cognitive load on working memory (Miller, 1956). By focusing on the results of completed subproblems rather than their detailed execution, humans effectively manage cognitive resources and improve their efficiency in solving complex, long-horizon tasks. Inspired by human cognition and problem-solving strategies, we propose a sophisticated hierarchical working memory management framework HIAGENT tailored for long-horizon agent tasks. The core idea of HI-AGENT is to trigger LLMs to generate subgoals, with each subgoal serving as a chunk of the working memory. Specifically, as shown in Figure 2, we first prompt the LLM to generate a subgoal, then create actions to achieve the subgoal and store the corresponding action-observation pairs in a memory chunk. Once the subgoal is completed, we summarize the memory chunk and append the subgoal-observation pair to the working memory. In a word, HIAGENT triggers LLMs to proactively decide to replace previous subgoals with summarized observations while retaining only the action-observation pairs relevant to the current subgoal. To provide more flexible working memory management, we also introduce a trajectory retrieval module, which can retrieve the detailed trajectory information of specific past subgoals when necessary.
借鉴认知科学原理（Newell 等，1972；Anderson，2013），人类通常将复杂问题分解为多个子问题，逐个解决。每个子问题被视为一个记忆“块”，从而减轻工作记忆的负担（Miller，1956）。通过关注已完成子问题的结果而非其详细执行，人类有效地管理认知资源并提高在复杂、长时程任务中的效率。受人类认知与问题解决策略启发，我们提出了面向长时程代理任务的精细分层工作记忆管理框架 HIAGENT。HI-AGENT 的核心思想是触发 LLM 生成子目标，将每个子目标作为工作记忆的一个块。具体地，如图 2 所示，我们首先提示 LLM 生成子目标，然后为实现该子目标创建动作并将相应的动作-观测对存入记忆块。子目标完成后，我们对记忆块进行摘要并将子目标-观测对追加到工作记忆。简言之，HIAGENT 触发 LLM 主动决定用摘要观测替换先前子目标，同时只保留与当前子目标相关的动作-观测对。为提供更灵活的工作记忆管理，我们还引入了轨迹检索模块，在必要时可检索特定过去子目标的详细轨迹信息。


To validate the effectiveness and efficiency of HIAGENT, we conduct experiments on five long-horizon agent tasks from AgentBoard (Ma et al., 2024). The experimental results show that the success rate of HIAGENT is twice that of the STANDARD strategy, and it exceeds the STANDARD strategy by 23.94% in progress rate. Additionally, HI-AGENT is more efficient than STANDARD strategy, reducing the average number of steps to complete tasks by 3.8, the context length by 35.02%, and the run time by 19.42%. Furthermore, to demonstrate that redundant context impairs the performance of LLM-based agents in long-horizon tasks, we compare HIAGENT to a method that generates subgoals without disregarding the detailed trajectory information of past subgoals. Experimental results show that HIAGENT improves the success rate by ${20}\%$ while reducing both runtime and the number of steps. By analyzing model performance across varying step counts, we find that HIAGENT not only consistently outperform STANDARD on progress rate but also show a higher likelihood of generating executable actions as the number of steps increased.
为验证 HIAGENT 的有效性与效率，我们在 AgentBoard（Ma 等，2024）中的五个长时程代理任务上进行了实验。实验结果显示，HIAGENT 的成功率为 STANDARD 策略的两倍，且在进展率上比 STANDARD 高出 23.94%。此外，HI-AGENT 比 STANDARD 策略更高效，平均完成任务所需步数减少 3.8 步，上下文长度缩短 35.02%，运行时间减少 19.42%。进一步地，为证明冗余上下文会削弱基于 LLM 的代理在长时程任务中的表现，我们将 HIAGENT 与一种在生成子目标时不舍弃过去子目标详细轨迹信息的方法进行了对比。实验结果表明，HIAGENT 在降低运行时间和步数的同时，将成功率提高了 ${20}\%$。通过分析不同步数下的模型表现，我们发现 HIAGENT 不仅在进展率上始终优于 STANDARD，而且随着步数增加，更有可能生成可执行动作。


## 2 Preliminary
## 2 预备知识


### 2.1 Large Language Model based Agent
### 2.1 基于大型语言模型的代理


Large Language Model (LLM) based agents are intelligent autonomous systems designed to perform complex tasks. These tasks can be formalized as a partially observable Markov decision process (POMDP), characterized by the tuple $\left( {S,O,A,T,R}\right)$ ,where: $S$ denotes the state space; $O$ represents the observation space; $A$ signifies the action space; $T : S \times  A \rightarrow  S$ embodies the transition function; $R : S \times  A \rightarrow  R$ encapsulates the reward function; An LLM-based agent operates as a policy $\pi \left( {{a}_{t} \mid  I,{o}_{t},{a}_{t - 1},{o}_{t - 1},\ldots ,{a}_{0},{o}_{0}}\right)$ ,which, given the historical action-observation pairs and instructions $I$ (encompassing in-context examples, environmental descriptions, etc.), generates an executable action ${a}_{t} \in  A$ . Each action precipitates a new state ${s}_{t + 1} \in  S$ and a subsequent observation ${o}_{t + 1} \in  O$ . This iterative interaction persists until either task completion or the agent reaches a predetermined maximum number of steps.
基于大型语言模型（LLM）的代理是用于执行复杂任务的智能自治系统。这些任务可形式化为部分可观测马尔可夫决策过程（POMDP），由元组 $\left( {S,O,A,T,R}\right)$ 表示，其中：$S$ 表示状态空间；$O$ 表示观测空间；$A$ 表示动作空间；$T : S \times  A \rightarrow  S$ 代表转移函数；$R : S \times  A \rightarrow  R$ 包含奖励函数；基于 LLM 的代理作为策略 $\pi \left( {{a}_{t} \mid  I,{o}_{t},{a}_{t - 1},{o}_{t - 1},\ldots ,{a}_{0},{o}_{0}}\right)$ 运行，基于历史动作-观测对和指令 $I$（包括上下文示例、环境描述等）生成可执行动作 ${a}_{t} \in  A$。每个动作导致新的状态 ${s}_{t + 1} \in  S$ 和随后观测 ${o}_{t + 1} \in  O$。此迭代交互持续直到任务完成或代理达到预设最大步数。


<img src="https://cdn.noedgeai.com/bo_d4nfq3f7aajc73frs8h0_2.jpg?x=195&y=191&w=597&h=528&r=0"/>



Figure 2: An overview of the process of HIAGENT.
图 2：HIAGENT 流程概览。


### 2.2 Working Memory
### 2.2 工作记忆


From the cognitive science perspective, working memory enables individuals to hold and manipulate information in real-time, facilitating complex cognitive tasks such as reasoning, comprehension, and learning (Newell et al., 1972; Anderson, 2013). In LLM-based agents, we define working memory as the essential historical information required by the LLM at a given moment $t$ to complete the current task. Effective working memory management allows for better integrating past experiences and current stimuli, leading to more informed and accurate decisions. It can be likened to the human process of attentional control and cognitive updating, which involves selectively focusing on relevant information, filtering out distractions, and continually updating the mental workspace with new and pertinent data. The STANDARD approach in Figure 1 stores all historical action-observation pairs in working memory, i.e., ${m}_{t}^{std} = \left( {{o}_{t},{a}_{t - 1},{o}_{t - 1},\ldots ,{a}_{0},{o}_{0}}\right)$ . Although this provides the LLM with comprehensive information, it also introduces redundancy, complicating the LLM's processing.
从认知科学的角度来看，工作记忆使个体能够实时存储和处理信息，有助于完成推理、理解和学习等复杂的认知任务（Newell 等人，1972 年；Anderson，2013 年）。在基于大语言模型（LLM）的智能体中，我们将工作记忆定义为大语言模型在给定时刻 $t$ 完成当前任务所需的重要历史信息。有效的工作记忆管理有助于更好地整合过去的经验和当前的刺激，从而做出更明智、准确的决策。这类似于人类的注意力控制和认知更新过程，即有选择地关注相关信息，过滤干扰因素，并不断用新的相关数据更新心理工作空间。图 1 中的 STANDARD 方法将所有历史动作 - 观察对存储在工作记忆中，即 ${m}_{t}^{std} = \left( {{o}_{t},{a}_{t - 1},{o}_{t - 1},\ldots ,{a}_{0},{o}_{0}}\right)$。虽然这为大语言模型提供了全面的信息，但也引入了冗余，使大语言模型的处理变得复杂。


## 3 Methodology
## 3 方法


### 3.1 Overview
### 3.1 概述


The core idea of HIAGENT is to employ subgoals for hierarchical management of working memory. More specifically, as is shown in Figure 2, the process of HIAGENT can be described as follows: (1) Before generating specific grounded actions, we prompt the LLM to first formulate a subgoal ${g}_{i}$ . Each subgoal serves as a milestone within the overall task. (2) Subsequently, the LLM generates precise actions to accomplish this subgoal. (3) Upon the LLM's determination that a particular subgoal has been fulfilled, we synthesize the corresponding action-observation pairs into a summarized observation ${s}_{i}$ (§3.3). We then obscure the action-observation pairs within the context,substituting them with ${s}_{i}$ . Consequently,the working memory of HIAGENT can be formalized as ${m}_{t} = \left( {{g}_{0},{s}_{0},\ldots ,{g}_{n - 1},{s}_{n - 1},{g}_{n},{a}_{n0},{o}_{n1},\ldots }\right)$ . (4) Additionally, we have incorporated a retrieval module to facilitate more flexible memory management(§3.4). For instance,if the ${q}_{th}$ subgoal is retrieved, we input the detailed action-observation pairs into the context rather than the summarized observation,i.e., ${m}_{t}^{\prime } = \; \left( {{g}_{0},{s}_{0},\ldots ,{g}_{q},{a}_{q0},{a}_{q0},\ldots ,{g}_{n},{a}_{n0},{o}_{n0},\ldots }\right)$ .
HIAGENT 的核心思想是采用子目标对工作记忆进行分层管理。更具体地说，如图 2 所示，HIAGENT 的过程可以描述如下：（1）在生成具体的实际动作之前，我们先促使大语言模型制定一个子目标 ${g}_{i}$。每个子目标都是整个任务中的一个里程碑。（2）随后，大语言模型生成精确的动作以实现该子目标。（3）当大语言模型确定某个子目标已完成时，我们将相应的动作 - 观察对合成为一个总结性观察 ${s}_{i}$（§3.3）。然后，我们在上下文中模糊动作 - 观察对，用 ${s}_{i}$ 替换它们。因此，HIAGENT 的工作记忆可以形式化为 ${m}_{t} = \left( {{g}_{0},{s}_{0},\ldots ,{g}_{n - 1},{s}_{n - 1},{g}_{n},{a}_{n0},{o}_{n1},\ldots }\right)$。（4）此外，我们还加入了一个检索模块，以实现更灵活的记忆管理（§3.4）。例如，如果检索到 ${q}_{th}$ 子目标，我们将详细的动作 - 观察对输入到上下文中，而不是总结性观察，即 ${m}_{t}^{\prime } = \; \left( {{g}_{0},{s}_{0},\ldots ,{g}_{q},{a}_{q0},{a}_{q0},\ldots ,{g}_{n},{a}_{n0},{o}_{n0},\ldots }\right)$。


### 3.2 Subgoal-based Hierarchical Working Memory
### 3.2 基于子目标的分层工作记忆


As is shown in Figure 2, at each time step, the LLM can either generate the next action for the current subgoal or generate a new subgoal when it determines that the existing subgoal has been accomplished. For the current subgoal, the agent retains all action-observation pairs, providing a detailed context for immediate decision-making. For past subgoals, only a summarized version of the observations is kept. This subgoal-based hierarchical management approach in HIAGENT is deeply motivated by cognitive science principles, drawing parallels with human cognition and problem-solving strategies (Newell et al., 1972; Anderson, 2013). Employing subgoals to compartmentalize action-observation pairs can be conceptualized as a form of chunking methodology. In human cognition, chunking allows individuals to group related information into meaningful units, thereby overcoming working memory limitations (Miller, 1956). Similarly, HIAGENT utilizes subgoals as cognitive chunks, encapsulating related actions and observations. This chunking mechanism enables the system to handle complex sequences of information more effectively, reducing cognitive load and enhancing overall performance. Furthermore, by generating subgoals before specific actions, the system mimics the human tendency to break down larger objectives into more manageable components. This methodology enhances computational efficiency and aligns with established theories of human information processing.
如图 2 所示，在每个时间步，大语言模型可以为当前子目标生成下一个动作，或者在确定现有子目标已完成时生成一个新的子目标。对于当前子目标，智能体保留所有动作 - 观察对，为即时决策提供详细的上下文。对于过去的子目标，只保留观察的总结版本。HIAGENT 中这种基于子目标的分层管理方法深受认知科学原理的启发，与人类的认知和解决问题的策略相似（Newell 等人，1972 年；Anderson，2013 年）。采用子目标对动作 - 观察对进行划分可以被视为一种组块方法。在人类认知中，组块使个体能够将相关信息组合成有意义的单元，从而克服工作记忆的限制（Miller，1956 年）。同样，HIAGENT 将子目标用作认知组块，封装相关的动作和观察。这种组块机制使系统能够更有效地处理复杂的信息序列，减轻认知负担，提高整体性能。此外，通过在具体动作之前生成子目标，该系统模仿了人类将较大目标分解为更易管理的组件的倾向。这种方法提高了计算效率，符合已有的人类信息处理理论。


### 3.3 Observation Summarization
### 3.3 观察总结


The process of observation summarization can be formalized as ${s}_{i} = S\left( {{g}_{i},{o}_{0},{a}_{0},\ldots ,{o}_{t}}\right)$ ,where $S$ can be implemented using either a Large Language Model (LLM) or alternative text summarization models. This function encapsulates the synthesis of historical observations and actions, contextual-ized by the current subgoal, to produce a concise representation of the agent's state. Furthermore, a crucial component of the summarized observation is assessing whether the current subgoal has been achieved. This evaluation serves as a pivotal guide for future subgoal generation, facilitating adaptive and goal-oriented behavior in the agent's decision-making process. By doing so, the agent can maintain a condensed yet informative context, balancing the need for historical information with efficiency. The example prompt can be found in Appendix ??
观察摘要的过程可以形式化为 ${s}_{i} = S\left( {{g}_{i},{o}_{0},{a}_{0},\ldots ,{o}_{t}}\right)$，其中 $S$ 可以由大型语言模型（LLM）或其他文本摘要模型实现。该函数封装了历史观测与动作的综合，并以当前子目标为上下文化，生成对代理状态的简明表示。此外，摘要观测的关键组成部分是评估当前子目标是否已达成。该评估为未来子目标生成提供关键指引，促进代理决策过程中的自适应与目标导向行为。通过此方式，代理能够保持简洁而富信息的上下文，在历史信息与效率之间取得平衡。示例提示见附录 ??


### 3.4 Trajectory Retrieval
### 3.4 轨迹检索


Despite the summarization, there may be instances where detailed past trajectory information becomes crucial for immediate decision-making. For instance, when a past subgoal execution fails, we need detailed trajectory information to determine the cause of failure. Moreover, reviewing past successful experiences can also increase the likelihood of success when facing novel challenges and scenarios. To address this, we introduce a trajectory retrieval module. When the LLM determines that detailed information from a past subgoal is necessary, it generates a retrieval function to recall the complete action-observation pairs for that subgoal, analogous to the way to generate actions. This selective retrieval allows the agent to access detailed historical data on demand without consistently carrying the full context.
尽管进行了摘要，但在某些情况下，详细的过去轨迹信息对即时决策仍至关重要。例如，当过去某个子目标执行失败时，我们需要详细的轨迹信息来判断失败原因。此外，回顾以往的成功经验也能提升在新挑战和场景中取得成功的概率。为此，我们引入了轨迹检索模块。当 LLM 判定需要某个过去子目标的详细信息时，它会生成一个检索函数以调出该子目标的完整动作-观测对，类似于生成动作的方式。这种选择性检索允许代理按需访问详细的历史数据，而无需始终携带完整上下文。


## 4 Experiments
## 4 实验


### 4.1 Experimental Setup
### 4.1 实验设置


Evaluation Tasks We conduct the experiments on five long-horizon agent tasks, which typically require more than 20 steps: (i) Blocksworld requires the model to arrange the blocks into a specified target configuration by executing a series of moves; (ii) Gripper involves moving objects between different rooms; (iii) Tyreworld simulates changing a car tire, including removing the flat tire, replacing it with a spare, and installing the new tire; (iv) Barman emulates a bartender's tasks in mixing cocktails, including combining various ingredients, shakers, and garnishing drinks; (v) Jericho (Hausknecht et al., 2020) is a suite of text-based adventure game environments designed to evaluate agents' ability to navigate and interact with fictional worlds. More details can be found in Appendix A.
评估任务 我们在五个长程任务上开展实验，这些任务通常需要超过20步：（i）Blocksworld 要求模型通过一系列移动将方块摆成指定目标配置；（ii）Gripper 涉及在不同房间间移动物体；（iii）Tyreworld 模拟更换汽车轮胎，包括拆除瘪胎、换上备胎并安装新轮胎；（iv）Barman 模拟调酒师的任务，混合各种配料、摇杯并装饰饮品；（v）Jericho (Hausknecht et al., 2020) 是一套基于文本的冒险游戏环境，用于评估代理在虚构世界中导航与交互的能力。更多细节见附录 A。


Evaluation Metrics We use multiple metrics to evaluate both the effectiveness and efficiency of LLM-based agents in solving long-horizon tasks: (i) Progress Rate (Ma et al., 2024) evaluates the advancement toward task completion. Specifically, a task consists of multiple goal conditions, and the progress rate is the proportion of goal conditions fulfilled by the model out of the total number of goal conditions. (ii) Success Rate measures the percentage of successful task completions. The success rate is 1 when the progress rate is 1. (iii) Average Steps counts the steps taken to complete the task; (iv) Context Efficiency is defined as the mean number of tokens in the in-trial context across all steps required to complete a given task. (v) Run Time evaluates the time required to complete tasks.
评估指标 我们使用多种指标评估基于 LLM 的代理在解决长程任务时的有效性与效率：（i）进展率（Ma et al., 2024）评估任务完成的推进情况。具体而言，任务由多个目标条件组成，进展率为模型满足的目标条件数占总目标条件数的比例。（ii）成功率衡量任务成功完成的百分比。当进展率为 1 时，成功率为 1。（iii）平均步数统计完成任务所需的步数；（iv）上下文效率定义为完成给定任务所需步骤中试内上下文的平均 token 数；（v）运行时间评估完成任务所需的时间。


Baselines STANDARD prompting strategy is a predominantly used method in current LLM-based agent literature (Yao et al., 2022b; Ma et al., 2024; Liu et al., 2023c). It operates by taking one action followed by one observation, providing a comparative baseline for evaluating the performance of HIAGENT.
基线 STANDARD 提示策略是当前基于 LLM 的代理文献中广泛使用的方法（Yao et al., 2022b; Ma et al., 2024; Liu et al., 2023c）。它通过执行“一步动作后跟一步观测”的方式运行，为评估 HIAGENT 的性能提供对照基线。


Implementation Details The implementation of evaluation tasks is based on AgentBoard (Ma et al., 2024). We set a maximum step limit of 30 for task configuration and provide one in-context example for each task. We employ GPT-4 (gpt-4-turbo) ${}^{1}$ as the LLM backbone for our experiments, serving both as the agent policy and the observation summarization model. We set the temperature hy-perparameter for LLM inference to 0 and topp to 1. Detailed prompt examples are provided in the Appendix B.
实现细节 评估任务的实现基于 AgentBoard (Ma et al., 2024)。我们将任务的最大步数限制设置为 30，并为每个任务提供一个上下文示例。我们在实验中采用 GPT-4 (gpt-4-turbo) ${}^{1}$ 作为 LLM 骨干，既作为代理策略又作为观测摘要模型。我们将 LLM 推理的温度超参数设为 0，topp 设为 1。详细的提示示例见附录 B。


### 4.2 Main Results
### 4.2 主要结果


As shown in Table 1, HIAGENT demonstrated substantial advancements over STANDARD. Overall, in terms of effectiveness, it increased the success rate by 21% and the progress rate by 23.94%. Regarding task execution efficiency, it reduced the average number of steps to completion by 3.8 , decreased the number of context tokens consumed by ${35}\%$ ,and reduced the run time by 19.42%. Furthermore, in certain tasks (blocksworld, barman, jericho), HIA-GENT even achieved more than double the progress rate improvement while maintaining efficiency. In tyreworld, the model not only achieved a 50% improvement in success rate but also reduced the average number of steps by 9.4. Although the progress rate slightly decreased by ${1.5}\%$ in the gripper task, context token usage was reduced by over ${50}\%$ .
如表 1 所示，HIAGENT 相较于 STANDARD 有显著进步。总体而言，在有效性方面，其成功率提高了 21%，进度率提高了 23.94%。在任务执行效率方面，它将完成任务的平均步骤数减少了 3.8，消耗的上下文令牌数减少了 ${35}\%$，运行时间减少了 19.42%。此外，在某些任务（积木世界、酒保、杰里科）中，HIA - GENT 在保持效率的同时，进度率提升甚至超过两倍。在轮胎世界任务中，该模型不仅成功率提高了 50%，还将平均步骤数减少了 9.4。虽然在抓爪任务中进度率略有下降（下降了 ${1.5}\%$），但上下文令牌使用量减少了超过 ${50}\%$。


---



${}^{1}$ We utilized the model via OpenAI API service.
${}^{1}$ 我们通过 OpenAI API 服务使用该模型。


---



Table 1: Performance of STANDARD and HIAGENT on 5 long-horizon agent tasks. We report on four metrics: Success Rate (SR), Progress Rate (PR), Average Steps (Steps), and Context Efficiency (Context), Run Time (Time). The symbol $\uparrow$ indicates that a higher value for the metric is preferable,while $\downarrow$ signifies that a lower value is considered better. In the Overall section, the result is obtained by averaging the values of a certain metric across various tasks.
表 1：STANDARD 和 HIAGENT 在 5 个长周期智能体任务上的表现。我们报告了四个指标：成功率（SR）、进度率（PR）、平均步骤数（Steps）、上下文效率（Context）和运行时间（Time）。符号 $\uparrow$ 表示该指标值越高越好，而 $\downarrow$ 表示该指标值越低越好。在“总体”部分，结果是通过对各项任务中某一指标的值求平均得到的。


<table><tr><td></td><td>SR $\uparrow$</td><td>PR↑</td><td>Steps $\downarrow$</td><td>Context $\downarrow$</td><td>Time $\downarrow$</td></tr><tr><td colspan="6">Blocksworld</td></tr><tr><td>STANDARD</td><td>30.00</td><td>35.00</td><td>25.00</td><td>100%</td><td>100%</td></tr><tr><td>HIAGENT</td><td>60.00 +30.00</td><td>80.00 +45.00</td><td>18.60 -6.40</td><td>67.46% -32.54%</td><td>63.47% -36.53%</td></tr><tr><td colspan="6">Gripper</td></tr><tr><td>STANDARD</td><td>50.00 +0.00</td><td>87.75</td><td>25.20</td><td>100%</td><td>100%</td></tr><tr><td>HIAGENT</td><td>50.00 +0.00</td><td>86.25 -1.50</td><td>24.80 -0.40</td><td>49.99% -50.01%</td><td>70.46% -29.54%</td></tr><tr><td colspan="6">Tyreworld</td></tr><tr><td>STANDARD</td><td>10.00</td><td>39.28</td><td>28.40</td><td>100%</td><td>100%</td></tr><tr><td>HIAGENT</td><td>60.00 +50.00</td><td>75.83 +36.55</td><td>19.00 -9.4</td><td>73.58% -26.42%</td><td>77.58% -22.42%</td></tr><tr><td colspan="6">Barman</td></tr><tr><td>STANDARD</td><td>10.00</td><td>17.50</td><td>26.85</td><td>100%</td><td>100%</td></tr><tr><td>HIAGENT</td><td>30.00 +20.00</td><td>40.83 +23.33</td><td>24.5 -2.35</td><td>67.02% -32.98%</td><td>95.54% -4.46%</td></tr><tr><td colspan="6">Jericho</td></tr><tr><td>STANDARD</td><td>5.00</td><td>13.51</td><td>26.60</td><td>100%</td><td>100%</td></tr><tr><td>HIAGENT</td><td>10.00 +5.00</td><td>29.85 +16.34</td><td>26.15 -0.45</td><td>66.86% -33.14%</td><td>95.85% -4.15%</td></tr><tr><td colspan="6">Overall</td></tr><tr><td>STANDARD</td><td>21.00</td><td>38.61</td><td>26.41</td><td>100%</td><td>100%</td></tr><tr><td>HIAGENT</td><td>42.00 +21.00</td><td>62.55+23.94</td><td>22.61 -3.80</td><td>64.98% -35.02%</td><td>80.58% -19.42%</td></tr></table>
<table><tbody><tr><td></td><td>SR $\uparrow$</td><td>PR↑</td><td>步骤 $\downarrow$</td><td>上下文 $\downarrow$</td><td>时间 $\downarrow$</td></tr><tr><td colspan="6">方块世界</td></tr><tr><td>标准</td><td>30.00</td><td>35.00</td><td>25.00</td><td>100%</td><td>100%</td></tr><tr><td>高智能体</td><td>60.00 +30.00</td><td>80.00 +45.00</td><td>18.60 -6.40</td><td>67.46% -32.54%</td><td>63.47% -36.53%</td></tr><tr><td colspan="6">夹持器</td></tr><tr><td>标准</td><td>50.00 +0.00</td><td>87.75</td><td>25.20</td><td>100%</td><td>100%</td></tr><tr><td>高智能体</td><td>50.00 +0.00</td><td>86.25 -1.50</td><td>24.80 -0.40</td><td>49.99% -50.01%</td><td>70.46% -29.54%</td></tr><tr><td colspan="6">轮胎世界</td></tr><tr><td>标准</td><td>10.00</td><td>39.28</td><td>28.40</td><td>100%</td><td>100%</td></tr><tr><td>高智能体</td><td>60.00 +50.00</td><td>75.83 +36.55</td><td>19.00 -9.4</td><td>73.58% -26.42%</td><td>77.58% -22.42%</td></tr><tr><td colspan="6">酒保</td></tr><tr><td>标准</td><td>10.00</td><td>17.50</td><td>26.85</td><td>100%</td><td>100%</td></tr><tr><td>高智能体</td><td>30.00 +20.00</td><td>40.83 +23.33</td><td>24.5 -2.35</td><td>67.02% -32.98%</td><td>95.54% -4.46%</td></tr><tr><td colspan="6">杰里科</td></tr><tr><td>标准</td><td>5.00</td><td>13.51</td><td>26.60</td><td>100%</td><td>100%</td></tr><tr><td>高智能体</td><td>10.00 +5.00</td><td>29.85 +16.34</td><td>26.15 -0.45</td><td>66.86% -33.14%</td><td>95.85% -4.15%</td></tr><tr><td colspan="6">总体</td></tr><tr><td>标准</td><td>21.00</td><td>38.61</td><td>26.41</td><td>100%</td><td>100%</td></tr><tr><td>高智能体</td><td>42.00 +21.00</td><td>62.55+23.94</td><td>22.61 -3.80</td><td>64.98% -35.02%</td><td>80.58% -19.42%</td></tr></tbody></table>


We can draw several conclusions from previous discussions:
我们可以从前述讨论中得出若干结论：


(1) HIAGENT is more effective than STANDARD, achieving huge improvements on both success rate and progress rate.
(1) HIAGENT 比 STANDARD 更有效，在成功率和进度率上均有巨大提升。


(2) HIAGENT is also more efficient than STANDARD, requiring fewer steps to complete tasks, utilizing shorter context lengths, and achieving faster runtime.
(2) HIAGENT 也比 STANDARD 更高效，完成任务所需步数更少，使用更短的上下文长度，运行更快。


## 5 Analysis
## 5 分析


To gain deeper insights into our approach, we explored the following research questions:
为深入理解我们的方法，我们探讨了以下研究问题：


(1) Are all modules effective for HIAGENT?
(1) 所有模块对 HIAGENT 都有效吗？


(2) Is HIAGENT consistently superior to the baseline at different steps?
(2) HIAGENT 在不同步骤上是否始终优于基线？


(3) Is improvement of HIAGENT solely derived from task decomposition?
(3) HIAGENT 的改进是否完全来自任务分解？


(4) How effective are the frameworks in generating executable actions?
(4) 这些框架在生成可执行动作方面有多有效？


(5) Are the observed performance improvements in HIAGENT statistically significant compared to STANDARD?
(5) 与 STANDARD 相比，HIAGENT 所观察到的性能提升是否具有统计显著性？


### 5.1 Answer 1: All Modules in HIAGENT are Effective for HIAGENT
### 5.1 回答 1：HIAGENT 中的所有模块对 HIAGENT 都有效


In this section, we conducted an ablation study to explore whether Observation Summarization and Trajectory Retrieval are effective.
在本节中，我们进行了消融研究以探究观察摘要与轨迹检索是否有效。


Observation Summarization is effective. We heuristically use the observation corresponding to the last action as the summarized observation when removing the Observation Summarization module. As is shown in Table 2 ("w/o OS"), there is a significant decline in performance across all metrics. Specifically, the success rate and progress rate were significantly impacted, decreasing by 30% and 7.6%, respectively. It indicates that the observation summarization module can comprehensively aggregate the detailed information within a trajectory, thereby aiding the reasoning of an LLM-based agent.
观察摘要是有效的。当移除观察摘要模块时，我们启发式地使用与最后一个动作对应的观测作为摘要观测。正如表 2（“w/o OS”）所示，各项指标均显著下降。具体而言，成功率和进度率分别显著下降了 30% 和 7.6%。这表明观察摘要模块能够全面汇聚轨迹中的详细信息，从而辅助基于大模型的代理进行推理。


Trajectory Retrieval is also crucial for performance enhancement. We hide all the detailed trajectory information of previous subgoals at each time step to verify the effectiveness of Trajectory Retrieval. According to the results in Table 2 ("w/o TR"), the success rate decreased by 10%, and the average steps increased by 1.2. This is because, while trajectory retrieval lengthens the reasoning steps of the LLM, it allows the agent to flexibly retrieve past trajectories under certain subgoals, which is more beneficial for identifying errors in previous actions.
轨迹检索对性能提升也至关重要。我们在每个时间步隐藏了先前子目标的所有详细轨迹信息以验证轨迹检索的有效性。根据表 2（“w/o TR”）的结果，成功率下降了 10%，平均步数增加了 1.2。这是因为尽管轨迹检索延长了大模型的推理步骤，它允许代理在特定子目标下灵活检索过去的轨迹，这更有利于识别之前动作中的错误。


The combination of Observation Summarization and Trajectory Retrieval yields significant improvement. We conducted an experiment where both modules were removed to validate the functionality and effectiveness of the combined Observation Summarization and Trajectory Retrieval modules. As shown in Table 2 ("w/o OS & TR"), there is a noticeable performance decline compared to HIAGENT, with the success rate decreasing by 20%. This decline is also evident when compared to the individual ablations of the Observation Summarization and Trajectory Retrieval modules, highlighting a substantial reduction in progress rate in their absence.
观察摘要与轨迹检索的组合带来显著改进。我们进行了同时移除两个模块的实验，以验证这两个模块组合的功能与有效性。如表 2（“w/o OS & TR”）所示，与 HIAGENT 相比性能明显下降，成功率降低了 20%。相较于单独移除观察摘要或轨迹检索的消融实验，这一下降也很明显，凸显出在它们缺失时进度率的大幅降低。


### 5.2 Answer 2: HIAGENT is consistently superior to STANDARD at different steps
### 5.2 回答 2：HIAGENT 在不同步骤上始终优于 STANDARD


To conduct a more granular study of HIAGENT's performance, we present the progress rate at different step counts (in intervals of 5 steps) in Figure 3. The experimental results indicate that overall, HIAGENT consistently achieves a higher progress rate at each step than STANDARD (f). Additionally, it is noteworthy that HIAGENT benefits more from an increased number of steps, whereas STANDARD does not. For example, in the blocksworld task (a) and barman task (b), STANDARD shows no progress rate increase between steps 15-25, whereas HIAGENT exhibits continuous growth. This further demonstrates HIAGENT's advantage in handling long-horizon agent tasks.
为了更细致地研究 HIAGENT 的性能，我们在图 3 中展示了不同步数（以 5 步为间隔）下的进度率。实验结果表明，总体而言，HIAGENT 在每一步的进度率始终高于 STANDARD (f)。此外，值得注意的是，HIAGENT 从步数增加中获益更多，而 STANDARD 则不然。例如，在积木世界任务 (a) 和酒保任务 (b) 中，STANDARD 在 15 - 25 步之间进度率没有增加，而 HIAGENT 则持续增长。这进一步证明了 HIAGENT 在处理长视野智能体任务方面的优势。


### 5.3 Answer 3: The improvement in H1AGENT is not solely attributed to task decomposition
### 5.3 回答 3：H1AGENT 的改进并非仅仅归因于任务分解


Using LLMs to generate subgoals has been employed in numerous studies and has demonstrated considerable performance advantages (Zhou et al., 2022; Yin et al., 2023). Therefore, a pertinent question arises: "Is the performance improvement attributed to HIAGENT merely related to task decomposition, rather than efficient working memory management?" To address this question, we implemented a new method that prompts the LLM to generate a subgoal before generating executable actions, followed by generating actions to achieve this subgoal. Unlike HIAGENT, this approach does not obscure the detailed trajectory information of previous subgoals. The experimental results, detailed in Table 3, indicate that although task decomposition can lead to a performance improvement $({30}\%$ in success rate), the success rate is still 20% lower than HIAGENT. Additionally, solely using task decomposition introduces inefficiencies, increasing runtime by 5.7% and context length by 12.8%. In summary, HIAGENT is more efficient and effective than task decomposition alone.
使用大语言模型（LLMs）生成子目标已在众多研究中得到应用，并展现出显著的性能优势（Zhou 等人，2022；Yin 等人，2023）。因此，一个相关问题随之而来：“HIAGENT 的性能提升仅仅与任务分解有关，而与高效的工作记忆管理无关吗？”为了解决这个问题，我们实施了一种新方法，即先提示大语言模型生成一个子目标，然后再生成实现该子目标的可执行动作。与 HIAGENT 不同，这种方法不会掩盖先前子目标的详细轨迹信息。表 3 详细记录的实验结果表明，尽管任务分解可以提高性能（成功率 $({30}\%$），但成功率仍比 HIAGENT 低 20%。此外，仅使用任务分解会导致效率低下，运行时间增加 5.7%，上下文长度增加 12.8%。总之，HIAGENT 比单纯的任务分解更高效、更有效。


### 5.4 Answer 4: HIAGENT is effective in generating executable actions even under long steps
### 5.4 回答 4：HIAGENT 即使在长步数情况下也能有效生成可执行动作


LLM-based agents sometimes generate actions that cannot be executed, such as attempting to retrieve objects from a closed container. This is typically due to LLMs' poor reasoning abilities. To investigate this, we calculated the proportion of executable actions generated by the model at each timestep, referred to as executability. As shown in Figure 4, HIAGENT is more likely to generate executable actions than STANDARD, further demonstrating the effectiveness of HIAGENT. Additionally, we observed that STANDARD is more prone to generating non-executable actions when the steps are longer (e.g., in the blocksworld, when the steps exceed 20, executability drops below 10%). This is because, as the working memory increases, the ability of LLMs to generate executable actions decreases. In contrast, HIAGENT maintains over 80% executability even with longer steps, indicating that the robustness to long steps is a key factor in the strong performance on long-horizon tasks.
基于大语言模型的智能体有时会生成无法执行的动作，例如试图从封闭容器中取出物体。这通常是由于大语言模型的推理能力较差所致。为了研究这一问题，我们计算了模型在每个时间步生成的可执行动作的比例，即可执行性。如图 4 所示，HIAGENT 比 STANDARD 更有可能生成可执行动作，进一步证明了 HIAGENT 的有效性。此外，我们观察到，当步数较长时，STANDARD 更容易生成不可执行的动作（例如，在积木世界任务中，当步数超过 20 时，可执行性降至 10%以下）。这是因为，随着工作记忆的增加，大语言模型生成可执行动作的能力会下降。相比之下，即使在长步数情况下，HIAGENT 的可执行性仍能保持在 80%以上，这表明对长步数的鲁棒性是其在长视野任务中表现出色的关键因素。


### 5.5 Answer 5: The observed performance improvements in HIAGENT are statistically significant compared to STANDARD
### 5.5 回答 5：与 STANDARD 相比，HIAGENT 观察到的性能提升具有统计学意义


To validate the statistical significance of the improvements in both effectiveness and efficiency, we selected the Progress Rate and Average Steps metrics for analysis. We employed the Wilcoxon signed-rank test (Woolson, 2005) for this purpose due to its suitability for comparing paired samples. This non-parametric test helps assess whether the observed differences are likely due to chance or represent a genuine effect. The results of our analysis are as follows: (i) For the Progress Rate, the test statistic is 144.0 with a p-value of ${2.38} \times  {10}^{-5}$ , indicating a statistically significant difference between HIAGENT and STANDARD; (ii) For the Average Steps, the test statistic is 112.5 with a p-value of 0.0016, also demonstrating a statistically significant difference. These results confirm that the observed improvements in both effectiveness and efficiency are not due to random variation, underscoring the superiority of HIAGENT.
为了验证有效性和效率提升的统计学意义，我们选择了进度率和平均步数指标进行分析。为此，我们采用了 Wilcoxon 符号秩检验（Woolson，2005），因为它适用于比较配对样本。这种非参数检验有助于评估观察到的差异是由于偶然因素还是真实效应造成的。我们的分析结果如下：(i) 对于进度率，检验统计量为 144.0，p 值为 ${2.38} \times  {10}^{-5}$，表明 HIAGENT 和 STANDARD 之间存在统计学上的显著差异；(ii) 对于平均步数，检验统计量为 112.5，p 值为 0.0016，也表明存在统计学上的显著差异。这些结果证实，观察到的有效性和效率提升并非随机变化所致，突显了 HIAGENT 的优越性。


Table 2: Ablation study of HIAGENT on tyreworld. "w/o OS" refers to removing the Observation Summarization module introduced by Section 3.3. "w/o TR" refers to removing the Trajectory Retrieval module introduced by Section 3.4. "w/o TR & OS" refers to removing both modules.
表 2：HIAGENT 在轮胎世界任务上的消融研究。“w/o OS” 指移除第 3.3 节介绍的观察总结模块。“w/o TR” 指移除第 3.4 节介绍的轨迹检索模块。“w/o TR & OS” 指同时移除这两个模块。


<table><tr><td>Model</td><td>SR $\uparrow$</td><td>PR $\uparrow$</td><td>Steps $\downarrow$</td><td>Context $\downarrow$</td><td>Time $\downarrow$</td></tr><tr><td>HIAGENT</td><td>60.0</td><td>75.8</td><td>19.0</td><td>100.0%</td><td>100.0%</td></tr><tr><td>w/o OS</td><td>30.0 -30.0</td><td>68.2 -7.6</td><td>24.2 +5.2</td><td>110.8% +10.8%</td><td>122.5% +22.5%</td></tr><tr><td>w/o TR</td><td>50.0 -10.0</td><td>76.9 +1.1</td><td>21.2 +2.2</td><td>105.0% +5.0%</td><td>107.5% +7.5%</td></tr><tr><td>w/o OS & TR</td><td>30.0 -30.0</td><td>62.4 -13.4</td><td>26.2 +7.2</td><td>107.2% + 7.2%</td><td>121.2% +21.2%</td></tr></table>
<table><tbody><tr><td>模型</td><td>SR $\uparrow$</td><td>PR $\uparrow$</td><td>步数 $\downarrow$</td><td>上下文 $\downarrow$</td><td>时间 $\downarrow$</td></tr><tr><td>HIAGENT</td><td>60.0</td><td>75.8</td><td>19.0</td><td>100.0%</td><td>100.0%</td></tr><tr><td>无 OS</td><td>30.0 -30.0</td><td>68.2 -7.6</td><td>24.2 +5.2</td><td>110.8% +10.8%</td><td>122.5% +22.5%</td></tr><tr><td>无 TR</td><td>50.0 -10.0</td><td>76.9 +1.1</td><td>21.2 +2.2</td><td>105.0% +5.0%</td><td>107.5% +7.5%</td></tr><tr><td>无 OS & TR</td><td>30.0 -30.0</td><td>62.4 -13.4</td><td>26.2 +7.2</td><td>107.2% + 7.2%</td><td>121.2% +21.2%</td></tr></tbody></table>


<img src="https://cdn.noedgeai.com/bo_d4nfq3f7aajc73frs8h0_6.jpg?x=191&y=553&w=1282&h=560&r=0"/>



Figure 3: Progress rate at different steps.
图 3：不同步骤的进度率。


Figure 4: Executability of actions at different steps.
图 4：不同步骤的动作可执行性。


## 6 Related Work
## 6 相关工作


Large Language Model based-Agent. Large Language Models (LLMs) have revolutionized the field of language agents, endowing them with the prowess to tackle intricate challenges through a logical sequence of actions (Xie et al., 2023; Xi et al., 2023; Wang et al., 2024). A series of works explored various applications of LLM-based agents, such as code generation (Wang et al., 2023b; Lin et al., 2018), web browsing (Yao et al., 2022a; Zhou et al., 2023a; Pan et al., 2024; Li and Waldo, 2024), robotics (Shridhar et al., 2020; Mu et al., 2024a,b), tool use (Li et al., 2023b; Wu et al., 2024; Qin et al., 2023), reasoning (Yang et al., 2024; Chen et al., 2025), planning (Xie et al., 2024), conducting research (Kang and Xiong, 2024), chip design and more. In addition, a great deal of work has explored the application of LLM-based agents in the field of multi-agent systems (Hong et al., 2023; Zhang et al., 2023a; Wu et al., 2023; Li et al., 2023a; Chen et al., 2023).
基于大语言模型的智能体。大语言模型（LLMs）彻底改变了语言智能体领域，赋予它们通过一系列逻辑动作来应对复杂挑战的能力（Xie 等人，2023；Xi 等人，2023；Wang 等人，2024）。一系列研究探索了基于大语言模型的智能体的各种应用，如代码生成（Wang 等人，2023b；Lin 等人，2018）、网页浏览（Yao 等人，2022a；Zhou 等人，2023a；Pan 等人，2024；Li 和 Waldo，2024）、机器人技术（Shridhar 等人，2020；Mu 等人，2024a,b）、工具使用（Li 等人，2023b；Wu 等人，2024；Qin 等人，2023）、推理（Yang 等人，2024；Chen 等人，2025）、规划（Xie 等人，2024）、开展研究（Kang 和 Xiong，2024）、芯片设计等等。此外，大量研究探索了基于大语言模型的智能体在多智能体系统领域的应用（Hong 等人，2023；Zhang 等人，2023a；Wu 等人，2023；Li 等人，2023a；Chen 等人，2023）。


This paper introduces a working memory management framework HIAGENT that can be universally applied to enhance the performance of other agent frameworks. For example, ReAct (Yao et al., 2022b) introduces a method where the LLM generates a chain of thought (Wei et al., 2022) before generating actions, and the trajectory formed by the triplet of "(thought, action, observation)" can be managed using HIAGENT. Additionally, HIAGENT has the potential to alleviate information management challenges in multi-agent frameworks (Hong et al., 2023).
本文介绍了一种工作记忆管理框架 HIAGENT，它可以普遍应用于提升其他智能体框架的性能。例如，ReAct（Yao 等人，2022b）引入了一种方法，即大语言模型在生成动作之前先生成思维链（Wei 等人，2022），并且可以使用 HIAGENT 管理由“（思维、动作、观察）”三元组形成的轨迹。此外，HIAGENT 有潜力缓解多智能体框架中的信息管理挑战（Hong 等人，2023）。


Table 3: Experimental results on tyreworld. "w. TD" refers to Task Decomposition, i.e., having the LLM generate subgoals without concealing detailed trajectory information of previous subgoals.
表 3：轮胎世界的实验结果。“w. TD”指任务分解，即让大语言模型生成子目标，且不隐藏先前子目标的详细轨迹信息。


<table><tr><td>Model</td><td>SR↑</td><td>PR $\uparrow$</td><td>Steps $\downarrow$</td><td>Context $\downarrow$</td><td>Time $\downarrow$</td></tr><tr><td>STANDARD</td><td>10.0</td><td>39.3</td><td>28.4</td><td>100%</td><td>100%</td></tr><tr><td>w. TD</td><td>40.0 +30.0</td><td>67.4 +28.1</td><td>22.8 -5.6</td><td>112.8% +12.8%</td><td>105.7% +5.7%</td></tr><tr><td>W. HIAGENT</td><td>60.0 +50.0</td><td>75.8 +36.5</td><td>19.0 -9.4</td><td>73.6% -26.4%</td><td>77.6% -22.4%</td></tr></table>
<table><tbody><tr><td>模型</td><td>SR↑</td><td>PR $\uparrow$</td><td>步数 $\downarrow$</td><td>上下文 $\downarrow$</td><td>时间 $\downarrow$</td></tr><tr><td>标准</td><td>10.0</td><td>39.3</td><td>28.4</td><td>100%</td><td>100%</td></tr><tr><td>含 TD</td><td>40.0 +30.0</td><td>67.4 +28.1</td><td>22.8 -5.6</td><td>112.8% +12.8%</td><td>105.7% +5.7%</td></tr><tr><td>含 HIAGENT</td><td>60.0 +50.0</td><td>75.8 +36.5</td><td>19.0 -9.4</td><td>73.6% -26.4%</td><td>77.6% -22.4%</td></tr></tbody></table>


Planning. Planning is a cornerstone of human intelligence (Chen et al., 2024b, 2025), representing a systematic approach to achieving goals through a series of deliberate actions (Yao et al., 2024; Zhang et al., 2023b; Song et al., 2023; Huang et al., 2023, 2022b; Liu et al., 2023a; Hu et al., 2023b; Ruan et al., 2023; Aghzal et al., 2023; Hu et al., 2024). It involves breaking down complex tasks into manageable sub-tasks, searching for potential solutions, and achieving a desired goal. Least-to-most (Zhou et al., 2022) and Plan-and-solve (Wang et al., 2023a) propose decomposing a complex question into a series of sub-questions. Lumos (Yin et al., 2023) and XAgent (Team, 2023) introduce an independent planning module for generating subgoals and use full context in the grounding module to complete each subgoal.
规划。规划是人类智能的基石（Chen 等人，2024b，2025），它代表了一种通过一系列深思熟虑的行动来实现目标的系统方法（Yao 等人，2024；Zhang 等人，2023b；Song 等人，2023；Huang 等人，2023，2022b；Liu 等人，2023a；Hu 等人，2023b；Ruan 等人，2023；Aghzal 等人，2023；Hu 等人，2024）。它包括将复杂任务分解为可管理的子任务、寻找潜在解决方案并实现预期目标。最少到最多（Zhou 等人，2022）和规划与解决（Wang 等人，2023a）方法提出将一个复杂问题分解为一系列子问题。Lumos（Yin 等人，2023）和 XAgent（团队，2023）引入了一个独立的规划模块来生成子目标，并在接地模块中使用完整上下文来完成每个子目标。


HIAGENT distinguishes itself from the literature by not only utilizing planning to enhance task performance but also by using subgoals as memory chunks to manage working memory hierarchically. This approach brings context efficiency and surpasses methods that rely solely on planning, as discussed in Section 5.3.
HIAGENT 与现有文献的不同之处在于，它不仅利用规划来提高任务性能，还将子目标作为记忆块来分层管理工作记忆。如第 5.3 节所述，这种方法提高了上下文效率，超越了仅依赖规划的方法。


Memory. The memory module in LLM-based agents is analogous to the human memory system, which is responsible for encoding, storing, and retrieving information (Zhang et al., 2024). The memory modules are typically divided into long-term memory and short-term memory. Long-term memory can usually be stored in an external database, while short-term memory (also known as working memory) is typically used directly as the context input of LLMs. Most current research papers primarily focus on managing long-term memory (Alonso et al., 2024; Maharana et al., 2024; Chen et al., 2024a; Xiao et al., 2024; Yuan et al., 2023; Wang et al., 2023c; Majumder et al., 2023; Hu et al., 2023a; Hao et al., 2024; Tu et al., 2023; Liang et al., 2023; Kagaya et al., 2024). Pioneer works include Memorybank (Zhong et al., 2024), with its global-level summaries, has made significant strides in distilling conversations into coherent narratives. Other works, such as Think-in-memory (Liu et al., 2023b) and the Retroformer (Yao et al., 2023), incorporated summary modules to manage long-term memories. Unlike these works, our study investigates how optimizing the management of working memory can enhance agent performance. Another line of research involves modifying the structure of transformers to enable LLMs to process longer contexts, thereby extending their working memory capabilities (Zhou et al., 2023b; Chevalier et al., 2023; Bertsch et al., 2024; Ruoss et al., 2023; Belt-agy et al., 2020; An et al., 2023).
记忆。基于大语言模型（LLM）的智能体中的记忆模块类似于人类的记忆系统，负责信息的编码、存储和检索（Zhang 等人，2024）。记忆模块通常分为长期记忆和短期记忆。长期记忆通常可以存储在外部数据库中，而短期记忆（也称为工作记忆）通常直接用作大语言模型的上下文输入。目前大多数研究论文主要关注长期记忆的管理（Alonso 等人，2024；Maharana 等人，2024；Chen 等人，2024a；Xiao 等人，2024；Yuan 等人，2023；Wang 等人，2023c；Majumder 等人，2023；Hu 等人，2023a；Hao 等人，2024；Tu 等人，2023；Liang 等人，2023；Kagaya 等人，2024）。先驱性工作包括 Memorybank（Zhong 等人，2024），它具有全局级别的摘要，在将对话提炼成连贯的叙述方面取得了显著进展。其他工作，如内存思考（Liu 等人，2023b）和 Retroformer（Yao 等人，2023），纳入了摘要模块来管理长期记忆。与这些工作不同，我们的研究探讨了如何通过优化工作记忆的管理来提高智能体的性能。另一类研究涉及修改变压器的结构，使大语言模型能够处理更长的上下文，从而扩展其工作记忆能力（Zhou 等人，2023b；Chevalier 等人，2023；Bertsch 等人，2024；Ruoss 等人，2023；Belt - agy 等人，2020；An 等人，2023）。


However, existing research has identified that LLMs encounter attention loss issues with lengthy texts (Liu et al., 2024). Consequently, we believe that investigating more efficient management of working memory remains a valuable endeavor.
然而，现有研究发现，大语言模型在处理长文本时会出现注意力丧失的问题（Liu 等人，2024）。因此，我们认为研究更高效的工作记忆管理方法仍然是一项有价值的工作。


## 7 Conclusion
## 7 结论


This paper proposes HIAGENT, a flexible framework that utilizes subgoals to manage the working memory of LLM-based agents. Experimental results from five long-horizon agent tasks demonstrate that HIAGENT outperforms the baseline model across all tasks, with an overall success rate more than double that of the baseline model. Furthermore, HIAGENT is more efficient, accomplishing tasks with fewer steps, in less runtime, and using shorter context. In the future, we hope HIAGENT can inspire more creative ideas on effectively managing the working memory of LLM-based agents.
本文提出了 HIAGENT，这是一个灵活的框架，利用子目标来管理基于大语言模型的智能体的工作记忆。五项长期智能体任务的实验结果表明，HIAGENT 在所有任务上都优于基线模型，总体成功率是基线模型的两倍多。此外，HIAGENT 更高效，完成任务所需的步骤更少、运行时间更短，且使用的上下文更短。未来，我们希望 HIAGENT 能激发更多关于有效管理基于大语言模型的智能体工作记忆的创新想法。


## Limitation
## 局限性


While HIAGENT reduces redundant context, it may still face challenges in extremely long-horizon tasks where memory constraints persist. Future work could explore more advanced retrieval strategies to further optimize memory efficiency. Moreover, our experiments primarily focus on benchmark tasks; extending the evaluation to more diverse real-world applications would provide deeper insights into the generalizability of our method.
虽然 HIAGENT 减少了冗余上下文，但在内存限制仍然存在的极长期任务中，它可能仍会面临挑战。未来的工作可以探索更先进的检索策略，以进一步优化内存效率。此外，我们的实验主要集中在基准任务上；将评估扩展到更多样化的现实应用中，将有助于更深入地了解我们方法的泛化能力。


## References
## 参考文献


Mohamed Aghzal, Erion Plaku, and Ziyu Yao. 2023. Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning. arXiv preprint arXiv:2310.03249.
Mohamed Aghzal、Erion Plaku 和 Ziyu Yao。2023 年。大语言模型能成为优秀的路径规划器吗？关于时空推理的基准测试与研究。预印本 arXiv:2310.03249。


Nick Alonso, Tomás Figliolia, Anthony Ndirango, and Beren Millidge. 2024. Toward conversational agents with context and time sensitive long-term memory. arXiv preprint arXiv:2406.00057.
Nick Alonso、Tomás Figliolia、Anthony Ndirango 和 Beren Millidge。2024 年。迈向具有上下文和时间敏感长期记忆的对话式智能体。预印本 arXiv:2406.00057。


Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088.
陈欣安, 龚珊珊, 钟明, 李向海, 张军, 孔令鹏, 丘希鹏. 2023. L-eval：为长上下文语言模型建立标准化评估。arXiv 预印本 arXiv:2307.11088.


John R Anderson. 2013. The architecture of cognition. Psychology Press.
约翰·R·安德森. 2013. 认知的架构。Psychology Press.


Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok, and Shashank Shet. 2024. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675-698.
Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok, Shashank Shet. 2024. Codeplan：使用 llms 与规划进行仓库级代码生成。Proceedings of the ACM on Software Engineering, 1(FSE):675-698.


Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.
Iz Beltagy, Matthew E Peters, Arman Cohan. 2020. Longformer：长文档变换器。arXiv 预印本 arXiv:2004.05150.


Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew Gormley. 2024. Unlimiformer: Long-range transformers with unlimited length input. Advances in Neural Information Processing Systems, 36.
Amanda Bertsch, Uri Alon, Graham Neubig, Matthew Gormley. 2024. Unlimiformer：支持无限长度输入的长程变换器。Advances in Neural Information Processing Systems, 36.


Nuo Chen, Hongguang Li, Juhua Huang, Baoyuan Wang, and Jia Li. 2024a. Compress to impress: Unleashing the potential of compressive memory in real-world long-term conversations. arXiv preprint arXiv:2402.11975.
陈诺, 李宏光, 黄菊华, 王宝元, 李佳. 2024a. Compress to impress：释放压缩记忆在真实长期对话中的潜力。arXiv 预印本 arXiv:2402.11975.


Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025. Towards reasoning era: A survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567.
陈岐广, 秦立波, 刘金浩, 彭登云, 关建南, 王鹏, 胡梦康, 周玉航, 高特, 车万祥. 2025. 迈向推理时代：长链式思考在推理大型语言模型中的综述。arXiv 预印本 arXiv:2503.09567.


Qiguang Chen, Libo Qin, Jiaqi Wang, Jingxuan Zhou, and Wanxiang Che. 2024b. Unlocking the capabilities of thought: A reasoning boundary framework to quantify and optimize chain-of-thought. Advances in Neural Information Processing Systems, 37:54872- 54904.
陈岐广, 秦立波, 王佳琪, 周景轩, 车万祥. 2024b. 解锁思考能力：用于量化与优化链式思考的推理边界框架。Advances in Neural Information Processing Systems, 37:54872-54904.


Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. 2023. Agent-verse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848.
陈维泽, 苏玉晟, 左敬伟, 杨成, 袁晨飞, 钱晨, 陈敏章, 秦宇嘉, 鲁雅熙, 谢若冰, 等. 2023. Agent-verse：促进多智能体协作并探索智能体的涌现行为。arXiv 预印本 arXiv:2308.10848.


Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi Chen. 2023. 使语言模型适应压缩上下文。arXiv 预印本 arXiv:2305.14788.


Jing Guo, Nan Li, Jianchuan Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si Zhang, and Ming Xu. 2023. Empowering working memory for large language model agents. arXiv preprint arXiv:2312.17259.
郭静, 李南, 齐建川, 杨航, 李瑞乔, 冯玉珍, 张思, 徐明. 2023. 为大型语言模型代理增强工作记忆。arXiv 预印本 arXiv:2312.17259.


Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2024. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. Advances in neural information processing systems, 36.
郝世博, 刘天阳, 王震, 胡志庭. 2024. Toolkengpt：通过工具嵌入用大量工具增强冷冻语言模型。Advances in Neural Information Processing Systems, 36.


Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. 2020. Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7903-7910.
Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, 杨兴迪. 2020. 交互式小说游戏：一场巨大的冒险。In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7903-7910.


Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. 2023. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352.
洪思睿, 郑夏武, 陈乔纳森, 程宇衡, 王金林, 张策尧, 王子立, Yau Steven Ka Shing, 林子娟, 周立阳, 等. 2023. Metagpt：用于多智能体协作的元编程框架。arXiv 预印本 arXiv:2308.00352.


Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. 2023a. Chatdb: Augmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901.
胡晨旭, 傅杰, 杜陈庄, 罗思淳, 赵俊博, 赵航. 2023a. Chatdb：将数据库作为符号记忆来增强 llms。arXiv 预印本 arXiv:2306.03901.


Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, and Ping Luo. 2023b. Tree-planner: Efficient close-loop task planning with large language models. arXiv preprint arXiv:2310.08582.
胡梦康, 穆尧, 余欣淼, 丁明宇, 吴世光, 邵文琪, 陈岐广, 王斌, 乔宇, 罗平. 2023b. Tree-planner：使用大型语言模型的高效闭环任务规划。arXiv 预印本 arXiv:2310.08582.


Mengkang Hu, Pu Zhao, Can Xu, Qingfeng Sun, Jian-guang Lou, Qingwei Lin, Ping Luo, Saravan Rajmo-han, and Dongmei Zhang. 2024. Agentgen: Enhancing planning abilities for large language model based agent via environment and task generation. arXiv preprint arXiv:2408.00764.
Mengkang Hu, Pu Zhao, Can Xu, Qingfeng Sun, Jian-guang Lou, Qingwei Lin, Ping Luo, Saravan Rajmo-han, and Dongmei Zhang. 2024. Agentgen: Enhancing planning abilities for large language model based agent via environment and task generation. arXiv preprint arXiv:2408.00764.


Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022a. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International conference on machine learning, pages 9118-9147. PMLR.
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022a. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International conference on machine learning, pages 9118-9147. PMLR.


Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, and Brian Ichter. 2023. Grounded decoding: Guiding text generation with grounded models for robot control. Preprint, arXiv:2303.00855.
Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, and Brian Ichter. 2023. Grounded decoding: Guiding text generation with grounded models for robot control. Preprint, arXiv:2303.00855.


Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tomp-son, Igor Mordatch, Yevgen Chebotar, Pierre Ser-manet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. 2022b. Inner monologue: Embodied reasoning through planning with language models. Preprint, arXiv:2207.05608.
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tomp-son, Igor Mordatch, Yevgen Chebotar, Pierre Ser-manet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. 2022b. Inner monologue: Embodied reasoning through planning with language models. Preprint, arXiv:2207.05608.


Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.


Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. 2024. Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents. arXiv preprint arXiv:2402.03610.
Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. 2024. Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents. arXiv preprint arXiv:2402.03610.


Hao Kang and Chenyan Xiong. 2024. Researcharena: Benchmarking llms' ability to collect and organize information as research agents. arXiv preprint arXiv:2406.10291.
Hao Kang and Chenyan Xiong. 2024. Researcharena: Benchmarking llms' ability to collect and organize information as research agents. arXiv preprint arXiv:2406.10291.


Eric Li and Jim Waldo. 2024. Websuite: Systematically evaluating why web agents fail. arXiv preprint arXiv:2406.01623.
Eric Li and Jim Waldo. 2024. Websuite: Systematically evaluating why web agents fail. arXiv preprint arXiv:2406.01623.


Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023a. Camel: Communicative agents for" mind" exploration of large language model society. Advances in Neural Information Processing Systems, 36:51991-52008.
Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023a. Camel: Communicative agents for" mind" exploration of large language model society. Advances in Neural Information Processing Systems, 36:51991-52008.


Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023b. Api-bank: A comprehensive benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244.
Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023b. Api-bank: A comprehensive benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244.


Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. arXiv e-prints, pages arXiv-2304.
Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. arXiv e-prints, pages arXiv-2304.


Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D Ernst. 2018. N12bash: A corpus and semantic parser for natural language interface to the linux operating system. arXiv preprint arXiv:1802.08979.
Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D Ernst. 2018. N12bash: A corpus and semantic parser for natural language interface to the linux operating system. arXiv preprint arXiv:1802.08979.


Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023a. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477.
Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023a. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477.


Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. 2023b. Think-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint arXiv:2311.08719.
Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. 2023b. Think-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint arXiv:2311.08719.


Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157-173.
Nelson F Liu、Kevin Lin、John Hewitt、Ashwin Paran-jape、Michele Bevilacqua、Fabio Petroni 和 Percy Liang。2024。Lost in the middle: How language models use long contexts。Transactions of the Association for Computational Linguistics, 12:157-173。


Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xu-anyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kai-wen Men, Kejuan Yang, et al. 2023c. Agent-bench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688.
Xiao Liu、Hao Yu、Hanchen Zhang、Yifan Xu、Xu-anyu Lei、Hanyu Lai、Yu Gu、Hangliang Ding、Kai-wen Men、Kejuan Yang 等。2023c。Agent-bench: Evaluating llms as agents。arXiv 预印本 arXiv:2308.03688。


Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. 2024. Agentboard: An analytical evaluation board of multi-turn llm agents. arXiv preprint arXiv:2401.13178.
Chang Ma、Junlei Zhang、Zhihao Zhu、Cheng Yang、Yujiu Yang、Yaohui Jin、Zhenzhong Lan、Lingpeng Kong 和 Junxian He。2024。Agentboard: An analytical evaluation board of multi-turn llm agents。arXiv 预印本 arXiv:2401.13178。


Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753.
Adyasha Maharana、Dong-Ho Lee、Sergey Tulyakov、Mohit Bansal、Francesco Barbieri 和 Yuwei Fang。2024。Evaluating very long-term conversational memory of llm agents。arXiv 预印本 arXiv:2402.17753。


Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter Clark. 2023. Clin: A continually learning language agent for rapid task adaptation and generalization. arXiv preprint arXiv:2310.10134.
Bodhisattwa Prasad Majumder、Bhavana Dalvi Mishra、Peter Jansen、Oyvind Tafjord、Niket Tandon、Li Zhang、Chris Callison-Burch 和 Peter Clark。2023。Clin: A continually learning language agent for rapid task adaptation and generalization。arXiv 预印本 arXiv:2310.10134。


Meta AI. 2024. Introducing meta Llama 3: The most capable openly available LLM to date. Accessed: 2024-04-18.
Meta AI。2024。Introducing meta Llama 3: The most capable openly available LLM to date。访问日期：2024-04-18。


George A Miller. 1956. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological review, 63(2):81.
George A Miller。1956。The magical number seven, plus or minus two: Some limits on our capacity for processing information。Psychological Review, 63(2):81。


Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu, Chongjian Ge, Runjian Chen, Zhix-uan Liang, Mengkang Hu, Chaofan Tao, et al. 2024a. Robocodex: Multimodal code generation for robotic behavior synthesis. arXiv preprint arXiv:2402.16117.
Yao Mu、Junting Chen、Qinglong Zhang、Shoufa Chen、Qiaojun Yu、Chongjian Ge、Runjian Chen、Zhix-uan Liang、Mengkang Hu、Chaofan Tao 等。2024a。Robocodex: Multimodal code generation for robotic behavior synthesis。arXiv 预印本 arXiv:2402.16117。


Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. 2024b. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural Information Processing Systems, 36.
Yao Mu、Qinglong Zhang、Mengkang Hu、Wenhai Wang、Mingyu Ding、Jun Jin、Bin Wang、Jifeng Dai、Yu Qiao 和 Ping Luo。2024b。Embodiedgpt: Vision-language pre-training via embodied chain of thought。Advances in Neural Information Processing Systems, 36。


Allen Newell, Herbert Alexander Simon, et al. 1972. Human problem solving, volume 104. Prentice-hall Englewood Cliffs, NJ.
Allen Newell、Herbert Alexander Simon 等。1972。Human problem solving, volume 104。Prentice-Hall, Englewood Cliffs, NJ。


OpenAI. 2022. Openai: Introducing chatgpt.
OpenAI。2022。Openai: Introducing chatgpt。


OpenAI. 2023. Gpt-4 technical report. Preprint, arXiv:2303.08774.
OpenAI。2023。Gpt-4 technical report。预印本，arXiv:2303.08774。


Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, et al. 2024. Webcanvas: Benchmarking web agents in online environments. arXiv preprint arXiv:2406.12373.
Yichen Pan、Dehan Kong、Sida Zhou、Cheng Cui、Yifei Leng、Bing Jiang、Hangyu Liu、Yanyi Shang、Shuyan Zhou、Tongshuang Wu 等。2024。Webcanvas: Benchmarking web agents in online environments。arXiv 预印本 arXiv:2406.12373。


Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 1-22.
Joon Sung Park、Joseph O'Brien、Carrie Jun Cai、Meredith Ringel Morris、Percy Liang 和 Michael S Bernstein。2023。Generative agents: Interactive simulacra of human behavior。载于 Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology，页码 1-22。


Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. 2018. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8494-8502.
Xavier Puig、Kevin Ra、Marko Boben、Jiaman Li、Tingwu Wang、Sanja Fidler 和 Antonio Torralba。2018。VirtualHome: Simulating household activities via programs。载于 Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition，页码 8494-8502。


Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789.
Yujia Qin、Shihao Liang、Yining Ye、Kunlun Zhu、Lan Yan、Yaxi Lu、Yankai Lin、Xin Cong、Xiangru Tang、Bill Qian 等。2023。Toolllm: Facilitating large language models to master 16000+ real-world apis。arXiv 预印本 arXiv:2307.16789。


Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. 2023. Tptu: Task planning and tool usage of large language model-based ai agents. arXiv preprint arXiv:2308.03427.
Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. 2023. Tptu: 基于大型语言模型的 AI 代理的任务规划与工具使用. arXiv preprint arXiv:2308.03427.


Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane Legg, and Joel Veness. 2023. Randomized positional encodings boost length generalization of transformers. arXiv preprint arXiv:2305.16843.
Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane Legg, and Joel Veness. 2023. 随机位置编码提升变压器的长度泛化能力. arXiv preprint arXiv:2305.16843.


Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36.
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: 具有语言强化学习的语言代理. Advances in Neural Information Processing Systems, 36.


Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768.
Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: 将文本与具身环境对齐以进行交互式学习. arXiv preprint arXiv:2010.03768.


Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023. Prog-prompt: Generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11523-11530. IEEE.
Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023. Prog-prompt: 使用大型语言模型生成情境化机器人任务计划. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11523-11530. IEEE.


Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. 2023. Llm-planner: Few-shot grounded planning for embodied agents with large language models. Preprint, arXiv:2212.04088.
Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. 2023. Llm-planner: 面向具身代理的少样本有根据规划，基于大型语言模型. Preprint, arXiv:2212.04088.


XAgent Team. 2023. Xagent: An autonomous agent for complex task solving.
XAgent Team. 2023. Xagent: 一种用于复杂任务求解的自主代理.


Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: 开放基础与微调对话模型. arXiv preprint arXiv:2307.09288.


Shangqing Tu, Chunyang Li, Jifan Yu, Xiaozhi Wang, Lei Hou, and Juanzi Li. 2023. Chatlog: Recording and analyzing chatgpt across time. arXiv preprint arXiv:2304.14106.
Shangqing Tu, Chunyang Li, Jifan Yu, Xiaozhi Wang, Lei Hou, and Juanzi Li. 2023. Chatlog: 记录并分析 ChatGPT 随时间的表现. arXiv preprint arXiv:2304.14106.


Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345.
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. 基于大型语言模型的自主代理综述. Frontiers of Computer Science, 18(6):186345.


Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023a. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091.
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023a. Plan-and-solve prompting: 通过大型语言模型提升零样本链式思维推理. arXiv preprint arXiv:2305.04091.


Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2023b. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691.
Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2023b. Mint: 在多轮与工具和语言反馈交互中评估大型语言模型. arXiv preprint arXiv:2309.10691.


Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jin-bing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. 2023c. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. arXiv preprint arXiv:2311.05997.
Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jin-bing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. 2023c. Jarvis-1: 具有记忆增强多模态语言模型的开放世界多任务代理. arXiv preprint arXiv:2311.05997.


Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting 促发大型语言模型的推理. Advances in neural information processing systems, 35:24824-24837.


Robert F Woolson. 2005. Wilcoxon signed-rank test. Encyclopedia of Biostatistics, 8.
Robert F Woolson. 2005. Wilcoxon 符号秩检验. Encyclopedia of Biostatistics, 8.


Mengsong Wu, Tong Zhu, Han Han, Chuanyuan Tan, Xiang Zhang, and Wenliang Chen. 2024. Seal-tools: Self-instruct tool learning dataset for agent tuning and detailed benchmark. arXiv preprint arXiv:2405.08355.
孟松武, 朱彤, 韩寒, 谭传元, 张翔, 与 陈文亮. 2024. Seal-tools: 用于代理微调的自我指令工具学习数据集及详细基准测试. arXiv preprint arXiv:2405.08355.


Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Auto-gen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155.
吴清云, Gagan Bansal, 张洁瑜, 吴怡然, 张少坤, 朱尔康, 李北宾, 蒋莉, 张晓云, 与 王驰. 2023. Auto-gen: 通过多代理对话框架实现下一代大模型应用. arXiv preprint arXiv:2308.08155.


Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yi-wen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864.
席志恒, 陈文祥, 辜鑫, 何伟, 丁怡文, 洪博洋, 张明, 王俊哲, 金森杰, 周恩宇, 等. 2023. 基于大语言模型的代理的兴起与潜力：综述. arXiv preprint arXiv:2309.07864.


Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. 2024. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory. arXiv preprint arXiv:2402.04617.
肖朝骏, 张鹏乐, 韩旭, 肖广轩, 林彦凯, 张政研, 刘知远, 韩松, 与 孙茂松. 2024. Infllm: 通过无训练记忆揭示大模型理解极长序列的内在能力. arXiv preprint arXiv:2402.04617.


Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. 2024. Travelplanner: A benchmark for real-world planning with language agents. arXiv preprint arXiv:2402.01622.
谢戬, 张凯, 陈江杰, 朱廷辉, 楼仁泽, 田元东, 肖阳华, 与 苏雨. 2024. Travelplanner: 面向真实世界规划的语言代理基准. arXiv preprint arXiv:2402.01622.


Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Lu-oxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. 2023. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634.
谢天保, 周凡, 程周俊, 石鹏, 翁陆轩, 刘一涛, 卓静华, 赵俊宁, 刘倩, 刘彻, 等. 2023. Openagents: 面向真实环境的语言代理开源平台. arXiv preprint arXiv:2310.10634.


Siwei Yang, Bingchen Zhao, and Cihang Xie. 2024. Aqa-bench: An interactive benchmark for evaluating llms' sequential reasoning ability. arXiv preprint arXiv:2402.09404.
杨思伟, 赵炳辰, 与 谢慈航. 2024. Aqa-bench: 一个评估大模型序列推理能力的交互式基准. arXiv preprint arXiv:2402.09404.


Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022a. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744-20757.
姚舜宇, Howard Chen, John Yang, 与 Karthik Narasimhan. 2022a. Webshop: 面向可扩展真实网络交互的有根植语言代理. Advances in Neural Information Processing Systems, 35:20744-20757.


Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36.
姚舜宇, 于典, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, 曹源, 与 Karthik Narasimhan. 2024. Tree of thoughts: 用大语言模型进行有意识的问题解决. Advances in Neural Information Processing Systems, 36.


Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022b. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.
姚舜宇, Jeffrey Zhao, 于典, 杜楠, Izhak Shafran, Karthik Narasimhan, 与 曹源. 2022b. React: 在语言模型中协同推理与行动. arXiv preprint arXiv:2210.03629.


Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. 2023. Retroformer: Retrospective large language agents with policy gradient optimization. arXiv preprint arXiv:2308.02151.
姚伟然, Shelby Heinecke, Juan Carlos Niebles, 刘志伟, 冯逸豪, 薛乐, Rithesh Murthy, 陈泽元, 张建国, Devansh Arpit, 等. 2023. Retroformer: 带策略梯度优化的回顾性大型语言代理. arXiv preprint arXiv:2308.02151.


Da Yin, Faeze Brahman, Abhilasha Ravichander, Khy-athi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. 2023. Lumos: Learning agents with unified data, modular design, and open-source llms. arXiv preprint arXiv:2311.05657.
尹达, Faeze Brahman, Abhilasha Ravichander, Khy-athi Chandu, Kai-Wei Chang, 崔耶真, 与 林玉辰. 2023. Lumos: 以统一数据、模块化设计和开源大模型学习代理. arXiv preprint arXiv:2311.05657.


Ruifeng Yuan, Shichao Sun, Zili Wang, Ziqiang Cao, and Wenjie Li. 2023. Evolving large language model assistant with long-term conditional memory. arXiv preprint arXiv:2312.17257.
袁瑞峰, 孙世超, 王子立, 曹子强, 与 李文杰. 2023. 以长期条件记忆演化大型语言模型助手. arXiv preprint arXiv:2312.17257.


Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. 2023a. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485.
张宏欣, 杜伟华, 单佳明, 周青宏, 杜宜伦, Joshua B Tenenbaum, 书天民, 与 甘闯. 2023a. 使用大语言模型模块化构建合作化具身代理. arXiv preprint arXiv:2307.02485.


Jiajie Zhang, Shulin Cao, Tingjia Zhang, Xin Lv, Jiaxin Shi, Qi Tian, Juanzi Li, and Lei Hou. 2023b. Reasoning over hierarchical question decomposition tree for explainable question answering. arXiv preprint arXiv:2305.15056.
Jiajie Zhang, Shulin Cao, Tingjia Zhang, Xin Lv, Jiaxin Shi, Qi Tian, Juanzi Li, and Lei Hou. 2023b. 在分层问题分解树上进行推理以实现可解释问答。arXiv preprint arXiv:2305.15056.


Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 2024. A survey on the memory mechanism of large language model based agents. arXiv preprint arXiv:2404.13501.
Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 2024. 关于基于大型语言模型代理记忆机制的综述。arXiv preprint arXiv:2404.13501.


Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2024. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19632-19642.
Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2024. Expel：大型语言模型代理是体验式学习者。收录于第38届AAAI人工智能会议论文集，页码19632-19642。


Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19724-19731.
Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024. Memorybank：用长期记忆增强大型语言模型。收录于第38届AAAI人工智能会议论文集，页码19724-19731。


Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022. 从易到难提示法使大型语言模型实现复杂推理。arXiv preprint arXiv:2205.10625.


Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. 2023a. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854.
Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. 2023a. Webarena：用于构建自主代理的真实网环境。arXiv preprint arXiv:2307.13854.


Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. 2023b. Recurrentgpt: Interactive generation of (arbitrarily) long text. arXiv preprint arXiv:2305.13304.
Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. 2023b. Recurrentgpt：交互式生成（任意）长文本。arXiv preprint arXiv:2305.13304.


## A More Details on Evaluation Tasks
## 关于评估任务的更多细节


### A.1 Blocksworld
### A.1 积木世界


## Action List
## 操作列表


1. pickup <block>: allows the arm to pick up a block from the table if it is clear and the arm is empty. After the pickup action, the arm will be holding the block, and the block will no longer be on the table or clear.
1. pickup <block>: 允许机械臂在桌面清空且机械臂为空时从桌上拾取一个方块。执行 pickup 后，机械臂将握住该方块，该方块不再在桌上且不再是 clear。


2. putdown <block>: allows the arm to put down a block on the table if it is holding a block. After the putdown action, the arm will be empty, and the block will be on the table and clear.
2. putdown <block>: 允许机械臂在握有方块时将其放在桌上。执行 putdown 后，机械臂将为空，且该方块将位于桌上并为 clear。


3. stack <block> <block>: allows the arm to stack a block on top of another block if the arm is holding the top block and the bottom block is clear. After the stack action, the arm will be empty, the top block will be on top of the bottom block, and the bottom block will no longer be clear.
3. stack <block> <block>: 允许机械臂在握住上方方块且下方方块为 clear 时将上方方块放到下方方块上。执行 stack 后，机械臂将为空，上方方块将位于下方方块之上，下方方块将不再为 clear。


4. unstack <block> <block>: allows the arm to unstack a block from on top of another block if the arm is empty and the top block is clear. After the unstack action, the arm will be holding the top block, the top block will no longer be on top of the bottom block, and the bottom block will be clear.
4. unstack <block> <block>: 允许机械臂在为空且上方方块为 clear 时将上方方块从下方方块上拿起。执行 unstack 后，机械臂将握住上方方块，上方方块将不再位于下方方块之上，下方方块将为 clear。


## Goal example
## 目标示例


b1 is on b2., b2 is on b3.
b1 在 b2 上。, b2 在 b3 上。


## Observation example
## 观测示例


b1 is on the table. b2 is on the table. B3 is on the table. Robot arm is empty. The b1 is clear. The b2 is clear. The b3 is clear.
b1 在桌上。 b2 在桌上。 B3 在桌上。 机械臂为空。 b1 是 clear。 b2 是 clear。 b3 是 clear。


## Action example
## 动作示例


pickup b2.
pickup b2。


### A.2 Gripper
### A.2 抓手


## Action List
## 动作列表


1. move <room1> <room2>: This action allows the robot to move from one room to another.The action has a single precondition, which is that the robot is currently in a room. The effect of this action is to move the robot to another room and to remove the fact that it is in the original room.
1. move <room1> <room2>: 此动作允许机器人从一间房间移动到另一间房间。该动作有一个前提，即机器人当前在某个房间。此动作的效果是将机器人移动到另一房间并移除其在原房间的事实。


2. pick <obj> <room> <gripper>: This action allows the robot to pick up an object using the gripper. The action has three preconditions: (1) the object is located in a room (2) the robot is currently in the same room and (3) the gripper is free (i.e., not holding any object). The effect of this action is to update the state of the world to show that the robot is carrying the object using the gripper, the object is no longer in the room, and the gripper is no longer free.
2. pick <obj> <room> <gripper>: 此动作允许机器人使用抓手拾取一个物体。该动作有三个前提：(1) 物体位于某房间；(2) 机器人当前在同一房间；(3) 抓手为空闲（即未握持任何物体）。此动作的效果是更新世界状态，表明机器人使用抓手携带该物体，物体不再在房间内，且抓手不再空闲。


3. drop <obj> <room> <gripper>: This action allows the robot to drop an object that it is carrying. The action has two preconditions: (1) the robot is currently carrying the object using the gripper, and (2) the robot is currently in a room. The effect of this action is to update the state of the world to show that the robot is no longer carrying the object using the gripper, the object is now located in the room, and the gripper is now free.
3. drop <obj> <room> <gripper>: 此动作允许机器人放下其携带的物体。该动作有两个前提：(1) 机器人当前使用抓手携带该物体；(2) 机器人当前在某房间。此动作的效果是更新世界状态，表明机器人不再使用抓手携带该物体，物体现在位于该房间，且抓手现在为空闲。


## Goal example
## 目标示例


ball1 is at roomb. , ball2 is at roomb., ball3 is at roomb. , ball4 is at room.
ball1 在 roomb。 , ball2 在 roomb., ball3 在 roomb. , ball4 在 room.


## Observation example
## 观察示例


Ball1 is a ball. Ball1 is carrying right. Ball2 is a ball. Ball2 is at rooma. Ball3 is a ball. Ball3 is at rooma. Ball4 is a ball. Ball4 is at rooma. Left is a gripper. Left is free. Right is a gripper. Robby is at rooma. Room rooma Room roomb.
Ball1 是一个球。 Ball1 正被 right 握持。 Ball2 是一个球。 Ball2 在 rooma。 Ball3 是一个球。 Ball3 在 rooma。 Ball4 是一个球。 Ball4 在 rooma。 Left 是一个夹持器。 Left 是空闲的。 Right 是一个夹持器。 Robby 在 rooma。 Room rooma Room roomb.


## Action example
## 操作示例


Pick up ball 1 at rooma with arm right.
在 rooma 用右臂拾起 ball 1。


### A.3 Tyreworld
### A.3 轮胎世界


## Action List
## 操作列表


1. open <container>: The precondition for this action is that the container is unlocked and closed. The effect of this action is that the container is open and not closed.
1. open <container>: 此操作的前提是容器已解锁且关闭。此操作的效果是容器为打开且不再关闭。


2. close <container>: The precondition for this action is that the container is open. The effect of this action is that the container is closed and not open.
2. close <container>: 此操作的前提是容器为打开。此操作的效果是容器为关闭且不再打开。


3. fetch <object> <container>: The precondition for this action is that the object is inside the container and the container is open. The effect of this action is that the object is held by the agent and not inside the container.
3. fetch <object> <container>: 此操作的前提是物体在容器内且容器为打开。此操作的效果是物体被代理持有且不再在容器内。


4. put-away <object> <container>: The precondition for this action is that the object is held by the agent and the container is open. The effect of this action is that the object is inside the container and not held by the agent.
4. put-away <object> <container>: 此操作的前提是物体被代理持有且容器为打开。此操作的效果是物体在容器内且不再被代理持有。


5. loosen <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is tight, and the hub is on the ground. The effect of this action is that the nut on hub is loose and not tight.
5. loosen <nut> <hub>: 此操作的前提是代理有扳手、轮毂上的螺母是紧固的且轮毂在地面上。此操作的效果是轮毂上的螺母为松动且不再是紧固的。


6. tighten <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is loose, and the hub is on the ground. The effect of this action is that the nut on hub is tight and not loose.
6. tighten <nut> <hub>: 此操作的前提是代理有扳手、轮毂上的螺母是松动的且轮毂在地面上。此操作的效果是轮毂上的螺母为紧固且不再是松动的。


7. jack-up <hub>: This action represents the process of lifting a hub off the ground using a jack. It requires the agent to have a jack and for the hub to be on the ground. After performing this action, the hub will no longer be on the ground and the agent will no longer have the jack.
7. jack-up <hub>: 此操作表示使用千斤顶将轮毂从地面抬起。它要求代理有千斤顶且轮毂在地面上。执行此操作后，轮毂将不再在地面上且代理将不再拥有千斤顶。


8. jack-down <hub>: This action represents the process of lowering a hub back to the ground from an elevated position using a jack. It requires the agent to have the hub off the ground. After performing this action, the hub will be back on the ground and the agent will have the jack.
8. jack-down <hub>: 此操作表示使用千斤顶将轮毂从抬起位置放回地面。它要求代理已将轮毂抬离地面。执行此操作后，轮毂将回到地面且代理将拥有千斤顶。


9. undo <nut> <hub>: This action undo the fastening of a nut on a hub. The preconditions are the hub is not on the ground (i.e., it has been jacked up), the hub is fastened, the agent has a wrench and the nut is loose. The effects are the agent has the nut, the hub is unfastened, the hub is no longer loose and the hub is not fastened anymore.
9. undo <nut> <hub>: 此操作撤销轮毂上螺母的紧固。前提是轮毂不在地面上（即已被顶起）、轮毂已被固定、代理有扳手且螺母是松动的。效果是代理获得螺母、轮毂被解固定、轮毂不再是松动的且不再被固定。


10. do-up <nut> <hub>: This action fasten a nut on a hub. The preconditions are the agent has a wrench, the hub is unfastened, the hub is not on the ground (i.e., it has been jacked up) and the agent has the nut to be fastened. The effects are the nut is now loose on the hub, the hub is fastened, the hub is no longer unfastened and the agent no longer has the nut.
10. 拧紧螺母 <螺母> <轮毂>：此操作是将螺母拧紧在轮毂上。前提条件是执行者有扳手，轮毂未拧紧，轮毂不在地面上（即已用千斤顶顶起），且执行者拥有待拧紧的螺母。效果是螺母现在松套在轮毂上，轮毂已拧紧，轮毂不再处于未拧紧状态，且执行者不再拥有该螺母。


11. remove-wheel <wheel> <hub>: This action removes a wheel from a hub. It can only be performed if the hub is not on the ground, the wheel is currently on the hub, and the hub is unfastened. After the action is performed, the agent will have the removed wheel and the hub will be free, meaning that the wheel is no longer on the hub.
11. 拆卸车轮 <车轮> <轮毂>：此操作是从轮毂上拆卸车轮。只有在轮毂不在地面上、车轮当前安装在轮毂上且轮毂未拧紧的情况下才能执行。操作执行后，执行者将获得拆卸下来的车轮，轮毂将处于空闲状态，即车轮不再安装在轮毂上。


12. put-on-wheel <wheel> <hub>: This action puts a wheel onto a hub. It can only be performed if the agent has the wheel, the hub is free, the hub is unfastened, and the hub is not on the ground. After the action is performed, the wheel will be on the hub, the hub will no longer be free, and the agent will no longer have the wheel.
12. 安装车轮 <车轮> <轮毂>：此操作是将车轮安装到轮毂上。只有在执行者拥有车轮、轮毂处于空闲状态、轮毂未拧紧且轮毂不在地面上的情况下才能执行。操作执行后，车轮将安装在轮毂上，轮毂不再处于空闲状态，且执行者不再拥有该车轮。


13. inflate <wheel>: This action inflates a wheel using a pump. It can only be performed if the agent has a pump, the wheel is not inflated, and the wheel is intact. After the action is performed, the wheel will be inflated.
13. 给轮胎充气 <车轮>：此操作是使用气泵给车轮充气。只有在执行者有气泵、车轮未充气且车轮完好无损的情况下才能执行。操作执行后，车轮将被充气。


## Goal example
## 目标示例


w1 is in boot.
车轮 w1 在后备箱里。


## Observation example
## 观察示例


Boot is closed. Boot is unlocked. Hub the-hub1 is fastened. Hub the-hub1 is on the ground. Jack is in boot. Pump is in boot. R1 is in boot. The nut nuts1 on the hub the-hub1 is tight. Wheel r1 is intact. Wheel r1 is not inflated. Wheel w1 is on hub the-hub1. Wrench is in boot.
后备箱关闭。后备箱未上锁。轮毂 the - hub1 已拧紧。轮毂 the - hub1 在地面上。千斤顶在后备箱里。气泵在后备箱里。车轮 r1 在后备箱里。轮毂 the - hub1 上的螺母 nuts1 很紧。车轮 r1 完好无损。车轮 r1 未充气。车轮 w1 安装在轮毂 the - hub1 上。扳手在后备箱里。


## Action example
## 动作示例


Open boot.
打开后备箱。


### A.4 Barman
### A.4 酒保


## Action List
## 动作列表


1. <hand> grasp <container>: Grasp a container
1. <手> 抓住 <容器>：抓住一个容器


2. <hand> leave <container>: Leave a container on the table
2. <手> 放下 <容器>：将容器放在桌子上


3. fill-shot <shot> <ingredient> <hand1> <hand2> <dispenser>: Fill a shot glass with an ingredient from dispenser
3. 倒一杯酒 <小酒杯> <酒品> <手 1> <手 2> <酒分配器>：从酒分配器中取酒品倒入小酒杯


4. refill-shot <shot> <ingredient> <handl> <hand2> <dispenser>: Refill a shot glass with an ingredient from dispenser
4. 再倒一杯酒 <小酒杯> <酒品> <手 1> <手 2> <酒分配器>：从酒分配器中取酒品再次倒入小酒杯


5. empty-shot <hand> <shot> <beverage>: Empty a shot glass 6. clean-shot <shot> <beverage> <handl> <hand2>: Clean a shot glass
5. empty-shot <hand> <shot> <beverage>：倒空一杯酒 6. clean-shot <shot> <beverage> <handl> <hand2>：清洗一个小酒杯


7. pour-shot-to-clean-shaker <shot> <ingredient> <shaker> <handl> <level1> <level2>: Pour an ingredient from a shot glass to a clean shaker from level1 to level2
7. pour-shot-to-clean-shaker <shot> <ingredient> <shaker> <handl> <level1> <level2>：将一种配料从一个小酒杯倒入一个干净的调酒器，从液位1到液位2


8. pour-shot-to-used-shaker <shot> <ingredient> <shaker> <handl> <level1> <level2>: Pour an ingredient from a shot glass to a used shaker from level1 to level2
8. pour-shot-to-used-shaker <shot> <ingredient> <shaker> <handl> <level1> <level2>：将一种配料从一个小酒杯倒入一个用过的调酒器，从液位1到液位2


9. empty-shaker <hand> <shaker> <cocktail> <level1> <level2>: Empty a shaker containing cocktail from level1 to level2
9. empty-shaker <hand> <shaker> <cocktail> <level1> <level2>：将装有鸡尾酒的调酒器从液位1倒空到液位2


10. clean-shaker <hand1> <hand2> <shaker>: Clean a shaker 11. shake <cocktail> <ingredient1> <ingredient2> <shaker> <hand1> <hand2>: Shake a cocktail in a shaker
10. clean-shaker <hand1> <hand2> <shaker>：清洗一个调酒器 11. shake <cocktail> <ingredient1> <ingredient2> <shaker> <hand1> <hand2>：在调酒器中摇晃鸡尾酒


12. pour-shaker-to-shot <beverage> <shot> <hand> <shaker> <level1> <level2>: Pour a beverage from a shaker to a shot glass from level 1 to level2
12. pour-shaker-to-shot <beverage> <shot> <hand> <shaker> <level1> <level2>：将一种饮料从调酒器倒入一个小酒杯，从液位1到液位2


## Goal example
## 目标示例


shot1 contains cocktail1.
小酒杯1装有鸡尾酒1。


## Observation example
## 观察示例


Cocktail1 part1 ingredient is ingredient1. Cocktail1 part2 ingredient is ingredient3. Cocktail2 part1 ingredient is ingredient2. Cocktail2 part2 ingredient is ingredient3. Cocktail3 part1 ingredient is ingredient1. Cocktail3 part2 ingredient is ingredient2. Dispenser1 dispenses ingredient1. Dispenser2 dispenses ingredient2. Dispenser3 dispenses ingredient3. Left hand is empty. Level 10 is next to level 11. Level 11 is next to level 12. Right hand is empty. Shaker1 is at empty level 10. Shaker1 is at level 10. Shaker1 is clean. Shaker1 is empty. Shaker1 is on the table. Shot1 is clean. Shot1 is empty. Shot1 is on the table. Shot2 is clean. Shot2 is empty. Shot2 is on the table. Shot3 is clean. Shot3 is empty. Shot3 is on the table. Shot4 is clean. Shot4 is empty. Shot4 is on the table.
鸡尾酒1的第一部分配料是配料1。鸡尾酒1的第二部分配料是配料3。鸡尾酒2的第一部分配料是配料2。鸡尾酒2的第二部分配料是配料3。鸡尾酒3的第一部分配料是配料1。鸡尾酒3的第二部分配料是配料2。分配器1分配配料1。分配器2分配配料2。分配器3分配配料3。左手为空。液位10与液位11相邻。液位11与液位12相邻。右手为空。调酒器1位于空的液位10处。调酒器1在液位10处。调酒器1是干净的。调酒器1是空的。调酒器1在桌子上。小酒杯1是干净的。小酒杯1是空的。小酒杯1在桌子上。小酒杯2是干净的。小酒杯2是空的。小酒杯2在桌子上。小酒杯3是干净的。小酒杯3是空的。小酒杯3在桌子上。小酒杯4是干净的。小酒杯4是空的。小酒杯4在桌子上。


## Action example
## 动作示例


right grasp shot1.
右手抓住小酒杯1。


### A.5 Jericho
### A.5 杰里科


## Action List
## 动作列表


1. Inventory: check things you are carrying
1. Inventory：查看你携带的物品


2. Look: check your surroundings
2. Look：查看你的周围环境


3. Examine <place/obj>: check the details of something
3. 检查 <place/obj>: 查看某物的细节


4. Take <obj>: pickup obj
4. 拿起 <obj>: 拾起 obj


5. Put down <obj>: leave a obj at your current place.
5. 放下 <obj>: 在你当前地点留下一个 obj。


6. Drop <obj>
6. 丢下 <obj>


7. Check valid actions: Check actions you can use
7. 检查可用动作: 检查你可以使用的动作


8. South: go south
8. 南: 向南走


9. North: go north
9. 北: 向北走


10. East: go east
10. 东: 向东走


11. West: go west
11. 西: 向西走


12. Up: go up
12. 上: 向上走


13. Down: go down
13. 下: 向下走


14. Check valid actions (Other available actions)
14. 检查可用动作（其他可用动作）


## Goal example
## 目标示例


You are the warrior Link that needs to save the princess from the castle.
你是需要从城堡救出公主的战士林克。


## Observation example
## 观察示例


You are at the path leading to the castle. The castle is to your north. There is a barrel in front of you.
你在通往城堡的小路上。城堡在你的北面。你面前有一个桶。


## Action example
## 操作示例


## B Prompt Examples
## B 提示示例


### B.1 STANDARD
### B.1 标准


## Environment Implementation
## 环境实现


Your goal is to replace flat tyres with intact tyres on the hubs. Remember to open boot first to get tools you need. Intact tyres should be inflated. The nuts should be tight on the hubs. The flat tyres, wrench, jack, and pump should be in the boot. The boot should be closed.
你的目标是在轮毂上将瘪胎换成完好的轮胎。记得先打开后备箱以取出所需工具。完好的轮胎应充气。轮毂上的螺母应拧紧。瘪胎、扳手、千斤顶和打气筒应在后备箱内。后备箱应关闭。


There are 13 actions defined in this domain:
此领域定义了 13 个动作：


open <container>: The precondition for this action is that the container is unlocked and closed. The effect of this action is that the container is open and not closed.
open <container>: 此动作的前提是容器已解锁且关闭。此动作的效果是容器为打开状态且不再关闭。


close <container>: The precondition for this action is that the container is open. The effect of this action is that the container is closed and not open.
close <container>: 此动作的前提是容器为打开状态。此动作的效果是容器为关闭状态且不再打开。


fetch <object> <container>: The precondition for this action is that the object is inside the container and the container is open. The effect of this action is that the object is held by the agent and not inside the container.
fetch <object> <container>: 此动作的前提是物体在容器内且容器为打开状态。此动作的效果是该物体被代理人持有且不再在容器内。


put-away <object> <container>: The precondition for this action is that the object is held by the agent and the container is open. The effect of this action is that the object is inside the container and not held by the agent.
put-away <object> <container>: 此动作的前提是该物体被代理人持有且容器为打开状态。此动作的效果是该物体在容器内且不再被代理人持有。


loosen <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is tight, and the hub is on the ground. The effect of this action is that the nut on hub is loose and not tight.
loosen <nut> <hub>: 此动作的前提是代理人有扳手、轮毂上的螺母是拧紧的且轮毂在地面上。此动作的效果是轮毂上的螺母变为松动且不再拧紧。


tighten <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is loose, and the hub is on the ground. The effect of this action is that the nut on hub is tight and not loose.
tighten <nut> <hub>: 此动作的前提是代理人有扳手、轮毂上的螺母是松动的且轮毂在地面上。此动作的效果是轮毂上的螺母变为拧紧且不再松动。


jack-up <hub>: This action represents the process of lifting a hub off the ground using a jack. It requires the agent to have a jack and for the hub to be on the ground. After performing this action, the hub will no longer be on the ground and the agent will no longer have the jack.
jack-up <hub>: 此动作表示使用千斤顶将轮毂从地面抬起的过程。它要求代理人有千斤顶且轮毂在地面上。执行此动作后，轮毂将不再接触地面且代理人将不再拥有千斤顶。


jack-down <hub>: This action represents the process of lowering a hub back to the ground from an elevated position using a jack. It requires the agent to have the hub off the ground. After performing this action, the hub will be back on the ground and the agent will have the jack.
jack-down <hub>: 此动作表示使用千斤顶将轮毂从抬起位置放回地面的过程。它要求代理人拥有已抬离地面的轮毂。执行此动作后，轮毂将回到地面且代理人将拥有千斤顶。


undo <nut> <hub>: This action undo the fastening of a nut on a hub. The preconditions are the hub is not on the ground (i.e., it has been jacked up), the hub is fastened, the agent has a wrench and the nut is loose. The effects are the agent has the nut, the hub is unfastened, the hub is no longer loose and the hub is not fastened anymore.
undo <nut> <hub>: 此动作用于解除轮毂上螺母的紧固。前提是轮毂不在地面（即已被顶起）、轮毂已被紧固、代理人有扳手且螺母为松动。效果是代理人获得螺母、轮毂变为未紧固、轮毂不再松动且轮毂不再被紧固。


do-up <nut> <hub>: This action fasten a nut on a hub. The preconditions are the agent has a wrench, the hub is unfastened, the hub is not on the ground (i.e., it has been jacked up) and the agent has the nut to be fastened. The effects are the nut is now loose on the hub, the hub is fastened, the hub is no longer unfastened and the agent no longer has the nut.
do-up <nut> <hub>: 此动作用于将螺母固定在轮毂上。前提是代理人有扳手、轮毂为未紧固、轮毂不在地面（即已被顶起）且代理人有要固定的螺母。效果是螺母现在在轮毂上为松动状态、轮毂被紧固、轮毂不再为未紧固且代理人不再拥有该螺母。


remove-wheel <wheel> <hub>: This action removes a wheel from a hub. It can only be performed if the hub is not on the ground, the wheel is currently on the hub, and the hub is unfastened. After the action is performed, the agent will have the removed wheel and the hub will be free, meaning that the wheel is no longer on the hub.
remove-wheel <wheel> <hub>：此操作是从轮毂上拆卸车轮。只有当轮毂不在地面上、车轮当前安装在轮毂上且轮毂未固定时，才能执行该操作。操作执行后，执行者将获得拆卸下来的车轮，且轮毂将处于空闲状态，即车轮不再安装在轮毂上。


put-on-wheel <wheel> <hub>: This action puts a wheel onto a hub. It can only be performed if the agent has the wheel, the hub is free, the hub is unfastened, and the hub is not on the ground. After the action is performed, the wheel will be on the hub, the hub will no longer be free, and the agent will no longer have the wheel.
put-on-wheel <wheel> <hub>：此操作是将车轮安装到轮毂上。只有当执行者拥有车轮、轮毂处于空闲状态、轮毂未固定且轮毂不在地面上时，才能执行该操作。操作执行后，车轮将安装在轮毂上，轮毂不再空闲，且执行者将不再拥有该车轮。


inflate <wheel>: This action inflates a wheel using a pump. It can only be performed if the agent has a pump, the wheel is not inflated, and the wheel is intact. After the action is performed, the wheel will be inflated.
inflate <wheel>：此操作是使用气泵给车轮充气。只有当执行者拥有气泵、车轮未充气且车轮完好无损时，才能执行该操作。操作执行后，车轮将被充气。


Here are examples:
以下是示例：


The goal is to satisfy the following conditions: w1 is in boot. (Note you need to open boot first so that you can extract tools from it.)
目标是满足以下条件：w1 在后备箱中。（注意，你需要先打开后备箱，以便从中取出工具。）


Observation: Boot is closed. Boot is unlocked. Hub the-hub1 is fastened. Hub the-hub1 is on the ground. Jack is in boot. Pump is in boot. R1 is in boot. The nut nuts1 on the hub the-hub1 is tight. Wheel r1 is intact. Wheel r1 is not inflated. Wheel w1 is on hub the-hub1. Wrench is in boot.
观察结果：后备箱关闭。后备箱已解锁。轮毂 the-hub1 已固定。轮毂 the-hub1 在地面上。千斤顶在后备箱中。气泵在后备箱中。R1 在后备箱中。轮毂 the-hub1 上的螺母 nuts1 很紧。车轮 r1 完好无损。车轮 r1 未充气。车轮 w1 安装在轮毂 the-hub1 上。扳手在后备箱中。


Action: check valid actions.
操作：检查有效操作。


Observation: Valid actions are: Open boot.
观察结果：有效操作有：打开后备箱。


Check valid actions. Look around.
检查有效操作。环顾四周。


Action: Open boot.
操作：打开后备箱。


Observation: Boot is open.
观察结果：后备箱已打开。


Action: fetch wrench from boot
操作：从后备箱中取出扳手


Observation: You have wrench.
观察结果：你已拿到扳手。


Action: check valid actions
操作：检查有效操作


Observation: Valid actions are: Fetch pump from boot., Fetch r1 from boot., Put-away wrench in boot., Close boot., Fetch jack from boot., Loosen the nut nuts 1 on the hub the-hub1., check valid actions
观察结果：有效操作有：从后备箱中取出气泵。、从后备箱中取出 r1。、将扳手放回后备箱。、关闭后备箱。、从后备箱中取出千斤顶。、松开轮毂 the-hub1 上的螺母 nuts 1。、检查有效操作


Action: Loosen the nut nuts1 on the hub the-hub1.
操作：松开轮毂 the-hub1 上的螺母 nuts1。


Observation: The nut nuts1 on the hub the-hub1 is loose.
观察：轮毂 the-hub1 上的螺母 nuts1 松动。


Action: fetch jack from boot
操作：从后备箱取出千斤顶


Observation: You have jack.
观察：你已拿到千斤顶。


Action: jack-up the hub the-hub1
操作：用千斤顶顶起轮毂 the-hub1


Observation: Hub the-hub1 is not on the ground.
观察：轮毂 the-hub1 已离开地面。


Action: Undo the fastening of the nut nuts1 on the hub the-hub1.
操作：松开轮毂 the-hub1 上螺母 nuts1 的固定装置。


Observation: Hub the-hub1 is unfastened.
观察：轮毂 the-hub1 已松开。


Action: Remove-wheel w1 from the hub the-hub1.
操作：从轮毂 the-hub1 上拆下轮胎 w1。


Observation: You have w1.
观察：你已拿到轮胎 w1。


Action: Put-away w1 in boot
操作：将轮胎 w1 放入后备箱


Observation: W1 is in boot. Goal is completed.
观察：轮胎 w1 已在后备箱。目标已完成。


You should perform actions to accomplish the goal: The goal is to satisfy the following conditions: Wheel r1 is inflated., r1 is on the-hub1., w1 is in boot.
你应执行操作以实现目标：目标是满足以下条件：轮胎 r1 已充气；轮胎 r1 安装在轮毂 the-hub1 上；轮胎 w1 在后备箱。


You should use the following commands for help when your action cannot be understood: check valid actions
当你的操作无法被理解时，你可以使用以下命令获取帮助：检查有效操作


Observation: Boot is closed. Boot is unlocked. Hub the-hub1 is fastened. Hub the-hub1 is on the ground. Jack is in boot. Pump is in boot. R1 is in boot. The nut nuts1 on the hub the-hub1 is tight. W1 is on the-hub1. Wheel r1 is intact. Wheel r1 is not inflated. Wrench is in boot.
观察：后备箱关闭。后备箱已解锁。轮毂 the-hub1 已固定。轮毂 the-hub1 在地面上。千斤顶在后备箱。打气筒在后备箱。轮胎 r1 在后备箱。轮毂 the-hub1 上的螺母 nuts1 拧紧。轮胎 w1 安装在轮毂 the-hub1 上。轮胎 r1 完好。轮胎 r1 未充气。扳手在后备箱。


Action: Open boot.
操作：打开后备箱。


Observation: Boot is open.
观察：后备箱已打开。


Action: jack-up the-hub1
操作：顶起轮毂 the-hub1


Observation: The action is not valid and therefore takes no effect. Please check valid actions. Action: jack-up the hub the-hub1
观察结果：该操作无效，因此不产生任何效果。请检查有效操作。操作：顶起轮毂 the-hub1


Observation: The action is not valid and therefore takes no effect. Please check valid actions.
观察结果：该操作无效，因此不产生任何效果。请检查有效操作。


Action: Loosen the nut nuts1 on the hub the-hub1.
操作：松开轮毂 the-hub1 上的螺母 nuts1。


Observation: The action is not valid and therefore takes no effect. Please check valid actions.
观察结果：该操作无效，因此不产生任何效果。请检查有效操作。


Action: Loosen the nut nuts1 on the hub the-hub1.
操作：松开轮毂 the-hub1 上的螺母 nuts1。


Observation: The action is not valid and therefore takes no effect. Please check valid actions.
观察结果：该操作无效，因此不产生任何效果。请检查有效操作。


Action: Loosen the nut nuts1 on the hub the-hub1.
操作：松开轮毂 the-hub1 上的螺母 nuts1。


Observation: The action is not valid and therefore takes no effect. Please check valid actions.
观察结果：该操作无效，因此不产生任何效果。请检查有效操作。


Action: Fetch wrench from boot.
操作：从后备箱取扳手。


Observation: You have wrench.
观察结果：你已拿到扳手。


### B.2 HIAGENT
### B.2 高级智能代理


## Environment Implementation
## 环境实现


Your goal is to replace flat tyres with intact tyres on the hubs. Remember to open boot first to get tools you need. Intact tyres should be inflated. The nuts should be tight on the hubs. The flat tyres, wrench, jack, and pump should be in the boot. The boot should be closed.
你的目标是将轮毂上的瘪胎更换为完好的轮胎。记得先打开后备箱获取所需工具。完好的轮胎应已充气。轮毂上的螺母应拧紧。瘪胎、扳手、千斤顶和打气筒应放在后备箱中。后备箱应关闭。


There are 13 actions defined in this domain:
此领域定义了 13 个操作：


open <container>: The precondition for this action is that the container is unlocked and closed. The effect of this action is that the container is open and not closed.
打开 <容器>：此操作的前提条件是容器已解锁且处于关闭状态。此操作的效果是容器打开且不再处于关闭状态。


close <container>: The precondition for this action is that the container is open. The effect of this action is that the container is closed and not open.
close <container>: 执行该动作的前提是容器是打开的。该动作的结果是容器被关闭且不再打开。


fetch <object> <container>: The precondition for this action is that the object is inside the container and the container is open. The effect of this action is that the object is held by the agent and not inside the container.
fetch <object> <container>: 执行该动作的前提是物体在容器内且容器是打开的。该动作的结果是物体被代理人持有且不再在容器内。


put-away <object> <container>: The precondition for this action is that the object is held by the agent and the container is open. The effect of this action is that the object is inside the container and not held by the agent.
put-away <object> <container>: 执行该动作的前提是物体被代理人持有且容器是打开的。该动作的结果是物体在容器内且不再被代理人持有。


loosen <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is tight, and the hub is on the ground. The effect of this action is that the nut on hub is loose and not tight.
loosen <nut> <hub>: 执行该动作的前提是代理人有扳手、轮毂上的螺母是紧的且轮毂在地上。该动作的结果是轮毂上的螺母变松且不再是紧的。


tighten <nut> <hub>: The precondition for this action is that the agent has a wrench, the nut on hub is loose, and the hub is on the ground. The effect of this action is that the nut on hub is tight and not loose.
tighten <nut> <hub>: 执行该动作的前提是代理人有扳手、轮毂上的螺母是松的且轮毂在地上。该动作的结果是轮毂上的螺母变紧且不再是松的。


jack-up <hub>: This action represents the process of lifting a hub off the ground using a jack. It requires the agent to have a jack and for the hub to be on the ground. After performing this action, the hub will no longer be on the ground and the agent will no longer have the jack.
jack-up <hub>: 该动作表示使用千斤顶将轮毂从地面抬起。它要求代理人有千斤顶且轮毂在地上。执行该动作后，轮毂将不再在地上且代理人将不再持有千斤顶。


jack-down <hub>: This action represents the process of lowering a hub back to the ground from an elevated position using a jack. It requires the agent to have the hub off the ground. After performing this action, the hub will be back on the ground and the agent will have the jack.
jack-down <hub>: 该动作表示使用千斤顶将轮毂从抬起的位置放回地面。它要求代理人已将轮毂抬离地面。执行该动作后，轮毂将回到地面且代理人将持有千斤顶。


undo <nut> <hub>: This action undo the fastening of a nut on a hub. The preconditions are the hub is not on the ground (i.e., it has been jacked up), the hub is fastened, the agent has a wrench and the nut is loose. The effects are the agent has the nut, the hub is unfastened, the hub is no longer loose and the hub is not fastened anymore.
undo <nut> <hub>: 该动作解除轮毂上螺母的紧固。前提是轮毂不在地上（即已用千斤顶抬起）、轮毂处于紧固状态、代理人有扳手且螺母是松的。结果是代理人获得螺母、轮毂被解固、轮毂不再是松的且轮毂不再处于紧固状态。


do-up <nut> <hub>: This action fasten a nut on a hub. The preconditions are the agent has a wrench, the hub is unfastened, the hub is not on the ground (i.e., it has been jacked up) and the agent has the nut to be fastened. The effects are the nut is now loose on the hub, the hub is fastened, the hub is no longer unfastened and the agent no longer has the nut.
do-up <nut> <hub>: 该动作将螺母拧紧于轮毂。前提是代理人有扳手、轮毂已被解固、轮毂不在地上（即已用千斤顶抬起）且代理人有要拧的螺母。结果是螺母现在在轮毂上是松的、轮毂被紧固、轮毂不再是解固状态且代理人不再持有螺母。


remove-wheel <wheel> <hub>: This action removes a wheel from a hub. It can only be performed if the hub is not on the ground, the wheel is currently on the hub, and the hub is unfastened. After the action is performed, the agent will have the removed wheel and the hub will be free, meaning that the wheel is no longer on the hub.
remove-wheel <wheel> <hub>: 该动作将车轮从轮毂上拆下。只有在轮毂不在地上、车轮当前装在轮毂上且轮毂已解固时才能执行。动作执行后，代理人将拿到被拆下的车轮且轮毂将空出，即车轮不再在轮毂上。


put-on-wheel <wheel> <hub>: This action puts a wheel onto a hub. It can only be performed if the agent has the wheel, the hub is free, the hub is unfastened, and the hub is not on the ground. After the action is performed, the wheel will be on the hub, the hub will no longer be free, and the agent will no longer have the wheel.
put-on-wheel <wheel> <hub>: 该动作将车轮装到轮毂上。只有在代理人持有车轮、轮毂空闲、轮毂已解固且轮毂不在地上时才能执行。动作执行后，车轮将装在轮毂上、轮毂不再空闲且代理人不再持有车轮。


inflate <wheel>: This action inflates a wheel using a pump. It can only be performed if the agent has a pump, the wheel is not inflated, and the wheel is intact. After the action is performed, the wheel will be inflated.
inflate <wheel>: 该动作使用打气筒给车轮充气。只有在代理人有打气筒、车轮未充气且车轮完好时才能执行。动作执行后，车轮将被充气。


Note: A subgoal is a milestone goal that you need to complete in order to achieve the final goal. When there is an unfinished subgoal, you need to ground the given subgoal to corresponding executable actions for solving the given task in the following format: "Action: action". When there is no current subgoal or you believe the previous subgoal has been completed (based on past actions and observations), you need to output the next subgoal to be completed and its first action in the following format: "Subgoal: subgoal Action: action". You cannot output two subgoals consecutively. Detailed trajectory information (action-observation pair) of previously satisfied subgoals will be hidden for context efficiency. If you believe that the detailed trajectory information of a particular subgoal is crucial for the current subgoal, you can use Action: "retrieve(subgoal_id)" to obtain the detailed trajectory information.
Note: A subgoal is a milestone goal that you need to complete in order to achieve the final goal. When there is an unfinished subgoal, you need to ground the given subgoal to corresponding executable actions for solving the given task in the following format: "Action: action". When there is no current subgoal or you believe the previous subgoal has been completed (based on past actions and observations), you need to output the next subgoal to be completed and its first action in the following format: "Subgoal: subgoal Action: action". You cannot output two subgoals consecutively. Detailed trajectory information (action-observation pair) of previously satisfied subgoals will be hidden for context efficiency. If you believe that the detailed trajectory information of a particular subgoal is crucial for the current subgoal, you can use Action: "retrieve(subgoal_id)" to obtain the detailed trajectory information.


Here are examples:
Here are examples:


The goal is to satisfy the following conditions: w1 is in boot. (Note you need to open boot first so that you can extract tools from it.)
The goal is to satisfy the following conditions: w1 is in boot. (Note you need to open boot first so that you can extract tools from it.)


Observation: Boot is closed. Boot is unlocked. Hub the-hub1 is fastened. Hub the-hub1 is on the ground. Jack is in boot. Pump is in boot. R1 is in boot. The nut nuts1 on the hub the-hub1 is tight. Wheel r1 is intact. Wheel r1 is not inflated. Wheel w1 is on hub the-hub1. Wrench is in boot.
观察：后备箱关闭。后备箱未上锁。轮毂 the-hub1 已固定。轮毂 the-hub1 放在地上。千斤顶在后备箱里。气泵在后备箱里。R1 在后备箱里。轮毂 the-hub1 上的螺母 nuts1 已拧紧。车轮 r1 完好。车轮 r1 未充气。车轮 w1 在轮毂 the-hub1 上。扳手在后备箱里。


1 Subgoal: Open the boot to access the tools.
1 子目标：打开后备箱以取出工具。


Action: check valid actions.
动作：检查可执行动作。


Observation: Valid actions are: Open boot.
观察：可执行动作有：打开后备箱。


Check valid actions. Look around.
检查可执行动作。环顾四周。


Action: Open boot.
动作：打开后备箱。


Observation: Boot is open. Subgoal is met.
观察：后备箱已打开。子目标已达成。


2 Subgoal: Fetch the wrench from the boot.
2 子目标：从后备箱取出扳手。


Action: fetch wrench from boot
动作：从后备箱取扳手


Observation: You have wrench. Subgoal is met.
观察：你已持有扳手。子目标已达成。


3 Subgoal: Loosen the nut on the hub.
3 子目标：松开轮毂上的螺母。


Action: Loosen the nut nuts2 on the hub the-hub1.
动作：松开位于轮毂 the-hub1 上的螺母 nuts2。


Observation: The action is not valid. Subgoal is not met.
观察：该动作无效。子目标未达成。


4 Subgoal: Examine errors and try to loosen the nut on the hub again.
4 子目标：检查错误并再次尝试松开轮毂上的螺母。


Action: retrieve(3)
动作：retrieve(3)


Observation: trajectory of Subgoal 3 is retrieved.
观察：已检索到子目标 3 的轨迹。


Action: check valid actions
操作：检查可用操作


Observation: Valid actions are: Fetch pump from boot., Fetch r1 from boot., Put-away wrench in boot., Close boot., Fetch jack from boot., Loosen the nut nuts 1 on the hub the-hub1., check valid actions
观察：可用操作有：从后备箱取出千斤顶。，从后备箱取出 r1。，把扳手放回后备箱。，关闭后备箱。，从后备箱取出千斤顶。，松开车轮毂 the-hub1 上的螺母 nuts 1。，检查可用操作


Action: Loosen the nut nuts on the hub the-hub1.
操作：松开车轮毂 the-hub1 上的螺母 nuts。


Observation: The nut nuts1 on the hub the-hub1 is loose. Subgoal is met.
观察：车轮毂 the-hub1 上的螺母 nuts1 已松动。子目标已达成。


5 Subgoal: Fetch the jack from the boot and jack up the hub. Action: fetch jack from boot
5 子目标：从后备箱取出千斤顶并顶起车轮毂。操作：从后备箱取出千斤顶


Observation: You have jack.
观察：你有千斤顶。


Action: jack-up the hub the-hub1
操作：顶起车轮毂 the-hub1


Observation: Hub the-hub1 is not on the ground. Subgoal is met.
观察：车轮毂 the-hub1 已离地。子目标已达成。


6 Subgoal: Unfasten the nut nuts1 and remove wheel w1.
6 子目标：松开螺母 nuts1 并拆下车轮 w1。


Action: Undo the fastening of the nut nuts1 on the hub the-hub1.
操作：解除车轮毂 the-hub1 上螺母 nuts1 的紧固。


Observation: Hub the-hub1 is unfastened.
观察：车轮毂 the-hub1 已解开紧固。


Action: Remove-wheel w1 from the hub the-hub1.
操作：从车轮毂 the-hub1 上拆下车轮 w1。


Observation: You have w1. Subgoal is met.
观察：你有 w1。子目标已达成。


7 Subgoal: Put away the wheel w1 in the boot.
7 子目标：把车轮 w1 放回后备箱。


Action: Put-away w1 in boot
操作：把 w1 放回后备箱


Observation: W1 is in boot. Goal is completed. You should perform actions to accomplish the goal: The goal is to satisfy the following conditions: Wheel r1 is inflated., r1 is on the-hub1., w1 is in boot.
观察：W1 在后备箱中。目标已完成。你应执行的操作以完成目标：目标是满足以下条件：车轮 r1 已充气。，r1 在 the-hub1 上。，w1 在后备箱中。


You should use the following commands for help when your action cannot be understood: check valid actions
当你的操作无法被理解时，你应使用以下命令寻求帮助：check valid actions


Observation: Boot is closed. Boot is unlocked. Hub the-hub1 is fastened. Hub the-hub1 is on the ground. Jack is in boot. Pump is in boot. R1 is in boot. The nut nuts1 on the hub the-hub1 is tight. W1 is on the-hub1. Wheel r1 is intact. Wheel r1 is not inflated. Wrench is in boot.
观察：后备箱关闭。后备箱未锁。轮毂 the-hub1 已固定。轮毂 the-hub1 在地面上。千斤顶在后备箱里。打气筒在后备箱里。R1 在后备箱里。轮毂 the-hub1 上的螺母 nuts1 是紧的。W1 在 the-hub1 上。车轮 r1 完好无损。车轮 r1 未充气。扳手在后备箱里。


1 Subgoal: Open the boot to access the tools.
1 子目标：打开后备箱以取用工具。


Observation: Boot is open.
观察：后备箱已打开。


2 Subgoal: Fetch the wrench from the boot.
2 子目标：从后备箱中取出扳手。


Observation: You have wrench.
观察：你已拿到扳手。


3 Subgoal: Loosen the nut on the hub the-hub1.
3 子目标：松开轮毂 the-hub1 上的螺母。


Observation: The nut nuts1 on the hub the-hub1 is loose.
观察：轮毂 the-hub1 上的螺母 nuts1 已松开。


4 Subgoal: Fetch the jack from the boot and jack up the hub the-hub1.
4 子目标：从后备箱中取出千斤顶并顶起轮毂 the-hub1。


Observation: You have jack and hub the-hub1 is elevated. Subgoal is met.
观察：你已拿到千斤顶且轮毂 the-hub1 已被顶起。子目标达成。


5 Subgoal: Unfasten the nut nuts1 and remove wheel w1 from the hub the-hub1.
5 子目标：拧下螺母 nuts1 并从轮毂 the-hub1 上取下车轮 w1。


Action: undo nuts1 on the-hub1
操作：拧下 the-hub1 上的 nuts1


Observation: Hub the-hub1 is unfastened. You have nuts1.
观察：轮毂 the-hub1 已松开。你已拿到 nuts1。


Action: Remove-wheel w1 from the-hub1
操作：从 the-hub1 上取下车轮 w1


Observation: The-hub1 is free. You have w1.
观察：the-hub1 已无阻碍。你已拿到 w1。


## C More details on Observation Summarization
## C 观察总结的更多细节


### C.1 Prompt Example
### C.1 提示示例


You are an advanced AI system tasked with summarizing and analyzing a series of action-observation pairs (trajectories) and determining whether a specific subgoal has been met.
你是一个高级 AI 系统，负责总结与分析一系列动作-观察对（轨迹），并判断是否达成特定子目标。


Your goal is to create a summary that captures all essential information, decisions, and outcomes from the given trajectories, and indicate whether the subgoal has been met based on the summarized observations.
你的目标是创建一个摘要，概括给定轨迹中所有关键信息、决策与结果，并根据摘要的观察判断子目标是否已达成。


If there are no valid actions taken, you need to analyze the reason.
如果没有采取任何有效行动，你需要分析原因。


###Instructions:
###Instructions:


1. Provide a summarized observation related to the subgoal in a concise manner.
1. 用简洁的方式提供与子目标相关的摘要性观察。


2. Determine whether the subgoal has been met.
2. 确定子目标是否已达成。


3. Do not output anything except whether summary and subgoal are met. Your output should be only one line. Do not output things like '##Summary', '##Summary and Analysis'.
3. 除了是否提供摘要和子目标是否达成之外不要输出任何其他内容。你的输出应仅为一行。不要输出诸如 '##Summary'、'##Summary and Analysis' 之类的内容。


\{example\}
\{example\}


##Trajectory
##Trajectory


\{formatted_trajectory\}
\{formatted_trajectory\}


##Subgoal:
##Subgoal:


\{subgoal\}
\{subgoal\}


###Output:
###Output:


### C.2 Comparison with Other Task Planning Methods
### C.2 Comparison with Other Task Planning Methods


To further validate the effectiveness of HIAGENT, we conducted comprehensive comparisons with other state-of-the-art efficient task planning methods, including Least-to-Most (Zhou et al., 2022) and Tree-Planner (Hu et al., 2023b).
为进一步验证 HIAGENT 的有效性，我们对比了其他最先进的高效任务规划方法，包括 Least-to-Most (Zhou et al., 2022) 和 Tree-Planner (Hu et al., 2023b)。


Comparison with Least-to-Most. As shown in Table 2, we adapted the Least-to-Most approach to agent scenarios as our Task Decomposition baseline. The results demonstrate that HIAGENT achieves a 20% higher success rate while maintaining better context efficiency. This improvement can be attributed to our hierarchical memory management approach that enables more effective reasoning over long-horizon tasks.
与 Least-to-Most 的比较。如表 2 所示，我们将 Least-to-Most 方法调整用于智能体场景，作为我们的任务分解基线。结果显示，HIAGENT 在保持更好上下文效率的同时，成功率提高了 20%。这一改进可归因于我们的分层记忆管理方法，使得在长时程任务上进行更有效的推理成为可能。


Comparison with Tree-Planner. We also compared HIAGENT with Tree-Planner, another efficient task planning method, on the Tyreworld environment. The results are shown in Table 4.
与 Tree-Planner 的比较。我们还在 Tyreworld 环境中将 HIAGENT 与另一种高效任务规划方法 Tree-Planner 进行了比较。结果见表 4。


Table 4: Performance comparison between HIAGENT and Tree-Planner on Tyreworld.
表 4：HIAGENT 与 Tree-Planner 在 Tyreworld 上的性能比较。


<table><tr><td>Method</td><td>Success Rate</td><td>Context Efficiency</td></tr><tr><td>HIAGENT</td><td>60.0%</td><td>100.0%</td></tr><tr><td>Tree-Planner</td><td>40.0%</td><td>94.7%</td></tr></table>
<table><tbody><tr><td>方法</td><td>成功率</td><td>上下文效率</td></tr><tr><td>HIAGENT</td><td>60.0%</td><td>100.0%</td></tr><tr><td>树规划器</td><td>40.0%</td><td>94.7%</td></tr></tbody></table>


While Tree-Planner maintains relatively efficient context usage (94.7%), it falls short of HIAGENT's performance on long-horizon tasks. This performance gap can be attributed to our more effective hierarchical memory management approach, which enables better reasoning over extended action sequences.
尽管 Tree-Planner 在上下文使用上相对高效（94.7%），但在长时程任务上仍不及 HIAGENT。这一差距可归因于我们更有效的分层记忆管理方法，使在更长动作序列上的推理更为出色。


These comprehensive comparisons with state-of-the-art methods further validate the effectiveness of our approach in handling complex task planning scenarios.
与最先进方法的这些全面比较进一步验证了我们方法在处理复杂任务规划场景方面的有效性。