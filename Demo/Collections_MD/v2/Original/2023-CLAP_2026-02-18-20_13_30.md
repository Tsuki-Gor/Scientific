# CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts
# CLAP：通过增强提示的对比学习实现内容与风格的分离


Yichao Cai. Yuhang Liu..., Zhen Zhang. and Javen Qinfeng Shi.
Yichao Cai. Yuhang Liu..., Zhen Zhang. and Javen Qinfeng Shi.


Australian Institute for Machine Learning, University of Adelaide, SA 5000, Australia \{yichao.cai,yuhang.liu01,zhen.zhang02,javen.shi\}@adelaide.edu.au
Australian Institute for Machine Learning, University of Adelaide, SA 5000, Australia \{yichao.cai,yuhang.liu01,zhen.zhang02,javen.shi\}@adelaide.edu.au


Abstract. Contrastive vision-language models, such as CLIP, have garnered considerable attention for various dowmsteam tasks, mainly due to the remarkable ability of the learned features for generalization. However, the features they learned often blend content and style information, which somewhat limits their generalization capabilities under distribution shifts. To address this limitation, we adopt a causal generative perspective for multimodal data and propose contrastive learning with data augmentation to disentangle content features from the original representations. To achieve this, we begin with exploring image augmentation techniques and develop a method to seamlessly integrate them into pre-trained CLIP-like models to extract pure content features. Taking a step further, recognizing the inherent semantic richness and logical structure of text data, we explore the use of text augmentation to isolate latent content from style features. This enables CLIP-like model's encoders to concentrate on latent content information, refining the learned representations by pre-trained CLIP-like models. Our extensive experiments across diverse datasets demonstrate significant improvements in zero-shot and few-shot classification tasks, alongside enhanced robustness to various perturbations. These results underscore the effectiveness of our proposed methods in refining vision-language representations and advancing the state-of-the-art in multimodal learning. ${}^{1}$
摘要。对比视觉-语言模型，如 CLIP，因其学习到的特征在泛化方面的显著能力而受到广泛关注，尽管它们在跨分布情形下的泛化能力常被限于所学特征中混入的内容与风格信息。为解决这一限制，我们采用多模态数据的因果生成视角，并提出带数据增强的对比学习以从原始表示中解耦内容特征。为实现这一目标，我们首先探索图像增强技术，并开发一种方法，将其无缝整合到预训练的类似 CLIP 的模型中，以提取纯内容特征。进一步地，认识到文本数据的固有语义丰富性与逻辑结构，我们探索使用文本增强来从风格特征中分离潜在内容。这使得 CLIP 式模型的编码器能够专注于潜在内容信息，通过预训练的 CLIP 式模型来细化学习到的表示。我们在多样数据集上的大量实验显示，在零样本和小样本分类任务中显著提升，并增强对各种扰动的鲁棒性。这些结果强调了我们提出的方法在细化视觉-语言表示和推动多模态学习前沿方面的有效性。 ${}^{1}$


Keywords: Data Augmentation - Latent Variables - Disentanglement
关键词：数据增强 - 潜变量 - 解耦


## 1 Introduction
## 1 引言


Vision-language models, exemplified by CLIP 36, have garnered substantial attention due to their exceptional generalization capabilities, achieved through the learned features, obtained by utilizing a cross-modal contrastive loss [20, 25, 36]. However, despite being pre-trained on extensive datasets, CLIP-like models fall short in disentangling latent content information and latent style information. Consequently, they are not immune to spurious correlations, i.e., style-related information is erroneously utilized to predict task-related labels. These limitations become evident in the presence of distribution shifts or adversarial attacks where spurious correlations often change across different environments. For examples, (1) a notable dependence on specific input text prompts has been reported for zero-shot capabilities 21, 47, 48 ; (2) performance decline in few-shot scenarios has been observed in few-shot learning scenarios [13, 36]; and (3) susceptibility to adversarial attacks has been explored [33, 43, 45].
以 CLIP36 为代表的视觉-语言模型因其通过跨模态对比损失获得的出色泛化能力而受到广泛关注，这些能力来自学习到的特征 [20, 25, 36]。然而，尽管在大规模数据集上进行了预训练，CLIP 式模型在解耦潜在内容信息与潜在风格信息方面仍存在不足。因此，它们并非对抗潜在相关性的免疫体，即用于预测任务标签的可能是风格相关信息，而非实际信息。当分布发生偏移或遭遇对抗攻击时，这些伪相关性常常在不同环境中发生变化，相关问题就会显现。例如：(1) 已有报道指出零样本能力对特定输入文本提示高度依赖 [21, 47, 48]；(2) 少样本学习场景下性能下降已被观察到 [13, 36]；(3) 对对抗攻击的敏感性也被研究 [33, 43, 45]。


---



${}^{1}$ Our code is available at https://github.com/YichaoCai1/CLAP
${}^{1}$ 我们的代码可在 https://github.com/YichaoCai1/CLAP 获取


---



<img src="https://cdn.noedgeai.com/bo_d6aqku77aajc739ardu0_1.jpg?x=398&y=333&w=1017&h=416&r=0"/>



Fig.1: Causal generative models of vision-language data. Image and text data are generated through distinct underlying deterministic processes, ${\mathbf{g}}_{\mathbf{x}}$ for images and ${\mathbf{g}}_{\mathbf{t}}$ for texts,derived from a unified latent space with latent content variables $\mathbf{c}$ and latent style variables $\mathbf{s}$ . Latent content $\mathbf{c}$ exclusively determines the sample label $\mathbf{y}$ . (a) Soft interventions on latent style variables $\mathbf{s}$ result in $\widetilde{\mathbf{s}}$ ,subsequently generating augmented images $\widetilde{\mathbf{x}}$ . (b) Due to the same latent space,soft interventions on latent style variables $\mathbf{s}$ can also result in $\widetilde{\mathbf{s}}$ ,producing augmented text $\widetilde{\mathbf{t}}$ . (c) A qualitative comparison of image features for zero-shot classification using "a photo of a [class]" prompts, visualized using class activation map (CAM) 32, demonstrates that while image augmentation can enhance CLIP features, the features obtained through text augmentation methods predominantly focus on the content.
图1：视觉-语言数据的因果生成模型。图像和文本数据通过不同的确定性过程生成，${\mathbf{g}}_{\mathbf{x}}$ 对于图像、${\mathbf{g}}_{\mathbf{t}}$ 对于文本，源自具有潜在内容变量 $\mathbf{c}$ 和潜在风格变量 $\mathbf{s}$ 的统一潜在空间。潜在内容 $\mathbf{c}$ 唯一决定样本标签 $\mathbf{y}$ 。(a) 对潜在风格变量 $\mathbf{s}$ 进行软干预会得到 $\widetilde{\mathbf{s}}$，进而生成增强图像 $\widetilde{\mathbf{x}}$ 。(b) 由于同一潜在空间，对潜在风格变量 $\mathbf{s}$ 的软干预也可能导致 $\widetilde{\mathbf{s}}$，生成增强文本 $\widetilde{\mathbf{t}}$ 。(c) 对零样本分类中的“拍一张 [类别] 的照片”提示所得到的图像特征进行对比，可通过 CAM（类别激活图）可视化，表明尽管图像增强可以提升 CLIP 的特征，但通过文本增强方法获得的特征主要聚焦于内容。 


Taking a causal perspective, this work begin with a simple yet effective method, image augmentation, to disentangle content and style information within the learned representations of CLIP-like models. This approach is inspired by recent advancements in theoretical development in causal representation learning 41, which demonstrate that augmented image can be interpreted as a result of soft interventions on latent style variables, as depicted in Fig. 1a. Such augmentation results in a natural data pair where content information remains unchanged while style information changes. Consequently, using contrastive learning, it becomes feasible to isolate the invariant content information from the variant style information. Motivated by this theoretical advancement, we propose a practical method to incorporate image augmentation into CLIP-like models to extract content information from the original learned features. Specifically, a disentangled network is designed to fine-tune the pre-trained CLIP model by using a contrastive loss with image augmentation.
从因果的视角出发，本工作以简单而有效的方法——图像增强，来在类 CLIP 模型的学习表示中解耦内容信息与风格信息。这一做法受到了因果表征学习理论发展的启发41的最新进展的启发，理论上可将增强后的图像解释为对潜在风格变量的软干预结果，如图1a所示。这种增强会产生一个自然的数据对，其中内容信息保持不变，而风格信息发生变化。因此，借助对比学习，能够从不变的内容信息与变化的风格信息之间分离出来。受这一理论进展的启发，我们提出一种实用方法，将图像增强引入类 CLIP 模型，以从原始学习特征中提取内容信息。具体地，设计了一种解耦网络，通过使用带有图像增强的对比损失，来微调预训练的 CLIP 模型。


Despite the advancements made in disentangling content and style information from the original features learned by CLIP-like models through image augmentation, we recognize an inherent limitation: it is generally challenging to design adequate image augmentations to ensure all style factors change in an image. Theoretically, disentangling content and style information necessitates changes in all style factors 41 . However, inducing sufficient changes in latent style through image augmentation poses challenges due to the high dimensionality and complexity of style information in image data. Achieving significant style variation via artificially designed image augmentation techniques, such as transforming a photograph of a dog into a sketch while preserving content but dramatically altering style, is notably difficult.
尽管通过图像增强在原始特征中解耦内容与风格信息取得了进展，我们也认识到一个固有局限：通常很难设计出足够良好的图像增强，使图像中所有风格因素都发生变化。从理论上讲，解耦内容与风格信息需要所有风格因素发生变化41。然而，通过图像增强来引入潜在风格的充分变化，在图像数据的高维性与风格信息的复杂性下仍然具有挑战性。通过人为设计的增强技术（如将狗的照片转化为草图，同时保持内容不变但显著改变风格）来实现显著的风格变异尤其困难。


Taking a further step, rather than relying on image augmentation, we explore the use of text augmentation to disentangle latent content and style factors. This shift is motivated by two key observations: 1) Vision and language data share the same latent space. Therefore, text augmentation can also be utilized to induce changes in latent style factors instead of image augmentation. 2) Text data inherently possesses high semanticity and logical structure, making it more amenable to property-wise manipulation compared to image data. Consequently, implementing sufficient style changes through text augmentation is more feasible than image augmentation, contributing to isolating content from style information, see Fig. 1c for visual comparison. For instance, transforming text from "a photo of a dog" to "a sketch of a dog" is straightforward in the language modality, whereas achieving a similar transformation in image data is challenging. Inspired by these observations, we posit that introducing style variations through text augmentation, as illustrated Fig. 1b, provides a more effective approach for learning vision-language content features than relying on image augmentation.
更进一步地，我们不再仅依赖图像增强，而是探索使用文本增强来解耦潜在的内容与风格因素。这一转变的动机来自两个关键观察：1）视觉与语言数据共享同一潜在空间。因此，文本增强也可用于引入潜在风格因素的变化，而非仅依赖图像增强。2）文本数据本身具有高度语义性和逻辑结构，相较于图像数据更易进行属性级操作。因此，通过文本增强实现足够的风格变化比通过图像增强更可行，有助于将内容从风格信息中分离，见图1c的直观对比。例如，将文本从“a photo of a dog”转换为“a sketch of a dog”在语言模态中是简单直接的，而在图像数据中实现类似变换则颇具挑战。受到这些观察的启发，我们提出通过文本增强引入风格变异，如图1b所示，提供比依赖图像增强更有效的学习视觉-语言内容特征的方法。


In summary, our contributions include: (1) Aimed at disentangling latent content and style factors to refine vision-language features of pre-trained CLIP-like models, we propose constrastive learning with data augmentation to fine tune the original features of pre-trained CLIP-like models from a causal perspective. (2) We present a novel method customized for pre-trained CLIP-like models. This method leverages a disentangled network, which is trained using contrastive learning with image augmentation, to extract latent content features from the learned features provided by image encoder of CLIP-like models. (3) We propose Contrastive Learning with Augmented Prompts (CLAP), to extract latent content features from representations of CLIP-like models. It begins by training a disentangled network using the pre-trained text encoder of CLIP-like models and text augmentation. Subsequently, the trained disentangled network is transferred to the image encoder of CLIP-like models. (4) Experiments conducted on a large real dataset demonstrate the effectiveness of the proposed image augmentation and text augmentation in terms of zero-shot and few-shot performance, as well as robustness against perturbations.
总之，我们的贡献包括：(1) 针对解耦潜在内容与风格因素，以优化预训练类 CLIP 模型的视觉-语言特征，我们提出从因果视角出发、结合数据增强的对比学习来微调原始特征的方法。 (2) 提出了一种面向预训练类 CLIP 模型的全新方法。该方法利用一个解耦网络，使用带图像增强的对比学习来从类 CLIP 模型图像编码器学习得到的学习特征中提取潜在内容特征。 (3) 提出“带增强提示的对比学习”（CLAP），用于从类 CLIP 模型的表示中提取潜在内容特征。其过程先用预训练的文本编码器以及文本增强对解耦网络进行训练，随后将训练好的解耦网络转移到类 CLIP 模型的图像编码器。 (4) 在大规模真实数据集上进行的实验展示了所提图像增强与文本增强在零样本与小样本性能以及对扰动的鲁棒性方面的有效性。


## 2 Related Work
## 2 相关工作


Contrastive Vision-Language Models Using a cross-modal contrastive loss, CLIP 36 revolutionarily introduced a scalable contrastive vision-language model by leveraging a large corpus of internet-sourced image-text pairs, demonstrating unprecedented zero-shot learning capabilities and exceptional generalization ability across datasets and supporting numerous downstream tasks 38. ALIGN 20 expanded the scale of contrastive vision-language modeling, training on up to one billion image-text pairs while integrating the vision transformer's self-attention mechanism [11], which further enhanced performance. Despite their successes, CLIP-like models exhibit sensitivity to input text prompts [21, 48], leading to variable performance across different prompts. Efforts to mitigate this prompt sensitivity through prompt learning and engineering [9, 14, 21, 47, 48] focus on customizing prompts for specific tasks but do not fundamentally enhance CLIP's representations. Furthermore, CLIP-like models are vulnerable to adversarial attacks [4, 12], with current strategies [33, 45] involving adversarial-natural image pairs to improve resilience. Our work diverges from task-specific approaches by aiming to enhance CLIP's representations from a disentanglement perspective, addressing the aforementioned issues inherent in CLIP-like models.
对比视觉-语言模型通过跨模态对比损失，CLIP 36 开创性地引入了一个可扩展的对比视觉-语言模型，利用大规模的互联网数据获取的图文对，展现出前所未有的零样本学习能力和在各数据集上的优越泛化能力，并支持大量后续任务38。ALIGN 20 将对比视觉-语言建模的规模扩展到最多十亿对图文数据，同时整合了视觉变换器的自注意力机制[11]，进一步提升了表现。尽管取得了成功，类似 CLIP 的模型对输入文本提示具有敏感性[21, 48]，导致在不同提示下性能不稳定。通过提示学习与工程[9, 14, 21, 47, 48]来缓解这类提示敏感性的努力，聚焦于为特定任务定制提示，但并未从根本上提升 CLIP 的表征能力。此外，CLIP 如型的模型也易受到对抗性攻击的影响[4, 12]，现有策略[33, 45]涉及对抗性-自然图像对以提高鲁棒性。我们的工作不同于面向任务特定的方法，旨在从解耦视角增强 CLIP 的表征，解决上述由 CLIP 类模型固有的问题。


Disentangled Representation Learning Aimed at segregating intrinsic latent factors in data into distinct, controllable representations, disentangled representation learning benefits various applications 24 40 44. Specifically, in classification tasks, it's shown that enhancing the model's performance and robustness against data distribution perturbations can be achieved by more effectively disentangling invariant content variables, without needing to identify all intrinsic latent variables completely [22, 26-28]. Within single modalities, studies such as [49] have illustrated that contrastive learning [7, 16, 18] can potentially reverse the data generative process, aiding in the separation of representations. Furthermore, 41 suggest that image augmentation can help isolate content variables from the latent space through significant stylistic changes. 19 employs mixture techniques for data augmentation, enabling more abundant cross-modal matches. Diverging from these methods, our approach focuses on employing text augmentation to disentangle latent content variables, introducing a unique approach to learn refined vision-language representations.
可解耦表示学习旨在将数据中的固有潜在因素分离成不同、可控的表示，可解耦表示学习对诸多应用具有收益 24 40 44。具体而言，在分类任务中，已显示通过更有效地将不变内容变量解耦来提升模型性能与对数据分布扰动的鲁棒性，而无需完全识别所有固有潜在变量 [22, 26-28]。在单一模态内，研究如 [49] 表明对比学习 [7, 16, 18] 有可能逆转数据生成过程，帮助分离表示。此外，41 指出图像增强可以通过显著的风格变化来从潜在空间中分离内容变量。19 采用混合技术进行数据增强，实现更丰富的跨模态匹配。区别于这些方法，我们的方法专注于通过文本增强来解耦潜在内容变量，给学习精细化的视觉-语言表示带来独特的研究路径。


## 3 A Causal Generative Model for Multi-Modal Data
## 3 面向多模态数据的因果生成模型


To understand pretrained CLIP-like models, we investigate the underlying causal generative process for vision-language data. We consider the following causal generative model as depicted in Fig. 1. In the proposed model, the shared latent space ruling vision and language data is divided into two distinct sub-spaces: one corresponding to the latent content variables $\mathbf{c}$ and the other to the latent style variables $\mathbf{s}$ . The latent content variables are posited to determine the object label $\mathbf{y}$ ,a relationship corroborated by prior studies 22,29,31. Furthermore,to elucidate the correlation between the latent style variable $\mathbf{s}$ and the object variable $\mathbf{y}$ ,our model incorporates the premise that the latent content variable $\mathbf{c}$ causally influences the latent style variable $\mathbf{s}$ ,in concordance with the principles of causal representation learning highlighted in recent literature 10, 29, 41. Additionally, considering the diversity between image data and text data, where information in image data is typically much more details while information in text data tends to be more logically structured nature, we posit distinct causal mechanisms for the generation processes. Our causal generative model is formulated as following structural causal models 2:
为了理解预训练的类似 CLIP 的模型，我们研究视觉-语言数据背后的因果生成过程。我们将下述因果生成模型作为图 1 所示的框架。在所提出的模型中，统治视觉和语言数据的共享潜在空间被分割为两个不同的子空间：一个对应潜在内容变量 $\mathbf{c}$，另一个对应潜在风格变量 $\mathbf{s}$ 。潜在内容变量被假设决定对象标签 $\mathbf{y}$，这一关系在既有研究 22,29,31 中得到证实。此外，为阐明潜在风格变量 $\mathbf{s}$ 与对象变量 $\mathbf{y}$ 之间的相关性，我们的模型包含了潜在内容变量 $\mathbf{c}$ 因果影响潜在风格变量 $\mathbf{s}$ 的前提，与最近文献中强调的因果表示学习原则相一致 10, 29, 41。再者，考虑到图像数据与文本数据在信息上的差异——图像信息通常更为细节化，而文本信息往往更具逻辑结构——我们提出对生成过程采用不同的因果机制。我们的因果生成模型按下列结构化因果模型形式化 2：


$$
\mathbf{s} \mathrel{\text{ := }} {\mathbf{g}}_{\mathbf{s}}\left( \mathbf{c}\right) ,\mathbf{x} \mathrel{\text{ := }} {\mathbf{g}}_{\mathbf{x}}\left( {\mathbf{c},\mathbf{s}}\right) ,\mathbf{t} \mathrel{\text{ := }} {\mathbf{g}}_{\mathbf{t}}\left( {\mathbf{c},\mathbf{s}}\right) ,\mathbf{y} \mathrel{\text{ := }} {\mathbf{g}}_{\mathbf{y}}\left( \mathbf{c}\right) . \tag{1}
$$



In Eq. 1),the style variable $\mathbf{s}$ is causally influenced by the content via ${\mathbf{g}}_{\mathbf{s}};\mathbf{x}$ and $\mathbf{t}$ denote visual and textual data,respectively. Both visual and textual data are causally produced by the shared latent variables $\mathbf{c}$ and $\mathbf{s}$ through distinct, reversible generative processes: ${\mathbf{g}}_{\mathbf{x}}$ for images and ${\mathbf{g}}_{\mathbf{t}}$ for text data,respectively. The label $\mathbf{y}$ of a sample is exclusively determined by the content variable $\mathbf{c}$ via ${\mathbf{g}}_{\mathbf{y}}$ .For simplicity,exogenous noises are implicitly assumed but not explicitly represented in the causal generative model's formulation, aligning with the common understanding that each latent variable is influenced by exogenous noise.
在式 1 中，风格变量 $\mathbf{s}$ 受到内容通过 ${\mathbf{g}}_{\mathbf{s}};\mathbf{x}$ 的因果影响，$\mathbf{t}$ 表示视觉和文本数据，分别。视觉和文本数据均由共享潜在变量 $\mathbf{c}$ 和 $\mathbf{s}$ 通过不同、可逆的生成过程因果地产生：图像用 ${\mathbf{g}}_{\mathbf{x}}$，文本数据用 ${\mathbf{g}}_{\mathbf{t}}$。样本的标签 $\mathbf{y}$ 仅由内容变量 $\mathbf{c}$ 通过 ${\mathbf{g}}_{\mathbf{y}}$ 决定。为简单起见，外源噪声在因果生成模型的表述中隐式假设存在但未显式呈现，符合每个潜在变量受外源噪声影响的普遍理解。


Recent seminal work in 41 has demonstrated that the latent content variable $\mathbf{c}$ can be identified up to block identifiability (i.e., $\mathbf{c}$ can be isolated from style variable $\mathbf{s}$ ),by requiring all latent style variables to change (e.g.,soft interventions on all latent style variables). This change can be achieved through image augmentation,i.e.,the augmented image $\widetilde{\mathbf{x}}$ can be interpreted as a generative result of $\widetilde{\mathbf{s}}$ ,which is produced through soft interventions on original latent style variables s. Despite such theoretical advancement, the practical implementation of this theoretical result within CLIP-like models remains unclear. In this study, we propose a practical method to disentangle content and style information within CLIP-like models by employing image augmentation, as detailed in Sec. 4.1. Moreover, we recognize that implementing sufficient changes on all latent style variables $\mathbf{s}$ through text augmentation is more feasible than image augmentation, due to high semanticity and logical structure in text data, we delve into the use of text augmentation to separate content information from style information, as discussed in Sec. 4.2
最近在 41 的开创性工作表明，潜在内容变量 $\mathbf{c}$ 可以在块可识别性范围内被识别（即 $\mathbf{c}$ 可以从风格变量 $\mathbf{s}$ 中分离出来），前提是要求所有潜在风格变量发生变化（例如对所有潜在风格变量进行软干预）。这种变化可以通过图像增强实现，即增强后的图像 $\widetilde{\mathbf{x}}$ 可以被解释为 $\widetilde{\mathbf{s}}$ 的生成结果，而 $\widetilde{\mathbf{s}}$ 是通过对原始潜在风格变量进行软干预得到的。尽管有这样的理论进展，但在 CLIP 类模型中实际落地仍不清晰。本研究提出通过使用图像增强来实现对 CLIP-类模型中内容与风格信息的解耦的实用方法，详见第 4.1 节。此外，鉴于文本数据的高语义性和逻辑结构，我们认为通过文本增强实现对所有潜在风格变量 $\mathbf{s}$ 的充分变化比图像增强更可行，因此我们将深入探讨利用文本增强来分离内容信息与风格信息，详见第 4.2 节。


## 4 Isolating Content from Style with Data Augmentation
## 4 通过数据增强实现内容与风格的分离


In this section, we propose the employment of data augmentation to extract content information from the learned features in pre-trained CLIP-like models. Essentially, data augmentation facilitates the alteration of style factors while preserving content factors. Consequently, leveraging contrastive learning enables the segregation of content information from style information. We delve into two distinct forms of data augmentation, namely image augmentation (Sec. 4.1) and text augmentation (Sec. 4.2).
在本节中，我们提出使用数据增强来从预训练的 CLIP 风格模型中学习到的特征中提取内容信息。本质上，数据增强有助于在保留内容因素的同时改变风格因素。因此，利用对比学习可以实现将内容信息从风格信息中分离。我们探讨两种不同形式的数据增强，即图像增强（Sec. 4.1）和文本增强（Sec. 4.2）。


### 4.1 Isolating Content from Style with Augmented Images
### 4.1 用增强图像实现内容与风格的分离


While recent studies (von et al., 2021) have offered assurance regarding the disentanglement of content and style through contrastive learning with data augmentation, it remains unclear how these theoretical findings can be applied to the realm of vision-language models. We convert the theoretical findings into CLIP-like models in the following. The theoretical findings suggest using In-foNCE loss 34 to extract content information, as outlined below:
尽管最近的研究（von 等，2021）已经就通过对比学习结合数据增强来实现内容与风格的解耦提供了保证，但仍不清楚这些理论发现如何应用于视觉-语言模型的领域。我们将在下文将理论发现转化为类似 CLIP 的模型。理论发现表明可使用 In-foNCE 损失 34 来提取内容信息，如下所述：


$$
\mathcal{L}\left( {\mathbf{f};{\left\{  {\mathbf{x}}_{i},{\widetilde{\mathbf{x}}}_{i}\right\}  }_{i = 1}^{b},\tau }\right)  =  - \frac{1}{b}\mathop{\sum }\limits_{{i = 1}}^{b}\log \frac{\exp \left\lbrack  {\left\langle  {\mathbf{f}\left( {\mathbf{x}}_{i}\right) ,\mathbf{f}\left( {\widetilde{\mathbf{x}}}_{i}\right) }\right\rangle  /\tau }\right\rbrack  }{\mathop{\sum }\limits_{{j = 1}}^{b}\exp \left\lbrack  {\left\langle  {\mathbf{f}\left( {\mathbf{x}}_{i}\right) ,\mathbf{f}\left( {\widetilde{\mathbf{x}}}_{j}\right) }\right\rangle  /\tau }\right\rbrack  }, \tag{2}
$$



<img src="https://cdn.noedgeai.com/bo_d6aqku77aajc739ardu0_5.jpg?x=397&y=333&w=1016&h=238&r=0"/>



Fig. 2: Refining CLIP through data augmentation. (a) Training involves a disentangled network ${\mathbf{f}}_{\mathbf{c}}$ ,utilizing contrastive loss on original and augmented image pairs $\mathbf{x}$ and $\widetilde{\mathbf{x}}$ , with CLIP's image encoder ${\mathbf{f}}_{\mathbf{x}}^{ * }$ holding frozen gradients. (b) More efficient content feature learning is achieved through contrastive learning with augmented text prompts $\mathbf{t}$ and $\widetilde{\mathbf{t}}$ ,using the fixed text encoder ${\mathbf{f}}_{\mathbf{t}}^{ * }$ of CLIP. (c) Inference stage: The trained disentangled network ${\mathbf{f}}_{\mathbf{c}}^{ * }$ integrates with CLIP’s text and image encoders, ${\mathbf{f}}_{\mathbf{t}}^{ * }$ and ${\mathbf{f}}_{\mathbf{x}}^{ * }$ ,to enable zero-shot inference for an input image $\mathbf{x}$ and class names ${\mathbf{t}}_{1}$ to ${\mathbf{t}}_{n}$ .
图 2：通过数据增强对 CLIP 进行优化。 (a) 训练包括一个解耦的网络 ${\mathbf{f}}_{\mathbf{c}}$，对原始图像对 $\mathbf{x}$ 和增强图像对 $\widetilde{\mathbf{x}}$ 进行对比损失，且 CLIP 的图像编码器 ${\mathbf{f}}_{\mathbf{x}}^{ * }$ 保持梯度冻结。 (b) 通过对比学习结合增强文本提示 $\mathbf{t}$ 和 $\widetilde{\mathbf{t}}$，并使用 CLIP 的固定文本编码器 ${\mathbf{f}}_{\mathbf{t}}^{ * }$，实现更高效的内容特征学习。 (c) 推理阶段：训练好的解耦网络 ${\mathbf{f}}_{\mathbf{c}}^{ * }$ 与 CLIP 的文本和图像编码器 ${\mathbf{f}}_{\mathbf{t}}^{ * }$ 和 ${\mathbf{f}}_{\mathbf{x}}^{ * }$ 集成，以实现对输入图像 $\mathbf{x}$ 和类别名称 ${\mathbf{t}}_{1}$ 到 ${\mathbf{t}}_{n}$ 的零样本推理。


where ${\left\{  {\mathbf{x}}_{i}\right\}  }_{i = 1}^{b}$ represents a batch of $b$ samples from the training dataset, $\mathbf{f}\left( {\mathbf{x}}_{i}\right)$ denotes sample ${\mathbf{x}}_{i}$ ’s features through model $\mathbf{f},{\widetilde{\mathbf{x}}}_{i}$ is the augmented counterpart of ${\mathbf{x}}_{i}$ ,and $\left\langle  {{\mathbf{z}}_{1},{\mathbf{z}}_{2}}\right\rangle$ represents the cosine similarity between two feature vectors, ${\mathbf{z}}_{1}$ and ${\mathbf{z}}_{2}$ ,and $\tau$ represents the temperature parameter influencing the loss.
其中 ${\left\{  {\mathbf{x}}_{i}\right\}  }_{i = 1}^{b}$ 表示训练数据集的一批 $b$ 样本，$\mathbf{f}\left( {\mathbf{x}}_{i}\right)$ 表示通过模型 $\mathbf{f},{\widetilde{\mathbf{x}}}_{i}$ 对样本 ${\mathbf{x}}_{i}$ 的特征提取得到的增强对应版本；$\left\langle  {{\mathbf{z}}_{1},{\mathbf{z}}_{2}}\right\rangle$ 表示两个特征向量之间的余弦相似度，${\mathbf{z}}_{1}$ 和 ${\mathbf{z}}_{2}$；$\tau$ 表示影响损失的温度参数。


We extend it to refine pre-trained vision-language models, utilizing contrastive learning with augmented images (hereinafter referred to as "Im.Aug"). As illustrated in Fig. 2a, we train a disentangled network on top of CLIP's pre-trained image encoder. To enhance training efficiency and the usability of the proposed method, we freeze the pre-trained image encoder. Based on an InfoNCE loss, the learning objective of Im.Aug is formulated as follows:
我们将其扩展用于细化预训练的视觉-语言模型，利用带增强图像的对比学习（以下称为“Im.Aug”）。如图 2a 所示，我们在 CLIP 的预训练图像编码器之上训练一个解耦网络。为提升训练效率和所提方法的可用性，我们对预训练图像编码器进行冻结。基于 InfoNCE 损失，Im.Aug 的学习目标可公式化如下：


$$
{\mathbf{f}}_{\mathbf{c}}^{ * } = \mathop{\operatorname{argmin}}\limits_{{\mathbf{f}}_{\mathbf{c}}}\underset{{\left\{  {\mathbf{x}}_{i}\right\}  }_{i = 1}^{b} \in  {\mathcal{D}}_{\mathbf{x}}}{\mathbb{E}}\mathcal{L}\left( {{\mathbf{f}}_{\mathbf{c}} \circ  {\mathbf{f}}_{\mathbf{x}}^{ * };{\left\{  {\mathbf{x}}_{i},{\widetilde{\mathbf{x}}}_{i}\right\}  }_{i = 1}^{b},\tau }\right) , \tag{3}
$$



where ${\mathcal{D}}_{\mathbf{x}}$ denotes the training image dataset and $b$ represents the batch size, ${\mathbf{f}}_{\mathbf{c}}$ is the disentangled network undergoing training. The pre-trained CLIP image encoder is represented by ${\mathbf{f}}_{\mathbf{x}}^{ * }$ ,with the asterisk "*" signifying that the model weights remain fixed. The variable ${\mathbf{x}}_{i}$ refers to an image sampled from ${\mathcal{D}}_{\mathbf{x}}$ ,and ${\widetilde{\mathbf{x}}}_{i}$ is its augmented view.
其中 ${\mathcal{D}}_{\mathbf{x}}$ 表示训练图像数据集，$b$ 表示批量大小，${\mathbf{f}}_{\mathbf{c}}$ 为正在训练的解耦网络。预训练的 CLIP 图像编码器由 ${\mathbf{f}}_{\mathbf{x}}^{ * }$ 表示，星号“*”表示模型权重保持固定。变量 ${\mathbf{x}}_{i}$ 指从 ${\mathcal{D}}_{\mathbf{x}}$ 采样的一张图像，${\widetilde{\mathbf{x}}}_{i}$ 是其增强视图。


The composition of the training dataset ${\mathcal{D}}_{\mathbf{x}}$ ,the image augmentation techniques used,the structure of the disentangled network ${\mathbf{f}}_{\mathbf{c}}$ ,and the utilization of ${\mathbf{f}}_{\mathbf{c}}^{ * }$ post-training are detailed in the following subsections.
训练数据集的组成 ${\mathcal{D}}_{\mathbf{x}}$、所使用的图像增强技术、解耦网络的结构 ${\mathbf{f}}_{\mathbf{c}}$、以及训练后对 ${\mathbf{f}}_{\mathbf{c}}^{ * }$ 的利用，详见下述子章节。


Data Synthesis and Image Augmentation To generate training image data, we combine class names with various image and object attributes to create text prompts for each class. Using a stable diffusion model 39, we produce synthetic images that comprise our training dataset $\mathcal{D}\mathbf{x}$ . The creation of template prompts for stable diffusion is based on attributes such as object size, color, image type, and art style. As detailed in Tab. 1, the attributes include 10 colors and 3 sizes for objects, and 8 types and 2 art styles for images. By assembling these attributes into prompts like "a [art style] [image type] of a [object size] [object color] [class]", we generate 480 unique texts for each class, from which one image per prompt is synthesized. Further details on image synthesis and examples are available in Appendix B.1. For the image augmentation procedures, we adopt techniques commonly used in contrastive learning practice [7, 8, 41, specifically random cropping and color distortion.
数据合成与图像增强 为生成训练图像数据，我们将类别名与各种图像及对象属性结合，创建每个类别的文本提示。使用稳定扩散模型 39，我们生成包含训练数据集 $\mathcal{D}\mathbf{x}$ 的合成图像。稳定扩散的模板提示的创建基于对象大小、颜色、图像类型和艺术风格等属性。如表1所述，属性包括对象的 10 种颜色和 3 种尺寸，以及图像的 8 种类型和 2 种艺术风格。通过将这些属性拼接成提示，如 "一个 [艺术风格] [图像类型] 的 [对象尺寸] [对象颜色] [类别]"，我们为每个类别生成 480 条唯一文本，从中对每条文本合成一张图像。关于图像合成的更多细节与示例，请参见附录 B.1。关于图像增强程序，我们采用对比学习实践中常用的技术 [7, 8, 41]，具体为随机裁剪和颜色扭曲。


Table 1: Template-based prompts. Attributes used to generate text prompts follow the structured format "a [art style] [image type] of a [object size] [object color] [class]", where "[class]" represents the class names.
表1：基于模板的提示。用于生成文本提示的属性遵循结构化格式 "一个 [艺术风格] [图像类型] 的 [对象尺寸] [对象颜色] [类别]"，其中 "[类别]" 表示类别名称。


<table><tr><td>Object Color</td><td>Object Size</td><td>Image Type</td><td>Art Style</td></tr><tr><td>yellow, green, black, blue, multicolored, orange. red, white, brown, purple</td><td>large, small, normal sized</td><td>painting, cartoon, infograph, sketch, photograph, clipart, mosaic art, sculpture</td><td>realistic, impressionistic</td></tr></table>
<table><tbody><tr><td>对象颜色</td><td>对象尺寸</td><td>图像类型</td><td>艺术风格</td></tr><tr><td>黄色、绿色、黑色、蓝色、彩色、橙色。 红色、白色、棕色、紫色</td><td>大、小、正常尺寸</td><td>绘画、卡通、信息图、草图、照片、剪贴画、马赛克艺术、雕塑</td><td>写实、印象派</td></tr></tbody></table>


Disentangled Network Structure Since the training process is based on CLIP's pre-trained lower-dimensional features, our disentangled network adopts a multi-layer perceptron (MLP) architecture. To fully benefit from the pre-trained CLIP text encoder, we construct a residual MLP featuring a zero-initialized projection, acting as the disentangled network, as depicted in Fig. 3. This design enables learning directly from the pre-trained representation space, avoiding a random starting point, inspired by ControlNet's zero-conv operation [46], which we adapt to a zero-linear operation within our residual MLP.
解耦网络结构 由于训练过程基于 CLIP 的预训练低维特征，我们的解耦网络采用多层感知机（MLP）架构。为了充分利用预训练的 CLIP 文本编码器，我们构建了一个带有零初始化投影的残差 MLP，作为解耦网络，如图 3所示。这一设计使得能够直接从预训练的表示空间学习，避免从随机起点开始，灵感来自 ControlNet 的零卷积操作 [46]，我们将其改为在我们的残差 MLP 中的零线性操作。


Within this architecture, the main branch includes a zero-initialized, bias-free linear layer positioned subsequent to the combination of a SiLU activation and a normally initialized linear layer. Conventionally, the dimensions of features before the initial linear layer, situated between the first and second linear layers,and following the second linear layer,are named as the input ${d}_{in}$ ,latent ${d}_{\text{ mid }}$ ,and output ${d}_{\text{ out }}$ dimensions,respectively. To rectify any mismatches between the input and output dimensions, the network employs nearest-neighbor downsampling within the shortcut path, thereby ensuring both alignment and the preservation of sharpness for the input features. During the inference stage, a weighting parameter $\alpha  > 0$ is introduced to modulate the portion of features emanating from the main branch before their integration with the input features, whereas this parameter remains constant at 1 throughout the training phase.
在该架构中，主分支包含一个零初始化、无偏置的线性层，位于将 SiLU 激活与正常初始化的线性层结合之后。通常，在第一条线性层、第一和第二线性层之间，以及第二条线性层之后的特征维度，分别被称为输入 ${d}_{in}$、潜在 ${d}_{\text{ mid }}$ 与输出 ${d}_{\text{ out }}$ 维度。为纠正输入与输出维度之间的不匹配，网络在快捷路径中采用最近邻下采样，从而实现对齐并保持输入特征的清晰度。在推理阶段，加入一个权重参数 $\alpha  > 0$ 用于调节主分支输出特征在与输入特征融合前的比例，而在训练阶段该参数恒定为 1。


Inference After training,the disentangled network ${\mathbf{f}}_{\mathbf{c}}^{ * }$ is utilized following CLIP's image encoder to extract visual content features. Moreover, given that vision-language data generation is rooted in a unified latent space, as depicted in Sec. 3 , ${\mathbf{f}}_{\mathrm{c}}^{ * }$ can be seamlessly integrated with CLIP’s image and text encoders to enhance zero-shot capabilities. As shown in Fig. 2c,for an image $\mathbf{x}$ ,the operation is formulated as the composition function ${\mathbf{f}}_{\mathbf{c}}^{ * } \circ  \overline{{\mathbf{f}}_{\mathbf{x}}^{ * }}\left( \mathbf{x}\right)$ ,and similarly,for a text t,as ${\mathbf{f}}_{\mathbf{c}}^{ * } \circ  {\mathbf{f}}_{\mathbf{t}}^{ * }\left( \mathbf{t}\right)$ . This integration preserves CLIP’s zero-shot functionality while achieving refined features through the improved disentanglement of content.
推理 训练后，解耦网络 ${\mathbf{f}}_{\mathbf{c}}^{ * }$ 在 CLIP 的图像编码器之后用于提取视觉内容特征。此外，鉴于视觉-语言数据生成基于统一的潜在空间，如第 3 节所示，${\mathbf{f}}_{\mathrm{c}}^{ * }$ 可以与 CLIP 的图像和文本编码器无缝整合，以提升零-shot 能力。如图 2c 所示，对于一张图像 $\mathbf{x}$，该操作被表达为组合函数 ${\mathbf{f}}_{\mathbf{c}}^{ * } \circ  \overline{{\mathbf{f}}_{\mathbf{x}}^{ * }}\left( \mathbf{x}\right)$，同样地，对于文本 t，则为 ${\mathbf{f}}_{\mathbf{c}}^{ * } \circ  {\mathbf{f}}_{\mathbf{t}}^{ * }\left( \mathbf{t}\right)$。这种整合在保持 CLIP 的零-shot 功能的同时，通过改进内容的解耦实现更精细的特征。


<img src="https://cdn.noedgeai.com/bo_d6aqku77aajc739ardu0_7.jpg?x=612&y=332&w=584&h=246&r=0"/>



Fig. 3: Structure of the disentangled network. The architecture encompass a residual block featuring a zero-initialized, bias-free linear layer to commence optimization from the input feature space. When the input and output dimension differ, a downsampling operation is utilized to achieve alignment. During inference,a scalar parameter $\alpha$ balance the main branch and input features before combination.
图 3：解耦网络的结构。该结构包含一个带有零初始化、无偏置线性层的残差块，以从输入特征空间开始优化。当输入与输出维度不同时，使用下采样操作来实现对齐。在推理阶段，一个标量参数 $\alpha$ 在组合前平衡主分支与输入特征。


### 4.2 Isolating Content from Style with Augmented Prompts
### 4.2 通过增强提示将内容与风格分离


Despite progress in disentangling content and style via image augmentation, adequately altering all style factors in an image remains challenging due to the high dimensionality and complexity of style information in images. Achieving substantial style changes through augmentation, essential for complete disentanglement 41, is difficult with existing image augmentation techniques. On the contrary, text data inherently possesses high semanticity and logical structure, making it more amenable to property-wise manipulation compared to image data. To further exploring the disentanglement of content, we propose Contrastive Learning with Augmented Prompts (CLAP).
尽管通过图像增强在分离内容与风格方面取得了进展，但由于图像中风格信息的高维性与复杂性，充分改变图像中的所有风格因子仍然具有挑战性。通过增强实现显著的风格变化、从而实现完全的内容与风格分离是困难的，现有的图像增强技术难以实现这一目标。相反，文本数据本身具有高度语义性和逻辑结构，使其在属性级操作方面比图像数据更易处理。为了进一步探索内容的解耦，我们提出带增强提示的对比学习（CLAP）。


As depicted in Fig. 2b, CLAP employs an InfoNCE loss to train a disentangled network atop CLIP's pre-trained text encoder, keeping the encoder's gradients fixed, similar to Im.Aug. Leveraging the simpler structure of text, the template-based prompts previously utilized for synthesizing images now serve as the training text dataset,denoted by ${\mathcal{D}}_{\mathbf{t}}$ . Utilizing the same disentangled network as in Im. Aug, the learning objective of CLAP is outlined as follows:
如图 2b 所示，CLAP 采用 InfoNCE 损失，在 CLIP 的预训练文本编码器之上训练一个解耦网络，并保持编码器的梯度固定，类似于 Im.Aug。利用文本的更简单结构，之前用于合成图像的模板化提示现在作为训练文本数据集，由 ${\mathcal{D}}_{\mathbf{t}}$ 表示。使用与 Im. Aug 相同的解耦网络，CLAP 的学习目标如下：


$$
{\mathbf{f}}_{\mathbf{c}}^{ * } = \mathop{\operatorname{argmin}}\limits_{{\mathbf{f}}_{\mathbf{c}}}\underset{{\left\{  {\mathbf{t}}_{i}\right\}  }_{i = 1}^{b} \in  {\mathcal{D}}_{\mathbf{t}}}{\mathbb{E}}\mathcal{L}\left( {{\mathbf{f}}_{\mathbf{c}} \circ  {\mathbf{f}}_{\mathbf{t}}^{ * };{\left\{  {\mathbf{t}}_{i},{\widetilde{\mathbf{t}}}_{i}\right\}  }_{i = 1}^{b},\tau }\right)  + \lambda \mathcal{L}\left( {{\mathbf{f}}_{\mathbf{c}} \circ  {\mathbf{f}}_{\mathbf{t}}^{ * };{\left\{  {\mathbf{t}}_{i}^{c},{\widetilde{\mathbf{t}}}_{i}\right\}  }_{i = 1}^{b},1}\right) , \tag{4}
$$



where ${\mathbf{f}}_{\mathbf{t}}^{ * }$ denotes the pre-trained CLIP text encoder. The term ${\mathbf{t}}_{i}$ references a text prompt from $\mathcal{D}\mathbf{t}$ ,and ${\widetilde{\mathbf{t}}}_{i}$ represents its augmented view,produced via prompt augmentation techniques. On the equation’s right side, ${\mathbf{t}}_{i}^{c}$ specifies the class name associated with the text prompt ${\mathbf{t}}_{i}$ . This strategy aims to enhance variations between prompt pairs,especially in cases where the text dataset ${\mathcal{D}}_{\mathbf{t}}$ has a very limited number of samples. Here, $\lambda$ serves for adjusting the second term's importance in the total loss function. All other symbols in Eq. 4 maintain their definitions as described earlier.
其中 ${\mathbf{f}}_{\mathbf{t}}^{ * }$ 表示预训练的 CLIP 文本编码器。术语 ${\mathbf{t}}_{i}$ 指代来自 $\mathcal{D}\mathbf{t}$ 的文本提示，而 ${\widetilde{\mathbf{t}}}_{i}$ 表示其通过提示增强技术得到的增强视图。在等式右侧，${\mathbf{t}}_{i}^{c}$ 指定与文本提示 ${\mathbf{t}}_{i}$ 相关的类别名称。该策略旨在增强提示对之间的变异，尤其在文本数据集 ${\mathcal{D}}_{\mathbf{t}}$ 的样本数量非常有限的情况下。这里，$\lambda$ 用于调整总损失函数中第二项的重要性。等式 4 中其他符号的定义保持不变。


Table 2: Prompt augmentation techniques. Various augmented views are generated from an original text prompt using specific augmentation techniques: OSD (Object Size Deletion), OCD (Object Color Deletion), ITD (Image Type Deletion), ASD (Art Style Deletion), and SPO (Swapping Prompt Order).
表 2：提示增强技术。通过使用特定的增强技术从原始文本提示生成多种增强视图：OSD（对象大小删除）、OCD（对象颜色删除）、ITD（图像类型删除）、ASD（艺术风格删除）和 SPO（提示顺序互换）。


<table><tr><td>Original</td><td>OSD</td><td>OCD</td><td>ITD</td><td>ASD</td><td>SPO</td></tr><tr><td>a realistic painting of a large red car</td><td>a realistic painting of a red car</td><td>a realistic painting of a large car</td><td>a realistic of a large red car</td><td>a painting of a large red car</td><td>a large red car in a realistic painting</td></tr></table>
<table><tbody><tr><td>原文</td><td>OSD</td><td>OCD</td><td>ITD</td><td>ASD</td><td>SPO</td></tr><tr><td>一幅真实感绘制的大红色汽车</td><td>一幅真实感绘制的红色汽车</td><td>一幅真实感绘制的大汽车</td><td>一幅真实感绘制的大红色汽车</td><td>一幅大红色汽车的绘画</td><td>在一个真实感绘画中的一辆大红色汽车</td></tr></tbody></table>


After training, the learned disentangled network is seamlessly integrated with both of CLIP's encoders to extract content representations, as depicted in Fig. 2c
经训练后，学到的解耦网络与 CLIP 的两个编码器无缝集成，以提取内容表征，如图 2c所示


Prompt Augmentation To ensure text prompts undergo stylistic changes without compromising their content, we have developed specific augmentation techniques for synthetic text prompts. Drawing inspiration from Easy Data Augmentation (EDA) techniques [42], we adapted the Random Deletion (RD) and Random Swap (RS) techniques from EDA, customizing them to suit our prompt structure. To avoid inadvertently altering the content by introducing new object names or changing the core idea of a text prompt, our augmentation methods do not include random word insertions or replacements. Our primary augmentation techniques are Object Size Deletion (OSD), Object Color Deletion (OCD), Image Type Deletion (ITD), Art Style Deletion (ASD), and Swapping Prompt Order (SPO), each applied with a certain probability, as detailed in Tab. 2. Additionally, for down-stream datasets with few categories, to rich the population of training samples, we use an additional augmentation, named IGN (Inserting Gaussian Noise). Following the initializing protocol of prompt learning methods 47, 48], we insert a zero-mean Gaussian noise with 0.02 standard deviation with a noise length equals to 4 , to the tokenized prompts.
Prompt Augmentation 为确保文本提示在不影响内容的前提下经历风格变化，我们开发了针对合成文本提示的特定增强技术。借鉴 Easy Data Augmentation (EDA) 技术[42]，我们对 EDA 的 Random Deletion (RD) 和 Random Swap (RS) 进行了改编，定制以适应我们的提示结构。为了避免通过引入新对象名称或改变文本提示的核心思想而无意中修改内容，我们的增强方法不包含随机单词插入或替换。我们主要的增强技术包括对象大小删除（OSD）、对象颜色删除（OCD）、图像类型删除（ITD）、艺术风格删除（ASD）和交换提示顺序（SPO），均以一定概率应用，详见表 2。此外，对于类别数量较少的下游数据集，为丰富训练样本，我们使用一种额外的增强，名为 IGN（插入高斯噪声）。遵循提示学习方法的初始化协议 47, 48]，我们在分词后的提示中插入一个均值为零、标准差为 0.02、长度为 4 的高斯噪声。


Intuitively, these prompt augmentation methods parallel random masking techniques used in image augmentation [6, 17]. However, prompt augmentations are more effective and precise than their image counterparts. This effectiveness arises because prompt augmentations can specifically target and eliminate a particular style element without impacting the content, whereas image masking, operating at the pixel or patch level, might inadvertently damage content information or lead to insufficient style changes.
直观地说，这些提示增强方法与图像增强中使用的随机掩蔽技术相似[6, 17]。然而，提示增强比其图像对应物更有效、也更精确。之所以如此有效，是因为提示增强可以专门定位并消除特定风格元素，而不影响内容；而在像素或补丁层面的图像掩蔽可能会无意中损坏内容信息或导致风格变换不足。


## 5 Experiments
## 5 实验


We conduct three primary experiments to assess our method: (1) zero-shot evaluation with diverse prompts to gauge zero-shot performance and its robustness to prompt perturbations; (2) linear probe tests on few-shot samples to evaluate the efficacy of the learned representations in few-shot settings; and (3) adversarial attack assessments on zero-shot and one-shot classifiers to determine their resistance to adversarial threats. We further conduct an ablative study on hyper-parameters, explore the impact of different prompt augmentation combinations and various sources of training prompts on CLAP's performance, and replicate experiments across different CLIP model sizes.
我们进行三项主要实验来评估我们的方法：（1）使用多样化提示的零样本评估，以衡量零样本性能及对提示扰动的鲁棒性；（2）对少样本进行线性探针测试，以评估在少样本设定下学习表征的有效性；以及（3）对零样本和一样本分类器进行对抗攻击评估，以确定其对对抗威胁的抵抗力。我们还对超参数进行消融研究，探讨不同提示增强组合以及各种提示来源对 CLAP 性能的影响，并在不同的 CLIP 模型规模上重复实验。


### 5.1 Experimental Setup
### 5.1 实验设置


Implementation. Im.Aug and CLAP are implemented using the ViT-B/16 CLIP model and executed on an NVIDIA RTX 3090 GPU. To ensure reproducibility, the random seed for all stochastic processes is fixed at 2024. More information on implementation details is provided in Appendix A.1
实现。Im.Aug 和 CLAP 使用 ViT-B/16 CLIP 模型实现，并在 NVIDIA RTX 3090 GPU 上运行。为确保可重复性，所有随机过程的随机种子固定为 2024。实现细节的更多信息见附录 A.1


Datasets. CLAP is assessed across four multi-domain datasets to examine its performance in varied environments: PACS 23 (4 domains, 7 categories), VLCS 1 (4 domains, 5 categories), OfficeHome 37 (4 domains, 65 categories), and DomainNet [35] (6 domains, 345 categories). For conciseness, we present average results across the domains for each dataset. Detailed experimental outcomes for each domain within these datasets are provided in Appendix A.4
数据集。CLAP 在四个多领域数据集上进行评估，以检验其在不同环境中的性能：PACS 23（4 个领域，7 个类别）、VLCS 1（4 个领域，5 个类别）、OfficeHome 37（4 个领域，65 个类别）以及 DomainNet [35]（6 个领域，345 个类别）。为简明起见，我们给出各数据集在各领域上的平均结果。各数据集在每个领域的详细实验结果见附录 A.4


Compute efficiency. CLAP demonstrates faster convergence and shorter training times compared to Im.Aug. For CLAP, training on the PACS and VLCS datasets is completed in roughly 11 minutes, OfficeHome in approximately 14 minutes, and DomainNet in about 47 minutes. In contrast, Im.Aug requires around 16 minutes for PACS and VLCS, 50 minutes for OfficeHome, and 3.3 hours for DomainNet. Both Im.Aug and CLAP maintain CLIP's inference efficiency due to the disentangled network's efficient two-layer MLP structure.
计算效率。与 Im.Aug 相比，CLAP 展现出更快的收敛和更短的训练时间。对于 CLAP，在 PACS 与 VLCS 数据集上训练约 11 分钟，OfficeHome 约 14 分钟，DomainNet 约 47 分钟。相比之下，Im.Aug 在 PACS 与 VLCS 约需 16 分钟，OfficeHome 50 分钟，DomainNet 3.3 小时。两者都维持 CLIP 的推理效率，因为解耦网络采用的两层 MLP 结构高效。


### 5.2 Main Results
### 5.2 主要结果


Zero-Shot Performance To assess zero-shot capabilities, CLAP undergoes evaluation using three specific fixed prompts: ZS(C), utilizing only the class name within "[class]"; ZS(PC), with the format "a photo of a [class]"; and ZS(CP), structured as "a [class] in a photo". To thoroughly examine zero-shot proficiency, a dynamic prompt, ZS(NC), formatted as "[noise][class]", is also used, where "[noise]" signifies the introduction of Gaussian noise characterized by a mean of 0 and a standard deviation of 0.02 .
零样本表现 为评估零样本能力，CLAP 使用三种固定提示进行评估：ZS(C)，仅在“[class]”中使用类别名；ZS(PC)，格式为“某 [class] 的照片”；以及 ZS(CP)，结构为“某 [class] 的照片中的一个”。为彻底检验零样本能力，还使用动态提示 ZS(NC)，格式为“[noise][class]”，其中 “[noise]” 表示引入的高斯噪声，均值为 0，标准差为 0.02。 


As presented in Tab. 3, CLAP surpasses both CLIP and Im.Aug across all evaluated prompts for every dataset. Unlike the uniform enhancement in zero-shot performance CLAP achieves over CLIP, Im.Aug displays inconsistent results. A closer examination reveals CLAP's superiority over CLIP is especially significant for the dynamic ZS(NC) prompt. This demonstrates CLAP's effectiveness in significantly improving zero-shot performance compared to the original CLIP representations.
如表 3 所示，CLAP 在所有数据集的所有评估提示上均超越 CLIP 与 Im.Aug。与 CLIP 的统一提升相比，Im.Aug 的结果不稳定。更仔细的比较表明，CLAP 对 CLIP 的显著超越在动态提示 ZS(NC) 上尤为显著。这表明 CLAP 能显著提升零样本性能，相较于原始 CLIP 表征具有更强的效果。


In assessing the model's robustness to prompt perturbations, we examine the variances in zero-shot performance across different prompts by analyzing the range $\left( R\right)$ and standard deviation $\left( \delta \right)$ of results derived from $\mathrm{{ZS}}\left( \mathrm{C}\right) ,\mathrm{{ZS}}\left( \mathrm{{CP}}\right)$ , and ZS(PC). Additionally,we investigate the decline $\left( {\Delta }_{\left( NC\right) }\right)$ in performance from ZS(C) to ZS(NC) as a broad indicator of resilience to noised prompts.
在评估模型对提示扰动的鲁棒性时，我们通过分析来自 $\mathrm{{ZS}}\left( \mathrm{C}\right) ,\mathrm{{ZS}}\left( \mathrm{{CP}}\right)$、以及 ZS(PC) 的结果范围 $\left( R\right)$ 与标准差 $\left( \delta \right)$，来考察在不同提示下的零-shot 性能方差。此外，我们还将 ZS(C) 相对于 ZS(NC) 的 Performance 下降 $\left( {\Delta }_{\left( NC\right) }\right)$ 作为对带噪声提示的鲁棒性的一般指示进行研究。


Table 3: Zero-shot results across three distinct prompts: "C" for "[class]", "CP" for "a [class] in a photo", "PC" for "a photo of a [class]", and a dynamic prompt "NC" for "[noise][class]" showcase that CLAP consistently outperforms CLIP's zero-shot performance across all datasets, whereas image augmentation exhibits mixed outcomes.
表 3：在三种不同提示下的零-shot 结果：将“C”表示为 “[class]”、将 “CP” 表示为“一个 [class] 的照片”、将 “PC” 表示为“[class] 的一张照片”，以及一个动态提示 “NC” 表示“[noise][class]”，显示 CLAP 在所有数据集上的零-shot 性能始终优于 CLIP，而图像增强的结果则呈现混合表现。


<table><tr><td rowspan="2">Prompt</td><td rowspan="2"></td><td colspan="5">Zero-shot performance, avg. top-1 acc. (%) (↑)</td></tr><tr><td>PACS</td><td>VLCS</td><td>Off.Home</td><td>Dom.Net</td><td>Overall</td></tr><tr><td rowspan="3">ZS(C)</td><td>CLIP</td><td>95.7</td><td>76.4</td><td>79.8</td><td>57.8</td><td>77.4</td></tr><tr><td>Im.Aug</td><td>96.5</td><td>79.5</td><td>77.0</td><td>51.5</td><td>76.1</td></tr><tr><td>CLAP</td><td>97.2</td><td>82.6</td><td>81.0</td><td>58.7</td><td>79.9</td></tr><tr><td rowspan="3">ZS(CP)</td><td>CLIP</td><td>95.2</td><td>82.0</td><td>79.5</td><td>57.0</td><td>78.4</td></tr><tr><td>Im.Aug</td><td>96.3</td><td>82.9</td><td>75.8</td><td>50.7</td><td>76.4</td></tr><tr><td>CLAP</td><td>97.3</td><td>83.4</td><td>80.5</td><td>58.0</td><td>79.8</td></tr><tr><td rowspan="3">ZS(PC)</td><td>CLIP</td><td>96.1</td><td>82.4</td><td>82.5</td><td>57.7</td><td>79.7</td></tr><tr><td>Im.Aug</td><td>96.5</td><td>83.0</td><td>78.6</td><td>51.6</td><td>77.4</td></tr><tr><td>CLAP</td><td>97.2</td><td>83.4</td><td>83.0</td><td>59.0</td><td>80.6</td></tr><tr><td rowspan="3">ZS(NC)</td><td>CLIP</td><td>90.8</td><td>68.3</td><td>71.5</td><td>51.0</td><td>70.4</td></tr><tr><td>Im.Aug</td><td>94.8</td><td>73.1</td><td>67.5</td><td>44.0</td><td>69.9</td></tr><tr><td>CLAP</td><td>97.2</td><td>81.0</td><td>73.5</td><td>52.6</td><td>76.1</td></tr></table>
<table><tbody><tr><td rowspan="2">提示</td><td rowspan="2"></td><td colspan="5">零样本性能，平均前1准确率（%）(↑)</td></tr><tr><td>PACS</td><td>VLCS</td><td>离线.家庭</td><td>Dom.网络</td><td>总体</td></tr><tr><td rowspan="3">ZS(C)</td><td>CLIP</td><td>95.7</td><td>76.4</td><td>79.8</td><td>57.8</td><td>77.4</td></tr><tr><td>Im.增强</td><td>96.5</td><td>79.5</td><td>77.0</td><td>51.5</td><td>76.1</td></tr><tr><td>CLAP</td><td>97.2</td><td>82.6</td><td>81.0</td><td>58.7</td><td>79.9</td></tr><tr><td rowspan="3">ZS(CP)</td><td>CLIP</td><td>95.2</td><td>82.0</td><td>79.5</td><td>57.0</td><td>78.4</td></tr><tr><td>Im.增强</td><td>96.3</td><td>82.9</td><td>75.8</td><td>50.7</td><td>76.4</td></tr><tr><td>CLAP</td><td>97.3</td><td>83.4</td><td>80.5</td><td>58.0</td><td>79.8</td></tr><tr><td rowspan="3">ZS(PC)</td><td>CLIP</td><td>96.1</td><td>82.4</td><td>82.5</td><td>57.7</td><td>79.7</td></tr><tr><td>Im.增强</td><td>96.5</td><td>83.0</td><td>78.6</td><td>51.6</td><td>77.4</td></tr><tr><td>CLAP</td><td>97.2</td><td>83.4</td><td>83.0</td><td>59.0</td><td>80.6</td></tr><tr><td rowspan="3">ZS(NC)</td><td>CLIP</td><td>90.8</td><td>68.3</td><td>71.5</td><td>51.0</td><td>70.4</td></tr><tr><td>Im.增强</td><td>94.8</td><td>73.1</td><td>67.5</td><td>44.0</td><td>69.9</td></tr><tr><td>CLAP</td><td>97.2</td><td>81.0</td><td>73.5</td><td>52.6</td><td>76.1</td></tr></tbody></table>


As presented in Tab. 4, CLAP significantly reduces the variance in zero-shot performance across various testing prompts, evidenced by markedly lower values of $\delta$ and $R$ ,and a less pronounced decrease in performance with a noised prompt, in contrast to Im.Aug and the baseline representations of CLIP. Although Im.Aug aids in reducing performance variance to some extent, its efficacy is notably inferior to that of CLAP. These findings highlight CLAP's enhanced robustness in maintaining consistent zero-shot performance across a diverse array of prompts.
如表 4 所示，CLAP 显著降低在不同测试提示下的零-shot 性能方差，表现为 $\delta$ 和 $R$ 的值显著下降，以及在带噪声提示下性能下降不那么显著，与 Im.Aug 和 CLIP 的基线表示相比更优。尽管 Im.Aug 在一定程度上有助于降低性能方差，但其效果显著落后于 CLAP。这些发现凸显了 CLAP 在跨多样化提示中维持一致零-shot 性能方面的增强鲁棒性。


Few-Shot Performance We conduct evaluations of 1-shot, 4-shot, 8-shot, and 16-shot linear probes across each domain within the four datasets. As illustrated in Fig. 4, CLAP significantly outperforms both CLIP and Im.Aug in few-shot learning scenarios. Notably, in the 1-shot setting CLAP achieves performance improvements over the linear-probe CLIP model by margins of +10%, +3.5%, +2.5%, and +1.5% on the PACS, VLCS, OfficeHome, and DomainNet datasets, respectively. These improvements are especially significant in comparison to the gains observed with Im.Aug counterparts, underpinning CLAP's efficacy in few-shot scenarios. For detailed quantitative results, please refer to Appendix A.4
小样本性能 我们在四个数据集的各个领域对 1-shot、4-shot、8-shot 和 16-shot 的线性探针进行评估。如图 4 所示，CLAP 在少样本学习场景中显著优于 CLIP 和 Im.Aug。特别是在 1-shot 设置中，CLAP 相较线性探针 CLIP 模型在 PACS、VLCS、OfficeHome 和 DomainNet 数据集上的性能提升分别为 +10%、+3.5%、+2.5% 和 +1.5%。与 Im.Aug 对应方法相比，这些提升尤为显著，凸显了 CLAP 在少样本场景中的有效性。详细定量结果，请参阅附录 A.4


Adversarial Performance To assess adversarial robustness, zero-shot (ZS(C)) and one-shot classifiers are evaluated against prominent adversarial attack methods, such as FGSM [15], PGD [30], and CW [5], by generating adversarial samples for testing. For FGSM, 1 adversarial iteration is employed, whereas for PGD and CW, 20 iterations are used, all with an epsilon of 0.031 . As indicated in Tab. 5, classifiers utilizing CLAP representations demonstrate superior resilience to these adversarial attacks compared to those based on CLIP representations. Across the four datasets, CLAP's zero-shot and 1-shot classifiers surpass CLIP by margins of +7.6% and +8.5% against FGSM, +1.0% and +11.7% against PGD-20, and +1.1% and +2.3% against CW-20, respectively. These figures notably exceed the performance improvements of +4.4% and +4.6% against FGSM, +0.3% and +6.2% against PGD-20, and 0% and +1.3% against CW-20 achieved by Im.Aug. The result suggests that CLAP efficiently enhances robustness against adversarial attacks in both zero-shot and one-shot scenarios.
对抗性性能 为评估对抗鲁棒性，对零-shot (ZS(C)) 和一-shot 分类器进行评估，针对 FGSM [15]、PGD [30] 和 CW [5] 等主流对抗攻击方法，通过生成对抗样本进行测试。对于 FGSM，使用 1 次对抗迭代；对于 PGD 和 CW，使用 20 次迭代，全部的 epsilon 为 0.031。如表 5 所示，使用 CLAP 表征的分类器在这些对抗攻击上的鲁棒性显著优于基于 CLIP 表征的分类器。跨四个数据集，CLAP 的零-shot 和 1-shot 分类器分别在 FGSM 上超越 CLIP 的幅度为 +7.6% 和 +8.5%，在 PGD-20 上分别为 +1.0% 和 +11.7%，在 CW-20 上分别为 +1.1% 和 +2.3%；这些数值明显超过 Im.Aug 在 FGSM、PGD-20、CW-20 上的提升 +4.4%、+4.6%、0% 与 +1.3% 的表现。该结果表明 CLAP 能在零-shot 与一-shot 情况下有效提升对抗攻击的鲁棒性。


Table 4: CLAP more effectively reduces zero-shot performance variance across prompts than image augmentation,with $R$ and $\delta$ indicating the range and standard deviation for $\mathrm{{ZS}}\left( \mathrm{C}\right) ,\mathrm{{ZS}}\left( \mathrm{{CP}}\right)$ ,and $\mathrm{{ZS}}\left( \mathrm{{PC}}\right)$ . The decrease ${\Delta }_{\left( NC\right) }$ from $\mathrm{{ZS}}\left( \mathrm{C}\right)$ to $\mathrm{{ZS}}\left( \mathrm{{NC}}\right)$ highlights CLAP's enhanced robustness against prompt perturbations.
表 4：CLAP 相较图像增强更有效地降低跨提示的零-shot 性能方差，其中 $R$ 和 $\delta$ 表示 $\mathrm{{ZS}}\left( \mathrm{C}\right) ,\mathrm{{ZS}}\left( \mathrm{{CP}}\right)$ 的量度范围和标准差，以及 $\mathrm{{ZS}}\left( \mathrm{{PC}}\right)$。从 $\mathrm{{ZS}}\left( \mathrm{C}\right)$ 到 $\mathrm{{ZS}}\left( \mathrm{{NC}}\right)$ 的 ${\Delta }_{\left( NC\right) }$ 下降凸显了 CLAP 在对提示扰动的鲁棒性方面的增强。


<table><tr><td rowspan="2"></td><td rowspan="2">Metric Method</td><td colspan="5">Performance variance, avg. top-1 acc. (%) (%) (↓)</td></tr><tr><td>PACS</td><td>VLCS</td><td>Off.Home</td><td>Dom.Net</td><td>Overall</td></tr><tr><td rowspan="3">$R$</td><td>CLIP</td><td>0.9</td><td>6.1</td><td>3.1</td><td>0.8</td><td>2.7</td></tr><tr><td>Im.Aug</td><td>0.1</td><td>3.6</td><td>2.8</td><td>0.9</td><td>1.9</td></tr><tr><td>CLAP</td><td>0.1</td><td>0.8</td><td>2.5</td><td>1.0</td><td>1.1</td></tr><tr><td rowspan="3">$\delta$</td><td>CLIP</td><td>0.4</td><td>2.8</td><td>1.4</td><td>0.4</td><td>1.2</td></tr><tr><td>Im.Aug</td><td>0.1</td><td>1.7</td><td>1.2</td><td>0.4</td><td>0.8</td></tr><tr><td>CLAP</td><td>0.0</td><td>0.4</td><td>1.1</td><td>0.4</td><td>0.5</td></tr><tr><td rowspan="3">${\Delta }_{\left( NC\right) }$</td><td>CLIP</td><td>4.9</td><td>8.1</td><td>8.3</td><td>6.8</td><td>7.0</td></tr><tr><td>Im.Aug</td><td>1.6</td><td>6.4</td><td>9.5</td><td>7.5</td><td>6.3</td></tr><tr><td>CLAP</td><td>0.0</td><td>1.6</td><td>7.5</td><td>6.1</td><td>3.8</td></tr></table>
<table><tbody><tr><td rowspan="2"></td><td rowspan="2">度量方法</td><td colspan="5">性能方差，avg. top-1 精度 (%) (%) (↓)</td></tr><tr><td>PACS</td><td>VLCS</td><td>Off.Home</td><td>Dom.Net</td><td>总体</td></tr><tr><td rowspan="3">$R$</td><td>CLIP</td><td>0.9</td><td>6.1</td><td>3.1</td><td>0.8</td><td>2.7</td></tr><tr><td>Im.Aug</td><td>0.1</td><td>3.6</td><td>2.8</td><td>0.9</td><td>1.9</td></tr><tr><td>CLAP</td><td>0.1</td><td>0.8</td><td>2.5</td><td>1.0</td><td>1.1</td></tr><tr><td rowspan="3">$\delta$</td><td>CLIP</td><td>0.4</td><td>2.8</td><td>1.4</td><td>0.4</td><td>1.2</td></tr><tr><td>Im.Aug</td><td>0.1</td><td>1.7</td><td>1.2</td><td>0.4</td><td>0.8</td></tr><tr><td>CLAP</td><td>0.0</td><td>0.4</td><td>1.1</td><td>0.4</td><td>0.5</td></tr><tr><td rowspan="3">${\Delta }_{\left( NC\right) }$</td><td>CLIP</td><td>4.9</td><td>8.1</td><td>8.3</td><td>6.8</td><td>7.0</td></tr><tr><td>Im.Aug</td><td>1.6</td><td>6.4</td><td>9.5</td><td>7.5</td><td>6.3</td></tr><tr><td>CLAP</td><td>0.0</td><td>1.6</td><td>7.5</td><td>6.1</td><td>3.8</td></tr></tbody></table>


<img src="https://cdn.noedgeai.com/bo_d6aqku77aajc739ardu0_11.jpg?x=392&y=912&w=1020&h=213&r=0"/>



Fig. 4: Few-shot linear probe comparisons of image-encoder features show that CLAP enhances CLIP's few-shot performance more effectively than Im.Aug. In the accompanying figure, "ZS" indicates the zero-shot performance using a "[class]" prompt.
图 4：对图像编码器特征的少样本线性探针比较表明，CLAP 比 Im.Aug. 更有效提升 CLIP 的少样本性能。在随附的图中，“ZS”表示使用“[class]”提示词的零样本性能。


### 5.3 More Analysis
### 5.3 更多分析


t-SNE Visualization In our t-SNE visualizations, we examine the representations of CLIP, Im.Aug, and CLAP for all images within the Art Painting domain of the PACS dataset. Fig. 5 shows that CLAP's image representations display a marked inter-class separation and tighter intra-class clustering than those of
t-SNE 可视化 在我们的 t-SNE 可视化中，我们检查 PACS 数据集的艺术绘画领域所有图像的 CLIP、Im.Aug 和 CLAP 的表示。图 5 显示，CLAP 的图像表示在类间的分离更明显、类内聚簇更紧密，相较于


Table 5: Image augmentation and CLAP both enhance CLIP's zero-shot with the "[class]" prompt and 1-shot robustness against adversarial attacks, with CLAP showing greater improvements.
表 5：图像增强和 CLAP 都提升 CLIP 的零样本表现，使用“[class]”提示词，并对对抗攻击具有 1-shot 鲁棒性，且 CLAP 的改进更大。


<table><tr><td colspan="2" rowspan="3">Setting Method</td><td colspan="13">Avg. top-1 acc. (%) under adversarial attacks(↑)</td></tr><tr><td colspan="4">FGSM</td><td colspan="4">PGD-20</td><td colspan="4">CW-20</td><td rowspan="2">Avg.</td></tr><tr><td>PACS</td><td>VLCS</td><td>O.H.</td><td>D.N.</td><td>PACS</td><td>VLCS</td><td>O.H.</td><td>D.N.</td><td>PACS</td><td>VLCS</td><td>O.H.</td><td>D.N.</td></tr><tr><td rowspan="3">ZS(C)</td><td>CLIP</td><td>86.8</td><td>65.6</td><td>57.9</td><td>22.5</td><td>29.1</td><td>2.0</td><td>10.1</td><td>10.7</td><td>27.4</td><td>1.5</td><td>7.4</td><td>7.6</td><td>29.2</td></tr><tr><td>Im.Aug</td><td>88.0</td><td>69.6</td><td>55.1</td><td>37.9</td><td>31.3</td><td>2.1</td><td>10.4</td><td>9.0</td><td>29.4</td><td>1.7</td><td>7.0</td><td>5.8</td><td>31.1</td></tr><tr><td>CLAP</td><td>88.7</td><td>71.9</td><td>58.5</td><td>44.2</td><td>30.8</td><td>3.2</td><td>10.6</td><td>11.2</td><td>29.8</td><td>2.3</td><td>8.1</td><td>8.0</td><td>32.7</td></tr><tr><td rowspan="3">1-shot</td><td>CLIP</td><td>66.7</td><td>45.2</td><td>34.3</td><td>22.5</td><td>34.8</td><td>16.0</td><td>5.6</td><td>11.3</td><td>18.9</td><td>0.7</td><td>4.5</td><td>3.2</td><td>23.7</td></tr><tr><td>Im.Aug</td><td>79.4</td><td>47.1</td><td>37.1</td><td>23.5</td><td>55.2</td><td>16.1</td><td>8.5</td><td>12.5</td><td>23.2</td><td>0.9</td><td>5.1</td><td>3.4</td><td>28.0</td></tr><tr><td>CLAP</td><td>89.6</td><td>52.2</td><td>37.1</td><td>23.9</td><td>73.4</td><td>21.2</td><td>7.4</td><td>12.5</td><td>27.0</td><td>1.1</td><td>5.0</td><td>3.5</td><td>31.9</td></tr></table>
<table><tbody><tr><td colspan="2" rowspan="3">设置方法</td><td colspan="13">对抗性攻击下的平均前n名准确率（%）(↑)</td></tr><tr><td colspan="4">FGSM</td><td colspan="4">PGD-20</td><td colspan="4">CW-20</td><td rowspan="2">平均值</td></tr><tr><td>PACS</td><td>VLCS</td><td>O.H.</td><td>D.N.</td><td>PACS</td><td>VLCS</td><td>O.H.</td><td>D.N.</td><td>PACS</td><td>VLCS</td><td>O.H.</td><td>D.N.</td></tr><tr><td rowspan="3">ZS(C)</td><td>CLIP</td><td>86.8</td><td>65.6</td><td>57.9</td><td>22.5</td><td>29.1</td><td>2.0</td><td>10.1</td><td>10.7</td><td>27.4</td><td>1.5</td><td>7.4</td><td>7.6</td><td>29.2</td></tr><tr><td>Im.Aug</td><td>88.0</td><td>69.6</td><td>55.1</td><td>37.9</td><td>31.3</td><td>2.1</td><td>10.4</td><td>9.0</td><td>29.4</td><td>1.7</td><td>7.0</td><td>5.8</td><td>31.1</td></tr><tr><td>CLAP</td><td>88.7</td><td>71.9</td><td>58.5</td><td>44.2</td><td>30.8</td><td>3.2</td><td>10.6</td><td>11.2</td><td>29.8</td><td>2.3</td><td>8.1</td><td>8.0</td><td>32.7</td></tr><tr><td rowspan="3">1-shot</td><td>CLIP</td><td>66.7</td><td>45.2</td><td>34.3</td><td>22.5</td><td>34.8</td><td>16.0</td><td>5.6</td><td>11.3</td><td>18.9</td><td>0.7</td><td>4.5</td><td>3.2</td><td>23.7</td></tr><tr><td>Im.Aug</td><td>79.4</td><td>47.1</td><td>37.1</td><td>23.5</td><td>55.2</td><td>16.1</td><td>8.5</td><td>12.5</td><td>23.2</td><td>0.9</td><td>5.1</td><td>3.4</td><td>28.0</td></tr><tr><td>CLAP</td><td>89.6</td><td>52.2</td><td>37.1</td><td>23.9</td><td>73.4</td><td>21.2</td><td>7.4</td><td>12.5</td><td>27.0</td><td>1.1</td><td>5.0</td><td>3.5</td><td>31.9</td></tr></tbody></table>


<img src="https://cdn.noedgeai.com/bo_d6aqku77aajc739ardu0_12.jpg?x=432&y=777&w=953&h=269&r=0"/>



Fig. 5: t-SNE visualizations of all images in the Art Painting of PACS dataset show CLAP outperforms the original CLIP and Im.Aug, with clearer inter-class distinctions and tighter intra-class clusters.
Fig. 5: t-SNE 可视化 PACS 数据集中艺术绘画的所有图像，显示 CLAP 在原始 CLIP 与 Im.Aug 的基础上具有更清晰的类间分隔和更紧凑的组内聚类。


CLIP and Im.Aug. This observation suggests that CLAP's representations are more closely tied to content information and less influenced by style information, in contrast to the other two.
CLIP 与 Im.Aug. 这一观察表明，CLAP 的表示与内容信息关系更紧密、对风格信息的影响更小，与另外两者形成对比。


Ablations In Fig. 6, we assess the zero-shot capabilities of our model using two distinct prompts, ZS(C) and ZS(PC), on the VLCS dataset. This analysis forms part of an ablative study aimed at understanding the influence of various hyper-parameters on model performance. Specifically, we examine: the dimensions of the latent layer within the MLP of the disentangled network, as illustrated in Fig. 6a, the temperature parameter $\left( \tau \right)$ in the loss function, as depicted in Fig. 6b and the weight coefficient $\left( \alpha \right)$ during the inference stage,as shown in Fig. 6c. Our findings indicate that CLAP consistently enhances zero-shot performance across all tested configurations for both prompts, while also significantly reducing the gap between the performances elicited by each prompt. These results underscore the efficacy of CLAP in accommodating a wide range of hyper-parameters.
Ablations 在图 6 中，我们在 VLCS 数据集上用两种不同的提示 ZS(C) 与 ZS(PC) 对模型的零-shot 能力进行评估。该分析构成消融研究的一部分，旨在理解不同超参数对模型性能的影响。具体而言，我们考察：如图 6a 所示的解耦网络中 MLP 的潜在层维度、损失函数中的温度参数 $\left( \tau \right)$、在推理阶段的权重系数 $\left( \alpha \right)$，如图 6c 所示。我们的发现表明，在所有测试配置下，CLAP 对两种提示都能稳定提升零-shot 性能，同时显著缩小两种提示所引发的性能差距。这些结果强调了 CLAP 在适应广泛超参数方面的有效性。


Prompt Augmentation Combinations We explore diverse combinations of our tailored prompt augmentation methods and examine Easy Data Augmentation (EDA) techniques 42 on the VLCS dataset. Each tested technique showcases CLAP's enhancements over CLIP, with details available in Appendix A.2
Prompt Augmentation Combinations 我们探索定制提示增强方法的多种组合，并在 VLCS 数据集上考察 Easy Data Augmentation (EDA) 技术 42。每种测试技术均展示了 CLAP 相较于 CLIP 的提升，具体细节见附录 A.2


<img src="https://cdn.noedgeai.com/bo_d6aqku77aajc739ardu0_13.jpg?x=393&y=345&w=1018&h=257&r=0"/>



Fig. 6: We conduct ablative study on hyper-parameter choices on the VLCS dataset, including latent dimensions, $\tau$ values,and $\alpha$ values during the inference stage. CLAP continuously enhance CLIP's performance throughout the tested values.
Fig. 6: 我们在 VLCS 数据集上对超参数选择进行消融研究，包括潜在维度、$\tau$ 值和推理阶段的 $\alpha$ 值。CLAP 在测试的各项数值中持续提升 CLIP 的性能。


Prompt Sources We assess the impact of different training prompt formats, originating from various synthetic sources, on the performance of the VLCS dataset, incorporating EDA techniques. Our evaluation includes our template-based prompts, LLM-generated prompts by ChatGPT-3.5 3 (with the generation process detailed in Appendix B.2 ), prompts structured as "a [random] style of [class]," where "[random]" is filled with terms from a random word generator ${}^{2}$ and prompts produced using the PromptStyler method [9]. The findings indicate that the training prompts with simpler forms tend to yield better performance, with detailed quantitative results presented in Appendix A.3
Prompt Sources 我们评估来自不同合成来源的训练提示格式对 VLCS 数据集性能的影响，并结合 EDA 技术。我们的评估包括基于模板的提示、ChatGPT-3.5 生成的提示（生成过程详见附录 B.2）、以“a [random] style of [class]”形式组织的提示（其中 “[random]” 由随机单词生成器 ${}^{2}$ 提供填充）以及使用 PromptStyler 方法 [9] 产生的提示。结果表明，形式较简单的训练提示往往获得更好的性能，详细量化结果见附录 A.3


Experiments on Different Model Scales In our repeated experiments assessing zero-shot performance on the ViT-L/14 and ResNet50x16 pre-trained with CLIP, we consistently find that CLAP improves zero-shot performance while also reducing performance variances. This consistent observation underscores CLAP's effectiveness in enhancing the quality of CLIP representations. For quantitative details supporting these findings, please see the Appendix C
不同模型规模的实验 在我们对 ViT-L/14 与 ResNet50x16 在 CLIP 预训练下进行的多次零-shot 性能评测中，我们始终发现 CLAP 提升了零-shot 性能，同时降低了性能方差。这一持续观察强调了 CLAP 在提升 CLIP 表征质量方面的有效性。关于支持这些发现的定量细节，请参见附录 C


## 6 Conclusion
## 6 结论


To enhance pre-trained CLIP-like models, this study delves into disentangling latent content variables. Through a causal analysis of the underlying generative processes of vision-language data, we discover that training a disentangled network in one modality can effectively disentangle content across both modalities. Given the high semantic nature of text data, we identify that disentanglement is more achievable within the language modality through text augmentation interventions. Building on these insights, we introduce CLAP (Contrastive Learning with Augmented Prompts) to acquire disentangled vision-language content features. Comprehensive experiments validate CLAP's effectiveness, demonstrating significant improvements in zero-shot and few-shot performance, and enhancing robustness against perturbations. We anticipate that our work will inspire further exploration into disentangling latent variables within vision-language models.
为了提升预训练的 CLIP 风格模型，本研究深入研究了潜在内容变量的解耦。通过对视觉-语言数据的潜在生成过程进行因果分析，我们发现，在一个模态中训练一个解耦网络可以有效解耦两种模态中的内容。鉴于文本数据的高语义性质，我们发现通过文本增强干预在语言模态中实现解耦更具可行性。在此基础上，我们引入 CLAP（对比学习与增强提示）以获得解耦的视觉-语言内容特征。全面实验验证了 CLAP 的有效性，显示零-shot 与少样本性能显著提升，并提高对扰动的鲁棒性。我们希望本工作能激发对视觉-语言模型中潜在变量解耦的进一步探索。


---



2 https://github.com/vaibhavsingh97/random-word
2 https://github.com/vaibhavsingh97/random-word


---



## References
## 参考文献


1. Albuquerque, I., Monteiro, J., Darvishi, M., Falk, T.H., Mitliagkas, I.: Generalizing to unseen domains via distribution matching. arXiv preprint arXiv:1911.00804 (2019). https://doi.org/10.48550/arXiv.1911.00804
1. Albuquerque, I., Monteiro, J., Darvishi, M., Falk, T.H., Mitliagkas, I.: Generalizing to unseen domains via distribution matching. arXiv preprint arXiv:1911.00804 (2019). https://doi.org/10.48550/arXiv.1911.00804


2. Bollen, K.A.: Structural equations with latent variables, vol. 210. John Wiley & Sons (1989). https://doi.org/10.1002/9781118619179
2. Bollen, K.A.: Structural equations with latent variables, vol. 210. John Wiley & Sons (1989). https://doi.org/10.1002/9781118619179


3. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-lakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems 33, 1877-1901 (2020), https://dl.acm.org/doi/abs/10.5555/3495724.3495883
3. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-lakantan, A., Shyam, P., Sastry, G., Askell, A., 等：语言模型是少样本学习者。神经信息处理系统进展 33，1877-1901 (2020)，https://dl.acm.org/doi/abs/10.5555/3495724.3495883


4. Carlini, N., Terzis, A.: Poisoning and backdooring contrastive learning. In: International Conference on Learning Representations (2021), https://openreview.net/forum?id=iC4UHbQ01Mp
4. 卡林尼, N., 特里斯, A.: 对比学习的中毒与后门攻击。In: International Conference on Learning Representations (2021), https://openreview.net/forum?id=iC4UHbQ01Mp


5. Carlini, N., Wagner, D.: Towards evaluating the robustness of neural networks. In: 2017 IEEE Symposium on Security and Privacy (SP). pp. 39-57. IEEE Computer Society (2017). https://doi.org/10.1109/SP.2017.49
5. 卡林尼, N., 沃格, D.: 评估神经网络鲁棒性的走向。In: 2017 IEEE Security and Privacy Symposium (SP). pp. 39-57. IEEE Computer Society (2017). https://doi.org/10.1109/SP.2017.49


6. Chen, P., Liu, S., Zhao, H., Jia, J.: Gridmask data augmentation. arXiv preprint arXiv:2001.04086 (2020). https://doi.org/10.48550/arXiv.2001.04086
6. 陈, P., 柳, S., 赵, H., 贾, J.: Gridmask 数据增强。arXiv 预印本 arXiv:2001.04086 (2020). https://doi.org/10.48550/arXiv.2001.04086


7. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: International conference on machine learning. pp. 1597-1607. PMLR (2020), https://dl.acm.org/doi/abs/10.5555/ 3524938.3525087
7. 陈, T., Kornblith, S., Norouzi, M., Hinton, G.: 视觉表征对比学习的一个简单框架。In: International conference on machine learning. pp. 1597-1607. PMLR (2020), https://dl.acm.org/doi/abs/10.5555/ 3524938.3525087


8. Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 15750-15758 (2021). https://doi.org/10.1109/CVPR46437.2021.01549
8. 陈, X., 何, K.: 探索简单的连体表示学习。In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 15750-15758 (2021). https://doi.org/10.1109/CVPR46437.2021.01549


9. Cho, J., Nam, G., Kim, S., Yang, H., Kwak, S.: Promptstyler: Prompt-driven style generation for source-free domain generalization. In: Proceedings of the IEEE international conference on computer vision. pp. 15702-15712 (2023). https: //doi.org/10.1109/ICCV51070.2023.01439
9. 曹, J., 南, G., 金, S., 杨, H., 郭, S.: Promptstyler：面向源无域泛化的提示驱动风格生成。In: Proceedings of the IEEE international conference on computer vision. pp. 15702-15712 (2023). https: //doi.org/10.1109/ICCV51070.2023.01439


10. Daunhawer, I., Bizeul, A., Palumbo, E., Marx, A., Vogt, J.E.: Identifiability results for multimodal contrastive learning. ICLR (2023), https://openreview.net/ forum?id=U_2kuqoTcB
10. 道恩豪厄尔, I., 比祖尔, A., 帕伦博, E., 马克斯, A., Vogt, J.E.: 多模态对比学习的可识别性结果。ICLR (2023), https://openreview.net/ forum?id=U_2kuqoTcB


11. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2020), https://openreview.net/forum?id= YicbFdNTTy
11. 多索维茨基, A., 贝耶, L., 科莱斯尼科夫, A., 韦森博恩, D., 赵, X., 伦特海纳, T., Dehghani, M., Minderer, M., Hei金, G., 盖利, S., et al.: 一张图片值16x16个词：大规模图像识别的变换器。In: International Conference on Learning Representations (2020), https://openreview.net/forum?id= YicbFdNTTy


12. Fort, S.: Adversarial vulnerability of powerful near out-of-distribution detection. arXiv preprint arXiv:2201.07012 (2022). https://doi.org/10.48550/arXiv.2201.07012
12. 福特, S.: 强大且近似分布外检测的对抗易受攻击性。arXiv 预印本 arXiv:2201.07012 (2022). https://doi.org/10.48550/arXiv.2201.07012


13. Gao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y., Li, H., Qiao, Y.: Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision 132(2), 581-595 (2024). https://doi.org/10.1007/ s11263-023-01891-x
13. 高, P., 耿, S., 张, R., 马, T., 方, R., 张, Y., 李, H., 乔, Y.: Clip-adapter：带特征适配器的更强视觉-语言模型。International Journal of Computer Vision 132(2), 581-595 (2024). https://doi.org/10.1007/ s11263-023-01891-x


14. Ge, C., Huang, R., Xie, M., Lai, Z., Song, S., Li, S., Huang, G.: Domain adaptation via prompt learning. arXiv preprint arXiv:2202.06687 (2022). https://doi.org/ 10.48550/arXiv.2202.06687
14. 葛, C., 黄, R., 叶, M., 赖, Z., 宋, S., 李, S., 黄, G.: 通过提示学习进行领域适应。arXiv 预印本 arXiv:2202.06687 (2022). https://doi.org/ 10.48550/arXiv.2202.06687


15. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial examples. In: International Conference on Learning Representations (2015). https: //doi.org/10.48550/arXiv.1412.6572
15. Goodfellow, I.J., Shlens, J., Szegedy, C.: 解释并利用对抗性样本。In: International Conference on Learning Representations (2015). https: //doi.org/10.48550/arXiv.1412.6572


16. Grill, J.B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., Do-ersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al.: Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems 33, 21271-21284 (2020), https://dl.acm.org/doi/abs/10.5555/3495724.3497510
16. Grill, J.B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., Do-ersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al.: Bootstrap your own latent—a new approach to self-supervised learning. Advances in neural information processing systems 33, 21271-21284 (2020), https://dl.acm.org/doi/abs/10.5555/3495724.3497510


17. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 16000-16009 (2022). https://doi.org/10.1109/CVPR52688.2022.01553
17. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: 掩码自编码器是可扩展的视觉学习者。In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 16000-16009 (2022). https://doi.org/10.1109/CVPR52688.2022.01553


18. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised visual representation learning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9729-9738 (2020). https://doi.org/10.1109/CVPR42600.2020.00975
18. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: 动量对比学习用于无监督视觉表征学习。In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9729-9738 (2020). https://doi.org/10.1109/CVPR42600.2020.00975


19. Hong, T., Guo, X., Ma, J.: Itmix: Image-text mix augmentation for transferring clip to image classification. In: 2022 16th IEEE International Conference on Signal Processing (ICSP). vol. 1, pp. 129-133. IEEE (2022). https://doi.org/10.1109/ ICSP56322.2022.9965292
19. Hong, T., Guo, X., Ma, J.: Itmix: Image-text mix 增强用于将 CLIP 转移到图像分类。In: 2022 16th IEEE International Conference on Signal Processing (ICSP). vol. 1, pp. 129-133. IEEE (2022). https://doi.org/10.1109/ ICSP56322.2022.9965292


20. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H., Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning with noisy text supervision. In: International conference on machine learning. pp. 4904-4916. PMLR (2021), https://proceedings.mlr.press/v139/jia21b.html
20. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H., Li, Z., Duerig, T.: 通过噪声文本监督进行大规模视觉与视觉-语言表征学习。In: International conference on machine learning. pp. 4904-4916. PMLR (2021), https://proceedings.mlr.press/v139/jia21b.html


21. Khattak, M.U., Rasheed, H., Maaz, M., Khan, S., Khan, F.S.: Maple: Multi-modal prompt learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19113-19122 (2023). https://doi.org/10.1109/CVPR52729.2023.01832
21. Khattak, M.U., Rasheed, H., Maaz, M., Khan, S., Khan, F.S.: Maple: 多模态提示学习。In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19113-19122 (2023). https://doi.org/10.1109/CVPR52729.2023.01832


22. Kong, L., Xie, S., Yao, W., Zheng, Y., Chen, G., Stojanov, P., Akinwande, V., Zhang, K.: Partial disentanglement for domain adaptation. In: International Conference on Machine Learning. pp. 11455-11472. PMLR (2022), https:// proceedings.mlr.press/v162/kong22a.html
22. Kong, L., Xie, S., Yao, W., Zheng, Y., Chen, G., Stojanov, P., Akinwande, V., Zhang, K.: 部分解 disentanglement 用于域自适应。In: International Conference on Machine Learning. pp. 11455-11472. PMLR (2022), https:// proceedings.mlr.press/v162/kong22a.html


23. Li, D., Yang, Y., Song, Y.Z., Hospedales, T.M.: Deeper, broader and artier domain generalization. In: Proceedings of the IEEE international conference on computer vision. pp. 5542-5550 (2017). https://doi.org/10.1109/ICCV.2017.591
23. Li, D., Yang, Y., Song, Y.Z., Hospedales, T.M.: 更深更广更具艺术性的领域泛化。In: Proceedings of the IEEE international conference on computer vision. pp. 5542-5550 (2017). https://doi.org/10.1109/ICCV.2017.591


24. Li, H., Wang, X., Zhang, Z., Yuan, Z., Li, H., Zhu, W.: Disentangled contrastive learning on graphs. Advances in Neural Information Processing Systems 34, 21872- 21884 (2021), https://dl.acm.org/doi/10.5555/3540261.3541935
24. Li, H., Wang, X., Zhang, Z., Yuan, Z., Li, H., Zhu, W.: 图上解 disentangled 对比学习。Advances in Neural Information Processing Systems 34, 21872- 21884 (2021), https://dl.acm.org/doi/10.5555/3540261.3541935


25. Li, Y., Liang, F., Zhao, L., Cui, Y., Ouyang, W., Shao, J., Yu, F., Yan, J.: Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. In: International Conference on Learning Representations (2021)
25. Li, Y., Liang, F., Zhao, L., Cui, Y., Ouyang, W., Shao, J., Yu, F., Yan, J.: 监督无处不在：一种数据高效的对比语言-图像预训练范式。In: International Conference on Learning Representations (2021)


26. Liu, Y., Zhang, Z., Gong, D., Gong, M., Huang, B., van den Hengel, A., Zhang, K., Shi, J.Q.: Identifiable latent polynomial causal models through the lens of change. In: The Twelfth International Conference on Learning Representations (2024), https://openreview.net/forum?id=ia9fK01Vjq
26. Liu, Y., Zhang, Z., Gong, D., Gong, M., Huang, B., van den Hengel, A., Zhang, K., Shi, J.Q.: 通过变更视角识别的潜在多项式因果模型。In: The Twelfth International Conference on Learning Representations (2024), https://openreview.net/forum?id=ia9fK01Vjq


27. Liu, Y., Zhang, Z., Gong, D., Gong, M., Huang, B., Hengel, A.v.d., Zhang, K., Shi, J.Q.: Identifying weight-variant latent causal models. arXiv preprint arXiv:2208.14153 (2022). https://doi.org/10.48550/arXiv.2208.14153
27. Liu, Y., Zhang, Z., Gong, D., Gong, M., Huang, B., Hengel, A.v.d., Zhang, K., Shi, J.Q.: 识别权重-变量潜在因果模型。arXiv 预印本 arXiv:2208.14153 (2022). https://doi.org/10.48550/arXiv.2208.14153


28. Liu, Y., Zhang, Z., Gong, D., Gong, M., Huang, B., Hengel, A.v.d., Zhang, K., Shi, J.Q.: Identifiable latent neural causal models. arXiv preprint arXiv:2403.15711 (2024). https://doi.org/10.48550/arXiv.2403.15711
28. Liu, Y., Zhang, Z., Gong, D., Gong, M., Huang, B., Hengel, A.v.d., Zhang, K., Shi, J.Q.: 可识别的潜在神经因果模型。arXiv 预印本 arXiv:2403.15711 (2024). https://doi.org/10.48550/arXiv.2403.15711


29. Liu, Y., Zhang, Z., Gong, D., Gong, M., Huang, B., Zhang, K., Shi, J.Q.: Identifying latent causal content for multi-source domain adaptation. arXiv preprint arXiv:2208.14161 (2022). https://doi.org/10.48550/arXiv.2208.14161
29. 刘云, 张正, 龚丹, 龚萌, 黄明, 张琪, 施晶琦: 鉴别多源领域自适应的潜在因果内容。arXiv 预印本 arXiv:2208.14161 (2022). https://doi.org/10.48550/arXiv.2208.14161


30. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning models resistant to adversarial attacks. In: International Conference on Learning Representations (2018), https://openreview.net/forum?id=rJzIBfZAb
30. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: 走向对对抗攻击具有鲁棒性的深度学习模型。 In: International Conference on Learning Representations (2018), https://openreview.net/forum?id=rJzIBfZAb


31. Mahajan, D., Tople, S., Sharma, A.: Domain generalization using causal matching. In: International Conference on Machine Learning. pp. 7313-7324. PMLR (2021), https://proceedings.mlr.press/v139/mahajan21b.html
31. Mahajan, D., Tople, S., Sharma, A.: 使用因果匹配的领域泛化。 In: International Conference on Machine Learning. pp. 7313-7324. PMLR (2021), https://proceedings.mlr.press/v139/mahajan21b.html


32. Mamooler, S.: Clip explainability. https://github.com/sMamooler/CLIP_ Explainability accessed: 2024-03-06
32. Mamooler, S.: Clip 可解释性。 https://github.com/sMamooler/CLIP_ Explainability accessed: 2024-03-06


33. Mao, C., Geng, S., Yang, J., Wang, X., Vondrick, C.: Understanding zero-shot adversarial robustness for large-scale models. In: The Eleventh International Conference on Learning Representations (2022), https://openreview.net/forum?id= P4bXCawRi5J
33. Mao, C., Geng, S., Yang, J., Wang, X., Vondrick, C.: 理解大规模模型的零样本对抗鲁棒性。 In: The Eleventh International Conference on Learning Representations (2022), https://openreview.net/forum?id= P4bXCawRi5J


34. Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). https://doi.org/10.48550/ arXiv.1807.03748
34. Oord, A.v.d., Li, Y., Vinyals, O.: 使用对比预测编码的表征学习。arXiv 预印本 arXiv:1807.03748 (2018). https://doi.org/10.48550/ arXiv.1807.03748


35. Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., Wang, B.: Moment matching for multi-source domain adaptation. In: Proceedings of the IEEE international conference on computer vision. pp. 1406-1415 (2019). https://doi.org/10.1109/ ICCV.2019.00149
35. Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., Wang, B.: 多源领域自适应的矩Moment匹配。 In: Proceedings of the IEEE International Conference on Computer Vision. pp. 1406-1415 (2019). https://doi.org/10.1109/ ICCV.2019.00149


36. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021), https://proceedings.mlr.press/v139/radford21a.html
36. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: 从自然语言监督中学习可迁移的视觉模型。 In: International Conference on Machine Learning. pp. 8748-8763. PMLR (2021), https://proceedings.mlr.press/v139/radford21a.html


37. Rahman, M.M., Fookes, C., Baktashmotlagh, M., Sridharan, S.: Multi-component image translation for deep domain generalization. In: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 579-588. IEEE (2019). https://doi.org/10.1109/WACV.2019.00067
37. Rahman, M.M., Fookes, C., Baktashmotlagh, M., Sridharan, S.: 深度域泛化的多组件图像翻译。 In: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 579-588. IEEE (2019). https://doi.org/10.1109/WACV.2019.00067


38. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: Zero-shot text-to-image generation. In: International Conference on Machine Learning. pp. 8821-8831. PMLR (2021), https://proceedings.mlr.press/v139/ramesh21a.html
38. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: 零样本文本到图像生成。 In: International Conference on Machine Learning. pp. 8821-8831. PMLR (2021), https://proceedings.mlr.press/v139/ramesh21a.html


39. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022). https://doi.org/10.1109/CVPR52729.2023.01389
39. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: 使用潜在扩散模型的高分辨率图像合成。 In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10684-10695 (2022). https://doi.org/10.1109/CVPR52729.2023.01389


40. Sanchez, E.H., Serrurier, M., Ortner, M.: Learning disentangled representations via mutual information estimation. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXII 16. pp. 205-221. Springer (2020). https://doi.org/10.1007/978-3-030-58542-6_13
40. Sanchez, E.H., Serrurier, M., Ortner, M.: 通过互信息估计学习解耦表示。 In: Computer Vision-ECCV 2020: 第16届欧洲会议，英国格拉斯哥，2020年8月23-28日， Proceedings, Part XXII 16. pp. 205-221. Springer (2020). https://doi.org/10.1007/978-3-030-58542-6_13


41. Von Kügelgen, J., Sharma, Y., Gresele, L., Brendel, W., Schölkopf, B., Besserve, M., Locatello, F.: Self-supervised learning with data augmentations provably isolates content from style. Advances in neural information processing systems 34, 16451-16467 (2021), https://dl.acm.org/doi/10.5555/3540261.3541519
41. Von Kügelgen, J., Sharma, Y., Gresele, L., Brendel, W., Schölkopf, B., Besserve, M., Locatello, F.: 通过数据增强的自监督学习能明确地将内容与风格分离。 Advances in neural information processing systems 34, 16451-16467 (2021), https://dl.acm.org/doi/10.5555/3540261.3541519


42. Wei, J., Zou, K.: Eda: Easy data augmentation techniques for boosting performance on text classification tasks. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 6382-6388 (2019). https://doi.org/10.18653/v1/D19-1670
42. Wei, J., Zou, K.: Eda: 轻量数据增强技术以提升文本分类任务的性能。In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 6382-6388 (2019). https://doi.org/10.18653/v1/D19-1670


43. Wortsman, M., Ilharco, G., Kim, J.W., Li, M., Kornblith, S., Roelofs, R., Lopes, R.G., Hajishirzi, H., Farhadi, A., Namkoong, H., et al.: Robust fine-tuning of zero-shot models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7959-7971 (2022). https://doi.org/10.1109/ CVPR52688.2022.00780
43. Wortsman, M., Ilharco, G., Kim, J.W., Li, M., Kornblith, S., Roelofs, R., Lopes, R.G., Hajishirzi, H., Farhadi, A., Namkoong, H., 等: 对零-shot 模型的鲁棒微调。In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7959-7971 (2022). https://doi.org/10.1109/ CVPR52688.2022.00780


44. Yang, M., Liu, F., Chen, Z., Shen, X., Hao, J., Wang, J.: Causalvae: Disentangled representation learning via neural structural causal models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9593-9602 (2021). https://doi.org/10.1109/CVPR46437.2021.00947
44. Yang, M., Liu, F., Chen, Z., Shen, X., Hao, J., Wang, J.: Causalvae: 通过神经结构化因果模型实现的解耦表示学习。In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9593-9602 (2021). https://doi.org/10.1109/CVPR46437.2021.00947


45. Yang, W., Mirzasoleiman, B.: Robust contrastive language-image pretraining against adversarial attacks. arXiv preprint arXiv:2303.06854 (2023). https://doi.org/10.48550/arXiv.2303.06854
45. Yang, W., Mirzasoleiman, B.: 针对对抗攻击的鲁棒对比学习的语言-图像预训练。arXiv 预印本 arXiv:2303.06854 (2023). https://doi.org/10.48550/arXiv.2303.06854


46. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: IEEE International Conference on Computer Vision (ICCV) (2023). https://doi.org/10.1109/ICCV51070.2023.00355
46. Zhang, L., Rao, A., Agrawala, M.: 为文本到图像扩散模型添加条件控制。In: IEEE International Conference on Computer Vision (ICCV) (2023). https://doi.org/10.1109/ICCV51070.2023.00355


47. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional prompt learning for vision-language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16816-16825 (2022). https://doi.org/10.1109/CVPR52688.2022.01631
47. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: 面向视觉-语言模型的条件提示学习。In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16816-16825 (2022). https://doi.org/10.1109/CVPR52688.2022.01631


48. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language models. International Journal of Computer Vision 130(9), 2337-2348 (2022). https://doi.org/10.1007/s11263-022-01653-1
48. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: 学习为视觉-语言模型生成提示。 International Journal of Computer Vision 130(9), 2337-2348 (2022). https://doi.org/10.1007/s11263-022-01653-1


49. Zimmermann, R.S., Sharma, Y., Schneider, S., Bethge, M., Brendel, W.: Contrastive learning inverts the data generating process. In: International Conference on Machine Learning. pp. 12979-12990. PMLR (2021), https://proceedings.mlr.press/v139/zimmermann21a.html
49. Zimmermann, R.S., Sharma, Y., Schneider, S., Bethge, M., Brendel, W.: 对比学习颠倒了数据产生过程。In: International Conference on Machine Learning. pp. 12979-12990. PMLR (2021), https://proceedings.mlr.press/v139/zimmermann21a.html


# CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts APPENDIX
# CLAP: 通过带增强提示的对比学习从内容与风格中分离 APPENDIX


Yichao Cai [9] Yuhang Liu [9] Zhen Zhang [9], and Javen Qinfeng Shi [9]
Yichao Cai [9] Yuhang Liu [9] Zhen Zhang [9], and Javen Qinfeng Shi [9]


Australian Institute for Machine Learning, University of Adelaide, SA 5000, Australia \{yichao.cai,yuhang.liu01,zhen.zhang02,javen.shi\}@adelaide.edu.au
Australian Institute for Machine Learning, University of Adelaide, SA 5000, Australia \{yichao.cai,yuhang.liu01,zhen.zhang02,javen.shi\}@adelaide.edu.au


## Overview of the Appendix:
## Appendix 概览：


- More details on experiments using the CLIP pre-trained ViT-B/16 model are provided in Appendix A, including implementation details in Appendix A.1, investigations into prompt augmentation combinations in Appendix A.2 analysis of different training prompt sources in Appendix A.3, and detailed experiment results for each dataset in Appendix A.4.
- 有关使用 CLIP 预训练 ViT-B/16 模型的更多实验细节在 Appendix A 给出，包括 Appendix A.1 的实现细节、Appendix A.2 的提示增强组合的研究、Appendix A.3 对不同训练提示源的分析，以及 Appendix A.4 对每个数据集的详细实验结果。


- The processes of data synthesis with large models used in our approach are outlined in Appendix B. The image synthesis procedure for Im.Aug is detailed in Appendix B.1 and the approach for generating "LLM" prompts, used in analyzing prompt sources, is described in Appendix B.2.
- 我们方法中使用的大模型数据合成过程在 Appendix B 中概述。Im.Aug 的图像合成过程在 Appendix B.1 详细说明，用于分析提示源的“LLM”提示生成方法在 Appendix B.2 描述。


- In Appendix C, we detail our repeated zero-shot experiments conducted with the CLIP pre-trained ViT-L/14 (Appendix C.1) and ResNet50x16 (Appendix C.2) models.
- 在 Appendix C 中，我们详细描述使用 CLIP 预训练 ViT-L/14 (Appendix C.1) 和 ResNet50x16 (Appendix C.2) 模型进行的重复零-shot 实验。


- In section Appendix D, we present discussions covering the underlying rationale for basing CLAP on the CLIP pre-trained models in Appendix D.1, and the impact of image augmentation and text augmentation in Appendix D.2.
- 在 Appendix D 第 1 节中，我们就将 CLAP 基于 CLIP 预训练模型的基本原理进行讨论；第 2 节中则讨论图像增强和文本增强的影响。


## A More on Experiments with ViT-B/16
## 关于 ViT-B/16 实验的更多内容


### A.1 Implementation Details
### A.1 实现细节


In this section, we detail the implementation of our experiments utilizing the CLIP pre-trained ViT-B/16 model:
在本节中，我们详细介绍使用 CLIP 预训练的 ViT-B/16 模型进行实验的实现：


Network. The network's output dimension is aligned with the 512-dimensional CLIP features, thereby obviating the need for input feature downsampling. The latent dimensions are tailored to each dataset: 256 for PACS, 448 for OfficeHome, and 512 for VLCS and DomainNet, to accommodate the variety of categories and complexity of datasets. The weight parameter $\alpha$ is adjusted to 0.208 for PACS, 0.056 for VLCS, 0.14 for OfficeHome, and 0.2 for DomainNet, while it is consistently maintained at 1 throughout the training phase.
网络。网络的输出维度与 512 维的 CLIP 特征对齐，因此不需要对输入特征进行下采样。潜在维度根据数据集进行定制：PACS 为 256，OfficeHome 为 448，VLCS 与 DomainNet 为 512，以适应不同类别和数据集的复杂性。权重参数 $\alpha$ 在 PACS、VLCS、OfficeHome、DomainNet 分别调整为 0.208、0.056、0.14 与 0.2，在整个训练阶段始终保持为 1。


Training CLAP. Training parameters are consistent across datasets, employing the Adam optimizer with a learning rate of 0.0001 , limiting training to 8,000 steps with checking the average loss every 480 steps, and instituting early stopping after five checkpoints without a loss decrease of at least 0.01 . Batch sizes are adjusted to 8 for PACS and VLCS, 96 for OfficeHome, and 384 for Domain-Net,with the temperature parameter $\tau$ set at 0.5 for PACS and VLCS,and 0.3 for OfficeHome and DomainNet. The loss coefficient $\lambda$ is set to 1 for PACS and VLCS, and 0.0001 for OfficeHome and DomainNet, due to the first two datasets have less classes. Prompt augmentations, OSD+OCD+SPO, are applied across datasets all with a 0.5 probability. For the PACS and VLCS datasets, Gaussian noise with a zero mean and a standard deviation of 0.02 is randomly inserted at the beginning, middle, or end of the augmented-view prompts to enrich the training samples. In the linear probe evaluations for few-shot analysis, L2 normalization and cross-entropy loss are utilized for training over 1,000 epochs with a batch size of 32, incorporating early stopping with a patience threshold of 10 epochs and a loss decrease criterion of 0.001 .
训练 CLAP。各数据集的训练参数保持一致，使用 Adam 优化器，学习率为 0.0001，训练上限为 8,000 步，每 480 步检查一次平均损失，在五个检查点后若损失未下降至少 0.01 则提前停止。批量大小在 PACS 与 VLCS 为 8，在 OfficeHome 为 96，在 Domain-Net 为 384；温度参数 $\tau$ 在 PACS 与 VLCS 设为 0.5，在 OfficeHome 与 DomainNet 设为 0.3。损失系数 $\lambda$ 在 PACS 与 VLCS 设为 1，在 OfficeHome 与 DomainNet 设为 0.0001，因为前两组数据集的类别较少。提示增强方法（OSD+OCD+SPO）在所有数据集中均以 0.5 的概率应用。对于 PACS 与 VLCS 数据集，在增强视图提示的开始、中间或结束处随机插入均值为 0、标准差为 0.02 的高斯噪声，以丰富训练样本。在少样本分析的线性探测评估中，使用 L2 归一化和交叉熵损失进行 1,000 次训练，批量大小为 32，包含提前停止，耐心阈值为 10 次迭代，损失下降阈值为 0.001。


Training Im.Aug. We train a disentangled network using image augmentation, applying the InfoNCE loss with a temperature parameter $\tau$ set to 0.5 . This include image augmentation techniques,image cropping (scale $\in  \left\lbrack  {{0.64},{1.0}}\right\rbrack$ ) and color distortion (brightness $= {0.5}$ ,hue $= {0.3}$ ),each with a probability of 0.5 . Other training and inference configurations for Im.Aug are consistent with those used for CLAP across all datasets.
训练 Im.Aug。我们使用图像增强训练一个解耦网络，应用带温度参数 $\tau$ 为 0.5 的 InfoNCE 损失。包括图像增强、裁剪（比例 $\in  \left\lbrack  {{0.64},{1.0}}\right\rbrack$）和颜色失真（亮度 $= {0.5}$、色调 $= {0.3}$），每项的概率为 0.5。Im.Aug 的其他训练与推理配置与 CLAP 在所有数据集上的一致。


### A.2 Prompt Augmentation Combinations
### A.2 提示增强组合


In Tab. 1, we explore different combinations of our tailored prompt augmentation techniques and EDA (Easy Data Augmentation) 42 techniques on the VLCS dataset. Each combination demonstrates CLAP's effectiveness in enhancing CLIP's performance and reducing performance disparities. The combination of OSD+OCS+SPO+IGN achieves the highest average accuracy and the least variance, outperforming the EDA techniques. Notably, even without incorporating random noise in the augmentations, CLAP significantly surpasses CLIP in handling perturbations on prompts,as evidenced by the largely reduced ${\Delta }_{\left( NC\right) }$ .
在表 1 中，我们研究了在 VLCS 数据集上不同的定制提示增强技术组合与 EDA（簡易数据增强）42 技术的效果。每种组合都展示了 CLAP 在提升 CLIP 表现和减小性能差异方面的有效性。OSD+OCS+SPO+IGN 的组合获得了最高的平均准确率与最小的方差，优于 EDA 技术。值得注意的是，即使在强化增强中不加入随机噪声，CLAP 在处理提示扰动方面也显著超过 CLIP，体现为 ${\Delta }_{\left( NC\right) }$ 的大幅降低。 


### A.3 Prompt Sources
### A.3 提示来源


In Tab. 2, we examine the effects of various training prompt formats, sourced from different synthetic origins, on the VLCS dataset performance, utilizing
在表 2 中，我们在 VLCS 数据集上研究来自不同合成来源的训练提示格式对性能的影响，利用


EDA techniques. The prompt formats are defined as follows: "Template" refers to the template-based prompts fundamental to our primary approach; "LLM" designates prompts created by ChatGPT-3.5 3, with the generation process elaborated in Appendix B.2. "Random" describes prompts formatted as "a [random] style of [class]," with "[random]" being replaced by terms from a random word generator; and "Prm.Stl." indicates vectorized prompts generated through PromptStyler [9].
EDA 技术。提示格式定义如下：“模板”为我们主要方法的基于模板的提示；“LLM”指由 ChatGPT-3.5 生成的提示，生成过程在附录 B.2 详细说明；“Random”描述为格式为“一个 [random] 风格的 [class]”的提示，其中 “[random]” 将由随机词生成器的词汇替换；“Prm.Stl.” 表示通过 PromptStyler [9] 生成的向量化提示。


Table 1: We evaluate prompt augmentation combinations on the VLCS dataset: OSD (①), OCD (②), ITD (③), ASD (④), SPO (⑤), and IGN (⑥). ZS(Avg.) shows average zero-shot accuracy acoss four distinct inference prompts. CLAP boosts CLIP's accuracy and reduces variances, with ①②⑤⑥ as the optimal combination.
表 1：我们在 VLCS 数据集上评估提示增强组合：OSD（①）、OCD（②）、ITD（③）、ASD（④）、SPO（⑤）和 IGN（⑥）。ZS(Avg.) 显示四种不同推理提示的平均零样本准确率。CLAP 提升了 CLIP 的准确性并降低了方差，且 ①②⑤⑥ 是最佳组合。


<table><tr><td rowspan="2">Metrics</td><td rowspan="2">CLIP (base)</td><td colspan="7">Avg. top-1 acc. (%) of different augmentations</td></tr><tr><td>EDA</td><td>①②③ <br> ④⑤⑥</td><td>①②③ <br> ④⑤</td><td>①②③ <br> ④⑥</td><td>①②③ <br> ④</td><td>③④⑤ <br> ⑥</td><td>①②⑤ <br> ⑥</td></tr><tr><td>ZS(Avg.) (↑)</td><td>77.3</td><td>81.6</td><td>82.0</td><td>80.1</td><td>82.0</td><td>79.6</td><td>82.1</td><td>82.6</td></tr><tr><td>$R\left(  \downarrow  \right)$</td><td>6.1</td><td>1.9</td><td>1.2</td><td>2.5</td><td>0.9</td><td>3.2</td><td>1.6</td><td>0.8</td></tr><tr><td>$\delta \left(  \downarrow  \right)$</td><td>2.8</td><td>0.9</td><td>0.6</td><td>1.2</td><td>0.4</td><td>1.5</td><td>0.7</td><td>0.4</td></tr><tr><td>${\Delta }_{\left( NC\right) }\left(  \downarrow  \right)$</td><td>8.1</td><td>2.3</td><td>1.7</td><td>3.0</td><td>1.8</td><td>3.4</td><td>2.0</td><td>1.6</td></tr></table>
<table><tbody><tr><td rowspan="2">度量</td><td rowspan="2">CLIP（基线）</td><td colspan="7">不同增强的平均前-1 准确率 (%)</td></tr><tr><td>EDA</td><td>①②③ <br/> ④⑤⑥</td><td>①②③ <br/> ④⑤</td><td>①②③ <br/> ④⑥</td><td>①②③ <br/> ④</td><td>③④⑤ <br/> ⑥</td><td>①②⑤ <br/> ⑥</td></tr><tr><td>ZS（平均） (↑)</td><td>77.3</td><td>81.6</td><td>82.0</td><td>80.1</td><td>82.0</td><td>79.6</td><td>82.1</td><td>82.6</td></tr><tr><td>$R\left(  \downarrow  \right)$</td><td>6.1</td><td>1.9</td><td>1.2</td><td>2.5</td><td>0.9</td><td>3.2</td><td>1.6</td><td>0.8</td></tr><tr><td>$\delta \left(  \downarrow  \right)$</td><td>2.8</td><td>0.9</td><td>0.6</td><td>1.2</td><td>0.4</td><td>1.5</td><td>0.7</td><td>0.4</td></tr><tr><td>${\Delta }_{\left( NC\right) }\left(  \downarrow  \right)$</td><td>8.1</td><td>2.3</td><td>1.7</td><td>3.0</td><td>1.8</td><td>3.4</td><td>2.0</td><td>1.6</td></tr></tbody></table>


Table 2: We employ EDA augmentation to train CLAP with diverse prompt sources on the VLCS dataset. Each prompt source contributes to improvements in CLIP's zero-shot performance, with "Random" and "Template" prompts, in their simpler forms, yielding better outcomes.
表2：我们采用 EDA 增强来训练 CLAP，使用来自不同提示来源的 VLCS 数据集。每个提示来源都提升 CLIP 的零-shot 性能，“Random”和“Template”提示在其较简单形式下，能够带来更好的结果。


<table><tr><td rowspan="2">Metrics</td><td rowspan="2">CLIP (base)</td><td colspan="4"></td></tr><tr><td>LLM</td><td>Random</td><td>Prm.Stl.</td><td>Template</td></tr><tr><td>ZS(Avg.) (↑)</td><td>77.3</td><td>78.2</td><td>81.6</td><td>81.2</td><td>81.6</td></tr><tr><td>$R\left(  \downarrow  \right)$</td><td>6.1</td><td>3.2</td><td>0.7</td><td>2.7</td><td>1.9</td></tr><tr><td>$\delta \left(  \downarrow  \right)$</td><td>2.8</td><td>1.5</td><td>0.3</td><td>1.2</td><td>0.9</td></tr><tr><td>${\Delta }_{\left( NC\right) }\left(  \downarrow  \right)$</td><td>8.1</td><td>3.3</td><td>2.3</td><td>3.0</td><td>2.3</td></tr></table>
<table><tbody><tr><td rowspan="2">指标</td><td rowspan="2">CLIP（基础）</td><td colspan="4"></td></tr><tr><td>LLM</td><td>随机</td><td>Prm.Stl.</td><td>模板</td></tr><tr><td>ZS(Avg.) (↑)</td><td>77.3</td><td>78.2</td><td>81.6</td><td>81.2</td><td>81.6</td></tr><tr><td>$R\left(  \downarrow  \right)$</td><td>6.1</td><td>3.2</td><td>0.7</td><td>2.7</td><td>1.9</td></tr><tr><td>$\delta \left(  \downarrow  \right)$</td><td>2.8</td><td>1.5</td><td>0.3</td><td>1.2</td><td>0.9</td></tr><tr><td>${\Delta }_{\left( NC\right) }\left(  \downarrow  \right)$</td><td>8.1</td><td>3.3</td><td>2.3</td><td>3.0</td><td>2.3</td></tr></tbody></table>


Our experimental results indicate that CLAP, when trained across these varied prompt formats, enhances the performance of CLIP. Notably, despite the complex generation mechanisms of "LLM" and "Prm.Stl." prompts, the simpler, random-styled and template-based prompts demonstrate superior efficacy. However, it is important to highlight that the improvements attributed to these diverse prompt formats, trained with EDA, do not surpass the best performance of the prompt augmentations tailored for template-based prompts.
我们的实验结果表明，在这些不同提示格式下训练的 CLAP 能提升 CLIP 的性能。值得注意的是，尽管 "LLM" 与 "Prm.Stl." 提示的生成机制较为复杂，但更简单的随机风格和基于模板的提示显示出更高的有效性。然而，需要强调的是，使用 EDA 训练的多样提示格式所带来的改进并未超过为模板型提示定制的提示增强所达到的最佳性能。


### A.4 Detailed Results on ViT-B/16
### A.4 ViT-B/16 的详细结果


Details on Zero-Shot Evaluations We present the domain-level zero-shot performance with various prompts across each dataset in Tab. 3. CLAP consistently enhances CLIP's zero-shot performance across these different prompts. Given that CLAP exclusively utilizes text data for training, it does not compromise CLIP's inherent ability to generalize across domains, which is acquired from its extensive training dataset. Rather, by achieving a more effective disentanglement of content, it unequivocally enhances CLIP's zero-shot performance across all dataset domains.
关于零样本评估的细节 我们在表3中给出各数据集的域级零样本性能，涵盖不同提示。CLAP 在这些不同提示下持续提升 CLIP 的零样本性能。鉴于 CLAP 仅使用文本数据进行训练，它并不损害 CLIP 从庞大训练数据集中获得的跨域泛化能力。相反，通过实现更有效的内容解耦，CLAP 能在所有数据集域中明确提升 CLIP 的零样本性能。


Table 3: Domain-level zero-shot results of the ViT-B/16 model on the test datasets.
表3：ViT-B/16 模型在测试数据集上的域级零样本结果。


<table><tr><td colspan="2" rowspan="3">Dataset Domains</td><td colspan="12">Domain-level avg. top-1 acc. (%) of zero-shot performance usig ViT-B/16 (↑)</td></tr><tr><td colspan="3">ZS(C)</td><td colspan="3">ZS(CP)</td><td colspan="3">ZS(PC)</td><td colspan="3">ZS(NC)</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>96.4</td><td>96.9</td><td>97.5</td><td>93.4</td><td>97.0</td><td>97.6</td><td>97.4</td><td>97.6</td><td>97.6</td><td>87.8</td><td>93.5</td><td>97.1</td></tr><tr><td>C.</td><td>98.9</td><td>99.0</td><td>98.9</td><td>99.0</td><td>99.2</td><td>99.0</td><td>99.1</td><td>99.0</td><td>98.9</td><td>95.4</td><td>97.6</td><td>98.8</td></tr><tr><td>P.</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.3</td><td>99.6</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>93.1</td><td>99.0</td><td>99.9</td></tr><tr><td>S.</td><td>87.7</td><td>90.1</td><td>92.5</td><td>89.2</td><td>89.6</td><td>92.5</td><td>88.1</td><td>89.4</td><td>92.3</td><td>87.1</td><td>89.3</td><td>93.1</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>99.7</td><td>99.8</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>87.0</td><td>96.0</td><td>99.9</td></tr><tr><td>L.</td><td>61.8</td><td>66.2</td><td>67.7</td><td>69.9</td><td>70.4</td><td>70.4</td><td>70.2</td><td>70.2</td><td>70.7</td><td>55.9</td><td>59.9</td><td>65.9</td></tr><tr><td>S.</td><td>70.1</td><td>74.8</td><td>78.0</td><td>73.3</td><td>76.0</td><td>77.2</td><td>73.6</td><td>76.4</td><td>76.9</td><td>61.4</td><td>66.2</td><td>75.3</td></tr><tr><td>V.</td><td>73.9</td><td>77.1</td><td>84.9</td><td>84.8</td><td>85.4</td><td>86.0</td><td>86.1</td><td>85.6</td><td>86.2</td><td>68.9</td><td>70.3</td><td>82.9</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>80.5</td><td>79.0</td><td>81.8</td><td>80.1</td><td>76.0</td><td>81.6</td><td>83.2</td><td>78.7</td><td>83.2</td><td>73.0</td><td>69.2</td><td>73.6</td></tr><tr><td>C.</td><td>64.6</td><td>59.6</td><td>66.4</td><td>63.7</td><td>58.9</td><td>65.4</td><td>68.1</td><td>61.9</td><td>69.0</td><td>57.0</td><td>52.0</td><td>60.4</td></tr><tr><td>P.</td><td>86.3</td><td>83.6</td><td>87.5</td><td>86.6</td><td>83.4</td><td>87.2</td><td>89.1</td><td>86.6</td><td>89.7</td><td>77.2</td><td>72.3</td><td>78.9</td></tr><tr><td>R.</td><td>88.0</td><td>85.9</td><td>88.5</td><td>87.6</td><td>84.8</td><td>87.7</td><td>89.8</td><td>87.2</td><td>90.0</td><td>79.0</td><td>76.5</td><td>81.1</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>71.0</td><td>64.3</td><td>71.9</td><td>70.5</td><td>62.1</td><td>72.0</td><td>71.3</td><td>63.4</td><td>72.8</td><td>63.2</td><td>53.9</td><td>64.6</td></tr><tr><td>I.</td><td>48.6</td><td>40.5</td><td>50.6</td><td>47.7</td><td>40.7</td><td>49.5</td><td>47.8</td><td>40.0</td><td>50.5</td><td>42.9</td><td>35.0</td><td>45.1</td></tr><tr><td>P.</td><td>66.6</td><td>59.1</td><td>67.7</td><td>66.0</td><td>59.0</td><td>67.3</td><td>66.5</td><td>59.8</td><td>68.4</td><td>57.2</td><td>50.4</td><td>59.4</td></tr><tr><td>Q.</td><td>14.9</td><td>12.4</td><td>15.2</td><td>13.3</td><td>11.5</td><td>13.8</td><td>14.1</td><td>11.8</td><td>14.3</td><td>12.0</td><td>9.2</td><td>13.1</td></tr><tr><td>R.</td><td>82.6</td><td>76.6</td><td>83.1</td><td>82.2</td><td>75.8</td><td>82.2</td><td>83.4</td><td>78.2</td><td>83.7</td><td>75.2</td><td>67.9</td><td>75.6</td></tr><tr><td>S.</td><td>63.1</td><td>56.1</td><td>63.7</td><td>62.2</td><td>55.0</td><td>63.1</td><td>63.4</td><td>56.4</td><td>64.4</td><td>55.7</td><td>47.5</td><td>57.6</td></tr></table>
<table><tbody><tr><td colspan="2" rowspan="3">数据集域</td><td colspan="12">域级平均零-shot 性能 top-1 准确率 (%)，使用 ViT-B/16 (↑)</td></tr><tr><td colspan="3">ZS(C)</td><td colspan="3">ZS(CP)</td><td colspan="3">ZS(PC)</td><td colspan="3">ZS(NC)</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>96.4</td><td>96.9</td><td>97.5</td><td>93.4</td><td>97.0</td><td>97.6</td><td>97.4</td><td>97.6</td><td>97.6</td><td>87.8</td><td>93.5</td><td>97.1</td></tr><tr><td>C.</td><td>98.9</td><td>99.0</td><td>98.9</td><td>99.0</td><td>99.2</td><td>99.0</td><td>99.1</td><td>99.0</td><td>98.9</td><td>95.4</td><td>97.6</td><td>98.8</td></tr><tr><td>P.</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.3</td><td>99.6</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>93.1</td><td>99.0</td><td>99.9</td></tr><tr><td>S.</td><td>87.7</td><td>90.1</td><td>92.5</td><td>89.2</td><td>89.6</td><td>92.5</td><td>88.1</td><td>89.4</td><td>92.3</td><td>87.1</td><td>89.3</td><td>93.1</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>99.7</td><td>99.8</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>87.0</td><td>96.0</td><td>99.9</td></tr><tr><td>L.</td><td>61.8</td><td>66.2</td><td>67.7</td><td>69.9</td><td>70.4</td><td>70.4</td><td>70.2</td><td>70.2</td><td>70.7</td><td>55.9</td><td>59.9</td><td>65.9</td></tr><tr><td>S.</td><td>70.1</td><td>74.8</td><td>78.0</td><td>73.3</td><td>76.0</td><td>77.2</td><td>73.6</td><td>76.4</td><td>76.9</td><td>61.4</td><td>66.2</td><td>75.3</td></tr><tr><td>V.</td><td>73.9</td><td>77.1</td><td>84.9</td><td>84.8</td><td>85.4</td><td>86.0</td><td>86.1</td><td>85.6</td><td>86.2</td><td>68.9</td><td>70.3</td><td>82.9</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>80.5</td><td>79.0</td><td>81.8</td><td>80.1</td><td>76.0</td><td>81.6</td><td>83.2</td><td>78.7</td><td>83.2</td><td>73.0</td><td>69.2</td><td>73.6</td></tr><tr><td>C.</td><td>64.6</td><td>59.6</td><td>66.4</td><td>63.7</td><td>58.9</td><td>65.4</td><td>68.1</td><td>61.9</td><td>69.0</td><td>57.0</td><td>52.0</td><td>60.4</td></tr><tr><td>P.</td><td>86.3</td><td>83.6</td><td>87.5</td><td>86.6</td><td>83.4</td><td>87.2</td><td>89.1</td><td>86.6</td><td>89.7</td><td>77.2</td><td>72.3</td><td>78.9</td></tr><tr><td>R.</td><td>88.0</td><td>85.9</td><td>88.5</td><td>87.6</td><td>84.8</td><td>87.7</td><td>89.8</td><td>87.2</td><td>90.0</td><td>79.0</td><td>76.5</td><td>81.1</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>71.0</td><td>64.3</td><td>71.9</td><td>70.5</td><td>62.1</td><td>72.0</td><td>71.3</td><td>63.4</td><td>72.8</td><td>63.2</td><td>53.9</td><td>64.6</td></tr><tr><td>I.</td><td>48.6</td><td>40.5</td><td>50.6</td><td>47.7</td><td>40.7</td><td>49.5</td><td>47.8</td><td>40.0</td><td>50.5</td><td>42.9</td><td>35.0</td><td>45.1</td></tr><tr><td>P.</td><td>66.6</td><td>59.1</td><td>67.7</td><td>66.0</td><td>59.0</td><td>67.3</td><td>66.5</td><td>59.8</td><td>68.4</td><td>57.2</td><td>50.4</td><td>59.4</td></tr><tr><td>Q.</td><td>14.9</td><td>12.4</td><td>15.2</td><td>13.3</td><td>11.5</td><td>13.8</td><td>14.1</td><td>11.8</td><td>14.3</td><td>12.0</td><td>9.2</td><td>13.1</td></tr><tr><td>R.</td><td>82.6</td><td>76.6</td><td>83.1</td><td>82.2</td><td>75.8</td><td>82.2</td><td>83.4</td><td>78.2</td><td>83.7</td><td>75.2</td><td>67.9</td><td>75.6</td></tr><tr><td>S.</td><td>63.1</td><td>56.1</td><td>63.7</td><td>62.2</td><td>55.0</td><td>63.1</td><td>63.4</td><td>56.4</td><td>64.4</td><td>55.7</td><td>47.5</td><td>57.6</td></tr></tbody></table>


Details on Few-Shot Evaluations We display the quantitative results of few-shot performance in Tab. 4. CLAP consistently enhances the few-shot capabilities, showcasing improvements across test datasets at a closer domain level.
关于少样本评估的细节 We display the quantitative results of few-shot performance in Tab. 4. CLAP consistently enhances the few-shot capabilities, showcasing improvements across test datasets at a closer domain level.


Details on Adversarial Evaluations In Tab. 5, we detail our adversarial performance evaluations for PACS, VLCS, OfficeHome, and DomainNet, respectively. CLAP enhances both zero-shot and one-shot performance across all domains of the tested datasets. While Im.Aug boosts one-shot robustness against adversarial tasks, its impact on zero-shot adversarial robustness is inconsistent.
关于对抗性评估的细节 In Tab. 5, we detail our adversarial performance evaluations for PACS, VLCS, OfficeHome, and DomainNet, respectively. CLAP enhances both zero-shot and one-shot performance across all domains of the tested datasets. While Im.Aug boosts one-shot robustness against adversarial tasks, its impact on zero-shot adversarial robustness is inconsistent.


Details on Ablative Analysis In Tab. 6, we provide detailed results from our analysis on zero-shot performance using various combinations of prompt augmentations. Additionally, in Tab. 7, we present the outcomes of our ablative studies focusing on the hyperparameters $\tau$ ,latent dimension,and $\alpha$ ,respectively, each evaluated domain-wise. The results indicate that CLAP is effective across a wide range of hyperparameters.
关于消融分析的细节 In Tab. 6, we provide detailed results from our analysis on zero-shot performance using various combinations of prompt augmentations. Additionally, in Tab. 7, we present the outcomes of our ablative studies focusing on the hyperparameters $\tau$ ,latent dimension,and $\alpha$ ,respectively, each evaluated domain-wise. The results indicate that CLAP is effective across a wide range of hyperparameters.


## B Data Synthesis
## B 数据合成


### B.1 Synthetic Image Generation
### B.1 合成图像生成


We employ the stable diffusion 39 v2.1 model for generating synthetic images used in our comparing experiments, specifically utilizing the Stable Diffusion v2-1 Model Card available on Hugging Face T For each class across the four datasets, we produce 480 images using our synthetic template prompts as input for the stable diffusion model. All generated images are of ${512} \times  {512}$ resolution. Examples of these synthetic images alongside their corresponding text prompts are displayed in Fig. 1
We employ the stable diffusion 39 v2.1 model for generating synthetic images used in our comparing experiments, specifically utilizing the Stable Diffusion v2-1 Model Card available on Hugging Face T For each class across the four datasets, we produce 480 images using our synthetic template prompts as input for the stable diffusion model. All generated images are of ${512} \times  {512}$ resolution. Examples of these synthetic images alongside their corresponding text prompts are displayed in Fig. 1


Table 4: Domain-level few-shot results of the ViT-B/16 model using the test datasets.
表 4：使用 ViT-B/16 模型在测试数据集上的领域级少样本结果。


<table><tr><td colspan="2" rowspan="3">Dataset Domains</td><td colspan="15">Domain-level avg. top-1 acc. (%) of few-shot performance of ViT-B/16 (↑)</td></tr><tr><td colspan="3">1-shot</td><td colspan="3">4-shot</td><td colspan="3">8-shot</td><td colspan="3">16-shot</td><td colspan="3">32-shot</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>79.5</td><td>84.1</td><td>94.5</td><td>92.4</td><td>96.4</td><td>97.2</td><td>95.1</td><td>97.2</td><td>98.4</td><td>97.9</td><td>98.1</td><td>98.4</td><td>98.8</td><td>99.1</td><td>98.9</td></tr><tr><td>C.</td><td>86.7</td><td>96.1</td><td>98.3</td><td>96.8</td><td>98.6</td><td>99.2</td><td>98.8</td><td>98.9</td><td>99.3</td><td>99.5</td><td>99.2</td><td>99.5</td><td>99.6</td><td>99.6</td><td>99.6</td></tr><tr><td>P.</td><td>97.4</td><td>99.8</td><td>99.9</td><td>99.6</td><td>99.8</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td></tr><tr><td>S.</td><td>75.1</td><td>80.0</td><td>87.3</td><td>91.1</td><td>92.3</td><td>92.5</td><td>92.3</td><td>92.3</td><td>92.9</td><td>92.4</td><td>92.6</td><td>93.1</td><td>93.9</td><td>94.2</td><td>94.1</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>99.2</td><td>99.7</td><td>99.8</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.8</td><td>99.7</td><td>99.9</td><td>99.7</td><td>99.9</td><td>99.9</td><td>99.9</td><td>100.0</td><td>99.9</td></tr><tr><td>L.</td><td>41.3</td><td>41.3</td><td>41.1</td><td>56.7</td><td>57.0</td><td>59.8</td><td>46.2</td><td>36.8</td><td>48.3</td><td>59.4</td><td>60.4</td><td>62.6</td><td>60.4</td><td>60.7</td><td>61.9</td></tr><tr><td>S.</td><td>45.3</td><td>46.1</td><td>50.8</td><td>61.9</td><td>63.7</td><td>69.0</td><td>67.4</td><td>67.7</td><td>71.3</td><td>75.9</td><td>76.8</td><td>80.9</td><td>77.4</td><td>78.6</td><td>81.0</td></tr><tr><td>V.</td><td>50.9</td><td>53.4</td><td>59.0</td><td>64.5</td><td>66.7</td><td>76.1</td><td>75.4</td><td>74.1</td><td>78.7</td><td>72.6</td><td>73.9</td><td>77.7</td><td>85.7</td><td>86.1</td><td>87.9</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>42.6</td><td>45.1</td><td>43.9</td><td>76.8</td><td>77.6</td><td>77.7</td><td>84.8</td><td>86.0</td><td>85.5</td><td>91.8</td><td>92.1</td><td>92.1</td><td>97.4</td><td>97.5</td><td>97.5</td></tr><tr><td>C.</td><td>40.1</td><td>45.0</td><td>43.8</td><td>69.9</td><td>70.2</td><td>70.5</td><td>75.8</td><td>75.9</td><td>76.6</td><td>81.6</td><td>81.6</td><td>81.6</td><td>89.0</td><td>89.0</td><td>89.2</td></tr><tr><td>P.</td><td>70.2</td><td>73.3</td><td>73.4</td><td>89.7</td><td>90.3</td><td>90.2</td><td>93.8</td><td>93.7</td><td>93.9</td><td>95.7</td><td>95.7</td><td>95.8</td><td>97.7</td><td>97.6</td><td>97.6</td></tr><tr><td>R.</td><td>58.4</td><td>59.3</td><td>59.4</td><td>81.7</td><td>83.1</td><td>82.9</td><td>89.7</td><td>89.5</td><td>89.9</td><td>92.9</td><td>92.7</td><td>93.2</td><td>95.8</td><td>95.8</td><td>95.8</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>42.1</td><td>43.6</td><td>43.8</td><td>66.8</td><td>67.5</td><td>67.8</td><td>74.2</td><td>74.3</td><td>74.6</td><td>78.5</td><td>78.6</td><td>78.8</td><td>82.8</td><td>82.8</td><td>82.7</td></tr><tr><td>I.</td><td>19.5</td><td>20.8</td><td>21.0</td><td>38.5</td><td>39.3</td><td>39.7</td><td>46.7</td><td>47.0</td><td>47.3</td><td>53.2</td><td>53.2</td><td>53.6</td><td>60.0</td><td>59.9</td><td>60.1</td></tr><tr><td>P.</td><td>32.1</td><td>33.5</td><td>34.2</td><td>60.5</td><td>60.9</td><td>61.5</td><td>68.0</td><td>68.0</td><td>68.7</td><td>72.5</td><td>72.6</td><td>73.0</td><td>76.7</td><td>76.6</td><td>76.8</td></tr><tr><td>Q.</td><td>15.2</td><td>15.3</td><td>15.3</td><td>30.0</td><td>29.6</td><td>29.9</td><td>37.1</td><td>36.4</td><td>36.8</td><td>43.8</td><td>43.4</td><td>43.5</td><td>49.4</td><td>49.1</td><td>49.0</td></tr><tr><td>R.</td><td>50.8</td><td>52.1</td><td>52.7</td><td>76.7</td><td>77.0</td><td>77.6</td><td>81.7</td><td>81.9</td><td>82.2</td><td>84.0</td><td>83.9</td><td>84.3</td><td>85.9</td><td>85.9</td><td>86.0</td></tr><tr><td>S.</td><td>33.1</td><td>33.9</td><td>34.8</td><td>56.2</td><td>56.6</td><td>57.2</td><td>62.9</td><td>62.9</td><td>63.7</td><td>67.8</td><td>67.7</td><td>68.1</td><td>72.5</td><td>72.3</td><td>72.6</td></tr></table>
<table><tbody><tr><td colspan="2" rowspan="3">数据集领域</td><td colspan="15">域级平均前1准确率 (%) 的少样本 ViT-B/16 表现（↑）</td></tr><tr><td colspan="3">1-shot</td><td colspan="3">4-shot</td><td colspan="3">8-shot</td><td colspan="3">16-shot</td><td colspan="3">32-shot</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>79.5</td><td>84.1</td><td>94.5</td><td>92.4</td><td>96.4</td><td>97.2</td><td>95.1</td><td>97.2</td><td>98.4</td><td>97.9</td><td>98.1</td><td>98.4</td><td>98.8</td><td>99.1</td><td>98.9</td></tr><tr><td>C.</td><td>86.7</td><td>96.1</td><td>98.3</td><td>96.8</td><td>98.6</td><td>99.2</td><td>98.8</td><td>98.9</td><td>99.3</td><td>99.5</td><td>99.2</td><td>99.5</td><td>99.6</td><td>99.6</td><td>99.6</td></tr><tr><td>P.</td><td>97.4</td><td>99.8</td><td>99.9</td><td>99.6</td><td>99.8</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td></tr><tr><td>S.</td><td>75.1</td><td>80.0</td><td>87.3</td><td>91.1</td><td>92.3</td><td>92.5</td><td>92.3</td><td>92.3</td><td>92.9</td><td>92.4</td><td>92.6</td><td>93.1</td><td>93.9</td><td>94.2</td><td>94.1</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>99.2</td><td>99.7</td><td>99.8</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.8</td><td>99.7</td><td>99.9</td><td>99.7</td><td>99.9</td><td>99.9</td><td>99.9</td><td>100.0</td><td>99.9</td></tr><tr><td>L.</td><td>41.3</td><td>41.3</td><td>41.1</td><td>56.7</td><td>57.0</td><td>59.8</td><td>46.2</td><td>36.8</td><td>48.3</td><td>59.4</td><td>60.4</td><td>62.6</td><td>60.4</td><td>60.7</td><td>61.9</td></tr><tr><td>S.</td><td>45.3</td><td>46.1</td><td>50.8</td><td>61.9</td><td>63.7</td><td>69.0</td><td>67.4</td><td>67.7</td><td>71.3</td><td>75.9</td><td>76.8</td><td>80.9</td><td>77.4</td><td>78.6</td><td>81.0</td></tr><tr><td>V.</td><td>50.9</td><td>53.4</td><td>59.0</td><td>64.5</td><td>66.7</td><td>76.1</td><td>75.4</td><td>74.1</td><td>78.7</td><td>72.6</td><td>73.9</td><td>77.7</td><td>85.7</td><td>86.1</td><td>87.9</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>42.6</td><td>45.1</td><td>43.9</td><td>76.8</td><td>77.6</td><td>77.7</td><td>84.8</td><td>86.0</td><td>85.5</td><td>91.8</td><td>92.1</td><td>92.1</td><td>97.4</td><td>97.5</td><td>97.5</td></tr><tr><td>C.</td><td>40.1</td><td>45.0</td><td>43.8</td><td>69.9</td><td>70.2</td><td>70.5</td><td>75.8</td><td>75.9</td><td>76.6</td><td>81.6</td><td>81.6</td><td>81.6</td><td>89.0</td><td>89.0</td><td>89.2</td></tr><tr><td>P.</td><td>70.2</td><td>73.3</td><td>73.4</td><td>89.7</td><td>90.3</td><td>90.2</td><td>93.8</td><td>93.7</td><td>93.9</td><td>95.7</td><td>95.7</td><td>95.8</td><td>97.7</td><td>97.6</td><td>97.6</td></tr><tr><td>R.</td><td>58.4</td><td>59.3</td><td>59.4</td><td>81.7</td><td>83.1</td><td>82.9</td><td>89.7</td><td>89.5</td><td>89.9</td><td>92.9</td><td>92.7</td><td>93.2</td><td>95.8</td><td>95.8</td><td>95.8</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>42.1</td><td>43.6</td><td>43.8</td><td>66.8</td><td>67.5</td><td>67.8</td><td>74.2</td><td>74.3</td><td>74.6</td><td>78.5</td><td>78.6</td><td>78.8</td><td>82.8</td><td>82.8</td><td>82.7</td></tr><tr><td>I.</td><td>19.5</td><td>20.8</td><td>21.0</td><td>38.5</td><td>39.3</td><td>39.7</td><td>46.7</td><td>47.0</td><td>47.3</td><td>53.2</td><td>53.2</td><td>53.6</td><td>60.0</td><td>59.9</td><td>60.1</td></tr><tr><td>P.</td><td>32.1</td><td>33.5</td><td>34.2</td><td>60.5</td><td>60.9</td><td>61.5</td><td>68.0</td><td>68.0</td><td>68.7</td><td>72.5</td><td>72.6</td><td>73.0</td><td>76.7</td><td>76.6</td><td>76.8</td></tr><tr><td>Q.</td><td>15.2</td><td>15.3</td><td>15.3</td><td>30.0</td><td>29.6</td><td>29.9</td><td>37.1</td><td>36.4</td><td>36.8</td><td>43.8</td><td>43.4</td><td>43.5</td><td>49.4</td><td>49.1</td><td>49.0</td></tr><tr><td>R.</td><td>50.8</td><td>52.1</td><td>52.7</td><td>76.7</td><td>77.0</td><td>77.6</td><td>81.7</td><td>81.9</td><td>82.2</td><td>84.0</td><td>83.9</td><td>84.3</td><td>85.9</td><td>85.9</td><td>86.0</td></tr><tr><td>S.</td><td>33.1</td><td>33.9</td><td>34.8</td><td>56.2</td><td>56.6</td><td>57.2</td><td>62.9</td><td>62.9</td><td>63.7</td><td>67.8</td><td>67.7</td><td>68.1</td><td>72.5</td><td>72.3</td><td>72.6</td></tr></tbody></table>


<img src="https://cdn.noedgeai.com/bo_d6aqku77aajc739ardu0_22.jpg?x=409&y=957&w=988&h=272&r=0"/>



Fig. 1: Examples of synthetic images created with SDv2.1 and associated prompts.
图 1：使用 SDv2.1 及相关提示创建的合成图像示例。


### B.2 LLM Prompts Generation
### B.2 LLM 提示生成


We utilize ChatGPT-3.5 3 to create the LLM prompts employed in our comparative analysis of different prompt sources. Fig. 2 illustrates the process of prompting ChatGPT-3.5 to generate text prompts for specific class names. For each class, we produce 120 samples, and below are a few examples from the generated prompts:
我们利用 ChatGPT-3.5 3 来创建在我们对不同提示来源的对比分析中使用的 LLM 提示。图 2 展示了让 ChatGPT-3.5 生成针对特定类别名的文本提示的过程。对于每个类别，我们生成 120 个样本，以下是生成的提示中的一些示例：


- Bird:
- 鸟：


---



1 https://huggingface.co/stabilityai/stable-diffusion-2-1
1 https://huggingface.co/stabilityai/stable-diffusion-2-1


---



Table 5: Domain-level results under adversarial attacks of ViT-B/16 on the datasets.
表 5：ViT-B/16 在数据集上的对抗攻击域级结果。


<table><tr><td rowspan="4" colspan="2">Dataset Domains</td><td colspan="18">Domain-level avg. top-1 acc. (%) under adversarial attackings using ViT-B/16 (↑)</td></tr><tr><td colspan="6">FGSM</td><td colspan="6">PGD-20</td><td colspan="6">CW-20</td></tr><tr><td></td><td>ZS-C</td><td></td><td colspan="3">1-shot</td><td colspan="3">ZS-C</td><td colspan="3">1-shot</td><td colspan="3">ZS-C</td><td colspan="3">1-shot</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>76.3</td><td>79.3</td><td>79.3</td><td>61.2</td><td>78.0</td><td>87.3</td><td>1.7</td><td>2.2</td><td>1.8</td><td>16.0</td><td>42.1</td><td>63.1</td><td>1.5</td><td>2.0</td><td>2.3</td><td>0.5</td><td>1.1</td><td>1.7</td></tr><tr><td>C.</td><td>94.9</td><td>95.0</td><td>94.0</td><td>66.5</td><td>84.2</td><td>95.1</td><td>33.3</td><td>37.7</td><td>35.6</td><td>33.3</td><td>57.2</td><td>86.1</td><td>28.8</td><td>34.0</td><td>33.2</td><td>11.9</td><td>23.6</td><td>31.8</td></tr><tr><td>P.</td><td>91.6</td><td>90.3</td><td>91.7</td><td>67.4</td><td>80.8</td><td>92.1</td><td>5.7</td><td>7.0</td><td>6.7</td><td>27.1</td><td>55.0</td><td>69.8</td><td>4.7</td><td>4.9</td><td>5.8</td><td>0.7</td><td>2.7</td><td>4.1</td></tr><tr><td>S.</td><td>84.5</td><td>87.5</td><td>89.8</td><td>71.6</td><td>74.6</td><td>83.8</td><td>75.8</td><td>78.4</td><td>79.2</td><td>63.0</td><td>66.3</td><td>74.6</td><td>74.5</td><td>76.8</td><td>77.9</td><td>62.7</td><td>65.4</td><td>70.3</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>55.3</td><td>53.8</td><td>55.5</td><td>25.8</td><td>28.8</td><td>25.3</td><td>4.4</td><td>5.1</td><td>4.7</td><td>2.0</td><td>5.2</td><td>2.5</td><td>2.9</td><td>3.1</td><td>3.5</td><td>0.7</td><td>1.2</td><td>1.0</td></tr><tr><td>L.</td><td>49.4</td><td>45.5</td><td>50.6</td><td>27.0</td><td>32.6</td><td>30.4</td><td>15.2</td><td>14.9</td><td>16.0</td><td>6.4</td><td>8.9</td><td>8.0</td><td>12.4</td><td>11.2</td><td>13.0</td><td>6.1</td><td>8.3</td><td>7.7</td></tr><tr><td>S.</td><td>61.7</td><td>58.1</td><td>62.5</td><td>48.0</td><td>46.9</td><td>51.6</td><td>13.2</td><td>13.9</td><td>14.0</td><td>8.6</td><td>10.7</td><td>10.0</td><td>9.2</td><td>8.8</td><td>10.2</td><td>8.3</td><td>7.9</td><td>8.4</td></tr><tr><td>V.</td><td>65.3</td><td>63.2</td><td>65.6</td><td>36.5</td><td>40.1</td><td>41.0</td><td>7.5</td><td>7.9</td><td>7.9</td><td>5.3</td><td>9.4</td><td>8.9</td><td>5.2</td><td>4.8</td><td>5.6</td><td>2.9</td><td>2.8</td><td>2.9</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>55.3</td><td>53.8</td><td>55.5</td><td>25.8</td><td>28.8</td><td>25.3</td><td>4.4</td><td>5.1</td><td>4.7</td><td>2.0</td><td>5.2</td><td>2.5</td><td>2.9</td><td>3.1</td><td>3.5</td><td>0.7</td><td>1.2</td><td>1.0</td></tr><tr><td>C.</td><td>49.4</td><td>45.5</td><td>50.6</td><td>27.0</td><td>32.6</td><td>30.4</td><td>15.2</td><td>14.9</td><td>16.0</td><td>6.4</td><td>8.9</td><td>8.0</td><td>12.4</td><td>11.2</td><td>13.0</td><td>6.1</td><td>8.3</td><td>7.7</td></tr><tr><td>P.</td><td>61.7</td><td>58.1</td><td>62.5</td><td>48.0</td><td>46.9</td><td>51.6</td><td>13.2</td><td>13.9</td><td>14.0</td><td>8.6</td><td>10.7</td><td>10.0</td><td>9.2</td><td>8.8</td><td>10.2</td><td>8.3</td><td>7.9</td><td>8.4</td></tr><tr><td>R.</td><td>65.3</td><td>63.2</td><td>65.6</td><td>36.5</td><td>40.1</td><td>41.0</td><td>7.5</td><td>7.9</td><td>7.9</td><td>5.3</td><td>9.4</td><td>8.9</td><td>5.2</td><td>4.8</td><td>5.6</td><td>2.9</td><td>2.8</td><td>2.9</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>57.8</td><td>50.9</td><td>58.8</td><td>33.3</td><td>34.3</td><td>35.0</td><td>21.6</td><td>18.7</td><td>22.8</td><td>18.4</td><td>19.6</td><td>20.0</td><td>15.8</td><td>12.5</td><td>16.6</td><td>7.0</td><td>7.5</td><td>7.8</td></tr><tr><td>I.</td><td>35.8</td><td>28.0</td><td>37.0</td><td>12.2</td><td>13.3</td><td>13.2</td><td>6.1</td><td>3.7</td><td>6.7</td><td>4.6</td><td>5.3</td><td>5.1</td><td>3.3</td><td>1.9</td><td>3.7</td><td>0.9</td><td>0.9</td><td>0.9</td></tr><tr><td>P.</td><td>43.9</td><td>39.0</td><td>44.3</td><td>18.4</td><td>20.6</td><td>20.3</td><td>3.1</td><td>2.8</td><td>3.3</td><td>8.6</td><td>10.4</td><td>9.9</td><td>1.8</td><td>1.3</td><td>1.9</td><td>0.3</td><td>0.3</td><td>0.3</td></tr><tr><td>Q.</td><td>12.9</td><td>10.3</td><td>13.2</td><td>10.9</td><td>10.8</td><td>11.1</td><td>8.4</td><td>6.8</td><td>8.6</td><td>5.4</td><td>5.4</td><td>5.6</td><td>7.1</td><td>5.4</td><td>7.4</td><td>4.9</td><td>4.8</td><td>5.1</td></tr><tr><td>R.</td><td>62.1</td><td>55.9</td><td>62.4</td><td>34.5</td><td>35.9</td><td>36.5</td><td>7.1</td><td>6.5</td><td>7.5</td><td>17.6</td><td>19.7</td><td>19.6</td><td>4.5</td><td>3.4</td><td>4.7</td><td>1.2</td><td>1.4</td><td>1.4</td></tr><tr><td>S.</td><td>49.1</td><td>43.3</td><td>49.7</td><td>25.7</td><td>26.0</td><td>27.5</td><td>17.8</td><td>15.5</td><td>18.6</td><td>13.6</td><td>14.4</td><td>15.1</td><td>13.4</td><td>10.2</td><td>13.9</td><td>5.0</td><td>5.2</td><td>5.6</td></tr></table>
<table><tbody><tr><td rowspan="4" colspan="2">数据集域</td><td colspan="18">在对抗攻击下，使用 ViT-B/16 的域级平均顶1准确率 (%)（↑）</td></tr><tr><td colspan="6">FGSM</td><td colspan="6">PGD-20</td><td colspan="6">CW-20</td></tr><tr><td></td><td>ZS-C</td><td></td><td colspan="3">1-shot</td><td colspan="3">ZS-C</td><td colspan="3">1-shot</td><td colspan="3">ZS-C</td><td colspan="3">1-shot</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>76.3</td><td>79.3</td><td>79.3</td><td>61.2</td><td>78.0</td><td>87.3</td><td>1.7</td><td>2.2</td><td>1.8</td><td>16.0</td><td>42.1</td><td>63.1</td><td>1.5</td><td>2.0</td><td>2.3</td><td>0.5</td><td>1.1</td><td>1.7</td></tr><tr><td>C.</td><td>94.9</td><td>95.0</td><td>94.0</td><td>66.5</td><td>84.2</td><td>95.1</td><td>33.3</td><td>37.7</td><td>35.6</td><td>33.3</td><td>57.2</td><td>86.1</td><td>28.8</td><td>34.0</td><td>33.2</td><td>11.9</td><td>23.6</td><td>31.8</td></tr><tr><td>P.</td><td>91.6</td><td>90.3</td><td>91.7</td><td>67.4</td><td>80.8</td><td>92.1</td><td>5.7</td><td>7.0</td><td>6.7</td><td>27.1</td><td>55.0</td><td>69.8</td><td>4.7</td><td>4.9</td><td>5.8</td><td>0.7</td><td>2.7</td><td>4.1</td></tr><tr><td>S.</td><td>84.5</td><td>87.5</td><td>89.8</td><td>71.6</td><td>74.6</td><td>83.8</td><td>75.8</td><td>78.4</td><td>79.2</td><td>63.0</td><td>66.3</td><td>74.6</td><td>74.5</td><td>76.8</td><td>77.9</td><td>62.7</td><td>65.4</td><td>70.3</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>55.3</td><td>53.8</td><td>55.5</td><td>25.8</td><td>28.8</td><td>25.3</td><td>4.4</td><td>5.1</td><td>4.7</td><td>2.0</td><td>5.2</td><td>2.5</td><td>2.9</td><td>3.1</td><td>3.5</td><td>0.7</td><td>1.2</td><td>1.0</td></tr><tr><td>L.</td><td>49.4</td><td>45.5</td><td>50.6</td><td>27.0</td><td>32.6</td><td>30.4</td><td>15.2</td><td>14.9</td><td>16.0</td><td>6.4</td><td>8.9</td><td>8.0</td><td>12.4</td><td>11.2</td><td>13.0</td><td>6.1</td><td>8.3</td><td>7.7</td></tr><tr><td>S.</td><td>61.7</td><td>58.1</td><td>62.5</td><td>48.0</td><td>46.9</td><td>51.6</td><td>13.2</td><td>13.9</td><td>14.0</td><td>8.6</td><td>10.7</td><td>10.0</td><td>9.2</td><td>8.8</td><td>10.2</td><td>8.3</td><td>7.9</td><td>8.4</td></tr><tr><td>V.</td><td>65.3</td><td>63.2</td><td>65.6</td><td>36.5</td><td>40.1</td><td>41.0</td><td>7.5</td><td>7.9</td><td>7.9</td><td>5.3</td><td>9.4</td><td>8.9</td><td>5.2</td><td>4.8</td><td>5.6</td><td>2.9</td><td>2.8</td><td>2.9</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>55.3</td><td>53.8</td><td>55.5</td><td>25.8</td><td>28.8</td><td>25.3</td><td>4.4</td><td>5.1</td><td>4.7</td><td>2.0</td><td>5.2</td><td>2.5</td><td>2.9</td><td>3.1</td><td>3.5</td><td>0.7</td><td>1.2</td><td>1.0</td></tr><tr><td>C.</td><td>49.4</td><td>45.5</td><td>50.6</td><td>27.0</td><td>32.6</td><td>30.4</td><td>15.2</td><td>14.9</td><td>16.0</td><td>6.4</td><td>8.9</td><td>8.0</td><td>12.4</td><td>11.2</td><td>13.0</td><td>6.1</td><td>8.3</td><td>7.7</td></tr><tr><td>P.</td><td>61.7</td><td>58.1</td><td>62.5</td><td>48.0</td><td>46.9</td><td>51.6</td><td>13.2</td><td>13.9</td><td>14.0</td><td>8.6</td><td>10.7</td><td>10.0</td><td>9.2</td><td>8.8</td><td>10.2</td><td>8.3</td><td>7.9</td><td>8.4</td></tr><tr><td>R.</td><td>65.3</td><td>63.2</td><td>65.6</td><td>36.5</td><td>40.1</td><td>41.0</td><td>7.5</td><td>7.9</td><td>7.9</td><td>5.3</td><td>9.4</td><td>8.9</td><td>5.2</td><td>4.8</td><td>5.6</td><td>2.9</td><td>2.8</td><td>2.9</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>57.8</td><td>50.9</td><td>58.8</td><td>33.3</td><td>34.3</td><td>35.0</td><td>21.6</td><td>18.7</td><td>22.8</td><td>18.4</td><td>19.6</td><td>20.0</td><td>15.8</td><td>12.5</td><td>16.6</td><td>7.0</td><td>7.5</td><td>7.8</td></tr><tr><td>I.</td><td>35.8</td><td>28.0</td><td>37.0</td><td>12.2</td><td>13.3</td><td>13.2</td><td>6.1</td><td>3.7</td><td>6.7</td><td>4.6</td><td>5.3</td><td>5.1</td><td>3.3</td><td>1.9</td><td>3.7</td><td>0.9</td><td>0.9</td><td>0.9</td></tr><tr><td>P.</td><td>43.9</td><td>39.0</td><td>44.3</td><td>18.4</td><td>20.6</td><td>20.3</td><td>3.1</td><td>2.8</td><td>3.3</td><td>8.6</td><td>10.4</td><td>9.9</td><td>1.8</td><td>1.3</td><td>1.9</td><td>0.3</td><td>0.3</td><td>0.3</td></tr><tr><td>Q.</td><td>12.9</td><td>10.3</td><td>13.2</td><td>10.9</td><td>10.8</td><td>11.1</td><td>8.4</td><td>6.8</td><td>8.6</td><td>5.4</td><td>5.4</td><td>5.6</td><td>7.1</td><td>5.4</td><td>7.4</td><td>4.9</td><td>4.8</td><td>5.1</td></tr><tr><td>R.</td><td>62.1</td><td>55.9</td><td>62.4</td><td>34.5</td><td>35.9</td><td>36.5</td><td>7.1</td><td>6.5</td><td>7.5</td><td>17.6</td><td>19.7</td><td>19.6</td><td>4.5</td><td>3.4</td><td>4.7</td><td>1.2</td><td>1.4</td><td>1.4</td></tr><tr><td>S.</td><td>49.1</td><td>43.3</td><td>49.7</td><td>25.7</td><td>26.0</td><td>27.5</td><td>17.8</td><td>15.5</td><td>18.6</td><td>13.6</td><td>14.4</td><td>15.1</td><td>13.4</td><td>10.2</td><td>13.9</td><td>5.0</td><td>5.2</td><td>5.6</td></tr></tbody></table>


- A pair of vibrant macaws converse in a lush, tropical rainforest, depicted in a lively, exotic wildlife painting.
- 一对生机勃勃的金刚鹦鹦在郁郁葱葱的热带雨林中交谈，展现于生动而异域的野生动物绘画。


- A solitary eagle watches over a vast, rugged canyon at sunrise, portrayed in a majestic, wilderness landscape photograph.
- 孤独的鹰在日出时分俯瞰广袤崎岖的峡谷，呈现于庄严的荒野风景 photograph。


- Dog:
- Dog:


- A sleek Whippet races in a competitive dog track, illustrated in a fast-paced, dynamic sports style.
- 一条修长的细犬在竞技犬道驰骋，以快速、动态的运动风格描绘。


- A sturdy and reliable English Bulldog watching over a small shop, its solid presence reassuring to the owner.
- 结实可靠的英国斗牛犬守望着一家小店，其坚定存在感让店主安心。


- Car:
- Car:


- A quirky art car parades through the streets in a colorful festival, captured in a fun, expressive style illustration.
- 古怪的艺术车在街头穿梭于多彩节日，捕捉于有趣、富有表现力的风格插画。


- A high-tech, autonomous car maneuvers through a smart city environment, portrayed in a futuristic, sci-fi digital art piece.
- 高科技的自动驾驶汽车在智能城市环境中穿行，呈现于未来感的科幻数字艺术作品。


- Chair:
- Chair:


- A folding chair at an outdoor wedding, elegantly decorated and part of a beautiful ceremony.
- 户外婚礼上的折叠椅，装饰优雅，成为美丽仪式的一部分。


- A high-end executive chair in a law firm, projecting authority and professionalism.
- 一把顶级高管椅在律师事务所中，散发权威与专业气质。


## Person:
## Person:


- An energetic coach motivates a team on a sports field, illustrated in an inspiring, leadership-focused painting.
- 激情的教练在运动场上激励团队，描绘出鼓舞人心的领导力主题画作。


- A graceful figure skater glides across an ice rink, captured in a delicate, winter-themed pastel drawing.
- 优雅的花样滑冰运动员在冰 rink 上滑行，捕捉于脆弱的冬日色调粉彩画中。


Table 6: Zero-Shot Performance on VLCS Dataset Across Varied Augmentation Combinations and Prompt Sources: ① Random Object Size Deletion, ② Random Object Color Deletion, (c) Random Image Type Deletion, (c) Random Art Style Deletion, (5) Random Swapping Order, (6) Addition of Gaussian Noise.
Table 6: Zero-Shot Performance on VLCS Dataset Across Varied Augmentation Deletions and Prompt Sources: ① Random Object Size Deletion, ② Random Object Color Deletion, (c) Random Image Type Deletion, (c) Random Art Style Deletion, (5) Random Swapping Order, (6) Gaussian Noise Addition.


<table><tr><td rowspan="3">Method Domains</td><td rowspan="3"></td><td colspan="11">Avg. top-1 acc. (%) (↑) of different augmentations and prompts on VLCS</td></tr><tr><td>CLIP</td><td>①②③</td><td>①②③</td><td>①②③</td><td>①②③</td><td>③④⑤</td><td>①②⑤</td><td colspan="4">EDA</td></tr><tr><td>(base)</td><td>④⑤⑥</td><td>④⑤</td><td>④⑥</td><td>④</td><td>⑥</td><td>⑥</td><td>LLM</td><td>Rand.</td><td>Pr.St.</td><td>Temp.</td></tr><tr><td rowspan="4">ZS(C)</td><td>C.</td><td>99.7</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.9</td><td>97.9</td><td>99.7</td><td>99.9</td><td>99.9</td></tr><tr><td>L.</td><td>61.8</td><td>66.6</td><td>62.3</td><td>67.0</td><td>62.2</td><td>66.2</td><td>67.7</td><td>66.2</td><td>69.0</td><td>67.3</td><td>66.5</td></tr><tr><td>S.</td><td>70.1</td><td>78.1</td><td>75.5</td><td>78.0</td><td>74.3</td><td>78.5</td><td>78.0</td><td>73.2</td><td>76.9</td><td>73.5</td><td>76.9</td></tr><tr><td>V.</td><td>73.9</td><td>82.8</td><td>80.6</td><td>83.2</td><td>79.3</td><td>82.7</td><td>84.9</td><td>72.6</td><td>81.8</td><td>81.8</td><td>81.9</td></tr><tr><td rowspan="4">ZS(CP)</td><td>C.</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.9</td><td>99.9</td></tr><tr><td>L.</td><td>69.9</td><td>69.3</td><td>67.9</td><td>69.6</td><td>68.4</td><td>70.0</td><td>70.4</td><td>69.3</td><td>70.4</td><td>71.2</td><td>69.7</td></tr><tr><td>S.</td><td>73.3</td><td>77.6</td><td>76.4</td><td>76.7</td><td>75.9</td><td>78.8</td><td>77.2</td><td>76.2</td><td>75.2</td><td>75.1</td><td>78.0</td></tr><tr><td>V.</td><td>84.8</td><td>85.3</td><td>84.0</td><td>85.3</td><td>84.2</td><td>85.1</td><td>86.0</td><td>77.0</td><td>84.2</td><td>86.0</td><td>84.6</td></tr><tr><td rowspan="4">ZS(PC)</td><td>C.</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td></tr><tr><td>L.</td><td>70.2</td><td>70.0</td><td>68.0</td><td>70.1</td><td>68.5</td><td>70.0</td><td>70.7</td><td>67.5</td><td>70.6</td><td>71.8</td><td>70.0</td></tr><tr><td>S.</td><td>73.6</td><td>76.6</td><td>75.6</td><td>76.0</td><td>74.8</td><td>77.8</td><td>76.9</td><td>76.9</td><td>75.1</td><td>74.9</td><td>78.2</td></tr><tr><td>V.</td><td>86.1</td><td>85.7</td><td>84.7</td><td>85.7</td><td>84.5</td><td>85.5</td><td>86.2</td><td>78.2</td><td>84.6</td><td>86.8</td><td>84.8</td></tr><tr><td rowspan="4">ZS(NC)</td><td>C.</td><td>87.0</td><td>99.8</td><td>99.6</td><td>99.8</td><td>99.4</td><td>99.7</td><td>99.9</td><td>95.3</td><td>98.6</td><td>99.6</td><td>99.8</td></tr><tr><td>L.</td><td>55.9</td><td>65.2</td><td>61.3</td><td>65.6</td><td>60.5</td><td>65.4</td><td>65.9</td><td>63.0</td><td>66.7</td><td>64.0</td><td>64.7</td></tr><tr><td>S.</td><td>61.4</td><td>75.6</td><td>70.3</td><td>75.2</td><td>68.3</td><td>74.9</td><td>75.3</td><td>68.9</td><td>73.3</td><td>69.8</td><td>73.0</td></tr><tr><td>V.</td><td>68.9</td><td>80.1</td><td>75.2</td><td>80.4</td><td>73.8</td><td>79.4</td><td>82.9</td><td>69.3</td><td>79.6</td><td>77.2</td><td>78.6</td></tr></table>
<table><tbody><tr><td rowspan="3">方法域</td><td rowspan="3"></td><td colspan="11">不同增强和提示在 VLCS 上的平均前1准确率 (%) (↑)</td></tr><tr><td>CLIP</td><td>①②③</td><td>①②③</td><td>①②③</td><td>①②③</td><td>③④⑤</td><td>①②⑤</td><td colspan="4">EDA</td></tr><tr><td>(base)</td><td>④⑤⑥</td><td>④⑤</td><td>④⑥</td><td>④</td><td>⑥</td><td>⑥</td><td>LLM</td><td>随机</td><td>Pr.St.</td><td>温度</td></tr><tr><td rowspan="4">ZS(C)</td><td>C.</td><td>99.7</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.9</td><td>97.9</td><td>99.7</td><td>99.9</td><td>99.9</td></tr><tr><td>L.</td><td>61.8</td><td>66.6</td><td>62.3</td><td>67.0</td><td>62.2</td><td>66.2</td><td>67.7</td><td>66.2</td><td>69.0</td><td>67.3</td><td>66.5</td></tr><tr><td>S.</td><td>70.1</td><td>78.1</td><td>75.5</td><td>78.0</td><td>74.3</td><td>78.5</td><td>78.0</td><td>73.2</td><td>76.9</td><td>73.5</td><td>76.9</td></tr><tr><td>V.</td><td>73.9</td><td>82.8</td><td>80.6</td><td>83.2</td><td>79.3</td><td>82.7</td><td>84.9</td><td>72.6</td><td>81.8</td><td>81.8</td><td>81.9</td></tr><tr><td rowspan="4">ZS(CP)</td><td>C.</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.9</td><td>99.9</td></tr><tr><td>L.</td><td>69.9</td><td>69.3</td><td>67.9</td><td>69.6</td><td>68.4</td><td>70.0</td><td>70.4</td><td>69.3</td><td>70.4</td><td>71.2</td><td>69.7</td></tr><tr><td>S.</td><td>73.3</td><td>77.6</td><td>76.4</td><td>76.7</td><td>75.9</td><td>78.8</td><td>77.2</td><td>76.2</td><td>75.2</td><td>75.1</td><td>78.0</td></tr><tr><td>V.</td><td>84.8</td><td>85.3</td><td>84.0</td><td>85.3</td><td>84.2</td><td>85.1</td><td>86.0</td><td>77.0</td><td>84.2</td><td>86.0</td><td>84.6</td></tr><tr><td rowspan="4">ZS(PC)</td><td>C.</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td></tr><tr><td>L.</td><td>70.2</td><td>70.0</td><td>68.0</td><td>70.1</td><td>68.5</td><td>70.0</td><td>70.7</td><td>67.5</td><td>70.6</td><td>71.8</td><td>70.0</td></tr><tr><td>S.</td><td>73.6</td><td>76.6</td><td>75.6</td><td>76.0</td><td>74.8</td><td>77.8</td><td>76.9</td><td>76.9</td><td>75.1</td><td>74.9</td><td>78.2</td></tr><tr><td>V.</td><td>86.1</td><td>85.7</td><td>84.7</td><td>85.7</td><td>84.5</td><td>85.5</td><td>86.2</td><td>78.2</td><td>84.6</td><td>86.8</td><td>84.8</td></tr><tr><td rowspan="4">ZS(NC)</td><td>C.</td><td>87.0</td><td>99.8</td><td>99.6</td><td>99.8</td><td>99.4</td><td>99.7</td><td>99.9</td><td>95.3</td><td>98.6</td><td>99.6</td><td>99.8</td></tr><tr><td>L.</td><td>55.9</td><td>65.2</td><td>61.3</td><td>65.6</td><td>60.5</td><td>65.4</td><td>65.9</td><td>63.0</td><td>66.7</td><td>64.0</td><td>64.7</td></tr><tr><td>S.</td><td>61.4</td><td>75.6</td><td>70.3</td><td>75.2</td><td>68.3</td><td>74.9</td><td>75.3</td><td>68.9</td><td>73.3</td><td>69.8</td><td>73.0</td></tr><tr><td>V.</td><td>68.9</td><td>80.1</td><td>75.2</td><td>80.4</td><td>73.8</td><td>79.4</td><td>82.9</td><td>69.3</td><td>79.6</td><td>77.2</td><td>78.6</td></tr></tbody></table>


## C Experiments on Other CLIP Model Scales
## C 其他 CLIP 模型尺度的实验


### C.1 Experiments on ViT-L/14
### C.1 ViT-L/14 的实验


We refined the output dimension to align with the input dimension of 768 . The chosen latent dimensions were 448 and 640 for PACS and VLCS, respectively, and 768 for both OfficeHome and DomainNet. The inference weighting $\alpha$ was set to 0.1 for PACS, 0.03 for VLCS, 0.14 for OfficeHome, and 0.2 for Domain-Net. All other training configurations remained consistent with the ViT-B/16 experiments across each dataset. The training configuration for Im.Aug was set the same as CLAP for each dataset,with the inference weighting $\alpha$ being 0.1 for PACS and 0.03 for the other three datasets.
我们将输出维度细化以与输入维度 768 对齐。为 PACS 与 VLCS 分别选择的潜在维度为 448 和 640，而 OfficeHome 与 DomainNet 两者则为 768。推理权重 $\alpha$ 在 PACS 设置为 0.1，在 VLCS 为 0.03，在 OfficeHome 为 0.14，在 Domain-Net 为 0.2。其他训练配置在各数据集与 ViT-B/16 实验保持一致。Im.Aug 的训练配置与每个数据集的 CLAP 相同，推理权重 $\alpha$ 在 PACS 为 0.1，其他三个数据集为 0.03。


Table 8 showcases the zero-shot results for the ViT-L/14 model using four distinct prompts, following the protocol established for the ViT-B/16 experiments. These results demonstrate that CLAP is more efficient than Im.Aug in enhancing zero-shot performance. Moreover, Tab. 9 illustrates that CLAP significantly reduces variations in zero-shot performance across different prompts, thereby confirming CLAP's performance improvements over CLIP across a range of model sizes. Detailed domain-level results are presented in Tab. 10, offering an in-depth analysis.
表 8 展示了在 ViT-L/14 模型下使用四个不同提示的零-shot 结果，遵循 ViT-B/16 实验建立的协议。这些结果表明 CLAP 在提升零-shot 表现方面比 Im.Aug 更高效。此外，表 9 显示 CLAP 显著降低不同提示之间的零-shot 表现差异，从而在多种模型规模上证实 CLAP 相对于 CLIP 的性能提升。更详尽的域级结果见表 10，提供深入分析。


### C.2 Experiments on ResNet50x16
### C.2 ResNet50x16 的实验


To validate our approach on different model structures, we repeated zero-shot experiments on the ResNet50x16 model pre-trained with CLIP. Since the output dimension of CLIP is the same as ViT-B/16, we used the same training configuration as ViT-B/16 for training Im.Aug and CLAP. For inference, we refined the weighting coefficient $\alpha$ to 0.1,1,0.03,and 0.1 for Im.Aug,and 0.03,0.2,0.06, and 0.1 for CLAP, for PACS, VLCS, OfficeHome, and DomainNet respectively.
为在不同模型结构上验证我们的方法，我们对使用 CLIP 预训练的 ResNet50x16 模型重复了零-shot 实验。由于 CLIP 的输出维度与 ViT-B/16 相同，因此训练 Im.Aug 和 CLAP 时采用与 ViT-B/16 相同的训练配置。对推理，将权重系数 $\alpha$ 调整为 Im.Aug 的 0.1、1、0.03、0.1，分别对应 PACS、VLCS、OfficeHome、DomainNet；而对 CLAP 调整为 0.03、0.2、0.06、0.1。


Table 7: Ablative study of hyperparameters on VLCS dataset using ViT-B/16 model.
表 7：基于 ViT-B/16 模型在 VLCS 数据集上的超参数消融研究。


<table><tr><td rowspan="3">Hyper- <br> parameters</td><td rowspan="3">Value</td><td colspan="12">Avg. top-1 acc. (%) (↑) using ViT-B/16 on VLCS dataset</td></tr><tr><td colspan="4">ZS (C)</td><td colspan="4">ZS (CP)</td><td colspan="4">ZS (PC)</td></tr><tr><td>C.</td><td>L.</td><td>S.</td><td>V.</td><td>C.</td><td>L.</td><td>S.</td><td>V.</td><td>C.</td><td>L.</td><td>S.</td><td>V.</td></tr><tr><td rowspan="5">$\tau$</td><td>0.1</td><td>99.9</td><td>67.6</td><td>77.5</td><td>84.2</td><td>99.9</td><td>70.9</td><td>74.9</td><td>85.9</td><td>99.9</td><td>71.2</td><td>74.6</td><td>86.3</td></tr><tr><td>0.3</td><td>99.9</td><td>66.3</td><td>77.2</td><td>82.4</td><td>99.9</td><td>69.9</td><td>76.7</td><td>85.2</td><td>99.9</td><td>69.9</td><td>76.4</td><td>85.4</td></tr><tr><td>0.5</td><td>99.9</td><td>67.7</td><td>78.0</td><td>84.9</td><td>99.9</td><td>70.4</td><td>77.2</td><td>86.0</td><td>99.9</td><td>70.7</td><td>76.9</td><td>86.2</td></tr><tr><td>0.7</td><td>99.9</td><td>65.9</td><td>77.7</td><td>83.1</td><td>99.9</td><td>68.9</td><td>77.9</td><td>84.9</td><td>99.9</td><td>69.6</td><td>77.7</td><td>85.0</td></tr><tr><td>0.9</td><td>99.9</td><td>66.0</td><td>77.6</td><td>83.3</td><td>99.9</td><td>69.0</td><td>77.9</td><td>85.0</td><td>99.9</td><td>69.7</td><td>77.5</td><td>85.0</td></tr><tr><td rowspan="7">Lantent dim.</td><td>128.0</td><td>99.9</td><td>66.0</td><td>77.6</td><td>82.6</td><td>99.9</td><td>70.0</td><td>77.4</td><td>85.4</td><td>99.9</td><td>70.1</td><td>77.1</td><td>85.7</td></tr><tr><td>192.0</td><td>99.9</td><td>64.9</td><td>77.9</td><td>83.0</td><td>99.9</td><td>68.9</td><td>78.0</td><td>85.6</td><td>99.9</td><td>69.0</td><td>77.8</td><td>86.0</td></tr><tr><td>256.0</td><td>99.9</td><td>63.8</td><td>77.6</td><td>82.7</td><td>99.9</td><td>67.6</td><td>78.7</td><td>84.8</td><td>99.9</td><td>67.8</td><td>78.6</td><td>85.2</td></tr><tr><td>320.0</td><td>99.9</td><td>66.0</td><td>77.8</td><td>82.9</td><td>99.9</td><td>69.2</td><td>78.1</td><td>85.3</td><td>99.9</td><td>69.7</td><td>77.7</td><td>85.5</td></tr><tr><td>384.0</td><td>99.9</td><td>65.8</td><td>76.9</td><td>82.8</td><td>99.9</td><td>69.4</td><td>77.5</td><td>85.3</td><td>99.9</td><td>69.6</td><td>77.0</td><td>85.5</td></tr><tr><td>448.0</td><td>99.9</td><td>65.8</td><td>77.4</td><td>82.1</td><td>99.9</td><td>69.7</td><td>77.6</td><td>84.9</td><td>99.9</td><td>69.9</td><td>77.1</td><td>85.6</td></tr><tr><td>512.0</td><td>99.9</td><td>67.7</td><td>78.0</td><td>84.9</td><td>99.9</td><td>70.4</td><td>77.2</td><td>86.0</td><td>99.9</td><td>70.7</td><td>76.9</td><td>86.2</td></tr><tr><td rowspan="7">$\alpha$</td><td>${10}^{-{1.5}}$</td><td>99.9</td><td>66.5</td><td>77.9</td><td>83.1</td><td>99.9</td><td>70.4</td><td>77.1</td><td>86.0</td><td>99.9</td><td>70.3</td><td>76.6</td><td>86.1</td></tr><tr><td>${10}^{-1}$</td><td>99.9</td><td>69.5</td><td>77.5</td><td>85.7</td><td>99.9</td><td>70.4</td><td>77.1</td><td>86.2</td><td>99.9</td><td>70.9</td><td>76.5</td><td>86.1</td></tr><tr><td>${10}^{-{0.5}}$</td><td>99.9</td><td>70.6</td><td>75.2</td><td>85.5</td><td>99.9</td><td>70.7</td><td>75.7</td><td>85.9</td><td>99.9</td><td>71.0</td><td>75.1</td><td>85.7</td></tr><tr><td>${10}^{0}$</td><td>99.8</td><td>71.5</td><td>73.5</td><td>83.5</td><td>99.9</td><td>71.7</td><td>74.4</td><td>85.8</td><td>99.8</td><td>72.3</td><td>73.5</td><td>85.5</td></tr><tr><td>${10}^{0.5}$</td><td>99.8</td><td>72.0</td><td>73.1</td><td>85.5</td><td>99.8</td><td>72.2</td><td>73.7</td><td>85.7</td><td>99.8</td><td>72.5</td><td>72.9</td><td>85.6</td></tr><tr><td>${10}^{1}$</td><td>99.8</td><td>72.1</td><td>72.8</td><td>85.4</td><td>99.8</td><td>72.3</td><td>73.4</td><td>85.7</td><td>99.8</td><td>72.5</td><td>72.9</td><td>85.5</td></tr><tr><td>${10}^{1.5}$</td><td>99.8</td><td>72.1</td><td>72.8</td><td>85.4</td><td>99.8</td><td>72.2</td><td>73.3</td><td>85.7</td><td>99.8</td><td>72.6</td><td>72.7</td><td>85.5</td></tr></table>
<table><tbody><tr><td rowspan="3">超参数</td><td rowspan="3">数值</td><td colspan="12">Avg. top-1 acc. (%) (↑) 使用 ViT-B/16 在 VLCS 数据集上的</td></tr><tr><td colspan="4">ZS (C)</td><td colspan="4">ZS (CP)</td><td colspan="4">ZS (PC)</td></tr><tr><td>C.</td><td>L.</td><td>S.</td><td>V.</td><td>C.</td><td>L.</td><td>S.</td><td>V.</td><td>C.</td><td>L.</td><td>S.</td><td>V.</td></tr><tr><td rowspan="5">$\tau$</td><td>0.1</td><td>99.9</td><td>67.6</td><td>77.5</td><td>84.2</td><td>99.9</td><td>70.9</td><td>74.9</td><td>85.9</td><td>99.9</td><td>71.2</td><td>74.6</td><td>86.3</td></tr><tr><td>0.3</td><td>99.9</td><td>66.3</td><td>77.2</td><td>82.4</td><td>99.9</td><td>69.9</td><td>76.7</td><td>85.2</td><td>99.9</td><td>69.9</td><td>76.4</td><td>85.4</td></tr><tr><td>0.5</td><td>99.9</td><td>67.7</td><td>78.0</td><td>84.9</td><td>99.9</td><td>70.4</td><td>77.2</td><td>86.0</td><td>99.9</td><td>70.7</td><td>76.9</td><td>86.2</td></tr><tr><td>0.7</td><td>99.9</td><td>65.9</td><td>77.7</td><td>83.1</td><td>99.9</td><td>68.9</td><td>77.9</td><td>84.9</td><td>99.9</td><td>69.6</td><td>77.7</td><td>85.0</td></tr><tr><td>0.9</td><td>99.9</td><td>66.0</td><td>77.6</td><td>83.3</td><td>99.9</td><td>69.0</td><td>77.9</td><td>85.0</td><td>99.9</td><td>69.7</td><td>77.5</td><td>85.0</td></tr><tr><td rowspan="7">潜在维度</td><td>128.0</td><td>99.9</td><td>66.0</td><td>77.6</td><td>82.6</td><td>99.9</td><td>70.0</td><td>77.4</td><td>85.4</td><td>99.9</td><td>70.1</td><td>77.1</td><td>85.7</td></tr><tr><td>192.0</td><td>99.9</td><td>64.9</td><td>77.9</td><td>83.0</td><td>99.9</td><td>68.9</td><td>78.0</td><td>85.6</td><td>99.9</td><td>69.0</td><td>77.8</td><td>86.0</td></tr><tr><td>256.0</td><td>99.9</td><td>63.8</td><td>77.6</td><td>82.7</td><td>99.9</td><td>67.6</td><td>78.7</td><td>84.8</td><td>99.9</td><td>67.8</td><td>78.6</td><td>85.2</td></tr><tr><td>320.0</td><td>99.9</td><td>66.0</td><td>77.8</td><td>82.9</td><td>99.9</td><td>69.2</td><td>78.1</td><td>85.3</td><td>99.9</td><td>69.7</td><td>77.7</td><td>85.5</td></tr><tr><td>384.0</td><td>99.9</td><td>65.8</td><td>76.9</td><td>82.8</td><td>99.9</td><td>69.4</td><td>77.5</td><td>85.3</td><td>99.9</td><td>69.6</td><td>77.0</td><td>85.5</td></tr><tr><td>448.0</td><td>99.9</td><td>65.8</td><td>77.4</td><td>82.1</td><td>99.9</td><td>69.7</td><td>77.6</td><td>84.9</td><td>99.9</td><td>69.9</td><td>77.1</td><td>85.6</td></tr><tr><td>512.0</td><td>99.9</td><td>67.7</td><td>78.0</td><td>84.9</td><td>99.9</td><td>70.4</td><td>77.2</td><td>86.0</td><td>99.9</td><td>70.7</td><td>76.9</td><td>86.2</td></tr><tr><td rowspan="7">$\alpha$</td><td>${10}^{-{1.5}}$</td><td>99.9</td><td>66.5</td><td>77.9</td><td>83.1</td><td>99.9</td><td>70.4</td><td>77.1</td><td>86.0</td><td>99.9</td><td>70.3</td><td>76.6</td><td>86.1</td></tr><tr><td>${10}^{-1}$</td><td>99.9</td><td>69.5</td><td>77.5</td><td>85.7</td><td>99.9</td><td>70.4</td><td>77.1</td><td>86.2</td><td>99.9</td><td>70.9</td><td>76.5</td><td>86.1</td></tr><tr><td>${10}^{-{0.5}}$</td><td>99.9</td><td>70.6</td><td>75.2</td><td>85.5</td><td>99.9</td><td>70.7</td><td>75.7</td><td>85.9</td><td>99.9</td><td>71.0</td><td>75.1</td><td>85.7</td></tr><tr><td>${10}^{0}$</td><td>99.8</td><td>71.5</td><td>73.5</td><td>83.5</td><td>99.9</td><td>71.7</td><td>74.4</td><td>85.8</td><td>99.8</td><td>72.3</td><td>73.5</td><td>85.5</td></tr><tr><td>${10}^{0.5}$</td><td>99.8</td><td>72.0</td><td>73.1</td><td>85.5</td><td>99.8</td><td>72.2</td><td>73.7</td><td>85.7</td><td>99.8</td><td>72.5</td><td>72.9</td><td>85.6</td></tr><tr><td>${10}^{1}$</td><td>99.8</td><td>72.1</td><td>72.8</td><td>85.4</td><td>99.8</td><td>72.3</td><td>73.4</td><td>85.7</td><td>99.8</td><td>72.5</td><td>72.9</td><td>85.5</td></tr><tr><td>${10}^{1.5}$</td><td>99.8</td><td>72.1</td><td>72.8</td><td>85.4</td><td>99.8</td><td>72.2</td><td>73.3</td><td>85.7</td><td>99.8</td><td>72.6</td><td>72.7</td><td>85.5</td></tr></tbody></table>


Table 11 showcases the zero-shot results for ResNet50x16 model across different prompts, substantiating that CLAP is more effective than Im.Aug in refining CLIP features. Moreover, Tab. 12 illustrates that both Im.Aug and CLAP reduce variations in zero-shot performance across different prompts, with the improvement of CLAP being more significant. The results validate our approach across different model scales, including both ViT-based and CNN-based structures. Domain-level results are detailed in Tab. 13
Table 11 展示了 ResNet50x16 模型在不同提示下的零-shot 结果，证明 CLAP 比 Im.Aug 在 refine CLIP 特征方面更有效。此外，Table 12 表明 Im.Aug 与 CLAP 都能降低不同提示下的零-shot 性能波动，其中 CLAP 的改进更显著。结果验证了我们的方法在不同模型尺度上的适用性，包括基于 ViT 与基于 CNN 的结构。域级结果在 Table 13 详述


## D Discussion
## D 讨论


### D.1 Rationale behind CLAP's Foundation on CLIP
### D.1 CLAP 基于 CLIP 的理论基础


The primary challenge in cross-modal transferability lies in the significant domain gap between text and image data, which typically hinders the direct application of models trained in one modality to another. For a causal explaination, despite the consistency of the content variable that dictates the object label across modalities, the generative processes from latent variables to observations inherent to each modality differ markedly. The CLIP model, trained on a comprehensive dataset of image-text pairs with a symmetric InfoNCE loss, significantly ameliorates this issue. By aligning the features of text and images into similar patterns, it facilitates leveraging a network trained atop the CLIP encoder of one modality as a viable proxy for the other. Consequently, this allows for the direct application of the disentangled network trained in the text modality atop CLIP's image encoder to refine representations.
跨模态可迁移性的主要挑战在于文本与图像数据之间存在显著的域差异，通常阻碍在一个模态中训练的模型直接应用于另一模态。就一个因果解释而言，尽管控制对象标签跨模态时内容变量是一致的，但从潜在变量到观测值的生成过程在每个模态中有显著差异。基于对齐大量图文对、使用对称 InfoNCE 损失训练的 CLIP 模型，显著缓解了这一问题。通过将文本与图像的特征对齐到相似的模式，它有助于将建立在某一模态 CLIP 编码器之上的网络作为另一模态的可行代理。因此，这使得直接在 CLIP 的图像编码器之上应用在文本模态中训练的解耦网络，以提升表示的能力成为可能。


You
You


I have some class names, such as "dog", "car", "person", "chair". I want you to generate various style of these objects in different art-style pictures. Providing me with a list of different sentences that decribe a scene contaning the object, with different object attributes (such as object size, condition or age( if applicable), colors), different backgrounds the object may apear in; different image types (such as photo, painting, etc).
我有一些类别名，例如“狗”、“车”、“人”、“椅子”。我希望你用不同的艺术风格在不同的图片中生成这些对象的多种风格。给我一系列描述场景的不同句子，描述对象的不同属性（如对象大小、状态或年龄（若适用）、颜色）、对象可能出现的不同背景；不同的图片类型（如照片、绘画等）


ChatGPT
ChatGPT


Absolutely, I'll provide you with a list of descriptive sentences for each of the class names "dog", "car", "person", "chair," incorporating various object attributes, backgrounds, and image types. Each sentence will describe a unique scene:
当然，我会为“狗”、“车”、“人”、“椅子”这几个类别名提供描述性句子清单，结合各种对象属性、背景和图像类型。每个句子将描述一个独特的场景：


Fig. 2: The prompting method we use for generating text prompts with ChatGPT-3.5.
Fig. 2：我们用于生成带有 ChatGPT-3.5 的文本提示的提示方法。


Table 8: Zero-shot performance across four prompts ("C", "PC", "CP") and 1 noised prompts ("NC") with CLIP pre-trained ViT-L/14 model. CLAP demonstrates consistent gains in zero-shot performance across all datasets, validating its effectiveness.
Table 8：在四个提示（“C”、“PC”、“CP”）和一个带噪声的提示（“NC”）下，使用 CLIP 预训练的 ViT-L/14 模型的零-shot 性能。CLAP 在所有数据集上的零-shot 性能均显示出稳定提升，验证了其有效性。


<table><tr><td rowspan="2">Prompt Method</td><td rowspan="2"></td><td colspan="5">Zero-shot performance, avg. top-1 acc. (%) (↑)</td></tr><tr><td>PACS</td><td>VLCS</td><td>OfficeHome</td><td>DomainNet</td><td>Overall</td></tr><tr><td rowspan="3">ZS(C)</td><td>CLIP</td><td>97.6</td><td>77.1</td><td>85.9</td><td>63.2</td><td>80.9</td></tr><tr><td>Im.Aug</td><td>98.3</td><td>78.5</td><td>86.0</td><td>63.4</td><td>81.6</td></tr><tr><td>CLAP</td><td>98.5</td><td>80.7</td><td>87.5</td><td>64.2</td><td>82.7</td></tr><tr><td rowspan="3">ZS(CP)</td><td>CLIP</td><td>97.3</td><td>80.6</td><td>86.0</td><td>62.0</td><td>81.5</td></tr><tr><td>Im.Aug</td><td>98.3</td><td>81.1</td><td>86.1</td><td>62.4</td><td>82.0</td></tr><tr><td>CLAP</td><td>98.5</td><td>81.4</td><td>87.9</td><td>63.7</td><td>82.9</td></tr><tr><td rowspan="3">ZS(PC) Im.Aug</td><td>CLIP</td><td>98.4</td><td>81.7</td><td>86.5</td><td>63.5</td><td>82.5</td></tr><tr><td></td><td>98.6</td><td>81.9</td><td>86.6</td><td>63.7</td><td>82.7</td></tr><tr><td>CLAP</td><td>98.6</td><td>82.2</td><td>88.0</td><td>64.5</td><td>83.3</td></tr><tr><td rowspan="3">ZS(NC)</td><td>CLIP</td><td>91.0</td><td>65.5</td><td>77.1</td><td>55.4</td><td>72.3</td></tr><tr><td>Im.Aug</td><td>95.6</td><td>69.3</td><td>77.1</td><td>55.7</td><td>74.4</td></tr><tr><td>CLAP</td><td>98.5</td><td>73.1</td><td>81.3</td><td>58.3</td><td>77.8</td></tr></table>
<table><tbody><tr><td rowspan="2">提示方法</td><td rowspan="2"></td><td colspan="5">零-shot 表现，平均前1名准确率 (%)（↑）</td></tr><tr><td>PACS</td><td>VLCS</td><td>OfficeHome</td><td>DomainNet</td><td>整体</td></tr><tr><td rowspan="3">ZS(C)</td><td>CLIP</td><td>97.6</td><td>77.1</td><td>85.9</td><td>63.2</td><td>80.9</td></tr><tr><td>Im.Aug</td><td>98.3</td><td>78.5</td><td>86.0</td><td>63.4</td><td>81.6</td></tr><tr><td>CLAP</td><td>98.5</td><td>80.7</td><td>87.5</td><td>64.2</td><td>82.7</td></tr><tr><td rowspan="3">ZS(CP)</td><td>CLIP</td><td>97.3</td><td>80.6</td><td>86.0</td><td>62.0</td><td>81.5</td></tr><tr><td>Im.Aug</td><td>98.3</td><td>81.1</td><td>86.1</td><td>62.4</td><td>82.0</td></tr><tr><td>CLAP</td><td>98.5</td><td>81.4</td><td>87.9</td><td>63.7</td><td>82.9</td></tr><tr><td rowspan="3">ZS(PC) Im.Aug</td><td>CLIP</td><td>98.4</td><td>81.7</td><td>86.5</td><td>63.5</td><td>82.5</td></tr><tr><td></td><td>98.6</td><td>81.9</td><td>86.6</td><td>63.7</td><td>82.7</td></tr><tr><td>CLAP</td><td>98.6</td><td>82.2</td><td>88.0</td><td>64.5</td><td>83.3</td></tr><tr><td rowspan="3">ZS(NC)</td><td>CLIP</td><td>91.0</td><td>65.5</td><td>77.1</td><td>55.4</td><td>72.3</td></tr><tr><td>Im.Aug</td><td>95.6</td><td>69.3</td><td>77.1</td><td>55.7</td><td>74.4</td></tr><tr><td>CLAP</td><td>98.5</td><td>73.1</td><td>81.3</td><td>58.3</td><td>77.8</td></tr></tbody></table>


### D.2 Impact of Image and Text Augmentations
### D.2 图像与文本增强的影响


Identifying pure content factors poses a significant challenge. This difficulty primarily arises from the need for finding effective augmentations of observational data to alter style factors significantly while preserving content integrity.
识别纯内容因子是一个重大挑战。这个困难主要来自于需找到有效的观测数据增强，以显著改变风格因子，同时保持内容完整性。


Through the cross-modal alignment provided by CLIP, we discovered that disentangling in one modality can seamlessly improve representations in both modalities. The impact of image augmentations has been well-explored and found effective at preserving content, but traditional methods do not impose sufficient changes to remove all style information. Our exploration of text augmentations reveals that the logical structure of text and the relative ease of implementing style changes can have a significant impact on achieving disentanglement. However, more efficient methods are worthy of exploration.
通过 CLIP 提供的跨模态对齐，我们发现单一模态的解耦可以无缝地提升两种模态的表征。图像增强的影响已被广泛探究且在保持内容方面被认为有效，但传统方法不足以对所有风格信息进行充分的改动。我们对文本增强的探索表明，文本的逻辑结构以及实现风格变更的相对容易性可能对实现解耦产生显著影响。然而，更高效的方法值得进一步探索。


Table 9: CLAP reduces the variance in zero-shot performance across different prompts with CLIP pre-trained ViT-L/14 model.
表9：CLAP 通过与 CLIP 预训练 ViT-L/14 模型配合，在不同提示下降低零样本性能的方差。


<table><tr><td colspan="2" rowspan="2">Metric Method</td><td colspan="5">Zero-shot variance, avg. top-1 acc. (%) (↓)</td></tr><tr><td>PACS</td><td>VLCS</td><td></td><td></td><td>Overall</td></tr><tr><td rowspan="3">$R$</td><td>CLIP</td><td>1.0</td><td>4.6</td><td>0.6</td><td>1.5</td><td>1.9</td></tr><tr><td>Im.Aug</td><td>0.3</td><td>3.4</td><td>0.6</td><td>1.3</td><td>1.4</td></tr><tr><td>CLAP</td><td>0.1</td><td>1.5</td><td>0.4</td><td>0.7</td><td>0.7</td></tr><tr><td rowspan="3">$\delta$</td><td>CLIP</td><td>0.4</td><td>2.0</td><td>0.3</td><td>0.6</td><td>0.8</td></tr><tr><td>Im.Aug</td><td>0.1</td><td>1.5</td><td>0.3</td><td>0.5</td><td>0.6</td></tr><tr><td>CLAP</td><td>0.0</td><td>0.6</td><td>0.2</td><td>0.3</td><td>0.3</td></tr><tr><td rowspan="3">${\Delta }_{\left( NC\right) }$</td><td>CLIP</td><td>6.6</td><td>11.5</td><td>8.8</td><td>7.8</td><td>8.7</td></tr><tr><td>Im.Aug</td><td>2.7</td><td>9.2</td><td>8.9</td><td>7.7</td><td>7.1</td></tr><tr><td>CLAP</td><td>0.1</td><td>7.7</td><td>6.3</td><td>5.9</td><td>5.0</td></tr></table>
<table><tbody><tr><td colspan="2" rowspan="2">度量方法</td><td colspan="5">零-shot 方差，平均前 1 名准确率 (%) (↓)</td></tr><tr><td>PACS</td><td>VLCS</td><td></td><td></td><td>总体</td></tr><tr><td rowspan="3">$R$</td><td>CLIP</td><td>1.0</td><td>4.6</td><td>0.6</td><td>1.5</td><td>1.9</td></tr><tr><td>Im.Aug</td><td>0.3</td><td>3.4</td><td>0.6</td><td>1.3</td><td>1.4</td></tr><tr><td>CLAP</td><td>0.1</td><td>1.5</td><td>0.4</td><td>0.7</td><td>0.7</td></tr><tr><td rowspan="3">$\delta$</td><td>CLIP</td><td>0.4</td><td>2.0</td><td>0.3</td><td>0.6</td><td>0.8</td></tr><tr><td>Im.Aug</td><td>0.1</td><td>1.5</td><td>0.3</td><td>0.5</td><td>0.6</td></tr><tr><td>CLAP</td><td>0.0</td><td>0.6</td><td>0.2</td><td>0.3</td><td>0.3</td></tr><tr><td rowspan="3">${\Delta }_{\left( NC\right) }$</td><td>CLIP</td><td>6.6</td><td>11.5</td><td>8.8</td><td>7.8</td><td>8.7</td></tr><tr><td>Im.Aug</td><td>2.7</td><td>9.2</td><td>8.9</td><td>7.7</td><td>7.1</td></tr><tr><td>CLAP</td><td>0.1</td><td>7.7</td><td>6.3</td><td>5.9</td><td>5.0</td></tr></tbody></table>


Table 10: Domain-level zero-shot results of the ViT-L/14 model on the test datasets.
表 10：ViT-L/14 模型在测试数据集上的领域级零样本结果。


<table><tr><td rowspan="3">Datasets</td><td rowspan="3">Domains</td><td colspan="12">Domain-level avg. top-1 acc. (%) of zero-shot performance using ViT-L/14 (↑)</td></tr><tr><td colspan="3">ZS(C)</td><td colspan="3">ZS(CP)</td><td colspan="3">ZS(PC)</td><td colspan="3">ZS(NC)</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>97.2</td><td>98.0</td><td>98.8</td><td>96.8</td><td>98.0</td><td>98.5</td><td>98.7</td><td>98.8</td><td>98.9</td><td>85.6</td><td>91.6</td><td>98.6</td></tr><tr><td>C.</td><td>99.5</td><td>99.6</td><td>99.8</td><td>98.3</td><td>99.6</td><td>99.7</td><td>99.5</td><td>99.6</td><td>99.7</td><td>95.9</td><td>98.1</td><td>99.6</td></tr><tr><td>P.</td><td>99.9</td><td>100.0</td><td>100.0</td><td>99.4</td><td>99.5</td><td>100.0</td><td>99.9</td><td>100.0</td><td>99.9</td><td>91.1</td><td>97.5</td><td>99.9</td></tr><tr><td>S.</td><td>93.8</td><td>95.7</td><td>95.5</td><td>94.8</td><td>96.0</td><td>95.7</td><td>95.4</td><td>95.9</td><td>95.8</td><td>91.5</td><td>95.2</td><td>95.8</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>87.5</td><td>87.9</td><td>94.4</td></tr><tr><td>L.</td><td>57.4</td><td>60.1</td><td>64.3</td><td>71.3</td><td>71.6</td><td>72.6</td><td>71.7</td><td>72.0</td><td>72.6</td><td>53.8</td><td>59.7</td><td>60.7</td></tr><tr><td>S.</td><td>71.0</td><td>72.4</td><td>74.4</td><td>66.2</td><td>67.4</td><td>66.8</td><td>69.9</td><td>70.4</td><td>69.9</td><td>55.9</td><td>60.5</td><td>62.9</td></tr><tr><td>V.</td><td>80.0</td><td>81.6</td><td>84.3</td><td>85.2</td><td>85.7</td><td>86.2</td><td>85.1</td><td>85.3</td><td>86.4</td><td>65.0</td><td>69.3</td><td>74.3</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>86.2</td><td>86.3</td><td>87.7</td><td>85.7</td><td>86.2</td><td>88.1</td><td>87.0</td><td>87.0</td><td>87.8</td><td>78.1</td><td>77.1</td><td>80.7</td></tr><tr><td>C.</td><td>73.3</td><td>73.4</td><td>75.7</td><td>73.8</td><td>73.4</td><td>76.0</td><td>73.1</td><td>73.5</td><td>76.0</td><td>65.9</td><td>66.3</td><td>70.6</td></tr><tr><td>P.</td><td>92.0</td><td>91.8</td><td>93.6</td><td>92.3</td><td>92.4</td><td>94.3</td><td>92.9</td><td>92.8</td><td>94.1</td><td>80.7</td><td>81.0</td><td>86.8</td></tr><tr><td>R.</td><td>92.2</td><td>92.7</td><td>93.0</td><td>92.2</td><td>92.4</td><td>93.4</td><td>93.1</td><td>93.3</td><td>93.9</td><td>83.8</td><td>84.0</td><td>86.9</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>78.4</td><td>78.5</td><td>79.1</td><td>77.5</td><td>77.7</td><td>78.8</td><td>79.4</td><td>79.4</td><td>79.7</td><td>70.0</td><td>70.4</td><td>72.8</td></tr><tr><td>I.</td><td>52.9</td><td>53.0</td><td>54.6</td><td>50.4</td><td>50.7</td><td>53.6</td><td>51.7</td><td>52.0</td><td>53.9</td><td>45.3</td><td>45.2</td><td>48.8</td></tr><tr><td>P.</td><td>70.4</td><td>70.8</td><td>72.4</td><td>68.9</td><td>69.9</td><td>72.1</td><td>69.9</td><td>70.6</td><td>72.7</td><td>59.9</td><td>60.3</td><td>64.8</td></tr><tr><td>Q.</td><td>21.5</td><td>21.6</td><td>22.5</td><td>20.6</td><td>20.9</td><td>21.7</td><td>22.6</td><td>22.8</td><td>22.9</td><td>17.9</td><td>18.4</td><td>20.2</td></tr><tr><td>R.</td><td>85.8</td><td>85.9</td><td>85.9</td><td>85.3</td><td>85.5</td><td>85.7</td><td>86.3</td><td>86.4</td><td>86.2</td><td>77.5</td><td>77.5</td><td>78.7</td></tr><tr><td>S.</td><td>70.2</td><td>70.4</td><td>70.7</td><td>69.4</td><td>69.8</td><td>70.6</td><td>71.0</td><td>71.3</td><td>71.5</td><td>62.0</td><td>62.2</td><td>64.6</td></tr></table>
<table><tbody><tr><td rowspan="3">数据集</td><td rowspan="3">领域</td><td colspan="12">基于 ViT-L/14 的零-shot 性能在领域级别的前 1 精度平均值 (%)（↑）</td></tr><tr><td colspan="3">ZS(C)</td><td colspan="3">ZS(CP)</td><td colspan="3">ZS(PC)</td><td colspan="3">ZS(NC)</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>97.2</td><td>98.0</td><td>98.8</td><td>96.8</td><td>98.0</td><td>98.5</td><td>98.7</td><td>98.8</td><td>98.9</td><td>85.6</td><td>91.6</td><td>98.6</td></tr><tr><td>C.</td><td>99.5</td><td>99.6</td><td>99.8</td><td>98.3</td><td>99.6</td><td>99.7</td><td>99.5</td><td>99.6</td><td>99.7</td><td>95.9</td><td>98.1</td><td>99.6</td></tr><tr><td>P.</td><td>99.9</td><td>100.0</td><td>100.0</td><td>99.4</td><td>99.5</td><td>100.0</td><td>99.9</td><td>100.0</td><td>99.9</td><td>91.1</td><td>97.5</td><td>99.9</td></tr><tr><td>S.</td><td>93.8</td><td>95.7</td><td>95.5</td><td>94.8</td><td>96.0</td><td>95.7</td><td>95.4</td><td>95.9</td><td>95.8</td><td>91.5</td><td>95.2</td><td>95.8</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>87.5</td><td>87.9</td><td>94.4</td></tr><tr><td>L.</td><td>57.4</td><td>60.1</td><td>64.3</td><td>71.3</td><td>71.6</td><td>72.6</td><td>71.7</td><td>72.0</td><td>72.6</td><td>53.8</td><td>59.7</td><td>60.7</td></tr><tr><td>S.</td><td>71.0</td><td>72.4</td><td>74.4</td><td>66.2</td><td>67.4</td><td>66.8</td><td>69.9</td><td>70.4</td><td>69.9</td><td>55.9</td><td>60.5</td><td>62.9</td></tr><tr><td>V.</td><td>80.0</td><td>81.6</td><td>84.3</td><td>85.2</td><td>85.7</td><td>86.2</td><td>85.1</td><td>85.3</td><td>86.4</td><td>65.0</td><td>69.3</td><td>74.3</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>86.2</td><td>86.3</td><td>87.7</td><td>85.7</td><td>86.2</td><td>88.1</td><td>87.0</td><td>87.0</td><td>87.8</td><td>78.1</td><td>77.1</td><td>80.7</td></tr><tr><td>C.</td><td>73.3</td><td>73.4</td><td>75.7</td><td>73.8</td><td>73.4</td><td>76.0</td><td>73.1</td><td>73.5</td><td>76.0</td><td>65.9</td><td>66.3</td><td>70.6</td></tr><tr><td>P.</td><td>92.0</td><td>91.8</td><td>93.6</td><td>92.3</td><td>92.4</td><td>94.3</td><td>92.9</td><td>92.8</td><td>94.1</td><td>80.7</td><td>81.0</td><td>86.8</td></tr><tr><td>R.</td><td>92.2</td><td>92.7</td><td>93.0</td><td>92.2</td><td>92.4</td><td>93.4</td><td>93.1</td><td>93.3</td><td>93.9</td><td>83.8</td><td>84.0</td><td>86.9</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>78.4</td><td>78.5</td><td>79.1</td><td>77.5</td><td>77.7</td><td>78.8</td><td>79.4</td><td>79.4</td><td>79.7</td><td>70.0</td><td>70.4</td><td>72.8</td></tr><tr><td>I.</td><td>52.9</td><td>53.0</td><td>54.6</td><td>50.4</td><td>50.7</td><td>53.6</td><td>51.7</td><td>52.0</td><td>53.9</td><td>45.3</td><td>45.2</td><td>48.8</td></tr><tr><td>P.</td><td>70.4</td><td>70.8</td><td>72.4</td><td>68.9</td><td>69.9</td><td>72.1</td><td>69.9</td><td>70.6</td><td>72.7</td><td>59.9</td><td>60.3</td><td>64.8</td></tr><tr><td>Q.</td><td>21.5</td><td>21.6</td><td>22.5</td><td>20.6</td><td>20.9</td><td>21.7</td><td>22.6</td><td>22.8</td><td>22.9</td><td>17.9</td><td>18.4</td><td>20.2</td></tr><tr><td>R.</td><td>85.8</td><td>85.9</td><td>85.9</td><td>85.3</td><td>85.5</td><td>85.7</td><td>86.3</td><td>86.4</td><td>86.2</td><td>77.5</td><td>77.5</td><td>78.7</td></tr><tr><td>S.</td><td>70.2</td><td>70.4</td><td>70.7</td><td>69.4</td><td>69.8</td><td>70.6</td><td>71.0</td><td>71.3</td><td>71.5</td><td>62.0</td><td>62.2</td><td>64.6</td></tr></tbody></table>


A promising direction for future research is to explore efficient combinations of both modalities to enhance disentangled semantics. As each modality has its unique advantages-Text data recapitulates properties well since it is preprocessed by human intelligence, while image data is more precise in depicting the exact same objects or events due to its more detailed nature - the impact of combining augmentations of both modalities could be substantial.
未来研究的一个有前景的方向是探索两种模态的高效组合以增强解耦的语义。由于每种模态各自具备独特优势——文本数据因由人类智能预处理而更好地概括属性，而图像数据由于更详尽的特性在呈现相同对象或事件时更为准确——将两种模态的增强进行结合的影响可能是显著的。


Table 11: Zero-shot performance with CLIP pre-trained ResNet50x16 model. CLAP demonstrates consistent enhancement across all datasets, validating its effectiveness.
表11：使用 CLIP 预训练的 ResNet50x16 模型的零-shot 性能。CLAP 在所有数据集上均显示出持续提升，验证了其有效性。


<table><tr><td></td><td rowspan="2">Method</td><td colspan="5">Zero-shot performance, avg. top-1 acc. (%) (↑)</td></tr><tr><td></td><td>PACS</td><td>VLCS</td><td colspan="3">DomainNet Overall</td></tr><tr><td rowspan="3">ZS(C)</td><td>CLIP</td><td>96.1</td><td>70.4</td><td>80.4</td><td>57.1</td><td>76.0</td></tr><tr><td>Im.Aug</td><td>96.4</td><td>74.7</td><td>80.4</td><td>57.1</td><td>77.2</td></tr><tr><td>CLAP</td><td>97.0</td><td>79.9</td><td>81.6</td><td>58.0</td><td>79.1</td></tr><tr><td rowspan="3">ZS(CP)</td><td>CLIP</td><td>95.0</td><td>73.5</td><td>79.0</td><td>56.1</td><td>75.9</td></tr><tr><td>Im.Aug</td><td>95.7</td><td>75.8</td><td>79.3</td><td>56.5</td><td>76.8</td></tr><tr><td>CLAP</td><td>96.7</td><td>80.3</td><td>79.9</td><td>57.4</td><td>78.6</td></tr><tr><td rowspan="3"></td><td>CLIP</td><td>96.5</td><td>78.4</td><td>81.7</td><td>57.1</td><td>78.4</td></tr><tr><td>ZS(PC) Im.Aug</td><td>97.0</td><td>79.8</td><td>81.8</td><td>57.4</td><td>79.0</td></tr><tr><td>CLAP</td><td>96.8</td><td>80.1</td><td>82.5</td><td>58.2</td><td>79.4</td></tr><tr><td rowspan="3"></td><td>CLIP</td><td>86.4</td><td>61.2</td><td>69.3</td><td>48.2</td><td>66.3</td></tr><tr><td>ZS(NC) Im.Aug</td><td>88.3</td><td>71.3</td><td>69.5</td><td>48.7</td><td>69.4</td></tr><tr><td>CLAP</td><td>94.9</td><td>80.1</td><td>71.9</td><td>50.6</td><td>74.4</td></tr></table>
<table><tbody><tr><td></td><td rowspan="2">方法</td><td colspan="5">零-shot 性能，平均前1准确率 (%)（↑）</td></tr><tr><td></td><td>PACS</td><td>VLCS</td><td colspan="3">DomainNet 总体</td></tr><tr><td rowspan="3">ZS(C)</td><td>CLIP</td><td>96.1</td><td>70.4</td><td>80.4</td><td>57.1</td><td>76.0</td></tr><tr><td>Im.Aug</td><td>96.4</td><td>74.7</td><td>80.4</td><td>57.1</td><td>77.2</td></tr><tr><td>CLAP</td><td>97.0</td><td>79.9</td><td>81.6</td><td>58.0</td><td>79.1</td></tr><tr><td rowspan="3">ZS(CP)</td><td>CLIP</td><td>95.0</td><td>73.5</td><td>79.0</td><td>56.1</td><td>75.9</td></tr><tr><td>Im.Aug</td><td>95.7</td><td>75.8</td><td>79.3</td><td>56.5</td><td>76.8</td></tr><tr><td>CLAP</td><td>96.7</td><td>80.3</td><td>79.9</td><td>57.4</td><td>78.6</td></tr><tr><td rowspan="3"></td><td>CLIP</td><td>96.5</td><td>78.4</td><td>81.7</td><td>57.1</td><td>78.4</td></tr><tr><td>ZS(PC) Im.Aug</td><td>97.0</td><td>79.8</td><td>81.8</td><td>57.4</td><td>79.0</td></tr><tr><td>CLAP</td><td>96.8</td><td>80.1</td><td>82.5</td><td>58.2</td><td>79.4</td></tr><tr><td rowspan="3"></td><td>CLIP</td><td>86.4</td><td>61.2</td><td>69.3</td><td>48.2</td><td>66.3</td></tr><tr><td>ZS(NC) Im.Aug</td><td>88.3</td><td>71.3</td><td>69.5</td><td>48.7</td><td>69.4</td></tr><tr><td>CLAP</td><td>94.9</td><td>80.1</td><td>71.9</td><td>50.6</td><td>74.4</td></tr></tbody></table>


Table 12: CLAP consistently reduces variances in zero-shot performance across different prompts with CLIP pre-trained ResNet50x16 model, validating its effectiveness.
表 12：CLAP 在不同提示下的零-shot 性能方差始终降低，使用 CLIP 预训练的 ResNet50x16 模型，验证了其有效性。


<table><tr><td colspan="2" rowspan="2">Metric Method</td><td colspan="5">Zero-shot variance, avg. top-1 acc. (%) (↓)</td></tr><tr><td>PACS</td><td>VLCS</td><td>OfficeHome DomainNet Overall</td><td></td><td></td></tr><tr><td rowspan="3">$R$</td><td>CLIP</td><td>1.5</td><td>8.0</td><td>2.7</td><td>1.1</td><td>3.3</td></tr><tr><td>Im.Aug</td><td>1.3</td><td>5.1</td><td>2.5</td><td>0.9</td><td>2.4</td></tr><tr><td>CLAP</td><td>0.3</td><td>0.4</td><td>2.6</td><td>0.8</td><td>1.0</td></tr><tr><td rowspan="3">$\delta$</td><td>CLIP</td><td>0.6</td><td>3.3</td><td>1.1</td><td>0.5</td><td>1.4</td></tr><tr><td>Im.Aug</td><td>0.5</td><td>2.2</td><td>1.0</td><td>0.4</td><td>1.0</td></tr><tr><td>CLAP</td><td>0.1</td><td>0.2</td><td>1.1</td><td>0.3</td><td>0.4</td></tr><tr><td rowspan="3">${\Delta }_{\left( NC\right) }$</td><td>CLIP</td><td>9.7</td><td>9.3</td><td>11.1</td><td>8.9</td><td>9.7</td></tr><tr><td>Im.Aug</td><td>8.1</td><td>3.5</td><td>10.9</td><td>8.5</td><td>7.7</td></tr><tr><td>CLAP</td><td>2.1</td><td>-0.1</td><td>9.7</td><td>7.5</td><td>4.8</td></tr></table>
<table><tbody><tr><td colspan="2" rowspan="2">度量方法</td><td colspan="5">零-shot 方差，avg. top-1 准确率 (%) (↓)</td></tr><tr><td>PACS</td><td>VLCS</td><td>OfficeHome DomainNet 总体</td><td></td><td></td></tr><tr><td rowspan="3">$R$</td><td>CLIP</td><td>1.5</td><td>8.0</td><td>2.7</td><td>1.1</td><td>3.3</td></tr><tr><td>Im.Aug</td><td>1.3</td><td>5.1</td><td>2.5</td><td>0.9</td><td>2.4</td></tr><tr><td>CLAP</td><td>0.3</td><td>0.4</td><td>2.6</td><td>0.8</td><td>1.0</td></tr><tr><td rowspan="3">$\delta$</td><td>CLIP</td><td>0.6</td><td>3.3</td><td>1.1</td><td>0.5</td><td>1.4</td></tr><tr><td>Im.Aug</td><td>0.5</td><td>2.2</td><td>1.0</td><td>0.4</td><td>1.0</td></tr><tr><td>CLAP</td><td>0.1</td><td>0.2</td><td>1.1</td><td>0.3</td><td>0.4</td></tr><tr><td rowspan="3">${\Delta }_{\left( NC\right) }$</td><td>CLIP</td><td>9.7</td><td>9.3</td><td>11.1</td><td>8.9</td><td>9.7</td></tr><tr><td>Im.Aug</td><td>8.1</td><td>3.5</td><td>10.9</td><td>8.5</td><td>7.7</td></tr><tr><td>CLAP</td><td>2.1</td><td>-0.1</td><td>9.7</td><td>7.5</td><td>4.8</td></tr></tbody></table>


Table 13: Domain-level zero-shot results using RestNet50x16 on the test datasets.
表 13：在测试数据集上使用 RestNet50x16 的域级零样本结果。


<table><tr><td rowspan="3">Datasets</td><td rowspan="3">Domains</td><td colspan="12">Domain-level avg. top-1 acc. (%) of zero-shot performance using RN50x16 (↑)</td></tr><tr><td colspan="3">ZS(C)</td><td colspan="3">ZS(CP)</td><td colspan="3">ZS(PC)</td><td colspan="3">ZS(NC)</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>95.7</td><td>95.8</td><td>97.2</td><td>93.7</td><td>95.5</td><td>96.5</td><td>95.7</td><td>96.7</td><td>96.8</td><td>81.2</td><td>84.0</td><td>94.5</td></tr><tr><td>C.</td><td>98.3</td><td>98.2</td><td>99.0</td><td>98.1</td><td>98.8</td><td>99.0</td><td>98.6</td><td>98.7</td><td>98.9</td><td>92.3</td><td>93.2</td><td>98.0</td></tr><tr><td>P.</td><td>98.9</td><td>98.6</td><td>99.9</td><td>98.4</td><td>97.8</td><td>99.8</td><td>99.8</td><td>99.9</td><td>99.9</td><td>85.3</td><td>87.3</td><td>95.2</td></tr><tr><td>S.</td><td>91.5</td><td>93.1</td><td>91.9</td><td>89.7</td><td>90.8</td><td>91.5</td><td>91.8</td><td>92.9</td><td>91.6</td><td>86.9</td><td>88.6</td><td>92.1</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>96.8</td><td>97.1</td><td>99.3</td><td>99.7</td><td>99.4</td><td>99.3</td><td>99.7</td><td>99.6</td><td>99.4</td><td>75.6</td><td>89.3</td><td>99.4</td></tr><tr><td>L.</td><td>53.4</td><td>60.8</td><td>65.9</td><td>51.6</td><td>58.9</td><td>67.3</td><td>59.5</td><td>68.1</td><td>66.8</td><td>54.1</td><td>60.6</td><td>67.0</td></tr><tr><td>S.</td><td>63.2</td><td>70.9</td><td>69.5</td><td>68.0</td><td>72.9</td><td>69.5</td><td>72.9</td><td>73.7</td><td>69.0</td><td>52.0</td><td>66.7</td><td>69.5</td></tr><tr><td>V.</td><td>68.4</td><td>70.1</td><td>85.2</td><td>74.5</td><td>72.1</td><td>85.3</td><td>81.7</td><td>78.0</td><td>85.2</td><td>63.1</td><td>68.5</td><td>84.5</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>82.2</td><td>82.5</td><td>83.5</td><td>79.7</td><td>79.9</td><td>80.6</td><td>82.0</td><td>82.4</td><td>83.4</td><td>67.7</td><td>68.9</td><td>72.1</td></tr><tr><td>C.</td><td>63.0</td><td>62.9</td><td>64.7</td><td>61.7</td><td>62.2</td><td>62.8</td><td>65.4</td><td>65.3</td><td>66.1</td><td>54.6</td><td>55.0</td><td>56.8</td></tr><tr><td>P.</td><td>88.2</td><td>87.9</td><td>89.0</td><td>87.4</td><td>87.5</td><td>88.5</td><td>90.0</td><td>89.9</td><td>90.6</td><td>75.4</td><td>75.3</td><td>78.2</td></tr><tr><td>R.</td><td>88.1</td><td>88.2</td><td>89.1</td><td>87.3</td><td>87.5</td><td>87.6</td><td>89.2</td><td>89.5</td><td>89.7</td><td>79.5</td><td>78.9</td><td>80.3</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>69.0</td><td>68.9</td><td>69.6</td><td>68.6</td><td>68.6</td><td>69.4</td><td>69.9</td><td>70.0</td><td>70.4</td><td>59.5</td><td>60.1</td><td>61.4</td></tr><tr><td>I.</td><td>51.0</td><td>51.1</td><td>52.7</td><td>48.2</td><td>49.0</td><td>50.6</td><td>48.2</td><td>48.9</td><td>50.7</td><td>41.2</td><td>41.6</td><td>44.3</td></tr><tr><td>P.</td><td>65.2</td><td>65.6</td><td>66.5</td><td>63.7</td><td>64.4</td><td>65.6</td><td>65.4</td><td>65.9</td><td>67.0</td><td>53.5</td><td>54.3</td><td>56.8</td></tr><tr><td>Q.</td><td>11.8</td><td>11.9</td><td>12.7</td><td>12.3</td><td>12.6</td><td>13.1</td><td>11.8</td><td>12.2</td><td>12.7</td><td>9.3</td><td>9.7</td><td>11.0</td></tr><tr><td>R.</td><td>82.1</td><td>82.2</td><td>83.1</td><td>81.6</td><td>81.8</td><td>82.6</td><td>83.3</td><td>83.4</td><td>83.8</td><td>72.9</td><td>73.0</td><td>74.7</td></tr><tr><td>S.</td><td>63.2</td><td>63.1</td><td>63.6</td><td>62.0</td><td>62.4</td><td>63.4</td><td>63.9</td><td>63.8</td><td>64.6</td><td>53.1</td><td>53.4</td><td>55.3</td></tr></table>
<table><tbody><tr><td rowspan="3">数据集</td><td rowspan="3">域</td><td colspan="12">在零-shot 性能下以 RN50x16 为基准的域级前1准确率（%）的平均值（↑）</td></tr><tr><td colspan="3">ZS(C)</td><td colspan="3">ZS(CP)</td><td colspan="3">ZS(PC)</td><td colspan="3">ZS(NC)</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>95.7</td><td>95.8</td><td>97.2</td><td>93.7</td><td>95.5</td><td>96.5</td><td>95.7</td><td>96.7</td><td>96.8</td><td>81.2</td><td>84.0</td><td>94.5</td></tr><tr><td>C.</td><td>98.3</td><td>98.2</td><td>99.0</td><td>98.1</td><td>98.8</td><td>99.0</td><td>98.6</td><td>98.7</td><td>98.9</td><td>92.3</td><td>93.2</td><td>98.0</td></tr><tr><td>P.</td><td>98.9</td><td>98.6</td><td>99.9</td><td>98.4</td><td>97.8</td><td>99.8</td><td>99.8</td><td>99.9</td><td>99.9</td><td>85.3</td><td>87.3</td><td>95.2</td></tr><tr><td>S.</td><td>91.5</td><td>93.1</td><td>91.9</td><td>89.7</td><td>90.8</td><td>91.5</td><td>91.8</td><td>92.9</td><td>91.6</td><td>86.9</td><td>88.6</td><td>92.1</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>96.8</td><td>97.1</td><td>99.3</td><td>99.7</td><td>99.4</td><td>99.3</td><td>99.7</td><td>99.6</td><td>99.4</td><td>75.6</td><td>89.3</td><td>99.4</td></tr><tr><td>L.</td><td>53.4</td><td>60.8</td><td>65.9</td><td>51.6</td><td>58.9</td><td>67.3</td><td>59.5</td><td>68.1</td><td>66.8</td><td>54.1</td><td>60.6</td><td>67.0</td></tr><tr><td>S.</td><td>63.2</td><td>70.9</td><td>69.5</td><td>68.0</td><td>72.9</td><td>69.5</td><td>72.9</td><td>73.7</td><td>69.0</td><td>52.0</td><td>66.7</td><td>69.5</td></tr><tr><td>V.</td><td>68.4</td><td>70.1</td><td>85.2</td><td>74.5</td><td>72.1</td><td>85.3</td><td>81.7</td><td>78.0</td><td>85.2</td><td>63.1</td><td>68.5</td><td>84.5</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>82.2</td><td>82.5</td><td>83.5</td><td>79.7</td><td>79.9</td><td>80.6</td><td>82.0</td><td>82.4</td><td>83.4</td><td>67.7</td><td>68.9</td><td>72.1</td></tr><tr><td>C.</td><td>63.0</td><td>62.9</td><td>64.7</td><td>61.7</td><td>62.2</td><td>62.8</td><td>65.4</td><td>65.3</td><td>66.1</td><td>54.6</td><td>55.0</td><td>56.8</td></tr><tr><td>P.</td><td>88.2</td><td>87.9</td><td>89.0</td><td>87.4</td><td>87.5</td><td>88.5</td><td>90.0</td><td>89.9</td><td>90.6</td><td>75.4</td><td>75.3</td><td>78.2</td></tr><tr><td>R.</td><td>88.1</td><td>88.2</td><td>89.1</td><td>87.3</td><td>87.5</td><td>87.6</td><td>89.2</td><td>89.5</td><td>89.7</td><td>79.5</td><td>78.9</td><td>80.3</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>69.0</td><td>68.9</td><td>69.6</td><td>68.6</td><td>68.6</td><td>69.4</td><td>69.9</td><td>70.0</td><td>70.4</td><td>59.5</td><td>60.1</td><td>61.4</td></tr><tr><td>I.</td><td>51.0</td><td>51.1</td><td>52.7</td><td>48.2</td><td>49.0</td><td>50.6</td><td>48.2</td><td>48.9</td><td>50.7</td><td>41.2</td><td>41.6</td><td>44.3</td></tr><tr><td>P.</td><td>65.2</td><td>65.6</td><td>66.5</td><td>63.7</td><td>64.4</td><td>65.6</td><td>65.4</td><td>65.9</td><td>67.0</td><td>53.5</td><td>54.3</td><td>56.8</td></tr><tr><td>Q.</td><td>11.8</td><td>11.9</td><td>12.7</td><td>12.3</td><td>12.6</td><td>13.1</td><td>11.8</td><td>12.2</td><td>12.7</td><td>9.3</td><td>9.7</td><td>11.0</td></tr><tr><td>R.</td><td>82.1</td><td>82.2</td><td>83.1</td><td>81.6</td><td>81.8</td><td>82.6</td><td>83.3</td><td>83.4</td><td>83.8</td><td>72.9</td><td>73.0</td><td>74.7</td></tr><tr><td>S.</td><td>63.2</td><td>63.1</td><td>63.6</td><td>62.0</td><td>62.4</td><td>63.4</td><td>63.9</td><td>63.8</td><td>64.6</td><td>53.1</td><td>53.4</td><td>55.3</td></tr></tbody></table>