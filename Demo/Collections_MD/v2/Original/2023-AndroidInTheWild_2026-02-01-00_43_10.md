# Android in the Wild: A Large-Scale Dataset for Android Device Control
# Android in the Wild: A Large-Scale Dataset for Android Device Control


Christopher Rawles*
克里斯托弗·罗尔斯*


Google Research
Google 研究


Alice Li*
Alice Li*


Google Research
Google Research


Daniel Rodriguez
丹尼尔·罗德里格斯


Google Research
谷歌研究


Oriana Riva
Oriana Riva


Google Research
谷歌研究


Timothy Lillicrap
蒂莫西·利利克拉普


Google DeepMind
Google DeepMind


## Abstract
## 摘要


There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface. We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10-13), and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance, and, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls to operate carousel widgets). We organize our dataset to encourage robustness analysis of device-control systems, i.e., how well a system performs in the presence of new task descriptions, new applications, or new platform versions. We develop two agents and report performance across the dataset. The dataset is available athttps://github.com/google-research/google-research/ tree/master/android_in_the_wild
越来越多的研究关注能够解释人类自然语言指令并通过直接控制其用户界面在数字设备上执行指令的设备控制系统。我们提出一个用于设备控制研究的数据集 Android in the Wild (AITW)，其规模比当前数据集大量级。该数据集包含对设备交互的人工演示，包括屏幕与操作，以及相应的自然语言指令。它包含71.5万条情节、覆盖3万条唯一指令、四个版本的 Android（v10-13）、以及八种设备类型（Pixel 2 XL 至 Pixel 6）且屏幕分辨率各异。它包含需要对语言和视觉上下文进行语义理解的多步任务。该数据集带来新挑战：必须从界面视觉外观推断可用的操作，并且操作空间不是基于简单的 UI 元素，而是由精确手势组成（如水平滑动以操作轮播部件）。我们组织数据集以促进对设备控制系统鲁棒性分析，即在遇到新任务描述、新应用程序或新平台版本时系统的表现。我们开发了两个代理并报告了跨数据集的性能。数据集可在 https://github.com/google-research/google-research/tree/master/android_in_the_wild 获取


## 1 Introduction
## 1 Introduction


Users complete tasks on mobile devices via a sequence of touches and gestures on the screen. Tasks can often be succinctly described using natural language commands, and, in many situations, it is valuable to be able to speak or type commands rather than interacting directly with the device. This has important implications for users who are unable to physically operate a device due to a physical (e.g., visual or motor disabilities) or situational (e.g., driving, cooking, etc.) impairment. It is therefore beneficial to build device-control systems that can interpret natural language instructions and execute them on a device without any manual intervention.
用户通过屏幕上的一连串触摸和手势在移动设备上完成任务。任务通常可以用自然语言命令简洁地描述，在许多情形下，能够说出或输入命令而非直接与设备交互，具有一定价值。这对因物理（如视觉或运动障碍）或情境（如驾驶、烹饪等）受限而无法实际操作设备的用户具有重要意义。因此，构建能够解读自然语言指令并在设备上执行而无需任何人工干预的设备控制系统是有益的。


Instead of using application-specific APIs, which are not generally available for any given application or function, these systems directly manipulate user interface (UI) elements on a screen, exactly as a human does [1, 28, 29, 35, 21]. Hence, to work correctly, it is essential for such systems to understand the screen, which usually means detecting position and inferring semantics of its UI elements. Device-control systems must also be able to map high-level commands to execution plans that can be carried out on the device. For example, understanding that the command "open my recent email with Jane" involves opening an email app, potentially tapping the search icon, typing "Jane", etc. Further, to be useful, they must be able to generalize across a variety of task instructions and UIs.
与其使用针对特定应用的 API，这些 API 对任何给定应用或功能通常并不可获得，不如直接操作屏幕上的用户界面（UI）元素，正如人类所做的那样 [1, 28, 29, 35, 21]。因此，正确工作对这类系统至关重要，即它们需要理解屏幕，通常意味着检测位置并推断其 UI 元素的语义。设备控制系统也必须能够将高级命令映射到可在设备上执行的执行计划。例如，理解命令“打开我和 Jane 的最近邮件”包括打开邮件应用、可能点击搜索图标、输入“Jane”等等。此外，为了有用，它们必须能够在各种任务指令和 UI 之间进行泛化。


---



*Equal contribution. Contact: crawles@google.com and lialice@google.com
*等贡献。联系方式：crawles@google.com 与 lialice@google.com


---



<img src="https://cdn.noedgeai.com/bo_d5v2u2f7aajc73870hc0_1.jpg?x=316&y=203&w=1172&h=709&r=0"/>



Figure 1: AITW data pipeline. Raters are given a randomly selected instruction. The raters execute the task by interacting with the device in a natural way. We capture precise gestures in addition to typing and the home and back button interactions (we plot swipes with the arrow pointing where the finger moves to). Hindsight relabeling of high-level episodes is used to generate single-step tasks.
图1：AITW 数据管线。评测者被给予一个随机选择的指令。评测者通过与设备自然交互来完成任务。我们除了记录打字、主页与返回按钮交互，还捕捉精确手势（并以箭头指向手指移动方向的方式绘制滑动）。对高层级情节进行事后重新标注，以生成单步任务。


The rapid development of general-purpose large foundation models (LLMs) [8, 6, 13] makes device-control systems more viable. Yet, there is a lack of datasets for training, fine-tuning, and evaluating these systems. Existing datasets [28, 9, 42, 37, 4] are limited in terms of number of human demonstrations and the diversity of task instructions, and they are platform specific (either Android or web). They also assume a tree-based representation of an application UI can be derived from platform-specific UI metadata (e.g., the View Hierarchy for Android and the DOM tree for the web). This assumption simplifies the problem, but limits the resulting systems to work in environments where high-quality UI metadata is available ${}^{2}$ Finally,some popular datasets (e.g.,MiniWoB++ dataset [29] and UIBert [4]) assume task instructions are specified as step-by-step commands referring to specific UI elements appearing on the screen ("Click the button in the dialog box labeled Cancel"), while users may use short commands that describe high-level goals (e.g., "turn on airplane mode") or pose questions (e.g., "Is it going to rain tomorrow?")
通用大规模基础模型（LLMs）的快速发展使设备控制系统更具可行性。然而，缺乏用于训练、微调和评估这些系统的数据集。现有数据集在示范次数和任务指令多样性方面有限，并且它们是特定平台的（要么是 Android，要么是网页）。它们还假设可以从平台特定的 UI 元数据推导出应用 UI 的基于树的表示（如 Android 的 View Hierarchy 和网页的 DOM 树）。这一假设简化了问题，但限制了所得到的系统只能在具有高质量 UI 元数据的环境中工作 ${}^{2}$ 最后，一些流行的数据集（如 MiniWoB++ 数据集 [29] 和 UIBert [4]）假设任务指令被指定为逐步命令，指向屏幕上出现的特定 UI 元素（如“点击对话框中标记为取消的按钮”），而用户可能使用描述高层目标的简短命令（如“打开飞行模式”）或提出问题（如“明天会下雨吗？”）


To drive research in this field, we release AITW (Figure 1), an Android device-control dataset which is orders of magnitude larger than existing datasets. It consists of ${715}\mathrm{k}$ episodes spanning ${30}\mathrm{k}$ unique task instructions collected across hundreds of Android apps and websites. Each episode consists of a goal instruction provided in natural language and a sequence of observation-action pairs describing the execution of the task. Observations consist of screenshots of the application UI. Gesture actions are represented as taps and drags at arbitrary <x,y> coordinates in the screen. Agents trained on this dataset can be evaluated using AndroidEnv [40], an open-source platform for developing and testing Android agents with the Android Emulator ${}^{3}$
为推动本领域研究，我们发布 AITW（图1），一个规模远超现有数据集的 Android 设备控制数据集。它包含 ${715}\mathrm{k}$ 条目，覆盖 ${30}\mathrm{k}$ 个独特任务指令，收集自数百个 Android 应用和网站。每条情节由自然语言提供的目标指令和描述任务执行过程的一系列观测-行动对组成。观测由应用 UI 的截图组成。手势操作以在屏幕任意 <x,y> 坐标处的轻点与拖动形式表示。对该数据集训练的智能体可以在 AndroidEnv [40] 上进行评估，这是一个用于在 Android 模拟器上开发和测试 Android 智能体的开源平台 ${}^{3}$


A key feature of our dataset is the diversity of task instructions and execution paths we collected, aimed to emulate real-world scenarios. We used multiple sources to collect high-level goal instructions: humans (both crowdsourced raters and us authors), LLM-generated prompts, and technical documentation (as in PixelHelp [28]). During crowdsourcing, raters were asked to both demonstrate full tasks and annotate sequences of screenshots (hindsight language relabeling [31, 32]), which allowed us to collect both multi-step and single-step task trajectories. We made the execution paths more varied by randomizing the application state, which forced the raters to demonstrate how to navigate to the relevant screens. Finally, we collected demonstrations on four versions of Android (v10-13) and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions.
我们数据集的一个关键特征是我们收集的任务指令和执行路径的多样性，旨在模拟真实世界场景。我们使用多个来源收集高级目标指令：人类（包括众包评测人员和作者本身）、LLM 生成的提示，以及技术文档（如 PixelHelp [28] 中所述）。在众包阶段，评测者被要求同时演示完整任务和标注一系列截图（事后语言再标注 [31, 32]），这使我们能够收集多步和单步任务轨迹。通过随机化应用状态，执行路径变得更加多样化，促使评测者演示如何导航到相关屏幕。最后，我们收集了四个版本的 Android（v10-13）和八种设备类型（Pixel 2 XL 到 Pixel 6），屏幕分辨率各异。


---



${}^{2}$ Since most users do not use UI metadata for interactions it tends to be poor quality or missing altogether. On Android, only applications registered as Accessibility tools can access the View Hierarchy [45]. On Windows, in many cases (e.g., Electron apps like Teams), UI trees are not easily accessible. Moreover, screen representations derived from UI metadata can be incomplete. On Android, WebViews and Canvas are not captured in the View Hierarchy, and many websites render directly to a Canvas, which does not contain any tree structure.
${}^{2}$ 由于大多数用户不使用 UI 元数据进行交互，其质量往往较差或完全缺失。在 Android 上，只有注册为无障碍工具的应用才能访问视图层次结构 [45]。在 Windows 上，很多情况下（如 Teams 等 Electron 应用）UI 树并不容易访问。此外，从 UI 元数据推导出的屏幕表示可能不完整。在 Android 上，WebViews 与 Canvas 不会被捕捉进视图层次结构，且许多网站直接渲染到 Canvas，而 Canvas 本身不含任何树结构。


https://developer.android.com/studio/run/emulator



---



<table><tr><td>Dataset</td><td>Platform</td><td>#Human demos</td><td>#Apps or websites</td><td>#Task steps</td><td>Observation format</td><td>Screen features</td><td>Real</td><td>High-level instruction</td></tr><tr><td>RicoSCA 28</td><td>Android (apps)</td><td>0</td><td>n/a</td><td>1.0</td><td>VH, screen</td><td>X</td><td>X</td><td>X</td></tr><tr><td>UIBert 4</td><td>Android (apps)</td><td>16,660</td><td>n/a</td><td>1.0</td><td>VH, screen</td><td>X</td><td>✓</td><td>X</td></tr><tr><td>MiniWoB++ 37. 29</td><td>synthetic web</td><td>17,971</td><td>100</td><td>2.3</td><td>DOM, screen</td><td>X</td><td>X</td><td>X</td></tr><tr><td>PixelHelp 28</td><td>Android (apps)</td><td>187</td><td>4</td><td>4.2</td><td>VH, screen</td><td>X</td><td>✓</td><td>✓</td></tr><tr><td>UGIF 42</td><td>Android (apps)</td><td>523</td><td>12</td><td>5.3</td><td>VH, screen</td><td>X</td><td>✓</td><td>✓</td></tr><tr><td>Mind2Web 14</td><td>web</td><td>2,350</td><td>137</td><td>7.3</td><td>DOM. screen</td><td>X</td><td>✓</td><td>✓</td></tr><tr><td>MoTIF 9</td><td>Android (apps)</td><td>4.707</td><td>125</td><td>4.5</td><td>VH, screen</td><td>X</td><td>✓</td><td>✓</td></tr><tr><td>AITW</td><td>Android (apps+web)</td><td>715,142</td><td>357+</td><td>6.5</td><td>screen</td><td>✓</td><td>✓</td><td>✓</td></tr></table>
<table><tbody><tr><td>数据集</td><td>平台</td><td>#人工演示</td><td>#应用或网站</td><td>#任务步骤</td><td>观测格式</td><td>屏幕特征</td><td>真实</td><td>高级别指令</td></tr><tr><td>RicoSCA 28</td><td>Android（应用）</td><td>0</td><td>n/a</td><td>1.0</td><td>VH、屏幕</td><td>X</td><td>X</td><td>X</td></tr><tr><td>UIBert 4</td><td>Android（应用）</td><td>16,660</td><td>n/a</td><td>1.0</td><td>VH、屏幕</td><td>X</td><td>✓</td><td>X</td></tr><tr><td>MiniWoB++ 37. 29</td><td>合成网络</td><td>17,971</td><td>100</td><td>2.3</td><td>DOM、屏幕</td><td>X</td><td>X</td><td>X</td></tr><tr><td>PixelHelp 28</td><td>Android（应用）</td><td>187</td><td>4</td><td>4.2</td><td>VH、屏幕</td><td>X</td><td>✓</td><td>✓</td></tr><tr><td>UGIF 42</td><td>Android（应用）</td><td>523</td><td>12</td><td>5.3</td><td>VH、屏幕</td><td>X</td><td>✓</td><td>✓</td></tr><tr><td>Mind2Web 14</td><td>网络</td><td>2,350</td><td>137</td><td>7.3</td><td>DOM. 屏幕</td><td>X</td><td>✓</td><td>✓</td></tr><tr><td>MoTIF 9</td><td>Android（应用）</td><td>4.707</td><td>125</td><td>4.5</td><td>VH、屏幕</td><td>X</td><td>✓</td><td>✓</td></tr><tr><td>AITW</td><td>Android（应用+网络）</td><td>715,142</td><td>357+</td><td>6.5</td><td>屏幕</td><td>✓</td><td>✓</td><td>✓</td></tr></tbody></table>


Table 1: Comparison of AITW to existing datasets. We consider platform, format of screen observations, presence of synthetic UIs or synthetic instructions ("Real"), and whether instructions are expressed as goals (high-level). For size comparison, we report the number of human demonstrations, apps/websites, and average task steps. AITW collects observations as screenshots and includes screen features (OCR and icon labels), which can be used to augment them.
表 1：AITW 与现有数据集的比较。我们考虑平台、屏幕观察的格式、是否存在合成 UI 或合成指令（“Real”），以及指令是否以目标（高层次）表达。为了进行规模比较，我们报告人类演示、应用/网站数量，以及平均任务步骤数。AITW 将观察以屏幕截图形式收集，并包含屏幕特征（OCR 与图标标签），可用于增强这些观察。


Device-control systems need to work on rapidly evolving software platforms, so an important metric for their success is generalizability to new tasks and applications. We organize our dataset to enable analysis of how trained systems perform in the presence of previously-seen tasks and applications, but also in the presence of new task descriptions, new Android versions, and new applications. Due to the lack of off-the-shelf pixel-based device-control models, to establish new state-of-the-art results on this dataset, we implement two agents: one trained from scratch using behavioural cloning (BC) and a second based on a pre-trained LLM.
设备控制系统需要在快速演化的软件平台上工作，因此其成功的重要指标是对新任务和新应用的泛化能力。我们将数据集组织为便于分析的方式，既能分析训练系统在此前见过的任务与应用中的表现，也能分析在新的任务描述、安卓版本和新应用中的表现。由于缺乏现成的基于像素的设备控制模型，为在该数据集上获得新的最先进结果，我们实现了两个代理：一个从零开始通过行为克隆（BC）进行训练，另一个基于预训练的大语言模型（LLM）。


We make the following contributions: $\left( i\right)$ we collect and release a dataset for device-control research, AITW, which is larger and more varied than existing datasets; (ii) we report performance for two models, which can serve as baselines for future work and (iii) we show how to use the dataset to conduct a generalization analysis.
我们作出以下贡献：$\left( i\right)$ 我们收集并公开用于设备控制研究的数据集 AITW，它比现有数据集更大且更具多样性；（ii）我们报告两种模型的性能，作为未来工作的基线；（iii）我们展示如何利用该数据集进行泛化分析。


## 2 Related work
## 2 相关工作


### 2.1 Device-control datasets
### 2.1 设备控制数据集


Table 1 provides a comparison of device-control datasets. Some datasets (top part of the table) target the problem of grounding referring expressions to UI elements on a screen. Every data instance in these datasets includes a screen, a low-level command (e.g., "click the menu button at the top left"), and a UI element corresponding to the command. In the RicoSCA dataset [28], commands are synthetically generated, while in MiniWoB++ [37, 29] sequences of low-level UI commands describe multi-step tasks (e.g., "find and click on the center of the circle, then press submit").
表 1 提供设备控制数据集的比较。一些数据集（表格上部）将问题定位于将指称表达式锚定到屏幕上的 UI 元素。所有数据实例都包含一个屏幕、一个低级命令（例如“点击左上角的菜单按钮”），以及与该命令对应的 UI 元素。在 RicoSCA 数据集 [28] 中，命令是通过合成生成的，而在 MiniWoB++ [37, 29] 中，低级 UI 命令序列描述多步任务（例如“找到并点击圆心，然后提交”）。


A second group of datasets contain instructions expressed as task goals. Each episode in these datasets is a sequence of action-observation pairs. The observations include screenshots and tree-based representations: View Hierarchy (VH) for Android and Document Object Model (DOM) for web-based applications. For instance, the PixelHelp dataset [28] comprises 187 high-level task goals and step-by-step instructions sourced from Pixel Phone Help pages. The UGIF dataset [42] contains similar queries but extends to multiple languages. The largest dataset to date is MoTIF [9], which consists of ${4.7}\mathrm{k}$ task demonstrations ${}^{4}$ with an average number of 6.5 steps and 276 unique task instructions. AITW is two orders of magnitude larger than MoTIF. In total, AITW consists of 715,142 episodes, spanning 30,378 unique prompts, with a small subset of the prompts inspired by the PixelHelp dataset. Observations are represented by screenshots along with pixel-based screen features.
第二组数据集包含以任务目标形式表达的指令。这些数据集中的每个段落是一组动作-观测对。观测包括屏幕截图和基于树的表示：Android 的视图层次结构（VH）和基于文档对象模型（DOM）的网页应用表示。例如，PixelHelp 数据集 [28] 由 187 个高级任务目标和来自 Pixel Phone Help 页面的一步步指令组成。UGIF 数据集 [42] 包含类似的查询，但扩展到多种语言。迄今为止最大的数据集是 MoTIF [9]，其包含 ${4.7}\mathrm{k}$ 条任务演示 ${}^{4}$，平均 6.5 步，276 条唯一的任务指令。AITW 规模比 MoTIF 大两个数量级。AITW 总共包含 715,142 个情节，覆盖 30,378 个唯一提示，且只有一小部分提示受 PixelHelp 数据集启发。观测由屏幕截图以及基于像素的屏幕特征组成。


---



${}^{4}$ This represents the number of "feasible" tasks. We do not consider tasks without a valid demonstration.
${}^{4}$ 这代表“可行”任务的数量。我们不考虑没有有效演示的任务。


---



<table><tr><td>Name</td><td>Task type</td><td>Description</td><td>Episodes</td><td>Screens</td><td>Prompts</td></tr><tr><td>GoogleApps</td><td rowspan="4">Multi-step</td><td>Tasks with Google apps (Gmail, Photos, Settings, etc.)</td><td>625,542</td><td>4,903,601</td><td>306</td></tr><tr><td>INSTALL</td><td>App installation and login tasks</td><td>25,760</td><td>250,058</td><td>688</td></tr><tr><td>WEBSHOPPING</td><td>Web shopping tasks</td><td>28,061</td><td>365,253</td><td>13,473</td></tr><tr><td>GENERAL</td><td>Misc web/app tasks and Q&A</td><td>9,476</td><td>85,413</td><td>545</td></tr><tr><td>SINGLE</td><td>Single-step</td><td>Mostly shopping tasks from WEBSHOPPING</td><td>26,303</td><td>85,668</td><td>15,366</td></tr><tr><td>Total</td><td></td><td></td><td>715,142</td><td>5,689,993</td><td>30,378</td></tr></table>
<table><tbody><tr><td>名称</td><td>任务类型</td><td>描述</td><td>剧集</td><td>屏幕</td><td>提示</td></tr><tr><td>GoogleApps</td><td rowspan="4">多步骤</td><td>含 Google 应用的任务（Gmail、照片、设置等）</td><td>625,542</td><td>4,903,601</td><td>306</td></tr><tr><td>安装</td><td>应用安装与登录任务</td><td>25,760</td><td>250,058</td><td>688</td></tr><tr><td>网购</td><td>网页购物任务</td><td>28,061</td><td>365,253</td><td>13,473</td></tr><tr><td>通用</td><td>杂项网页/应用任务与问答</td><td>9,476</td><td>85,413</td><td>545</td></tr><tr><td>单一</td><td>单步</td><td>主要来自 WEBSHOPPING 的购物任务</td><td>26,303</td><td>85,668</td><td>15,366</td></tr><tr><td>总计</td><td></td><td></td><td>715,142</td><td>5,689,993</td><td>30,378</td></tr></tbody></table>


Table 2: Composition of the AITW dataset.
表2：AITW 数据集的组成。


### 2.2 UI representation and automation models
### 2.2 UI 表示与自动化模型


Research on device control is mainly focused on two problems: understanding UIs and automating tasks. Existing work on the first problem utilizes self-supervised [21, 4, 5, 27] and supervised methods [10, 30, 11, 46] to train UI understanding models. In some cases, these models are fine-tuned for simple grounding tasks (e.g., referring expression component retrieval [4]), along with widget captioning or question answering tasks [27, 4].
设备控制研究主要聚焦于两大问题：理解 UI 和自动化任务。现有关于第一问题的工作利用自监督[21, 4, 5, 27] 与监督方法[10, 30, 11, 46]来训练 UI 理解模型。在某些情况下，这些模型会针对简单定位任务（如指称表达组件检索[4]）进行微调，并辅以部件描述或问答任务[27, 4]。


For task automation, Li et al. [28] decompose the problem in two stages: an action phrase-extraction stage to transform step-by-step instructions into actionable phrases, and a grounding stage that executes these instructions. Venkatesh et al. [42] utilize an LLM to parse the instruction before executing "macros" (e.g., tap(), toggle()) during the grounding phase. AppBuddy [38] train an RL agent to interact with on-screen UI elements to achieve tasks. LLMs can also understand and operate UI screens [43]. On the web front, previous studies have developed RL [20, 25, 29, 18], behavioral cloning [24], and LLM-based models [19, 26, 17]. These approaches utilize Document Object Model (DOM) inputs and often evaluate results on a simulated environment, MiniWob++ [37, 29]. Finally, LLMs have shown impressive results leveraging APIs, when they are available, for performing higher-level tasks [36, 33, 34].
在任务自动化方面，Li 等人[28] 将问题分为两个阶段：操作短语提取阶段将逐步指令转化为可执行短语，以及执行这些指令的定位阶段。Venkatesh 等人[42] 在定位阶段执行前使用大型语言模型（LLM）解析指令，然后再执行“宏指令”（如 tap(), toggle()）。AppBuddy [38] 训练 RL 代理与屏幕上的 UI 元素交互以完成任务。LLMs 也能理解并操作 UI 屏幕[43]。在网页端，先前研究发展了 RL[20, 25, 29, 18]、行为克隆[24] 与基于 LLM 的模型[19, 26, 17]。这些方法利用 DOM 输入，通常在模拟环境 MiniWob++[37, 29] 上评估结果。最后，当 APIs 可用时，LLMs 在执行更高层任务时展现出令人印象深刻的结果[36, 33, 34]。


## 3 Android in the Wild (AITW)
## 3 Android in the Wild (AITW)


Table 2 shows the composition of AITW in terms of category and type of tasks. Overall, AITW consists of four multi-step datasets, GOOGLEAPPS, INSTALL, WEBSHOPPING, and GENERAL, along with a single-step dataset SINGLE.
表2 显示 AITW 在类别和任务类型方面的组成。总体而言，AITW 由四个多步数据集 GOOGLEAPPS、INSTALL、WEBSHOPPING、GENERAL 组成，以及一个单步数据集 SINGLE。


The dataset is collected in a two-stage pipeline shown in Figure 1 First, we ask the raters to perform end-to-end tasks on emulators. Then the raters apply hindsight language relabeling [31, 32] to the trajectories that were collected in the first stage. We ask the raters to identify and label simple action sequences. We refer to these as single-step tasks.
数据集以两阶段流水线收集，如图1所示。首先，我们请评测者在模拟器上执行端到端任务。然后评测者对第一阶段收集的轨迹应用后验语言重标注[31, 32]。我们请评测者识别并标注简单的动作序列。我们将其称为单步任务。


Our recording system uses AndroidEnv [40] with the Android Emulator. The environment supports 3 action types \{TOUCH, LIFT, REPEAT\} with an (x, y) tuple indicating the on-screen position of the action. We record the TOUCH and LIFT actions. In response to an action, the environment returns an RGB screenshot, along with additional metadata such as the opened application. Raters interact with the emulated device using a mouse and keyboard on a desktop computer. Click events are logged as touch events. We provide dedicated buttons for Home, Back and Enter actions along with a field for entering text. We encourage the raters to use the dedicated buttons when necessary, however we require them to use a dedicated input text field for typing; we do not allow them to use the on-screen keyboard. We also ask the raters to indicate when they have completed the task or if they deem the task to be impossible to complete by pressing a button on our data collection UI.
我们的记录系统使用 AndroidEnv [40] 和 Android Emulator。该环境支持三种动作类型 {TOUCH、LIFT、REPEAT}，用一个 (x, y) 元组表示在屏幕上的动作位置。我们记录 TOUCH 与 LIFT 动作。对一个动作的响应，环境返回一个 RGB 截图，以及打开的应用等额外元数据。评测者使用桌面计算机的鼠标和键盘与模拟设备交互。点击事件被记录为触摸事件。我们为 Home、Back 和 Enter 动作提供专用按钮，并提供文本输入字段。我们鼓励评测者在必要时使用专用按钮，但要求使用专用文本输入字段进行打字；不允许使用屏幕键盘。我们还要求评测者在完成任务或认为任务无法完成时，通过在数据采集 UI 上按下按钮来表示。


The system captures the raw observations and actions at ${10}\mathrm{\;{Hz}}$ . Mouse presses and releases are recorded as TOUCH and LIFT, respectively. For touch events, we log the start and end position of the virtual finger's gesture, which we call a "dual-point" gesture. A scroll is represented by a start and end position,and a tap is a special case where the start and end are approximately equal $( <  = {0.04}$ Euclidean distance away). Figure 1 contains an example of a tap and horizontal scroll gesture using this formulation. We found the dual-point gesture abstraction to be a good trade-off between data compression and precision, allowing us to represent arbitrary drags that are needed to operate widgets, including scrolling through a menu and operating carousel widgets. After identifying dual-point gestures, we drop LIFT actions. Button presses and type events are logged as additional actions types. For type events, we log the typed text.
系统在 ${10}\mathrm{\;{Hz}}$ 时捕获原始观测与动作。鼠标按下和抬起分别记录为 TOUCH 与 LIFT。对于触摸事件，记录虚拟手指手势的起点和终点位置，我们称之为“双点”手势。滚动由起点和终点位置表示，点击是一种特殊情况，即起点与终点大致相等 $( <  = {0.04}$ 欧几里得距离的距离）。图1 给出使用此表示法的点击与水平滚动手势的示例。我们发现双点手势抽象在数据压缩与精度之间取得了良好平衡，能够表示操作部件所需的任意拖动，包括翻阅菜单和操作轮播部件。识别出双点手势后，我们移除 LIFT 动作。按钮按下与输入事件作为额外的动作类型被记录。对于输入事件，记录所输入的文本。


In summary, AITW's actions are described by four fields: type, touch_point, lift_point (only for gesture actions), and typed_text (only for typing actions). The type field can be one of the following: dual-point gesture, type, go_back, go_home, enter, task_complete, or task_impossible.
总之，AITW 的动作由四个字段描述：type、touch_point、lift_point（仅对手势动作）、typed_text（仅对打字动作）。type 字段可以是以下之一：双点手势、type、go_back、go_home、enter、task_complete 或 task_impossible。


We post-process RGB screenshots to map them to a set of detected UI elements. Each element has a bounding box and either OCR-detected text or an icon class label (one of the possible 96 icon types detected using IconNet [39]). The OCR outputs describe most of the text on the screen, although certain characters can be misidentified and text blocks are not always grouped as desired. Although this screen representation inferred from pixels is noisy and not as comprehensive as that obtained from UI metadata, we provide these features for convenience and expect developers will replace them with more powerful screen understanding models. We use these features for training and evaluating our models.
我们对 RGB 截图进行后处理，将其映射为一组检测到的 UI 元素。每个元素有一个边界框，且要么有 OCR 检出的文本，要么有图标类别标签（在通过 IconNet [39] 检测到的 96 种图标类型之一）。OCR 输出描述了屏幕上大部分文本，尽管某些字符可能被误识别，文本块也并不总是按期望分组。尽管基于像素推断的屏幕表示噪声较大、也不如来自 UI 元数据的表示那么全面，我们仍提供这些特征以便使用，且期望开发者用更强大的屏幕理解模型替换它们。我们将这些特征用于训练和评估我们的模型。


### 3.1 Multi-step task trajectories
### 3.1 多步任务轨迹


We first create high-level task instructions from various sources: (1) the authors, (2) a subset of PixelHelp [28] instructions that were deemed achievable, and (3) an LLM prompted to generate instructions. Next, we randomly assign instructions to raters and they follow them to complete tasks. Every task requires multiple steps to be performed. For example, the task "show my schedule for next week in Google Calendar" could correspond to the following steps: 1) opening Google calendar, 2) selecting "week view", and 3) opening next week. For each episode, we reset the environment to a random starting screen.
我们首先从多种来源创建高层次任务指令：（1）作者，（2）被认为可实现的一部分 PixelHelp [28] 指令，以及（3）被提示生成指令的大语言模型。接着，我们随机把指令分配给评审者，由他们按照指令完成任务。每个任务都需要执行多步。例如任务“在 Google 日历中显示下周的日程”可能对应以下步骤：1) 打开 Google 日历，2) 选择“周视图”，3) 打开下周。对于每一集，我们将环境重置到一个随机起始屏幕。


We ask the raters to interact with the device in a natural way, to avoid clicking on anything unrelated to the task, and to avoid unnecessary scrolling. To help guide the raters we prompt them with the following "Imagine a friend is asking you to perform the task on their phone..." The raters end a task with a special "status" action: either task_complete or task_impossible. A task is deemed impossible when an invalid or unavailable instruction is given, e.g., "turn on flashlight" on an emulator or "show my starred emails" when the Internet is not available. For instructions that result in verification rather than a state change (e.g., if the prompt is "Turn wifi off" and WiFi is found to be already off), we ask the raters to mark the task as successful.
我们请评审者以自然方式与设备互动，避免点击与任务无关的内容，避免不必要的滚动。为帮助引导评审者，我们以如下提示：“想象一个朋友让你在他/她的手机上完成此任务……”评审者以“任务完成”或“任务不可行”的特殊“状态”操作结束任务。当给出无效或不可用的指令时，任务被视为不可行，例如在模拟器上“打开手电筒”或在没有 Internet 的情况下“显示我的星标邮件”。对于那些仅验证而非状态变更的指令（例如提示为“关闭 WiFi”，但发现 WiFi 已经处于关闭状态），我们请评审者将任务标记为成功。


### 3.2 Hindsight language relabeling
### 3.2 事后性语言重标定


Single-step task demonstrations cannot be collected in the usual way of giving raters instructions and asking them to solve end-to-end tasks, since they require the relevant preceding steps to be executed. For example, in the task we described above, we cannot ask the raters to demonstrate "go to next week" unless they are already in the week view of the calendar app. Rather than asking raters to manually perform the single steps, we utilize event-selectable hindsight language relabeling [31, 32] to label previously collected trajectories.
单步任务演示无法用通常的方式收集，即不给评审者指令而直接让他们解决端到端任务，因为需要执行相关的前置步骤。例如，在前述任务中，若评审者尚未进入日历应用的周视图，我们就不能要求他们演示“进入下周”。与其让评审者手动执行单一步骤，我们更倾向使用事件可选择的事后性语言重标定 [31, 32]，对先前收集的轨迹进行标注。


To collect single-step demonstrations, we provide the raters observation-action sequences of multistep task trajectories and ask them to identify and annotate shorter sequences (around two to five frames). We instruct them to label single steps, e.g., "add item to cart", "show the settings", "show me my bookmarks". We ask that they label at least K subsequences (K >= 3 in our case) per video.
为收集单步演示，我们向评审者提供多步任务轨迹的观测-动作序列，并请他们识别并标注更短的序列（大约两到五帧）。我们指示他们标注单步，例如“将商品加入购物车”、“显示设置”、“显示我的书签”等。我们要求他们在每个视频中标注至少 K 个子序列（在我们的情况下 K ≥ 3）。


We instruct the raters to avoid the following words: "click", "select", "tap", "touch" or "scroll down/up/left/right", since these can be easily synthetically created, and instead ask them to write descriptive phrases that describe the result of the action (e.g., instead of "tap airplane mode", write the label "disable airplane mode").
我们指示评审者避免使用以下词语：“点击”、“选择”、“轻触”、“触摸”或“向下/向上/向左/向右滚动”，因为这些更容易被人工合成出来；相反，请他们撰写描述性短语来描述动作结果（例如，不要写“轻触飞行模式”，改写为标签“禁用飞行模式”）。


### 3.3 Dataset summary
### 3.3 数据集摘要


With reference to Table 2, we describe the 5 sub-categories of AITW.
参见表2，我们描述 AITW 的5个子类别。


<img src="https://cdn.noedgeai.com/bo_d5v2u2f7aajc73870hc0_5.jpg?x=311&y=199&w=1175&h=658&r=0"/>



Figure 2: Statistics for AITW. a) Episode length distribution. b) Episode length distribution by dataset group. c) Frequency of Android apps in the dataset. d) Token analysis including distribution of instruction length and token frequency for GOOGLEAPPS and GENERAL.
图2：AITW 的统计信息。a) 集数长度分布。b) 按数据集分组的集数长度分布。c) 数据集中 Android 应用的出现频率。d) 包含指令长度分布和 GOOGLEAPPS 与 GENERAL 的令牌频率的令牌分析。


GoogleEAPPS contains high-level tasks with some overlap from PixelHelp [28] which involve various Google applications such as Gmail, Calendar, Photos, Settings, etc.
GoogleEAPPS 包含与 PixelHelp [28] 有部分重叠的高层任务，涉及 Gmail、日历、相册、设置等多种 Google 应用。


INSTALL contains high-level tasks related to installing and uninstalling apps, app login, and app login support (e.g., "forgot password") for 88 different apps available on the Google Play store.
INSTALL 包含与安装和卸载应用、应用登录以及应用登录支持（例如“忘记密码”）相关的高层任务，涉及 Google Play 商店中可用的 88 种不同应用。


WEBSHOPPING contains tasks related to shopping on E-commerce websites. Example tasks include searching for an item, adding an item to the cart, viewing the shopping cart, etc.
WEBSHOPPING 包含与在电子商务网站上购物相关的任务。示例任务包括搜索商品、将商品加入购物车、查看购物车等。


GENERAL contains miscellaneous tasks (e.g., "play the new Taylor Swift video on YouTube"), mostly centered around question and answering (Q & A) (e.g., "How much does a 2 bedroom apartment rent cost in San Francisco?") and interacting with third-party apps and websites.
GENERAL 包含其他杂项任务（例如在 YouTube 播放 Taylor Swift 的新视频），大多以问答（Q&A）为中心（例如“在旧金山一套两居室的租金是多少？”）以及与第三方应用和网站进行互动。


SINGLE contains single-step tasks manually annotated using hindsight relabeling, mostly from WEBSHOPPING (e.g., "Close the pop-up then add first item to cart", "clear items from cart"). It also contains a smaller amount of episodes (560) from a variety of Google apps and third-party websites.
SINGLE 包含使用事后重标注手动注释的单步任务，主要来自 WEBSHOPPING（例如“先关闭弹窗再将第一项加入购物车”、“清空购物车中的商品”）。它还包含来自各种 Google 应用和第三方网站的较少量剧集（560 集）。


In Figure 2, we report statistics about AITW. The episode length distribution (Figure 2a), measured as number of steps required to complete the task, shows that tasks are of moderate length (between 2 and 16 steps for the 5th to 95th percentile, respectively) and that WEBSHOPPING tasks are generally the longest (Figure 2b). Chrome and Google apps are the most commonly used apps (Figure 2c). Overall, the dataset spans 159 Android apps and 198+ websites 5
在图2中，我们报告关于 AITW 的统计信息。任务完成所需的步骤数的分布（图2a）显示任务长度适中（第5至第95百分位分别在2到16步之间）， WEBSHOPPING 任务通常最长（图2b）。Chrome 和 Google 应用是最常使用的应用（图2c）。总体而言，数据集覆盖 159 个安卓应用和超过 198 个网站。5


(Figure 2d) shows summary statistics of the instructions. Instructions lengths fall between 4 and 24 for the 5th to 95th percentile, respectively, and are not overloaded with technical terms such as "click", "tap", "menu", "button", etc. which is generally the case for low-level UI commands provided in existing datasets [37, 4].
(图2d) 显示指令的汇总统计。指令长度在第5至第95百分位之间分别落在 4 到 24 之间，并且并未充斥诸如“点击”、“点击触摸”、“菜单”、“按钮”等技术术语，这在现有数据集中提供的低级 UI 命令中通常如此 [37, 4]。


## 4 Experimental setup
## 4 实验设置


With the ultimate goal of building automation systems that can generalize to new scenarios, we use a standard test split and also design four experimental setups to evaluate Out-of-Distribution (OOD) generalization.
为实现能够泛化到新场景的自动化系统的最终目标，我们使用标准测试拆分，并设计四种实验设置来评估分布外（OOD）泛化能力。


---



${}^{5}$ This number is a conservative estimate computed using heuristics.
${}^{5}$ 这一数值是通过启发式方法保守估计得出的。


---



Table 3: Examples of subject templates.
表3：主题模板示例。


<table><tr><td>Instruction</td><td>Subject template</td><td>Split</td></tr><tr><td>open app grab and go to login screen</td><td>open \{subject1\} and</td><td>train</td></tr><tr><td>open walmart and go to shopping cart</td><td>go to \{subject2\}</td><td>train</td></tr><tr><td>search newegg.com on google</td><td>search \{subject1\}</td><td>val</td></tr><tr><td>search usb-c to usb-a on ebay</td><td>on \{subject2\}</td><td>val</td></tr><tr><td>add jbl flip 4 to the cart on bestbuy</td><td>add \{subject1\} to the</td><td>test</td></tr><tr><td>add acer nitro to the cart on target</td><td>cart on \{subject2\}</td><td>test</td></tr></table>
<table><tbody><tr><td>指令</td><td>主题模板</td><td>分割</td></tr><tr><td>打开应用 获取并进入登录屏幕</td><td>打开 \{subject1\} 并</td><td>训练</td></tr><tr><td>打开沃尔玛 并转到购物车</td><td>前往 \{subject2\}</td><td>训练</td></tr><tr><td>在谷歌搜索 newegg.com</td><td>搜索 \{subject1\}</td><td>值</td></tr><tr><td>在 ebay 搜索 usb-c to usb-a</td><td>在 \{subject2\}</td><td>值</td></tr><tr><td>在 bestbuy 将 jbl flip 4 加入购物车</td><td>将 \{subject1\} 加入到</td><td>测试</td></tr><tr><td>在 target 将 acer nitro 加入购物车</td><td>购物车在 \{subject2\}</td><td>测试</td></tr></tbody></table>


Standard. We randomly split each dataset (the four multi-step datasets and SINGLE) episode wise into a training, validation, and test set (80/10/10%). Because the datasets different sizes, we evaluate each of them separately, then take the average score across them; we do the same for OOD setups.
标准。我们对每个数据集（四个多步骤数据集和 SINGLE）的每个 episodio 按 episodio 逐步随机分割成训练、验证和测试集（80/10/10%）。由于数据集大小不同，我们对每个数据集分别评估，然后对它们的平均分进行汇总；OOD 设置亦同样处理。


Unseen Android version. To evaluate a system's performance on an unseen Android version - which contains unseen graphical components and execution flows - we partition our data as follows: We put episodes collected on Android versions 10, 11, and 12 into the training and validation sets, maintaining a 90/10% split respectively. Then, we create a separate test set comprising entirely of episodes captured on Android version 13 devices.
未见 Android 版本。为了评估系统在未见 Android 版本上的表现——其中包含未见的图形组件与执行流程——我们按以下方式划分数据：将 Android 版本 10、11、12 收集的 эпизоды 放入训练和验证集，分别保持 90/10% 的分割。随后，创建一个仅由在 Android 13 设备上捕获的 эпизоды 构成的单独测试集。


Unseen subject and unseen verb. This setup is aimed at evaluating generalization to unseen instructions. Due to the large number of prompts in AITW, it is infeasible to manually group similar tasks together. Simply splitting based on exact match of the raw instructions would be the most straightforward way to automatically assign splits. However, similar instructions with minor changes in language would potentially be seen in both training and testing.
未见主题与未见动词。此设置旨在评估对未见指令的泛化能力。由于 AITW 提示数量庞大，手动将相似任务归为一组并不可行。仅基于原始指令的逐字匹配进行分割是最直接的自动分配方式；然而，语言稍有变动的相似指令可能在训练和测试中同时出现。


To better differentiate the training and test sets, we develop instruction templates by masking out either verb or subject phrases (examples provided in Table 3). By splitting data based on these templates, we can assess a system's ability to generalize to unseen language patterns, and occasionally to entirely new tasks. For instance, all instructions following the template add \{subject1\} to the cart on \{subject2\}" are grouped together, ensuring they are not represented in both training and testing sets. Similarly, verb-based templates such as open the shopping cart" and "view the shopping cart" would be assigned to the same split.
为更好地区分训练集与测试集，我们通过遮蔽动词或主语短语来构建指令模板（表 3 提供了示例）。基于这些模板进行数据分割，可以评估系统对未见语言模式的泛化能力，以及偶尔对全新任务的泛化能力。例如，所有遵循模板“将{subject1}加入购物车于{subject2}”的指令被归为一组，确保它们不在训练集和测试集同时出现。类似地，以动词为基础的模板如“打开购物车”和“查看购物车”也会被分到同一分组。


We extract the templates for each instruction, by prompting a few-shot LLM [13]. In total, we extract 6,111 subject templates and 22,122 verb templates. For both types, we randomly assign each template to a train, validation or test split (with 80/10/10%). Then for each episode, we determine its template based on its instruction, and map the episode to a split.
我们通过提示少量示例的大型语言模型 [13] 提取每条指令的模板。总共提取了 6,111 个主语模板和 22,122 个动词模板。对于这两种类型，我们将每个模板随机分配到训练、验证或测试集（80/10/10%）。然后对于每个 эпизод，根据其指令确定其模板，并将该 эпизод映射到一个分组。


Unseen domain. This split is designed to test an agent's ability to generalize to unseen apps and websites, which we refer to as domains. For WEBSHOPPING and GENERAL, we perform the split based on the web domain, as inferred from the instructions. For INSTALL tasks, we divide the data based on the app name, but we restrict these tasks to only those that require interaction with the installed app (e.g., performing a 'forgot password' request). Each domain, along with all associated episodes, is randomly assigned to a train/validation/test split (80/10/10%). We exclude SINGLE, as there are no distinguishable domains across tasks, and GOOGLEAPPS, due to the limited number of distinct apps.
未见领域。该拆分旨在测试智能体对未见应用与网站（我们称为领域）的泛化能力。对于 WEBSHOPPING 和 GENERAL，基于指令推断的网页域进行拆分。对于 INSTALL 任务，我们按应用名称划分数据，但仅限于那些需要与已安装应用交互的任务（如执行“忘记密码”请求）。每个领域及其所有相关 эпизоды 将随机分配到训练/验证/测试集（80/10/10%）。不包括 SINGLE，因为任务之间没有可区分的领域；也不包括 GOOGLEAPPS，因为可区别的应用数量有限。


## 5 Experiments
## 5 实验


In this section, we report results of two device-control agent models evaluated on AITW. Both models take as input a task instruction, the current screen's pixel-derived features (included in the dataset), and (optionally) a stacked history of screen observations and actions.
在本节中，我们报告对 AITW 评估的两种设备控制代理模型的结果。两种模型输入均为任务指令、当前屏幕的像素派生特征（数据集中包含）以及（可选的）屏幕观察与操作的历史堆栈。


### 5.1 Models
### 5.1 模型


BC. We implement a Transformer-based [41] Behavioural Cloning (BC) agent. The agent's output is in line with the AITW's data format. It outputs an action type and a gesture. The action type can be dual-point gesture, type, go_back, go_home, enter, task_complete, or task_impossible. The gesture action includes two spatial points, a touch and a lift position. This approach gives this agent a large and flexible action space, as it is able to predict taps and scrolls at arbitrary locations, rather than at specific UI elements as in existing work [4, 28]. We consider two variants of the agent, depending on whether it takes as input the screen-action history (2 prior steps), BC-history, or not, BC-single. Appendix B. 1 provides more implementation details.
BC。我们实现了一个基于 Transformer 的 [41] 行为克隆（BC）代理。代理的输出符合 AITW 的数据格式。它输出一个动作类型和一个手势。动作类型可以是双点手势、输入、后退、返回主页、进入、任务完成或任务不可能。手势包含两个空间点、一个触摸点和一个抬起点。该方法使该代理拥有一个大而灵活的动作空间，可以在任意位置预测点击和滚动，而不是像现有工作 [4, 28] 那样仅在特定 UI 元素上操作。我们根据是否输入屏幕-动作历史（前两步）、BC-历史或不输入，区分两种变体：BC-历史、BC-单点。附录 B.1 提供了更多实现细节。


LLM. We feed to PaLM 2 [3] a textual description of the screen and ask it to predict an action among the supported actions in AITW. We adopt a previously-proposed LLM-based design for device control [43], where the input screen (represented by an Android VH) is converted to HTML syntax. We use a modified version of their prompt (see Appendix B.2), and convert the OCR and detected icons to HTML. We create a zero-shot (LLM-0) and a 5-shot Chain-of-Thought (CoT) [44] (LLM-hist-5-CoT) version, which also contains history on prior actions taken by the agent, as we observed improves model performance. This model takes the same inputs as the BC model, but as in the original implementation [43], it can only click on detected UI elements, rather than at arbitrary locations and scrolling at precise locations. Since AITW was collected by humans performing precise gestures, some of the recorded gestures are not associated with OCR/Icon-detected UI elements, thus being not feasible for the LLM-based model. This could potentially be ameliorated in future versions by outputting a <x,y> output, rather than tapping specific elements.
LLM。我们向 PaLM 2 [3] 提供屏幕的文本描述，并要求其在 AITW 支持的动作中预测一个动作。我们采用先前提出的基于 LLM 的设备控制设计 [43]，其中输入屏幕（由 Android VH 表示）被转换为 HTML 语法。我们使用他们提示的一个修改版本（见附录 B.2），并将 OCR 与检测到的图标转换为 HTML。我们创建了零-shot（LLM-0）和一个包含代理前动作历史的五-shot Chain-of-Thought (CoT) 版本（LLM-hist-5-CoT），我们观察到这能提升模型性能。该模型与 BC 模型同样的输入，但像原实现 [43] 那样，它只能点击检测到的 UI 元素，而不能在任意位置点击或在特定位置滚动。由于 AITW 是由人类执行精确手势收集的，一些记录的手势并未与 OCR/图标检测到的 UI 元素相关，因此对基于 LLM 的模型不可行。未来版本可能通过输出 <x,y> 坐标来替代点击特定元素来缓解这一点。


### 5.2 Evaluation methodology and metrics
### 5.2 评估方法与指标


Online evaluation of device-control systems is hard because the execution environment generally does not provide a reward signal. Human validation of such systems can be leveraged, however watching and judging an agent's behaviour in real-time requires constant attention and is error prone. We propose an offline evaluation method which is cheaper and reproducible at the expense of accuracy.
设备控制系统的在线评估较难，因为执行环境通常不提供奖励信号。然而，可以利用对系统进行人类验证，然而在实时观看并判断代理的行为时需要持续关注，且易出错。我们提出一种离线评估方法，成本更低、可重复，但代价是精确性降低。


We devise and release the code for action matching to evaluate an agent's action's alignment with the ground truth. Two actions can match if their action types are equal. For dual-point taps, they are considered equal if they fall within a 14% screen distance from each other. Alternatively, if the tap actions occur within the same detected bounding box (augmented to 240% of their total size during action matching) they are considered equal. Finally, two dual-point scrolls are equal if they have the same primary scroll axis (vertical or horizontal).
我们设计并发布了用于动作匹配的代码，以评估代理动作与地真实值的对齐情况。若两者动作类型相同，则视为匹配。对双点点按，当两者相距在屏幕的 14% 内时视为相等。或者如果两次点击在同一个检测到的边界框内（在动作匹配阶段扩大到总尺寸的 240%），也视为相等。最后，若两次双点滚动具有相同的主滚动轴（纵向或横向），则视为相等。


Using action matching, we compute partial and complete action matching scores (originally proposed by Li et al. [28]). A partial score is defined as the number of correct actions divided by the episode length, and the complete score is defined as a partial match of 1.0.
使用动作匹配，我们计算部分匹配分数和完全匹配分数（最初由 Li 等人 [28] 提出）。部分分数定义为正确动作数量除以剧集长度，完全分数定义为部分匹配的 1.0。


To validate offline evaluation results, for subsets of the data, we also perform online evaluation. A human marks an episode as failed if any of the agent actions are incorrect, and correct when the agent performs a correct action on every step and achieves the expected goal. Human validation scores typically outperform complete action matching scores due to the multiple valid action alternatives one can take to complete a task. For instance, pressing the navigation bar's back button is functionally similar to using an app-specific back button. As action matching relies on distance-based measures, these actions are deemed distinct.
为了验证离线评估结果，对数据子集我们也进行在线评估。若代理的任何动作错误，人类将剧集标记为失败；若代理在每一步都执行正确动作并达到预期目标，则为正确。由于存在多种完成任务的有效动作选择，人工验证的分数通常优于完整动作匹配分数。例如，点击导航栏的返回按钮在功能上与使用应用内置返回按钮相似。由于动作匹配基于距离的度量，这些动作被视为不同。


### 5.3 Results
### 5.3 结果


We evaluate the four agents on the five AITW splits described in § 4 For the BC agent, we train and test using all the data. For the LLM agent, due to the high computational overhead, we test on a random sample of 288 episodes for each split. Table 4 reports the average partial matching scores.
我们在 §4 描述的五个 AITW 切分上评估四个代理。对 BC 代理，使用所有数据进行训练和测试。对 LLM 代理，由于计算开销较高，我们在每个切分上随机抽取 288 个剧集进行测试。表 4 报告平均部分匹配分数。


The BC agent performs the best across all splits. It performs reasonably well on the OOD tasks, particularly on the subject and verb template splits, indicating the model is generalizing to unseen language instructions and tasks. The LLM-based model only sees a small amount (only those k-shot that are in the prompt) of the training distribution for the OOD experiments. Making use of fine-tuning for future experiments would allow us to leverage more of the training data.
BC 代理在所有切分中表现最佳。对 OOD 任务也表现相当不错，特别是在主题和动词模板切分上，表明模型能对未见语言指令与任务进行泛化。基于 LLM 的模型在 OOD 实验中仅看到少量训练分布（提示中的那些 k-shot）。未来通过微调可利用更多训练数据来提升性能。


The performance of the LLM-based models suffers due to its element-based action space. For the standard test set, for example, 33% of the episodes have some non-element tap actions (i.e., only <x,y> location), which are infeasible for this modelling approach. Across the feasible actions, LLM-hist-5-CoT has a partial match score of 58%.
基于 LLM 的模型的表现受限于其基于元素的动作空间。例如，在标准测试集上，33% 的剧集存在某些非元素点击动作（即仅 <x,y> 位置），这对该建模方法不可行。就可行动作而言，LLM-hist-5-CoT 的部分匹配分数为 58%。


<table><tr><td rowspan="2">Model</td><td rowspan="2">Standard</td><td colspan="4">Out-of-domain generalization</td></tr><tr><td>Version</td><td>Subject</td><td>Verb</td><td>Domain</td></tr><tr><td>BC-single</td><td>68.7</td><td>59.2</td><td>64.2</td><td>66.4</td><td>52.2</td></tr><tr><td>BC-history</td><td>73.1</td><td>63.2</td><td>68.5</td><td>70.4</td><td>59.7</td></tr><tr><td>LLM-0 43</td><td>30.9 [25.6, 36.6]</td><td>31.6 [26.3, 37.3]</td><td>33.7 [28.2, 39.5]</td><td>32.6 [27.3, 38.4]</td><td>25.3 [20.4, 30.8]</td></tr><tr><td>LLM-hist-5-CoT</td><td>39.6 [33.9, 45.5]</td><td>29.5 [24.3, 35.1]</td><td>44.4 [38.6, 50.4]</td><td>41.7 [35.9, 47.6]</td><td>35.8 [30.2, 41.6]</td></tr></table>
<table><tbody><tr><td rowspan="2">模型</td><td rowspan="2">标准</td><td colspan="4">超出域的泛化</td></tr><tr><td>版本</td><td>主题</td><td>动词</td><td>领域</td></tr><tr><td>BC-单一</td><td>68.7</td><td>59.2</td><td>64.2</td><td>66.4</td><td>52.2</td></tr><tr><td>BC-历史</td><td>73.1</td><td>63.2</td><td>68.5</td><td>70.4</td><td>59.7</td></tr><tr><td>LLM-0 43</td><td>30.9 [25.6, 36.6]</td><td>31.6 [26.3, 37.3]</td><td>33.7 [28.2, 39.5]</td><td>32.6 [27.3, 38.4]</td><td>25.3 [20.4, 30.8]</td></tr><tr><td>LLM-hist-5-CoT</td><td>39.6 [33.9, 45.5]</td><td>29.5 [24.3, 35.1]</td><td>44.4 [38.6, 50.4]</td><td>41.7 [35.9, 47.6]</td><td>35.8 [30.2, 41.6]</td></tr></tbody></table>


Table 4: Partial match scores across standard and OOD generalization splits. For the LLM agent, the estimated score and binomial proportion 95% confidence interval are shown. BC evaluation is on the entire test sets; confidence intervals are $< {0.1}\%$ and are excluded for brevity.
表4：标准与OOD泛化分割下的部分匹配分数。对于LLM代理，显示了估计分数和二项式比例的95%置信区间。BC评估在整组测试集上；置信区间为$< {0.1}\%$，为简明起见省略。


<img src="https://cdn.noedgeai.com/bo_d5v2u2f7aajc73870hc0_8.jpg?x=535&y=575&w=709&h=287&r=0"/>



Figure 3: True complete match (estimated using human evaluation), and partial and complete match (both estimated using automated evaluation) for BC-history. True complete match is based on a subset of episodes; 95% confidence bounds are reported. Partial match is correlated with true complete match, while the complete match heuristic is a lower bound score.
图3：BC-history 的真实完全匹配（通过人工评估估计）以及部分匹配与完全匹配（均通过自动评估估计）。真实完全匹配基于子集剧集；给出95%置信区间。部分匹配与真实完全匹配相关，而完全匹配启发式是一个下界分数。


We perform human evaluation for BC-history on a small subset from GOOGLEAPPS (on average 86.5 episodes from each split). We use this dataset portion because it has the largest training set, but we exclude the domain split due to the limited number of apps. As shown in Figure 3, we find that action matching is a reasonable approximation of true success rates.
我们对GOOGLEAPPS的少量子集进行BC-history的人类评估（每个分割平均86.5个剧集）。选用该数据集部分是因为它具有最大的训练集，但由于应用数量有限，我们排除了领域分割。如图3所示，我们发现行动匹配是对真实成功率的一个合理近似。


As expected, the agent performs the best on the standard test split. Compared to what is observed across all dataset portions (Table 4) its performance on the standard set is higher, but on subject and verb OOD splits is lower. This is due to the nature of GOOGLEAPPS data (see Table 2) where the tasks are rather distinct (few unique prompts) which makes the verb and subject generalization hard, but at the same time every prompt has many demonstrations, which makes the standard test easier.
与预期相符，代理在标准测试分割上表现最佳。与所有数据集部分（表4）相比，其在标准集上的表现较高，但在主语和动词的OOD分割上较低。这归因于GOOGLEAPPS数据的性质（见表2），任务相当不同（很少有独特提示），这使得动词和主语泛化困难；但同时每个提示都有大量示例，这使得标准测试更容易。


Although the automated complete match is low, we note the agent is correct for the majority of steps as indicated by the partial match scores > 0.5. We confirmed this was the case by visual inspection. The agent typically performs many of the initial steps correct, but it is more error prone farther in the trajectory.
尽管自动完成匹配较低，我们注意到多数步骤的正确性由部分匹配分数>0.5所指示，代理在视觉检查中也证实了这一点。代理通常在初始阶段执行了许多正确步骤，但在轨迹后段更易出错。


In summary, across the four splits, partial match tends to be correlated with true complete match. It is a reliable approximation especially if the number of steps in a task is small. Automated complete metrics represent a lower bound score of the true value.
总之，在四个分割中，部分匹配往往与真实完全匹配相关。若任务步骤数较少，则这是一个可靠的近似。自动化的完全指标代表真实值的下界分数。


## 6 Discussion
## 6 讨论


### 6.1 Data Limitations
### 6.1 数据局限性


User Demographics Distribution. The raters are not a representative sample of the entire world population. The screens they visit, containing dynamic content from the Internet, are not representative of the rich variety of content and languages of the world. Similarly, the dataset prompts are exclusively in English, although they could potentially be translated and evaluated using multilingual models.
用户人口统计分布。评阅者并不能代表全球人群的样本。其访问的屏幕包含来自互联网的动态内容，并不能代表世界上丰富多样的内容与语言。同样，数据集提示仅以英语给出，尽管它们理论上可以翻译并使用多语言模型进行评估。


Rater device interaction. Raters use a mouse and keyboard rather than the native touch-based interface. This may result in somewhat different user patterns.
评阅者设备交互。评阅者使用鼠标和键盘，而非原生的触控界面。这可能导致用户使用模式有所不同。


Form factor. The dataset is derived from mobile phone user interactions. The dataset could be augmented with more form factors, such as tablets, to increase generalization.
外形尺寸。数据集来自手机用户交互。可通过增加平板等更多形式因子来扩充，以提升泛化能力。


UI Drift/Evolution. Our dataset includes an unseen domain split, containing new and unseen UIs, but it may not fully represent the continuous evolution of a given app or website's UI. This dynamic change is an essential aspect of real-world interfaces but is a complex phenomenon to capture comprehensively. However, we do capture some of this drift through the unseen Android version split, reflecting changes in Google apps' UI over various Android versions.
UI漂移/演化。我们的数据集中包含一个未见领域分割，包含新的未见UI，但它可能无法充分代表某个应用或网站的UI的持续演变。这一动态变化是现实世界界面的一个关键方面，但要全面捕捉是一个复杂的现象。不过，我们通过未见Android版本分割捕获了一些漂移，反映了Google应用在不同Android版本下UI的变化。


### 6.2 Ethical considerations
### 6.2 伦理考量


Privacy. The raters were instructed not to enter any Personal Identifiable Information (PII) during collection. The dataset does not contain any interactions from real users.
隐私。评阅者被指示在收集过程中不输入任何个人身份信息（PII）。数据集不包含真实用户的交互。


Malicious use. Malicious actors could use the dataset for undesired purposes such as overriding anti-fraud mechanisms like CAPTCHAs. Malicious actors could also manipulate prompts and/or screen representations of deployed models to achieve undesirable goals.
恶意用途。恶意行为者可能会利用数据集来进行如覆盖反欺诈机制（如验证码）的不当行为。恶意行为者还可能操纵提示和/或已部署模型的屏幕呈现以实现不良目标。


## 7 Future Work
## 7 未来工作


Multimodal modeling. The LLM-based model, adapted from prior work [43], is not as performant as the bespoke BC model. This model consumes a text-based screen representation and cannot output a <x,y> coordinate-based output. A multimodal foundation model [2, 12] that consumes raw pixels and outputs gestures at arbitrary points would be a natural next model type to investigate. Furthermore, any foundation models may benefit from fine-tuning on the AITW training sets.
多模态建模。基于大语言模型的模型，改自以往工作[43]，性能不如定制的 BC 模型。该模型接收基于文本的屏幕表示，不能输出基于 <x,y> 坐标的输出。一个接收原始像素并在任意点输出手势的多模态基础模型[2, 12]，将是一个自然的下一个待研究的模型类型。此外，任何基础模型在 AITW 训练集上的微调都可能受益。


Multiple ways to achieve a task. There are often multiple ways to achieve a task. Future evaluation methods could be more "lenient" and not penalize correct agent actions that do not match human demonstrations. Furthermore, constraining agents to achieve goals in "optimal" ways, however that may be defined, may increase user satisfaction with trained models.
完成任务的多种方式。通常有多种方式来完成任务。未来的评估方法可以更“宽松”，不对与人类示范不一致的正确代理操作进行惩罚。此外，将代理限制为以“最优”方式实现目标，无论其定义为何，可能会提高对训练模型的用户满意度。


## 8 Conclusions
## 8 结论


Mobile device control via natural language commands has broad application. It requires translating high-level instructions into execution plans that operate the device interface as a human would. Recent advancements in general-purpose large foundation models have opened doors for creating such device-control systems, however there remains a substantial void due to the dearth of large, comprehensive datasets essential for training and evaluating these systems.
通过自然语言指令实现对移动设备的控制具有广泛应用。它需要将高层指令转化为执行计划，以像人类一样操作设备界面。最近在通用大基础模型方面的进展为创建此类设备控制系统打开了大门，然而由于缺乏用于训练和评估这些系统的大规模、全面的数据集，仍存在明显空白。


Addressing these gaps, we present AITW, which is significantly larger and more diverse than existing device-control datasets. AITW consists of ${715}\mathrm{k}$ episodes across more than 350 Android applications and websites, and a variety of task instructions and execution paths, a realistic representation of real-world system interactions.
为弥补这些差距，我们提出 AITW，其规模和多样性显著大于现有的设备控制数据集。AITW 由跨越超过 350 个 Android 应用和网站的 ${715}\mathrm{k}$ 集，以及各种任务指令和执行路径组成，真实再现了现实世界系统交互。


Through dataset structure, we provide experimental setups for evaluation under varying conditions, including novel tasks and language, Android versions, and applications and websites. We trained and ran models on the data and demonstrated how to evaluate model performance under novel conditions. We hope AITW will spur research to create more powerful device automation models.
通过数据集结构，我们提供在不同条件下的评估实验设置，包括新任务和新语言、Android 版本，以及应用和网站。我们在数据上训练并运行模型，展示在新条件下评估模型性能的方法。我们希望 AITW 能推动研究，创建更强大的设备自动化模型。


## Acknowledgements
## 致谢


The authors thank Gabriel Taubman, James Stout, Gregory Wayne, and Max Lin for insightful discussions throughout. Thanks to Elisabeth Chauncey for help with dataset release. Thank you to JD Chen for helpful feedback on early manuscript versions. Daniel Toyama, Philippe Hamel, and Anita Gergely provided essential Android environment assistance. We also thank our raters for collecting our data.
作者感谢 Gabriel Taubman、James Stout、Gregory Wayne 与 Max Lin 在整个过程中的富有洞见的讨论。感谢 Elisabeth Chauncey 在数据集发布方面的帮助。感谢 JD Chen 对早期手稿版本的有益反馈。Daniel Toyama、Philippe Hamel 与 Anita Gergely 提供了关键的 Android 环境协助。也感谢评审者们为数据收集所做的努力。


## References
## 参考文献


[1] Adept. ACT-1: Transformer for Actions, 2022. https://www.adept.ai/act
[1] Adept. ACT-1: Transformer for Actions, 2022. https://www.adept.ai/act


[2] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro,
[2] J.-B. Alayrac、J. Donahue、P. Luc、A. Miech、I. Barr、Y. Hasson、K. Lenc、A. Mensch、K. Millican、M. Reynolds、R. Ring、E. Rutherford、S. Cabi、T. Han、Z. Gong、S. Samangooei、M. Monteiro、


J. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan. Flamingo: a visual language model for few-shot learning, 2022.
J. Menick、S. Borgeaud、A. Brock、A. Nematzadeh、S. Sharifzadeh、M. Binkowski、R. Barreira、O. Vinyals、A. Zisserman、以及 K. Simonyan。Flamingo：用于小样本学习的可视化语言模型，2022。


[3] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra, E. Moreira, M. Omernick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H. Abrego, J. Ahn, J. Austin, P. Barham, J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng, C. Cherry, C. A. Choquette-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani, S. Dev, J. Devlin, M. Díaz, N. Du, E. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag, X. Garcia, S. Gehrmann, L. Gonzalez, G. Gur-Ari, S. Hand, H. Hashemi, L. Hou, J. Howland, A. Hu, J. Hui, J. Hurwitz, M. Isard, A. Ittycheriah, M. Jagielski, W. Jia, K. Kenealy, M. Krikun, S. Kudugunta, C. Lan, K. Lee, B. Lee, E. Li, M. Li, W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu, M. Maggioni, A. Mahendru, J. Maynez, V. Misra, M. Moussalem, Z. Nado, J. Nham, E. Ni, A. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif, B. Richter, P. Riley, A. C. Ros, A. Roy, B. Saeta, R. Samuel, R. Shelby, A. Slone, D. Smilkov, D. R. So, D. Sohn, S. Tokumine, D. Valter, V. Vasudevan, K. Vodrahalli, X. Wang, P. Wang, Z. Wang, T. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. Xue, P. Yin, J. Yu, Q. Zhang, S. Zheng, C. Zheng, W. Zhou, D. Zhou, S. Petrov, and Y. Wu. Palm 2 technical report, 2023.
[3] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra, E. Moreira, M. Omernick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H. Abrego, J. Ahn, J. Austin, P. Barham, J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng, C. Cherry, C. A. Choquette-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani, S. Dev, J. Devlin, M. Díaz, N. Du, E. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag, X. Garcia, S. Gehrmann, L. Gonzalez, G. Gur-Ari, S. Hand, H. Hashemi, L. Hou, J. Howland, A. Hu, J. Hui, J. Hurwitz, M. Isard, A. Ittycheriah, M. Jagielski, W. Jia, K. Kenealy, M. Krikun, S. Kudugunta, C. Lan, K. Lee, B. Lee, E. Li, M. Li, W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu, M. Maggioni, A. Mahendru, J. Maynez, V. Misra, M. Moussalem, Z. Nado, J. Nham, E. Ni, A. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif, B. Richter, P. Riley, A. C. Ros, A. Roy, B. Saeta, R. Samuel, R. Shelby, A. Slone, D. Smilkov, D. R. So, D. Sohn, S. Tokumine, D. Valter, V. Vasudevan, K. Vodrahalli, X. Wang, P. Wang, Z. Wang, T. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. Xue, P. Yin, J. Yu, Q. Zhang, S. Zheng, C. Zheng, W. Zhou, D. Zhou, S. Petrov, and Y. Wu. Palm 2 技术报告，2023。


[4] C. Bai, X. Zang, Y. Xu, S. Sunkara, A. Rastogi, J. Chen, and B. A. y Arcas. UIBert: Learning generic multimodal representations for UI understanding. In Z. Zhou, editor, Proc. of the 30th International Joint Conference on Artificial Intelligence, IJCAI 2021, pages 1705-1712. ijcai.org, 2021.
[4] C. Bai, X. Zang, Y. Xu, S. Sunkara, A. Rastogi, J. Chen, and B. A. y Arcas. UIBert：学习用于 UI 理解的通用多模态表示。编辑：Z. Zhou，收录于人工智能国际联合会议 IJCAI 2021，卷号 1705-1712。ijcai.org，2021。


[5] P. Banerjee, S. Mahajan, K. Arora, C. Baral, and O. Riva. Lexi: Self-supervised learning of the UI language. In Proc. of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, December 2022.
[5] P. Banerjee, S. Mahajan, K. Arora, C. Baral, and O. Riva. Lexi：UI 语言的自监督学习。收录于 2022 年自然语言处理经验方法会议（Empirical Methods in Natural Language Processing）的论文集。计算语言学协会，2022 年 12 月。


[6] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji, A. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz, J. Ryan, C. Ré, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tramèr, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang. On the opportunities and risks of foundation models, 2022.
[6] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji, A. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz, J. Ryan, C. Ré, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tramèr, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang. 关于基础模型的机遇与风险，2022。


[7] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018.
[7] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, 以及 Q. Zhang。JAX：Python+NumPy 程序的可组合转换，2018。


[8] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020.
[8] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, 以及 D. Amodei。语言模型是少-shot 学习者，2020。


[9] A. Burns, D. Arsan, S. Agrawal, R. Kumar, K. Saenko, and B. A. Plummer. Mobile app tasks with iterative feedback (motif): Addressing task feasibility in interactive visual environments. CoRR, abs/2104.08560, 2021.
[9] A. Burns, D. Arsan, S. Agrawal, R. Kumar, K. Saenko, 与 B. A. Plummer。移动应用任务的迭代反馈（motif）：在交互式可视环境中解决任务可行性。CoRR，abs/2104.08560，2021。


[10] J. Chen, C. Chen, Z. Xing, X. Xu, L. Zhu, G. Li, and J. Wang. Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning. In Proc. of the ACM/IEEE 42nd International Conference on Software Engineering, ICSE '20, pages 322-334, 2020.
[10] 陈杰, 陈晨, 邢哲, 徐翔, 朱磊, 李刚, 及 王俊. 解盲你的应用：利用深度学习为移动 GUI 组件预测自然语言标签。 In Proc. of the ACM/IEEE 42nd International Conference on Software Engineering, ICSE '20, pages 322-334, 2020.


[11] J. Chen, M. Xie, Z. Xing, C. Chen, X. Xu, L. Zhu, and G. Li. Object detection for graphical user interface: Old fashioned or deep learning or a combination? In Proc. of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2020, pages 1202-1214, 2020.
[11] 陈杰, 谢默, 邢哲, 陈晨, 徐翔, 朱磊, 及 李刚. 图形用户界面的目标检测：传统还是深度学习，或是两者结合？ In Proc. of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2020, pages 1202-1214, 2020.


[12] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue, A. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini, C. Jia, B. K. Ayan, C. Riquelme, A. Steiner, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut. Pali: A jointly-scaled multilingual language-image model, 2023.
[12] 陈希, 王翔, 常品佑, 皮尔吉欧瓦尼, 帕德列夫斯基, Salz, Goodman, Grycner, Mustafa, Beyer, Kolesnikov, Puigcerver, 丁宁, 荣客, Riquelme, Steiner, Angelova, 里海, Houlsby,  Soricut。Pali: 一个联合尺度的多语种语言-图像模型，2023。


[13] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel. Palm: Scaling language modeling with pathways, 2022.
[13] Chowdhery 等人。Palm: 通过 Pathways 对语言建模进行大规模扩展，2022。


[14] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2Web: Towards a generalist agent for the web, 2023.
[14] 邓欣, 顾毅, 郑 博, 陈思, Stevens, 王斌, 孙晟, 苏阳。Mind2Web：迈向通用网络代理，2023。


[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv: 1810.04805, 2018.
[15] Devlin 等, Chang, Lee, Toutanova. Bert：用于语言理解的深度双向变换器的预训练。arXiv 预印本 arXiv: 1810.04805, 2018。


[16] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs/2010.11929, 2020.
[16] Dosovitskiy 等, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, Houlsby。一个图像等价于 16x16 个词：用于大规模图像识别的变换器。CoRR, abs/2010.11929, 2020。


[17] H. Furuta, O. Nachum, K.-H. Lee, Y. Matsuo, S. S. Gu, and I. Gur. Multimodal web navigation with instruction-finetuned foundation models, 2023.
[17] Furuta 等, Nachum, Lee, Matsuo, Gu, Gur。带指令微调的多模态网页导航基础模型，2023。


[18] I. Gur, N. Jaques, Y. Miao, J. Choi, M. Tiwari, H. Lee, and A. Faust. Environment generation for zero-shot compositional reinforcement learning, 2022.
[18] I. Gur, N. Jaques, Y. Miao, J. Choi, M. Tiwari, H. Lee, and A. Faust. 针对零-shot 组合强化学习的环境生成，2022。


[19] I. Gur, O. Nachum, Y. Miao, M. Safdari, A. Huang, A. Chowdhery, S. Narang, N. Fiedel, and A. Faust. Understanding html with large language models, 2023.
[19] I. Gur, O. Nachum, Y. Miao, M. Safdari, A. Huang, A. Chowdhery, S. Narang, N. Fiedel, and A. Faust. 使用大型语言模型理解 html，2023。


[20] I. Gur, U. Rueckert, A. Faust, and D. Hakkani-Tur. Learning to Navigate the Web. In 7th International Conference on Learning Representations (ICLR '19), May 6-9 2019.
[20] I. Gur, U. Rueckert, A. Faust, and D. Hakkani-Tur. 学习导航网页。在第七届国际学习表征会议（ICLR '19），2019年5月6-9日。


[21] Z. He, S. Sunkara, X. Zang, Y. Xu, L. Liu, N. Wichers, G. Schubiner, R. B. Lee, and J. Chen. ActionBert: Leveraging User Actions for Semantic Understanding of User Interfaces. In 35th AAAI Conference on Artificial Intelligence, AAAI 2021, pages 5931-5938, 2021.
[21] Z. He, S. Sunkara, X. Zang, Y. Xu, L. Liu, N. Wichers, G. Schubiner, R. B. Lee, and J. Chen. ActionBert：利用用户行为对用户界面的语义理解。第35届人工智能国际会议（AAAI 2021），页面5931-5938，2021。


[22] T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX, 2020.
[22] T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku：JAX 的十四行诗，2020。


[23] M. W. Hoffman, B. Shahriari, J. Aslanides, G. Barth-Maron, N. Momchev, D. Sinopanlikov, P. Stańczyk, S. Ramos, A. Raichuk, D. Vincent, L. Hussenot, R. Dadashi, G. Dulac-Arnold, M. Orsini, A. Jacq, J. Ferret, N. Vieillard, S. K. S. Ghasemipour, S. Girgin, O. Pietquin, F. Behbahani, T. Norman, A. Abdolmaleki, A. Cassirer, F. Yang, K. Baumli, S. Henderson, A. Friesen, R. Haroun, A. Novikov, S. G. Colmenarejo, S. Cabi, C. Gulcehre, T. L. Paine, S. Srinivasan, A. Cowie, Z. Wang, B. Piot, and N. de Freitas. Acme: A research framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020.
[23] M. W. Hoffman, B. Shahriari, J. Aslanides, G. Barth-Maron, N. Momchev, D. Sinopanlikov, P. Stańczyk, S. Ramos, A. Raichuk, D. Vincent, L. Hussenot, R. Dadashi, G. Dulac-Arnold, M. Orsini, A. Jacq, J. Ferret, N. Vieillard, S. K. S. Ghasemipour, S. Girgin, O. Pietquin, F. Behbahani, T. Norman, A. Abdolmaleki, A. Cassirer, F. Yang, K. Baumli, S. Henderson, A. Friesen, R. Haroun, A. Novikov, S. G. Colmenarejo, S. Cabi, C. Gulcehre, T. L. Paine, S. Srinivasan, A. Cowie, Z. Wang, B. Piot, and N. de Freitas. Acme：一种用于分布式强化学习的研究框架。arXiv 预印本 arXiv:2006.00979，2020。


[24] P. C. Humphreys, D. Raposo, T. Pohlen, G. Thornton, R. Chhaparia, A. Muldal, J. Abramson, P. Georgiev, A. Santoro, and T. Lillicrap. A data-driven approach for learning to control computers. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 9466-9482. PMLR, 17-23 Jul 2022.
[24] P. C. Humphreys, D. Raposo, T. Pohlen, G. Thornton, R. Chhaparia, A. Muldal, J. Abramson, P. Georgiev, A. Santoro, and T. Lillicrap. 一种数据驱动的学习控制计算机方法。收录于 K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, S. Sabato 编辑的《第39届国际机器学习大会论文集》，Machine Learning Research 论文集 第162卷，页面9466-9482。PMLR，2022年7月17-23日。


[25] S. Jia, J. Kiros, and J. Ba. Dom-q-net: Grounded rl on structured language, 2019.
[25] S. Jia, J. Kiros, and J. Ba. Dom-q-net：在结构化语言上进行 groundedrl，2019。


[26] G. Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks, 2023.
[26] G. Kim, P. Baldi, and S. McAleer. 语言模型能解决计算机任务，2023。


[27] K. Lee, M. Joshi, I. Turc, H. Hu, F. Liu, J. Eisenschlos, U. Khandelwal, P. Shaw, M.-W. Chang, and K. Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding, 2022.
[27] K. Lee, M. Joshi, I. Turc, H. Hu, F. Liu, J. Eisenschlos, U. Khandelwal, P. Shaw, M.-W. Chang, and K. Toutanova. Pix2struct：将截图解析视为视觉语言理解的预训练，2022。


[28] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge. Mapping natural language instructions to mobile UI action sequences. In Proc. of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 8198-8210. Association for Computational Linguistics, 2020.
[28] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge. 将自然语言指令映射到移动 UI 动作序列。发表于第58届计算语言学会年会（ACL 2020），在线，2020年7月5-10日，页面8198-8210。计算语言学协会，2020。


[29] E. Z. Liu, K. Guu, P. Pasupat, and P. Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In 6th International Conference on Learning Representations (ICLR '18), 2018.
[29] E. Z. Liu, K. Guu, P. Pasupat, and P. Liang. 基于工作流引导探索的网页界面强化学习。第6届国际学习表征大会（ICLR '18），2018。


[30] T. F. Liu, M. Craft, J. Situ, E. Yumer, R. Mech, and R. Kumar. Learning design semantics for mobile apps. In Proc. of the 31st Annual ACM Symposium on User Interface Software and Technology, UIST '18, page 569-579, New York, NY, USA, 2018. Association for Computing Machinery.
[30] T. F. Liu, M. Craft, J. Situ, E. Yumer, R. Mech, and R. Kumar. 为移动应用学习设计语义。In Proc. of the 31st Annual ACM Symposium on User Interface Software and Technology, UIST '18, page 569-579, New York, NY, USA, 2018. Association for Computing Machinery.


[31] C. Lynch and P. Sermanet. Language conditioned imitation learning over unstructured data. Robotics: Science and Systems, 2021.
[31] C. Lynch and P. Sermanet. 在非结构化数据上进行语言条件模仿学习。Robotics: Science and Systems, 2021.


[32] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong, and P. Florence. Interactive language: Talking to robots in real time, 2022.
[32] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong, and P. Florence. 互动语言：实时对话机器人，2022。


[33] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: Large language model connected with massive apis, 2023.
[33] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: 连接大量 API 的大语言模型，2023。


[34] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian, S. Zhao, R. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, and M. Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023.
[34] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian, S. Zhao, R. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, and M. Sun. Toolllm：促使大语言模型掌握 16000+ 个真实世界的 API，2023。


[35] A. Richard, H. Kuehne, and J. Gall. Weakly supervised action learning with RNN based fine-to-coarse modeling. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1273-1282, 2017.
[35] A. Richard, H. Kuehne, and J. Gall. 以 RNN 为基础的自上而下粗粒度建模实现的弱监督动作学习。In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)，页面 1273-1282，2017。


[36] T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools, 2023.
[36] T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer：语言模型可以自学使用工具，2023。


[37] T. Shi, A. Karpathy, L. Fan, J. Hernandez, and P. Liang. World of bits: An open-domain platform for web-based agents. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3135-3144. PMLR, 06-11 Aug 2017.
[37] T. Shi, A. Karpathy, L. Fan, J. Hernandez, and P. Liang. 世界中的位：面向开放域的网页代理平台。In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3135-3144. PMLR, 2017 年 6-11 月 06-11 Aug 2017.


[38] M. Shvo, Z. Hu, R. T. Icarte, I. Mohomed, A. Jepson, and S. A. McIlraith. Appbuddy: Learning to accomplish tasks in mobile apps via reinforcement learning, 2021.
[38] M. Shvo, Z. Hu, R. T. Icarte, I. Mohomed, A. Jepson, and S. A. McIlraith. Appbuddy：通过强化学习在移动应用中完成任务，2021。


[39] S. Sunkara, M. Wang, L. Liu, G. Baechler, Y.-C. Hsiao, J. Chen, A. Sharma, and J. W. W. Stout. Towards better semantic understanding of mobile interfaces. In Proc. of the 29th International Conference on Computational Linguistics, pages 5636-5650. International Committee on Computational Linguistics, Oct. 2022.
[39] S. Sunkara, M. Wang, L. Liu, G. Baechler, Y.-C. Hsiao, J. Chen, A. Sharma, and J. W. W. Stout. 走向对移动界面的更好语义理解。In Proc. of the 29th International Conference on Computational Linguistics, pages 5636-5650. International Committee on Computational Linguistics, Oct. 2022.


[40] D. Toyama, P. Hamel, A. Gergely, G. Comanici, A. Glaese, Z. Ahmed, T. Jackson, S. Mourad, and D. Precup. Androidenv: A reinforcement learning platform for android, 2021.
[40] D. Toyama, P. Hamel, A. Gergely, G. Comanici, A. Glaese, Z. Ahmed, T. Jackson, S. Mourad, and D. Precup. Androidenv：一个用于 Android 的强化学习平台，2021。


[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2017.
[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need，2017。


[42] S. G. Venkatesh, P. Talukdar, and S. Narayanan. Ugif: Ui grounded instruction following, 2022.
[42] S. G. Venkatesh, P. Talukdar, and S. Narayanan. Ugif：Ui 级指令跟随，2022。


[43] B. Wang, G. Li, and Y. Li. Enabling conversational interaction with mobile ui using large language models. In Proc. of the 2023 CHI Conference on Human Factors in Computing Systems, CHI '23. Association for Computing Machinery, 2023.
[43] B. Wang, G. Li, and Y. Li. 使用大型语言模型实现与移动 UI 的对话交互。In Proc. of the 2023 CHI Conference on Human Factors in Computing Systems, CHI '23. Association for Computing Machinery, 2023.


[44] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.
[44] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-of-thought prompting 引发大型语言模型的推理，2023。


[45] XDA. Google is trying to limit what apps can use an Accessibility Service (again), 2021. https://www.xda-developers.com/ google-trying-limit-apps-accessibility-service/
[45] XDA. Google 正在再次限制应用可以使用的无障碍服务，2021。https://www.xda-developers.com/ google-trying-limit-apps-accessibility-service/


[46] X. Zhang, L. de Greef, A. Swearngin, S. White, K. Murray, L. Yu, Q. Shan, J. Nichols, J. Wu, C. Fleizach, A. Everitt, and J. P. Bigham. Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels. In Proc. of the 2021 CHI Conference on Human Factors in Computing Systems, CHI '21, 2021.
[46] X. Zhang, L. de Greef, A. Swearngin, S. White, K. Murray, L. Yu, Q. Shan, J. Nichols, J. Wu, C. Fleizach, A. Everitt, and J. P. Bigham. Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels. In Proc. of the 2021 CHI Conference on Human Factors in Computing Systems, CHI '21, 2021.


## Appendix A Dataset collection
## Appendix A 数据集收集


### A.1 Crowdsourcing
### A.1 众包


This work was carried out by participants who are paid contractors. Those contractors received a standard contracted wage, which complies with living wage laws in their country of employment. Due to global privacy concerns, we cannot include more details about our participants, e.g., estimated hourly wage or total amount spent on compensation.
本研究由受雇佣的承包商参与完成。这些承包商获得了标准合同工资，符合其工作所在国家的生活工资法规定。由于全球隐私 concerns，我们无法提供关于参与者的更多细节，例如估算的时薪或用于补偿的总金额。


We provided raters with a detailed instructional document and a video tutorial, followed by having them perform test demonstrations using our system. For multi-step task trajectories, we ensured quality and diversity through manual inspection of a subset of demonstrations. Tasks were marked as complete with the task_complete action once a rater completed an assignment, including cases where the task was already completed. In contrast, the task_impossible action was used to indicate infeasible tasks, such as turning on a flashlight in an emulator.
我们向评估者提供了详细的指令文档和一个视频教程，然后让他们使用我们的系统进行测试演示。对于多步骤任务的轨迹，我们通过对部分演示进行人工检查来确保质量和多样性。任务在评估者完成任务后通过 task_complete 操作记为完成，即使该任务已提前完成。相反，使用 task_impossible 操作表示不可行的任务，例如在模拟器中打开手电筒。


For hindsight-language relabeling, we conducted manual reviews of a sample of labeled trajectories due to the nuanced nature of natural language use. The aim was to encourage the creation of descriptive, unambiguous labels and discourage the use of oversimplified technical terms or vague language in order to collect clear and useful task descriptions that cannot be captured using automatic heuristics.
对于回顾性语言重新标注，由于自然语言使用的微妙性质，我们对标注轨迹的样本进行了人工评审。目的是鼓励创建描述性、明确的标签，防止使用过于简化的技术术语或模糊语言，以便收集清晰且有用的任务描述，这些描述不能通过自动启发式方法捕捉到。


### A.2 Prompt generation
### A.2 提示生成


We use the following prompt to extract the subject templates and a similar prompt for the verb templates:
我们使用下列提示来提取主体模板，并为动词模板使用类似的提示：


---



#Identify subject variables in commands.
#在命令中识别主体变量。


phrase = ["show me popular videos on youtube",
短语 = ["show me popular videos on youtube",


"whats the weather?",
"whats the weather?",


&nbsp;&nbsp;&nbsp;&nbsp;"go to espn.com",
&nbsp;&nbsp;&nbsp;&nbsp;"go to espn.com",


&nbsp;&nbsp;&nbsp;&nbsp;"click the top result",
&nbsp;&nbsp;&nbsp;&nbsp;"click the top result",


&nbsp;&nbsp;&nbsp;&nbsp;"open calendar and show me the fourth week of next month",
&nbsp;&nbsp;&nbsp;&nbsp;"open calendar and show me the fourth week of next month",


&nbsp;&nbsp;&nbsp;&nbsp;<INPUT_INSTRUCTIONS>]
&nbsp;&nbsp;&nbsp;&nbsp;<INPUT_INSTRUCTIONS>]


&nbsp;&nbsp;&nbsp;&nbsp;result = ["show me \{subject1\} on \{subject2\}",
&nbsp;&nbsp;&nbsp;&nbsp;result = ["show me \{subject1\} on \{subject2\}",


&nbsp;&nbsp;&nbsp;&nbsp;"whats \{subject1\}?",
&nbsp;&nbsp;&nbsp;&nbsp;"这是什么 {subject1}？",


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"go to \{subject1\}",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"前往 {subject1}"，


&nbsp;&nbsp;&nbsp;&nbsp;"click \{subject1\}",
&nbsp;&nbsp;&nbsp;&nbsp;"点击 {subject1}"，


&nbsp;&nbsp;&nbsp;&nbsp;"open \{subject1\} and show me \{subject2\} of \{subject3\}",
&nbsp;&nbsp;&nbsp;&nbsp;"打开 {subject1} 并展示 {subject2} 的 {subject3}"，


---



### A.3 Examples
### A.3 示例


Example of episodes from AITW are show in Figures 4 [5] and 6
来自 AITW 的剧集示例在图 4[5] 和 6 中显示


## Appendix B Experiment details
## 附录 B 实验细节


### B.1 Behavioral Cloning
### B.1 行为克隆


The Behavioral Cloning (BC), shown in Figure 7 is a Transformer-based architecture [41] that takes a task instruction, the current screen, and a stacked history of screen observations and actions as input. The model is conditioned on BERT [15] embeddings of the natural language instruction. For the screen input the model embeds each detected OCR and detected icon to a vector of size 512 using the following procedure. We embed the text using a pre-trained BERT model taking the output from the CLS token, which is then linearly projected from size 732 to 512. For the icons, we learn an embedding from the ID, which we add element-wise to the BERT embedding. Following similar approaches [28, 16], we add to this the spatial information by learning four embeddings for each of the bounding box points, which are binned into 200 and 96 elements vertically and horizontally. For the screen history (excluding the current screen), we embed the <x,y> positions of the touch and lift actions, which are added to the element encoding, using a dummy value for non-gesture actions. We found that including action history improves performance.
行为克隆（BC），如图 7 所示，是一个基于 Transformer 的架构 [41]，以任务指令、当前屏幕以及屏幕观测和动作的堆叠历史作为输入。该模型以自然语言指令的 BERT [15] 嵌入为条件。对于屏幕输入，模型将检测到的 OCR 和检测到的图标嵌入为大小为 512 的向量，过程如下。我们使用从 CLS 标记输出的预训练 BERT 模型对文本进行嵌入，然后线性投影从大小 732 到 512。对于图标，我们从 ID 学习一个嵌入，将其逐元素加到 BERT 嵌入之上。沿用类似方法 [28, 16]，还通过为每个边界框点学习四个嵌入来加入空间信息，这些点按垂直方向 200、水平方向 96 的分箱进行离散化。对于屏幕历史（不含当前屏幕），我们对触摸与提起动作的位置 <x,y> 进行嵌入，作为元素编码的附加，非手势动作使用一个虚拟值。我们发现包含动作历史能提升性能。


<img src="https://cdn.noedgeai.com/bo_d5v2u2f7aajc73870hc0_15.jpg?x=309&y=247&w=1184&h=805&r=0"/>



Figure 4: Example episode from the dataset.
Figure 4: 数据集的示例片段。


<img src="https://cdn.noedgeai.com/bo_d5v2u2f7aajc73870hc0_15.jpg?x=312&y=1196&w=1180&h=830&r=0"/>



Figure 5: Example episode from the dataset.
Figure 5: 数据集的示例片段。


<img src="https://cdn.noedgeai.com/bo_d5v2u2f7aajc73870hc0_16.jpg?x=336&y=193&w=1132&h=1150&r=0"/>



Figure 6: Example episode from the dataset.
Figure 6: 数据集的示例片段。


<img src="https://cdn.noedgeai.com/bo_d5v2u2f7aajc73870hc0_16.jpg?x=406&y=1423&w=991&h=561&r=0"/>



Figure 7: Architecture diagram of the BC agent.
Figure 7: BC智能体的架构图。


Table 5: Partial match scores across generalization splits and datasets for BC-history.
Table 5: BC-history 在泛化分割与数据集上的部分匹配分数。


<table><tr><td>Dataset</td><td>Standard</td><td>Version</td><td>Subject</td><td>Verb</td><td>Domain</td></tr><tr><td>GoogleApps</td><td>75.7</td><td>63.4</td><td>48.4</td><td>57.4</td><td>n/a</td></tr><tr><td>GENERAL</td><td>63.7</td><td>52.5</td><td>65.4</td><td>64.1</td><td>n/a</td></tr><tr><td>WEBSHOPPING</td><td>68.5</td><td>56.7</td><td>69.5</td><td>68.4</td><td>49.6</td></tr><tr><td>INSTALL</td><td>77.5</td><td>66.5</td><td>76.4</td><td>77.7</td><td>66.9</td></tr><tr><td>SINGLE</td><td>80.3</td><td>77.0</td><td>82.8</td><td>84.6</td><td>62.6</td></tr></table>
<table><tbody><tr><td>数据集</td><td>标准</td><td>版本</td><td>主题</td><td>动词</td><td>领域</td></tr><tr><td>GoogleApps</td><td>75.7</td><td>63.4</td><td>48.4</td><td>57.4</td><td>n/a</td></tr><tr><td>通用</td><td>63.7</td><td>52.5</td><td>65.4</td><td>64.1</td><td>n/a</td></tr><tr><td>网购</td><td>68.5</td><td>56.7</td><td>69.5</td><td>68.4</td><td>49.6</td></tr><tr><td>安装</td><td>77.5</td><td>66.5</td><td>76.4</td><td>77.7</td><td>66.9</td></tr><tr><td>单一</td><td>80.3</td><td>77.0</td><td>82.8</td><td>84.6</td><td>62.6</td></tr></tbody></table>


For gesture actions, the agent outputs a dual-point output. We found such a formulation useful for interacting with many common widgets (e.g., carousel widgets, switching months in a calendar, controlling sliders), which require precise scrolling.
对于手势操作，智能体输出双点输出。我们发现这样的表述对于与许多常见控件交互（如轮播控件、日历中切换月份、控制滑块等）非常有用，这些都需要精确滚动。


We train the agent using the standard cross-entropy loss using a 2x2 slice of a V2 Tensor Processing Unit (TPU). The agent is implemented using Acme [23], Haiku [22], and JAX [7]. The Transformer has 4 layers, a dropout rate of 0.1 , and we train use the AdamW optimizer with a learning rate of 0.0001, and a batch size of 128.
我们使用标准交叉熵损失在一个 V2 TPU 的 2x2 切片上训练智能体。智能体使用 Acme [23]、Haiku [22] 和 JAX [7] 实现。Transformer 有 4 层， dropout 率为 0.1，我们使用 AdamW 优化器，学习率 0.0001，批量大小 128。


For evaluation we train and perform a hyperparameter search via grid search on the validation set. We choose the best performing model and run it on the test set for the final numbers.
在评估方面，我们在验证集上进行训练并通过网格搜索进行超参数搜索。我们选择表现最好的模型，并在测试集上运行以获得最终数值。


Table 5 reports a breakdown of the performance of the BC-history agent (our best performing agent) across the different dataset splits and portions.
表 5 报告 BC-history 智能体（我们表现最好的智能体）在不同数据集分割和部分上的性能细分。


### B.2 LLM
### B.2 LLM


We use the following prompt for LLM-0:
我们对 LLM-0 使用以下提示：


---



&nbsp;&nbsp;&nbsp;&nbsp;Given a mobile screen and a question, provide the action based on the screen
&nbsp;&nbsp;&nbsp;&nbsp;\t给定一个移动屏幕和一个问题，请基于屏幕给出动作


information.
信息。


Available Actions:
可用操作：


\{"action_type": "click", "idx": <element_idx>\}
{"action_type": "click", "idx": <element_idx>}


&nbsp;&nbsp;&nbsp;&nbsp;\{"action_type": "type", "text": <text>
&nbsp;&nbsp;&nbsp;&nbsp;\t{"action_type": "type", "text": <text>


\{"action_type": "navigate_home"\}
{"action_type": "navigate_home"}


&nbsp;&nbsp;&nbsp;&nbsp;\{"action_type": "navigate_back"\}
&nbsp;&nbsp;&nbsp;&nbsp;{"action_type": "navigate_back"}


&nbsp;&nbsp;&nbsp;&nbsp;\{"action_type": "scroll", "direction": "up"\}
&nbsp;&nbsp;&nbsp;&nbsp;{"action_type": "scroll", "direction": "up"}


\{"action_type": "scroll", "direction": "down"\}
{"action_type": "scroll", "direction": "down"}


&nbsp;&nbsp;&nbsp;&nbsp;\{"action_type": "scroll", "direction": "left"\}
&nbsp;&nbsp;&nbsp;&nbsp;{"action_type": "scroll", "direction": "left"}


\{"action_type": "scroll", "direction": "right"\}
{"action_type": "scroll", "direction": "right"}


Screen:
屏幕：


<SCREEN_REPRESENTATION>



Instruction: <SCROUNDING_GOAL>
说明： <SCROUNDING_GOAL>


Answer:
回答：


---



We use the following prompt for LLM-hist-5-CoT:
我们使用以下提示用于 LLM-hist-5-CoT：


---



&nbsp;&nbsp;&nbsp;&nbsp;Given a mobile screen and a question, provide the action based on the screen
&nbsp;&nbsp;&nbsp;&nbsp;给定移动端屏幕和问题，基于屏幕提供操作


information.
信息。


Available Actions:
可用操作：


&nbsp;&nbsp;&nbsp;&nbsp;\{"action_type": "click", "idx": <element_idx>\}
&nbsp;&nbsp;&nbsp;&nbsp;{"action_type": "click", "idx": <element_idx>}


&nbsp;&nbsp;&nbsp;&nbsp;\{"action_type": "type", "text": <text>
&nbsp;&nbsp;&nbsp;&nbsp;{"action_type": "type", "text": <text>


&nbsp;&nbsp;&nbsp;&nbsp;\{"action_type": "navigate_home"\}
&nbsp;&nbsp;&nbsp;&nbsp;{"action_type": "navigate_home"}


&nbsp;&nbsp;&nbsp;&nbsp;\{"action_type": "navigate_back"\}
&nbsp;&nbsp;&nbsp;&nbsp;{"action_type": "navigate_back"}


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\{"action_type": "scroll", "direction": "up"\}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{"action_type": "scroll", "direction": "up"}


\{"action_type": "scroll", "direction": "down"\}
{"action_type": "scroll", "direction": "down"}


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\{"action_type": "scroll", "direction": "left"\}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{"action_type": "scroll", "direction": "left"}


&nbsp;&nbsp;&nbsp;&nbsp;\{"action_type": "scroll", "direction": "right"\}
&nbsp;&nbsp;&nbsp;&nbsp;{"action_type": "scroll", "direction": "right"}


---



---



&nbsp;&nbsp;&nbsp;&nbsp;Previous Actions:
&nbsp;&nbsp;&nbsp;&nbsp;Previous Actions:


&nbsp;&nbsp;&nbsp;&nbsp;\{"step_idx": 0, "action_description": "press [HOME key]"\}
&nbsp;&nbsp;&nbsp;&nbsp;\{"step_idx": 0, "action_description": "按下 [HOME 键]"\}


&nbsp;&nbsp;&nbsp;&nbsp;\{"step_idx": 2, "action_description": "click [Google Icon]"\}
&nbsp;&nbsp;&nbsp;&nbsp;\{"step_idx": 2, "action_description": "点击 [Google 图标]"\}


\{"step_idx": 3, "action_description": "click [search for hotels]"\}
\{"step_idx": 3, "action_description": "点击 [搜索酒店]"\}


Screen:
Screen:


<img id=0 class="IconGoogle" alt="Google Icon"> </img>
<img id=0 class="IconGoogle" alt="Google Icon"> </img>


<img id=1 class="IconX" alt="Close Icon"> </img>
<img id=1 class="IconX" alt="Close Icon"> </img>


<p id=2 class="text" alt="search for hotels"> search for hotels </p>
<p id=2 class="text" alt="search for hotels"> 搜索酒店 </p>


<p id=3 class="text" alt="in"> in </p>
<p id=3 class="text" alt="in"> in </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=4 class="text" alt="mexico city mexico"> mexico city mexico </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=4 class="text" alt="mexico city mexico"> 墨西哥城，墨西哥 </p>


<img id=5 class="IconMagnifyingGlass" alt="Search Icon"> </img>
<img id=5 class="IconMagnifyingGlass" alt="Search Icon"> </img>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=6 class="text" alt="Share"> Share </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=6 class="text" alt="Share"> 分享 </p>


<p id=7 class="text" alt="Select alI"> Select alI </p>
<p id=7 class="text" alt="Select alI"> 选择 All </p>


<p id=8 class="text" alt="Cut"> Cut </p>
<p id=8 class="text" alt="Cut"> 剪切 </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=9 class="text" alt="Copy"> Copy </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=9 class="text" alt="Copy"> 复制 </p>


<p id=10 class="text" alt="hotel in mex"> hotel in mex </p>
<p id=10 class="text" alt="hotel in mex"> 墨西哥的酒店 </p>


<img id=11 class="IconMagnifyingGlass" alt="Search Icon"> </img>
<img id=11 class="IconMagnifyingGlass" alt="Search Icon"> </img>


<p id=12 class="text" alt="best hotel"> best hotel </p>
<p id=12 class="text" alt="best hotel"> 最好的酒店 </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=13 class="text" alt="mexico city"> mexico city </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=13 class="text" alt="mexico city"> 墨西哥城 </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=14 class="text" alt="in"> in </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=14 class="text" alt="in"> 在 </p>


<img id=15 class="IconMagnifyingGlass" alt="Search Icon"> </img>
<img id=15 class="IconMagnifyingGlass" alt="Search Icon"> </img>


<p id=16 class="text" alt="K"> K </p>
<p id=16 class="text" alt="K"> K </p>


<p id=17 class="text" alt="hotel ciudad"> hotel ciudad </p>
<p id=17 class="text" alt="hotel ciudad"> 酒店 城市 </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=18 class="text" alt="de mexico"> de mexico </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=18 class="text" alt="de mexico"> 墨西哥城的 </p>


<img id=20 class="IconVBackward" alt="Left Icon"> </img>
<img id=20 class="IconVBackward" alt="Left Icon"> </img>


<p id=19 class="text" alt="gran"> gran </p>
<p id=19 class="text" alt="gran"> 大 </p>


<img id=21 class="IconNavBarCircle" alt="Home Icon"> </img>
<img id=21 class="IconNavBarCircle" alt="Home Icon"> </img>


<img id=22 class="IconNavBarRect" alt="Overview Icon"> </img>
<img id=22 class="IconNavBarRect" alt="Overview Icon"> </img>


Instruction: What time is it in Berlin?
Instruction: What time is it in Berlin?


Answer: Let's think step by step. I see unrelated search results in the Google app,
Answer: Let's think step by step. I see unrelated search results in the Google app,


&nbsp;&nbsp;&nbsp;&nbsp;I must clear the search bar, so the action is \{"action_type": "click", "idx": 1\}
&nbsp;&nbsp;&nbsp;&nbsp;I must clear the search bar, so the action is \{"action_type": "click", "idx": 1\}


Previous Actions:
Previous Actions:


\{"step_idx": 0, "action_description": "click [DISMISS]"\}
{"step_idx": 0, "action_description": "click [DISMISS]"} 


Screen:
屏幕：


&nbsp;&nbsp;&nbsp;&nbsp;<p id=0 class="text" alt="Update your"> Update your </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=0 class="text" alt="Update your"> 更新你的 </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=1 class="text" alt="Gmail app"> Gmail app </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=1 class="text" alt="Gmail app"> Gmail 应用 </p>


<p id=2 class="text" alt="attach files from"> attach files from </p>
<p id=2 class="text" alt="attach files from"> 从这里附加文件 </p>


<p id=3 class="text" alt="To"> To </p>
<p id=3 class="text" alt="To"> 至于 </p>


<p id=4 class="text" alt="download the"> download the </p>
<p id=4 class="text" alt="download the"> 下载 </p>


<p id=5 class="text" alt="Drive,"> Drive, </p>
<p id=5 class="text" alt="Drive,"> Drive， </p>


<p id=6 class="text" alt="latest"> latest </p>
<p id=6 class="text" alt="latest"> 最新 </p>


<p id=7 class="text" alt="version"> version </p>
<p id=7 class="text" alt="version"> 版本 </p>


<p id=8 class="text" alt="of"> of </p>
<p id=8 class="text" alt="of"> 的 </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=9 class="text" alt="Gmail"> Gmail </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=9 class="text" alt="Gmail"> Gmail </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=10 class="text" alt="UPDATE"> UPDATE </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=10 class="text" alt="UPDATE"> 更新 </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=11 class="text" alt="DISMISS"> DISMISS </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=11 class="text" alt="DISMISS"> 取消 </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=12 class="text" alt="Got"> Got </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=12 class="text" alt="Got"> 已获得 </p>


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p id=13 class="text" alt="it"> it </p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p id=13 class="text" alt="it"> 它 </p>


<img id=14 class="IconVBackward" alt="Left Icon"> </img>
<img id=14 class="IconVBackward" alt="Left Icon"> </img>


Instruction: see creations saved in the google photos
指令：查看保存在 Google 照片中的创作


&nbsp;&nbsp;&nbsp;&nbsp;Answer: Let's think step by step. I see a popup, I need to open Google Photos, so
&nbsp;&nbsp;&nbsp;&nbsp;Answer: 让我们一步一步来。 我看到一个弹出窗口，我需要打开 Google 照片，所以


the action is \{"action_type": "click", "idx": 11\}
action 是 \{"action_type": "click", "idx": 11\}


Previous Actions:
先前的操作：


Screen:
屏幕：


<p id=0 class="text" alt="M"> M </p>
<p id=0 class="text" alt="M"> M </p>


<p id=1 class="text" alt="New in Gmail"> New in Gmail </p>
<p id=1 class="text" alt="New in Gmail"> Gmail 新功能 </p>


<p id=2 class="text" alt="All the features you"> All the features you </p>
<p id=2 class="text" alt="All the features you"> 所有功能你们都可用 </p>


---



---



&nbsp;&nbsp;&nbsp;&nbsp;<p id=3 class="text" alt="love with"> love with </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=3 class="text" alt="love with"> 爱上</p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=4 class="text" alt="a fresh"> a fresh </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=4 class="text" alt="a fresh"> 一处新</p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=5 class="text" alt="look"> look </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=5 class="text" alt="look"> 观看</p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=6 class="text" alt="new"> new </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=6 class="text" alt="new"> 新的</p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=7 class="text" alt="GOT IT"> GOT IT </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=7 class="text" alt="GOT IT"> 明白了</p>


Instruction: open app "Google Play services"
指令：打开应用“Google Play 服务”


&nbsp;&nbsp;&nbsp;&nbsp;Answer: Let's think step by step. I see the GMail app, I need to open the app
&nbsp;&nbsp;&nbsp;&nbsp;Answer: 让我们一步一步来。 我看到 Gmail 应用，我需要打开该应用


drawer, so the action is \{"action_type": "navigate_home"\}
抽屉， action 为 {"action_type": "navigate_home"}


Previous Actions:
上一个操作：


Screen:
屏幕：


&nbsp;&nbsp;&nbsp;&nbsp;<p id=0 class="text" alt="Tuesday, Aug"> Tuesday, Aug </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=0 class="text" alt="Tuesday, Aug"> 周二，八月 </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=1 class="text" alt="9"> 9 </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=1 class="text" alt="9"> 9 </p>


<img id=2 class="IconChat" alt="Chat Icon"> </img>
<img id=2 class="IconChat" alt="Chat Icon"> </img>


<img id=3 class="IconGoogle" alt="Google Icon"> </img>
<img id=3 class="IconGoogle" alt="Google Icon"> </img>


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Instruction: open app "Messenger Lite" (install if not already installed)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;说明：打开应用 "Messenger Lite"（若未安装请安装）


&nbsp;&nbsp;&nbsp;&nbsp;Answer: Let's think step by step. I see the home screen, I need to open the app
&nbsp;&nbsp;&nbsp;&nbsp;答案：让我们一步步来。我看到主屏幕，需要打开应用


&nbsp;&nbsp;&nbsp;&nbsp;drawer, I should swipe up, so the action is \{"action_type": "scroll", "direction":
&nbsp;&nbsp;&nbsp;&nbsp;抽屉，我应该向上滑动，因此操作为 {"action_type": "scroll", "direction":


&nbsp;&nbsp;&nbsp;&nbsp;"down"\}
&nbsp;&nbsp;&nbsp;&nbsp;"down"}


Previous Actions:
上一个操作：


\{"step_idx": 0, "action_description": "scroll down"\}
\{"step_idx": 0, "action_description": "向下滚动"\}


Screen:
屏幕：


<img id=0 class="IconThreeDots" alt="More Icon"> </img>
<img id=0 class="IconThreeDots" alt="More Icon"> </img>


<p id=1 class="text" alt="Search your phone and more"> Search your phone and more </p>
<p id=1 class="text" alt="Search your phone and more"> 在手机中搜索等


<p id=2 class="text" alt="M"> M </p>
 M 


<p id=3 class="text" alt="0"> 0 </p>
 0 


<img id=4 class="IconPlay" alt="Play Icon"> </img>
<img id=4 class="IconPlay" alt="Play Icon"> </img>


<p id=5 class="text" alt="Clock"> Clock </p>
 Clock 


&nbsp;&nbsp;&nbsp;&nbsp;<p id=6 class="text" alt="YouTube"> YouTube </p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;YouTube 


&nbsp;&nbsp;&nbsp;&nbsp;<p id=7 class="text" alt="Photos"> Photos </p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Photos 


&nbsp;&nbsp;&nbsp;&nbsp;<p id=8 class="text" alt="Gmail"> Gmail </p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gmail 


&nbsp;&nbsp;&nbsp;&nbsp;<p id=9 class="text" alt="All apps"> All apps </p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;All apps 


&nbsp;&nbsp;&nbsp;&nbsp;<p id=10 class="text" alt="g"> g </p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;g 


<p id=11 class="text" alt="0"> 0 </p>
 0 


<img id=12 class="IconTakePhoto" alt="Camera Icon"> </img>
<img id=12 class="IconTakePhoto" alt="Camera Icon"> </img>


<p id=13 class="text" alt="10"> 10 </p>
 10 


&nbsp;&nbsp;&nbsp;&nbsp;<p id=14 class="text" alt="Calendar"> Calendar </p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Calendar 


&nbsp;&nbsp;&nbsp;&nbsp;<p id=15 class="text" alt="Camera"> Camera </p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Camera 


&nbsp;&nbsp;&nbsp;&nbsp;<p id=16 class="text" alt="Chrome"> Chrome </p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Chrome 


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p id=17 class="text" alt="Clock"> Clock </p>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Clock 


&nbsp;&nbsp;&nbsp;&nbsp;<p id=18 class="text" alt="0"> 0 </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=18 class="text" alt="0"> 0 </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=19 class="text" alt="M"> M </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=19 class="text" alt="M"> M </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=20 class="text" alt="B"> B </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=20 class="text" alt="B"> B </p>


<img id=21 class="IconPerson" alt="Person Icon"> </img>
<img id=21 class="IconPerson" alt="Person Icon"> </img>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=22 class="text" alt="Gmail"> Gmail </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=22 class="text" alt="Gmail"> Gmail </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=23 class="text" alt="Drive"> Drive </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=23 class="text" alt="Drive"> Drive </p>


<p id=24 class="text" alt="Files"> Files </p>
<p id=24 class="text" alt="Files"> 文件 </p>


<p id=25 class="text" alt="Contacts"> Contacts </p>
<p id=25 class="text" alt="Contacts"> 联系人 </p>


<img id=27 class="IconGoogle" alt="Google Icon"> </img>
<img id=27 class="IconGoogle" alt="Google Icon"> </img>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=26 class="text" alt="G 00"> G 00 </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=26 class="text" alt="G 00"> G 00 </p>


<img id=28 class="IconLocation" alt="Location Icon"> </img>
<img id=28 class="IconLocation" alt="Location Icon"> </img>


<img id=29 class="IconCall" alt="Phone Icon"> </img>
<img id=29 class="IconCall" alt="Phone Icon"> </img>


<img id=30 class="IconChat" alt="Chat Icon"> </img>
<img id=30 class="IconChat" alt="Chat Icon"> </img>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=31 class="text" alt="Google"> Google </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=31 class="text" alt="Google"> 谷歌 </p>


&nbsp;&nbsp;&nbsp;&nbsp;<p id=32 class="text" alt="Maps"> Maps </p>
&nbsp;&nbsp;&nbsp;&nbsp;<p id=32 class="text" alt="Maps"> 地图 </p>


Instruction: Search for hotels in Chicago.
Instruction: Search for hotels in Chicago.


&nbsp;&nbsp;&nbsp;&nbsp;Answer: Let's think step by step. I see the app drawer, I need to search, so the
&nbsp;&nbsp;&nbsp;&nbsp;答案：让我们一步步来思考。我看到应用抽屉，我需要搜索，所以


action is \{"action_type": "click", "idx": 27\}
动作是 \{"action_type": "click", "idx": 27\}


---



Previous Actions:
先前的操作：


<HISTORY>



Screen:
屏幕：


<SCREEN_REPRESENTATION>



Instruction: <GROUNDING_GOAL>
指令： <GROUNDING_GOAL>


Answer: Let's think step by step. I see
答案：让我们一步步来思考。我看到


## Appendix C Dataset format
## 附录 C 数据集格式


Each datapoint is stored as a TFRecord file with compression type 'GZIP' with the following fields:
每个数据点以压缩类型为 'GZIP' 的 TFRecord 文件存储，字段如下：


- android_api_level: the Android API level of the emulator the episode was collected from
- android_api_level：从中收集该剧集的模拟器的 Android API 级别


- current_activity: the name of the activity running when the example was collected
- current_activity：收集样本时正在运行的活动名称


- device_type: the device type of the emulator the episode was collected from, mostly Pixel devices with one custom device image
- device_type：收集样本的模拟器设备类型，主要是 Pixel 设备，少量自定义设备镜像


- episode_id: the unique identifier for the episode the example is from
- episode_id：该样本所属剧集的唯一标识符


- episode_length: the overall number of steps in the episode
- episode_length：剧集的总步骤数


- goal_info: the natural language instruction the episode is demonstrating
- goal_info：剧集所展示的自然语言指令


- image/channels, image/height, image/width: the number of channels, height, and width of the screenshot
- image/channels、image/height、image/width：屏幕截图的通道数、高度和宽度


- image/encoded: the encoded screenshot
- image/encoded：编码后的屏幕截图


- image/ui_annotations_positions: a flattened array of coordinates representing the bounding boxes of the UI annotations; the coordinates are in (y, x, height, width) format and the length of this array is 4 * num_elements
- image/ui_annotations_positions: 一个扁平化的坐标数组，表示 UI 注解的边界框；坐标格式为 (y, x, 高度, 宽度)，数组长度为 4 * num_elements


- image/ui_annotations_text: the OCR-detected text associated with the UI element
- image/ui_annotations_text: 与 UI 元素相关联的 OCR 检测文本


- image/ui_annotations_ui_types: the type of UI element for each annotation, can be an icon or just text
- image/ui_annotations_ui_types: 对应注解的 UI 元素类型，可以是图标或仅文本


- results/action_type: the type of the predicted action (see 'Action space' for more details)
- results/action_type: 预测动作的类型（详见“行动空间”）


- results/type_action: if the action is a type then this holds the text string that was typed
- results/type_action: 如果动作是一个文本输入类型，则此处保存输入的文本字符串


- results/yx_touch, results/yx_lift: the (y, x) coordinates for the touch and lift point of a dual point action
- results/yx_touch, results/yx_lift: 双点动作的触摸与抬起点的 (y, x) 坐标


- step_id: the example's zero-indexed step number within the episode (i.e. if step_id is 2, then this is the third step of the episode)
- step_id: 示例在该剧集中的零起始步骤号（即若 step_id 为 2，则为剧集的第三步）