# SEA: Semantic Map Prediction for Active Exploration of Uncertain Areas
# SEA：用于不确定区域主动探索的语义地图预测


Hongyu Ding ${}^{1, * }$ ,Xinyue Liang ${}^{1, * }$ ,Yudong Fang ${}^{2}$ ,You Wu ${}^{1}$ ,
丁鸿宇 ${}^{1, * }$ ,梁欣悦 ${}^{1, * }$ ,方宇东 ${}^{2}$ ,吴宥 ${}^{1}$ ,


Jieqi ${\mathrm{{Shi}}}^{2, \dagger  }$ ,Jing ${\mathrm{{Huo}}}^{1, \dagger  }$ ,Wenbin ${\mathrm{{Li}}}^{2}$ ,Jing ${\mathrm{{Wu}}}^{3}$ ,Yu-Kun ${\mathrm{{Lai}}}^{3}$ ,Yang ${\mathrm{{Gao}}}^{2}$
洁琦 ${\mathrm{{Shi}}}^{2, \dagger  }$ ,晶 ${\mathrm{{Huo}}}^{1, \dagger  }$ ,文彬 ${\mathrm{{Li}}}^{2}$ ,晶 ${\mathrm{{Wu}}}^{3}$ ,余坤 ${\mathrm{{Lai}}}^{3}$ ,杨 ${\mathrm{{Gao}}}^{2}$


${Abstract}$ - In this paper,we propose SEA,a novel approach for active robot exploration through semantic map prediction and a reinforcement learning-based hierarchical exploration policy. Unlike existing learning-based methods that rely on one-step waypoint prediction, our approach enhances the agent's long-term environmental understanding to facilitate more efficient exploration. We propose an iterative prediction-exploration framework that explicitly predicts the missing areas of the map based on current observations. The difference between the actual accumulated map and the predicted global map is then used to guide exploration. Additionally, we design a novel reward mechanism that leverages reinforcement learning to update the long-term exploration strategies, enabling us to construct an accurate semantic map within limited steps. Experimental results demonstrate that our method significantly outperforms state-of-the-art exploration strategies, achieving superior coverage ares of the global map within the same time constraints. Project page: https://robo-lavira.github.io/sea-active-exp/
${Abstract}$ - 本文提出了SEA，一种通过语义地图预测和基于强化学习的分层探索策略实现主动机器人探索的新方法。不同于依赖一步航点预测的现有学习方法，我们的方法增强了智能体对环境的长期理解，从而促进更高效的探索。我们提出了一个迭代的预测-探索框架，基于当前观测显式预测地图中缺失区域。实际累积地图与预测全局地图之间的差异用于指导探索。此外，我们设计了一种新颖的奖励机制，利用强化学习更新长期探索策略，使我们能够在有限步数内构建准确的语义地图。实验结果表明，我们的方法显著优于最先进的探索策略，在相同时间限制内实现了更优的全局地图覆盖率。项目页面：https://robo-lavira.github.io/sea-active-exp/


## I. INTRODUCTION
## 一、引言


With the advancement of embodied agents, research on active exploration in unknown environments has grown rapidly. Exploration tasks in embodied intelligence differ in focus, including discovering and cataloging objects [1], [2], locating specific targets [3]-[5], maximizing explored area within limited time [6]-[8], and achieving accurate environment reconstruction.
随着具身智能体的发展，未知环境中的主动探索研究迅速增长。具身智能中的探索任务侧重点不同，包括发现和分类物体[1]，[2]，定位特定目标[3]-[5]，在有限时间内最大化探索区域[6]-[8]，以及实现精确的环境重建。


Traditional exploration algorithms rely on hand-crafted optimization methods to maximize coverage. Graph-based methods [9] are precise but inflexible, often falling into local optima, while sampling-based methods [10]-[12] handle high-dimensional spaces but cannot guarantee optimality. These approaches generally face a trade-off between global awareness and exploration efficiency. To overcome this, reinforcement learning (RL) and deep learning (DL) have been introduced, leveraging neural networks' implicit long-term memory to infer global structure [13]. However, studies on spatial intelligence reveal that neural networks often fail to capture topology and lack robust 3D perception [14], limiting their reliability in complex scenes.
传统探索算法依赖手工设计的优化方法以最大化覆盖率。基于图的方法[9]精确但不灵活，常陷入局部最优；采样方法[10]-[12]能处理高维空间但无法保证最优性。这些方法通常在全局感知和探索效率之间存在权衡。为克服此问题，引入了强化学习（RL）和深度学习（DL），利用神经网络的隐式长期记忆推断全局结构[13]。然而，空间智能研究表明，神经网络常难以捕捉拓扑结构且缺乏稳健的三维感知[14]，限制了其在复杂场景中的可靠性。


<!-- Media -->



<!-- figureText: How to establish a large Global Semantic Map Mapper \\\$’xploration Exploring Areas of Uncertainty semantic map in limited time? observation action Unknown Scene -->



<img src="https://cdn.noedgeai.com/bo_d3ve1ejef24c73d2hujg_0.jpg?x=918&y=521&w=713&h=412&r=0"/>



Fig. 1: Constructing a global semantic map within limited time steps, with the semantic map guiding active exploration. In our framework, we encourage the agent to take action to explore the uncertain areas on the semantic map to enhance exploration efficiency.
图1：在有限时间步内构建全局语义地图，语义地图引导主动探索。在我们的框架中，鼓励智能体采取行动探索语义地图上的不确定区域，以提升探索效率。


<!-- Media -->



To address these limitations, we integrate the adaptive planning capabilities of RL with explicit spatial reasoning to achieve more efficient and flexible exploration. Inspired by vision-and-language navigation (VLN), where semantic maps improve memory [15], [16], we argue that learning-based exploration should return to its core goal: actively and comprehensively covering unknown environments while building complete and accurate maps for decision-making.
为解决这些限制，我们将强化学习的自适应规划能力与显式空间推理相结合，实现更高效灵活的探索。受视觉与语言导航（VLN）启发，语义地图提升记忆能力[15]，[16]，我们认为基于学习的探索应回归其核心目标：主动且全面覆盖未知环境，同时构建完整准确的地图以辅助决策。


The processes of explicit semantic map construction and unexplored area exploration operate in an iterative manner. The semantic map serves both as a structured representation of the environment and a form of spatial memory. It enables the agent to distinguish between explored and unexplored regions while continuously refining its action strategies for maximum efficiency. Simultaneously, the exploration process actively refines the map, ultimately ensuring the most rapid and comprehensive coverage of the environment. This interplay fosters an exploration strategy that balances exploration, which encourages enlarging the map, and correction, which encourages adding observations to refine uncertain areas.
显式语义地图构建与未探索区域探索过程以迭代方式进行。语义地图既是环境的结构化表示，也是空间记忆形式，使智能体区分已探索与未探索区域，同时持续优化行动策略以实现最高效率。探索过程则主动完善地图，最终确保环境覆盖的速度和全面性。此交互促进了一种平衡探索（扩大地图）与校正（通过观测完善不确定区域）的探索策略。


To achieve this, we train a prediction-based completion network that estimates complete maps from local observations, guided by prior knowledge of room layouts, object semantics, and shapes. A confidence-aware fusion module integrates observations into a global map, where discrepancies between predicted and accumulated maps highlight unexplored regions. By converting these unknown areas into a probabilistic map, we constrain the exploration policy to uncertain regions lacking prior knowledge, which are also areas we regard as more valuable and worth exploring. A tailored reward function further encourages accurate semantic coverage, improving waypoint selection and overall efficiency.
为此，我们训练了基于预测的补全网络，利用房间布局、物体语义和形状的先验知识，从局部观测估计完整地图。一个置信度感知融合模块将观测整合进全局地图，预测与累积地图的差异突出未探索区域。通过将这些未知区域转化为概率地图，我们将探索策略限制在缺乏先验知识的不确定区域，这些区域也被视为更有价值且值得探索。定制的奖励函数进一步鼓励准确的语义覆盖，提升航点选择和整体效率。


---

<!-- Footnote -->



${}^{ * }$ Equal Contribution, ${}^{ \dagger  }$ Corresponding Author
${}^{ * }$ 贡献相等，${}^{ \dagger  }$ 通讯作者


${}^{1}$ Hongyu Ding,Xinyue Liang,You Wu and Jing Huo are with the School of Computer Science, Nanjing University, China. Emails: \{hongyuding, MF21330051, you\}@smail.nju.edu.cn, huojing@nju.edu.cn
${}^{1}$ 丁鸿宇、梁欣悦、吴宥和霍晶均来自南京大学计算机科学学院，中国。邮箱：\{hongyuding, MF21330051, you\}@smail.nju.edu.cn，huojing@nju.edu.cn


${}^{2}$ Yudong Fang,Jieqi Shi,Wenbin Li and Yang Gao are with the School of Intelligence Science and Technology, Nanjing University, China. Emails: 231880023@smail.nju.edu.cn, \{isjieqi, liwenbin, gaoy\}@nju.edu.cn
${}^{2}$方宇东、石洁琦、李文彬和高扬均来自南京大学智能科学与技术学院，中国。电子邮件：231880023@smail.nju.edu.cn，\{isjieqi, liwenbin, gaoy\}@nju.edu.cn


${}^{3}$ Jing Wu and Yu-Kun Lai are with Cardiff University,United Kingdom. Emails: \{WuJ11, LaiY4\}@cardiff.ac.uk
${}^{3}$吴静和赖宇坤来自英国卡迪夫大学。电子邮件：\{WuJ11, LaiY4\}@cardiff.ac.uk


<!-- Footnote -->

---



In summary, we propose SEA, a semantic-guided exploration framework that focuses on learning-based exploration and presents four key contributions: 1)We propose a prediction-based semantic map completion method to identify unexplored areas and guide long-term exploration strategies for faster map coverage. 2)We design a reward function that optimizes exploration waypoint selection by encouraging agents to prioritize uncertain regions. 3) Our approach efficiently explores complex environments while simultaneously generating an accurate global semantic map, enabling better navigation in unknown areas. 4)The proposed method outperforms state-of-the-art DRL-based exploration approaches on the Habitat dataset, demonstrating superior exploration efficiency and semantic map accuracy.
总之，我们提出了SEA，一种语义引导的探索框架，专注于基于学习的探索，并提出了四个关键贡献：1）我们提出了一种基于预测的语义地图补全方法，用于识别未探索区域并指导长期探索策略，以加快地图覆盖速度。2）我们设计了一个奖励函数，通过鼓励智能体优先考虑不确定区域来优化探索路径点选择。3）我们的方法高效地探索复杂环境，同时生成准确的全局语义地图，从而实现对未知区域的更好导航。4）所提方法在Habitat数据集上优于最先进的基于深度强化学习（DRL）的探索方法，展示了更优的探索效率和语义地图准确性。


## II. RELATED WORK
## 二、相关工作


## A. Semantic Scene Completion
## A. 语义场景补全


A concept closely related to our work is Semantic Scene Completion (SSC), which aims to infer the complete geometry and semantics of a scene from sparse inputs like depth maps or monocular RGB images. Early work primarily focused on object-level completion, reconstructing full geometry from limited 2D/3D observations [17], [18]. With the rise of deep learning, research has shifted toward completing full semantic scenes by jointly predicting semantics and geometry [19]-[21]. However, a key distinction from our approach is that most SSC methods reconstruct only the current visible scene in dense 3D space, without predicting unseen areas beyond the field of view.
与我们工作密切相关的一个概念是语义场景补全（Semantic Scene Completion, SSC），其目标是从稀疏输入如深度图或单目RGB图像推断场景的完整几何和语义。早期工作主要关注对象级补全，从有限的二维/三维观测重建完整几何[17]，[18]。随着深度学习的发展，研究转向通过联合预测语义和几何来完成完整语义场景[19]-[21]。然而，与我们方法的一个关键区别是，大多数SSC方法仅重建当前可见场景的密集三维空间，而不预测视野之外的未知区域。


Several studies combine completion with reconstruction and exploration [22]-[24]. For instance, PredRecon [22] applies object-level completion to guide UAV waypoint selection for reconstruction, yet it focuses on object geometry rather than scene-level exploration. The heuristic-based method in [24] identifies unknown frontiers and applies rule-based strategies to guide exploration. Although it integrates unknown-area prediction, it only distinguishes between explored and unexplored regions and lacks perceptual priors for active exploration.
一些研究将补全与重建和探索结合起来[22]-[24]。例如，PredRecon[22]应用对象级补全指导无人机航点选择以进行重建，但其关注点是对象几何而非场景级探索。[24]中的启发式方法识别未知边界并应用基于规则的策略指导探索。尽管它集成了未知区域预测，但仅区分已探索和未探索区域，缺乏用于主动探索的感知先验。


## B. Occupancy Mapping and Semantic Mapping
## B. 占用映射与语义映射


The objective of mapping is to construct a structured representation of an environment. Depending on whether semantic information is considered, this can be categorized into occupancy and semantic mapping. In occupancy mapping, Chaplot et al. introduced an active navigation framework that projects egocentric views to generate obstacle maps [6]. Other works construct obstacle maps [2], [25]-[27] or build topological maps capturing spatial relations among rooms and objects [28]-[34].
映射的目标是构建环境的结构化表示。根据是否考虑语义信息，可分为占用映射和语义映射。在占用映射中，Chaplot等人提出了一个主动导航框架，将自我中心视角投影生成障碍物地图[6]。其他工作构建障碍物地图[2]，[25]-[27]或构建捕捉房间与物体空间关系的拓扑地图[28]-[34]。


While some works [35], [36] attempt to enhance environmental observations by optimizing camera rotation for next-best-view selection, they do not guide the agent toward active exploration for improved semantic mapping. Similarly, studies on optimizing exploration perspectives for point cloud reconstruction [37]-[40] adopt a next-best-view approach but are constrained by the high storage cost and computational resources required for $3\mathrm{D}$ point clouds. As a result, many semantic mapping approaches struggle to integrate with autonomous exploration tasks.
虽然一些工作[35]，[36]尝试通过优化摄像机旋转选择最佳视角以增强环境观测，但它们并未引导智能体进行主动探索以提升语义映射。同样，针对点云重建优化探索视角的研究[37]-[40]采用了下一最佳视角方法，但受限于点云存储成本和计算资源的高昂。因此，许多语义映射方法难以与自主探索任务结合。


## C. Active Exploration
## C. 主动探索


Active exploration aims to enlarge the explored area or discover more objects in an environment using limited steps. Our proposed task falls into this category but places a distinct emphasis on combining exploration with semantic map construction, requiring more explicit environmental modeling.
主动探索旨在利用有限步数扩大已探索区域或发现更多环境中的物体。我们提出的任务属于此类，但特别强调将探索与语义地图构建相结合，要求更明确的环境建模。


Most existing exploration approaches [1], [2], [6]-[8], [26], [41] focus on maximizing coverage or object diversity but lack explicit semantic reasoning. The works most related to ours are SSCNav [42] and L2M [43], which leverage predicted semantic local maps and uncertainty estimation. However, they are limited to object-goal navigation. SSCNav discards local maps at each step, preventing global map accumulation, and its confidence maps are used to correct errors rather than guide exploration in uncertain regions. L2M also exploits uncertainty, but only for specific object categories relevant to its navigation task, without extending to global semantic completion.
大多数现有探索方法[1]，[2]，[6]-[8]，[26]，[41]侧重于最大化覆盖率或物体多样性，但缺乏明确的语义推理。与我们工作最相关的是SSCNav[42]和L2M[43]，它们利用预测的语义局部地图和不确定性估计。然而，它们仅限于对象目标导航。SSCNav在每步丢弃局部地图，阻碍了全局地图的累积，其置信度地图用于纠正错误而非引导不确定区域的探索。L2M也利用不确定性，但仅针对其导航任务相关的特定对象类别，未扩展到全局语义补全。


The recent work GLEAM [44] focuses on large-scale training and cross-dataset policy generalization. Its focus on scalability is complementary to ours; while GLEAM advances generalizable exploration, our method addresses the critical need for semantic uncertainty and completion to construct high-fidelity maps under a limited budget. To our knowledge, no prior work simultaneously explores unseen indoor environments and constructs predictive semantic maps to guide the process. We argue that semantic reasoning, fundamental to how humans explore, is a highly promising direction for enhancing both efficiency and scene understanding in embodied AI.
近期工作GLEAM[44]聚焦于大规模训练和跨数据集策略泛化。其对可扩展性的关注与我们互补；GLEAM推进了可泛化探索，而我们的方法解决了在有限预算下构建高保真地图所需的语义不确定性和补全的关键需求。据我们所知，尚无先前工作能同时探索未知室内环境并构建预测性语义地图以指导过程。我们认为，语义推理是人类探索的基础，是提升具身人工智能效率和场景理解的极具潜力的方向。


## III. APPROACH
## 三、方法


Our goal is to actively explore the environment while constructing an accurate semantic map within a limited number of steps. To achieve this, we propose a framework consisting of three key modules that facilitate both exploration and mapping. The overall approach is illustrated in Figure 2, Specifically, we introduce three major components: (i) ASC-based Local Mapper that performs local map prediction and confidence estimation. (ii) Two-stage Navigator that decouples the exploration task into long-term and short-term policies, allowing for more efficient and structured exploration. (iii) Confidence-aware Full Mapper that accumulates local maps while incorporating confidence-based adjustments.
我们的目标是在有限步数内主动探索环境，同时构建准确的语义地图。为此，我们提出了一个由三个关键模块组成的框架，以促进探索和地图构建。整体方法如图2所示，具体包括三个主要组件：(i) 基于ASC的局部映射器，用于局部地图预测和置信度估计。(ii) 两阶段导航器，将探索任务解耦为长期和短期策略，实现更高效且结构化的探索。(iii) 置信度感知全局映射器，在累积局部地图的同时结合基于置信度的调整。


<!-- Media -->



<!-- figureText: Forward / Turn _ow-Level Waypoint Reward Function Map Coverage Full Projected Full Completed Semantic Confidence Map ${M}_{proj}$ Map ${M}_{cmplt}$ Completion Network Confidence Local Completed Local Confidence Map ${m}_{cmplt}$ Map ${m}_{conf}$ action interaction ${obs}$ RGB Semantic Observation Segmentation proj. Local Projected Map ${m}_{proj}$ Depth image Robot Pose -->



<img src="https://cdn.noedgeai.com/bo_d3ve1ejef24c73d2hujg_2.jpg?x=253&y=134&w=1284&h=818&r=0"/>



Fig. 2: Overview of SEA. After obtaining the egocentric observations of RGB-D image and the ground truth position from Habitat simulator as the input, we employ panoramic semantic segmentation and project the egocentric observations onto the 2D plane to build a local map. The projected local map is then processed by our Local Mapper and Full Mapper modules to generate local completed and confidence maps, which are subsequently accumulated into full projected and full completed maps. Besides, RL Navigator takes the local maps to select the long-term goal to explore, which is then passed to the short-term policy module to make the decision of the next action.
图2：SEA概览。在从Habitat模拟器获取RGB-D图像的自我中心观察和真实位置作为输入后，我们采用全景语义分割并将自我中心观察投影到二维平面以构建局部地图。投影的局部地图随后由我们的局部映射器和全局映射器模块处理，生成局部完成地图和置信度地图，随后累积成全局投影地图和全局完成地图。此外，强化学习导航器利用局部地图选择长期探索目标，随后传递给短期策略模块以决定下一步动作。


<!-- Media -->



## A. Task Definition
## A. 任务定义


In an unknown environment, the agent begins at a random position, actively exploring the scene while building a semantic map. At each timestep $t$ ,the agent acquires RGB-D observation ${o}_{t}$ and an accurate pose ${p}_{t}$ from the environment. The agent is supposed to learn an exploration policy $\pi \left( {{a}_{t} \mid  {o}_{t},{p}_{t}}\right)$ which aims to explore as rapid as possible while constructing an accurate semantic map by executing action ${a}_{t} \in  \{$ MoveForward,RotateRight,Right,RotateLeft $\}$ within limited $k$ timesteps.
在未知环境中，智能体从随机位置开始，主动探索场景并构建语义地图。在每个时间步$t$，智能体从环境中获取RGB-D观察${o}_{t}$和准确的位姿${p}_{t}$。智能体应学习一种探索策略$\pi \left( {{a}_{t} \mid  {o}_{t},{p}_{t}}\right)$，旨在通过执行动作${a}_{t} \in  \{$ MoveForward,RotateRight,Right,RotateLeft $\}$，在有限的$k$时间步内尽可能快速地探索，同时构建准确的语义地图。


## B. ASC-based Local Mapper
## B. 基于ASC的局部映射器


Building a complete map to represent the global environment requires fusing information from egocentric views. Existing DRL-based approaches [45] directly accumulate maps by projecting only the areas visible to the agent, leading to inefficient mapping. We highlight that, with prior knowledge of indoor scenes, unseen areas can be predicted and completed, enhancing the semantic map with broader and more accurate coverage.
构建完整地图以表示全局环境需要融合自我中心视角的信息。现有基于深度强化学习（DRL）的方案[45]仅通过投影智能体可见区域直接累积地图，导致映射效率低下。我们强调，利用室内场景的先验知识，可以预测和补全未见区域，从而增强语义地图的覆盖范围和准确性。


To address this, we propose the ASC-based Local Mapper, which projects local maps from egocentric observations and fills unseen areas with predicted semantics. To improve exploration efficiency, the agent is encouraged to explore regions with relatively inaccurate semantics. Additionally, the Local Mapper estimates semantic confidence and generates a confidence map to guide future exploration.
为此，我们提出了基于ASC的局部映射器，该模块从自我中心观察中投影局部地图，并用预测的语义信息填充未见区域。为提高探索效率，鼓励智能体探索语义相对不准确的区域。此外，局部映射器估计语义置信度并生成置信度地图，以指导后续探索。


1) Panoramic Segmented Local Map: In this module, we first perform panoramic segmentation of RGB observation according to the 40 semantic categories of the MP3D dataset. Our panoramic semantic segmentation employs off-the-shelf ACNet [46] and leverages the pretrained panoramic segmentation model from SSCNav [42] which is trained with 209,200 RGB-D images of 40 object categories from MP3D training houses. As a comparison, SemExp [4] utilizes only 16 semantic categories relevant to the Habitat Object Goal Navigation task. In contrast, our local maps include all 40 semantic categories plus an additional void category. Moreover, our method provides a panoramic semantic view, excluding unseen areas such as floors and walls.
1) 全景分割局部地图：在该模块中，我们首先根据MP3D数据集的40个语义类别对RGB观察进行全景分割。我们的全景语义分割采用现成的ACNet [46]，并利用SSCNav [42]中预训练的全景分割模型，该模型使用MP3D训练房屋中209,200张包含40个物体类别的RGB-D图像训练。相比之下，SemExp [4]仅使用与Habitat目标导航任务相关的16个语义类别。我们的局部地图则包含全部40个语义类别及一个额外的空白类别。此外，我们的方法提供全景语义视图，排除地板和墙壁等未见区域。


We combine depth observation and segmented RGB observation to compute each point in the egocentric $3\mathrm{D}$ point cloud. To project the 3D map into 2D space, we accumulate the semantic point cloud based on height and obtain the 2D semantic local projected map ${m}_{\text{proj }}$ of size $M \times  M$ . In addition,we determine the world coordinates of this $2\mathrm{D}$ local map using the ground-truth rotation and translation of the agents.
我们结合深度观察和分割后的RGB观察，计算自我中心$3\mathrm{D}$点云中的每个点。为将三维地图投影到二维空间，我们基于高度累积语义点云，获得尺寸为$M \times  M$的二维语义局部投影地图${m}_{\text{proj }}$。此外，我们利用智能体的真实旋转和平移确定该$2\mathrm{D}$局部地图的世界坐标。


We emphasize that, although we do not explore real-world data expansion in this paper, we have aimed to replicate real-world noise conditions in this section to facilitate future research. To align with the semantic noise encountered in real-world applications, we employ a segmentation model for semantic segmentation instead of using the ground-truth annotations. Regarding pose estimation, while we obtain ground-truth poses directly from the simulator, this process can be replicated in the real world using a SLAM front end. Considering that common front-end methods like VINS can achieve an accuracy of $7\mathrm{\;{cm}}$ ATE in a ${147}\mathrm{\;m}$ trajectory [47], we believe this assumption is reasonable.
我们强调，尽管本文未涉及真实世界数据扩展，但本节已尽力模拟真实世界的噪声条件，以便未来研究。为贴合真实应用中的语义噪声，我们采用语义分割模型进行语义分割，而非使用真实标注。关于位姿估计，虽然我们直接从模拟器获取真实位姿，但该过程可通过SLAM前端在真实世界中实现。考虑到常用前端方法如VINS在${147}\mathrm{\;m}$轨迹中可达到$7\mathrm{\;{cm}}$的绝对轨迹误差（ATE）[47]，我们认为该假设合理。


<!-- Media -->



<!-- figureText: ${m}_{proj}$ ${m}_{pred}$ ${\text{mask}}_{us} \cdot  {m}_{\text{pred}}$ ${m}_{cmplt}$ ${m}_{gt}$ ${\mathcal{L}}_{CE}$ ${\text{mask}}_{us}$ -->



<img src="https://cdn.noedgeai.com/bo_d3ve1ejef24c73d2hujg_3.jpg?x=240&y=138&w=1338&h=475&r=0"/>



Fig. 3: The approach for building a local semantic completed map. We pass the local projected map ${m}_{proj}$ through the semantic completed network to get the semantic prediction map ${m}_{\text{pred }}$ ,and extract the unseen mask ${\operatorname{mask}}_{us}$ from the unseen part. As shown in Equation 1,we fill the unseen part of ${m}_{proj}$ with that in ${m}_{pred}$ to form ${m}_{cmplt}$ .
图3：构建局部语义完整地图的方法。我们将局部投影地图${m}_{proj}$输入语义完成网络，得到语义预测地图${m}_{\text{pred }}$，并从未见区域提取未见掩码${\operatorname{mask}}_{us}$。如公式1所示，我们用${m}_{pred}$中的内容填充${m}_{proj}$的未见部分，形成${m}_{cmplt}$。


<!-- Media -->



2) Local Map Completion: By leveraging prior knowledge of indoor scenes and objects, unseen areas can be predicted and completed based on the projected map, expanding accurate semantic coverage within a limited number of steps and improving exploration efficiency. In our framework, we generate a completed semantic local map, covering the entire local map and filling unknown regions with predicted semantics. Unlike SSCNav's completion method, our approach maintains consistency between observed and predicted areas during training.
2) 局部地图补全：通过利用室内场景和物体的先验知识，可以基于投影地图预测并补全未见区域，在有限步数内扩展准确的语义覆盖，提升探索效率。在我们的框架中，生成了一个完整的语义局部地图，覆盖整个局部地图并用预测的语义填充未知区域。与SSCNav的补全方法不同，我们的方法在训练过程中保持观测区域与预测区域的一致性。


As illustrated in Figure 3, we calculate the unseen area in ${m}_{\text{proj }}$ to obtain a mask ${\text{ mask }}_{us} \in  \{ 0,1{\} }^{M \times  M}$ . We exploit ResNet with a fully convolutional neural network with 1 maxpooling layer, 4 down-sample residual blocks, and 5 up-sample residual blocks, following the same architecture in SSCNav [48]. Differently, we use only one observed projected map ${m}_{proj}$ to get the predicted map ${m}_{pred}$ of the same size,and fill the unseen part of ${m}_{proj}$ with that in ${m}_{\text{pred }}$ to form ${m}_{\text{cmplt }}$ for consistency. The loss is calculated as the Cross Entropy Loss between ${m}_{\text{cmplt }}$ and ${m}_{gt}$ .
如图3所示，我们计算${m}_{\text{proj }}$中的未见区域以获得掩码${\text{ mask }}_{us} \in  \{ 0,1{\} }^{M \times  M}$。我们采用基于ResNet的全卷积神经网络，包含1个最大池化层、4个下采样残差块和5个上采样残差块，架构与SSCNav [48]相同。不同的是，我们仅使用一个观测投影地图${m}_{proj}$来获得同尺寸的预测地图${m}_{pred}$，并用${m}_{\text{pred }}$中的内容填充${m}_{proj}$的未见部分，形成${m}_{\text{cmplt }}$以保持一致性。损失函数为${m}_{\text{cmplt }}$与${m}_{gt}$之间的交叉熵损失。


$$
{m}_{\text{cmplt }} = \left( {1 - {\text{ mask }}_{us}}\right)  \odot  {m}_{\text{proj }} + {\text{ mask }}_{us} \odot  {m}_{\text{pred }} \tag{1}
$$



3) Local Map Confidence Estimation: If the predicted area of ${m}_{\text{cmplt }}$ lacks accuracy,semantic noise may introduce errors into the projected map, negatively affecting subsequent exploration. Since our goal is to maximize Accurate Semantic Coverage (ASC) within a limited number of steps, repeatedly exploring areas where the completed semantic map is already accurate or revisiting previously explored locations would be inefficient, limiting the expansion of full scene coverage.
3) 局部地图置信度估计：如果${m}_{\text{cmplt }}$的预测区域不准确，语义噪声可能会引入误差，影响投影地图，进而负面影响后续探索。由于我们的目标是在有限步数内最大化准确语义覆盖（ASC），反复探索已准确完成的语义地图区域或重复访问先前探索位置效率低下，限制了全场景覆盖的扩展。


To address this, we introduce a novel method that prioritizes areas where the completed semantic map is less accurate. We select such maps as high-potential regions for the next long-term exploration target. The agent is then guided by this target to explore these areas efficiently. In summary, since inaccurate regions require further exploration, we predict a semantic confidence map ${m}_{\text{conf }} \in  {\left\lbrack  0,1\right\rbrack  }^{M \times  M}$ for ${m}_{\text{cmplt }}$ to assess confidence levels and guide exploration.
为此，我们提出一种新方法，优先考虑语义完成地图准确度较低的区域。我们将这些区域选为下一阶段长期探索的高潜力目标，引导智能体高效探索。总之，由于不准确区域需要进一步探索，我们预测语义置信度地图${m}_{\text{conf }} \in  {\left\lbrack  0,1\right\rbrack  }^{M \times  M}$以评估置信度并指导探索。


We compare ${m}_{\text{cmplt }}$ and the ground truth local map ${m}_{\text{cmplt }}$ to generate a semantic accuracy map ${m}_{\text{acc }} \in$ $\{ 0,1{\} }^{M \times  M}$ ,which serves as supervision for the predicted confidence map. Using the same ResNet-based architecture as the completion network,we take ${m}_{\text{pred }}$ as the input during training and output ${m}_{\text{conf }}$ . We only apply the Cross Entropy Loss of the unseen part between prediction map for optimization. During navigation, repeated exploration can be avoided by using ${m}_{\text{conf }}$ as a guidance. With this approach, the RL Navigator based on ${m}_{\text{conf }}$ can efficiently construct a more accurate semantic map within a limited number of steps.
我们将${m}_{\text{cmplt }}$与真实局部地图${m}_{\text{cmplt }}$对比，生成语义准确度地图${m}_{\text{acc }} \in$$\{ 0,1{\} }^{M \times  M}$，作为预测置信度地图的监督。采用与补全网络相同的基于ResNet的架构，训练时以${m}_{\text{pred }}$为输入，输出${m}_{\text{conf }}$。仅对未见部分的预测图应用交叉熵损失进行优化。导航过程中，利用${m}_{\text{conf }}$作为指导可避免重复探索。通过此方法，基于${m}_{\text{conf }}$的强化学习导航器能在有限步数内高效构建更准确的语义地图。


## C. Two-Stage Navigator
## C. 两阶段导航器


We divide the exploration task into two distinct stages. First, we select a long-term goal to guide the agent's navigation over the next $\mu$ steps. Then,we apply short-term policies for path planning and action selection, ensuring efficient movement toward the goal.
我们将探索任务划分为两个不同阶段。首先，选择一个长期目标，引导智能体在接下来的$\mu$步内导航。然后，应用短期策略进行路径规划和动作选择，确保高效朝目标移动。


1) Confidence-aware Long-term Policy: Existing approaches [1], [6] select long-term goals across the entire map. However, our experiments show that these methods often select goals near map corners, limiting their ability to utilize environmental information effectively. This inefficiency negatively impacts the overall exploration process.
1) 置信度感知的长期策略：现有方法[1]，[6]在整个地图上选择长期目标。然而，我们的实验表明，这些方法常选取地图角落附近的目标，限制了环境信息的有效利用，降低了整体探索效率。


To address these limitations, we propose leveraging the Soft Actor-Critic (SAC) algorithm [49], an off-policy, model-free reinforcement learning method that enables more effective long-term goal selection. Within a single long-term goal period, the agent's reachable distance remains limited to the $M \times  M$ local map. This allows us to select a new long-term goal based on the current ${m}_{\text{proj }},{m}_{\text{cmplt }}$ and ${m}_{\text{conf }}$ . The interval for long-term goal planning is set to $\mu  = {25}$ . The outputs of the Local Mapper,including ${m}_{\text{proj }},{m}_{\text{cmplt }}$ and ${m}_{\text{conf }}$ ,are fed into the SAC module for exploration. Additionally, the embeddings of the agent's current position and orientation are included as inputs.
为了解决这些限制，我们提出利用软演员-评论家（Soft Actor-Critic，SAC）算法[49]，这是一种离策略、无模型的强化学习方法，能够更有效地选择长期目标。在单个长期目标周期内，智能体的可达距离仍然局限于$M \times  M$局部地图。这使我们能够基于当前的${m}_{\text{proj }},{m}_{\text{cmplt }}$和${m}_{\text{conf }}$选择新的长期目标。长期目标规划的时间间隔设定为$\mu  = {25}$。局部映射器的输出，包括${m}_{\text{proj }},{m}_{\text{cmplt }}$和${m}_{\text{conf }}$，被输入到SAC模块进行探索。此外，还将智能体当前位置和朝向的嵌入作为输入。


Meanwhile, the Confidence-Aware Full Mapper (Subsection III.D) updates the full projected map. The differences between the coverage ${\Delta }_{\text{cover }}$ of the full projected maps is computed to determine the coverage reward ${r}_{\text{cover }}$ . Since the predicted map area can theoretically expand indefinitely, maximizing its coverage alone is not meaningful. Therefore, we use the variance in Accurate Semantic Coverage (ASC) between consecutive steps of the completed map to compute the completion reward ${r}_{asc}$ . Besides,we calculate the confidence of the long-term goal ${r}_{\text{conf }}$ on ${m}_{\text{conf }}$ .
同时，置信感知全局映射器（见III.D小节）更新完整的投影地图。通过计算完整投影地图覆盖范围${\Delta }_{\text{cover }}$的差异来确定覆盖奖励${r}_{\text{cover }}$。由于预测地图区域理论上可以无限扩展，仅最大化其覆盖率没有意义。因此，我们使用已完成地图连续步骤间准确语义覆盖（Accurate Semantic Coverage，ASC）的方差来计算完成奖励${r}_{asc}$。此外，我们还计算长期目标${r}_{\text{conf }}$在${m}_{\text{conf }}$上的置信度。


At time $t$ ,we define the following reward ${r}_{t}$ in reinforcement learning training,where ${\eta }_{c},{\eta }_{asc}$ and ${\eta }_{\text{conf }}$ are hyper parameters that balance different loss terms,
在时间点$t$，我们在强化学习训练中定义以下奖励${r}_{t}$，其中${\eta }_{c},{\eta }_{asc}$和${\eta }_{\text{conf }}$是平衡不同损失项的超参数，


$$
{r}_{t} = {r}_{\text{cover }} + {r}_{\text{asc }} + {r}_{\text{conf }} \tag{2}
$$



$$
 = {\eta }_{c}{\Delta }_{c} + {\eta }_{asc}{\Delta }_{asc} + {\eta }_{conf}\operatorname{con}{f}_{t}
$$



where ${r}_{\text{cover }}$ is the change in projected map coverage $\left( {\Delta }_{c}\right)$ , ${r}_{asc}$ is the change in Accurate Semantic Coverage of the completed map $\left( {\Delta }_{asc}\right)$ ,and ${r}_{conf}$ is the confidence value of the chosen goal. The hyperparameters ${\eta }_{c},{\eta }_{asc}$ ,and ${\eta }_{\text{conf }}$ balance these terms.
其中${r}_{\text{cover }}$是投影地图覆盖率变化$\left( {\Delta }_{c}\right)$，${r}_{asc}$是已完成地图准确语义覆盖变化$\left( {\Delta }_{asc}\right)$，${r}_{conf}$是所选目标的置信值。超参数${\eta }_{c},{\eta }_{asc}$和${\eta }_{\text{conf }}$用于平衡这些项。


2) Short-term Policy: For the short-term policy, a deterministic path planning method, Fast Marching Method (FMM) [50],is used. Based on ${m}_{proj}$ ,FMM plans a path toward the selected long-term goal and determines the agent's next action. It maintains an obstacle local map, allowing the agent to navigate efficiently while avoiding obstacles during exploration.
2）短期策略：对于短期策略，采用确定性路径规划方法快速行进法（Fast Marching Method，FMM）[50]。基于${m}_{proj}$，FMM规划通向选定长期目标的路径，并确定智能体的下一步动作。它维护一个障碍物局部地图，使智能体在探索过程中能够高效导航并避开障碍物。


## D. Confidence-aware Full Mapper
## D. 置信感知全局映射器


Based on the output of the Local Mapper(Section III.B), we construct the full projected map ${M}_{proj}$ ,full completed map ${M}_{\text{cmplt }}$ and full confidence map ${M}_{\text{conf }}$ to obtain the semantic map of an indoor scene. Firstly,we establish ${M}_{\text{conf }}$ by accumulating local confidence map ${m}_{\text{conf }}$ according to the position. If a region overlaps with a previously explored area, we retain the higher confidence value. We define this process as confidence-aware accumulation. Simultaneously, we generate a mask for the retained parts of the current ${m}_{conf}$ ,denoted as ${\operatorname{mask}}_{conf}$ .
基于局部映射器（见III.B节）的输出，我们构建完整投影地图${M}_{proj}$、完整完成地图${M}_{\text{cmplt }}$和完整置信度地图${M}_{\text{conf }}$，以获取室内场景的语义地图。首先，我们通过根据位置累积局部置信度地图${m}_{\text{conf }}$来建立${M}_{\text{conf }}$。如果某区域与先前探索区域重叠，我们保留较高的置信度值。我们将此过程定义为置信感知累积。同时，我们为当前${m}_{conf}$中保留的部分生成掩码，记为${\operatorname{mask}}_{conf}$。


We then apply ${\operatorname{mask}}_{\text{conf }}$ to ${m}_{\text{proj }}$ and ${m}_{\text{cmplt }}$ before stitching them into ${M}_{\text{proj }}$ and ${M}_{\text{cmplt }}$ according to the world coordinates of the local maps. Finally, we evaluate the metrics using ${M}_{\text{proj }}$ and ${M}_{\text{cmplt }}$ .
然后，我们将${\operatorname{mask}}_{\text{conf }}$应用于${m}_{\text{proj }}$和${m}_{\text{cmplt }}$，并根据局部地图的世界坐标将它们拼接成${M}_{\text{proj }}$和${M}_{\text{cmplt }}$。最后，我们使用${M}_{\text{proj }}$和${M}_{\text{cmplt }}$评估指标。


<!-- Media -->



TABLE I: Results after 500 steps on MP3D datasets.
表I：MP3D数据集上500步后的结果。


<table><tr><td>$\mathbf{{Method}}$</td><td>CovP $\left( {m}^{2}\right)  \uparrow$</td><td>ASCP $\left( {m}^{2}\right)  \uparrow$</td><td>$\mathbf{{CovC}}$ $\left( {m}^{2}\right)  \uparrow$</td><td>ASCC $\left( {m}^{2}\right)  \uparrow$</td></tr><tr><td>SemExp*</td><td>96.61</td><td>36.54</td><td>101.64</td><td>40.92</td></tr><tr><td>Impact</td><td>104.53</td><td>42.19</td><td>111.53</td><td>45.14</td></tr><tr><td>EE</td><td>102.79</td><td>40.95</td><td>109.47</td><td>46.48</td></tr><tr><td>SSCNav</td><td>14.61</td><td>-</td><td>14.61</td><td>-</td></tr><tr><td>ARiADNE*</td><td>104.25</td><td>-</td><td>104.25</td><td>-</td></tr><tr><td>SEA (Ours)</td><td>111.74</td><td>49.53</td><td>117.14</td><td>53.99</td></tr></table>
<table><tbody><tr><td>$\mathbf{{Method}}$</td><td>CovP $\left( {m}^{2}\right)  \uparrow$</td><td>ASCP $\left( {m}^{2}\right)  \uparrow$</td><td>$\mathbf{{CovC}}$ $\left( {m}^{2}\right)  \uparrow$</td><td>ASCC $\left( {m}^{2}\right)  \uparrow$</td></tr><tr><td>语义表达*</td><td>96.61</td><td>36.54</td><td>101.64</td><td>40.92</td></tr><tr><td>影响</td><td>104.53</td><td>42.19</td><td>111.53</td><td>45.14</td></tr><tr><td>EE</td><td>102.79</td><td>40.95</td><td>109.47</td><td>46.48</td></tr><tr><td>SSC导航</td><td>14.61</td><td>-</td><td>14.61</td><td>-</td></tr><tr><td>ARiADNE*</td><td>104.25</td><td>-</td><td>104.25</td><td>-</td></tr><tr><td>SEA（本方法）</td><td>111.74</td><td>49.53</td><td>117.14</td><td>53.99</td></tr></tbody></table>


TABLE II: Ablation results on MP3D Val datasets.
表 II：MP3D 验证集上的消融实验结果。


<table><tr><td>$\mathbf{{Method}}$</td><td>CovP $\left( {m}^{2}\right)  \uparrow$</td><td>ASCP $\left( {m}^{2}\right)  \uparrow$</td><td>$\mathbf{{CovC}}$ $\left( {m}^{2}\right)  \uparrow$</td><td>ASCC $\left( {m}^{2}\right)  \uparrow$</td></tr><tr><td>w/o ${r}_{\text{cover }}$</td><td>104.20</td><td>43.10</td><td>106.36</td><td>44.15</td></tr><tr><td>w/o ${r}_{asc}$</td><td>111.03</td><td>48.76</td><td>115.08</td><td>51.66</td></tr><tr><td>w/o ${r}_{conf}$</td><td>110.59</td><td>48.07</td><td>114.74</td><td>50.49</td></tr><tr><td>w/o ${m}_{cmplt}$</td><td>109.11</td><td>47.87</td><td>112.29</td><td>49.10</td></tr><tr><td>w/o ${m}_{conf}$</td><td>106.63</td><td>45.80</td><td>108.18</td><td>46.88</td></tr><tr><td>SEA</td><td>111.74</td><td>49.53</td><td>117.14</td><td>53.99</td></tr></table>
<table><tbody><tr><td>$\mathbf{{Method}}$</td><td>CovP $\left( {m}^{2}\right)  \uparrow$</td><td>ASCP $\left( {m}^{2}\right)  \uparrow$</td><td>$\mathbf{{CovC}}$ $\left( {m}^{2}\right)  \uparrow$</td><td>ASCC $\left( {m}^{2}\right)  \uparrow$</td></tr><tr><td>无 ${r}_{\text{cover }}$</td><td>104.20</td><td>43.10</td><td>106.36</td><td>44.15</td></tr><tr><td>无 ${r}_{asc}$</td><td>111.03</td><td>48.76</td><td>115.08</td><td>51.66</td></tr><tr><td>无 ${r}_{conf}$</td><td>110.59</td><td>48.07</td><td>114.74</td><td>50.49</td></tr><tr><td>无 ${m}_{cmplt}$</td><td>109.11</td><td>47.87</td><td>112.29</td><td>49.10</td></tr><tr><td>无 ${m}_{conf}$</td><td>106.63</td><td>45.80</td><td>108.18</td><td>46.88</td></tr><tr><td>SEA</td><td>111.74</td><td>49.53</td><td>117.14</td><td>53.99</td></tr></tbody></table>


<!-- Media -->



## IV. EXPERIMENT RESULTS
## 四、实验结果


## A. Experiment Setup
## A. 实验设置


Dataset and Simulator. We use Habitat [48] as the simulation platform and MP3D [51] as the dataset. MP3D is an RGB-D indoor dataset with 90 large-scale scenes: 61 for training, 11 for validation, and 18 for testing. Our method is trained with $1\mathrm{M}$ observation frames from the training split and evaluated with ${50}\mathrm{k}$ frames from the validation split. For semantic completion and confidence modeling, we adopt the local map completion dataset from SSCNav, which contains over ${52.3}\mathrm{k}$ pairs of ground-truth and projected maps. Ground-truth full maps ${M}_{gt}$ for evaluation are generated following SMNet [52] using MP3D semantics.
数据集与模拟器。我们使用Habitat [48]作为仿真平台，MP3D [51]作为数据集。MP3D是一个包含90个大规模场景的RGB-D室内数据集：61个用于训练，11个用于验证，18个用于测试。我们的方法在训练集的$1\mathrm{M}$观察帧上训练，并在验证集的${50}\mathrm{k}$帧上评估。对于语义补全和置信度建模，我们采用SSCNav的局部地图补全数据集，该数据集包含超过${52.3}\mathrm{k}$对真实与投影地图。用于评估的真实完整地图${M}_{gt}$按照SMNet [52]基于MP3D语义生成。


Metrics. To assess the effectiveness of semantic mapping, we define the following metrics. CovP is the area covered by ${M}_{\text{proj }}$ in ${m}^{2}$ and $\mathbf{{Cov}}\mathbf{C}$ is the coverage of ${M}_{\text{cmplt }}$ in ${m}^{2}$ . ASCP is accurate semantic coverage which is calculated as the interaction in ${m}^{2}$ between ${M}_{\text{proj }}$ and ${M}_{gt}$ while ASCC uses ${M}_{\text{cmplt }}$ .
指标。为了评估语义映射的有效性，我们定义了以下指标。CovP是${M}_{\text{proj }}$在${m}^{2}$中的覆盖面积，$\mathbf{{Cov}}\mathbf{C}$是${M}_{\text{cmplt }}$在${m}^{2}$中的覆盖率。ASCP是准确语义覆盖，计算为${m}^{2}$中${M}_{\text{proj }}$与${M}_{gt}$的交集，而ASCC则使用${M}_{\text{cmplt }}$。


Following [7], the maximum exploration step is set to 500. The local map resolution is ${128} \times  {128}$ ,and the full map is ${1280} \times  {1280}$ (corresponding to ${48} \times  {48},{m}^{2}$ in the physical world). Although insufficient for full exploration, this effectively highlights differences across strategies for semantic map construction. Long-term goals are updated every 10 steps. Empirically, ASC is about half of total coverage. We estimate ${\Delta }_{asc}$ by doubling it and compute ${\Delta }_{\text{conf }}$ by scaling ${\Delta }_{\text{asc }}$ by 1000. The final reward (Eq. 2) is calculated with hyperparameters ${\eta }_{c} = {0.6},{\eta }_{asc} = {0.4}$ ,and ${\eta }_{\text{conf }} = {0.4}$ .
参照[7]，最大探索步数设为500。局部地图分辨率为${128} \times  {128}$，完整地图为${1280} \times  {1280}$（对应物理世界中的${48} \times  {48},{m}^{2}$）。虽然不足以完成全面探索，但这有效突出不同策略在语义地图构建上的差异。长期目标每10步更新一次。经验上，ASC约为总覆盖的一半。我们通过将${\Delta }_{asc}$翻倍估计其值，并通过将${\Delta }_{\text{asc }}$乘以1000计算${\Delta }_{\text{conf }}$。最终奖励（公式2）使用超参数${\eta }_{c} = {0.6},{\eta }_{asc} = {0.4}$和${\eta }_{\text{conf }} = {0.4}$计算。


## B. Baselines
## B. 基线方法


In our simulation experiments, the agent's pose is obtained directly from Habitat's ground-truth position sensor, avoiding pose estimation and simplifying mapping. This allows us to focus on evaluating exploration speed, coverage integrity, and semantic certainty. Accordingly, we select baselines that emphasize map coverage.
在我们的仿真实验中，智能体的位姿直接由Habitat的真实位置传感器获取，避免了位姿估计，简化了映射过程。这使我们能够专注于评估探索速度、覆盖完整性和语义确定性。因此，我们选择了强调地图覆盖的基线方法。


<!-- Media -->



<!-- figureText: SemExp* EE Impact SEA (Ours) ARiADNE RGB Projected Map local view waypoints Ground Truth Semantic Map Full Projected Map Full Completed Map Trajectory -->



<img src="https://cdn.noedgeai.com/bo_d3ve1ejef24c73d2hujg_5.jpg?x=168&y=139&w=1450&h=536&r=0"/>



Fig. 4: The qualitative evaluation of different methods during evaluation, including the full projected map and the full completed map. To provide a clearer view,we enlarge ${m}_{proj}$ and ${m}_{cmplt}$ built at a specific time point in the bottom left corner. Since ARiADNE has no semantic maps, we use different views according to its original paper for illustration.
图4：不同方法在评估过程中的定性评估，包括完整投影地图和完整补全地图。为提供更清晰的视图，我们放大了左下角特定时间点构建的${m}_{proj}$和${m}_{cmplt}$。由于ARiADNE没有语义地图，我们根据其原论文使用不同视图进行说明。


<!-- Media -->



leftmargin $= {.5}\mathrm{\;{cm}}$
左边距 $= {.5}\mathrm{\;{cm}}$


- SemExp* [4]: Originally designed for object-goal navigation, SemExp shares its architecture with ANS [6], which targets occupied coverage. We modify its reward to prioritize coverage and retrain it on the same dataset, effectively converting it into an exploration method.
- SemExp* [4]：最初设计用于目标导航，SemExp与针对占用覆盖的ANS [6]共享架构。我们修改其奖励以优先覆盖，并在相同数据集上重新训练，有效将其转化为探索方法。


- Impact [7]: Trained with purely intrinsic rewards, Impact explores to maximize occupied coverage. We evaluate it using the provided trained model under the same setting reported in the original paper.
- Impact [7]：仅使用内在奖励训练，Impact旨在最大化占用覆盖。我们使用原论文提供的训练模型，在相同设置下进行评估。


- EE [2]: This work analyzes how reward design affects coverage and object detection. We adopt the released model with the best coverage performance for evaluation.
- EE [2]：该工作分析了奖励设计如何影响覆盖和目标检测。我们采用其发布的覆盖性能最佳模型进行评估。


- ARiADNE [53] is a state-of-the-art RL-based exploration method that outperforms classical approaches like TARE [12] on grid maps. For a fair comparison, we adapted it to match our method's mapping scope and integrated our short-term policy, FMM, as its fine-grained exploration strategy in the continuous Habitat environment.
- ARiADNE [53]是一种基于强化学习的先进探索方法，在网格地图上优于传统方法如TARE [12]。为公平比较，我们调整其映射范围以匹配我们的方法，并在连续的Habitat环境中集成了我们的短期策略FMM作为其细粒度探索策略。


## C. Results on MP3D Datasets
## C. MP3D数据集上的结果


As shown in Table 1, our method outperforms existing approaches in both global map coverage and semantic accuracy, for both projected and completed maps. After 500 steps, it achieves ${111.74}{\mathrm{\;m}}^{2}$ of occupied coverage and ${49.53}{\mathrm{\;m}}^{2}$ of accurate semantic coverage on the projected map.
如表1所示，我们的方法在全局地图覆盖和语义准确性上均优于现有方法，无论是投影地图还是补全地图。经过500步后，在投影地图上实现了${111.74}{\mathrm{\;m}}^{2}$的占用覆盖和${49.53}{\mathrm{\;m}}^{2}$的准确语义覆盖。


ARiADNE, originally trained in simple, flat environments with regular layouts, performs poorly on MP3D, which contains diverse and irregular structures, often with multiple floors. For fairness, we report ARiADNE* on single-level scenes, where its performance is comparable to other baselines. These results highlight the advantage of our method in the selection of routes and the prediction of long-term semantics. By avoiding redundant exploration in already confident areas, it reallocates effort to uncertain regions, leading to more efficient exploration.
ARiADNE最初在简单、平坦且布局规则的环境中训练，在包含多样且不规则结构、通常有多层楼的MP3D数据集上表现不佳。为公平起见，我们报告了ARiADNE*在单层场景中的表现，其性能与其他基线相当。这些结果凸显了我们方法在路径选择和长期语义预测上的优势。通过避免在已确信区域的冗余探索，它将精力重新分配到不确定区域，从而实现更高效的探索。


Notably, the best coverage reported in Impact [7] when evaluated on the MP3D Val split is ${144.64}{\mathrm{\;m}}^{2}$ . However,this difference arises because our experimental setup limits the depth range of map projection to $5\mathrm{\;m}$ ,whereas the mapper in Impact’s code uses ${10}\mathrm{\;m}$ . Consequently,the coverage of our semantic map mapper appears lower than in Impact's original paper. Nevertheless, since all experiments are conducted under a consistent setting in our study, the results remain valid and reliable.
值得注意的是，Impact [7]在MP3D验证集上的最佳覆盖率为${144.64}{\mathrm{\;m}}^{2}$。然而，这一差异源于我们的实验设置将地图投影的深度范围限制为$5\mathrm{\;m}$，而Impact代码中的映射器使用的是${10}\mathrm{\;m}$。因此，我们语义地图映射器的覆盖率看起来低于Impact原文中的结果。尽管如此，由于我们所有实验均在一致的设置下进行，结果依然有效且可靠。


Figure 4 further illustrates agent trajectories in an unseen MP3D scene. Our method yields the widest coverage and highest semantic accuracy. The close-up around the dining table shows how multi-view observations produce a more precise semantic map, surpassing SemExp*, EE, and Impact in detail and correctness.
图4进一步展示了在未见过的MP3D场景中智能体的轨迹。我们的方法实现了最广泛的覆盖和最高的语义准确率。餐桌周围的特写显示，多视角观测生成了更精确的语义地图，在细节和正确性上超越了SemExp*、EE和Impact。


Furthermore, to demonstrate the effectiveness of each module in our proposed method, we conducted a comprehensive ablation study. Specifically, we systematically removed each component from the global input of the RL navigator as well as individual reward mechanisms. As quantitatively presented in Table II, our experimental results demonstrate that each module contributes significantly to the semantic map construction process. The most substantial performance degradation was observed when ${m}_{\text{cmplt }}$ and ${m}_{\text{conf }}$ were excluded from the global input, indicating that the superior performance primarily stems from the map-based inputs to the reinforcement learning model for long-term goal selection, rather than relying solely on feedback rewards. Moreover,our analysis reveals that ${m}_{\text{conf }}$ and ${r}_{\text{conf }}$ ,which specifically identify and quantify the semantically uncertain regions in the currently explored map, thereby enabling more effective optimization of long-term goal selection. The differential impact between these components suggests that the explicit representation of uncertainty plays a crucial role in guiding the exploration process.
此外，为了验证我们所提方法中各模块的有效性，我们进行了全面的消融研究。具体而言，我们系统地移除了强化学习导航器全局输入中的各个组件以及单独的奖励机制。如表II中定量展示的实验结果所示，每个模块对语义地图构建过程均有显著贡献。当从全局输入中排除${m}_{\text{cmplt }}$和${m}_{\text{conf }}$时，性能下降最为明显，表明优异性能主要源于基于地图的输入对强化学习模型进行长期目标选择的支持，而非仅依赖反馈奖励。此外，我们的分析显示，${m}_{\text{conf }}$和${r}_{\text{conf }}$专门识别并量化当前已探索地图中的语义不确定区域，从而实现更有效的长期目标选择优化。这些组件间的差异性影响表明，不确定性的显式表示在引导探索过程中起着关键作用。


<!-- Media -->



<!-- figureText: t=64 t=88 t=112 t=144 t=168 t=192 t=240 t=272 cushior sink floor cabinet window misc 3rd-person Semantic door wall table chair counter sofa -->



<img src="https://cdn.noedgeai.com/bo_d3ve1ejef24c73d2hujg_6.jpg?x=173&y=145&w=1455&h=352&r=0"/>



Fig. 5: The qualitative evaluation of SEA in a real-world setting. The top row shows third-person views of the robot's navigation at different timesteps. The bottom row displays the corresponding semantic mapping results. In the semantic maps, the red circle indicates the robot's pose and orientation, the green cross marks the next waypoint predicted by our model, and the blue line represents the planned path to that waypoint. The legend at the bottom explains the color-coding for different object categories in the semantic map.
图5：SEA在真实环境中的定性评估。上排展示了机器人在不同时间步的第三人称导航视角。下排显示对应的语义映射结果。在语义地图中，红色圆圈表示机器人的位置和朝向，绿色十字标记由我们模型预测的下一个航路点，蓝色线条表示规划至该航路点的路径。底部图例说明了语义地图中不同物体类别的颜色编码。


<!-- Media -->



## D. Real-world Deployment
## D. 真实环境部署


To further validate the practicality of our approach, we conducted a real-world experiment in an indoor office environment using the Agilex Cobot Magic mobile platform. The robot was equipped with an Intel RealSense D435i camera at the front, and we utilized the robot's native wheel odometry for localization. The system demonstrated high efficiency,with an average inference time of ${0.08}\mathrm{\;s}$ for waypoint prediction and an average time of ${0.75}\mathrm{\;s}$ per frame for semantic map construction. Within a fixed number of steps, our method enabled the robot to actively explore the environment, producing a semantic map that captured both the structural layout and object categories with high fidelity. This demonstrates its robustness and effectiveness beyond simulation. Representative results of the constructed semantic map in the office environment are shown in Figure 5
为进一步验证我们方法的实用性，我们在室内办公环境中使用Agilex Cobot Magic移动平台进行了真实实验。机器人前端配备了Intel RealSense D435i摄像头，定位采用机器人自带的轮式里程计。系统表现出高效性，航路点预测的平均推理时间为${0.08}\mathrm{\;s}$，语义地图构建的平均每帧时间为${0.75}\mathrm{\;s}$。在固定步数内，我们的方法使机器人能够主动探索环境，生成的语义地图高保真地捕捉了结构布局和物体类别，展示了其在仿真之外的鲁棒性和有效性。办公环境中构建的语义地图代表性结果见图5。


## V. CONCLUSION
## V. 结论


In this work, we introduced SEA, a visual exploration framework that prioritizes low-confidence regions on the semantic map to achieve rapid semantic coverage. By leveraging reinforcement learning for long-term goal selection, SEA guides the agent toward uncertain areas, enabling efficient observation of semantic objects, refinement of the map, and effective exploration of unseen environments within limited steps. Experimental results show that SEA consistently outperforms existing DL- and DRL-based exploration methods in both semantic map construction and indoor exploration.
本文提出了SEA，一种视觉探索框架，优先关注语义地图中置信度较低的区域，以实现快速的语义覆盖。通过利用强化学习进行长期目标选择，SEA引导智能体前往不确定区域，有效观察语义对象、细化地图，并在有限步数内高效探索未知环境。实验结果表明，SEA在语义地图构建和室内探索方面均持续优于现有基于深度学习和深度强化学习的探索方法。


Limitations and Future Work. Our study focused on predicting completed maps from local projections within a ${4.8} \times  {4.8}{m}^{2}$ region around the agent. Extending completion and confidence estimation to the global map remains challenging due to model size, runtime constraints, and the mismatch between long-term goal selection and feasible movement ranges. Additionally, noise artifacts caused by distant objects beyond the simulator's depth range highlight the need for more effective denoising methods that preserve semantic accuracy. We believe addressing these challenges will further advance semantic exploration and support broader applications of embodied AI in real-world environments. REFERENCES
局限性与未来工作。我们的研究聚焦于预测智能体周围${4.8} \times  {4.8}{m}^{2}$区域内的局部投影完成地图。由于模型规模、运行时间限制以及长期目标选择与可行移动范围的不匹配，将完成度和置信度估计扩展到全局地图仍具挑战性。此外，模拟器深度范围之外远距离物体引起的噪声伪影凸显了需要更有效的去噪方法以保持语义准确性。我们相信，解决这些挑战将进一步推动语义探索的发展，并支持具身人工智能在真实环境中的更广泛应用。参考文献


[1] D. S. Chaplot, H. Jiang, S. Gupta, and A. Gupta, "Semantic curiosity for active visual learning," in Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VI 16. Springer, 2020, pp. 309-326.
[1] D. S. Chaplot, H. Jiang, S. Gupta, 和 A. Gupta, “用于主动视觉学习的语义好奇心（Semantic curiosity）,” 载于《计算机视觉-ECCV 2020：第16届欧洲会议，英国格拉斯哥，2020年8月23-28日，论文集，第六部分16》，施普林格，2020年，第309-326页。


[2] S. K. Ramakrishnan, D. Jayaraman, and K. Grauman, "An exploration of embodied visual exploration," International Journal of Computer Vision, vol. 129, pp. 1616-1649, 2021.
[2] S. K. Ramakrishnan, D. Jayaraman, 和 K. Grauman, “具身视觉探索的研究,” 《国际计算机视觉杂志》，第129卷，第1616-1649页，2021年。


[3] J. Ye, D. Batra, A. Das, and E. Wijmans, "Auxiliary tasks and exploration enable objectgoal navigation," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 16117- 16126.
[3] J. Ye, D. Batra, A. Das, 和 E. Wijmans, “辅助任务与探索促进目标物体导航,” 载于《IEEE/CVF国际计算机视觉会议论文集》，2021年，第16117-16126页。


[4] D. S. Chaplot, D. P. Gandhi, A. Gupta, and R. R. Salakhutdinov, "Object goal navigation using goal-oriented semantic exploration," Advances in Neural Information Processing Systems, vol. 33, pp. 4247-4258, 2020.
[4] D. S. Chaplot, D. P. Gandhi, A. Gupta, 和 R. R. Salakhutdinov, “基于目标导向语义探索的目标物体导航,” 《神经信息处理系统进展》，第33卷，第4247-4258页，2020年。


[5] M. Zhu, B. Zhao, and T. Kong, "Navigating to objects in unseen environments by distance prediction," in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2022, pp. 10571-10578.
[5] M. Zhu, B. Zhao, 和 T. Kong, “通过距离预测实现未知环境中的物体导航,” 载于2022年IEEE/RSJ智能机器人与系统国际会议（IROS），IEEE，2022年，第10571-10578页。


[6] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdi-nov, "Learning to explore using active neural slam," arXiv preprint arXiv:2004.05155, 2020.
[6] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, 和 R. Salakhutdinov, “使用主动神经SLAM学习探索,” arXiv预印本 arXiv:2004.05155, 2020年。


[7] R. Bigazzi, F. Landi, S. Cascianelli, L. Baraldi, M. Cornia, and R. Cucchiara, "Focus on impact: indoor exploration with intrinsic motivation," IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 2985-2992, 2022.
[7] R. Bigazzi, F. Landi, S. Cascianelli, L. Baraldi, M. Cornia, 和 R. Cucchiara, “聚焦影响力：基于内在动机的室内探索,” 《IEEE机器人与自动化快报》，第7卷，第2期，第2985-2992页，2022年。


[8] L. Juncheng, M. Brendan, and M. Steven, "Learning to explore by reinforcement over high-level options," arXiv preprint arXiv:2111.01364, 2021.
[8] L. Juncheng, M. Brendan, 和 M. Steven, “通过高层选项的强化学习实现探索,” arXiv预印本 arXiv:2111.01364, 2021年。


[9] D. Dolgov, S. Thrun, M. Montemerlo, and J. Diebel, "Practical search techniques in path planning for autonomous driving," 2008.
[9] D. Dolgov, S. Thrun, M. Montemerlo, 和 J. Diebel, “自动驾驶路径规划中的实用搜索技术,” 2008年。


[10] B. Zhou, F. Gao, L. Wang, C. Liu, and S. Shen, "Robust and efficient quadrotor trajectory generation for fast autonomous flight," IEEE Robotics and Automation Letters, vol. 4, no. 4, pp. 3529-3536, 2019.
[10] B. Zhou, F. Gao, L. Wang, C. Liu, 和 S. Shen, “用于快速自主飞行的鲁棒高效四旋翼轨迹生成,” 《IEEE机器人与自动化快报》，第4卷，第4期，第3529-3536页，2019年。


[11] B. Zhou, Y. Zhang, X. Chen, and S. Shen, "Fuel: Fast uav exploration using incremental frontier structure and hierarchical planning," IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 779-786, 2021.
[11] B. Zhou, Y. Zhang, X. Chen, 和 S. Shen, “FUEL：利用增量前沿结构和分层规划的快速无人机探索,” 《IEEE机器人与自动化快报》，第6卷，第2期，第779-786页，2021年。


[12] C. Cao, H. Zhu, H. Choset, and J. Zhang, "Tare: A hierarchical framework for efficiently exploring complex $3\mathrm{\;d}$ environments," Robotics: Science and Systems XVII, 2021. [Online]. Available: https://api.semanticscholar.org/CorpusID:235399646
[12] C. Cao, H. Zhu, H. Choset, 和 J. Zhang, “TARE：一种高效探索复杂$3\mathrm{\;d}$环境的分层框架,” 《机器人学：科学与系统》第十七届，2021年。[在线]. 可获取：https://api.semanticscholar.org/CorpusID:235399646


[13] Y. Cao, J. Lew, J. Liang, J. Cheng, and G. Sartoretti, "Dare: Diffusion policy for autonomous robot exploration," arXiv preprint arXiv:2410.16687, 2024.
[13] Y. Cao, J. Lew, J. Liang, J. Cheng, 和 G. Sartoretti, “DARE：用于自主机器人探索的扩散策略,” arXiv预印本 arXiv:2410.16687, 2024年。


[14] J. Yang, S. Yang, A. W. Gupta, R. Han, L. Fei-Fei, and S. Xie, "Thinking in space: How multimodal large language models see, remember, and recall spaces," arXiv preprint arXiv:2412.14171, 2024.
[14] J. Yang, S. Yang, A. W. Gupta, R. Han, L. Fei-Fei, 和 S. Xie, “空间思维：多模态大型语言模型如何观察、记忆与回忆空间,” arXiv预印本 arXiv:2412.14171, 2024年。


[15] Z. Wang, X. Li, J. Yang, Y. Liu, and S. Jiang, "Gridmm: Grid memory map for vision-and-language navigation," 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 15579- 15590, 2023. [Online]. Available: https://api.semanticscholar.org/ CorpusID:260125382
[15] Z. Wang, X. Li, J. Yang, Y. Liu, 和 S. Jiang, “GridMM：用于视觉与语言导航的网格记忆地图,” 2023年IEEE/CVF国际计算机视觉会议（ICCV），第15579-15590页，2023年。[在线]. 可获取：https://api.semanticscholar.org/CorpusID:260125382


[16] S. Chen, P.-L. Guhur, M. Tapaswi, C. Schmid, and I. Laptev, "Think global, act local: Dual-scale graph transformer for vision-and-language navigation," 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16516-16526, 2022. [Online]. Available: https://api.semanticscholar.org/CorpusID:247084359
[16] S. Chen, P.-L. Guhur, M. Tapaswi, C. Schmid, 和 I. Laptev, “全球思考，局部行动：用于视觉与语言导航的双尺度图变换器”，2022年IEEE/CVF计算机视觉与模式识别会议（CVPR），第16516-16526页，2022年。[在线]. 可获取：https://api.semanticscholar.org/CorpusID:247084359


[17] W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert, "Pcn: Point completion network," 2018 International Conference on 3D Vision (3DV), pp. 728-737, 2018. [Online]. Available: https: //api.semanticscholar.org/CorpusID:51908879
[17] W. Yuan, T. Khot, D. Held, C. Mertz, 和 M. Hebert, “PCN：点云补全网络”，2018年国际三维视觉会议（3DV），第728-737页，2018年。[在线]. 可获取：https://api.semanticscholar.org/CorpusID:51908879


[18] L. P. Tchapmi, V. Kosaraju, H. Rezatofighi, I. D. Reid, and S. Savarese, "Topnet: Structural point cloud decoder," 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 383-392, 2019. [Online]. Available: https://api.semanticscholar.org/CorpusID: 198184513
[18] L. P. Tchapmi, V. Kosaraju, H. Rezatofighi, I. D. Reid, 和 S. Savarese, “TopNet：结构化点云解码器”，2019年IEEE/CVF计算机视觉与模式识别会议（CVPR），第383-392页，2019年。[在线]. 可获取：https://api.semanticscholar.org/CorpusID:198184513


[19] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. A. Funkhouser, "Semantic scene completion from a single depth image," 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 190-198, 2016. [Online]. Available: https://api.semanticscholar.org/CorpusID:20416090
[19] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, 和 T. A. Funkhouser, “基于单幅深度图的语义场景补全”，2017年IEEE计算机视觉与模式识别会议（CVPR），第190-198页，2016年。[在线]. 可获取：https://api.semanticscholar.org/CorpusID:20416090


[20] Y. Li, Z. Yu, C. B. Choy, C. Xiao, J. M. Álvarez, S. Fidler, C. Feng, and A. Anandkumar, "Voxformer: Sparse voxel transformer for camera-based 3d semantic scene completion," 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9087-9098, 2023. [Online]. Available: https://api.semanticscholar.org/CorpusID: 257102923
[20] Y. Li, Z. Yu, C. B. Choy, C. Xiao, J. M. Álvarez, S. Fidler, C. Feng, 和 A. Anandkumar, “Voxformer：基于稀疏体素的摄像机三维语义场景补全变换器”，2023年IEEE/CVF计算机视觉与模式识别会议（CVPR），第9087-9098页，2023年。[在线]. 可获取：https://api.semanticscholar.org/CorpusID:257102923


[21] H. Jiang, T. Cheng, N. Gao, H. Zhang, T. Lin, W. Liu, and X. Wang, "Symphonize 3d semantic scene completion with contextual instance queries," CVPR, 2024.
[21] H. Jiang, T. Cheng, N. Gao, H. Zhang, T. Lin, W. Liu, 和 X. Wang, “Symphonize：结合上下文实例查询的三维语义场景补全”，CVPR，2024年。


[22] C. Feng, H. Li, F. Gao, B. Zhou, and S. Shen, "Predrecon: A prediction-boosted planning framework for fast and high-quality autonomous aerial reconstruction," in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 1207- 1213.
[22] C. Feng, H. Li, F. Gao, B. Zhou, 和 S. Shen, “PredRecon：一种基于预测增强的快速高质量自主空中重建规划框架”，2023年IEEE国际机器人与自动化会议（ICRA），IEEE，2023年，第1207-1213页。


[23] C. Gao, G. Yang, X. Chen, and B. M. Chen, "Active view planner for infrastructure 3d reconstruction," 2024 IEEE 18th International Conference on Control & Automation (ICCA), pp. 400-405, 2024. [Online]. Available: https://api.semanticscholar.org/CorpusID: 271462597
[23] C. Gao, G. Yang, X. Chen, 和 B. M. Chen, “基础设施三维重建的主动视角规划器”，2024年IEEE第18届国际控制与自动化会议（ICCA），第400-405页，2024年。[在线]. 可获取：https://api.semanticscholar.org/CorpusID:271462597


[24] Z. Xu, C. Suzuki, X. Zhan, and K. Shimada, "Heuristic-based incremental probabilistic roadmap for efficient uav exploration in dynamic environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 11832-11838, 2023. [Online]. Available: https://api.semanticscholar.org/CorpusID:262043787
[24] Z. Xu, C. Suzuki, X. Zhan, 和 K. Shimada, “基于启发式的增量概率路网，用于动态环境中无人机高效探索”，2024年IEEE国际机器人与自动化会议（ICRA），第11832-11838页，2023年。[在线]. 可获取：https://api.semanticscholar.org/CorpusID:262043787


[25] T. Chen, S. Gupta, and A. Gupta, "Learning exploration policies for navigation," arXiv preprint arXiv:1903.01959, 2019.
[25] T. Chen, S. Gupta, 和 A. Gupta, “导航探索策略学习”，arXiv预印本 arXiv:1903.01959，2019年。


[26] S. K. Ramakrishnan, Z. Al-Halah, and K. Grauman, "Occupancy anticipation for efficient exploration and navigation," in Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V 16. Springer, 2020, pp. 400-418.
[26] S. K. Ramakrishnan, Z. Al-Halah, 和 K. Grauman, “占用预测用于高效探索与导航”，载于《计算机视觉-ECCV 2020：第16届欧洲会议，英国格拉斯哥，2020年8月23-28日，论文集，第五部分》，施普林格，2020年，第400-418页。


[27] V. D. Sharma, J. Chen, A. Shrivastava, and P. Tokekar, "Occupancy map prediction for improved indoor robot navigation," arXiv preprint arXiv:2203.04177, 2022.
[27] V. D. Sharma, J. Chen, A. Shrivastava, 和 P. Tokekar, “占用地图预测以提升室内机器人导航”，arXiv预印本 arXiv:2203.04177，2022年。


[28] S. D. Morad, S. Liwicki, R. Kortvelesy, R. Mecca, and A. Prorok, "Graph convolutional memory using topological priors," arXiv preprint arXiv:2106.14117, 2021.
[28] S. D. Morad, S. Liwicki, R. Kortvelesy, R. Mecca, 和 A. Prorok, “利用拓扑先验的图卷积记忆,” arXiv预印本 arXiv:2106.14117, 2021.


[29] F. Wang, C. Zhang, F. Tang, H. Jiang, Y. Wu, and Y. Liu, "Lightweight object-level topological semantic mapping and long-term global localization based on graph matching," arXiv preprint arXiv:2201.05977, 2022.
[29] F. Wang, C. Zhang, F. Tang, H. Jiang, Y. Wu, 和 Y. Liu, “基于图匹配的轻量级对象级拓扑语义映射与长期全局定位,” arXiv预印本 arXiv:2201.05977, 2022.


[30] Y. Li, Y. Ma, X. Huo, and X. Wu, "Remote object navigation for service robots using hierarchical knowledge graph in human-centered environments," Intelligent Service Robotics, vol. 15, no. 4, pp. 459- 473, 2022.
[30] Y. Li, Y. Ma, X. Huo, 和 X. Wu, “在人本环境中使用分层知识图谱的服务机器人远程对象导航,” 智能服务机器人, 第15卷, 第4期, 页459-473, 2022.


[31] R. Dang, Z. Shi, L. Wang, Z. He, C. Liu, and Q. Chen, "Unbiased directed object attention graph for object navigation," in Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 3617-3627.
[31] R. Dang, Z. Shi, L. Wang, Z. He, C. Liu, 和 Q. Chen, “用于对象导航的无偏向定向对象注意力图,” 载于第30届ACM国际多媒体会议论文集, 2022, 页3617-3627.


[32] D. S. Chaplot, R. Salakhutdinov, A. Gupta, and S. Gupta, "Neural topological slam for visual navigation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 12875-12884.
[32] D. S. Chaplot, R. Salakhutdinov, A. Gupta, 和 S. Gupta, “用于视觉导航的神经拓扑SLAM,” 载于IEEE/CVF计算机视觉与模式识别会议论文集, 2020, 页12875-12884.


[33] Z. Ravichandran, L. Peng, N. Hughes, J. D. Griffith, and L. Carlone, "Hierarchical representations and explicit memory: Learning effective navigation policies on $3\mathrm{\;d}$ scene graphs using graph neural networks," in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 9272-9279.
[33] Z. Ravichandran, L. Peng, N. Hughes, J. D. Griffith, 和 L. Carlone, “分层表示与显式记忆：利用图神经网络在$3\mathrm{\;d}$场景图上学习有效导航策略,” 载于2022年国际机器人与自动化会议(ICRA). IEEE, 2022, 页9272-9279.


[34] T. Zheng, Z. Duan, J. Wang, G. Lu, S. Li, and Z. Yu, "Research on distance transform and neural network lidar information sampling classification-based semantic segmentation of $2\mathrm{\;d}$ indoor room maps," Sensors, vol. 21, no. 4, p. 1365, 2021.
[34] T. Zheng, Z. Duan, J. Wang, G. Lu, S. Li, 和 Z. Yu, “基于距离变换和神经网络激光雷达信息采样分类的$2\mathrm{\;d}$室内房间地图语义分割研究,” 传感器, 第21卷, 第4期, 页1365, 2021.


[35] D. Jayaraman and K. Grauman, "Look-ahead before you leap: end-to-end active recognition by forecasting the effect of motion," in Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14. Springer, 2016, pp. 489-505.
[35] D. Jayaraman 和 K. Grauman, “三思而后行：通过预测运动效果实现端到端主动识别,” 载于计算机视觉-ECCV 2016: 第14届欧洲会议, 荷兰阿姆斯特丹, 2016年10月11-14日, 论文集，第五部分14. Springer, 2016, 页489-505.


[36] ——, “Learning to look around: Intelligently exploring unseen environments for unknown tasks," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 1238-1247.
[36] ——, “学会环顾四周：智能探索未知环境以完成未知任务,” 载于IEEE计算机视觉与模式识别会议论文集, 2018, 页1238-1247.


[37] A. Guédon, T. Monnier, P. Monasse, and V. Lepetit, "Macarons: Mapping and coverage anticipation with rgb online self-supervision," arXiv preprint arXiv:2303.03315, 2023.
[37] A. Guédon, T. Monnier, P. Monasse, 和 V. Lepetit, “Macarons：基于RGB在线自监督的映射与覆盖预测,” arXiv预印本 arXiv:2303.03315, 2023.


[38] T. Cieslewski, E. Kaufmann, and D. Scaramuzza, "Rapid exploration with multi-rotors: A frontier selection method for high speed flight," in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017, pp. 2135-2142.
[38] T. Cieslewski, E. Kaufmann, 和 D. Scaramuzza, “多旋翼快速探索：一种用于高速飞行的前沿选择方法,” 载于2017年IEEE/RSJ国际智能机器人与系统会议(IROS). IEEE, 2017, 页2135-2142.


[39] J. I. Vasquez-Gomez, L. E. Sucar, R. Murrieta-Cid, and E. Lopez-Damian, "Volumetric next-best-view planning for 3d object reconstruction with positioning error," International Journal of Advanced Robotic Systems, vol. 11, no. 10, p. 159, 2014.
[39] J. I. Vasquez-Gomez, L. E. Sucar, R. Murrieta-Cid, 和 E. Lopez-Damian, “考虑定位误差的三维对象重建体积下一最佳视角规划,” 国际高级机器人系统杂志, 第11卷, 第10期, 页159, 2014.


[40] J. I. Vasquez-Gomez, L. E. Sucar, and R. Murrieta-Cid, "View/state planning for three-dimensional object reconstruction under uncertainty," Autonomous Robots, vol. 41, pp. 89-109, 2017.
[40] J. I. Vasquez-Gomez, L. E. Sucar, 和 R. Murrieta-Cid, “不确定性下的三维对象重建视角/状态规划,” 自主机器人, 第41卷, 页89-109, 2017.


[41] S. Liu and T. Okatani, "Symmetry-aware neural architecture for embodied visual navigation," arXiv preprint arXiv:2112.09515, 2021.
[41] S. Liu 和 T. Okatani, “考虑对称性的具身视觉导航神经架构,” arXiv预印本 arXiv:2112.09515, 2021.


[42] Y. Liang, B. Chen, and S. Song, "Sscnav: Confidence-aware semantic scene completion for visual semantic navigation," in 2021 IEEE international conference on robotics and automation (ICRA). IEEE, 2021, pp. 13 194-13 200.
[42] Y. Liang, B. Chen, 和 S. Song, “SSCNav：面向视觉语义导航的置信度感知语义场景补全,” 载于2021年IEEE国际机器人与自动化会议(ICRA). IEEE, 2021, 页13194-13200.


[43] G. Georgakis, B. Bucher, K. Schmeckpeper, S. Singh, and K. Dani-ilidis, "Learning to map for active semantic goal navigation," arXiv preprint arXiv:2106.15648, 2021.
[43] G. Georgakis, B. Bucher, K. Schmeckpeper, S. Singh, 和 K. Dani-ilidis, “学习映射以实现主动语义目标导航,” arXiv预印本 arXiv:2106.15648, 2021.


[44] X. Chen, T. Wang, Q. Li, T. Huang, J. Pang, and T. Xue, "Gleam: Learning generalizable exploration policy for active mapping in complex 3d indoor scenes," 2025.
[44] X. Chen, T. Wang, Q. Li, T. Huang, J. Pang, 和 T. Xue, “Gleam：在复杂三维室内场景中学习可泛化的主动映射探索策略,” 2025.


[45] Z. Li, J. Xin, and N. Li, "Autonomous exploration and mapping for mobile robots via cumulative curriculum reinforcement learning," in 2023 IEEE/RSJ Ineternational Conference on Intelligent Robots and Systems (IROS). IEEE, 2023, pp. 7495-7500.
[45] Z. Li, J. Xin, 和 N. Li, “通过累积课程强化学习实现移动机器人自主探索与映射,” 载于2023 IEEE/RSJ国际智能机器人与系统会议（IROS），IEEE, 2023, 页7495-7500.


[46] X. Hu, K. Yang, L. Fei, and K. Wang, "Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation," in 2019 IEEE International Conference on Image Processing (ICIP). IEEE, 2019, pp. 1440-1444.
[46] X. Hu, K. Yang, L. Fei, 和 K. Wang, “Acnet：基于注意力机制的网络用于RGB-D语义分割中互补特征的利用,” 载于2019 IEEE国际图像处理会议（ICIP），IEEE, 2019, 页1440-1444.


[47] C. Campos, R. Elvira, J. J. G. Rodr'iguez, J. M. M. Montiel, and J. D. Tardós, "Orb-slam3: An accurate open-source library for visual, visual-inertial, and multimap slam," IEEE Transactions on Robotics, vol. 37, pp. 1874-1890, 2020. [Online]. Available: https://api.semanticscholar.org/CorpusID:220713377
[47] C. Campos, R. Elvira, J. J. G. Rodríguez, J. M. M. Montiel, 和 J. D. Tardós, “ORB-SLAM3：一个精确的开源视觉、视觉惯性及多地图SLAM库,” IEEE机器人学报, 第37卷, 页1874-1890, 2020. [在线]. 可用: https://api.semanticscholar.org/CorpusID:220713377


[48] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik et al., "Habitat: A platform for embodied ai research," in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 9339-9347.
[48] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik 等, “Habitat：一个用于具身人工智能研究的平台,” 载于IEEE/CVF国际计算机视觉会议论文集, 2019, 页9339-9347.


[49] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor," in International conference on machine learning. PMLR, 2018, pp. 1861-1870.
[49] T. Haarnoja, A. Zhou, P. Abbeel, 和 S. Levine, “软演员-评论家（Soft Actor-Critic）：一种带随机策略的离策略最大熵深度强化学习方法,” 载于国际机器学习会议，PMLR, 2018, 页1861-1870.


[50] J. A. Sethian, "A fast marching level set method for monotonically advancing fronts." Proceedings of the National Academy of Sciences, vol. 93, no. 4, pp. 1591-1595, 1996.
[50] J. A. Sethian, “一种用于单调推进前沿的快速行进水平集方法,” 美国国家科学院院刊, 第93卷, 第4期, 页1591-1595, 1996.


[51] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang, "Matterport3d: Learning from rgb-d data in indoor environments," arXiv preprint arXiv:1709.06158, 2017.
[51] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, 和 Y. Zhang, “Matterport3D：基于室内环境RGB-D数据的学习,” arXiv预印本 arXiv:1709.06158, 2017.


[52] V. Cartillier, Z. Ren, N. Jain, S. Lee, I. Essa, and D. Batra, "Semantic mapnet: Building allocentric semantic maps and representations from egocentric views," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 2, 2021, pp. 964-972.
[52] V. Cartillier, Z. Ren, N. Jain, S. Lee, I. Essa, 和 D. Batra, “Semantic MapNet：从自我视角构建分配中心语义地图及表示,” 载于AAAI人工智能会议论文集, 第35卷, 第2期, 2021, 页964-972.


[53] Y. Cao, T. Hou, Y. Wang, X. Yi, and G. Sartoretti, "Ariadne: A reinforcement learning approach using attention-based deep networks for exploration," 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 10 219-10 225, 2023. [Online]. Available: https://api.semanticscholar.org/CorpusID:256358779
[53] Y. Cao, T. Hou, Y. Wang, X. Yi, 和 G. Sartoretti, “Ariadne：一种基于注意力深度网络的强化学习探索方法,” 2023 IEEE国际机器人与自动化会议（ICRA），页10219-10225, 2023. [在线]. 可用: https://api.semanticscholar.org/CorpusID:256358779