# Android in the Zoo: Chain-of-Action-Thought for GUI Agents
# Android in the Zoo: GUI 代理的行动-思考链


Jiwen Zhang ${}^{1,2} *$ ,Jihao Wu ${}^{2}$ ,Yihua Teng ${}^{2}$ ,Minghui Liao ${}^{2}$ , Nuo Xu ${}^{2}$ , Xiao Xiao ${}^{2}$ , Zhongyu Wei ${}^{1 \dagger  }$ , Duyu Tang ${}^{2 \dagger  }$
Jiwen Zhang ${}^{1,2} *$ ,Jihao Wu ${}^{2}$ ,Yihua Teng ${}^{2}$ ,Minghui Liao ${}^{2}$ , Nuo Xu ${}^{2}$ , Xiao Xiao ${}^{2}$ , Zhongyu Wei ${}^{1 \dagger  }$ , Duyu Tang ${}^{2 \dagger  }$


${}^{1}$ Fudan University ${}^{2}$ Huawei Inc.
${}^{1}$ 复旦大学 ${}^{2}$ 华为公司


jiwenzhang21@m.fudan.edu.cn



\{wujihao,tengyihua,liaominghui1,xunuo4,xiaoxiao55\}@huawei.com
\{wujihao,tengyihua,liaominghui1,xunuo4,xiaoxiao55\}@huawei.com


zywei@fudan.edu.cn duyutang@huawei.com
zywei@fudan.edu.cn duyutang@huawei.com


https://github.com/IMNearth/CoAT



## Abstract
## 摘要


Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typically consider little semantic information carried out by intermediate screen-shots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon three off-the-shelf LMMs, CoAT significantly improves the action prediction compared to previous proposed context modeling. To further facilitate the research in this line, we construct a dataset Android-In-The-Zoo (AITZ), which contains 18,643 screen-action pairs together with chain-of-action-thought annotations. Experiments show that fine-tuning a 1B model (i.e. AUTO-UI-base) on our AITZ dataset achieves on-par performance with CogAgent-Chat-18B.
大型语言模型（LLM）推动了智能手机领域大量自主 GUI 代理的兴起，通过预测一系列 API 操作来完成由自然语言触发的任务。尽管任务高度依赖以往动作和视觉观测，现有研究通常很少考虑中间屏幕快照和屏幕操作所承载的语义信息。为此，本研究提出行动-思考链（Chain-of-Action-Thought，简称 CoAT），它包含对前行动作的描述、当前屏幕，以及更重要的对应执行哪些行动及所选行动带来结果的思考。我们在三种现成的大型多模态模型的零-shot 设置中展示，CoAT 在动作预测方面显著优于先前提出的上下文建模。为进一步促进该研究方向，我们构建了 Android-In-The-Zoo（AITZ）数据集，包含 18,643 对屏幕-动作及其行动链思考注释。实验表明，在我们的 AITZ 数据集上对 1B 模型（即 AUTO-UI-base）进行微调，达到与 CogAgent-Chat-18B 相当的性能。


## 1 Introduction
## 1 引言


Nowadays, smartphones have become an essential part of daily lives. AUTOnomous operation of Graphical User Interfaces (GUI) by human instructions can substantially simplify everyday routines. Such tasks, formalized as GUI Navigation (Li et al., 2020b; Sun et al., 2022b), therefore carry immense social importance, especially for people with physical disabilities (Nanavati et al., 2023).
如今，智能手机已成为日常生活的关键组成部分。由人类指令实现的图形用户界面（GUI）自主操作，能显著简化日常事务。因此，作为 GUI 导航（Li 等，2020b；Sun 等，2022b）形式的任务，具有巨大的社会意义，尤其对行动能力受限者（Nanavati 等，2023）尤为重要。


Recent works have explored prompt engineering (Wen et al., 2023; Zhang and Zhang, 2023), finetuning (Hong et al., 2023) and memory augmentation (Lee et al., 2023) to utilize the capability of large language models (LLM) on interactive mobile environments. However, progress is held back due to the scarcity of attention paid on the underlying semantics of smartphone operations. GUI navigation usually entails initially observing the screen, considering the next action to take, and reflecting on the outcome of that action (Zhang et al., 2024a). Previous works (Zhang and Zhang, 2023; Cheng et al., 2024) ignore the logic behind diverse actions on the screen, concentrating solely on the coordinates of an operation, such as "click on (0.17, 0.89)", which is quite insufficient. As shown in Figure 1, we need explicit explanations for the intermediate results during GUI navigation:
最近的工作探索了提示工程（Wen 等，2023；Zhang 与 Zhang，2023）、微调（Hong 等，2023）与记忆增强（Lee 等，2023），以利用大型语言模型（LLM）在交互式移动环境中的能力。然而，由于对智能手机操作背后的语义关注不足，进展受限。GUI 导航通常包含先观察屏幕、考虑下一步行动、再反思该行动的结果等步骤（Zhang 等，2024a）。此前的工作（Zhang 与 Zhang，2023；Cheng 等，2024）忽略了屏幕上不同操作背后的逻辑，只关注操作的坐标，例如“在（0.17，0.89）处单击”，这远远不够。如图 1 所示，我们在 GUI 导航过程中需要对中间结果给出明确解释：


- Screen Context: In which app or interface did the action occur? This helps to learn the background and possible effects of the action.
- 屏幕上下文：操作发生在哪个应用或界面？这有助于学习操作的背景与可能的影响。


- Action Think: Why the specific action on the current screen is chosen? Does it facilitate the completion of user query? Such thinking process helps the agent to better capture the user intent.
- 行动思考：为何在当前屏幕选择了特定的动作？它是否有助于完成用户查询？这样的思考过程有助于代理更好地捕捉用户意图。


- Action Target: Which UI element is the action operating on? A button, an icon, or a link?
- 行动目标：该动作作用于哪个 UI 元素？是按钮、图标还是链接？


- Action Result: What change will this action cause? Understanding this ensures the consistency of the agent decision-making process.
- 行动结果：此动作会引起哪些变化？理解这一点有助于确保代理决策过程的一致性。


In order to equip existing GUI agents with such capability, we summarize the series of navigation steps as Chain-of-Action-Thought (CoAT), including the screen description, the thinking process about the next action, the textual next action description, and the possible action outcomes. Screen description, together with the screenshots, provides the agent with information basis for decision-making (Wang et al., 2021). Whereas action think, action description and action result demonstrate the rationale between operations. Equipped with CoAT, we achieve significant improvements in the action prediction across three off-the-shelf large multimodal models (LMM) compared to standard context prompting, including GPT-4V (OpenAI, 2023), Gemini-Pro-Vision (Team et al., 2023) and Qwen-VL-Max (Bai et al., 2023).
为使现有 GUI 代理具备上述能力，我们将一系列导航步骤总结为行动-思考链（CoAT），包括屏幕描述、对下一步行动的思考过程、文本形式的下一步行动描述以及可能的行动结果。屏幕描述以及屏幕截图为代理提供决策的信息基础（Wang 等，2021）。而行动思考、行动描述与行动结果展示了操作之间的理论依据。凭借 CoAT，我们在三种现成的大型多模态模型（LMM）上的动作预测相较于标准上下文提示取得显著提升，包括 GPT-4V（OpenAI，2023）、Gemini-Pro-Vision（Team 等，2023）和 Qwen-VL-Max（Bai 等，2023）。


---



* This work was done during this author's internship at Shanghai Research Center of Huawei Inc.
* 本研究是在华为公司上海研究中心实习期间完成的。


† Corresponding Author.
† 通信作者。


---



<img src="https://cdn.noedgeai.com/bo_d5vch5ref24c73bqi1u0_1.jpg?x=186&y=192&w=1268&h=481&r=0"/>



Figure 1: The working process of Chain-of-Action-Thought. The agent will observe the screen, think about actions on current screen to fulfill the user query, describe its next action, act and finally reflect on action results.
图1：Chain-of-Action-Thought 的工作过程。代理将观察屏幕，思考对当前屏幕的行动以完成用户查询，描述其下一步行动，执行并最终对行动结果进行反思。


<table><tr><td rowspan="2">Dataset</td><td rowspan="2">#Episodes</td><td rowspan="2">#Unique Instructions</td><td rowspan="2">#Apps</td><td rowspan="2">#Steps</td><td colspan="5">Annotation</td></tr><tr><td>screen desc</td><td>action coord</td><td>action desc</td><td>action thinking</td><td>episode feasibility</td></tr><tr><td>PixelHelp (Li et al., 2020b)</td><td>187</td><td>187</td><td>4</td><td>~4</td><td></td><td>✓</td><td></td><td></td><td></td></tr><tr><td>MoTIF (Burns et al., 2021)</td><td>4707</td><td>270</td><td>125</td><td>4.5</td><td></td><td>✓</td><td></td><td></td><td>✓</td></tr><tr><td>UGIF (Venkatesh et al., 2022)</td><td>523</td><td>480</td><td>12</td><td>6.3</td><td></td><td>✓</td><td>✓</td><td></td><td></td></tr><tr><td>Meta-GUI (Sun et al., 2022a)</td><td>4684</td><td>1125</td><td>11</td><td>5.3</td><td></td><td>✓</td><td></td><td></td><td></td></tr><tr><td>AITW (Rawles et al., 2023)</td><td>715142</td><td>30378</td><td>357+</td><td>6.5</td><td></td><td>✓</td><td></td><td></td><td></td></tr><tr><td>AITZ (Ours)</td><td>2504</td><td>2504</td><td>70+</td><td>7.5</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr></table>
<table><tbody><tr><td rowspan="2">数据集</td><td rowspan="2">#剧集</td><td rowspan="2">#唯一指令</td><td rowspan="2">#应用</td><td rowspan="2">#步骤</td><td colspan="5">标注</td></tr><tr><td>屏幕描述</td><td>动作坐标</td><td>动作描述</td><td>动作思考</td><td>剧集可行性</td></tr><tr><td>PixelHelp (Li 等人, 2020b)</td><td>187</td><td>187</td><td>4</td><td>~4</td><td></td><td>✓</td><td></td><td></td><td></td></tr><tr><td>MoTIF (Burns 等人, 2021)</td><td>4707</td><td>270</td><td>125</td><td>4.5</td><td></td><td>✓</td><td></td><td></td><td>✓</td></tr><tr><td>UGIF (Venkatesh 等人, 2022)</td><td>523</td><td>480</td><td>12</td><td>6.3</td><td></td><td>✓</td><td>✓</td><td></td><td></td></tr><tr><td>Meta-GUI (Sun 等人, 2022a)</td><td>4684</td><td>1125</td><td>11</td><td>5.3</td><td></td><td>✓</td><td></td><td></td><td></td></tr><tr><td>AITW (Rawles 等人, 2023)</td><td>715142</td><td>30378</td><td>357+</td><td>6.5</td><td></td><td>✓</td><td></td><td></td><td></td></tr><tr><td>AITZ（我们）</td><td>2504</td><td>2504</td><td>70+</td><td>7.5</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr></tbody></table>


Table 1: Comparison of AITZ to existing Android GUI datasets. We consider the number of episodes, instructions, related apps, average steps and granularity of annotations. Specifically, action semantics includes action descriptions and action thinkings, while episode feasibility refers to the success verification of collected episodes.
表 1：AITZ 与现有 Android GUI 数据集的比较。我们考量剧集数量、指令、相关应用、平均步骤与注释粒度。具体而言，动作语义包括动作描述与动作思考，而剧集可行性指收集剧集的成功验证。


However, complex context modeling of language models emerges at a large model scale (Zhang et al., 2023). Without high quality CoAT-driven data, smaller models can not possess the desired ability through fine-tuning. To remedy this blank, we propose a new dataset Android-In-The-Zoo (AITZ). AITZ is the first dataset that connects the perception (of screen layouts and UI elements) and the cognition (of action decision-making process) together. Based on the screen episodes from (Rawles et al., 2023), we leverage the most-capable proprietary model, GPT-4V (OpenAI, 2023), and state-of-the-art icon detection model (Liu et al., 2018) to generate candidate answers for the screen descriptions, action thinkings and next action descriptions. These candidates are further validated and refined by human to guarantee alignment with the screenshots. Finally, AITZ contains about 19,000 screenshots spanning over 70 Android apps, coupled with $4 \times$ useful annotations compared with action coordinate labels only. We verify the effectiveness of CoAT by additionally finetuning a small multimodal agent from scratch on our AITZ dataset. Experiments show that our proposed chain-of-action-thought improves both the goal progress and the learning efficiency of GUI agents.
然而，语言模型的复杂上下文建模在大模型规模下逐渐显现（Zhang 等，2023）。若无高质量的 CoAT 驱动数据，较小的模型也难以通过微调获得所需能力。为解决这一空缺，我们提出一个新数据集 Android-In-The-Zoo (AITZ)。AITZ 是首个将感知（屏幕布局与 UI 元素）与认知（动作决策过程）结合在一起的数据集。基于 Rawles 等，2023 年的屏幕剧集，我们利用最具能力的专有模型 GPT-4V（OpenAI，2023）以及最先进的图标检测模型（Liu 等，2018）为屏幕描述、动作思考和下一个动作描述生成候选答案。随后由人类进一步验证和润色，以确保与截图对齐。最终，AITZ 包含约 19,000 张截图，覆盖 70 款以上的 Android 应用，并附带比仅有动作坐标标签更有用的 $4 \times$ 注解。我们通过在 AITZ 数据集上从零开始微调一个小型多模态代理来进一步验证 CoAT 的有效性。实验表明，我们提出的行动思维链（CoAT）同时提升 GUI 代理的目标进展与学习效率。


Our contributions are summarized as follows:
我们的贡献概要如下：


- We propose Chain-of-Action-Thought (CoAT), a novel prompting paradigm to explicitly capture the underlying semantics during navigation actions, allowing GUI agents to perceive, think and decide in an interleaved manner.
- 我们提出行动思维链（CoAT），一种新颖的提示范式，能够在导航动作过程中显式捕捉潜在语义，使 GUI 代理能够以互相交错的方式感知、思考与决策。


- We construct Android-In-The-Zoo (AITZ), the first and largest fine-grained dataset in the Android GUI navigation field. AITZ consisting of 2504 unique instructions and 18,643 screen-action pairs together with four types of semantic annotations, spanning over 70 Android apps.
- 我们构建 Android-In-The-Zoo（AITZ），在 Android GUI 导航领域首个也是最大的细粒度数据集。AITZ 由 2504 条独特指令、18,643 对屏幕-动作对以及四种语义注释组成，覆盖 70 款以上的 Android 应用。


- We conduct both zero-shot and fine-tuning evaluation on the AITZ dataset, validating the necessity and effectiveness of proposed chain-of-action-thought prompting.
- 我们在 AITZ 数据集上进行零样本和微调评估，验证所提行动思维链提示的必要性与有效性。


<img src="https://cdn.noedgeai.com/bo_d5vch5ref24c73bqi1u0_2.jpg?x=192&y=192&w=750&h=369&r=0"/>



Figure 2: Chain-of-Action-Thought compared with three typical prompting methods for GUI tasks, including Standard (Rawles et al., 2023) prompting, Chain-of-Action (Zhang and Zhang, 2023) prompting and Chain-of-Thought (Wei et al., 2022) prompting.
图 2：行动思维链（CoAT）与 GUI 任务三种典型提示方法的比较，包括标准提示（Rawles 等，2023）、行动链提示（Zhang 与 Zhang，2023）和思维链提示（Wei 等，2022）。


<table><tr><td rowspan="2">Prompt</td><td rowspan="2">Metric</td><td colspan="3">Model</td></tr><tr><td>QwenVL</td><td>Gemini-PV</td><td>GPT-4V</td></tr><tr><td rowspan="2">CoA</td><td>hit</td><td>94.5</td><td>99.8</td><td>99.3</td></tr><tr><td>acc</td><td>44.4</td><td>47.7</td><td>62.8</td></tr><tr><td rowspan="2">CoT</td><td>hit</td><td>95.6</td><td>97.5</td><td>97.1</td></tr><tr><td>acc</td><td>49.4</td><td>52.0</td><td>64.1</td></tr><tr><td rowspan="2">CoAT</td><td>hit</td><td>96.3</td><td>96.4</td><td>98.2</td></tr><tr><td>acc</td><td>52.4</td><td>54.5</td><td>73.5</td></tr></table>
<table><tbody><tr><td rowspan="2">提示</td><td rowspan="2">指标</td><td colspan="3">模型</td></tr><tr><td>QwenVL</td><td>Gemini-PV</td><td>GPT-4V</td></tr><tr><td rowspan="2">CoA</td><td>命中</td><td>94.5</td><td>99.8</td><td>99.3</td></tr><tr><td>准确</td><td>44.4</td><td>47.7</td><td>62.8</td></tr><tr><td rowspan="2">思维链</td><td>命中</td><td>95.6</td><td>97.5</td><td>97.1</td></tr><tr><td>准确</td><td>49.4</td><td>52.0</td><td>64.1</td></tr><tr><td rowspan="2">思考链</td><td>命中</td><td>96.3</td><td>96.4</td><td>98.2</td></tr><tr><td>准确</td><td>52.4</td><td>54.5</td><td>73.5</td></tr></tbody></table>


Table 2: Quantitative comparison of three prompting methods on Qwen-VL-Max, Gemini-1.0-Pro-Vision and GPT-4V. CoA and CoT are short for chain-of-action and chain-of-thought, respectively. "hit" means format hit rate, and "acc" means accuracy.
表2：在 Qwen-VL-Max、Gemini-1.0-Pro-Vision 与 GPT-4V 上三种提示方法的定量比较。CoA 与 CoT 分别是行动链与思维链的缩写。"hit" 指格式命中率，"acc" 指准确率。


## 2 Chain-of-Action-Thought (CoAT)
## 2 链路行动思考（CoAT）


### 2.1 Definition
### 2.1 定义


Consider a general GUI navigation agent with a user query $u \in  \mathcal{U}$ to solve. At time step $t$ ,an agent receives a screenshot observation ${o}_{t} \in  \mathcal{O}$ from the environment and takes an action ${a}_{t} \in  \mathcal{A}$ following some policy $\pi \left( {{a}_{t} \mid  {o}_{t},{h}_{t - 1},u}\right)$ where ${h}_{t - 1} = \; \left( {{o}_{1},{a}_{1},\ldots ,{o}_{t - 1},{a}_{t - 1}}\right)$ is the history for the agent. Directly learning the policy is challenging as the relations between history, current observations, and possible actions are highly implicit. For example, knowing the search bar is already active is necessary for an agent to make the next action decision to type text. Therefore, we define Chain-of-Action-Thought (CoAT) as a shortcut to comprehend the interaction dynamics during navigation.
考虑一个通用 GUI 导航代理，用户请求 $u \in  \mathcal{U}$ 来解决问题。在时间步 $t$，代理从环境接收一个屏幕截图观测 ${o}_{t} \in  \mathcal{O}$，并按照某个策略 $\pi \left( {{a}_{t} \mid  {o}_{t},{h}_{t - 1},u}\right)$ 采取动作 ${a}_{t} \in  \mathcal{A}$，其中 ${h}_{t - 1} = \; \left( {{o}_{1},{a}_{1},\ldots ,{o}_{t - 1},{a}_{t - 1}}\right)$ 表示代理的历史。直接学习策略具有挑战性，因为历史、当前观测和可能动作之间的关系高度隐含。例如，知道搜索栏已经处于激活状态对于代理决定下一步输入文本很有必要。因此，我们将链路行动思考（CoAT）定义为理解导航过程中的交互动态的一种捷径。


The basic components of CoAT, marked as grey-bordered boxes on the right side of Figure 1, are:
CoAT 的基本组成部分，如图1右侧灰框所示，分别是：


- Screen Description (SD) describes the main content of the given screenshots, including the screen type and primary apps or widgets presented. Screen description provides the textual context for further decision-making.
- 屏幕描述（SD）描述给定屏幕截图的主要内容，包括屏幕类型以及呈现的主要应用或小部件。屏幕描述为进一步决策提供文本上下文。


- Action Think (AT) analyzes the user query and current screen, and combines the history information to infer the possible actions that help to fulfil the target. Mathematically, action think provides a conditional probability $p\left( {{AT} \mid  {o}_{t},u,{h}_{t - 1}}\right)$ . If the action think summarizes the current state perfectly and contains reasonable action plans, the decision can be made by calculating $p\left( {{a}_{t} \mid  {AT}}\right)$ .
- 动作思考（AT）分析用户请求和当前屏幕，并将历史信息结合起来推断有助于实现目标的可能动作。数学上，动作思考提供一个条件概率 $p\left( {{AT} \mid  {o}_{t},u,{h}_{t - 1}}\right)$ 。如果动作思考能完美总结当前状态并包含合理的行动计划，则可通过计算 $p\left( {{a}_{t} \mid  {AT}}\right)$ 来做出决策。


- Next Action Description (AD) illustrates the UI element or screen functions being operated, i.e. "click on the shopping cart icon" or "scroll up to open the app drawer". Action description helps to form a readable action history.
- 下一步动作描述（AD）说明正在操作的 UI 元素或屏幕功能，即“点击购物车图标”或“向上滚动以打开应用抽屉”等。动作描述有助于形成可读的动作历史。


- Action Result (AR) connects the current screen ${o}_{t}$ and next action ${a}_{t}$ to the future observations ${o}_{t + 1}$ ,by synthesizing the action outcomes after comparing the screenshot before and after the action. Usually,at time step $t$ ,we combine last action result $A{R}_{t - 1}$ with previous action descriptions to form a continuous and consistent history.
- 动作结果（AR）通过比较动作前后的截图，将当前屏幕 ${o}_{t}$ 与下一动作 ${a}_{t}$ 连接到未来观测 ${o}_{t + 1}$，从而综合行动结果。通常，在时间步 $t$ 时，我们将上一步动作结果 $A{R}_{t - 1}$ 与前面的动作描述结合起来，形成连续且一致的历史。


Since each CoAT component carries useful semantics, it is free to combine them according to language models used. Our further experiments will validate the effectiveness and flexibility of the application of proposed CoAT framework.
由于每个 CoAT 组成部分都承载有用的语义，我们可以根据所用的语言模型自由地组合它们。我们后续的实验将验证所提 CoAT 框架在应用上的有效性与灵活性。


### 2.2 Comparison
### 2.2 对比


Figure 2 compares proposed CoAT with Standard (Rawles et al., 2023), Chain-of-Action (CoA) (Zhang and Zhang, 2023) and Chain-of-Thought (CoT) (Wei et al., 2022) prompting methods. The proposed CoAT carries explicitly more semantic information about the screen and actions. To further validate the effectiveness of CoAT, we conduct a preliminary experiment on 50 episodes randomly sampled from AITW (Rawles et al., 2023) dataset. We select three most capable proprietary models, i.e. GPT-4V (OpenAI, 2023), Gemini-Pro-Vision (Team et al., 2023) and Qwen-VL-Max (Bai et al., 2023), to be the GUI agent and apply different prompting methods on them. To ensure an accurate measurement of action prediction accuracy, we use set-of-mark tagging method (Yan et al., 2023) to annotate UI elements on screen. As shown in Table 2, agents with CoAT surpass CoA and CoT by a large margin. Moreover, GPT- 4V demonstrates optimal performance, making it a good collaborator for subsequent data collection.
图2 将所提的 CoAT 与标准方法（Rawles 等，2023）、行动链（CoA，Zhang 与 Zhang，2023）以及思维链（CoT，Wei 等，2022）进行对比。所提的 CoAT 显示出对屏幕及动作的显式语义信息更多。为进一步验证 CoAT 的有效性，我们在从 AITW（Rawles 等，2023）数据集中随机抽取的 50 个剧集上进行初步实验。我们选择三种最具能力的专有模型，即 GPT-4V（OpenAI，2023）、Gemini-Pro-Vision（Team 等，2023）和 Qwen-VL-Max（Ba i 等，2023），作为 GUI 代理并在它们上应用不同的提示方法。为确保对动作预测准确性的测量，我们使用集合标记标注法（Yan 等，2023）对屏幕上的 UI 元素进行注释。如表2所示，使用 CoAT 的代理显著优于 CoA 与 CoT。此外，GPT-4V 展现出最佳性能，成为后续数据收集的良好协作者。


<img src="https://cdn.noedgeai.com/bo_d5vch5ref24c73bqi1u0_3.jpg?x=211&y=196&w=1223&h=522&r=0"/>



Figure 3: AITZ data collection pipeline. During sampling process, human annotators first verify the clustering results, and then check whether the sampled episode successfully complete the query. During annotation process, human annotators examine and correct the GPT generated semantic descriptions.
图3：AITZ 数据采集流程。在采样过程中，人工标注者先验证聚类结果，然后检查采样的剧集是否成功完成查询。在标注过程中，人工标注者检查并纠正 GPT 生成的语义描述。


<img src="https://cdn.noedgeai.com/bo_d5vch5ref24c73bqi1u0_3.jpg?x=190&y=843&w=1271&h=316&r=0"/>



Figure 4: Three typical cases of wrong episodes in AITW (Rawles et al., 2023) dataset. We take the task to 'check the settings for the spotify app' as example. There exists 15 episodes corresponding to this instruction, and among them 13 do not actually open the spotify app. This highlights the reasonability to perform data validation.
图 4：AITW（Rawles 等，2023）数据集中错误 episodio 的三种典型情况。以“检查 Spotify 应用设置”为例。共有 15 条与该指令相关的 episode，其中 13 条并未实际打开 Spotify 应用。这凸显了数据校验的合理性。


## 3 Android in the Zoo (AITZ)
## 3 Android in the Zoo (AITZ)


There is a lack of data that captures the underlying semantics of the CoAT paradigm, hindering small models from obtaining this ability. We therefore propose to construct a novel, high-quality and comprehensive dataset to remedy this blank.
缺乏能捕捉 CoAT 范式底层语义的数据，妨碍小模型获得此能力。因此我们提出构建一个新颖的、高质量且全面的数据集来补齐这一空缺。


### 3.1 Data Collection
### 3.1 数据收集


Instruction Sampling We build our dataset upon the currently most scaled Android GUI navigation dataset, AITW (Rawles et al., 2023). AITW dataset has 715k episodes spanning 30k unique instructions. We observe that (1) the diversity of instructions mainly comes from the subset WEBSHOP-PING, and these instructions have clear templates, as shown in Table 3; (2) the richness of episodes results from subset GOOGLEAPPS, where each instruction corresponds to more than 2000 episodes. However, within the AITW dataset, there exist numerous mismatch cases between the observed screenshots and the instructions (see Figure 4). Thus, we sample the instructions and episodes to reduce redundancy and filter the error cases by using a subset-specific sampling strategy:
指令采样 我们将数据集建立在当前规模最大的 Android GUI 导航数据集之一，AITW（Rawles 等，2023）。AITW 数据集包含 71.5 万个 episode，覆盖 3 万条唯一指令。我们观察到：(1) 指令的多样性主要来自子集 WEBSHOP-PING，这些指令具有清晰的模板，如表 3 所示；(2) episode 的丰富性源自子集 GOOGLEAPPS，其中每条指令对应超过 2000 条 episode。然而，在 AITW 数据集中，观测到的截图与指令之间存在大量不匹配的情况（见图 4）。因此，我们对指令和 episode 进行抽样，以降低冗余并使用子集特定的抽样策略筛选错误案例：


- For subset GENERAL, GOOGLEAPPS and INSTALL, as there are few unique instructions in each subset,we uniformly sample $x$ samples for each instruction $\left( {x = 3,5,3\text{ respectively }}\right)$ .
- 对子集 GENERAL、GOOGLEAPPS 和 INSTALL，由于每个子集中的唯一指令较少，我们对每条指令均匀抽取 $x$ 条样本 $\left( {x = 3,5,3\text{ respectively }}\right)$ 。


- For subset WEBSHOPPING, we conduct balanced sampling on the categories of shopping web-sites/apps and the objects involved.
- 对子集 WEBSHOPPING，在购物网站/应用类别及所涉及对象上进行平衡抽样。


- For subset SINGLE, as the instructions are diverse and cluttered, we perform clustering and then conduct balanced sampling on the clustered data.
- 对子集 SINGLE，由于指令多样且混乱，我们进行聚类后对聚类数据进行平衡抽样。


This results in a total number of 3461 unique instructions, corresponding to 7180 episodes. We recruit ten annotators to manually verify the correctness of the sampled episodes. Finally, for 5147 successful episodes, we randomly select one episode paired with each unique instruction.
这使总共得到 3461 条唯一指令，对应 7180 条 episode。我们招募十位标注人员对抽样的 episode 的正确性进行人工核验。最终，对 5147 条成功的 episode，我们在每个唯一指令配对中随机选取一条 episode。


Semantic Annotation It is crucial for GUI agents to understand the screen information and make decisions accordingly. To mitigate the lack of such detailed data, we leverage GPT-4V through Azure-API as the navigation expert and prompt it to do the screen description, action thinking, next action description and action result summarization tasks. Note that the amount of information used to generate semantic annotations varies. For example, the screen description is query-independent, whereas for next action description, both the query and the coordinate of golden actions are provided for reference (see Appendix A. 2 for more details). Thanks to the correctness check at instruction sampling stage, the golden actions have all been verified. We then recruit three experts who have a good understanding of UI elements as annotators to examine whether the generated action description, action thinking and action result match the golden actions. Once inconsistency is found, annotators will manually revise the action descriptions, and enforce GPT-4V to regenerate the action thoughts and action results based on the correct descriptions.
语义标注 对 GUI 代理而言，理解屏幕信息并据此做出决策至关重要。为缓解缺乏此类详细数据的问题，我们通过 Azure-API 调用 GPT-4V 作为导航专家，指示其完成屏幕描述、行动思维、下一步行动描述和行动结果摘要等任务。请注意，用于生成语义标注的信息量会有所不同，例如屏幕描述是与查询无关的，而下一步行动描述则会提供查询和黄金动作的坐标以供参考（详见附录 A.2 的更多细节）。由于指令采样阶段的正确性校验，黄金动作已全部核验。接着，我们招募三位对 UI 元素有良好理解的专家作为标注者，检查生成的行动描述、行动思维与行动结果是否与黄金动作相匹配。一旦发现不一致，标注者将手动修正行动描述，并强制 GPT-4V 根据正确描述重新生成行动思维和行动结果。


<table><tr><td>Shopping web/app</td><td>Instruction Template</td><td>#Instructions</td><td>#Episodes</td></tr><tr><td rowspan="6">amazon</td><td>add something to the cart on amazon</td><td>80</td><td>180</td></tr><tr><td>clear/empty cart, then add something to the cart on amazon</td><td>111</td><td>135</td></tr><tr><td>clear/empty cart, search for something, select the first entry and add to cart on amazon</td><td>105</td><td>124</td></tr><tr><td>clear cart, search for something, select the first entry, add to cart on amazon, and checkout</td><td>110</td><td>135</td></tr><tr><td>show/view the shopping cart, search for something on amazon and add it to the cart</td><td>42</td><td>52</td></tr><tr><td>show/view the shopping cart, add something to the cart on amazon, then checkout</td><td>59</td><td>75</td></tr></table>
<table><tbody><tr><td>购物网站/应用</td><td>指令模板</td><td>＃指令</td><td>＃剧集</td></tr><tr><td rowspan="6">amazon</td><td>在 amazon 上把某物加入购物车</td><td>80</td><td>180</td></tr><tr><td>清空/清空购物车，然后在 amazon 上加入某物</td><td>111</td><td>135</td></tr><tr><td>清空/清空购物车，搜索某物，选择第一条并加入购物车在 amazon</td><td>105</td><td>124</td></tr><tr><td>清空购物车，搜索某物，选择第一条，加入购物车在 amazon，然后结账</td><td>110</td><td>135</td></tr><tr><td>显示/查看购物车，在 amazon 上搜索某物并将其加入购物车</td><td>42</td><td>52</td></tr><tr><td>显示/查看购物车，在 amazon 上加入某物到购物车，然后结账</td><td>59</td><td>75</td></tr></tbody></table>


Table 3: An example of repeating instructions with the same template on WEBSHOPPING subset of AITW dataset. We take instructions related to 'amazon' for demonstration. Similar templates can be found for samples related to other shopping websites/apps, including 'bestbuy', 'ebay', 'costco' and 'walmart'.
表 3：在 WEBSHOPPING 子集的 AITW 数据集中使用同一模板重复指令的示例。为演示，我们取与“amazon”相关的指令。其他购物网站/应用相关样本也可找到类似模板，包括“bestbuy”、“ebay”、“costco”和“walmart”。


<img src="https://cdn.noedgeai.com/bo_d5vch5ref24c73bqi1u0_4.jpg?x=194&y=546&w=1206&h=326&r=0"/>



Figure 5: Distributions of (a) the length of three different types of semantic annotations and (b) the phrase frequencies of clicked UI elements on the AITZ dataset. The size of each word corresponds to its tf-idf score.
图 5：（a）三种不同类型语义注释长度的分布，以及（b）在 AITZ 数据集上被点击的 UI 元素短语频次。每个词的大小与其 tf-idf 分数相对应。


### 3.2 Dataset Analysis
### 3.2 数据集分析


We compare our AITZ dataset with the most related Android GUI navigation datasets, including PixelHelp (Li et al., 2020b), MOTIF (Burns et al., 2021), UGIF (Venkatesh et al., 2022), Meta-GUI (Sun et al., 2022b) and AITW (Rawles et al., 2023). Our dataset contains the same magnitude of human demonstration as these smaller datasets, but with a significantly greater richness of instructions. Table 1 demonstrates that our dataset is unique, converting rich semantic information.
我们将我们的 AITZ 数据集与最相关的 Android GUI 导航数据集进行比较，包括 PixelHelp（Li et al., 2020b）、MOTIF（Burns et al., 2021）、UGIF（Venkatesh et al., 2022）、Meta-GUI（Sun et al., 2022b）和 AITW（Rawles et al., 2023）。与这些较小的数据集相比，我们的数据集包含相同量级的人类演示，但在指令的丰富性方面显著更为丰富。表 1 证明了我们数据集的独特性，能够转化丰富的语义信息。


<table><tr><td rowspan="2">Subset</td><td colspan="2">Train</td><td colspan="2">Test</td></tr><tr><td>#Episodes</td><td>#Screens</td><td>#Episodes</td><td>#Screens</td></tr><tr><td>GENERAL</td><td>323</td><td>2405</td><td>156</td><td>1202</td></tr><tr><td>INSTALL</td><td>286</td><td>2519</td><td>134</td><td>1108</td></tr><tr><td>GoogleApps</td><td>166</td><td>1268</td><td>76</td><td>621</td></tr><tr><td>SINGLE</td><td>844</td><td>2594</td><td>0</td><td>0</td></tr><tr><td>WEBSHOPPING</td><td>379</td><td>5133</td><td>140</td><td>1793</td></tr><tr><td>Total</td><td>1998</td><td>13919</td><td>506</td><td>4724</td></tr></table>
<table><tbody><tr><td rowspan="2">子集</td><td colspan="2">训练</td><td colspan="2">测试</td></tr><tr><td>#剧集</td><td>#屏幕</td><td>#剧集</td><td>#屏幕</td></tr><tr><td>通用</td><td>323</td><td>2405</td><td>156</td><td>1202</td></tr><tr><td>安装</td><td>286</td><td>2519</td><td>134</td><td>1108</td></tr><tr><td>Google应用</td><td>166</td><td>1268</td><td>76</td><td>621</td></tr><tr><td>单一</td><td>844</td><td>2594</td><td>0</td><td>0</td></tr><tr><td>网上购物</td><td>379</td><td>5133</td><td>140</td><td>1793</td></tr><tr><td>总计</td><td>1998</td><td>13919</td><td>506</td><td>4724</td></tr></tbody></table>


Table 4: Detailed statistics of the training and test split of AITZ dataset. Since SINGLE subset contains single-step tasks only, we place all SINGLE data and related episodes into the training set.
表 4：AITZ 数据集训练与测试划分的详细统计。由于 SINGLE 子集仅包含单步任务，因此将所有 SINGLE 数据及相关剧集放入训练集。


In Figure 5, we provide statistics of the AITZ dataset, including the distribution of textual lengths and the word cloud of operated UI elements. Specifically, the majority of screen descriptions consist of ${80} \sim  {120}$ words,while most action think have ${30} \sim  {70}$ words. The action result exhibits a narrower range, from 20 to 80 words.
在图 5 中，我们提供 AITZ 数据集的统计信息，包括文本长度分布以及操作界面的词云。具体而言，大多数屏幕描述由 ${80} \sim  {120}$ 个词组成，而大多数动作思考由 ${30} \sim  {70}$ 个词组成。动作结果的词数范围较窄，为 20 到 80 词之间。


## 4 Experimental Setup
## 4 实验设置


### 4.1 Baseline Models
### 4.1 基线模型


CogAgent (Hong et al., 2023) is a LLM-based multimodal GUI agent built upon CogVLM (Wang et al., 2023b). It scales the image resolution up to ${1120} \times  {1120}$ by fusing high-resolution features to every decoder layer with cross-attention. CogAgent is pre-trained on a handful of tasks aimed to adapt it for GUI application scenarios, i.e. text recognition (Schuhmann et al., 2022), visual grounding (Li et al., 2023a), and GUI imagery (Hong et al., 2023). It is further finetuned with GUI tasks on web (Deng et al., 2023) and smartphones (Rawles et al., 2023). Since the training data for CogAgent is not publicly available, we conduct a zero-shot evaluation to assess to what extent CoAT supports the task.
CogAgent（Hong 等，2023）是一个基于 LLM 的多模态 GUI 智能体，建立在 CogVLM（Wang 等，2023b）之上。它通过在每个解码器层与跨注意力融合高分辨率特征，将图像分辨率提升到 ${1120} \times  {1120}$。CogAgent 针对 GUI 应用场景进行少量任务的自监督预训练，即文本识别（Schuhmann 等，2022）、视觉定位（Li 等，2023a）以及 GUI 图像（Hong 等，2023）。随后在网页 GUI 任务（Deng 等，2023）和智能手机 GUI 任务（Rawles 等，2023）上进行微调。由于 CogAgent 的训练数据并非公开可用，我们进行零-shot 评估以评估 CoAT 在多大程度上支持该任务。


<table><tr><td rowspan="3">Mode</td><td rowspan="3">Model</td><td colspan="9">Atomic</td><td>Episodic</td></tr><tr><td rowspan="2">SCROLL</td><td colspan="2">CLICK</td><td colspan="2">TYPE</td><td rowspan="2">PRESS</td><td rowspan="2">STOP</td><td colspan="2">Total</td><td rowspan="2">GP</td></tr><tr><td>type</td><td>match</td><td>type</td><td>match</td><td>type</td><td>match</td></tr><tr><td rowspan="2">ZS</td><td>CogAgent</td><td>56.41</td><td>79.90</td><td>51.50</td><td>67.40</td><td>34.00</td><td>48.30</td><td>4.76</td><td>65.86</td><td>44.52</td><td>13.82</td></tr><tr><td>+CoAT</td><td>70.22</td><td>88.23</td><td>66.15</td><td>45.80</td><td>21.80</td><td>45.95</td><td>24.60</td><td>72.59</td><td>53.28</td><td>17.13</td></tr><tr><td rowspan="2">FT</td><td>AUTO-UI</td><td>74.88</td><td>44.37</td><td>12.72</td><td>73.00</td><td>67.80</td><td>49.09</td><td>60.12</td><td>73.79</td><td>34.46</td><td>6.59</td></tr><tr><td>+CoAT</td><td>61.40</td><td>74.56</td><td>32.20</td><td>87.80</td><td>81.40</td><td>57.70</td><td>74.40</td><td>82.98</td><td>47.69</td><td>14.51</td></tr></table>
<table><tbody><tr><td rowspan="3">模式</td><td rowspan="3">模型</td><td colspan="9">原子</td><td>情境记忆</td></tr><tr><td rowspan="2">滚动</td><td colspan="2">点击</td><td colspan="2">输入</td><td rowspan="2">按下</td><td rowspan="2">停止</td><td colspan="2">总计</td><td rowspan="2">GP</td></tr><tr><td>类型</td><td>匹配</td><td>类型</td><td>匹配</td><td>类型</td><td>匹配</td></tr><tr><td rowspan="2">ZS</td><td>CogAgent</td><td>56.41</td><td>79.90</td><td>51.50</td><td>67.40</td><td>34.00</td><td>48.30</td><td>4.76</td><td>65.86</td><td>44.52</td><td>13.82</td></tr><tr><td>+CoAT</td><td>70.22</td><td>88.23</td><td>66.15</td><td>45.80</td><td>21.80</td><td>45.95</td><td>24.60</td><td>72.59</td><td>53.28</td><td>17.13</td></tr><tr><td rowspan="2">FT</td><td>自动UI</td><td>74.88</td><td>44.37</td><td>12.72</td><td>73.00</td><td>67.80</td><td>49.09</td><td>60.12</td><td>73.79</td><td>34.46</td><td>6.59</td></tr><tr><td>+CoAT</td><td>61.40</td><td>74.56</td><td>32.20</td><td>87.80</td><td>81.40</td><td>57.70</td><td>74.40</td><td>82.98</td><td>47.69</td><td>14.51</td></tr></tbody></table>


Table 5: Main results of CogAgent and AUTO-UI on AITZ dataset. ZS and FT are short for zero-shot and finetuning evaluation, respectively. For CLICK and TYPE actions, which is more complicated than the other three, we additionally report the action type prediction accuracy, marked as 'type' in this table. Total action-matching score is also included. 'GP' is short for goal progress. The best result of each model is marked in bold.
表5：CogAgent 与 AUTO-UI 在 AITZ 数据集上的主要结果。ZS 和 FT 分别代表零-shot 与微调评估。在 CLICK 与 TYPE 动作（比其他三种更复杂）中，我们还额外报告动作类型预测准确度，在本表中标记为 'type'。总动作匹配分数也包含在内。'GP' 为目标进度。每个模型的最佳结果以粗体标注。


AUTO-UI (Zhang and Zhang, 2023) is a specialized model for GUI navigation on AITW (Rawles et al., 2023) dataset. Screen features are extracted by the encoder from BLIP-2 (Li et al., 2023a) and fed into FLAN-Alpaca to decode actions. AUTO-UI is trained on a randomly split training set, covering 80% of AITW episodes, and evaluated on 10% randomly selected testing episodes. As AITW dataset has a large amount of repeating and problematic data, resulting in almost identical distributions between its training and test set. Therefore, we train this model from scratch on the training split of AITZ to validate the necessity and helpfulness of the fine-grained semantic annotations provided by AITZ dataset.
AUTO-UI（Zhang and Zhang，2023）是一个针对 AITW（Rawles 等，2023）数据集的 GUI 导航专用模型。屏幕特征由 BLIP-2（Li 等，2023a）的编码器提取后送入 FLAN-Alpaca 进行动作解码。AUTO-UI 在随机划分的训练集上训练，覆盖 80% 的 AITW 片段，并在随机选择的 10% 测试片段上评估。由于 AITW 数据集存在大量重复和有问题的数据，导致其训练集与测试集之间的分布几乎一致。因此，我们在 AITZ 的训练划分上从头训练该模型，以验证 AITZ 数据集提供的细粒度语义注释的必要性和帮助。


### 4.2 Evaluation Metrics
### 4.2 评估指标


Atomic Metrics Following (Zhang and Zhang, 2023; Hong et al., 2023), we compute the screenwise action-matching score ("match" for short). An action is correct if both the action type and the action details (i.e. scroll direction, typed text, clicked position and pressed button) match the gold ones.
原子级指标：遵循（Zhang and Zhang，2023；Hong 等，2023），我们计算屏幕级的动作匹配分数（简称“match”）。若动作类型与动作细节（即滚动方向、文本输入、点击位置与按下按钮）都与黄金标准相符，则动作正确。


Episodic Metrics As the GUI navigation is a sequential decision-making problem, it is crucial to evaluate the progress made by the agent towards the user query. Therefore, we propose to use goal progress, a metric indicating the relative position where the first error occurs in the sequence.
情节性指标：由于 GUI 导航是一个序贯决策问题，评估智能体朝向用户查询所取得的进展至关重要。因此，我们提出使用目标进度这一指标，表示序列中首次错误发生时的相对位置。


### 4.3 Implementation Details
### 4.3 实现细节


We randomly split 70% episodes as training data, and 30% episodes as testing data (1998/506). It is notable that, as the episodes and instructions in AITZ are distinct, the training set and test set ensure no information leakage. The detailed statistics are in Table 4. For AUTO-UI, we adopt the same weight initialization strategies as (Zhang and Zhang, 2023) and fine-tune the models up to 10 epochs, with a learning rate of 1e-4. For CogA-gent, we utilize the trained model weights from CogAgent-Chat and prompt it to use different semantic annotations. For both models, we keep the original output format unchanged but add extra information to the input or output of these models.
我们随机将 70% 的片段划分为训练数据，30% 的片段划分为测试数据（1998/506）。值得注意的是，由于 AITZ 的片段与指令彼此不同，训练集与测试集确保信息不泄露。详细统计见表4。对于 AUTO-UI，我们采用与（Zhang 和 Zhang，2023）相同的权重初始化策略，微调模型最多 10 轮，学习率为 1e-4。对于 CogAgent，我们利用从 CogAgent-Chat 训练得到的模型权重，并提示其使用不同的语义注释。对于两种模型，我们保持原输出格式不变，但对输入或输出添加额外信息。


## 5 Experiments
## 5 实验


### 5.1 Zero-Shot Evaluation
### 5.1 零-shot 评估


We perform a zero-shot evaluation to investigate the benefit of directly using these screen and action semantics as input. Here, we select CogAgent (Hong et al., 2023) for illustration as it is trained to perform GUI tasks and expected to possess generalization abilities since its foundation language model is CogVLM-7B. We verify the impact of the proposed chain-of-action thought by adding action think to the prompt input of CogAgent. As shown in Table 5, CoAT contributes significant improvements to the overall model performance. Moreover, the first and last line in Table 5 indicate the fact that fine-tuning a small agent with model size $\sim  1\mathrm{\;B}$ (i.e. AUTO-UI-base (Zhang and Zhang, 2023)) using CoAT can obtain comparable performance with a LLM-based agent, demonstrating the strong potential of CoAT on GUI navigation tasks.
我们进行零-shot 评估，以探究直接将这些屏幕与操作语义作为输入的收益。在此，我们选择 CogAgent（Hong 等，2023）作为示例，因为它经过训练用于执行 GUI 任务，且其基础语言模型为 CogVLM-7B，预计具备泛化能力。我们通过在 CogAgent 的提示输入中加入行动思维来验证所提出的行动链思考（CoAT）的影响。如表 5 所示，CoAT 对整体模型性能有显著提升。此外，表 5 的首行和末行显示，使用 CoAT 对一个规模为 $\sim  1\mathrm{\;B}$ 的小型代理进行微调（即 AUTO-UI-base（Zhang 和 Zhang，2023））后，可以获得与基于 LLM 的代理相当的性能，显示了 CoAT 在 GUI 导航任务上的强大潜力。


A more detailed comparison between CogAgent and AUTO-UI on model architecture, training data and performance can be found in Appendix C.2.
CogAgent 与 AUTO-UI 在模型架构、训练数据与性能方面的更详细对比可在附录 C.2 中找到。


<table><tr><td rowspan="3"></td><td colspan="4">Semantic Annotations</td><td colspan="9">Atomic</td><td>Episodic</td></tr><tr><td colspan="2">input</td><td colspan="2">output</td><td rowspan="2">SCROLL</td><td colspan="2">CLICK</td><td colspan="2">TYPE</td><td rowspan="2">PRESS</td><td rowspan="2">STOP</td><td colspan="2">Total</td><td rowspan="2">GP</td></tr><tr><td>SD</td><td>PAR</td><td>AT</td><td>AD</td><td>type</td><td>match</td><td>type</td><td>match</td><td>type</td><td>match</td></tr><tr><td>(1)</td><td></td><td></td><td></td><td></td><td>74.88</td><td>44.37</td><td>12.72</td><td>73.00</td><td>67.80</td><td>49.09</td><td>60.12</td><td>73.79</td><td>34.46</td><td>6.59</td></tr><tr><td>(2)</td><td>✓</td><td></td><td></td><td></td><td>87.85</td><td>49.52</td><td>20.21</td><td>81.40</td><td>64.20</td><td>53.52</td><td>49.80</td><td>80.55</td><td>39.33</td><td>10.71</td></tr><tr><td>(3)</td><td></td><td>✓</td><td></td><td></td><td>78.54</td><td>63.23</td><td>29.39</td><td>85.60</td><td>79.40</td><td>55.35</td><td>79.17</td><td>83.91</td><td>48.35</td><td>14.06</td></tr><tr><td>(4)</td><td>✓</td><td>✓</td><td></td><td></td><td>80.53</td><td>59.10</td><td>25.95</td><td>80.60</td><td>62.40</td><td>55.09</td><td>57.14</td><td>81.77</td><td>42.38</td><td>13.64</td></tr><tr><td>(5)</td><td></td><td></td><td>✓</td><td></td><td>80.87</td><td>43.09</td><td>13.16</td><td>89.80</td><td>78.60</td><td>46.74</td><td>25.00</td><td>73.45</td><td>32.68</td><td>9.08</td></tr><tr><td>(6)</td><td></td><td></td><td></td><td>✓</td><td>57.74</td><td>59.39</td><td>17.47</td><td>72.80</td><td>67.00</td><td>49.87</td><td>61.71</td><td>72.21</td><td>35.18</td><td>8.37</td></tr><tr><td>(7)</td><td></td><td></td><td>✓</td><td>✓</td><td>27.62</td><td>75.06</td><td>28.85</td><td>86.60</td><td>76.60</td><td>49.61</td><td>42.66</td><td>75.42</td><td>36.91</td><td>11.96</td></tr><tr><td>(8)</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>31.28</td><td>81.29</td><td>33.21</td><td>79.40</td><td>61.40</td><td>51.70</td><td>35.12</td><td>77.54</td><td>37.66</td><td>13.34</td></tr><tr><td>(9)</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>61.40</td><td>74.56</td><td>32.20</td><td>87.80</td><td>81.40</td><td>57.70</td><td>74.40</td><td>82.98</td><td>47.69</td><td>14.51</td></tr><tr><td>(10)</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>32.45</td><td>82.46</td><td>32.99</td><td>80.40</td><td>59.20</td><td>52.48</td><td>34.33</td><td>78.32</td><td>37.42</td><td>13.90</td></tr></table>
<table><tbody><tr><td rowspan="3"></td><td colspan="4">语义注释</td><td colspan="9">原子</td><td>情节性</td></tr><tr><td colspan="2">输入</td><td colspan="2">输出</td><td rowspan="2">滚动</td><td colspan="2">点击</td><td colspan="2">类型</td><td rowspan="2">按下</td><td rowspan="2">停止</td><td colspan="2">总计</td><td rowspan="2">GP</td></tr><tr><td>SD</td><td>PAR</td><td>AT</td><td>AD</td><td>type</td><td>match</td><td>type</td><td>match</td><td>type</td><td>match</td></tr><tr><td>(1)</td><td></td><td></td><td></td><td></td><td>74.88</td><td>44.37</td><td>12.72</td><td>73.00</td><td>67.80</td><td>49.09</td><td>60.12</td><td>73.79</td><td>34.46</td><td>6.59</td></tr><tr><td>(2)</td><td>✓</td><td></td><td></td><td></td><td>87.85</td><td>49.52</td><td>20.21</td><td>81.40</td><td>64.20</td><td>53.52</td><td>49.80</td><td>80.55</td><td>39.33</td><td>10.71</td></tr><tr><td>(3)</td><td></td><td>✓</td><td></td><td></td><td>78.54</td><td>63.23</td><td>29.39</td><td>85.60</td><td>79.40</td><td>55.35</td><td>79.17</td><td>83.91</td><td>48.35</td><td>14.06</td></tr><tr><td>(4)</td><td>✓</td><td>✓</td><td></td><td></td><td>80.53</td><td>59.10</td><td>25.95</td><td>80.60</td><td>62.40</td><td>55.09</td><td>57.14</td><td>81.77</td><td>42.38</td><td>13.64</td></tr><tr><td>(5)</td><td></td><td></td><td>✓</td><td></td><td>80.87</td><td>43.09</td><td>13.16</td><td>89.80</td><td>78.60</td><td>46.74</td><td>25.00</td><td>73.45</td><td>32.68</td><td>9.08</td></tr><tr><td>(6)</td><td></td><td></td><td></td><td>✓</td><td>57.74</td><td>59.39</td><td>17.47</td><td>72.80</td><td>67.00</td><td>49.87</td><td>61.71</td><td>72.21</td><td>35.18</td><td>8.37</td></tr><tr><td>(7)</td><td></td><td></td><td>✓</td><td>✓</td><td>27.62</td><td>75.06</td><td>28.85</td><td>86.60</td><td>76.60</td><td>49.61</td><td>42.66</td><td>75.42</td><td>36.91</td><td>11.96</td></tr><tr><td>(8)</td><td>✓</td><td></td><td>✓</td><td>✓</td><td>31.28</td><td>81.29</td><td>33.21</td><td>79.40</td><td>61.40</td><td>51.70</td><td>35.12</td><td>77.54</td><td>37.66</td><td>13.34</td></tr><tr><td>(9)</td><td></td><td>✓</td><td>✓</td><td>✓</td><td>61.40</td><td>74.56</td><td>32.20</td><td>87.80</td><td>81.40</td><td>57.70</td><td>74.40</td><td>82.98</td><td>47.69</td><td>14.51</td></tr><tr><td>(10)</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>32.45</td><td>82.46</td><td>32.99</td><td>80.40</td><td>59.20</td><td>52.48</td><td>34.33</td><td>78.32</td><td>37.42</td><td>13.90</td></tr></tbody></table>


Table 6: Ablation study of different semantic annotation components on AUTO-UI. SD and PAR mean screen description and previous action result, whereas AT and AD represent action think and next action description, respectively. For CLICK and TYPE actions, which is more complicated than the other three, we additionally report the action type prediction accuracy, marked as 'type' in this table. Total action-matching score is also included. 'GP' is short for goal progress. The best result is marked in bold while the runner-up is underlined.
表6：对AUTO-UI中不同语义注释组件的消融研究。SD和PAR分别表示屏幕描述和上一步操作结果，而AT和AD分别代表行动思路与下一步行动描述。对于CLICK和TYPE这两类比其他三类更复杂的动作，我们还额外报告动作类型预测准确度，在本表中标记为“type”。总动作匹配分数也包含在内。“GP”是目标进展的缩写。最佳结果以粗体标记，次优结果以下划线表示。


<img src="https://cdn.noedgeai.com/bo_d5vch5ref24c73bqi1u0_6.jpg?x=192&y=912&w=609&h=400&r=0"/>



Figure 6: Total action-matching score and goal progress over training epochs on AUTO-UI model.
图6：AUTO-UI模型在训练周期上的总动作匹配分数与目标进展。


### 5.2 Fine-tuning Evaluation
### 5.2 微调评估


To evaluate the influence of individual components of CoAT, we perform an ablation study by incorporating them alternately. We split the annotations into 'input' and 'output' groups, indicating where the extra information comes in during the model training. Specifically, we put screen description and previous action result as additional input information, as they do not provide direct help to the current action decision. Action think and next action description are added to the output so that the agent can learn such thinking process.
为评估CoAT各组成部分的影响，我们通过轮流整合它们来进行消融研究。我们将注释分成“输入”（input）和“输出”（output）两组，表示模型训练时额外信息的进入位置。具体地，我们将屏幕描述和上一步操作结果作为额外输入信息，因为它们对当前动作决策并无直接帮助。行动思考和下一步行动描述被加入输出，以便代理学习此类思考过程。


From Table 6, we observe that previous action result, especially combined with action think and action description, significantly improve the overall action prediction accuracy of AUTO-UI. As the coherence of decision-making process is enhanced by previous action result, there is a notable increase in the STOP action-matching score (from 60.12 to 79.17). Experiment (5)-(7) demonstrate that learning to engage in action thinking without additional input is challenging. However, when screen description and/or previous action result are added to the input, the performance of AUTO-UI improves immediately, especially in predicting CLICK actions. This validates the necessity and effectiveness of such semantic annotations. There is a minor decrease in both action-matching score and the goal progress when screen description is added, as seen in line (9) and (10). We attribute this to the low resolution of the visual encoder used by AUTO-UI, resulting in an inability to effectively utilize the information in screen descriptions. Figure 6 further illustrates the improvement in training efficiency when trained with our AITZ data.
从表6中可以看出，上一步操作结果，特别是与行动思考和行动描述结合后，显著提升了AUTO-UI的整体动作预测准确度。由于上一步操作结果提升了决策过程的一致性，STOP动作匹配分数有显著提升（从60.12到79.17）。实验（5）–（7）表明在没有额外输入的情况下学习进行行动思考具有挑战性。然而，当屏幕描述和/或上一步操作结果被添加到输入中时，AUTO-UI的性能立即提升，尤其是在预测CLICK动作方面。这验证了此类语义注释的必要性和有效性。当屏幕描述被添加时，动作匹配分数和目标进展均略有下降，见（9）和（10）行。我们将此归因于AUTO-UI所使用的视觉编码器分辨率较低，因此无法有效利用屏幕描述中的信息。图6进一步说明了在使用我们的AITZ数据训练时训练效率的提升。


### 5.3 Qualitative Analysis
### 5.3 定性分析


We conduct the thorough analysis on wrong cases, as shown in Figure 7. AUTO-UI struggles with correctly judging the task execution progress, as the action history provided as a series of action types and coordinates is hard to understand. Previous action result mitigates this problem by explicitly describing the result of the previous actions in words. This highlights the importance of safeguarding the coherence of action decision by establishing connections between two time steps. For CogAgent, we carefully inspect its output, which is composed of three parts: action plan, next action and grounded operation. It seems that CogAgent does not take historical information into account, as its predictions at each step only consider the current information, leading to repetitive and ineffective actions. For example, as the corresponding action plan generated by CogAgent is to "1. Open Browser Menu, 2. Select 'New Incognito Tab' from the dropdown menu", it repeatedly attempts to open the menu icon (see the right-side case in Figure 7). Adding a short-cut action chain-of-thought, i.e. action think from AITZ dataset, into the model input helps to alleviate this issue.
我们对错误案例进行了深入分析，如图7所示。AUTO-UI在正确判断任务执行进度方面存在困难，因为以一系列动作类型和坐标组成的动作历史难以理解。上一步操作结果通过以文字形式明确描述前一步操作的结果来缓解这一问题。这凸显了通过在两个时间步之间建立联系来维护行动决策一致性的重要性。对于CogAgent，我们仔细检查其输出，它由三部分组成：行动计划、下一步行动与有据可依的操作。似乎CogAgent未考虑历史信息，因为其在每一步的预测仅考虑当前信息，导致重复且低效的行动。例如，由CogAgent生成的相应行动计划为“1. 打开浏览器菜单，2. 从下拉菜单中选择‘新隐身标签’”，它会反复尝试打开菜单图标（见图7右侧案例）。将短路的行动思维过程链，即来自AITZ数据集的行动思考，加入模型输入有助于缓解这一问题。


<img src="https://cdn.noedgeai.com/bo_d5vch5ref24c73bqi1u0_7.jpg?x=188&y=184&w=1271&h=453&r=0"/>



Figure 7: Qualitative examples for AUTO-UI and CogAgent. This figure presents qualitative results where different types of errors are corrected by applying additional semantic annotations (yellow shadowed boxes).
图7：AUTO-UI与CogAgent的定性示例。该图展示通过应用额外的语义注释（黄色阴影框）纠正的不同类型错误的定性结果。


### 5.4 Generalization Evaluation
### 5.4 泛化评估


Generalization abilities are crucial for GUI agents. Previous experiments in this paper are actually a reflection of the generalization ability over unseen instructions, as we put emphasis on separation based on instructions (see Section 3.1 and Section 4.3). The generalization over unseen apps is another important perspective. Hence, we re-partition the dataset based on the separation of apps, resulting in a train split (1519 episodes) spanning 63 apps and a test split (459 episodes) spanning 10 apps. We follow the implementation details in Section 4.3 and the results are shown in Table 7. By adding CoAT-driven data during training, the agent could generalize to unseen apps better (9.4% v.s. 5.1% on the episodic goal progress). This demonstrates that CoAT is generalizable and helpful for action decision-making on unseen apps.
泛化能力对GUI代理至关重要。本文中的先前实验实际上反映了对未见指令的泛化能力，因为我们强调基于指令的分离（见第3.1节和第4.3节）。对未见应用的泛化也是一个重要的视角。因此，我们基于应用的分离重新划分数据集，得到训练集（1519个情节）覆盖63个应用和测试集（459个情节）覆盖10个应用。我们遵循第4.3节中的实现细节，结果见表7。通过在训练中加入由CoAT驱动的数据，代理对未见应用的泛化能力更强（在情节目标进展上的提升为9.4%对比5.1%）。这表明CoAT对未见应用的动作决策具有普遍性和帮助作用。


## 6 Related Works
## 6 相关工作


GUI Navigation AUTOmatic execution of user instructions on smartphones or websites is an advanced task, as it requires the agent to not only perceive but also deduce. Previous works concentrate on evaluating the ability of models to identify different UI elements (Shi et al., 2017; Zhang et al., 2021; Sunkara et al., 2022), and to fulfil a user-queried task by either statically operating on a series of pre-collected GUI screenshots (Li et al., 2020b; Venkatesh et al., 2022; Zhang and Zhang, 2023; Deng et al., 2023) or dynamically interacting with an alive Android device (Yang et al., 2023a). However, these works separate the ability of element recognition and action inference, causing a discrepancy between the user intent and the performed actions (Wei et al., 2022; Baechler et al., 2024). Our CoAT framework bridges this gap by allowing GUI agents to recall history actions, perceive the current screen, and decide on the future actions based on these useful semantics.
GUI 导航 自动执行智能手机或网站上的用户指令是一项高级任务，因为它不仅需要感知还需要推理。以往工作集中于评估模型识别不同 UI 元素的能力（Shi et al., 2017; Zhang et al., 2021; Sunkara et al., 2022），并通过静态对一系列预采集的 GUI 截图执行用户查询任务（Li et al., 2020b; Venkatesh et al., 2022; Zhang and Zhang, 2023; Deng et al., 2023）或动态与实时 Android 设备交互来完成任务（Yang et al., 2023a）。然而，这些工作将元素识别能力与动作推断能力分离，导致用户意图与执行动作之间存在差异（Wei et al., 2022; Baechler et al., 2024）。我们的 CoAT 框架通过使 GUI 代理能够回忆历史动作、感知当前屏幕，并基于这些有用语义决定未来动作来弥合这一差距。


Large Multimodal Models (LMM) Recent years have witnessed the rise of numerous large multimodal models (Liu et al., 2023a,b; Zhu et al., 2023; Zeng et al., 2023). Usually, visual signals are encoded by a vision transformer (Doso-vitskiy et al., 2020) and further incorporated in LLMs (Radford et al., 2021) through linear projection (Tsimpoukelli et al., 2021), Q-former (Li et al., 2023a) or cross-attention layers (Alayrac et al., 2022). For general purpose LMMs, the low resolution of visual encoders $\left( {{224} \times  {224}}\right)$ captures only coarse visual information. CogAgent (Hong et al., 2023) deals with this problem by using the original ViT-L (Dosovitskiy et al., 2020) to encode high-resolution visual features up to ${1120} \times  {1120}$ , and fusing them with every decoder layers through cross-attention. Whereas Monkey (Li et al., 2023b) equips the visual encoder from QWen-VL (Bai et al., 2023) with individual LoRA adapter (Hu et al., 2021) for each patch to scale the image resolution up to ${896} \times  {1344}$ pixels. Consequent works (Yu et al., 2024; Chen et al., 2024; Lu et al., 2024a) all incorporate high-resolution image encoders, indicating a popular trend for the future.
大规模多模态模型（LMM） 近些年涌现出大量的大规模多模态模型（Liu et al., 2023a,b; Zhu et al., 2023; Zeng et al., 2023）。通常，视觉信号由视觉变换器编码（Doso-vitskiy et al., 2020），再通过线性投影（Tsimpoukelli et al., 2021）、Q-former（Li et al., 2023a）或跨注意力层（Alayrac et al., 2022）融入到大语言模型（LLM）中。对通用目的的 LMM，视觉编码器的低分辨率 $\left( {{224} \times  {224}}\right)$ 仅捕捉粗略视觉信息。CogAgent（Hong et al., 2023）通过使用原始 ViT-L（Dosovitskiy et al., 2020）编码分辨率高达 ${1120} \times  {1120}$ 的视觉特征，并通过跨注意力与每个解码器层进行融合来解决此问题。而 Monkey（Li et al., 2023b）则为来自 QWen-VL（Bai et al., 2023）的视觉编码器为每个补丁配备单独的 LoRA 适配器（Hu et al., 2021），以将图像分辨率提升至 ${896} \times  {1344}$ 像素。随后的工作（Yu et al., 2024; Chen et al., 2024; Lu et al., 2024a）都整合了高分辨率图像编码器，显示出未来的一个流行趋势。


<table><tr><td rowspan="2">Model</td><td colspan="6">Action-Matching Score</td><td rowspan="2">Goal Progress</td></tr><tr><td>TOTAL</td><td>CLICK</td><td>TYPE</td><td>PRESS</td><td>STOP</td><td>SCROLL</td></tr><tr><td>AUTO-UI</td><td>28.5</td><td>10.7</td><td>59.2</td><td>27.6</td><td>41.1</td><td>69.7</td><td>5.1</td></tr><tr><td>AUTO-UI + CoAT</td><td>31.8</td><td>19.7</td><td>61.2</td><td>49.1</td><td>55.2</td><td>74.9</td><td>9.4</td></tr></table>
<table><tbody><tr><td rowspan="2">模型</td><td colspan="6">动作匹配分数</td><td rowspan="2">目标进度</td></tr><tr><td>合计</td><td>点击</td><td>输入</td><td>按下</td><td>停止</td><td>滚动</td></tr><tr><td>自动 UI</td><td>28.5</td><td>10.7</td><td>59.2</td><td>27.6</td><td>41.1</td><td>69.7</td><td>5.1</td></tr><tr><td>自动 UI + CoAT</td><td>31.8</td><td>19.7</td><td>61.2</td><td>49.1</td><td>55.2</td><td>74.9</td><td>9.4</td></tr></tbody></table>


Table 7: Generalization results over the unseen apps under fine-tuning settings.
表 7：在微调设置下对未见应用的泛化结果。


LMM as GUI Agents A number of works have utilized LMMs' domain knowledge and emergent zero-shot embodied abilities to perform complex task planning and reasoning (Yang et al., 2023b; Wang et al., 2023c; Ikeuchi et al., 2023). For GUI navigation, the introduction of LMMs surpasses previous works that transform the UI layouts and elements into the text-only HTML format (Li et al., 2020a; Zhang et al., 2021; Wang et al., 2023a). One line of work adopts GPT-4V directly as the GUI agent and prompts it to perform the task (Yan et al., 2023; Yang et al., 2023a; Zheng et al., 2024), while other methods focus on tuning a smaller LMM on GUI-related datasets to acquire the domain-specific knowledge (Zhang and Zhang, 2023), or train a LMM from scratch on GUI-specified pre-training tasks (Hong et al., 2023; Baechler et al., 2024; You et al., 2024; Cheng et al., 2024). We evaluate two agents on the proposed AITZ dataset, and prove that our proposed chain-of-action-thought helps agents adapt to GUI tasks better and more quickly.
LMM 作为 GUI 代理 若干研究利用 LMM 的领域知识与突现的零-shot 具备实体化能力，执行复杂任务规划与推理（Yang 等, 2023b; Wang 等, 2023c; Ikeuchi 等, 2023）。在 GUI 导航方面，引入 LMM 超越了将 UI 布局与元素转换为纯文本 HTML 格式的早期工作（Li 等, 2020a; Zhang 等, 2021; Wang 等, 2023a）。有一类工作直接将 GPT-4V 作为 GUI 代理并提示其执行任务（Yan 等, 2023; Yang 等, 2023a; Zheng 等, 2024），另一些方法则在与 GUI 相关的数据集上微调较小的 LMM 以获取领域特定知识（Zhang 与 Zhang, 2023），或在 GUI 指定的预训练任务上从零开始训练 LMM（Hong 等, 2023; Baechler 等, 2024; You 等, 2024; Cheng 等, 2024）。我们在提出的 AITZ 数据集上评估了两位代理，并证明我们提出的行动链式推理（CoAT）帮助代理更好且更快地适应 GUI 任务。


## 7 Conclusion
## 7 结论


In conclusion, our work aims to bolster the navigation ability of LMM-based GUI agents. We propose Chain-of-Action-Thought (CoAT) by analyzing human orienteering processes. We start by verifying that CoAT is superior to three typical context modeling methods. In order to inject CoAT-like thinking capabilities into existing GUI agents, we further generated a set of high-quality CoAT-driven data through cooperation between human experts and GPT-4V, namely Android-In-The-Zoo (AITZ) dataset. AITZ enriches this field with a robust dataset that bridges perception and cognition, facilitating effective training and reliable evaluation for GUI navigation agents. Experiments demonstrate the efficiency and usefulness of proposed chain-of-action-thought paradigm.
总之，我们的工作旨在提升基于 LMM 的 GUI 代理的导航能力。我们通过分析人类定向过程提出行动链式思维（CoAT）。首先验证 CoAT 相较于三种典型的上下文建模方法具有优势。为了将类似 CoAT 的思维能力注入现有 GUI 代理，我们进一步通过人类专家与 GPT-4V 的协作生成了一组高质量的以 CoAT 为驱动的数据，即 Android-In-The-Zoo（AITZ）数据集。AITZ 用一个桥接感知与认知的健壮数据集丰富了该领域，有助于对 GUI 导航代理进行有效训练与可靠评估。实验表明提出的行动链式推理范式具有高效性与实用性。


## 8 Limitations
## 8 局限性


We developed CoAT and AITZ with the goal of enabling LLM Agents to mimic the cognitive processes of humans. Although our experiments proved that it is possible to stimulate the reasoning ability of language models (i.e. GPT-4V (OpenAI, 2023), CogAgent (Hong et al., 2023) and AUTO-UI (Zhang and Zhang, 2023)) in GUI scenarios through zero-shot prompting or fine-tuning, the different model structure and training data used by current specified models for GUI tasks make the comparison less intuitive. To what extent the image resolution and GUI-related pretraining tasks (i.e. text recognition, GUI imagery (Hong et al., 2023), screen question-answering (Baechler et al., 2024; You et al., 2024) and GUI grounding (Cheng et al., 2024)) influence the navigation performance remains under-explored. We leave it for future work to precisely measure the impact of image resolution, text recognition ability, GUI grounding ability of LMMs on GUI navigation tasks.
我们开发 CoAT 与 AITZ 的目标，是使大语言模型代理能够模拟人类的认知过程。尽管我们的实验证明通过零-shot 提示或微调，在 GUI 场景中有可能激发语言模型（如 GPT-4V（OpenAI, 2023）、CogAgent（Hong 等, 2023）和 AUTO-UI（Zhang 与 Zhang, 2023））的推理能力，但当前用于 GUI 任务的模型在结构与训练数据上存在差异，导致比较不够直观。图像分辨率与 GUI 相关的预训练任务（如文本识别、GUI 图像（Hong 等, 2023）、屏幕问答（Baechler 等, 2024; You 等, 2024）以及 GUI 定位（Cheng 等, 2024））在导航性能上的影响尚未充分研究。后续工作将精准衡量图像分辨率、文本识别能力、GUI 定位能力对 GUI 导航任务的影响。


## 9 Ethics
## 9 伦理


Android-In-The-Zoo (AITZ) dataset is sourced from open-source datasets AITW (Rawles et al., 2023), which is permitted for academic use. During our data collection, specifically, during the instruction-episode correctness checks, we ensured that privacy concerns were addressed, and the sampled data does not include any real personal information (fake or meaningless data are allowed). Since AITZ dataset contains only semantic annotations on smartphone operations, the use of this data poses neither ethical risks nor harmful guidance.
Android-In-The-Zoo（AITZ）数据集源自开源数据集 AITW（Rawles 等, 2023），可用于学术用途。在数据收集过程中，尤其是在指令-情节正确性检查阶段，我们确保隐私问题得到处理，抽样数据不包含任何真实个人信息（可包含伪造或无意义数据）。由于AITZ数据集仅包含对智能手机操作的语义注释，使用该数据既不会带来伦理风险，也不会给出有害的指引。


## 10 Acknowledgements
## 10 致谢


This work is supported by National Natural Science Foundation of China (No. 62176058) and National Key R&D Program of China (2023YFF1204800). The project's computational resources are supported by CFFF platform of Fudan University.
本工作得到中国国家自然科学基金（编号 62176058）及中国国家重点研发计划（2023YFF1204800）资助。该项目的计算资源由复旦大学 CFFF 平台提供。


## References
## 参考文献


Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds 等. 2022. Flamingo：用于少量学习的视觉语言模型。神经信息处理系统进展，35：23716-23736。


Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Cärbune, Jason Lin, Jindong Chen, and Abhanshu Sharma. 2024. Screenai: A vision-language model for ui and infographics understanding. arXiv preprint arXiv:2402.04615.
Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Cärbune, Jason Lin, Jindong Chen, Abhanshu Sharma. 2024. Screenai：一种用于 UI 与信息图理解的视觉-语言模型。arXiv 预印本 arXiv:2402.04615。


Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966.
秦泽拜、白帅、杨书生、王世杰、谭思男、王朋、林君阳、周翔、周晶仁。 2023. Qwen-vl：具多样能力的前沿大型视觉-语言模型。arXiv 预印本 arXiv:2308.12966。


Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan A Plummer. 2021. Mobile app tasks with iterative feedback (motif): Addressing task feasibility in interactive visual environments. arXiv preprint arXiv:2104.08560.
Andrea Burns、Deniz Arsan、Sanjna Agrawal、Ranjitha Kumar、Kate Saenko、Bryan A Plummer。2021. 以迭代反馈为任务的移动应用任务（motif）：在交互式视觉环境中解决任务可行性。arXiv 预印本 arXiv:2104.08560。


Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. 2024. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821.
Zhe Chen、Weiyun Wang、Hao Tian、Shenglong Ye、Zhangwei Gao、Erfei Cui、Wenwen Tong、Kongzhi Hu、Jiapeng Luo、Zheng Ma 等。2024. 我们距离 GPT-4v 还有多远？通过开源套件缩小商业多模态模型的差距。arXiv 预印本 arXiv:2404.16821。


Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. 2024. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935.
Kanzhi Cheng、Qiushi Sun、Yougang Chu、Fangzhi Xu、Yantao Li、Jianbing Zhang、Zhiyong Wu。2024. Seeclick：利用 GUI 基地对高级视觉 GUI 代理进行锚定。arXiv 预印本 arXiv:2401.10935。


Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070.
Xiang Deng、Yu Gu、Boyuan Zheng、Shijie Chen、Samuel Stevens、Boshi Wang、Huan Sun、Yu Su。2023. Mind2web：面向通用网络的代理。arXiv 预印本 arXiv:2306.06070。


Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations.
Alexey Dosovitskiy、Lucas Beyer、Alexander Kolesnikov、Dirk Weissenborn、Xiaohua Zhai、Thomas Unterthiner、Mostafa Dehghani、Matthias Minderer、Georg Heigold、Sylvain Gelly 等。2020. 一张图片值 16x16 个词：用于大规模图像识别的变换器。国际学习表示会议（ICLR）。


Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. 2023. Cogagent: A visual language model for gui agents. arXiv preprint arXiv:2312.08914.
Wenyi Hong、Weihan Wang、Qingsong Lv、Jiazheng Xu、Wenmeng Yu、Junhui Ji、Yan Wang、Zihan Wang、Yuxiao Dong、Ming Ding 等。2023. Cogagent：用于 GUI 代理的可视语言模型。arXiv 预印本 arXiv:2312.08914。


Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations.
Edward J Hu、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi Li、Shean Wang、Lu Wang、Weizhu Chen 等。2021. Lora：大语言模型的低秩适配。在人类学习表示大会（ICLR）上。


Katsushi Ikeuchi, Jun Takamatsu, Kazuhiro Sasabuchi, Naoki Wake, and Atsushi Kanehiro. 2023. Applying learning-from-observation to household service robots: three common-sense formulation. arXiv preprint arXiv:2304.09966.
Katsushi Ikeuchi、Jun Takamatsu、Kazuhiro Sasabuchi、Naoki Wake、Atsushi Kanehiro。2023. 将从观测中学习应用于家务服务机器人：三种常识表述。arXiv 预印本 arXiv:2304.09966。


Sunjae Lee, Junyoung Choi, Jungjae Lee, Hojun Choi, Steven Y Ko, Sangeun Oh, and Insik Shin. 2023. Explore, select, derive, and recall: Augmenting llm with human-like memory for mobile task automation. arXiv preprint arXiv:2312.03003.
Sunjae Lee、Junyoung Choi、Jungjae Lee、Hojun Choi、Steven Y Ko、Sangeun Oh、Insik Shin。2023. 探索、选择、推导与回忆：为移动任务自动化增强具有类人记忆的 llm。arXiv 预印本 arXiv:2312.03003。


Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023a. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597.
Junnan Li、Dongxu Li、Silvio Savarese、Steven Hoi。2023a. Blip-2：通过冻结的图像编码器与大语言模型对语言-图像进行预训练的引导。arXiv 预印本 arXiv:2301.12597。


Toby Jia-Jun Li, Tom Mitchell, and Brad Myers. 2020a. Interactive task learning from gui-grounded natural language instructions and demonstrations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 215-223.
Toby Jia-Jun Li、Tom Mitchell、Brad Myers。2020a. 通过 GUI-环境自然语言指令与演示进行交互式任务学习。收录于：第58届年度计算语言学协会系统演示会议论文集，215-223 页。


Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. 2020b. Mapping natural language instructions to mobile ui action sequences. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8198-8210.
Yang Li、Jiacong He、Xin Zhou、Yuan Zhang、Jason Baldridge。2020b. 将自然语言指令映射为移动 UI 动作序列。收录于：第58届年度计算语言学协会大会论文集，8198-8210 页。


Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. 2023b. Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint arXiv:2311.06607.
Zhang Li、Biao Yang、Qiang Liu、Zhiyin Ma、Shuo Zhang、Jingxu Yang、Yabo Sun、Yuliang Liu、Xiang Bai。2023b. Monkey：图像分辨率与文本标签是大型多模态模型的重要因素。arXiv 预印本 arXiv:2311.06607。


Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744.
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. 通过视觉指令微调改进基线。 arXiv 预印本 arXiv:2310.03744。


Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. arXiv preprint arXiv:2304.08485.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. 视觉指令微调。 arXiv 预印本 arXiv:2304.08485。


Thomas F Liu, Mark Craft, Jason Situ, Ersin Yumer, Radomir Mech, and Ranjitha Kumar. 2018. Learning design semantics for mobile apps. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology, pages 569-579.
Thomas F Liu, Mark Craft, Jason Situ, Ersin Yumer, Radomir Mech, and Ranjitha Kumar. 2018. 为移动应用学习设计语义。 于第31届ACM用户界面软件与技术峰会论文集，页码569-579。


Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhu-oshu Li, Yaofeng Sun, et al. 2024a. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525.
Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhu-oshu Li, Yaofeng Sun, 等人。 2024a. Deepseek-vl：迈向现实世界的视觉-语言理解。 arXiv 预印本 arXiv:2403.05525。


Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. 2024b. Gui odyssey: A comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451.
Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. 2024b. Gui odyssey：一个覆盖移动设备跨应用 GUI 导航的综合数据集。 arXiv 预印本 arXiv:2406.08451。


Amal Nanavati, Vinitha Ranganeni, and Maya Cakmak. 2023. Physically assistive robots: A systematic review of mobile and manipulator robots that physically assist people with disabilities. Annual Review of Control, Robotics, and Autonomous Systems, 7.
Amal Nanavati, Vinitha Ranganeni, and Maya Cakmak. 2023. 具身辅助机器人：对帮助残障人士的移动与末端执行机器人系统的系统综述。 Annual Review of Control, Robotics, and Autonomous Systems, 7。


OpenAI. 2023. Gpt-4 technical report. arXiv:2303.08774.
OpenAI. 2023. Gpt-4 技术报告。 arXiv:2303.08774。


Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-try, Amanda Askell, Pamela Mishkin, Jack Clark, 等人。 2021. 从自然语言监督学习可迁移的视觉模型。 于国际机器学习会议，页码8748-8763。 PMLR。


Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2023. Android in the wild: A large-scale dataset for android device control. arXiv preprint arXiv:2307.10088.
Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2023. Android in the wild: 一个大规模的安卓设备控制数据集。 arXiv 预印本 arXiv:2307.10088。


Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022. Laion- 5b: An open large-scale dataset for training next generation image-text models. ArXiv, abs/2210.08402.
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022. Laion-5b：用于训练下一代图像文本模型的开源大规模数据集。 ArXiv, abs/2210.08402。


Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. 2017. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pages 3135-3144. PMLR.
Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. 2017. World of bits：一个面向开放域的网页代理平台。 于国际机器学习会议，页码3135-3144。 PMLR。


Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. 2022a. Meta-gui: Towards multi-modal conversational agents on mobile gui. In Conference on Empirical Methods in Natural Language Processing.
Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. 2022a. Meta-gui：面向移动 GUI 的多模态对话代理。 于自然语言处理经验方法大会。


Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. 2022b. Meta-gui: Towards multi-modal conversational agents on mobile gui. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6699-6712.
Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. 2022b. Meta-gui：面向移动 GUI 的多模态对话代理。 于2022年自然语言处理经验方法大会论文集，页码6699-6712。


Srinivas Sunkara, Maria Wang, Lijuan Liu, Gilles Baechler, Yu-Chung Hsiao, Abhanshu Sharma, James Stout, et al. 2022. Towards better semantic understanding of mobile interfaces. arXiv preprint arXiv:2210.02663.
Srinivas Sunkara, Maria Wang, Lijuan Liu, Gilles Baechler, Yu-Chung Hsiao, Abhanshu Sharma, James Stout, 等人。 2022. 面向移动界面的语义理解提升。 arXiv 预印本 arXiv:2210.02663。


Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, 等人。 2023. Gemini：一系列高度强大的多模态模型。 arXiv 预印本 arXiv:2312.11805。


Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200-212.
Maria Tsimpoukelli、Jacob L Menick、Serkan Cabi、SM Eslami、Oriol Vinyals、以及 Felix Hill。2021。使用冻结语言模型的多模态少样本学习。Advances in Neural Information Processing Systems，34:200-212。


Sagar Gubbi Venkatesh, Partha Talukdar, and Srini Narayanan. 2022. Ugif: Ui grounded instruction following. arXiv preprint arXiv:2211.07615.
Sagar Gubbi Venkatesh、Partha Talukdar、Srini Narayanan。2022。Ugif：Ui 作为指令跟随的基于 Ui 的方法。arXiv 预印本 arXiv:2211.07615。


Bryan Wang, Gang Li, and Yang Li. 2023a. Enabling conversational interaction with mobile ui using large language models. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1-17.
Bryan Wang、Gang Li、Yang Li。2023a。在移动端 UI 上利用大型语言模型实现对话式交互。In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems，页码 1-17。


Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. 2021. Screen2words: Automatic mobile ui summarization with multimodal learning. In The 34th Annual ACM Symposium on User Interface Software and Technology, pages 498- 510.
Bryan Wang、Gang Li、Xin Zhou、Zhourong Chen、Tovi Grossman、Yang Li。2021。Screen2words：通过多模态学习实现移动端 UI 自动摘要。In The 34th Annual ACM Symposium on User Interface Software and Technology，页码 498-510。


Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023b. Cogvlm: Visual expert for pretrained language models. ArXiv, abs/2311.03079.
Weihan Wang、Qingsong Lv、Wenmeng Yu、Wenyi Hong、Ji Qi、Yan Wang、Junhui Ji、Zhuoyi Yang、Lei Zhao、Xixuan Song、Jiazheng Xu、Bin Xu、Juanzi Li、Yuxiao Dong、Ming Ding、Jie Tang。2023b。Cogvlm：为预训练语言模型提供视觉专家。ArXiv，abs/2311.03079。


Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. 2023c. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560.
Zihao Wang、Shaofei Cai、Guanzhou Chen、Anji Liu、Xiaojian Ma、Yitao Liang。2023c。Describe、explain、plan and select：通过大型语言模型实现开放世界多任务代理的互动规划。arXiv 预印本 arXiv:2302.01560。


Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.
Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Fei Xia、Ed Chi、Quoc V Le、Denny Zhou 等。2022。通过链式思考提示在大型语言模型中引出推理。Advances in Neural Information Processing Systems，35:24824-24837。


Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. 2023. Empowering llm to use smartphone for intelligent task automation. arXiv preprint arXiv:2308.15272.
Hao Wen、Yuanchun Li、Guohong Liu、Shanhui Zhao、Tao Yu、Toby Jia-Jun Li、Shiqi Jiang、Yunhao Liu、Yaqin Zhang、Yunxin Liu。2023。使 llm 能使用智能手机实现智能任务自动化。arXiv 预印本 arXiv:2308.15272。


An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. 2023. Gpt- 4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562.
An Yan、Zhengyuan Yang、Wanrong Zhu、Kevin Lin、Linjie Li、Jianfeng Wang、Jianwei Yang、Yiwu Zhong、Julian McAuley、Jianfeng Gao 等。2023。Gpt-4v in wonderland：用于零样本智能手机 GUI 导航的大型多模态模型。arXiv 预印本 arXiv:2311.07562。


Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Ze-biao Huang, Bin Fu, and Gang Yu. 2023a. Appa-gent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771.
Zhao Yang、Jiaxuan Liu、Yucheng Han、Xin Chen、Ze-biao Huang、Bin Fu、Gang Yu。2023a。Appa-gent：将多模态智能体视为智能手机用户。arXiv 预印本 arXiv:2312.13771。


Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023b. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381.
Zhengyuan Yang、Linjie Li、Jianfeng Wang、Kevin Lin、Ehsan Azarnasab、Faisal Ahmed、Zicheng Liu、Ce Liu、Michael Zeng、Lijuan Wang。2023b。Mm-react：对话型大语言模型的多模态推理与行动提示。arXiv 预印本 arXiv:2303.11381。


Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. 2024. Ferret-ui: Grounded mobile ui understanding with multimodal llms. arXiv preprint arXiv:2404.05719.
Keen You、Haotian Zhang、Eldon Schoop、Floris Weers、Amanda Swearngin、Jeffrey Nichols、Yinfei Yang、Zhe Gan。2024。Ferret-ui：基于多模态大模型的具对齐的移动 UI 理解。arXiv 预印本 arXiv:2404.05719。


Ya-Qi Yu, Minghui Liao, Jihao Wu, Yongxin Liao, Xiaoyu Zheng, and Wei Zeng. 2024. Texthawk: Exploring efficient fine-grained perception of multimodal large language models. arXiv preprint arXiv:2404.09204.
Ya-Qi Yu、Minghui Liao、Jihao Wu、Yongxin Liao、Xiaoyu Zheng、Wei Zeng。2024。Texthawk：探索多模态大模型的高效细粒度感知。arXiv 预印本 arXiv:2404.09204。


Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao Kong. 2023. What matters in training a gpt4-style language model with multimodal inputs? arXiv preprint arXiv:2307.02469.
Yan Zeng、Hanbo Zhang、Jiani Zheng、Jiangnan Xia、Guoqiang Wei、Yang Wei、Yuchen Zhang、Tao Kong。2023。训练 gpt4 风格语言模型时多模态输入中的关键要点？ arXiv 预印本 arXiv:2307.02469。


Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. 2024a. Ufo: A ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939.
Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. 2024a. Ufo: A ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939.


Jiwen Zhang, Yaqi Yu, Minghui Liao, Wentao Li, Jihao Wu, and Zhongyu Wei. 2024b. Ui-hawk: Unleashing the screen stream understanding for gui agents. Preprints.
Jiwen Zhang, Yaqi Yu, Minghui Liao, Wentao Li, Jihao Wu, and Zhongyu Wei. 2024b. Ui-hawk: Unleashing the screen stream understanding for gui agents. Preprints.


Xiaoyi Zhang, Lilian de Greef, Amanda Swearngin, Samuel White, Kyle Murray, Lisa Yu, Qi Shan, Jeffrey Nichols, Jason Wu, Chris Fleizach, et al. 2021. Screen recognition: Creating accessibility metadata for mobile applications from pixels. association for computing machinery, new york, ny, usa.
Xiaoyi Zhang, Lilian de Greef, Amanda Swearngin, Samuel White, Kyle Murray, Lisa Yu, Qi Shan, Jeffrey Nichols, Jason Wu, Chris Fleizach, et al. 2021. Screen recognition: Creating accessibility metadata for mobile applications from pixels. association for computing machinery, new york, ny, usa.


Zhuosheng Zhang and Aston Zhang. 2023. You only look at screens: Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436.
Zhuosheng Zhang and Aston Zhang. 2023. You only look at screens: Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436.


Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923.
Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923.


Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024. Gpt-4v (ision) is a generalist web agent, if grounded. arXiv preprint arXiv:2401.01614.
Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024. Gpt-4v (ision) is a generalist web agent, if grounded. arXiv preprint arXiv:2401.01614.


Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592.


## A Data Collection
## A Data Collection


Our data construction pipeline is shown in Figure 3. We leverage the strong world knowledge and generation ability of GPT-4V, combined with critical human verification, to ensure high-quality data. We ask our annotators to detect only factual errors, which could hardly introduce human bias.
Our data construction pipeline is shown in Figure 3. We leverage the strong world knowledge and generation ability of GPT-4V, combined with critical human verification, to ensure high-quality data. We ask our annotators to detect only factual errors, which could hardly introduce human bias.


### A.1 Instruction Sampling Process
### A.1 Instruction Sampling Process


We first checked the instruction distribution in the original AITW dataset based on the split from (Zhang and Zhang, 2023). We find that the instruction distribution in the training, validation and test sets are almost the same, which means there is a serious problem of data leakage. To avoid such problem exists in our constructed datasets, we perform instruction sampling.
We first checked the instruction distribution in the original AITW dataset based on the split from (Zhang and Zhang, 2023). We find that the instruction distribution in the training, validation and test sets are almost the same, which means there is a serious problem of data leakage. To avoid such problem exists in our constructed datasets, we perform instruction sampling.


- SINGLE: Given the complexity and variety of instructions in this dataset, we first clustered them and then performed balanced sampling based on the categories. The clustering process is as follows: (1) Identify the main verb in each instruction, typically the first word, and group the instructions by this verb. (2) For each group of instructions, we manually classify those with fewer than 50 samples that show clear patterns. Then for groups with more than 50 samples, use tf-idf for clustering. Finally, we manually verify the clustering results.
- SINGLE: Given the complexity and variety of instructions in this dataset, we first clustered them and then performed balanced sampling based on the categories. The clustering process is as follows: (1) Identify the main verb in each instruction, typically the first word, and group the instructions by this verb. (2) For each group of instructions, we manually classify those with fewer than 50 samples that show clear patterns. Then for groups with more than 50 samples, use tf-idf for clustering. Finally, we manually verify the clustering results.


- WEB_SHOPPING: We performed balanced sampling based on the types of shopping websites and the objects involved.
- WEB_SHOPPING: We performed balanced sampling based on the types of shopping websites and the objects involved.


- GENERAL, INSTALL, GOOGLE_APPS: Since these three datasets have a limited number of instructions, we did not perform extensive filtering during sampling. Instead, we uniformly sampled x instructions per user. For INSTALL and GENERAL, $x = 3$ ; for GOOGLE_APPS, $x = 5$ .
- GENERAL, INSTALL, GOOGLE_APPS: Since these three datasets have a limited number of instructions, we did not perform extensive filtering during sampling. Instead, we uniformly sampled x instructions per user. For INSTALL and GENERAL, $x = 3$ ; for GOOGLE_APPS, $x = 5$ .


During the instruction sampling stage, we recruited 10 annotators to verify whether the episodes have successfully completed the tasks required by the instructions. Our data quality inspection team conducted a secondary validation of sampled results.
During the instruction sampling stage, we recruited 10 annotators to verify whether the episodes have successfully completed the tasks required by the instructions. Our data quality inspection team conducted a secondary validation of sampled results.


### A.2 Semantic Annotation Process
### A.2 Semantic Annotation Process


We leverage GPT-4V through Amazon Azure-API as the navigation expert and prompt it to do the following generation tasks:
我们通过亚马逊 Azure-API 使用GPT-4V作为导航专家，并对其进行以下生成任务的提示：


1. Screen Description: describe the main content of the given screenshots, including the screen type, and primary apps or widgets presented.
1. 屏幕描述：描述给定截图的主要内容，包括屏幕类型以及呈现的主要应用或小部件。


2. Action Grounding: given the coordinates of the correct next actions, generate action descriptions. Specifically, we simplify the action spaces into 5 action categories, including SCROLL(direction), TYPE(text), PRESS(button), CLICK(point) and STOP(task state). We ask GPT-4V to describe the UI element the click action is operating on, by drawing the bounding box of the clicked area through icon detection model from (Liu et al., 2018). The descriptions for other types of actions are generated using templates.
2. 行动基础：给定正确下一步行动的坐标，生成行动描述。具体地，我们将行动空间简化为5种行动类别，包括 SCROLL(direction)、TYPE(text)、PRESS(button)、CLICK(point) 和 STOP(task state)。我们要求 GPT-4V 描述点击操作所作用的 UI 元素，通过从（Liu 等，2018）中的图标检测模型绘制被点击区域的边界框来实现。其他类型行动的描述使用模板生成。


3. Action Thinking: think about what actions need to be performed on the current screen to complete the user query, and describe the results of the correct next action based on screenshots before and after the action.
3. 行动思考：思考在当前屏幕上需要执行哪些动作以完成用户查询，并在动作前后截图的基础上描述正确下一步行动的结果。


<table><tr><td rowspan="8">P</td><td>Screen Description</td></tr><tr><td>I will give you a screenshot of a mobile phone. <br> **SCREEN**: ___ screen_img_ <br> **TASK**: Your task is to summarize this screen about its main content and its functionality, i.e. the type of the screen, together with primary icons or apps on the screen. You should describe the screen with necessary details, but not too long. Your output must be less than five sentences.</td></tr><tr><td>Action Think</td></tr><tr><td>**QUERY**: The query you need to complete is to ___query___ <br> **ACTION HISTORY**: To proceed with the query, your past actions include: ___action___history___ <br> **SCREEN**: screen_img_ <br> **SCREEN DESCRIPTION**: __________________ <br> **TASK**: Given the screen and above information, you have two tasks to do. Firstly, based on the history and the current screen, you should estimate the execution progress of query in one sentence. Answer with format: 'Reflection: ...' Secondly, you should analyze the screen for relevant details that might pertain to the given query. This includes checking for specific applications, icons, or buttons that are visible, and any information or results that are currently displayed on the screen. Then, describe possible actions you may conduct. You must answer by two sentences with the format: 'Think: ... Possible actions are ..."</td></tr><tr><td>Action Description</td></tr><tr><td>To fulfill the following query, an expert have clicked on the screen. <br> **QUERY**: ___query___ <br> **SCREEN**: ___screen_img___ The screen labeled with expert action is also given to you. <br> **SCREEN WITH ACTION**: ___labelled_screen___ The expert action ___ is labelled as a blue cross marker `+` on this screen. <br> **EXPERT ACTION**: ` expert_action_ <br> **SCREEN COORDINATE SYSTEM**: A coordinate (x, y) represents a point on the screen. The first value, labeled as ’x’, horizontal, i.e. x ranges from 0 to 1, meaning the position of point ranges from the left to right, where x<0.4 means left, 0.4<=x<=0.6 means middle and x>0.6 means right. The second value, labeled as ’y’, is vertical, i.e. y ranges from 0 to 1,meaning the position of point ranges from the bottom to top. where y<0.2 means bottom, 0.2<=y<0.4 means lower, + 0.4<=y<0.5 means lower middle, 0.5<=y<=0.6 means upper middle, 0.6<y<0=0.8 means upper, and y>0.8 means top. <br> **TASK**: Based on above information, your task is to answer: Which UI element (icon, app, search bar, results, etc) is this expert action clicking on and where is it located? You should think step by step as follows: The coordinate in expert action is ___. As stated, the first value `x` is ___, which means the click point locates at ___. The second value `y` is ___, which means the click point locates at the ___ and ___ part of the screen. Combined with the blue cross marker '+', answer with format: the expert action is to click on the ___ located at ___.</td></tr><tr><td>Action Result</td></tr><tr><td>To fulfill the following query, you have performed an action on screen. <br> **QUERY**: ___query___ <br> **ACTION**: correct next action <br> The screenshots before and after the action are: <br> **SCREEN BEFORE ACTION**: ___screen_before___ <br> **SCREEN AFTER ACTION**: screen after <br> **TASK**: Your task is to explain why this action can facilitate the completion of query. Answer with format: By doing so, ... Your output must be within two sentences, one sentence about the consequences and one sentence about the reason. Keep your answer as concise and brief as possible.</td></tr></table>
<table><tbody><tr><td rowspan="8">P</td><td>屏幕描述</td></tr><tr><td>我将给你一张手机截图。 <br/> **SCREEN**: ___ screen_img_ <br/> **TASK**: 你的任务是总结此屏幕的主要内容及其功能，即屏幕类型，以及屏幕上的主要图标或应用。你应以必要的细节描述屏幕，但不要过长。你的输出必须少于五句话。</td></tr><tr><td>行动思考</td></tr><tr><td>**QUERY**: 你需要完成的查询是 ___query___ <br/> **ACTION HISTORY**: 为继续查询，你过去的操作包括： ___action___history___ <br/> **SCREEN**: screen_img_ <br/> **SCREEN DESCRIPTION**: __________________ <br/> **TASK**: 给定屏幕及以上信息，你有两项任务。首先，基于历史记录与当前屏幕，您应用一句话估算查询的执行进度。请用格式：'Reflection: ...'。其次，您应分析屏幕以获取与给定查询相关的细节。这包括查看是否可见的具体应用、图标或按钮，以及屏幕上当前显示的任何信息或结果。然后，描述您可能采取的行动。您必须以两句话作答，格式为：'Think: ... Possible actions are ...'</td></tr><tr><td>行动描述</td></tr><tr><td>为完成以下查询，专家已在屏幕上点击过。 <br/> **QUERY**: ___query___ <br/> **SCREEN**: ___screen_img___ 显示有专家动作的屏幕也给出给你。 <br/> **SCREEN WITH ACTION**: ___labelled_screen___ 专家动作 ___ 被标注为蓝色十字标记 `+` 在此屏幕上。 <br/> **EXPERT ACTION**: ` expert_action_ <br/> **SCREEN COORDINATE SYSTEM**: 坐标 (x, y) 表示屏幕上的点。第一值，标记为 ’x’，水平方向，即 x 的取值范围为 0 到 1，表示点的位置从左到右，其中 x<0.4 表示左侧，0.4<=x<=0.6 表示中间，x>0.6 表示右侧。第二值，标记为 ’y’，是垂直方向，即 y 的取值范围为 0 到 1，表示点的位置从下到上，其中 y<0.2 表示底部，0.2<=y<0.4 表示下方，+ 0.4<=y<0.5 表示下中，0.5<=y<=0.6 表示上中，0.6<y<0.8 表示上方，0.8="" 以上表示顶部。="" <br=""> **TASK**: 基于以上信息，你的任务是回答：这个专家动作点击的是哪个 UI 元素（图标、应用、搜索栏、结果等），以及它位于何处？你应按如下步骤逐步思考：专家动作中的坐标为 ___. 如前所述，第一值 `x` 是 ___，这意味着点击点位于 ___. 第二值 `y` 是 ___，这意味着点击点位于屏幕的 ___ 和 ___ 部分。结合蓝色十字标记 '+'，请以格式回答：专家动作是在位于 ___ 的 ______ clicking。 </y<0.8></td></tr><tr><td>行动结果</td></tr><tr><td>为完成以下查询，你已经在屏幕上执行了一个动作。 <br/> **QUERY**: ___query___ <br/> **ACTION**: 合适的下一步行动 <br/> 截屏操作前后的屏幕为： <br/> **SCREEN BEFORE ACTION**: ___screen_before___ <br/> **SCREEN AFTER ACTION**: screen after <br/> **TASK**: 你的任务是解释为何此动作有助于完成查询。请用格式：通过这样做，... 你的输出必须简洁明了，两句话，一句描述后果，一句给出原因。</td></tr></tbody></table>


Figure 8: Prompt to generate candidate answers for four types of semantic annotations.
图 8：生成四类语义注释候选答案的提示。


Three experts who have a good understanding of UI elements are recruited as annotators to verify whether the generated action description matches the labelled golden actions and the generated action thinkings. Once inconsistency is found, annotators will manually revise the action descriptions, and enforce GPT-4V to regenerate the action thoughts based on the correct action descriptions. The prompt we use are shown in Figure 8.
招募三位对 UI 元素有良好理解的专家作为标注者，以验证生成的动作描述是否与标记的黄金动作及生成的动作思考相一致。一旦发现不一致，标注者将手动修正动作描述，并强制 GPT-4V 基于正确的动作描述重新生成动作思考。我们使用的提示如图 8 所示。


### A.3 Action Space
### A.3 动作空间


As stated before in Appendix A.2, we simplify the action spaces into 5 action categories. The reason behind this is, we observe that within the AITW dataset, 'DUAL_POINT' action type seamlessly covers both 'CLICK' and 'SCROLL' actions. In most cases, the action point of 'SCROLL' action conveys little information, but the scroll direction matters. There are also few operations that require dragging apps, such as editing the main screen. Therefore, we manually split the 'DUAL_POINT' action type into 'CLICK' and 'SCROLL', where 'CLICK' action involves coordinate prediction and 'SCROLL' action is purely textual. The action space is summarized as follows:
如附录 A.2 所述，我们将动作空间简化为 5 种动作类别。其背后的原因是，在 AITW 数据集中，“DUAL_POINT”动作类型无缝覆盖了“CLICK”和“SCROLL”两种动作。在大多数情况下，"SCROLL" 动作的焦点信息较少，但滚动方向很重要。也有少量操作需要拖动应用，例如编辑主屏幕。因此，我们将“DUAL_POINT”动作类型手动拆分为“CLICK”和“SCROLL”，其中“CLICK”涉及坐标预测，而“SCROLL”仅是文本性描述。动作空间总结如下：


- CLICK(coord_y: float, coord_x: float): This action clicks a specific point on the screen. It is necessary to combine the annotation of UI elements to identify the icon and/or area clicked. Note that we use the relative pixel coordinate system, where (0, 0) means the top-left and (1,1) means the bottom right corner of the screen. For example, click (0.11, 0.92) taps a point located at the top-right corner of the screen.
- CLICK(coord_y: float, coord_x: float)：此动作点击屏幕上的某一个点。需要结合 UI 元素标注来识别被点击的图标和/或区域。请注意，我们使用相对像素坐标系，其中 (0, 0) 表示屏幕左上角，(1, 1) 表示屏幕右下角。例如，点击 (0.11, 0.92) 即在屏幕右上角的一个点上点按。


- SCROLL(direction: str): This actions means the finger movements like a real human user. For example, scroll up means the action gesture is from bottom to top, leading either the app drawer to be opened, or the current screen to go down and reveal more contents. There are four options for direction: up, down, left and right.
- SCROLL(direction: str)：此动作表示像真实用户那样的手指滑动。例如，向上滚动意味着滑动手势从下向上进行，可能打开应用抽屉，或使当前屏幕向下滚动以显示更多内容。方向有四个选项：up、down、left、right。


- TYPE(text: str): This action allow the agent to directly type texts into an input field, skipping the inefficient keyboard operations. For example, type "what is CoAT" inputs the string "what is CoAT" to the text input field at one time.
- TYPE(text: str)：此动作允许代理直接在输入框中输入文本，跳过低效的键盘操作。例如，输入 “what is CoAT” 会一次性将字符串“what is CoAT”输入到文本输入框中。


<img src="https://cdn.noedgeai.com/bo_d5vch5ref24c73bqi1u0_13.jpg?x=219&y=196&w=1214&h=307&r=0"/>



Figure 9: The instruction distribution (grouped by verbs and nouns) for SINGLE subset in original AITW dataset.
图 9：原始 AITW 数据集 SINGLE 子集的指令分布（按动词和名词分组）。


<table><tr><td></td><td colspan="3">Model Architecture</td><td colspan="2">Training Data</td></tr><tr><td></td><td>Visual Encoder</td><td>Language Backbone</td><td>Image Resolution</td><td>Pre-training</td><td>Fine-tuning</td></tr><tr><td>AUTO-UI (1.2B)</td><td>Single Encoder (985M) BLIP2-opt-2.7b</td><td>FLAN-alphaca -base(200M)</td><td>224 x 224</td><td>/</td><td>AITW / AITZ</td></tr><tr><td>CogAgent (18B)</td><td>Dual Encoder (11B) Low-Res: EVA2-CLIP-E High-Res: EVA2-CLIP-L</td><td>CogVLM-7B</td><td>1120 x 1120</td><td>276M data spanning over text recognition, visual grounding and gui imagery tasks</td><td>1M data, including Mind2Web, AITW, public VQA data ...</td></tr></table>
<table><tbody><tr><td></td><td colspan="3">模型架构</td><td colspan="2">训练数据</td></tr><tr><td></td><td>视觉编码器</td><td>语言骨干</td><td>图像分辨率</td><td>预训练</td><td>微调</td></tr><tr><td>AUTO-UI (1.2B)</td><td>单一编码器 (985M) BLIP2-opt-2.7b</td><td>FLAN-alphaca -base(200M)</td><td>224 x 224</td><td>/</td><td>AITW / AITZ</td></tr><tr><td>CogAgent (18B)</td><td>双编码器 (11B) 低分辨率: EVA2-CLIP-E 高分辨率: EVA2-CLIP-L</td><td>CogVLM-7B</td><td>1120 x 1120</td><td>涵盖文本识别、视觉对齐与 GUI 图像任务的 276M 数据</td><td>1M 数据，包括 Mind2Web、AITW、公开 VQA 数据等</td></tr></tbody></table>


Table 8: Comparison between AUTO-UI (Zhang and Zhang, 2023) and CogAgent (Hong et al., 2023). Note that, for low-resolution images, following CogVLM (Wang et al., 2023b), CogAgent adopts a visual encoder with 5B parameters and a visual expert module with 6B parameters.
表 8：AUTO-UI（Zhang and Zhang, 2023）与 CogAgent（Hong et al., 2023）的对比。注意，对于低分辨率图像，遵循 CogVLM（Wang et al., 2023b），CogAgent 采用参数量为 5B 的视觉编码器和参数量为 6B 的视觉专家模块。


- PRESS(button: str): The Android system provides several system level shortcut buttons, such as back button that enables the user back to the previous interface, and home button that allows a direct return to the home screen. Moreover, enter button is another virtual button that submits the typed query. This action means to press on one of the system level virtual buttons.
- PRESS(button: str): Android 系统提供若干系统级快捷按钮，例如返回按钮，可使用户返回上一个界面；主页按钮，允许直接返回主屏幕。此外，回车按钮是另一个提交已输入查询的虚拟按钮。此操作意味着点击其中一个系统级虚拟按钮。


- STOP(task_state: str): This action allows the agent to stop and end the query execution in time, either when it considers the task is completed or the task is impossible. For example, stop and set the query as completed means the user query has been successfully completed.
- STOP(task_state: str): 此操作允许代理在合适时刻停止并结束查询执行，或在任务被认为完成或不可能完成时停止。例如，停止并将查询标记为完成，意味着用户查询已成功完成。


We map the actions predicted by AUTO-UI and CogAgent to this space to ensure the reliability and consistency in comparison.
我们将 AUTO-UI 与 CogAgent 预测的操作映射到此空间，以确保比较中的可靠性与一致性。


## B Dataset Details
## B 数据集详情


Since AITZ is built upon AITW, it inherits the dataset structure that contains five subsets, 4 of which are multi-step tasks (GENERAL, GOOGLEAPPs, INSTALL, WEBSHOPPING) and 1 is single-step tasks (SINGLE).
由于 AITZ 建立在 AITW 之上，它继承了包含五个子集的数据集结构，其中 4 个是多步骤任务（GENERAL、GOOGLEAPPs、INSTALL、WEBSHOPPING），1 个是单步任务（SINGLE）。


- GENERAL: Tasks including question-and-answering (i.e. "What is the capital of ...?") and interacting with 3rd party apps/websites (i.e. "Install/Open the xxx app"). Therefore, this subset has 24 apps in total, within which are google chrome( 72%), google maps (9%), google play store (5%), clock (4%), settings (2%) and others (8%). The training split has 19 apps and the testing split has 17 apps, where 12 apps are shared across the training and testing split.
- GENERAL：包含问答（如“...的首都是什么？”）和与第三方应用/网站交互（如“安装/打开 xxx 应用”）的任务。因此，该子集共有 24 个应用，其中 Google Chrome（72%）、Google 地图（9%）、Google Play 商店（5%）、时钟（4%）、设置（2%）及其他（8%）。训练集有 19 个应用，测试集有 17 个应用，其中 12 个应用在训练集与测试集之间共享。


- INSTALL: High-level tasks related to installing, uninstalling and logging into apps. This subset in-vloves 79 apps in total via the entrance of google play store. The training split has 77 apps and the testing split has 71 apps, where 69 apps are shared across the training and testing split.
- INSTALL：与安装、卸载和登录应用相关的高层任务。该子集通过 Google Play 商店入口共包含 79 个应用。训练集有 77 个应用，测试集有 71 个应用，其中 69 个应用在训练集与测试集之间共享。


- WEBSHOPPING: Tasks related to shopping on e-commerce websites, including ebay (17%), amazon (17%), bestbuy (15%), walmart (13%), newegg (10%), target (10%), costco (9%), lowes (2%) and others (7%). As we have stated in Section 3.1, the apps involved in webshopping are relatively fixed, so different instructions are more crucial for distinguishing different scenarios. This is the reason why we have done instruction sampling to separate different instructions.
- WEBSHOPPING：与在电子商务网站购物相关的任务，包括 eBay（17%）、亚马逊（17%）、Best Buy（15%）、沃尔玛（13%）、Newegg（10%）、Target（10%）、Costco（9%）、Lowes（2%）和其他（7%）。如 第 3.1 节所述，网页购物涉及的应用相对固定，因此不同指令在区分不同场景方面更加关键。这也是我们进行指令采样以区分不同指令的原因。


- GOOGLEAPPS: Tasks that involve the use of 14 Google applications, including settings (25%), google chrome (22%), google play store (15%), gmail (14%), google maps (6%), calendar (6%), clock (5%), google photos (3%) and others (4%). The training split spans 14 apps and the testing split spans 10 apps.
- GOOGLEAPPS：涉及使用 14 种 Google 应用的任务，包括设置（25%）、Google Chrome（22%）、Google Play 商店（15%）、Gmail（14%）、Google 地图（6%）、日历（6%）、时钟（5%）、Google 照片（3%）及其他（4%）。训练集覆盖 14 个应用，测试集覆盖 10 个应用。


<table><tr><td>Model</td><td>Prompt</td><td>UI Reps.</td><td>Hit Rate</td><td>Total</td><td>CLICK</td><td>SCROLL</td><td>PRESS</td><td>TYPE</td><td>STOP</td></tr><tr><td rowspan="6">QWen-VL</td><td rowspan="2">CoA</td><td>txt</td><td>82.53</td><td>35.86</td><td>44.96</td><td>34.21</td><td>0</td><td>34.04</td><td>4.08</td></tr><tr><td>tag</td><td>94.48</td><td>44.37</td><td>60.07</td><td>7.89</td><td>0</td><td>48.94</td><td>0</td></tr><tr><td rowspan="2">CoT</td><td>txt</td><td>84.37</td><td>41.61</td><td>56.83</td><td>2.63</td><td>4.35</td><td>40.43</td><td>4.08</td></tr><tr><td>tag</td><td>95.63</td><td>49.43</td><td>69.42</td><td>2.63</td><td>4.35</td><td>40.43</td><td>2.04</td></tr><tr><td rowspan="2">CoAT</td><td>txt</td><td>94.02</td><td>52.41</td><td>72.3</td><td>7.89</td><td>13.04</td><td>34.04</td><td>10.2</td></tr><tr><td>tag</td><td>96.32</td><td>51.95</td><td>70.5</td><td>2.63</td><td>8.7</td><td>46.81</td><td>10.2</td></tr><tr><td rowspan="6">Gemini-PV</td><td rowspan="2">CoA</td><td>txt</td><td>89.43</td><td>42.99</td><td>60.79</td><td>13.16</td><td>4.35</td><td>21.28</td><td>4.08</td></tr><tr><td>tag</td><td>99.77</td><td>54.48</td><td>79.86</td><td>10.53</td><td>13.04</td><td>10.64</td><td>6.12</td></tr><tr><td rowspan="2">CoT</td><td>txt</td><td>95.86</td><td>49.2</td><td>67.27</td><td>26.32</td><td>21.74</td><td>19.15</td><td>6.12</td></tr><tr><td>tag</td><td>97.47</td><td>51.95</td><td>74.46</td><td>21.05</td><td>13.04</td><td>12.77</td><td>4.08</td></tr><tr><td rowspan="2">CoAT</td><td>txt</td><td>97.01</td><td>52.41</td><td>69.42</td><td>23.68</td><td>30.43</td><td>34.04</td><td>6.12</td></tr><tr><td>tag</td><td>95.4</td><td>53.33</td><td>72.66</td><td>23.68</td><td>21.74</td><td>29.79</td><td>4.08</td></tr><tr><td rowspan="6">GPT-4V</td><td rowspan="2">CoA</td><td>txt</td><td>92.41</td><td>55.17</td><td>74.1</td><td>42.11</td><td>39.13</td><td>8.51</td><td>10.2</td></tr><tr><td>tag</td><td>99.31</td><td>62.76</td><td>86.69</td><td>44.74</td><td>26.09</td><td>14.89</td><td>4.08</td></tr><tr><td rowspan="2">CoT</td><td>txt</td><td>98.16</td><td>66.21</td><td>89.57</td><td>39.47</td><td>39.13</td><td>12.77</td><td>18.37</td></tr><tr><td>tag</td><td>97.01</td><td>64.14</td><td>86.33</td><td>39.47</td><td>39.13</td><td>21.28</td><td>10.2</td></tr><tr><td rowspan="2">CoAT</td><td>txt</td><td>98.39</td><td>71.72</td><td>86.33</td><td>47.37</td><td>43.48</td><td>48.94</td><td>42.86</td></tr><tr><td>tag</td><td>98.16</td><td>71.49</td><td>86.69</td><td>42.11</td><td>43.48</td><td>57.45</td><td>34.69</td></tr></table>
<table><tbody><tr><td>模型</td><td>提示</td><td>UI 表示</td><td>命中率</td><td>总计</td><td>点击</td><td>滚动</td><td>按压</td><td>输入</td><td>停止</td></tr><tr><td rowspan="6">QWen-VL</td><td rowspan="2">CoA</td><td>txt</td><td>82.53</td><td>35.86</td><td>44.96</td><td>34.21</td><td>0</td><td>34.04</td><td>4.08</td></tr><tr><td>tag</td><td>94.48</td><td>44.37</td><td>60.07</td><td>7.89</td><td>0</td><td>48.94</td><td>0</td></tr><tr><td rowspan="2">CoT</td><td>txt</td><td>84.37</td><td>41.61</td><td>56.83</td><td>2.63</td><td>4.35</td><td>40.43</td><td>4.08</td></tr><tr><td>tag</td><td>95.63</td><td>49.43</td><td>69.42</td><td>2.63</td><td>4.35</td><td>40.43</td><td>2.04</td></tr><tr><td rowspan="2">CoAT</td><td>txt</td><td>94.02</td><td>52.41</td><td>72.3</td><td>7.89</td><td>13.04</td><td>34.04</td><td>10.2</td></tr><tr><td>tag</td><td>96.32</td><td>51.95</td><td>70.5</td><td>2.63</td><td>8.7</td><td>46.81</td><td>10.2</td></tr><tr><td rowspan="6">Gemini-PV</td><td rowspan="2">CoA</td><td>txt</td><td>89.43</td><td>42.99</td><td>60.79</td><td>13.16</td><td>4.35</td><td>21.28</td><td>4.08</td></tr><tr><td>tag</td><td>99.77</td><td>54.48</td><td>79.86</td><td>10.53</td><td>13.04</td><td>10.64</td><td>6.12</td></tr><tr><td rowspan="2">CoT</td><td>txt</td><td>95.86</td><td>49.2</td><td>67.27</td><td>26.32</td><td>21.74</td><td>19.15</td><td>6.12</td></tr><tr><td>tag</td><td>97.47</td><td>51.95</td><td>74.46</td><td>21.05</td><td>13.04</td><td>12.77</td><td>4.08</td></tr><tr><td rowspan="2">CoAT</td><td>txt</td><td>97.01</td><td>52.41</td><td>69.42</td><td>23.68</td><td>30.43</td><td>34.04</td><td>6.12</td></tr><tr><td>tag</td><td>95.4</td><td>53.33</td><td>72.66</td><td>23.68</td><td>21.74</td><td>29.79</td><td>4.08</td></tr><tr><td rowspan="6">GPT-4V</td><td rowspan="2">CoA</td><td>txt</td><td>92.41</td><td>55.17</td><td>74.1</td><td>42.11</td><td>39.13</td><td>8.51</td><td>10.2</td></tr><tr><td>tag</td><td>99.31</td><td>62.76</td><td>86.69</td><td>44.74</td><td>26.09</td><td>14.89</td><td>4.08</td></tr><tr><td rowspan="2">CoT</td><td>txt</td><td>98.16</td><td>66.21</td><td>89.57</td><td>39.47</td><td>39.13</td><td>12.77</td><td>18.37</td></tr><tr><td>tag</td><td>97.01</td><td>64.14</td><td>86.33</td><td>39.47</td><td>39.13</td><td>21.28</td><td>10.2</td></tr><tr><td rowspan="2">CoAT</td><td>txt</td><td>98.39</td><td>71.72</td><td>86.33</td><td>47.37</td><td>43.48</td><td>48.94</td><td>42.86</td></tr><tr><td>tag</td><td>98.16</td><td>71.49</td><td>86.69</td><td>42.11</td><td>43.48</td><td>57.45</td><td>34.69</td></tr></tbody></table>


Table 9: Complete comparison results of three prompting methods on Qwen-VL-Max, Gemini-1.0-Pro-Vision and GPT-4V. "Prompt" means different prompting methods. "UI Reps." denotes the representation methods of screen elements, including set-of-mark tagging (tag) and textual representation (txt). "Hit Rate" means the format hit rate. The evaluation metric is the action prediction accuracy(%).
表9：三种提示方法在 Qwen-VL-Max、Gemini-1.0-Pro-Vision 和 GPT-4V 上的完整对比结果。“Prompt”表示不同的提示方法。“UI Reps.”表示屏幕元素的表示方法，包括集合标记(tag)和文本表示(txt)。“Hit Rate”表示格式命中率。评估指标为动作预测准确率（%）。


- SINGLE: Single-step tasks that mainly come from WebShopping, spanning about 10 apps. Only used for training.
- SINGLE：来自 WebShopping 的单步任务，大约涵盖 10 个应用。仅用于训练。/


## C Experiment Details
## C 实验细节


### C.1 Comparison between Prompting Methods
### C.1 提示方法之间的对比


In Section 2.2 we conducted a preliminary experiment to demonstrate that CoAT is more effective than previous context modeling methods. Specifically, for CoA prompting, the input to GUI agents includes system prompt, current screenshot, history actions and user request. For CoT prompting, the input to GUI agents includes system prompt, current screenshot and user request. For CoAT promprting, we firstly require the agent to observe current screenshot and generate screen descriptions. Then, the input contains system prompt, current screenshot, screen description, history actions, previous action results and user request.
在 2.2 节中，我们进行了一项初步实验，以证明 CoAT 相较于以往的上下文建模方法更为有效。具体而言，对于 CoA 提示，输入给 GUI 代理的内容包括系统提示、当前屏幕截图、历史操作和用户请求。对于 CoT 提示，输入包括系统提示、当前屏幕截图和用户请求。对于 CoAT 提示，我们首先要求代理观察当前屏幕截图并生成屏幕描述。然后，输入包含系统提示、当前屏幕截图、屏幕描述、历史操作、上一次操作结果和用户请求。


For all threee prompting methods, the system prompt contains information about the valid action space and corresponding desired output format. If the representation of UI elements is set-of-mark tagging, another screenshot with annotated UI elements will be additionally added to the input, otherwise a textual representation of UI elements is appended. Figure 10 show a visualization example of these two UI representations.
对于三种提示方法，系统提示包含关于有效动作空间及相应输出格式的信息。如果 UI 元素的表示是集合标记(tag)，则会在输入中再附加一个带注释的截图；否则会附加对 UI 元素的文本表示。Figure 10 展示了这两种 UI 表示的可视化示例。


The complete experiment results are shown in Table 9. From Table 9, GPT-4V prompted by CoAT takes the lead position in the overall performance and in the prediction of each type of actions. Compared with plain textual representations, agents equipped with set-of-mark tagging generally performs better. This encourages future work to put more emphasis on the visual perception of UI elements, improve the image resolution and multi-image processing ability of GUI agents.
完整的实验结果如表 9 所示。从表 9 可以看出，在整体性能及对每种类型动作的预测上，GPT-4V 通过 CoAT 获得领先。与纯文本表示相比，使用集合标记的代理通常表现更好。这促使未来工作更加重视 UI 元素的视觉感知、提高图像分辨率以及 GUI 代理的多图像处理能力。


### C.2 Comparison between Baselines
### C.2 基线方法之间的对比


As shown in Table 5, we conclude that "AUTO-UI + CoAT is on par with CogAgent-Chat-18B" based on the fact that the model architecture and training data of AUTO-UI is inferior to CogAgent, but after fine-tuning on AITZ dataset, they achieve similar performance on goal process (AUTO-UI is even slightly higher.) We summarize the differences between two models in Table 8 for a quick look. Following is the detailed explanation:
如表 5 所示，我们基于 AUTO-UI 的模型架构和训练数据劣于 CogAgent 的事实，得出“AUTO-UI + CoAT 与 CogAgent-Chat-18B 水平相当”的结论，但在 AITZ 数据集上的微调后，它们在目标过程上的表现相近（AUTO-UI 甚至略高）。为快速查看，我们在表 8 总结两者的差异。以下为详细解释：


<img src="https://cdn.noedgeai.com/bo_d5vch5ref24c73bqi1u0_15.jpg?x=191&y=197&w=1247&h=629&r=0"/>



Figure 10: Visualization of Set-Of-Mark tagging and corresponding textual representations.
Figure 10：集合标记（Set-Of-Mark）标注与相应文本表示的可视化。


1. The lesser volume of training data used by AUTO-UI compared with CogAgent. Specifically, AUTO-UI underwent fine-tuning solely on the AITZ dataset, in contrast to CogAgent's extensive fine-tuning across the entire AITW dataset. Moreover, CogAgent introduced GUI imagery tasks during the pre-training phase. Hence, it is highly optimized for GUI scenarios.
1. 与 CogAgent 相比，AUTO-UI 的训练数据量较少。具体而言，AUTO-UI 仅在 AITZ 数据集上进行微调，而 CogAgent 则在整个 AITW 数据集上进行了广泛微调。此外，CogAgent 在预训练阶段引入了 GUI 图像任务。因此，它在 GUI 场景中高度优化。


2. The different resolution of visual encoders. Specifically, AUTO-UI employs the visual encoder from BLIP2 with a 224 x 224 resolution, whereas CogAgent combines ViT-L with the visual encoder from CogVLM to scale the resolution up to 1120 x 1120.
2. 视觉编码器的分辨率不同。具体而言，AUTO-UI 使用 BLIP2 的视觉编码器，分辨率为 224 x 224；而 CogAgent 将 ViT-L 与 CogVLM 的视觉编码器结合，将分辨率提升到 1120 x 1120。


3. Despite AUTO-UI + CoAT being trained with significantly less data and without any additional pre-training efforts, it managed to outperform CogAgent in terms of action prediction accuracy and goal progress, underscoring the effectiveness and value of our proposed method and dataset, as shown in Table 5.
3. 尽管 AUTO-UI + CoAT 的训练数据明显较少且没有额外的预训练努力，但在动作预测准确性和目标进展方面仍然超越 CogAgent，凸显我们所提出的方法和数据集的有效性和价值，如表 5 所示。


## D Discussions About Screen Description
## D 关于屏幕描述的讨论


As we have stated in Section 5.2, the image resolution that LMMs can handle is crucial for understanding the screen description. Our statement is supported by two experiments:
正如我们在 5.2 节所述，LMM 能处理的图像分辨率对理解屏幕描述至关重要。我们有两项实验来支持这一点：


(1) An exploration experiment on Monkey (Li et al., 2023b), with screen description as additional input: Monkey is a large multimodal model that could process images with resolutions up to 1344x896. We ablate the usage of screen description, and train the model to output the action think together with the action decision for 2 epochs. The total action matching score rises from 22.7% to 26.3%. (2) A validation experiment on UI-Hawk (Zhang et al., 2024b), with screen description as learning target. UI-Hawk is a specialized version of TextHawk (Yu et al., 2024) for UI understanding and we have observed similar improvements. For UI-Hawk, we integrate the learning of screen description by separating the training process into two stages. During stage one, the model learns to describe the screen. During stage two, the model learns to decide on its next-step action on a more complicated navigation dataset, GUI-Odyssey (Lu et al., 2024b). The total action matching score rises from 69% towards 72% by adding the stage one training process.
(1) Monkey 的探索性实验（Li 等，2023b），将屏幕描述作为额外输入：Monkey 是一个可处理高达 1344x896 分辨率图像的大型多模态模型。我们在训练时去掉屏幕描述的使用，让模型在 2 个时期内输出动作思考与动作决策。动作匹配总分从 22.7% 提升至 26.3%。(2) UI-Hawk 的验证实验（Zhang 等，2024b），以屏幕描述作为学习目标。UI-Hawk 是 TextHawk（Yu 等，2024）的一个 UI 理解的专用版本，我们观察到类似的改进。对于 UI-Hawk，我们通过将训练过程分为两个阶段来整合屏幕描述的学习。第一阶段，模型学习描述屏幕；第二阶段，模型在更复杂的导航数据集 GUI-Odyssey（Lu 等，2024b）上学习决定下一步操作。通过加入第一阶段训练，总体动作匹配分数从 69% 提升到接近 72%。


We leave it for future work to conduct a thorough analysis on the influencing factors of screen description, such as image resolution, model architecture, UI related pre-training, etc.
我们将其留给未来工作，对屏幕描述的影响因素进行深入分析，如图像分辨率、模型架构、与 UI 相关的预训练等。