# AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents
# AMEX: 面向移动 GUI 代理的 Android 多注解 Expo 数据集


Yuxiang Chai ${}^{1 * }$ , Siyuan Huang ${}^{2 * }$ , Yazhe Niu ${}^{1}$ , Han Xiao ${}^{1}$ , Liang Liu ${}^{3}$ , Guozhi Wang ${}^{3}$ , Dingyu Zhang ${}^{1}$ , Shuai Ren ${}^{3}$ , Hongsheng Li ${}^{1 \dagger  }$ ,
Yuxiang Chai ${}^{1 * }$ , Siyuan Huang ${}^{2 * }$ , Yazhe Niu ${}^{1}$ , Han Xiao ${}^{1}$ , Liang Liu ${}^{3}$ , Guozhi Wang ${}^{3}$ , Dingyu Zhang ${}^{1}$ , Shuai Ren ${}^{3}$ , Hongsheng Li ${}^{1 \dagger  }$ ,


${}^{1}$ MMLab @ CUHK, ${}^{2}$ SJTU, ${}^{3}$ vivo AI Lab,
${}^{1}$ MMLab @ CUHK, ${}^{2}$ SJTU, ${}^{3}$ vivo AI Lab,


*Equal contribution, †Corresponding author: hsli@ee.cuhk.edu.hk
* equal contribution, † 投稿作者: hsli@ee.cuhk.edu.hk


## Abstract
## 摘要


AI agents have drawn increasing attention mostly on their ability to perceive environments, understand tasks, and autonomously achieve goals. To advance research on AI agents in mobile scenarios, we introduce the Android Multi-annotation EXpo (AMEX), a comprehensive, large-scale dataset designed for generalist mobile GUI-control agents which are capable of completing tasks by directly interacting with the graphical user interface (GUI) on mobile devices. AMEX comprises over 104K high-resolution screenshots from popular mobile applications, which are annotated at multiple levels. Unlike existing GUI-related datasets, e.g., Rico (Deka et al., 2017), AITW (Rawles et al., 2024b), etc., AMEX includes three levels of annotations: GUI interactive element grounding, GUI screen and element functionality descriptions, and complex natural language instructions with stepwise GUI-action chains. We develop this dataset from a more instructive and detailed perspective, complementing the general settings of existing datasets. Additionally, we finetune a baseline model SPHINX Agent and illustrate the effectiveness of AMEX.
AI 代理受到了越来越多的关注，主要在于它们感知环境、理解任务并自主实现目标的能力。为推动移动场景下的 AI 代理研究，我们引入 Android 多注解 Expo（AMEX），这是一个面向通用移动 GUI 控制代理的综合性、大规模数据集，能够通过直接与移动设备的图形用户界面（GUI）交互来完成任务。AMEX 包含来自流行移动应用的超过 104K 张高分辨率截图，具有多级注释。与现有 GUI 相关数据集（如 Rico（Deka 等，2017）、AITW（Rawles 等，2024b）等）不同，AMEX 包含三层注释：GUI 交互元素定位、GUI 屏幕与元素的功能描述，以及带有逐步 GUI 动作链的复杂自然语言指令。我们从更具指导性和细化的角度发展该数据集，补充现有数据集的一般设置。此外，我们对基线模型 SPHINX Agent 进行了微调，并展示了 AMEX 的有效性。


## 1 Introduction
## 1 引言


In recent years, AI-powered virtual assistants, such as Siri, Bixby, and Xiao AI, have evolved as key tools for facilitating interactions between users and mobile devices. These assistants have demonstrated to be effective in managing routine tasks such as setting alarms, performing web searches, and reporting weather conditions. However, their functionality is often restricted to system-built applications or third-party apps supported by application programming interfaces (APIs). This limitation highlights a significant gap between the capabilities of current AI assistants and the diverse, open-ended ways humans interact with mobile devices.
近年来，基于 AI 的虚拟助手，如 Siri、Bixby、Xiao AI，作为促进用户与移动设备交互的关键工具而发展起来。这些助手在设定闹钟、进行网络搜索、报告天气等日常任务方面已被证明是有效的。然而，它们的功能通常仅限于系统内置应用或由应用程序接口（API）支持的第三方应用。这一局限性突显了当前 AI 助手能力与人类在移动设备上进行多样化、开放式交互之间的显著差距。


Unlike AI assistants, human users can accomplish tasks on mobile devices by relying solely on visual information from the screen. Humans intuitively interpret graphical user interfaces (GUIs), analyze page layouts, and infer the functionalities of interactive elements to complete tasks in various apps. Inspired by this human ability, researchers are exploring new paradigms for mobile interaction that leverage vision and natural language processing. One such paradigm involves the development of Mobile GUI-Control Agents, or GUI Agents, which aim to directly interact with screen elements based on visual and textual inputs, such as screenshots and natural language instructions. These agents hold the potential to transcend the limitations of traditional API-based assistants, enabling more flexible and universal interactions with any app or mobile interface.
与 AI 助手不同，人类用户能够仅凭屏幕上的可视信息在移动设备上完成任务。人类直观地解读图形用户界面（GUI），分析页面布局，推断交互元素的功能，以在各类应用中完成任务。受此人类能力启发，研究人员正在探索利用视觉与自然语言处理的新型移动交互范式。其中一种范式是开发移动 GUI 控制代理，或 GUI 代理，旨在基于视觉和文本输入（如截图和自然语言指令）直接与屏幕元素交互。这些代理有望超越传统基于 API 的助手的局限性，实现对任何应用或移动界面的更灵活、普遍的交互。


Existing GUI agents show promise but face significant challenges in real-world applications. Their effectiveness is possibly limited by a lack of understanding of GUI layouts and the functionalities of interactive elements through the observation of wrong predictions. These limitations largely arise from the absence of comprehensive datasets that adequately reflect the complexity and diversity of mobile GUI environments. While several instructional datasets (Burns et al., 2021; Sunkara et al., 2022; Gubbi Venkatesh et al., 2024; Rawles et al., 2024b) have been introduced to address this issue, they struggle with limitations such as inaccurate annotations, insufficient task diversity, and a lack of representativeness for general mobile usage. Similarly, other efforts (Deka et al., 2017; Li et al., 2020; Feng et al., 2024; Bunian et al., 2021) that focus on GUI element annotation are constrained by limited dataset scale, outdated app versions, and traditional annotation styles that do not provide sufficient information for GUI agents.
现有 GUI 代理虽有前景，但在实际应用中仍面临重大挑战。其有效性可能受限于对 GUI 布局及交互元素功能的理解不足，导致观察到错误预测。这些局限性主要源于缺乏能充分反映移动 GUI 环境的复杂性和多样性的综合数据集。尽管已有若干教学数据集（Burns 等，2021；Sunkara 等，2022；Gubbi Venkatesh 等，2024；Rawles 等，2024b）被提出以解决这一问题，但它们在注释不准确、任务多样性不足，以及对一般移动使用的代表性不足等方面存在局限性。类似地，其他专注于 GUI 元素注释的工作（Deka 等，2017；Li 等，2020；Feng 等，2024；Bunian 等，2021）也受到数据集规模有限、应用版本过时、传统注释风格等限制，无法为 GUI 代理提供充分信息。


Human users approach GUI-based tasks by integrating multiple cognitive processes. They interpret the overall layout of the page, identify interactive elements and assess their functionality, such as recognizing that a bell icon likely leads to notifications and a magnifier icon likely triggers a search function. Users then decompose natural language instructions into actionable steps, navigating across multiple screens to achieve their goals. To design effective GUI agents, it is essential to replicate these human cognitive processes, which requires datasets that comprehensively capture the structure, functionality, and interaction patterns of mobile GUIs.
人类用户在基于 GUI 的任务中会整合多种认知过程。他们解读页面的整体布局，识别交互元素并评估其功能，例如识别铃铛图标可能引导到通知，放大镜图标可能触发搜索功能。然后用户将自然语言指令分解为可执行步骤，在多张屏幕间导航以实现目标。为了设计有效的 GUI 代理，必须复制这些人类认知过程，这需要能够全面捕捉移动 GUI 的结构、功能和交互模式的数据集。


<img src="https://cdn.noedgeai.com/bo_d5vcgqjef24c73bqi1jg_1.jpg?x=263&y=192&w=1122&h=732&r=0"/>



Figure 1: An example of a screenshot-instruction (Blue tab) pair illustrating the multi-level annotation of AMEX. Red boxes + Brown tabs: selected GUI interactive elements and their corresponding functionalities. Green tab: the detailed annotation of the element. Yellow tab: the description of the entire screenshot. Purple hand icon + tab: the current action and the annotation.
图 1：一个屏幕截图-指令（蓝色选项卡）对的示例，展示 AMEX 的多级标注。红色框 + 棕色选项卡：所选的 GUI 交互元素及其对应功能。绿色选项卡：元素的详细标注。黄色选项卡：整个屏幕截图的描述。紫色手形图标 + 选项卡：当前操作与标注。


To address this need, we introduce a new dataset, the Android Multi-annotation EXpo (AMEX), specifically designed to advance the development of GUI agents by providing a multi-level understanding of mobile GUIs. AMEX includes three levels of annotations: (i) GUI interactive element grounding, (ii) GUI screen and element functionality descriptions, and (iii) instructions with GUI-action chains. The dataset comprises over 104K high-resolution screenshots, 21K screen descriptions with ${300}\mathrm{\;K}$ element-wise functionalities,and approximately 3,000 unique complex instructions, with an average of 12.8 steps (see Figure 1, Table 1, and Table 2). To ensure annotation precision and quality, all annotations, including bounding boxes, screen and element functionalities, and instructions with GUI-action chains, are verified by human annotators trained with detailed guidelines.
为满足这一需求，我们引入了一个新数据集 Android 多级标注资料库（AMEX），专门设计用于通过提供对移动 GUI 的多层次理解来推动 GUI 代理的发展。AMEX 包含三层标注：(i) GUI 交互元素定位，(ii) GUI 屏幕及元素功能描述，以及 (iii) 带 GUI-动作链的指令。数据集包含超过 104K 张高分辨率截图、21K 个屏幕描述及 ${300}\mathrm{\;K}$ 元素级功能描述，以及大约 3,000 个独特的复杂指令，平均 12.8 步（见图 1、表 1 与表 2）。为确保标注的准确性和质量，所有标注（包括边界框、屏幕与元素功能描述，以及带 GUI-动作链的指令）均由经过详细指南培训的人类标注人员进行核验。


Our contributions can be summarized as follows: (a) We collect and release AMEX, a multi-level dataset, providing reliable and detailed understandings of the smartphone GUI environment, which can serve as a strong supplementary dataset to boost the performance of GUI agents; (b) We train SPHINX Agents, which can serve as the baseline models for future researches on GUI agents and illustrates the effectiveness of AMEX.
我们的贡献可概括如下： (a) 我们收集并发布 AMEX，一套多层数据集，提供对智能手机 GUI 环境的可靠且细致的理解，可作为强有力的补充数据集以提升 GUI 代理的表现；(b) 我们训练 SPHINX 代理，可作为未来 GUI 代理研究的基线模型，并展示 AMEX 的有效性。


## 2 Related Work
## 2 相关工作


### 2.1 GUI-related Datasets
### 2.1 GUI 相关数据集


Table 1 compares several popular GUI-related datasets on Android. While some works (Deng et al., 2024; Liu et al., 2018; Shi et al., 2017; Wu et al., 2023) focus on the web or desktop platforms, on Android OS, many works (Deka et al., 2017; Li et al., 2020; Feng et al., 2024; Bunian et al., 2021) have focused on identifying various types of GUI elements, assigning classes to different elements. This annotation style can be defined as a traditional icon classification and detection problem. In many cases, however, the functionality of an icon depends on the context of the GUI and many elements on mobile devices are not an icon and their functionalities are also critical. Besides, most of the datasets (Deka et al., 2017; Bunian et al., 2021; Li et al., 2020) are outdated, where the screenshot interfaces are completely different from modern apps. The most recent MUD (Feng et al., 2024) only provides 18K screenshots, which are insufficient for understanding the layouts and elements for LLMs.
表 1 对几种流行的 Android GUI 相关数据集进行比较。虽然部分工作（Deng 等，2024；Liu 等，2018；Shi 等，2017；Wu 等，2023）聚焦于网页或桌面平台，在 Android 操作系统上，许多工作（Deka 等，2017；Li 等，2020；Feng 等，2024；Bunian 等，2021）则聚焦于识别各种 GUI 元素、为不同元素赋予类别。这种标注风格可被定义为传统的图标分类与检测问题。然而，在许多情况下，图标的功能取决于 GUI 的上下文，而移动设备上的许多元素并非图标，其功能也同样关键。此外，大多数数据集（Deka 等，2017；Bunian 等，2021；Li 等，2020）已经过时，截图界面与现代应用完全不同。最新的 MUD（Feng 等，2024）仅提供 18K 张截图，难以让大模型理解布局和元素。


Table 1: Comparison of mobile GUI-related datasets. Scale: the number of unique instructions on general third-party apps, average steps per instruction, number of screenshots and element functionality descriptions. Diversity: screenshot description, element labels, element functionality and the action details for stepwise operation.
表 1：移动 GUI 相关数据集的比较。尺度：通用第三方应用的独特指令数量、每条指令的平均步数、截图数量及元素功能描述数量。多样性：截图描述、元素标签、元素功能与逐步操作的行动细节。


<table><tr><td>Dataset</td><td>Screen Desc.</td><td>Screen Element</td><td>Element Func.</td><td>Task & Action</td><td>#Screen-shots</td><td>#Element Func.</td><td>#Unique General Inst.</td><td>#Avg Steps</td></tr><tr><td>RICO</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>72K</td><td>-</td><td>-</td><td>-</td></tr><tr><td>RICO semantics</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>72K</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VINS</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>4K</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MUD</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>18K</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PixelHelp</td><td>✘</td><td>✘</td><td>✘</td><td>✓</td><td>800</td><td>-</td><td>187</td><td>4.2</td></tr><tr><td>UGIF</td><td>✘</td><td>✘</td><td>✘</td><td>✓</td><td>3.3K</td><td>-</td><td>480</td><td>6.3</td></tr><tr><td>MoTIF</td><td>✘</td><td>✘</td><td>✘</td><td>✓</td><td>21K</td><td>-</td><td>480</td><td>4.5</td></tr><tr><td>AitW</td><td>✘</td><td>*</td><td>✘</td><td>✓</td><td>510K</td><td>-</td><td>1539</td><td>6.5</td></tr><tr><td>Aitz</td><td>✓</td><td>*</td><td>☒</td><td>✓</td><td>18K</td><td>18K</td><td>2504</td><td>7.5</td></tr><tr><td>ANDROIDCONTROL</td><td>✘</td><td>*</td><td>✘</td><td>✓</td><td>99K</td><td>-</td><td>15,283</td><td>4.8</td></tr><tr><td>AMEX</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>104K</td><td>296K</td><td>3046</td><td>12.8</td></tr></table>
<table><tbody><tr><td>数据集</td><td>屏幕描述</td><td>屏幕元素</td><td>元素功能</td><td>任务与操作</td><td>屏幕截图</td><td>元素功能</td><td>唯一通用实例</td><td>平均步骤数</td></tr><tr><td>RICO</td><td>×</td><td>✓</td><td>×</td><td>×</td><td>72K</td><td>-</td><td>-</td><td>-</td></tr><tr><td>RICO 语义</td><td>×</td><td>✓</td><td>×</td><td>×</td><td>72K</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VINS</td><td>×</td><td>✓</td><td>×</td><td>×</td><td>4K</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MUD</td><td>×</td><td>✓</td><td>×</td><td>×</td><td>18K</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PixelHelp</td><td>×</td><td>×</td><td>×</td><td>✓</td><td>800</td><td>-</td><td>187</td><td>4.2</td></tr><tr><td>UGIF</td><td>×</td><td>×</td><td>×</td><td>✓</td><td>3.3K</td><td>-</td><td>480</td><td>6.3</td></tr><tr><td>MoTIF</td><td>×</td><td>×</td><td>×</td><td>✓</td><td>21K</td><td>-</td><td>480</td><td>4.5</td></tr><tr><td>AitW</td><td>×</td><td>*</td><td>×</td><td>✓</td><td>510K</td><td>-</td><td>1539</td><td>6.5</td></tr><tr><td>Aitz</td><td>✓</td><td>*</td><td>☒</td><td>✓</td><td>18K</td><td>18K</td><td>2504</td><td>7.5</td></tr><tr><td>ANDROIDCONTROL</td><td>×</td><td>*</td><td>×</td><td>✓</td><td>99K</td><td>-</td><td>15,283</td><td>4.8</td></tr><tr><td>AMEX</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>104K</td><td>296K</td><td>3046</td><td>12.8</td></tr></tbody></table>


* indicates elements are mis-annotated and the bounding boxes are not well-aligned.
* 表示元素被错误注释，边界框未对齐。


indicates containing only action results of the element interacted at each step.
表示仅包含在每一步与之交互的元素的操作结果。


Table 2: AMEX statistics
表2：AMEX 统计数据


<table><tr><td>#Screenshots</td><td>#Apps</td><td>#Interactive Elements</td><td>#Functionalities</td><td>#Instructions</td><td>#Avg. Steps</td></tr><tr><td>104,876</td><td>192</td><td>1,659,647</td><td>296,075</td><td>3046</td><td>12.8</td></tr></table>
<table><tbody><tr><td>#屏幕截图</td><td>#应用</td><td>#交互元素</td><td>#功能</td><td>#说明</td><td>#平均步骤</td></tr><tr><td>104,876</td><td>192</td><td>1,659,647</td><td>296,075</td><td>3046</td><td>12.8</td></tr></tbody></table>


Other studies (Burns et al., 2021; Rawles et al., 2024b; Zhang et al., 2024; Gubbi Venkatesh et al., 2024) primarily emphasize action-observation pairs during instructional operations, but their annotations are limited and often require supplemental View Hierarchy (VH) data for each screen-shot. AITW (Rawles et al., 2024b) provides both screen GUI element annotations and instructions, but it includes only a small portion of instructions for third-party apps, with most operations conducted on Chrome and other system-built apps. Additionally, each instruction is repeated multiple times, resulting in significant data redundancy. In response, AITZ (Zhang et al., 2024) filters AITW thoroughly, selecting 2.5K unique instructions and episodes, and introduces the Chain-of-Action-Thought framework to annotate action results and page descriptions better. Despite this refinement, AITZ contains only 18K screen-action pairs. ANDROIDCONTROL (Li et al., 2024) is another large-scale instruction-based dataset, however, the element-wise annotations contain heavy redundancy and misaligned cases due to the lack of human verification.
其他研究（ Burns 等，2021； Rawles 等，2024b； Zhang 等，2024； Gubbi Venkatesh 等，2024）主要强调在教学操作中的动作-观察对，但其注释有限，通常需要为每个屏幕快照补充视图层次（VH）数据。AITW（Rawles 等，2024b）同时提供屏幕 GUI 元素注释和指令，但仅包含第三方应用的大量指令中的一小部分，大多数操作在 Chrome 及其他系统自带应用上进行。此外，每条指令重复多次，造成显著的数据冗余。为此，AITZ（Zhang 等，2024）对 AITW 进行彻底筛选，挑选出 2.5K 条唯一指令与剧集，并引入行动-思维链（Chain-of-Action-Thought）框架，以更好地注释行动结果与页面描述。尽管有这一改进，AITZ 仅包含 18K 条屏-行动对。ANDROIDCONTROL（Li 等，2024）是另一项大规模基于指令的数据集，然而由于缺乏人工验证，元素级注释包含大量冗余且对齐错误的情况。


### 2.2 GUI-related Systems and Agents
### 2.2 与 GUI 相关的系统与智能体


(Wang et al., 2023) first starts to apply Large Language Model (LLM) on GUI, but it remains on tasks only interacted on single page, which are more like question-answer tasks rather than end-to-end instructional tasks. Recent advancements have leveraged the extensive world knowledge and robust embodied capabilities of LLMs (Gu and Dao, 2024; Gur et al., 2023; Touvron et al., 2023) for task planning and reasoning. Works (Zhang et al., 2023; Zheng et al., 2024) have utilized business-level models (e.g., GPT-4V), employing extensive prompt engineering to guide the LLM in executing complex tasks. The effectiveness of these methods requires a meticulous prompt design to achieve optimal results. Alternatively, another research line focuses on fine-tuning smaller LLMs on GUI-specific datasets to imbue them with domain-specific knowledge. For example, CogA-gent (Hong et al., 2024) enhances performance in GUI-related tasks by integrating a high-resolution cross-module that fuses image features from various levels. CoCo-Agent (Ma et al., 2024) and ANDROIDCONTROL (Li et al., 2024), unlike other agents taking only screenshots as input, use element layouts from accessibility trees or view hierarchy as additional input to enhance the performance. However, still many apps don't support accessibility information or only provide very little of that.
Wang 等，2023 首次将大语言模型（LLM）应用于 GUI，但其任务仍仅限于在单页上互动，更多像问答任务而非端到端指令任务。最近的进展利用了 LLM 的广泛世界知识与强健的具身能力（Gu 与 Dao，2024；Gur 等，2023；Touvron 等，2023）来进行任务规划与推理。Zhang 等，2023；Zheng 等，2024 的工作则使用企业级模型（如 GPT-4V），通过广泛的提示工程来引导 LLM 执行复杂任务。这些方法的有效性需要精心设计的提示以实现最佳结果。另一条研究路线则专注于在 GUI 专用数据集上对较小的 LLM 进行微调，以赋予其领域知识。例如，CogA-gent（Hong 等，2024）通过整合一个高分辨率跨模块，将来自各级别的图像特征融合，提升 GUI 相关任务的性能。CoCo-Agent（Ma 等，2024）和 ANDROIDCONTROL（Li 等，2024）与仅将屏幕截图作为输入的其他智能体不同，使用无障碍树或视图层次中的元素布局作为附加输入以提升性能。然而，仍有许多应用并不支持无障碍信息，或仅提供极少量此类信息。


<img src="https://cdn.noedgeai.com/bo_d5vcgqjef24c73bqi1jg_3.jpg?x=213&y=196&w=1230&h=695&r=0"/>



Figure 2: Overview of the data collection pipeline. The raw data is from two subsets collected by human annotators and an autonomous tool. Annotators record the GUI-action chains simultaneously while collecting the screenshots and corresponding XMLs. Then raw data is sent to annotators to filter GUI element bounding boxes, and then the boxes with corresponding raw screenshots are sent as input to GPT and Gemini to extract the GUI screen and element descriptions, which are then manually checked by humans.
图 2：数据采集管道概览。原始数据来自两组由人工标注者和一个自动化工具收集的子集。标注者在收集屏幕截图及相应 XML 的同时，记录 GUI 操作链。然后将原始数据送给标注者以过滤 GUI 元素边界框，然后带有相应原始屏幕截图的边框框被作为输入送入 GPT 与 Gemini，以提取 GUI 屏幕与元素描述，接着由人工进行人工核对。


## 3 Android Multi-annotation EXpo (AMEX)
## 3 Android 多注释 EXpo（AMEX）


When receiving an instruction, a human user first analyzes the overall screen layout to form a basic understanding of the current Android environment. The user then identifies interactive elements and areas, and assesses the functionalities of those elements. Finally, the user breaks down the instruction into simple, step-by-step actions on each screen. Based on this human cognitive process, we design three levels of annotations in our proposed AMEX training set: (i) GUI interactive element grounding (see Section 3.2), (ii) GUI screen and element descriptions (see Section 3.3), and (iii) instructions with GUI-action chains (see Section 3.4). The overall statistics of AMEX are listed in Table 2.
在接收指令时，人工用户首先分析整体屏幕布局，以对当前 Android 环境形成基本理解。用户随后识别可交互的元素与区域，并评估这些元素的功能。最后，用户将指令分解为逐屏幕的简单步骤。基于这一人类认知过程，我们在提出的 AMEX 训练集中设计了三层注释：（i）GUI 交互元素定位（见第 3.2 节），（ii）GUI 屏幕与元素描述（见第 3.3 节），以及（iii）带有 GUI-动作链的指令（见第 3.4 节）。AMEX 的总体统计如表 2 所示。


### 3.1 Data Collection Pipeline
### 3.1 数据收集管线


An overview of the data collection pipeline is illustrated in Figure 2. The raw data is collected through two methods: human instruction-following GUI manipulations and autonomous GUI controls. Human GUI manipulations involve recording stepwise operations for each instruction, and simultaneously storing screenshots and each screen's Extensible Markup Language (XML) data. In parallel, an autonomous script controls emulators to collect additional screenshots and their XMLs. These two subsets comprise the entire raw dataset. Then for each screenshot, initial bounding boxes of interactive elements and their in-app descriptions (if available) are parsed from the corresponding XML. Human annotators then review each screenshot to filter out all the misaligned boxes, which serve as the interactive element grounding annotations. With the in-app descriptions, GPT and Gemini generates the functionalities of the selected elements and provides descriptions for the whole screenshot. Annotators then further check the quality of the descriptions of functionalities. More details are discussed in the following sections and Appendix A.1.
数据收集管线的概览如图 2 所示。原始数据通过两种方法收集：人工指令跟随 GUI 操作与自动化 GUI 控制。人工 GUI 操作包括记录每条指令的逐步操作，同时存储每个屏幕的截图及其 XML 数据。并行地，自动脚本控制模拟器以收集额外的屏幕截图及其 XML。这两组子集构成整个原始数据集。然后对每个屏幕截图，从相应的 XML 中解析交互元素的初始边界框及其应用内描述（如有）。人工标注者随后检查每个屏幕截图，以滤除所有对齐错误的边界框，作为交互元素定位注释。结合应用内描述，GPT 与 Gemini 生成所选元素的功能并为整张屏幕提供描述。标注者随后进一步检查功能描述的质量。更多细节见下文及附录 A.1。


<img src="https://cdn.noedgeai.com/bo_d5vcgqjef24c73bqi1jg_4.jpg?x=199&y=187&w=1258&h=636&r=0"/>



Figure 3: Demonstrations of element annotations of AITW and AMEX. (a) Element bounding boxes in AITW (left) and AMEX (right). Boxes in AMEX are well aligned but boxes in AITW might be misaligned and mis-annotated. (b) GUI interactive elements in AMEX. Red boxes: clickable elements. Blue boxes: horizontally scrollable elements. Yellow box: vertically scrollable element.
图3：AITW 与 AMEX 的元素标注演示。 (a) AITW（左）与 AMEX（右）的元素边界框。AMEX 的框对齐良好，但 AITW 的框可能错位或标注错误。 (b) AMEX 的 GUI 交互元素。红框：可点击元素。蓝框：水平可滚动元素。黄框：垂直可滚动元素。


### 3.2 Level I: GUI Interactive Element Grounding
### 3.2 级别 I：GUI 交互元素定位


Existing datasets (Deka et al., 2017; Sunkara et al., 2022; Feng et al., 2024; Bunian et al., 2021) typically classify elements on the screen, such as icons, texts and images, based on their types. Instead of adhering to the traditional classification paradigm, we define interactive elements more broadly as any elements that users can interact with, regardless of their specific types (see Figure 3a). Specifically, interactive elements in our dataset are only categorized into two subsets: (i) clickable elements and (ii) scrollable elements. The motivation of our annotation is mainly from two facts: (i) no existing dataset provides information on the "scroll" action, which is crucial especially for agents to know the horizontally scrollable area. (ii) "click" action is the most used action and many "compound" elements cannot be simply categorized. As illustrated in Figure 3b red boxes, four "compound" elements in the middle contain both image and text and the definition of "clickable" is more suitable with the functionality in Level 2 for agents to understand the element.
现有数据集（Deka 等，2017；Sunkara 等，2022；Feng 等，2024；Bunian 等，2021）通常基于元素的类型对屏幕上的元素进行分类，如图标、文本和图片等。我们不沿用传统的分类范式，而将交互元素更广义地定义为用户可以交互的任意元素，无论其具体类型如何（见图 3a）。具体来说，我们的数据集中的交互元素仅分为两类： (i) 可点击元素和 (ii) 可滚动元素。我们标注的动机主要来自两点事实： (i) 现有数据集没有提供“滚动”动作的信息，这对代理尤其是了解水平可滚动区域至关重要； (ii) “点击”动作是最常用的动作，许多“组合”元素不能简单地归类。如图 3b 的红框所示，中间的四个“组合”元素同时包含图像和文本，且“可点击”的定义更适合与 Level 2 的功能性帮助代理理解元素。


Clickable Elements are the most common components in a screen. They typically include click-able icons, images, texts, and compounds that combine several categories. We also include certain "typeable" elements, such as search bars, because most typeable elements require a prior click action to enable typing.
可点击元素是屏幕上最常见的组件。它们通常包括可点击的图标、图片、文本，以及组合了多种类别的元素。我们还包括某些“可输入”的元素，如搜索栏，因为大多数可输入元素需要先点击才能启用输入。


Scrollable Elements typically occupy larger areas on the screen. In most cases, a scrollable element area contains many clickable elements and supports a pair of actions, such as "scroll down" and "scroll up" or "scroll left" and "scroll right." Figure 3b illustrates these two types of scrollable elements.
可滚动元素通常在屏幕上占据更大区域。在大多数情况下，可滚动区域包含许多可点击的元素，并支持一对动作，如“向下滚动”和“向上滚动”或“向左滚动”和“向右滚动”。图 3b 说明了这两种类型的可滚动元素。


### 3.3 Level II: GUI Screen and Element Functionality Descriptions
### 3.3 级别 II：GUI 屏幕与元素的功能描述


Previous works (Deka et al., 2017; Feng et al., 2024; Bunian et al., 2021) on GUI elements often rely on predefined class names, such as "image" and "text", to convey the underlying meaning of each element. Rico Semantics (Sunkara et al., 2022) annotates the icons with slightly more detailed semantics information but the annotations are still classifications on icons. However, this classification-based method has significant limitations. For instance, a "plus" symbol typically indicates "plus" in calculator interfaces, but it means "create a new task" in a ToDo interface. This class-based annotation approach focuses on class labels rather than truly understanding the functionality of each element in the surrounding context, which may lead to a misunderstanding of the page layout.
先前的工作（Deka 等，2017；Feng 等，2024；Bunian 等，2021）在 GUI 元素上常依赖预定义的类名，如“image”和“text”，以传达每个元素的潜在含义。Rico 语义（Sunkara 等，2022）给图标添加了稍微更详细的语义信息，但注释仍然是对图标的分类。然而，这种基于分类的方法存在显著局限性。例如，在计算器界面中，“加号”通常表示“加号”，而在待办事项界面中则表示“创建新任务”。这种基于类别的注释方法关注类别标签，而不是在周边上下文中真正理解每个元素的功能，这可能导致对页面布局的误解。


To ensure the dataset is truly instructive rather than merely icon detection, we focus on describing screen status and element functionalities. Consider the above mentioned example: instead of providing a bounding box for the icon and labeling it as "ICON_PLUS", we present the actual functionality of the element within its context (e.g., "Create a new task"). This strategy offers a clear, instructive, and detailed description of the interactive element, enhancing the dataset's applicability. Besides, the traditional categories of "IMAGE" and "TEXT" provides very little information of the actual element functionality. Our annotation will provide a much more detailed and informative functionality for those elements (e.g., "View the detail page of the 'Jazz Classics' playlist" for the red left bottom "compound" element in Figure 3b). Further elaboration on the collection and processing is provided in Appendix A.1.4.
为确保数据集真正具有指导性而不仅仅是图标检测，我们着重描述屏幕状态和元素功能。取上例：不是提供图标的边界框并标注为“ICON_PLUS”，而是在其上下文中呈现元素的实际功能（例如“创建新任务”）。这一策略为交互元素提供清晰、具指导性且详细的描述，提升数据集的适用性。此外，传统的“IMAGE”和“TEXT”类别对实际元素功能提供的信息非常有限。我们的标注将为这些元素提供更详细且信息性更强的功能描述（例如在图 3b 的左下红色“组合”元素中，描述为“查看‘Jazz Classics’播放列表的详情页”）。关于集合与处理的进一步阐述见附录 A.1.4。


### 3.4 Level III: Instructions with GUI-Action Chains
### 3.4 级别 III：带 GUI 动作链的指令


Table 1 illustrates that most instruction-based datasets provide an average number of operation steps below 7, which implies most of the instructions are simpler than tasks in real-world. For example, users often apply filters or sort by different attributes when searching, which causes more complex instructions and longer action chains. In order to address this situation, we collect a set of complex instructions with GUI-action chains on popular apps from Google play store.
表 1 显示大多数基于指令的数据集给出的操作步骤平均数低于 7，这意味着大多数指令在现实世界中比任务更简单。例如，用户在搜索时常应用筛选或按不同属性排序，这会产生更复杂的指令和更长的动作链。为应对这种情况，我们在 Google Play 商店的热门应用上收集了一组带 GUI 动作链的复杂指令。


We define our action space for stepwise GUI operations similarly to AITW: \{TAP, SCROLL, TYPE, PRESS_BACK, PRESS_HOME, PRESS_ENTER, TASK_COMPLETE, TASK_IMPOSSIBLE\}. TAP actions are characterized by identical touch and lift coordinates, while SCROLL actions involve distinct touch and lift coordinates. TYPE actions are annotated with a type_text attribute specifying the input text. The three PRESS actions correspond to system-level button presses (back, home, enter). TASK_COMPLETE and TASK_IMPOSSIBLE serve as terminal flags for instructions. Additionally, for instructions that involve information query (e.g., "What is the lowest price of the men's belt?"), we associate the TASK_COMPLETE action with a region of interest, which is defined as the bounding box of the area on the current screenshot, where the answer is expected to appear (see examples in Appendix A.3). This comprehensive action space allows users to fully simulate a wide range of typical use cases. Detailed collection process and instruction generation are in Appendix A.1.5.
我们将逐步 GUI 操作的行动空间定义为类似 AITW 的 {TAP, SCROLL, TYPE, PRESS_BACK, PRESS_HOME, PRESS_ENTER, TASK_COMPLETE, TASK_IMPOSSIBLE}。TAP 操作的特征是相同的触摸与抬起坐标，而 SCROLL 操作涉及不同的触摸与抬起坐标。TYPE 操作带有指定输入文本的 type_text 属性。三种 PRESS 操作对应系统级按钮的按下（返回、主页、回车）。TASK_COMPLETE 与 TASK_IMPOSSIBLE 作为指令的终端标志。此外，对于涉及信息查询的指令（例如“男士腰带的最低价格是多少？”），我们将 TASK_COMPLETE 动作为当前屏幕截图区域的兴趣区域进行关联，该区域定义为答案预计出现的区域的边界框（见附录 A.3 的示例）。这一综合的行动空间使用户能够全面模拟广泛的典型用例。详细的采集过程和指令生成见附录 A.1.5。


Table 3: ScreenSpot mobile subset experiment results.
表格 3：ScreenSpot 移动子集实验结果。


<table><tr><td>Model</td><td>Size</td><td>Icon / Widget</td></tr><tr><td>Fuyu</td><td>8B</td><td>1.3%</td></tr><tr><td>CogAgent</td><td>18B</td><td>24.0%</td></tr><tr><td>SeeClick</td><td>9.6B</td><td>52.0%</td></tr><tr><td>Qwen2-VL</td><td>7B</td><td>60.7%</td></tr><tr><td>GPT-4V w. OmniParser</td><td>-</td><td>57%</td></tr><tr><td>SphAgent</td><td>7B</td><td>72.6%</td></tr></table>
<table><tbody><tr><td>模型</td><td>尺寸</td><td>图标 / 小部件</td></tr><tr><td>冬</td><td>8B</td><td>1.3%</td></tr><tr><td>齿轮代理</td><td>18B</td><td>24.0%</td></tr><tr><td>SeeClick</td><td>9.6B</td><td>52.0%</td></tr><tr><td>Qwen2-VL</td><td>7B</td><td>60.7%</td></tr><tr><td>GPT-4V 及 OmniParser</td><td>-</td><td>57%</td></tr><tr><td>SphAgent</td><td>7B</td><td>72.6%</td></tr></tbody></table>


## 4 Experiments
## 4 实验


AMEX serves as a supplementary dataset for other large-scale instruction-based datasets. To test the effectiveness of AMEX, we directly finetune SPHINX models (Liu et al., 2024) without any tricks and conduct evaluation on three other benchmark datasets. We choose ScreenSpot (Cheng et al., 2024) as the benchmark of our agent's element grounding ability and AITW and ANDROIDCON-TROL as the benchmarks of our agent's GUI-control ability.
AMEX 作为对其他大规模基于指令的数据集的补充数据集。为了测试 AMEX 的有效性，我们在不使用任何技巧的情况下直接对 SPHINX 模型（Liu 等, 2024）进行微调，并对另外三个基准数据集进行评估。我们选择 ScreenSpot（Cheng 等, 2024）作为对我们代理的元素定位能力的基准，以及 AITW 和 ANDROIDCON-TROL 作为对我们代理的 GUI 控制能力的基准。


### 4.1 SPHINX Agent
### 4.1 SPHINX Agent


Our SPHINX agent, SphAgent for short, is initialized from SPHINX (Liu et al., 2024) and it is finetuned for the GUI-control tasks and element grounding tasks. For both tasks, we train the model in a pure vision-based principle, which means the model understands the environment by only screen-shots, without any other supplementary information such as accessibility trees or view hierarchy, due to the fact that many apps don't support or only provide very little accessibility tree or view hierarchy element information. We consider vision-based agents are more applicable in more various use cases.
我们的 SPHINX 代理，简称 SphAgent，从 SPHINX（Liu 等, 2024）初始化，专为 GUI 控制任务和元素定位任务进行微调。对于两类任务，我们在纯视觉原则下训练模型，这意味着模型仅通过屏幕截图来理解环境，而不依赖于诸如可访问性树或视图层级等其他补充信息，原因是许多应用程序不支持或仅提供极少的可访问性树或视图层级元素信息。我们认为基于视觉的代理在更多场景中更具适用性。


### 4.2 ScreenSpot
### 4.2 ScreenSpot


ScreenSpot (Cheng et al., 2024) is a benchmark dataset which contains over 600 UI screenshots from mobile devices, desktops and websites. It is designed to evaluate the model's element grounding capability, where each human-annotated functionality corresponds to an interactive element. We train SPHINX agent on AMEX level 1 and level 2 data to evaluate the element grounding capability.
ScreenSpot（Cheng 等, 2024）是一个基准数据集，包含来自移动设备、桌面和网站的超过 600 张 UI 截图。它用于评估模型的元素定位能力，其中每个人工标注的功能对应一个交互元素。我们在 AMEX 等级 1 和等级 2 数据上训练 SPHINX 代理，以评估其元素定位能力。


Table 3 lists the evaluation results on the ScreenSpot mobile subset. The models (Bavishi et al., 2023; Hong et al., 2024; Cheng et al., 2024; Wang et al., 2024) in first four rows are LVLMs specially trained on GUI data. OmniParser (Wan et al., 2024) is a combination of an icon detection model, an icon description model and an OCR module, which serves as a screenshot parser for other LVLMs such as GPT-4V. During evaluation, our SphAgent surpasses four LVLMs by a large margin on the "Icon / Widget" subset without any training tricks, proving that the functionality understanding can largely boost the performance of GUI grounding for agents. The above results and comparisons strongly prove the effectiveness and applicability of AMEX in GUI element grounding tasks.
表 3 列出了 ScreenSpot 移动子集的评估结果。前四行中的模型（Bavishi 等, 2023；Hong 等, 2024；Cheng 等, 2024；Wang 等, 2024）是在 GUI 数据上专门训练的 LVLMs。OmniParser（Wan 等, 2024）是一个由图标检测模型、图标描述模型和 OCR 模块组成的组合体，作为对其他 LVLMs（如 GPT-4V）的截图解析器。在评估过程中，我们的 SphAgent 在 “Icon / Widget” 子集上以较大优势超越四个 LVLM，且无需任何训练技巧，证明了功能理解在很大程度上能够提升 GUI 定位任务中的代理性能。上述结果与对比强烈证明了 AMEX 在 GUI 元素定位任务中的有效性与适用性。


Table 4: Experiment results on ANDROIDCONTROL. "Click & Long Press" is specially computed to show the improvements on these actions. The best results are in bold for different subsets and levels.
Table 4: ANDROIDCONTROL 的实验结果。“Click & Long Press” 是专门计算以展示这些动作上的改进。不同子集和等级的最佳结果以加粗显示。


<table><tr><td>Agent</td><td>#data</td><td>Training Data</td><td>Task Level</td><td>IDD</td><td>Category Unseen</td><td>App Unseen</td><td>Task Unseen</td><td>Overall</td><td>Click & Long Press</td></tr><tr><td rowspan="2">SphAgent</td><td rowspan="2">178K</td><td rowspan="2">ANDROIDCTRL</td><td>High Lv.</td><td>58.3</td><td>40.6</td><td>39.4</td><td>52.2</td><td>49.8</td><td>34.1</td></tr><tr><td>Low Lv.</td><td>79.8</td><td>71.0</td><td>71.1</td><td>83.5</td><td>75.8</td><td>53.0</td></tr><tr><td rowspan="2">SphAgent</td><td rowspan="2">178K</td><td rowspan="2">70% ANDRCTRL <br> + 10% AMEX</td><td>High Lv.</td><td>60.4</td><td>41.7</td><td>43.4</td><td>59.3</td><td>52.8</td><td>37.3</td></tr><tr><td>Low Lv.</td><td>81.9</td><td>73.2</td><td>72.6</td><td>87.5</td><td>77.7</td><td>56.7</td></tr><tr><td rowspan="2">SphAgent</td><td rowspan="2">712K</td><td>ANDROIDCTRL</td><td>High Lv.</td><td>70.5</td><td>51.6</td><td>55.6</td><td>70.2</td><td>61.7</td><td>50.8</td></tr><tr><td>+ AMEX</td><td>Low Lv.</td><td>88.9</td><td>81.8</td><td>76.9</td><td>92.6</td><td>84.2</td><td>67.0</td></tr></table>
<table><tbody><tr><td>代理</td><td>#数据</td><td>训练数据</td><td>任务等级</td><td>IDD</td><td>类别未见</td><td>应用未见</td><td>任务未见</td><td>整体</td><td>点击与长按</td></tr><tr><td rowspan="2">SphAgent</td><td rowspan="2">178K</td><td rowspan="2">ANDROIDCTRL</td><td>高等级</td><td>58.3</td><td>40.6</td><td>39.4</td><td>52.2</td><td>49.8</td><td>34.1</td></tr><tr><td>低等级</td><td>79.8</td><td>71.0</td><td>71.1</td><td>83.5</td><td>75.8</td><td>53.0</td></tr><tr><td rowspan="2">SphAgent</td><td rowspan="2">178K</td><td rowspan="2">70% ANDRCTRL <br/> + 10% AMEX</td><td>高等级</td><td>60.4</td><td>41.7</td><td>43.4</td><td>59.3</td><td>52.8</td><td>37.3</td></tr><tr><td>低等级</td><td>81.9</td><td>73.2</td><td>72.6</td><td>87.5</td><td>77.7</td><td>56.7</td></tr><tr><td rowspan="2">SphAgent</td><td rowspan="2">712K</td><td>ANDROIDCTRL</td><td>高等级</td><td>70.5</td><td>51.6</td><td>55.6</td><td>70.2</td><td>61.7</td><td>50.8</td></tr><tr><td>+ AMEX</td><td>低等级</td><td>88.9</td><td>81.8</td><td>76.9</td><td>92.6</td><td>84.2</td><td>67.0</td></tr></tbody></table>


### 4.3 ANDROIDCONTROL
### 4.3 ANDROIDCONTROL


ANDROIDCONTROL (Li et al., 2024) is a large-scale benchmark dataset to evaluate the agent's GUI-control ability to complete tasks on general apps (see more detailed description of the dataset and splits in Appendix A.2.1). We evaluate three SphAgents trained on (i) only ANDROIDCONTROL, (ii) random 70% ANDROIDCONTROL data mixed with random 10% AMEX level 1 and 2, (iii) a mix of ANDROIDCONTROL and AMEX level 1 and 2. The (ii) experiment ensures the same number of data point as in (i) experiment. The action space of ANDROIDCONTROL is different from AMEX level 3 and AITW, so we only use level 1 and level 2 data during the experiments (ii) and (iii).
ANDROIDCONTROL (Li et al., 2024) 是一个大规模基准数据集，用于评估代理在通用应用中完成任务的 GUI 控制能力（有关数据集及分割的更详细描述见附录 A.2.1）。我们评估在 (i) 仅使用 ANDROIDCONTROL、(ii) 将随机 70% 的 ANDROIDCONTROL 数据混合随机 10% 的 AMEX 1 级和 2 级数据、(iii) ANDROIDCONTROL 与 AMEX 1 级和 2 级数据混合训练的三种 SphAgent。实验 (ii) 确保数据点数量与实验 (i) 相同。ANDROIDCONTROL 的动作空间与 AMEX 3 级和 AITW 不同，因此在实验 (ii) 和 (iii) 仅使用 1 级和 2 级数据。


Table 4 lists the evaluation results on ANDROID-CONTROL test set. The evaluation results of experiment (i) and (iii) strongly support the effectiveness of AMEX level 1 and level 2. Adding full data of AMEX level 1 and level 2 leads to an average 10% overall performance gain and strongly improves the "click" and "long press" actions by more than 14%. Also, the comparative results from experiment (i) and (ii) shows that even using the same number of data points, replacing down-stream instructional task data with environment understanding data would also lift the performance of agents on both low-level and high-level at an average of 2.5%. This performance gain indicates the effectiveness of our multi-level annotations.
表 4 列出在 ANDROID-CONTROL 测试集上的评估结果。实验 (i) 和 (iii) 的评估结果强有力地支持 AMEX 1 级和 2 级的有效性。加入 AMEX 1 级和 2 级全部数据可带来平均 10% 的总体性能提升，并将“点击”和“长按”动作显著提升超过 14%。另外，实验 (i) 与 (ii) 的对比结果显示，即使使用相同数量的数据点，用环境理解数据替换下游指令任务数据，也会使代理在低级和高级任务上的性能平均提升约 2.5%。此性能提升表明了我们的多级标注的有效性。


### 4.4 AirW
### 4.4 AirW


AITW is another large-scale benchmark dataset to evaluate the agent's GUI-control ability to complete task goals. We evaluate three SphAgents trained on (i) only AITW, (ii) a mix of AITW and AMEX. Since the GUI-action chains in AMEX share the same action space with AITW, we utilize all three levels of AMEX in the mixed data.
AITW 也是一个大型基准数据集，用于评估代理在 GUI 控制方面完成任务目标的能力。我们评估在 (i) 仅使用 AITW、(ii) AITW 与 AMEX 的混合数据上训练的三种 SphAgent。由于 AMEX 的 GUI 动作链与 AITW 共享相同的动作空间，我们在混合数据中使用 AMEX 的所有三个等级。


Table 5 lists the evaluation results on AITW test set, which has five subsets for different types of tasks. SphAgent trained on the mixed data (AITW + AMEX) achieves the highest overall accuracy among all agents, and when compared to the SphA-gent trained on only AITW, the accuracies in "General" and "Single" categories have a notable gain of around $5\%$ and the overall accuracy lifts by 2.5%. That the gains in other categories are not remarkable is possibly due to the mis-aligned element-wise annotation and unregulated instruction operation annotation. In Appendix A.2.2 we further explore the dataset evaluation and explain the reason why the evaluation results might be misleading.
表 5 列出在 AITW 测试集上的评估结果，该测试集对不同类型任务分为五个子集。在混合数据 (AITW + AMEX) 上训练的 SphAgent 在所有代理中取得最高的总体准确率，相较仅在 AITW 上训练的 SphAgent，在“General（通用）”与“Single（单一）”类别的准确率有显著提升，约为 $5\%$，总体准确率提升约 2.5%。其他类别的提升不显著，可能是由于逐元素标注错位和指令操作标注不规范所致。在附录 A.2.2 中，我们进一步探讨数据集评估并解释评估结果为何可能具有误导性。


### 4.5 Cross-Domain Experiments
### 4.5 Cross-Domain Experiments


Since AMEX Level 3 shares the same action space only with AITW, we conducted two experiments: (i) training SphAgent on AMEX and testing it on AITW, and (ii) training SphAgent on AITW and testing it on AMEX. The results, as shown in the last row of Table 5, indicate poor performance when the agent is trained only on AMEX and tested on AITW. Similarly, Table 6 demonstrates poor performance when the agent is trained on AITW and tested on AMEX. This significant discrepancy in evaluation results is likely due to the domain gap between the two datasets. First, the styles of collecting GUI-action chains differ between the two datasets. For instance, annotators in AITW tend to directly search for apps, whereas annotators in AMEX prefer navigating through the app library to locate apps. This inconsistency in interaction strategies poses a challenge for the agent, which evaluates tasks based on static frames rather than dynamically adapting to different usage patterns. Second, although both datasets share the same action space, they differ in terms of visual appearance and versioning. The apps in AMEX are updated to their latest versions, while those in AITW are based on older versions. These versioning differences lead to changes in the visual appearance of GUI elements (e.g., color schemes, icons, or button designs), which can negatively impact the agent's ability to recognize and interact with them.
由于 AMEX 3 级仅与 AITW 共享相同的动作空间，我们进行两组实验：(i) 在 AMEX 上训练 SphAgent 并在 AITW 上测试，以及 (ii) 在 AITW 上训练 SphAgent 并在 AMEX 上测试。表 5 最后一行所示的结果表明，当代理仅在 AMEX 上训练而在 AITW 上测试时，性能较差。表 6 也显示在 AITW 上训练而在 AMEX 上测试时性能较差。这一评估结果的显著差异，很可能是由于两个数据集之间的域差。首先，收集 GUI 动作链的风格在两个数据集之间存在差异。例如，AITW 的标注者倾向于直接搜索应用，而 AMEX 的标注者更倾向于通过应用库导航来定位应用。这种交互策略的不一致给代理带来挑战，代理需要基于静态帧评估任务，而不是动态适应不同的使用模式。其次，尽管两个数据集共享相同的动作空间，但在视觉呈现和版本控制方面存在差异。AMEX 的应用更新到最新版本，而 AITW 的则基于较旧的版本。这些版本差异会导致 GUI 元素的视觉外观变化（如配色、图标或按钮设计），从而对代理识别和互动能力产生负面影响。


Table 5: Experiment results on AITW.
表 5：AITW 的实验结果。


<table><tr><td>Agent</td><td>Training Data</td><td>General</td><td>Install</td><td>G-Apps</td><td>Single</td><td>WebShopping</td><td>Overall</td></tr><tr><td>SphAgent</td><td>AITW</td><td>68.2</td><td>80.5</td><td>73.3</td><td>85.4</td><td>74</td><td>76.28</td></tr><tr><td>SphAgent</td><td>AITW + AMEX</td><td>73.1</td><td>80.6</td><td>73.4</td><td>90.8</td><td>75.8</td><td>78.72</td></tr><tr><td>SphAgent</td><td>AMEX</td><td>51.5</td><td>55.6</td><td>57.1</td><td>61.9</td><td>55.1</td><td>56.2</td></tr></table>
<table><tbody><tr><td>代理</td><td>训练数据</td><td>通用</td><td>安装</td><td>G-应用</td><td>单一</td><td>网页购物</td><td>总体</td></tr><tr><td>SphAgent</td><td>AITW</td><td>68.2</td><td>80.5</td><td>73.3</td><td>85.4</td><td>74</td><td>76.28</td></tr><tr><td>SphAgent</td><td>AITW + AMEX</td><td>73.1</td><td>80.6</td><td>73.4</td><td>90.8</td><td>75.8</td><td>78.72</td></tr><tr><td>SphAgent</td><td>AMEX</td><td>51.5</td><td>55.6</td><td>57.1</td><td>61.9</td><td>55.1</td><td>56.2</td></tr></tbody></table>


Table 6: Experiment results on AMEX. The test apps are excluded during the training of SphAgent on AMEX.
表6：AMEX上的实验结果。测试应用在SphAgent在AMEX上的训练过程中被排除。


<table><tr><td>Agent</td><td>Training Data</td><td>Gmail</td><td>Booking</td><td>YT-Music</td><td>SHEIN</td><td>NBC</td><td>CM</td><td>ToDo</td><td>Signal</td><td>Yelp</td><td>Overall</td></tr><tr><td>SphAgent</td><td>AITW</td><td>32.1</td><td>45.9</td><td>46.1</td><td>35.1</td><td>48.3</td><td>61.1</td><td>55.9</td><td>43.3</td><td>42.9</td><td>45.6</td></tr><tr><td>SphAgent</td><td>AMEX</td><td>63.7</td><td>67.4</td><td>76.1</td><td>70.0</td><td>68.5</td><td>62.7</td><td>78.6</td><td>68.2</td><td>68.9</td><td>69.3</td></tr></table>
<table><tbody><tr><td>代理</td><td>训练数据</td><td>Gmail</td><td>预订</td><td>YT-Music</td><td>SHEIN</td><td>NBC</td><td>CM</td><td>待办</td><td>Signal</td><td>Yelp</td><td>总体</td></tr><tr><td>SphAgent</td><td>AITW</td><td>32.1</td><td>45.9</td><td>46.1</td><td>35.1</td><td>48.3</td><td>61.1</td><td>55.9</td><td>43.3</td><td>42.9</td><td>45.6</td></tr><tr><td>SphAgent</td><td>AMEX</td><td>63.7</td><td>67.4</td><td>76.1</td><td>70.0</td><td>68.5</td><td>62.7</td><td>78.6</td><td>68.2</td><td>68.9</td><td>69.3</td></tr></tbody></table>


## 5 Discussions
## 5 讨论


### 5.1 Limitations and Future Work
### 5.1 局限性与未来工作


Multi-lingual Most existing datasets are limited to English, with UGIF (Gubbi Venkatesh et al., 2024) being a notable exception, as it includes instructions and screenshots in eight languages. The AMEX dataset contains a small number of screen-shots in Chinese and Spanish, primarily due to strict registration and login requirements for Chinese apps and a lack of expertise in other languages. Future work should incorporate multilingual screenshots, functionalities, and instructions to create a more robust and comprehensive multi-lingual environment for GUI agents.
多语言 多数现有数据集限于英语，UGIF（Gubbi Venkatesh 等，2024）是一个显著例外，它包含八种语言的指令与屏幕截图。AMEX 数据集中包含少量中文和西班牙语的截图，主要是因为对中文应用的严格注册与登录要求以及对其他语言的专业知识不足。未来工作应纳入多语言屏幕截图、功能与指令，以创建一个更健壮、全面的多语言 GUI 代理环境。


Online Evaluation Due to the limitation that SPHINX is not deployable, we encountered issues trying online evaluation such as Android-World (Rawles et al., 2024a). In future work, online dynamic evaluation is more accurate and simulate the real-world agent usage.
在线评测 由于 SPHINX 无法部署，我们在尝试在线评测（如 Android-World，Rawles 等，2024a）时遇到问题。未来工作中 Online 动态评测将更为准确并能更真实地模拟代理的使用场景。


Multi-Platform Android is an open-source OS and the community provides well-developed tools for interactions and emulations. iOS is another widely used mobile device OS but it is restricted and hard to run test on. In the future, a dataset extended to other OS or platforms would provide more general information.
多平台 Android 是一个开源操作系统，社区提供了成熟的交互与仿真工具。iOS 作为另一种广泛使用的移动设备操作系统，但它受限且测试困难。未来扩展到其他操作系统或平台的数据集将提供更广泛的信息。


### 5.2 Ethical Considerations
### 5.2 伦理考量


- The accounts registered and logged in are all for testing purposes, not including any personal information. The dataset doesn't contain any private or personal information.
- 注册并登录的账户均用于测试目的，不包含任何个人信息。数据集中不含私密或个人信息。


- The dataset, if misused, could be exploited for undesirable purposes, such as anti-fraud mechanisms and anti-script verification codes (see Appendix A.4), potentially leading to harm.
- 数据集若被滥用，可能被用于不良目的，如反欺诈机制与验证码（见附录 A.4），可能造成伤害。


- Annotators received remuneration in line with local wage standards for their annotation works.
- 为注释工作，标注人员的报酬符合当地工资标准。


## 6 Conclusion
## 6 结论


As AI agents become more prevalent, mobile GUI agents are emerging as a research hotspot. To address the lack of fundamental understanding of GUI elements in existing datasets, we present the Android Multi-annotation EXpo (AMEX) dataset, which includes three levels of annotation to provide a more instructive and detailed understanding of UI screens and elements. Additionally, we introduce a state-of-the-art SPHINX Agent, which can serve as a baseline model for future research.
随着 AI 代理的日益普及，移动 GUI 代理正成为研究热点。为解决现有数据集中对 GUI 元素缺乏基本理解的问题，我们提出 Android 多注释体验（AMEX）数据集，包含三个层次的注释，以提供对 UI 屏幕与元素更具指导性和细致的理解。此外，我们引入了最先进的 SPHINX 代理，可作为未来研究的基线模型。


## References
## 参考文献


Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Taşırlar. 2023. Introducing our multimodal models.
Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, 与 Sagnak Taşırlar. 2023. Introducing our multimodal models.


Sara Bunian, Kai Li, Chaima Jemmali, Casper Harteveld, Yun Fu, and Magy Seif Seif El-Nasr. 2021. Vins: Visual search for mobile user interface design. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-14.
Sara Bunian, Kai Li, Chaima Jemmali, Casper Harteveld, Yun Fu 与 Magy Seif Seif El-Nasr. 2021. Vins: Visual search for mobile user interface design. 收录于 2021 CHI Conference on Human Factors in Computing Systems 论文集，页面 1-14。


Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan A Plummer. 2021. Mobile app tasks with iterative feedback (motif): Addressing task feasibility in interactive visual environments. arXiv preprint arXiv:2104.08560.
Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko 与 Bryan A Plummer. 2021. Mobile app tasks with iterative feedback (motif): Addressing task feasibility in interactive visual environments. arXiv 预印本 arXiv:2104.08560。


Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. 2024. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9313- 9332, Bangkok, Thailand. Association for Computational Linguistics.
Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang 与 Zhiyong Wu. 2024. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. 收录于 第 62 届计算语言学协会年会（卷 1：长论文），页码 9313-9332，泰国曼谷。计算语言学协会。


Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hi-bschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. 2017. Rico: A mobile app dataset for building data-driven design applications. In Proceedings of the 30th Annual Symposium on User Interface Software and Technology, UIST '17.
Biplab Deka、Zifeng Huang、Chad Franzen、Joshua Hi-bschman、Daniel Afergan、Yang Li、Jeffrey Nichols、Ranjitha Kumar。2017。Rico：用于构建数据驱动设计应用的移动应用数据集。In Proceedings of the 30th Annual Symposium on User Interface Software and Technology, UIST '17.


Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. 2024. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 36.
Xiang Deng、Yu Gu、Boyuan Zheng、Shijie Chen、Sam Stevens、Boshi Wang、Huan Sun、Yu Su。2024。Mind2web：迈向通用网页智能体。Advances in Neural Information Processing Systems，36。


Sidong Feng, Suyu Ma, Han Wang, David Kong, and Chunyang Chen. 2024. Mud: Towards a large-scale and noise-filtered ui dataset for modern style ui modeling. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI '24, New York, NY, USA. Association for Computing Machinery.
Sidong Feng、Suyu Ma、Han Wang、David Kong、Chunyang Chen。2024。Mud：面向现代风格 UI 建模的大规模、去噪 UI 数据集。In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems，CHI '24，New York, NY, USA。Association for Computing Machinery。


Albert Gu and Tri Dao. 2024. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling.
Albert Gu、Tri Dao。2024。Mamba：具有选择性状态空间的线性时间序列建模。In First Conference on Language Modeling。


Sagar Gubbi Venkatesh, Partha Talukdar, and Srini Narayanan. 2024. UGIF-DataSet: A new dataset for cross-lingual, cross-modal sequential actions on the UI. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 1390-1399, Mexico City, Mexico. Association for Computational Linguistics.
Sagar Gubbi Venkatesh、Partha Talukdar、Srini Narayanan。2024。UGIF-DataSet：一个用于跨语言、跨模态的 UI 顺序动作数据集。In Findings of the Association for Computational Linguistics: NAACL 2024，页面 1390-1399，墨西哥城，墨西哥。Association for Computational Linguistics。


Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Saf-dari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. 2023. Understanding HTML with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2803-2821, Singapore. Association for Computational Linguistics.
Izzeddin Gur、Ofir Nachum、Yingjie Miao、Mustafa Saf-dari、Austin Huang、Aakanksha Chowdhery、Sharan Narang、Noah Fiedel、Aleksandra Faust。2023。使用大型语言模型理解 HTML。In Findings of the Association for Computational Linguistics: EMNLP 2023，页面 2803-2821，新加坡。Association for Computational Linguistics。


Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. 2024. Cogagent: A visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14281-14290.
Wenyi Hong、Weihan Wang、Qingsong Lv、Jiazheng Xu、Wenmeng Yu、Junhui Ji、Yan Wang、Zihan Wang、Yuxiao Dong、Ming Ding 等。2024。Cogagent：面向 GUI 代理的视觉语言模型。In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition，页面 14281-14290。


Wei Li, William E Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. 2024. On the effects of data scale on ui control agents. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track.
Wei Li、William E Bishop、Alice Li、Christopher Rawles、Folawiyo Campbell-Ajala、Divya Tyamagundlu、Oriana Riva。2024。数据规模对 UI 控制代理的影响。In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track。


Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. 2020. Mapping natural language instructions to mobile UI action sequences. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8198-8210, Online. Association for Computational Linguistics.
Yang Li、Jiacong He、Xin Zhou、Yuan Zhang、Jason Baldridge。2020。将自然语言指令映射为移动 UI 动作序列。In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics，页面 8198-8210，在线。Association for Computational Linguistics。


Dongyang Liu, Renrui Zhang, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Yu Qiao, Hongsheng Li, and Peng Gao. 2024. SPHINX-x: Scaling data and parameters for a family of multimodal large language models. In Forty-first International Conference on Machine Learning.
Dongyang Liu、Renrui Zhang、Longtian Qiu、Siyuan Huang、Weifeng Lin、Shitian Zhao、Shijie Geng、Ziyi Lin、Peng Jin、Kaipeng Zhang、Wenqi Shao、Chao Xu、Conghui He、Junjun He、Hao Shao、Pan Lu、Yu Qiao、Hongsheng Li、Peng Gao。2024。SPHINX-x：为一系列多模态大语言模型扩展数据与参数。In Forty-first International Conference on Machine Learning。


Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tian-lin Shi, and Percy Liang. 2018. Reinforcement learning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802.
Evan Zheran Liu、Kelvin Guu、Panupong Pasupat、Tian-lin Shi、Percy Liang。2018。基于工作流引导探索的网页界面强化学习。arXiv 预印本 arXiv:1802.08802。


Xinbei Ma, Zhuosheng Zhang, and Hai Zhao. 2024. Coco-agent: A comprehensive cognitive mllm agent for smartphone gui automation. In Findings of the Association for Computational Linguistics ACL 2024, pages 9097-9110.
Xinbei Ma、Zhuosheng Zhang、Hai Zhao。2024。Coco-agent：用于智能手机 GUI 自动化的全方位认知大模型代理。In Findings of the Association for Computational Linguistics ACL 2024，页面 9097-9110。


Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. 2024. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research.
Maxime Oquab、Timothée Darcet、Théo Moutakanni、Huy V. Vo、Marc Szafraniec、Vasil Khalidov、Pierre Fernandez、Daniel HAZIZA、Francisco Massa、Alaaeldin El-Nouby、Mido Assran、Nicolas Ballas、Wojciech Galuba、Russell Howes、Po-Yao Huang、Shang-Wen Li、Ishan Misra、Michael Rabbat、Vasu Sharma、Gabriel Synnaeve、Hu Xu、Hervé Jegou、Julien Mairal、Patrick Labatut、Armand Joulin、以及 Piotr Bojanowski。2024。DINOv2：在没有监督的情况下学习鲁棒的视觉特征。Transactions on Machine Learning Research。


Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tya-magundlu, Timothy Lillicrap, and Oriana Riva. 2024a. Androidworld: A dynamic benchmarking environment for autonomous agents. Preprint, arXiv:2405.14573.
Christopher Rawles、Sarah Clinckemaillie、Yifan Chang、Jonathan Waltz、Gabrielle Lau、Marybeth Fair、Alice Li、William Bishop、Wei Li、Folawiyo Campbell-Ajala、Daniel Toyama、Robert Berry、Divya Tya-magundlu、Timothy Lillicrap，以及 Oriana Riva。2024a。Androidworld：一个用于自主代理的动态基准环境。Preprint，arXiv:2405.14573。


Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2024b. An-droidinthewild: A large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36.
Christopher Rawles、Alice Li、Daniel Rodriguez、Oriana Riva、以及 Timothy Lillicrap。2024b。An-droidinthewild：一个用于安卓设备控制的大规模数据集。神经信息处理系统进展，36。


Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. 2017. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pages 3135-3144. PMLR.
Tianlin Shi、Andrej Karpathy、Linxi Fan、Jonathan Hernandez、以及 Percy Liang。2017。Bits 世界：一个开放域的基于网络的代理平台。在国际机器学习会议论文集，页码 3135-3144。PMLR。


Srinivas Sunkara, Maria Wang, Lijuan Liu, Gilles Baechler, Yu-Chung Hsiao, Jindong Chen, Abhan-shu Sharma, and James W. W. Stout. 2022. Towards better semantic understanding of mobile interfaces. In Proceedings of the 29th International Conference on Computational Linguistics, pages 5636- 5650, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.
Srinivas Sunkara、Maria Wang、Lijuan Liu、Gilles Baechler、Yu-Chung Hsiao、Jindong Chen、Abhan-shu Sharma、以及 James W. W. Stout。2022。走向对移动界面的更好语义理解。在“第29届国际计算语言学会议论文集”中，页码 5636-5650，庆州，韩国。国际计算语言学委员会。


InternLM Team. 2023. Internlm: A multilingual language model with progressively enhanced capabilities.
InternLM Team。2023。Internlm：具备逐步增强能力的多语言语言模型。


Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.
Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar 等。2023。Llama：开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971。


Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, and Zhibo Yang. 2024. Omniparser: A unified framework for text spotting key information extraction and table recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15641-15653.
Jianqiang Wan、Sibo Song、Wenwen Yu、Yuliang Liu、Wenqing Cheng、Fei Huang、Xiang Bai、Cong Yao、Zhibo Yang。2024。Omniparser：一个用于文本识别、关键信息抽取和表格识别的统一框架。在IEEE/CVF计算机视觉与模式识别大会论文集中，页码 15641-15653。


Bryan Wang, Gang Li, and Yang Li. 2023. Enabling conversational interaction with mobile ui using large language models. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1-17.
Bryan Wang、Gang Li、Yang Li。2023。通过大型语言模型实现移动 UI 的对话交互。在2023 CHI 计算系统人机交互大会论文集中，页码 1-17。


Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi-hao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. arXiv preprint arXiv:2409.12191.
Peng Wang、Shuai Bai、Sinan Tan、Shijie Wang、Zhi-hao Fan、Jinze Bai、Keqin Chen、Xuejing Liu、Jialin Wang、Wenbin Ge 等。2024。Qwen2-vl：在任意分辨率下提升视觉-语言模型对世界的感知。arXiv 预印本 arXiv:2409.12191。


Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xin-lei Chen, Zhuang Liu, In So Kweon, and Saining Xie. 2023. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16133-16142.
Sanghyun Woo、Shoubhik Debnath、Ronghang Hu、Xin-lei Chen、Zhuang Liu、In So Kweon、以及 Saining Xie。2023。Convnext v2：与掩码自编码器共同设计与缩放卷积网络。在IEEE/CVF视觉识别协会大会论文集，页码 16133-16142。


Jason Wu, Siyan Wang, Siman Shen, Yi-Hao Peng, Jeffrey Nichols, and Jeffrey P Bigham. 2023. Webui: A dataset for enhancing visual ui understanding with web semantics. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1-14.
Jason Wu、Siyan Wang、Siman Shen、Yi-Hao Peng、Jeffrey Nichols、以及 Jeffrey P Bigham。2023。Webui：一个用于通过网络语义增强视觉用户界面理解的数据集。在2023 CHI 计算系统人机交互大会论文集中，页码 1-14。


Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chun-yuan Li, and Jianfeng Gao. 2023. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441.
Jianwei Yang、Hao Zhang、Feng Li、Xueyan Zou、Chun-yuan Li、以及 Jianfeng Gao。2023。 Set-of-mark 提示法在 GPT-4v 中释放非凡的视觉对齐能力。arXiv 预印本 arXiv:2310.11441。


Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. CoRR, abs/2312.13771.
Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. CoRR, abs/2312.13771.


Jiwen Zhang, Jihao Wu, Teng Yihua, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. 2024. Android in the zoo: Chain-of-action-thought for GUI agents. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 12016-12031, Miami, Florida, USA. Association for Computational Linguistics.
Jiwen Zhang, Jihao Wu, Teng Yihua, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. 2024. Android in the zoo: Chain-of-action-thought for GUI agents. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 12016-12031, Miami, Florida, USA. Association for Computational Linguistics.


Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024. GPT-4V(ision) is a generalist web agent, if grounded. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 61349-61385. PMLR.
Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024. GPT-4V(ision) is a generalist web agent, if grounded. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 61349-61385. PMLR.


## A Appendix / supplemental material
## A Appendix / supplemental material


### A.1 Pipeline Details
### A.1 Pipeline Details


The raw data is collected using Android emulators, specifically Android Virtual Device and Geny-motion ${}^{1}$ . We developed tools to autonomously perform emulator operations and record human actions. These tools leverage Appium ${}^{2}$ ,an open-source, cross-platform test automation tool for Android, iOS, and web apps, under Apache license. Appium collects device screenshots during operations, each of which also corresponds to an XML file recording the basic screen layout with element attributes, such as bounding boxes and in-app descriptions.
原始数据通过 Android 模拟器收集，具体为 Android Virtual Device 和 Geny-motion ${}^{1}$ 。我们开发了工具来自主执行模拟器操作并记录人为动作。这些工具利用 Appium ${}^{2}$，一个开源的、跨平台的 Android、iOS 和网页应用测试自动化工具，基于 Apache 许可。在操作期间，Appium 会收集设备截图，每个截图还对应一个记录界面基本布局及元素属性（如边界框和应用内描述）的 XML 文件。


#### A.1.1 Collection details
#### A.1.1 收集细节


As mentioned in Section 3.1, the collection of screenshots and their XML data utilizes two methods. Here, we provide a more detailed pipeline for executing an autonomous script to perform operations on an Android emulator. The script is designed to traverse an app in an unconstrained manner and collect data. It performs actions (see Appendix A.1.2) at regular time intervals to allow each page to fully load. Then the script captures the current screenshot along with the corresponding XML file. Upon the raw data (yellow part in Figure 2), we extract bounding boxes for interactive elements from the XML associated with each screenshot. Then annotators manually inspect each screenshot, identifying and selecting only the elements that are actually visible within the interface (see Appendix A.1.3 for detailed reasons).
如第 3.1 节所述，截图及其 XML 数据的收集使用两种方法。这里，我们提供一个更详细的流水线，用于在 Android 模拟器上执行自主脚本以进行操作。该脚本设计为以无约束的方式遍历应用并收集数据。它在固定时间间隔执行操作（见附录 A.1.2），以便每一页都能完全加载。随后脚本捕获当前截图及其对应的 XML 文件。对于原始数据（图 2 的黄色部分），我们从每张截图关联的 XML 中提取交互元素的边界框。然后标注者手动检查每张截图，识别并仅选择在界面中实际可见的元素（详见附录 A.1.3 的具体原因）。


---



${}^{1}$ https://www.genymotion.com/
${}^{1}$ https://www.genymotion.com/


${}^{2}$ https://appium.io/docs/en/latest/
${}^{2}$ https://appium.io/docs/en/latest/


---



#### A.1.2 Autonomous script details
#### A.1.2 Autonomous script details


The autonomous script controls the emulator using three actions: TAP, SCROLL, TYPE.
自主脚本使用三种动作控制模拟器：TAP、SCROLL、TYPE。


- For the TAP action, we employ two algorithms. The first randomly selects a clickable element on the current screen, while the second computes the index of a clickable element using a formula to ensure the elements chosen are likely unique. We apply one of these algorithms randomly for different executions.
- 对于 TAP 动作，我们使用两种算法。第一种随机选择当前屏幕上可点击的元素，第二种通过一个公式计算可点击元素的索引，以确保所选元素具有较高的唯一性。不同执行中随机应用其中一种算法。


- For the SCROLL action, we classify whether an area is vertically or horizontally scrollable by setting a width-height ratio threshold, $\mathcal{R}$ . If the element’s ratio exceeds $\mathcal{R}$ ,it is considered horizontally scrollable; otherwise, it is vertically scrollable. We then randomly select a scrollable element on the current screen and perform a scroll action based on its type.
- 对于 SCROLL 动作，我们通过设定宽高比阈值 $\mathcal{R}$，来分类区域是纵向可滚动还是横向可滚动。如果元素的比值超过 $\mathcal{R}$，则视为横向可滚动；否则视为纵向可滚动。然后在当前屏幕上随机选择一个可滚动元素，并根据其类型执行滚动动作。


- For the TYPE action, we pre-define a list of phrases relevant to the category of apps being tested. For example, ["Women's dress", "Nike sneakers", ...] for clothing shopping apps. These phrases are primarily used in search scenarios.
- 对于 TYPE 动作，我们预定义与被测试应用类别相关的一组短语。例如，服装购物应用中的短语为 [\"Women's dress\", \"Nike sneakers\", ...]。这些短语主要用于搜索场景。


#### A.1.3 GUI clickable element filtering
#### A.1.3 GUI 可点击元素过滤


The raw XML information sometimes contains the elements that are covered by other elements or layers. Figure 4 illustrates the same image before and after filtering. Blue boxes in Figure 4a are those elements under the current active layer and they still show up from XML parsing. Thus we need human annotators to filter out these blocked elements to get Figure 4b.
原始 XML 信息有时包含被其他元素或图层覆盖的元素。图 4 展示了同一图像在过滤前后的效果。图 4a 中蓝色框选的是当前活动图层下的元素，它们在 XML 解析时仍然会出现。因此需要人工标注者对这些被阻挡的元素进行过滤，以获得图 4b。


#### A.1.4 GUI screen and element functionality description collection details
#### A.1.4 GUI 界面与元素功能描述收集细节


We adopt GPT-40 and Gemini 1.5 Pro to describe screens and interpret element functionalities separately. Given the compact and dense nature of GUI elements, we apply Set of Mark (SoM) techniques (Yang et al., 2023) to boost GPT and Gemini's capability for visual localization. The coordinates of these elements are obtained and cleaned in the preceding phases (Section 3.2). Additionally, recognizing that some elements possess abstract functionalities that are challenging to discern, we supplement the prompts for GPT and Gemini with in-app descriptions extracted from XMLs to enhance their comprehension of the screen. After the generation, we further utilize GPT-40 to compare the functionalities of the same element generated by two models and keep them if two functionalities have the same meaning. Finally, human annotators verify the quality and the functionality accuracy rate is above 97%.
我们分别采用 GPT-40 与 Gemini 1.5 Pro 来描述屏幕并解释元素功能。鉴于 GUI 元素的紧凑与密集性，我们应用集合标记（SoM）技术（Yang 等，2023）以提升 GPT 与 Gemini 的视觉定位能力。这些元素的坐标在前置阶段（第 3.2 节）获取并清洗。此外，考虑到某些元素具有较为抽象、难以分辨的功能，我们在 GPT 与 Gemini 的提示中补充从 XML 中提取的应用内描述，以增强对屏幕的理解。生成后，我们再利用 GPT-40 比较同一元素在两种模型中产生的功能是否同义，如同义则保留。最终，人工标注者对质量进行核验，功能准确率高于 97%。


We apply the Set-of-Mark (SoM) technique when using GPT and Gemini as the description generator. The SoM technique is a visual prompting method designed to enhance the visual grounding capabilities of large multimodal models (LMMs), such as GPT-4V, by overlaying visual marks on image regions. This involves partitioning an image into semantically meaningful regions and adding distinct marks (e.g., alphanumeric characters, masks, or boxes) to these regions. It demonstrates significant improvements in precision and accuracy over traditional prompting methods and other state-of-the-art models. Figure 5 shows the screenshot with SoM technique.
在使用 GPT 与 Gemini 作为描述生成器时，我们应用集合标记（SoM）技术。SoM 技术是一种可视提示方法，旨在通过在图像区域上叠加可视标记来提升大型多模态模型（LMM，如 GPT-4V）的视觉定位能力。这包括将图像划分为语义上有意义的区域，并为这些区域添加不同的标记（如字母数字字符、遮罩或框）。与传统提示方法及其他前沿模型相比，它在精准度和正确性方面表现出显著提升。图 5 显示了应用 SoM 技术的屏幕截图。


The most recurring mistake is the misinterpretation of the content description. For instance, some "compound" elements, which may contain several text, rating stars, image, etc., have couple of content descriptions. Those mixed content descriptions may cause the misunderstanding of the real functionality of the compound element. In IMDB app, a row of movie entry sometimes would consist of a poster, a text name, ratings of the movie, and the price of renting. The desired functionality of the entry could be "view the detail page of the movie xxx", while GPT might generate "view the price for renting the movie xxx", which is not exactly what we want. However, we use two LLMs to double check their generations on the same element functionality and we only keep the elements with the same functionalities generated by two LLMs.
最常见的错误是对内容描述的误解。例如，一些“复合”元素可能包含文本、评分星、图片等多种内容，其内容描述可能不一致。这些混合的内容描述可能导致对复合元素实际功能的误解。在 IMDB 应用中，一行电影条目有时由海报、文本名称、电影评分及租赁价格等组成。该条目的期望功能可能是“查看电影 xxx 的详情页”，而 GPT 可能生成“查看电影 xxx 的租赁价格”，这并非我们真正需要的功能。然而，我们使用两种大型语言模型对相同元素的功能进行双重检验，只有两者生成的功能完全一致时才保留该元素。


The prompt is in the following Listing 1 format.
提示格式如下 Listing 1。


#### A.1.5 GUI-Action chain collection details
#### A.1.5 GUI-Action 链收集细节


The instruction generation process comprises three phases. Initially, human annotators create 5-10 complex instructions for each target app, considering its specific functions and capabilities. These initial instructions, combined with relevant app meta-data collected online, serve as input for GPT. It then generates a larger set of 80-100 instructions that exhibit similar structure and intent to the human-provided examples. However, due to GPT's limitations on understanding real-world constraints, human filtering is adopted to modify or remove any unreasonable or impractical instructions. For example, the human-proposed instruction "Open Booking.com, search for stays in Washington DC from 2024 June 11 to June 15." would generate a similar but impractical "Open Booking.com, search for stays in New York from 2024 May 3 to May 7", which needs modification since the dates are not selectable when the task is generated.
指令生成过程分为三个阶段。初步阶段，人工标注者为每个目标应用创建 5-10 条复杂指令，考虑其具体功能与能力。这些初始指令，结合网上收集的相关应用元数据，作为 GPT 的输入。随后，GPT 会生成一个更大集合的 80-100 条指令，其结构和意图与人工提供的示例类似。然而，由于 GPT 在理解现实世界约束方面的局限，需要人工筛选来修改或删除任何不合理或不可执行的指令。例如，人工提出的指令“打开 Booking.com，在 2024 年 6 月 11 日至 6 月 15 日在 Washington DC 查找住宿”会生成一个类似但不现实的“打开 Booking.com，在 2024 年 5 月 3 日至 5 月 7 日在纽约查找住宿”，因为时间的选择在任务生成时不可选，需要修改。


<img src="https://cdn.noedgeai.com/bo_d5vcgqjef24c73bqi1jg_11.jpg?x=439&y=290&w=769&h=715&r=0"/>



Figure 4: Demonstration of human annotator filtering. (a) before filtering. (b) after filtering.
图 4：人工标注者过滤示例。（a）过滤前。（b）过滤后。


<img src="https://cdn.noedgeai.com/bo_d5vcgqjef24c73bqi1jg_11.jpg?x=189&y=1277&w=1274&h=706&r=0"/>



Figure 5: Demonstrations of SoM technique on screenshots.
图 5：SoM 技术在截图上的示例。


---



Based on the screenshot of an Android mobile phone from the
基于来自 Android 手机的屏幕截图


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;APP_NAME, please follow the instructions:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;APP_NAME，请按以下指示执行：


&nbsp;&nbsp;&nbsp;&nbsp;1. Understand the Page Content:
&nbsp;&nbsp;&nbsp;&nbsp;1. 理解页面内容：


&nbsp;&nbsp;&nbsp;&nbsp;- Analyze the overall content of the page.
&nbsp;&nbsp;&nbsp;&nbsp;- 分析页面的整体内容。


&nbsp;&nbsp;&nbsp;&nbsp;- Provide a brief summary of the page content in 1-2 sentences.
&nbsp;&nbsp;&nbsp;&nbsp;- 用 1-2 句话简要概述页面内容。


&nbsp;&nbsp;&nbsp;&nbsp;2. Explain Highlighted Areas:
&nbsp;&nbsp;&nbsp;&nbsp;2. 解释高亮区域：


&nbsp;&nbsp;&nbsp;&nbsp;- Each highlighted area is either clickable or scrollable.
&nbsp;&nbsp;&nbsp;&nbsp;- 高亮区域要么可以点击，要么可以滚动。


&nbsp;&nbsp;&nbsp;&nbsp;- Treat each highlighted area as a unique and separate entity,
&nbsp;&nbsp;&nbsp;&nbsp;- 将每个高亮区域视为独立的实体，


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;using identifiers such as <Region 1> etc.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用诸如 <Region 1> 等标识符。


- If the highlighted area is a general icon, provide its type
- 如果高亮区域是一般图标，请先提供其类型


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first in the format ICON_Magnifying_Glass.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;，格式为 ICON_Magnifying_Glass。


- If the highlighted area is more complex, provide a brief
- 如果高亮区域更为复杂，请提供简要


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;description in the format Element('a poster of
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;描述，格式 Element('a poster of


&nbsp;&nbsp;&nbsp;&nbsp;the movie named <La La Land>').
&nbsp;&nbsp;&nbsp;&nbsp;电影名为 <La La Land>').


&nbsp;&nbsp;&nbsp;&nbsp;- Explain the purpose or functionality of each highlighted
&nbsp;&nbsp;&nbsp;&nbsp;- 解释每个高亮区域的目的或功能


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;area. In other words, what result will happen
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;区域的作用。换言之，点击或滚动标记的


or what's the user's intention when the marked <@area is
或用户在点击时的意图是什么？


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clicked or scrolled?
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在点击或滚动时？


&nbsp;&nbsp;&nbsp;&nbsp;- Some functionality may require an overall analysis, and try
&nbsp;&nbsp;&nbsp;&nbsp;- 某些功能可能需要整体分析，请尽量


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to give the functionality specifically and
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;给出具体的功能描述，并且


&nbsp;&nbsp;&nbsp;&nbsp;related to the current screenshot.
&nbsp;&nbsp;&nbsp;&nbsp;与当前截图相关的功能。


&nbsp;&nbsp;&nbsp;&nbsp;3. Additional Information for Highlighted Areas:
&nbsp;&nbsp;&nbsp;&nbsp;3. 高亮区域的附加信息：


&nbsp;&nbsp;&nbsp;&nbsp;There are total NUMBER elements to annotate.
&nbsp;&nbsp;&nbsp;&nbsp;总共有 NUMBER 个需要标注的元素。


MarkerInformation
MarkerInformation


4. Output Format:
4. 输出格式：


The output should be in JSON format as follows:
输出应为如下的 JSON 格式：


&nbsp;&nbsp;&nbsp;&nbsp;\{



&nbsp;&nbsp;&nbsp;&nbsp;"overall_page_content": "1-2 sentences summarizing the page
&nbsp;&nbsp;&nbsp;&nbsp;"overall_page_content": "1-2 句话概述页面


&nbsp;&nbsp;&nbsp;&nbsp;"Region 1": "ICON_XXX_XXX <functionality>: xxxxx",
&nbsp;&nbsp;&nbsp;&nbsp;"Region 1": "ICON_XXX_XXX <functionality>: xxxxx",


&nbsp;&nbsp;&nbsp;&nbsp;"Region 2": "Element('a poster of a movie xxxx')
&nbsp;&nbsp;&nbsp;&nbsp;"Region 2": "Element('a poster of a movie xxxx')


&nbsp;&nbsp;&nbsp;&nbsp;<functionality>: click to see the details and forward the
&nbsp;&nbsp;&nbsp;&nbsp;<text>点击查看详情并转发</text>


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;purchase page of the movie xx",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;购买页面：电影 xx 的购买页面


\}



---



Listing 1: An example prompt for guiding GPT4o to generate the element functionalities for the given Screenshot.
<text>示例提示：引导 GPT4o 为给定截图生成元素功能。</text>


Since the meta information online doesn't reflect all functions of an app and sometimes may include misleading information, the instruction generated by GPT may have two types of impracticabilities: (i) the task is unrelated to the app activity (ii) some of the steps in the instruction are impractical. For instance, an app "SEPHORA" only sells beauty related products, but GPT may generate tasks that searches for electronics. Another example would be about a ticket booking app "SeekGeek", which doesn't provide a sort option "sort by price from high to low". While human created task would include "sort by price from low to high", GPT would generate something with"sort by price from high to low". This step is impractical so we would filter it out.
<text>由于在线元信息并未反映应用的所有功能，且有时可能包含误导信息，GPT 生成的指令可能存在两类不可行之处：（i）任务与应用活动无关；（ii）指令中的某些步骤不可行。例如，一个名为“SEPHORA”的应用仅销售美妆相关产品，但 GPT 可能生成搜索电子产品的任务。另一个例子是票务预订应用“SeekGeek”，它没有“按价格从高到低排序”的选项而人类创作的任务会包含“按价格从低到高排序”，GPT 可能生成“按价格从高到低排序”的内容。这一步不可行，因此我们会将其过滤掉。</text>


Human annotators are each assigned a random selection of apps and their associated instructions, which they are asked to complete in a natural manner. In contrast to the AITW dataset, our collection methodology allows annotators to make errors and take incorrect steps, leading to a greater prevalence of PRESS_BACK actions. This is motivated by our observation that agents trained on existing datasets exhibit difficulty navigating back to previous pages due to insufficient experience with the PRESS_BACK action. Additionally, after completing an information query task, annotators are asked to manually mark the region of interest on the screenshot using our annotation tool.
<text>人工标注者各自随机分配若干应用及其相关指令，要求自然完成。与 AITW 数据集相比，我们的采集方法允许标注者出错并采取错误步骤，从而导致 PRESS_BACK 行为的出现率更高。这源于我们观察到在现有数据集上训练的代理在返回上一页时难以导航，因为缺乏对 PRESS_BACK 行为的经验。此外，在完成信息查询任务后，标注者还需使用我们的标注工具手动在屏幕截图上标记感兴趣区域。</text>


#### A.1.6 Collection resources details
<text>#### A.1.6 收集资源细节</text>


- GUI interactive element grounding takes approximately 3000 human-hour to filter bounding boxes described in Appendix A.1.3.
<text>- GUI 交互元素定位大约需要 3000 人时用于筛选附录 A.1.3 中描述的边界框。</text>


- GUI screen and element descriptions use GPT- 40 and Gemini API, which consumes about 800 dollars.
<text>- GUI 屏幕与元素描述使用 GPT-40 与 Gemini API，耗资约 800 美元。</text>


- Instructions with GUI-action chains take approximately 300 human-hour.
<text>- 具有 GUI 动作链的指令大约需要 300 人时。</text>


### A.2 Experiment details
<text>### A.2 实验细节</text>


In our implementation, we utilize the internlm-7b variant of the SPHINX model, as detailed in (Liu et al., 2024). The pre-trained checkpoint for this model was sourced from the official repository mentioned in (Team, 2023). For image processing,the input images,each sized ${1024} \times  {1024}$ ,are segmented into sub-images. Visual features from these sub-images are extracted using two distinct visual encoders: DINOv2 (Oquab et al., 2024) and ConvNext (Woo et al., 2023). To ensure compatibility in feature dimensions across different modules, linear projection layers are employed to align the channel dimensions. Regarding the model's parameter settings, as outlined in Section 4.1, we configure the history window size to four. Additionally, we introduce a special token, <ICON>, specifically designed to identify interactive elements within the interface, strengthening the model's interpretability and responsiveness to user interactions. The agent is trained on a cluster with 3 nodes, each with eight NVIDIA A100 (80GB) GPUs. The fine-tuning was completed in four epochs.
<text>在我们的实现中，使用 SPHINX 模型的 internlm-7b 变体，如（Liu 等，2024）所述。该模型的预训练检查点来自（Team，2023）中提及的官方仓库。对于图像处理，输入图像每个尺寸为 ${1024} \times  {1024}$，被分割成子图像。这些子图像的视觉特征通过两种不同的视觉编码器提取：DINOv2（Oquab 等，2024）和 ConvNext（Woo 等，2023）。为确保不同模块之间特征维度兼容，使用线性投影层将通道维度对齐。关于模型的参数设置，如第 4.1 节所述，我们将历史窗口大小设为四。并且，我们引入一个专门用于识别界面中的交互元素的特殊标记 <ICON>，以增强模型对用户交互的可解释性与响应性。代理在一个由 3 节点组成的集群上训练，每个节点配备 eight NVIDIA A100（80GB）GPU。微调在四个时代完成。</text>


#### A.2.1 ANDROIDCONTROL details
<text>#### A.2.1 ANDROIDCONTROL 细节</text>


ANDROIDCONTROL splits the test set into four categories, i.e., in-domain data (IDD), category unseen, app unseen, and task unseen. Besides, the instructions are split into two levels. The high-level instructions are simply the goal of the task and the low-level instructions are the guides for each step during the task execution. Low-level tasks are easier for agents due to the fact that the agent only need to perform the action specified by the step-wise low-level instruction, while the high-level tasks require the agents to perform actions based on the overall instruction. For example, a step-wise low-level instruction is "click the profile element at the bottom right of the screen.", compared with high-level instruction "Change my profile name to Unknown111".
<text>ANDROIDCONTROL 将测试集分为四类：域内数据（IDD）、类别未观测、应用未观测、任务未观测。此外，指令分为两个层级。高层指引仅是任务目标，低层指引则是任务执行过程中的每一步指引。相较于高层任务，低层任务对代理更易，因为代理只需执行逐步低层指令所指定的动作，而高层任务需要代理基于总体指令执行动作。例如，逐步低层指令是“点击屏幕右下角的个人资料元素”，与高层指令“将我的个人资料名改为 Unknown111”相比。</text>


#### A.2.2 AITW details
<text>#### A.2.2 AITW 细节</text>


We observe that there are several types of error cases in the AITW test set (see Figure 6). To assess the unreliability of the original AITW test set annotations, we hire human annotators to evaluate two subsets derived from the original test set. One subset is randomly chosen from episodes where SphA-gent receives low accuracy (named Mis-Match Random), i.e., about 66% compared to the overall 78.72% shown in Table 5, indicating a high mismatch between inference results and AITW annotations. The other subset is randomly selected from the remaining episodes (named Random). Annotators first filter out repeated and redundant screen-shots (see Figure 6a). For each remaining screen-shot, the annotators then record the accuracies of SphAgent-inferred actions and the original AITW annotations versus our human evaluations. Figure 7 illustrates cases where annotators mark both the inferred action and the original annotation as correct, even though they interact with different elements. Table 7 presents the accuracy from human evaluation. The table indicates that the low SphAgent accuracy in the MMR subset are primarily due to unsatisfactory annotations (AITW Anno), which are used as ground truth during the evaluation. Both SphAgent and AITW Anno in the "MMR" subset have lower accuracies than those in the "Random" subset, highlighting that the tasks in the MMR subset are relatively more challenging. Furthermore, SphAgent achieves better human evaluation results than the original annotations, demonstrating its effectiveness and close alignment with human judgment. Comparing the results from Table 5 and Table 7, the notable differences in overall scores (i.e., 78.72%, 86.43%, and 93.95%) underscore the unreliability and misleading nature of the AITW test set annotations.
我们观察到 AITW 测试集存在若干错误情况类型（见图 6）。为评估原始 AITW 测试集标注的不可靠性，我们雇用人工标注者对源测试集中衍生的两个子集进行评估。一个子集从 SphAgent 取得较低准确率的情节中随机选取（命名为 Mis-Match Random），即大约 66%，相较于表 5 中的 78.72% 的总体水平，表明推断结果与 AITW 标注之间存在较高的不匹配。另一子集从剩余情节中随机选取（命名为 Random）。标注者首先筛除重复且冗余的截图（见图 6a）。对于每张剩余截图，标注者记录 SphAgent 推断的动作与原始 AITW 标注与我们的人类评估之间的准确性。图 7 展示了标注者在推断动作与原始标注虽互动于不同元素但都标记为正确的案例。表 7 给出基于人工评估的准确性。表格表明，MMR 子集中的低 SphAgent 准确率主要归因于不令人满意的标注（AITW Anno），在评估中作为真值使用。“MMR”子集中的 SphAgent 与 AITW Anno 的准确率均低于“Random”子集，强调 MMR 子集的任务相对更具挑战性。此外，SphAgent 在人工评估上的结果优于原始标注，体现其有效性以及与人类判断的高度一致性。对比表 5 与表 7 的结果，总体分数（即 78.72%、86.43%、以及 93.95%）的显著差异强调了 AITW 测试集标注的不可靠性与误导性。


### A.3 More AMEX examples
### A.3 更多 AMEX 示例


See Figure 8 for more examples of GUI interactive elements grounding and description. See Figure 9 for more examples of instruction with GUI-action chains.
请参见图 8 了解更多 GUI 交互元素的定位与描述示例。请参见图 9 了解更多带 GUI 动作链的指令示例。


### A.4 Examples of Ethical Problems
### A.4 伦理问题示例


Figure 10 shows examples of anti-script mechanism where the agent can correctly enter the verification codes.
图 10 展示了反脚本机制的示例，在该机制下代理能够正确输入验证码。


Table 7: Human evaluation results on 5%-subsampled AITW test sub-set. "MMR" stands for "Mis-Match Random" subset.
表 7：对 5% 子样本 AITW 测试子集的人类评估结果。“MMR”代表“Mis-Match Random”子集。


<table><tr><td>Subset</td><td>Action Sources</td><td>General</td><td>Install</td><td>G-Apps</td><td>Single</td><td>WebShopping</td><td>Overall</td></tr><tr><td rowspan="2">MMR</td><td>SphAgent vs. Human</td><td>90.91</td><td>83.65</td><td>87.50</td><td>87.68</td><td>82.39</td><td>86.43</td></tr><tr><td>AITW Anno. vs. Human</td><td>88.18</td><td>75.00</td><td>82.69</td><td>84.78</td><td>75.57</td><td>81.25</td></tr><tr><td rowspan="2">Random</td><td>SphAgent vs. Human</td><td>92.31</td><td>93.75</td><td>94.33</td><td>94.83</td><td>94.55</td><td>93.95</td></tr><tr><td>AITW Anno. vs. Human</td><td>94.23</td><td>90.34</td><td>93.62</td><td>93.97</td><td>95.00</td><td>93.43</td></tr></table>
<table><tbody><tr><td>子集</td><td>操作源</td><td>通用</td><td>安装</td><td>G-应用</td><td>单一</td><td>网络购物</td><td>总体</td></tr><tr><td rowspan="2">MMR</td><td>SphAgent 对人类</td><td>90.91</td><td>83.65</td><td>87.50</td><td>87.68</td><td>82.39</td><td>86.43</td></tr><tr><td>AITW 注解对人类</td><td>88.18</td><td>75.00</td><td>82.69</td><td>84.78</td><td>75.57</td><td>81.25</td></tr><tr><td rowspan="2">随机</td><td>SphAgent 对人类</td><td>92.31</td><td>93.75</td><td>94.33</td><td>94.83</td><td>94.55</td><td>93.95</td></tr><tr><td>AITW 注解对人类</td><td>94.23</td><td>90.34</td><td>93.62</td><td>93.97</td><td>95.00</td><td>93.43</td></tr></tbody></table>


Instruction: Add "jbl charge 4" to the cart on ebay.com, then select checkout.
指令：在 ebay.com 将 "jbl charge 4" 加入购物车，然后选择结账。


<img src="https://cdn.noedgeai.com/bo_d5vcgqjef24c73bqi1jg_14.jpg?x=249&y=979&w=1127&h=988&r=0"/>



Figure 6: Error cases in AITW test set. Red dots indicate the actions from the AITW annotations.
图6：AITW 测试集中的错误情况。红点表示来自 AITW 标注的操作。


<img src="https://cdn.noedgeai.com/bo_d5vcgqjef24c73bqi1jg_15.jpg?x=261&y=849&w=1124&h=546&r=0"/>



Figure 7: Purple dots indicate the SphAgent inferred actions and red dots indicate the AITW annotations. Human annotators mark them both correct even though they are clicking different elements.
图7：紫点表示 SphAgent 推断的操作，红点表示 AITW 的标注。人工标注者尽管点击的元素不同，仍标记两者都正确。


2154



Figure 8: More examples
图8：更多示例


<img src="https://cdn.noedgeai.com/bo_d5vcgqjef24c73bqi1jg_16.jpg?x=234&y=259&w=1175&h=1774&r=-1"/>



<img src="https://cdn.noedgeai.com/bo_d5vcgqjef24c73bqi1jg_17.jpg?x=205&y=315&w=1244&h=817&r=0"/>



Open IMDB. Search the movie "Titanic". Who is the top-billed cast member?
打开 IMDB。搜索电影《泰坦尼克号》。谁是片头主演？


<img src="https://cdn.noedgeai.com/bo_d5vcgqjef24c73bqi1jg_17.jpg?x=207&y=1208&w=1243&h=810&r=0"/>



Figure 9: More examples
图9：更多示例


<img src="https://cdn.noedgeai.com/bo_d5vcgqjef24c73bqi1jg_18.jpg?x=205&y=728&w=1248&h=813&r=0"/>



Figure 10: Demonstration of anti-script mechanism where agents can enter the verification codes correctly.
图10：展示反脚本机制的示范，代理人能够正确输入验证码。