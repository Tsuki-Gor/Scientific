# VERIGUI: VERIFIABLE LONG-CHAIN GUI DATASET
# VERIGUI：可验证的长链 GUI 数据集


VeriGUI Team
VeriGUI 团队


## ABSTRACT
## 摘要


Recent studies have delved into constructing autonomous agents capable of performing complex Graphical User Interface (GUI)-based computer tasks, with the potential to revolutionize human-computer interaction. Despite encouraging results, existing efforts mainly focus on short-term interactions and rely on outcome-only verification, thereby limiting their scalability in real-world GUI applications that demand long-horizon task decomposition and execution. In this work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed to facilitate the development and evaluation of generalist GUI agents operating in realistic computer environments. Our dataset emphasizes two critical dimensions: (1) long-chain complexity, with tasks decomposed into a sequence of interdependent subtasks spanning hundreds of steps, explicitly designed to allow any subtask to serve as a valid starting point; and (2) subtask-level verifiability, which enables diverse exploration strategies within each subtask, while ensuring that each subtask-level goal remains verifiable and consistent. The dataset consists of GUI task trajectories across both desktop and web, annotated by human experts. Extensive experiments on VeriGUI using various agents with different foundation models reveal significant performance gaps in handling long-horizon tasks, highlighting the need for more robust planning and decision-making capabilities in GUI agents.
近期研究致力于构建能够执行复杂图形用户界面（GUI）任务的自主代理，有望革新人机交互。尽管结果令人鼓舞，现有工作主要聚焦短期交互并依赖仅结果的验证，因而限制了在需要长远任务分解与执行的真实 GUI 应用中的可扩展性。本文提出 VeriGUI，一种新颖的可验证长链 GUI 数据集，旨在促进通用 GUI 代理在真实计算环境中的开发与评估。我们的数据集强调两大维度：（1）长链复杂性，任务被分解为数百步的相互依赖子任务序列，明确设计为任一子任务都可作为有效起点；（2）子任务级可验证性，允许在每个子任务内进行多样化探索策略，同时确保每个子任务目标可验证且一致。数据集由覆盖桌面与网页的 GUI 任务轨迹构成，由人工专家标注。对 VeriGUI 上不同基础模型的各类代理进行的大量实验显示，在处理长远任务时存在显著性能差距，凸显了 GUI 代理在规划与决策能力上需要更强的鲁棒性。


ethtps://github.com/VeriGUI-Team/VeriGUI
ethtps://github.com/VeriGUI-Team/VeriGUI


https://huggingface.co/datasets/2077AIDataFoundation/VeriGUI



## 1 INTRODUCTION
## 1 引言


Autonomous Graphical User Interface (GUI) agents have recently demonstrated extraordinary capabilities in interactive computer tasks by following high-level instructions (Wang et al., 2024; Zhang et al., 2024a; Nguyen et al., 2024), supporting diverse workflows from web browsing to desktop applications (Ning et al., 2025; Hu et al., 2024). Recent breakthroughs in Multimodal Large Language Models (MLLMs) (Zhang et al., 2024c; Team et al., 2023; Achiam et al., 2023; Bai et al., 2025; Liu et al., 2023) have enabled promising prototypes of such agents that can perform complex decision-making tasks without relying on hard-coded automation or domain-specific scripting (Tan et al., 2024; Xie et al., 2023). However, developing such general-purpose GUI agents involves multiple complex processes, as it requires the ability to perceive complex visual layouts (Hong et al., 2024; Gou et al., 2024; Cheng et al., 2024), plan over long action sequences (Zhang et al., 2024d; Agashe et al., 2024), and generalize across dynamic and heterogeneous platforms (Wu et al., 2024; Zhang et al., 2025). This also poses a new challenge: how to obtain high-quality datasets that capture diverse, realistic human-computer interactions at scale to evaluate these agents effectively (Deng et al., 2023; Li et al., 2025; Liu et al., 2024b).
自主图形用户界面（GUI）代理最近在遵循高层指令的交互式计算任务中展现出非凡能力（Wang et al., 2024；Zhang et al., 2024a；Nguyen et al., 2024），支持从网页浏览到桌面应用等多样化工作流（Ning et al., 2025；Hu et al., 2024）。多模态大语言模型（MLLMs）的最新突破（Zhang et al., 2024c；Team et al., 2023；Achiam et al., 2023；Bai et al., 2025；Liu et al., 2023）推动了这类代理的有希望原型，使其能在无需硬编码自动化或领域特定脚本的情况下执行复杂决策任务（Tan et al., 2024；Xie et al., 2023）。然而，开发此类通用 GUI 代理涉及多项复杂过程，它要求感知复杂视觉布局的能力（Hong et al., 2024；Gou et al., 2024；Cheng et al., 2024）、对长动作序列的规划能力（Zhang et al., 2024d；Agashe et al., 2024），以及跨动态且异构平台的泛化能力（Wu et al., 2024；Zhang et al., 2025）。这也带来新挑战：如何获取高质量数据集以大规模捕捉多样且真实的人机交互，从而有效评估这些代理（Deng et al., 2023；Li et al., 2025；Liu et al., 2024b）。


To address this challenge, various datasets and benchmarks have been released to facilitate the development of autonomous GUI agents (Zhang et al., 2025; Yang et al., 2025; He et al., 2024). Despite encouraging results, existing GUI datasets still suffer from two major limitations. First, most recent datasets focus on relatively short-term interactions (Lù et al., 2024; Chen et al., 2025), where the agent can complete a task in just a few steps (e.g., mostly less than 10 steps), typically by identifying a UI element and executing a corresponding action (Li et al., 2025; Deng et al., 2023). For example, a task like "Search for an email about the invoice" can typically be completed in just three steps: open the email app, click the search bar, and type the keyword. Such interactions rarely require long-horizon planning or multi-step reasoning (Gao et al., 2024; Bonatti et al., 2024; Zheng et al., 2024), both of which are essential for solving real-world workflows involving conditional task dependencies and intermediate state tracking (Deng et al., 2023; Yang et al., 2025). Second, existing evaluation protocols typically rely on outcome-only validation such as checking whether the final page URL has been reached (Zhou et al., 2023a; Xie et al., 2024; Zhao et al., 2025). This coarse-grained supervision fails to capture the quality of intermediate subtasks, especially when tasks involve multiple interdependent subtasks (Pan et al., 2024). In such cases, when agents fail to achieve the desired goal, it is often unclear where or why the failure occurred, thereby making it difficult to support improvements to agent capability.
为应对该挑战，已有多种数据集与基准发布以促进自主 GUI 代理的发展（Zhang et al., 2025；Yang et al., 2025；He et al., 2024）。尽管结果可喜，现有 GUI 数据集仍存在两大局限。首先，大多数近期数据集侧重相对短期的交互（Lù et al., 2024；Chen et al., 2025），代理通常能在很少步数内完成任务（例如，大多少于 10 步），通常通过识别 UI 元素并执行相应操作来完成（Li et al., 2025；Deng et al., 2023）。例如“搜索有关发票的邮件”这样的任务通常可在三步内完成：打开邮件应用、点击搜索栏、输入关键词。这类交互很少需要长远规划或多步推理（Gao et al., 2024；Bonatti et al., 2024；Zheng et al., 2024），而这些正是解决涉及条件依赖与中间状态追踪的真实工作流所必需的（Deng et al., 2023；Yang et al., 2025）。其次，现有评估协议通常依赖仅结果的验证，如检查是否到达最终页面 URL（Zhou et al., 2023a；Xie et al., 2024；Zhao et al., 2025）。这种粗粒度的监督无法捕捉中间子任务的质量，尤其当任务包含多个相互依赖的子任务时（Pan et al., 2024）。在此类情况下，当代理未达成预期目标时，常难以明确故障发生的环节或原因，进而阻碍对代理能力的改进。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_19_27_677ce3.jpg"/>



Figure 1: An overview of the VeriGUI dataset, which emphasizes (1) long-chain complexity, where each tasks consist of interdependent subtasks that span hundreds of steps, with each subtask serving as a valid starting point; and (2) subtask-level verifiability, enabling diverse exploration strategies while ensuring that the goal of each subtask is verifiable and consistent.
图 1：VeriGUI 数据集概览，强调 (1) 长链复杂性：每个任务由跨越数百步的相互依赖子任务组成，每个子任务可作为有效的起点；以及 (2) 子任务级可验证性：允许多样的探索策略，同时确保每个子任务的目标可验证且一致。


In this work, we introduce VeriGUI, a new verifiable long-chain dataset tailored for the development and evaluation of GUI agents. VeriGUI encompasses various richly annotated GUI task trajectories across desktop and web. All trajectories are carefully created and annotated by human experts, ensuring long-chain complexity and subtask-level verifiability, as shown in Fig. 1. (1) The long-chain complexity of VeriGUI features tasks that require agents to perform sequences of 4-8 interdependent subtasks with hundreds of GUI operations, often involving transitions across multiple applications or webpages. Notably, each subtask is designed to serve as a valid starting point, enabling agent evaluation across different task stages. To succeed, agents must engage in adaptive reasoning to manage dynamic task flows. This setup encourages the development of agents with robust planning, memory, and decision-making abilities across a wide range of complex GUI environments. (2) The subtask-level verifiability of VeriGUI enables a fine-grained assessment of intermediate results at every subtask rather than solely at the final outcome. Note that a subtask consists of multiple steps with specific GUI operations. Instead of verifying the low-level steps, the dataset focuses on evaluating whether the goal of each subtask has been correctly achieved, providing a more informative supervision signal. Thus, the dataset also supports open-ended interaction within each subtask, encouraging agents to explore diverse strategies to accomplish the goal of each subtask, rather than adhering to a fixed sequence of steps. Our core contributions are summarized as follows:
在本工作中，我们提出 VeriGUI，一种针对 GUI 代理开发与评估的新型可验证长链数据集。VeriGUI 包含多种经过丰富标注的桌面与网页 GUI 任务轨迹。所有轨迹均由人类专家精心创建与标注，确保长链复杂性与子任务级可验证性，如图 1 所示。(1) VeriGUI 的长链复杂性体现在任务要求代理完成 4–8 个相互依赖的子任务序列，涉及数百次 GUI 操作，常跨多应用或网页切换。值得注意的是，每个子任务均被设计为可作为有效起点，支持对不同任务阶段的代理评估。要成功，代理必须进行自适应推理以应对动态任务流。本设置促使开发在复杂 GUI 环境中具有稳健规划、记忆与决策能力的代理。(2) VeriGUI 的子任务级可验证性允许对每个子任务的中间结果进行细粒度评估，而不仅仅看最终结果。注意子任务由多步具体 GUI 操作构成。数据集并不验证低层次步骤，而是侧重评估每个子任务目标是否被正确达成，从而提供更具信息量的监督信号。因此，数据集也支持每个子任务内的开放式交互，鼓励代理探索多样策略以实现子任务目标，而非遵循固定步骤序列。我们的核心贡献总结如下：


- We present VeriGUI, a large-scale, human-annotated dataset of verifiable long-chain GUI tasks designed to support research on autonomous agents in real-world computer environments.
- 我们提出 VeriGUI，一套大规模、人工标注的可验证长链 GUI 任务数据集，旨在支持现实计算环境中自治代理的研究。


- We design a comprehensive benchmark on top of VeriGUI, supporting multiple levels of evaluation, including task success rate, task completion rate, and action efficiency. This enables fine-grained analysis of agent capabilities across different stages of task execution and provides deeper insights into failure modes and planning bottlenecks.
- 我们基于 VeriGUI 设计了一个综合基准，支持多层次评估，包括任务成功率、任务完成率与动作效率，能够对任务执行不同阶段的代理能力进行细粒度分析，并深入揭示失败模式与规划瓶颈。


- Extensive experiments with a range of various agents using state-of-the-art foundation models reveal substantial performance gaps on long-chain tasks, underscoring current limitations in complex planning and decision-making in GUI agents.
- 我们对多种基于最先进基础模型的代理进行了大量实验，结果显示在长链任务上存在显著性能差距，凸显当前 GUI 代理在复杂规划与决策方面的局限性。


Table 1: Comparison of existing GUI datasets and benchmarks with VeriGUI. Platform indicates whether the benchmark supports web or desktop applications. #Steps refers to the average or range of steps per task. Verifiability describes how task trajectories are validated. Human demonstration indicates the presence of collected expert trajectories. Executability denotes whether an executable environment is available. Interaction defines the structure of the action space. Note that for VeriGUI, the #Steps reflects the average number of GUI operations in the human demonstration dataset.
表 1：现有 GUI 数据集与基准与 VeriGUI 的比较。Platform 指示基准是否支持网页或桌面应用；#Steps 指每个任务的平均或范围步数；Verifiability 描述任务轨迹如何被验证；Human demonstration 表示是否收集了专家演示轨迹；Executability 指是否提供可执行环境；Interaction 定义动作空间结构。注意 VeriGUI 的 #Steps 反映的是人类演示数据集中平均的 GUI 操作次数。


<table><tr><td>Datasets and Benchmarks</td><td>Platform</td><td>#Steps</td><td>Verifiability</td><td>Human Demonstration</td><td>Executability</td><td>Interaction</td></tr><tr><td>VisualWebArena (Koh et al., 2024)</td><td>Web</td><td>9.6</td><td>Outcome</td><td>✘</td><td>✓</td><td>Web Element</td></tr><tr><td>VisualWebBench (Liu et al., 2024b)</td><td>Web</td><td>1.0</td><td>Outcome</td><td>✓</td><td>✘</td><td>Grounding</td></tr><tr><td>WebArena (Zhou et al., 2023a)</td><td>Web</td><td>-</td><td>Outcome</td><td>✘</td><td>✓</td><td>Web Element</td></tr><tr><td>Mind2Web (Deng et al., 2023)</td><td>Web</td><td>7.3</td><td>Step</td><td>✓</td><td>✓</td><td>Web Element</td></tr><tr><td>WebShop (Yao et al., 2022)</td><td>Web</td><td>11.3</td><td>Outcome</td><td>✘</td><td>✓</td><td>Web Element</td></tr><tr><td>WebVoyager (He et al., 2024)</td><td>Web</td><td>[3, 15]</td><td>Outcome</td><td>✓</td><td>✓</td><td>Web Element</td></tr><tr><td>WebCanvas (Pan et al., 2024)</td><td>Web</td><td>8.4</td><td>Step</td><td>✘</td><td>✓</td><td>Web Element</td></tr><tr><td>WebWalker (Wu et al., 2025)</td><td>Web</td><td>4.6</td><td>Outcome</td><td>✘</td><td>✓</td><td>Web Element</td></tr><tr><td>WebLINX (Lù et al., 2024)</td><td>Web</td><td>43.0</td><td>Outcome</td><td>✓</td><td>✘</td><td>Web Element</td></tr><tr><td>OSWorld (Xie et al., 2024)</td><td>Desktop + Web</td><td>[1, 15]</td><td>Outcome</td><td>✘</td><td>✓</td><td>GUI Operations</td></tr><tr><td>AgentStudio (Zheng et al., 2024)</td><td>Desktop + Web</td><td>[1, 30]</td><td>Outcome</td><td>✘</td><td>✓</td><td>GUI Operations</td></tr><tr><td>GUI-World (Chen et al., 2025)</td><td>Desktop + Web</td><td>-</td><td>Outcome</td><td>✓</td><td>✘</td><td>GUI Operations</td></tr><tr><td>WindowsAgentArena (Bonatti et al., 2024)</td><td>Desktop + Web</td><td>8.1</td><td>Outcome</td><td>✘</td><td>✓</td><td>GUI Operations</td></tr><tr><td>WorldGUI (Zhao et al., 2025)</td><td>Desktop + Web</td><td>-</td><td>Outcome</td><td>✘</td><td>✓</td><td>GUI Operations</td></tr><tr><td>TongUI (Zhang et al., 2025)</td><td>Desktop + Web</td><td>[1, 9]</td><td>Outcome</td><td>✓</td><td>✘</td><td>GUI Operations</td></tr><tr><td>GUI-Robust (Yang et al., 2025)</td><td>Desktop + Web</td><td>-</td><td>Step</td><td>✓</td><td>✘</td><td>GUI Operations</td></tr><tr><td>AssistGUI (Gao et al., 2024)</td><td>Desktop</td><td>[10, 25]</td><td>Outcome</td><td>✘</td><td>✓</td><td>GUI Operations</td></tr><tr><td>ScreenSpot-Pro (Li et al., 2025)</td><td>Desktop+Web</td><td>1.0</td><td>Outcome</td><td>✓</td><td>✘</td><td>Grounding</td></tr><tr><td>VeriGUI (Ours)</td><td>Besktop + Web</td><td>214.4</td><td>Subtask</td><td>✓</td><td>✓</td><td>GUI Operations</td></tr></table>
<table><tbody><tr><td>数据集与基准</td><td>平台</td><td>#步骤</td><td>可验证性</td><td>人工示范</td><td>可执行性</td><td>交互</td></tr><tr><td>VisualWebArena (Koh et al., 2024)</td><td>网络</td><td>9.6</td><td>结果</td><td>✘</td><td>✓</td><td>网页元素</td></tr><tr><td>VisualWebBench (Liu et al., 2024b)</td><td>网络</td><td>1.0</td><td>结果</td><td>✓</td><td>✘</td><td>基于定位</td></tr><tr><td>WebArena (Zhou et al., 2023a)</td><td>网络</td><td>-</td><td>结果</td><td>✘</td><td>✓</td><td>网页元素</td></tr><tr><td>Mind2Web (Deng et al., 2023)</td><td>网络</td><td>7.3</td><td>步骤</td><td>✓</td><td>✓</td><td>网页元素</td></tr><tr><td>WebShop (Yao et al., 2022)</td><td>网络</td><td>11.3</td><td>结果</td><td>✘</td><td>✓</td><td>网页元素</td></tr><tr><td>WebVoyager (He et al., 2024)</td><td>网络</td><td>[3, 15]</td><td>结果</td><td>✓</td><td>✓</td><td>网页元素</td></tr><tr><td>WebCanvas (Pan et al., 2024)</td><td>网络</td><td>8.4</td><td>步骤</td><td>✘</td><td>✓</td><td>网页元素</td></tr><tr><td>WebWalker (Wu et al., 2025)</td><td>网络</td><td>4.6</td><td>结果</td><td>✘</td><td>✓</td><td>网页元素</td></tr><tr><td>WebLINX (Lù et al., 2024)</td><td>网络</td><td>43.0</td><td>结果</td><td>✓</td><td>✘</td><td>网页元素</td></tr><tr><td>OSWorld (Xie et al., 2024)</td><td>桌面 + 网络</td><td>[1, 15]</td><td>结果</td><td>✘</td><td>✓</td><td>GUI 操作</td></tr><tr><td>AgentStudio (Zheng et al., 2024)</td><td>桌面 + 网络</td><td>[1, 30]</td><td>结果</td><td>✘</td><td>✓</td><td>GUI 操作</td></tr><tr><td>GUI-World (Chen et al., 2025)</td><td>桌面 + 网络</td><td>-</td><td>结果</td><td>✓</td><td>✘</td><td>GUI 操作</td></tr><tr><td>WindowsAgentArena (Bonatti et al., 2024)</td><td>桌面 + 网络</td><td>8.1</td><td>结果</td><td>✘</td><td>✓</td><td>GUI 操作</td></tr><tr><td>WorldGUI (Zhao et al., 2025)</td><td>桌面 + 网络</td><td>-</td><td>结果</td><td>✘</td><td>✓</td><td>GUI 操作</td></tr><tr><td>TongUI (Zhang et al., 2025)</td><td>桌面 + 网络</td><td>[1, 9]</td><td>结果</td><td>✓</td><td>✘</td><td>GUI 操作</td></tr><tr><td>GUI-Robust (Yang et al., 2025)</td><td>桌面 + 网络</td><td>-</td><td>步骤</td><td>✓</td><td>✘</td><td>GUI 操作</td></tr><tr><td>AssistGUI (Gao et al., 2024)</td><td>桌面</td><td>[10, 25]</td><td>结果</td><td>✘</td><td>✓</td><td>GUI 操作</td></tr><tr><td>ScreenSpot-Pro (Li et al., 2025)</td><td>桌面+网络</td><td>1.0</td><td>结果</td><td>✓</td><td>✘</td><td>基于定位</td></tr><tr><td>VeriGUI (Ours)</td><td>桌面 + 网络</td><td>214.4</td><td>子任务</td><td>✓</td><td>✓</td><td>GUI 操作</td></tr></tbody></table>


## 2 RELATED WORKS
## 2 相关工作


### 2.1 GUI DATASETS & BENCHMARKS
### 2.1 GUI 数据集与基准


Large-scale GUI datasets and benchmarks are fundamental for training and evaluating autonomous agents in realistic human-computer interaction settings (Liu et al., 2024b; He et al., 2024; Chen et al., 2025; Zhang et al., 2025; Gao et al., 2024; Pan et al., 2024), as summarized in Tab. 1. Early web datasets and benchmarks (Shi et al., 2017; Liu et al., 2018; Yao et al., 2022) relied on simplified simulations, while recent efforts (Deng et al., 2023; Zhou et al., 2023a; Koh et al., 2024) shift toward real-world browser environments for more realistic evaluation. VisualWebBench (Liu et al., 2024b) emphasizes visual grounding and reasoning via webpage screenshots but lacks interaction capabilities. On the desktop side, OSWorld (Xie et al., 2024) and WindowsAgentArena (Bonatti et al., 2024) evaluate agents in full-featured OS environments with programmatic feedback. Other datasets and benchmarks, such as GUI-Robust (Yang et al., 2025) and WorldGUI (Zhao et al., 2025), explore robustness under varied and abnormal conditions, while ScreenSpot (Li et al., 2025) focuses on spatial element grounding rather than full task execution. However, most existing datasets rely on outcome-only verification. Several datasets (Deng et al., 2023; Yang et al., 2025; Pan et al., 2024) provide step-level annotations (e.g., specific GUI actions or URL match), but require agents to strictly follow predefined action sequences. This design restricts the exploration capabilities of agents required in real-world applications. Moreover, these datasets emphasize short-term interactions, offering limited insight into agent decision-making quality over long, interdependent task sequences. VeriGUI addresses these gaps by enabling subtask-level supervision and open-ended exploration across long-horizon GUI workflows.
大规模 GUI 数据集与基准对于在真实人机交互场景中训练与评估自主代理至关重要（Liu et al., 2024b; He et al., 2024; Chen et al., 2025; Zhang et al., 2025; Gao et al., 2024; Pan et al., 2024），如表 1 所示。早期网页数据集与基准（Shi et al., 2017; Liu et al., 2018; Yao et al., 2022）依赖简化的模拟，而近期工作（Deng et al., 2023; Zhou et al., 2023a; Koh et al., 2024）则转向真实浏览器环境以实现更现实的评估。VisualWebBench（Liu et al., 2024b）强调通过网页截图进行视觉定位与推理，但缺乏交互能力。桌面方向上，OSWorld（Xie et al., 2024）和 WindowsAgentArena（Bonatti et al., 2024）在功能齐全的操作系统环境中、结合程序化反馈评估代理。其他数据集与基准，如 GUI-Robust（Yang et al., 2025）和 WorldGUI（Zhao et al., 2025），探讨在多变与异常条件下的鲁棒性，而 ScreenSpot（Li et al., 2025）侧重空间元素定位而非完整任务执行。然而，大多数现有数据集仅依赖结果验证。若干数据集（Deng et al., 2023; Yang et al., 2025; Pan et al., 2024）提供步骤级注释（例如，特定 GUI 操作或 URL 匹配），但要求代理严格遵循预定义操作序列。这种设计限制了代理在现实应用中所需的探索能力。此外，这些数据集强调短期交互，对代理在长时、相互依赖任务序列中的决策质量提供的洞见有限。VeriGUI 通过支持子任务级监督和在长时 GUI 工作流中的开放式探索弥补了这些空白。


### 2.2 GUI AGENTS
### 2.2 GUI 代理


The emergence of MLLMs like GPT-4V (Achiam et al., 2023), Gemini-Pro (Team et al., 2023), and Qwen-VL (Bai et al., 2025) has catalyzed progress in generalist GUI agents capable of interpreting screen content and executing natural language instructions. Recent agent architectures such as Show-UI (Lin et al., 2025) and UI-TARS (Qin et al., 2025) extend MLLMs with task planning modules, visual grounding techniques, and hierarchical memory (Zheng et al., 2024; Zhang et al., 2024b; Hong et al., 2024; You et al., 2024; Tan et al., 2024). These systems highlight two critical capabilities: element grounding, i.e., recognizing actionable UI components from raw pixels or accessibility metadata (Li et al., 2025); and long-horizon planning, i.e., decomposing high-level instructions into coherent action sequences (Zhao et al., 2025). Several works improve agent planning and reasoning capabilities via prompt engineering (Tan et al., 2024; Zheng et al., 2024; Zhou et al., 2023b; 2024), supervised fine-tuning (Lin et al., 2025; Qin et al., 2025), or reinforcement learning (Luo et al., 2025; Zhou et al., 2025). In parallel to GUI agents, substantial progress has been made in deep research agents (Song et al., 2025; Jin et al., 2025; Zheng et al., 2025; Zhu et al., 2025a; Shi et al., 2025; Zhu et al., 2025b) that perform multi-hop web search and synthesis via search tool-augmented LLMs. Unlike GUI agents, these systems interact through textual APIs rather than visual interfaces. Despite promising results on existing tasks, our experiments show that current agents struggle with multi-step decision-making and error recovery in complex workflows, underscoring the need for benchmarks like VeriGUI that explicitly test long-chain generalization.
多模态大模型（MLLM）如 GPT-4V（Achiam et al., 2023）、Gemini-Pro（Team et al., 2023）和 Qwen-VL（Bai et al., 2025）的出现推动了能理解屏幕内容并执行自然语言指令的通用 GUI 代理的发展。近期代理架构如 Show-UI（Lin et al., 2025）和 UI-TARS（Qin et al., 2025）在 MLLM 基础上扩展了任务规划模块、视觉定位技术与分层记忆（Zheng et al., 2024; Zhang et al., 2024b; Hong et al., 2024; You et al., 2024; Tan et al., 2024）。这些系统突出了两项关键能力：元素定位，即从原始像素或辅助可访问性元数据中识别可操作的 UI 组件（Li et al., 2025）；以及长时规划，即将高层指令分解为连贯的操作序列（Zhao et al., 2025）。若干工作通过提示工程（Tan et al., 2024; Zheng et al., 2024; Zhou et al., 2023b; 2024）、监督微调（Lin et al., 2025; Qin et al., 2025）或强化学习（Luo et al., 2025; Zhou et al., 2025）提升代理的规划与推理能力。与 GUI 代理并行，深度研究代理（Song et al., 2025; Jin et al., 2025; Zheng et al., 2025; Zhu et al., 2025a; Shi et al., 2025; Zhu et al., 2025b）在通过搜索工具增强的大型语言模型上实现了多跳网页检索与合成。与 GUI 代理不同，这些系统通过文本型 API 交互而非视觉界面。尽管在现有任务上取得了可观成果，我们的实验表明当前代理在复杂工作流中的多步决策与错误恢复方面仍存在困难，凸显了像 VeriGUI 这样明确测试长链泛化能力的基准的必要性。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_19_27_9659f1.jpg"/>



Figure 2: The VeriGUI dataset consists of various GUI tasks spanning both desktop and web.
图 2：VeriGUI 数据集包含跨桌面与网页的各类 GUI 任务。


## 3 VERIGUI DATASET
## 3 VeriGUI 数据集


In this section, we present the task formulation, data collection procedure, and statistical analysis of the VeriGUI dataset. As shown in Fig. 2, VeriGUI comprises two primary categories: web and desktop tasks. Specifically, the web tasks focus on deep research requiring multi-hop information retrieval and reasoning ${}^{1}$ , whereas the desktop tasks emphasize application operation involving intricate GUI interactions and systematic state management.
本节介绍 VeriGUI 数据集的任务表述、数据收集流程与统计分析。如图 2 所示，VeriGUI 包含两类主要任务：网页任务与桌面任务。具体而言，网页任务侧重需要多跳信息检索与推理的深度研究 ${}^{1}$ ，而桌面任务强调涉及复杂 GUI 交互与系统化状态管理的应用操作。


### 3.1 TASK FORMULATION
### 3.1 任务表述


We formulate GUI-based tasks in VeriGUI as a Partially Observable Markov Decision Process (POMDP),defined by the tuple $\langle \mathcal{S},\mathcal{O},\mathcal{A},P,O,R\rangle$ ,where $\mathcal{S}$ is the set of environment states, representing the full underlying system configuration. $\mathcal{O}$ is the observation space,and $O : \mathcal{S} \rightarrow  \mathcal{O}$ is the observation function, which models the partial observations the agent receives from the environment. $\mathcal{A}$ is the action space of GUI operations. $P : \mathcal{S} \times  \mathcal{A} \times  \mathcal{S} \rightarrow  \left\lbrack  {0,1}\right\rbrack$ is the state transition function, modeling the (often non-deterministic) dynamics of the GUI environment in response to actions. $R$ is the reward function,which is defined through subtask-level verifiable goals.
我们在 VeriGUI 中将基于 GUI 的任务表述为部分可观测马尔可夫决策过程（POMDP），由元组 $\langle \mathcal{S},\mathcal{O},\mathcal{A},P,O,R\rangle$ 定义，其中 $\mathcal{S}$ 是环境状态集合，表示完整的底层系统配置。$\mathcal{O}$ 是观测空间，$O : \mathcal{S} \rightarrow  \mathcal{O}$ 是观测函数，用于建模代理从环境接收的部分观测。$\mathcal{A}$ 是 GUI 操作的动作空间。$P : \mathcal{S} \times  \mathcal{A} \times  \mathcal{S} \rightarrow  \left\lbrack  {0,1}\right\rbrack$ 是状态转移函数，建模 GUI 环境对动作响应的（常为非确定性）动态。$R$ 是奖励函数，通过子任务级可验证目标定义。


---



${}^{1}$ The current version of VeriGUI focuses on deep research tasks. Future versions will support a wider range of interactive tasks involving interface manipulation, such as filling out forms and setting preferences.
${}^{1}$ 当前版本的 VeriGUI 侧重于深度研究任务。未来版本将支持更广泛的交互式任务，涉及界面操作，例如填写表单和设置偏好。


---



For each GUI task in VeriGUI with an instruction $Q$ ,we obtain a complete task trajectory $\tau  = \; \left( {{o}_{0},{a}_{0},{o}_{1},{a}_{1},\ldots ,{o}_{T}}\right)$ ,where $T$ denotes the number of steps in the trajectory. To capture intermediate results and provide dense supervision,we decompose $\tau$ into a sequence of $K$ subtasks ${\tau }^{\left( 1\right) },{\tau }^{\left( 2\right) },\ldots ,{\tau }^{\left( K\right) }$ ,such that $\tau  = {\tau }^{\left( 1\right) } \circ  {\tau }^{\left( 2\right) } \circ  \cdots  \circ  {\tau }^{\left( K\right) }$ ,where $\circ$ denotes trajectory concatenation. The subtask ${\tau }^{\left( k\right) } = \left( {{o}_{{t}_{k}},{a}_{{t}_{k}},\ldots ,{a}_{{t}_{k + 1} - 1},{o}_{{t}_{k + 1}}}\right)$ corresponds to a contiguous segment of the full trajectory,where ${t}_{k}$ and ${t}_{k + 1}$ denote the start and end timesteps. Each subtask ${\tau }^{\left( k\right) }$ is associated with a sub-instruction ${Q}^{\left( k\right) }$ and an independent subtask-level goal function ${G}^{\left( k\right) } : {\tau }^{\left( k\right) } \rightarrow  \{ 0,1\}$ ,which determines whether the agent has correctly completed the $k$ -th goal. The corresponding subtask-level reward is defined as ${R}^{\left( k\right) } = {G}^{\left( k\right) }\left( {\tau }^{\left( k\right) }\right)  \in  \{ 0,1\}$ .
对于 VeriGUI 中带有指令 $Q$ 的每个 GUI 任务，我们获得完整的任务轨迹 $\tau  = \; \left( {{o}_{0},{a}_{0},{o}_{1},{a}_{1},\ldots ,{o}_{T}}\right)$，其中 $T$ 表示轨迹的步数。为捕捉中间结果并提供密集监督，我们将 $\tau$ 分解为一系列 $K$ 个子任务 ${\tau }^{\left( 1\right) },{\tau }^{\left( 2\right) },\ldots ,{\tau }^{\left( K\right) }$，使得 $\tau  = {\tau }^{\left( 1\right) } \circ  {\tau }^{\left( 2\right) } \circ  \cdots  \circ  {\tau }^{\left( K\right) }$，其中 $\circ$ 表示轨迹拼接。子任务 ${\tau }^{\left( k\right) } = \left( {{o}_{{t}_{k}},{a}_{{t}_{k}},\ldots ,{a}_{{t}_{k + 1} - 1},{o}_{{t}_{k + 1}}}\right)$ 对应于完整轨迹的连续片段，其中 ${t}_{k}$ 和 ${t}_{k + 1}$ 表示起始和结束时间步。每个子任务 ${\tau }^{\left( k\right) }$ 关联一个子指令 ${Q}^{\left( k\right) }$ 和一个独立的子任务级目标函数 ${G}^{\left( k\right) } : {\tau }^{\left( k\right) } \rightarrow  \{ 0,1\}$，用于判定代理是否正确完成第 $k$ 个目标。相应的子任务级奖励定义为 ${R}^{\left( k\right) } = {G}^{\left( k\right) }\left( {\tau }^{\left( k\right) }\right)  \in  \{ 0,1\}$。


Observation Space. We consider two types of GUI tasks in VeriGUI: (1) web tasks, where the observation $\mathcal{O}$ includes a webpage screenshot and the HTML DOM tree. These provide both visual and structural cues for decision-making. (2) desktop tasks,where the observation $\mathcal{O}$ only consists of a desktop GUI screenshot. Compared to web environments, desktop tasks often lack structured DOM data, making perception more dependent on visual signals.
观测空间。我们在 VeriGUI 中考虑两类 GUI 任务：(1) 网页任务，其观测 $\mathcal{O}$ 包含网页截图和 HTML DOM 树。这些为决策提供视觉和结构提示。 (2) 桌面任务，其观测 $\mathcal{O}$ 仅由桌面 GUI 截图组成。与网页环境相比，桌面任务通常缺乏结构化的 DOM 数据，使感知更依赖视觉信号。


Action Space. The action space $\mathcal{A}$ defines a unified set of GUI operations applicable across both web and desktop tasks, as shown in Tab. 2. These actions cover common interaction modalities such as click, input, and key events. During execution, the agent selects one action per step from this action set. In some cases, the result_state () action is used by the model to output the final result. The specific mapping between the actions recorded during data collection and the GUI actions is provided in Appendix A.
动作空间。动作空间 $\mathcal{A}$ 定义了一组统一的 GUI 操作，适用于网页和桌面任务，如表 2 所示。这些动作覆盖常见的交互模式，如点击、输入和按键事件。执行过程中，代理每步从该动作集中选择一项。在某些情况下，模型使用 result_state () 动作来输出最终结果。数据收集中记录的动作与 GUI 动作之间的具体映射见附录 A。


Table 2: GUI actions in VeriGUI
表 2：VeriGUI 中的 GUI 动作


<table><tr><td>Action Type</td><td>Description</td></tr><tr><td>left_click ([x, y])</td><td>Left-click at given coordinates</td></tr><tr><td>right_click([x, y])</td><td>Right-click at given coordinates</td></tr><tr><td>drag([x, y])</td><td>Drag at given coordinates</td></tr><tr><td>scroll()</td><td>Scroll vertically</td></tr><tr><td>input (text)</td><td>Type a string input</td></tr><tr><td>key_down(key)</td><td>Press down a single key</td></tr><tr><td>result_state()</td><td>Output result statement</td></tr></table>
<table><tbody><tr><td>操作类型</td><td>描述</td></tr><tr><td>left_click ([x, y])</td><td>在指定坐标左键单击</td></tr><tr><td>right_click([x, y])</td><td>在指定坐标右键单击</td></tr><tr><td>drag([x, y])</td><td>在指定坐标拖动</td></tr><tr><td>scroll()</td><td>垂直滚动</td></tr><tr><td>input (text)</td><td>输入字符串</td></tr><tr><td>key_down(key)</td><td>按下单个按键</td></tr><tr><td>result_state()</td><td>输出结果状态</td></tr></tbody></table>


Goal Space. For web tasks, the goal is defined as obtaining a correct textual answer through interaction with the webpage. The agent must actively search, navigate, and reason over web content to extract the required information. A goal is considered successfully completed if the final output text matches the expected answer. For desktop tasks, goals are defined as reaching specific system states, such as enabling a configuration or launching an application. Goal completion is determined by verifying whether the current GUI or system state satisfies the intended task outcome, based on screenshots or accessibility properties. Subtask-level goal functions ${G}^{\left( k\right) }$ provide binary supervision for each sub-instruction and define the subtask-level reward ${R}^{\left( k\right) }$ used for training and evaluation.
目标空间。对于 Web 任务，目标定义为通过与网页交互获取正确的文本答案。智能体必须主动搜索、导航并对网页内容进行推理以提取所需信息。若最终输出文本与期望答案匹配，则视为目标成功完成。对于桌面任务，目标定义为达到特定系统状态，例如启用某项配置或启动某个应用程序。目标完成通过基于截图或无障碍属性验证当前 GUI 或系统状态是否满足预期任务结果来判断。子任务级目标函数 ${G}^{\left( k\right) }$ 为每个子指令提供二元监督，并定义用于训练与评估的子任务级奖励 ${R}^{\left( k\right) }$。


### 3.2 DATA COLLECTION.
### 3.2 数据收集。


Data Source. The VeriGUI dataset is constructed from a wide range of real-world GUI environments encompassing both web and desktop platforms. (1) For web tasks, we specifically focus on deep research scenarios involving information retrieval and reasoning. Thus, we curate data from publicly accessible and authoritative sources, including official websites of government agencies, academic institutions, online encyclopedias, financial databases, and news portals. These tasks cover five primary thematic domains: scientific and academic research; finance and economics; technology and innovation; arts and entertainment; and social policy and sustainability. This categorization ensures diverse topical coverage and reflects realistic user intentions in complex information-seeking scenarios. (2) For desktop tasks, we include three representative domains of applications commonly used in professional and everyday workflows: office productivity software (e.g., Word, Excel, and PowerPoint), system utilities (e.g., settings configuration and file management), and professional tools (e.g., VS Code and Adobe applications). These tasks capture multi-step GUI interactions that require structured reasoning, interface navigation, and sequential decision-making.
数据来源。VeriGUI 数据集由覆盖 Web 与桌面平台的各类真实 GUI 环境构建。（1）对于 Web 任务，我们特别聚焦涉及信息检索与推理的深度研究场景。因此，我们从公开且权威的来源策划数据，包括政府机构官网、学术机构官网、在线百科、金融数据库和新闻门户。这些任务涵盖五个主要主题领域：科研与学术研究；金融与经济；科技与创新；艺术与娱乐；以及社会政策与可持续发展。该分类确保了主题覆盖的多样性并反映复杂信息检索场景中的真实用户意图。（2）对于桌面任务，我们包含了三类在专业与日常工作流中常用的代表性应用领域：办公生产力软件（如 Word、Excel 和 PowerPoint）、系统工具（如设置配置与文件管理）以及专业工具（如 VS Code 与 Adobe 应用）。这些任务捕捉需要结构化推理、界面导航与顺序决策的多步 GUI 交互。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_19_27_7e9614.jpg"/>



Figure 3: An overview of the proposed VeriGUI framework, consisting of two stages: task instruction construction and human demonstration collection. The framework combines LLM-based generation with human annotation to ensure realistic, high-quality GUI tasks and demonstrations.
图 3：所提出 VeriGUI 框架的概览，包括两个阶段：任务指令构建与人工演示收集。该框架结合了基于大模型的生成与人工注释，以确保现实且高质量的 GUI 任务与演示。


Task Instruction Generation. To generate realistic and executable instructions, we develop a multistage pipeline combining language model generation with human curation, as shown in the left part of Fig. 3. Initially, a small batch of seed instructions is manually selected for each topical domain. These seed instructions, representing high-level user intents, are input to a language model to generate a large number of candidate tasks. Human annotators then review these outputs, selecting only those that are grammatically clear, semantically meaningful, and practically feasible. Once a vetted pool of main tasks is established, the language model is prompted to perform subtask decomposition to obtain complete task instructions, including detailed sub-instructions of each subtask. This process is guided by seed instructions and strict formatting constraints. After generation, each batch of instructions undergoes automated filtering, followed by a second, stricter verification phase involving multiple passes of model-based evaluation. Only those tasks that pass all verification rounds are retained. This procedure enables efficient instruction generation while maintaining the factual correctness, diversity, and task feasibility necessary for GUI datasets.
任务指令生成。为生成现实且可执行的指令，我们开发了结合语言模型生成与人工策划的多阶段流水线，如图 3 左侧所示。最初，为每个主题领域手工选取一小批种子指令。这些代表高级用户意图的种子指令被输入到语言模型以生成大量候选任务。人工注释者随后审查这些输出，仅保留语法清晰、语义合理且实际可行的项。在建立经过审核的主任务池后，提示语言模型进行子任务分解以获得完整任务指令，包括每个子任务的详细子指令。该过程受种子指令和严格格式约束指导。生成后，每批指令经过自动过滤，随后进入更严格的二次验证阶段，涉及多轮基于模型的评估。仅通过所有验证回合的任务被保留。该流程在保持事实正确性、多样性与任务可行性的同时，提高了指令生成的效率，满足 GUI 数据集的需求。


Human Demonstration Collection. Human annotators manually execute each task based on the given final instruction and record the complete trajectory demonstration, as shown in the right part of Fig. 3. Before execution, human annotators refine the subtask sequence to ensure feasibility and smooth operation, allowing adjustments as needed during interaction. Demonstrations are recorded using screen capture tools, with detailed annotations including action logs, observation logs, and subtask-level goals. To ensure high-quality supervision and accurate benchmarking, all trajectory demonstrations undergo strict quality control. This includes both automatic checks and manual review to verify the correctness of subtask outcomes, coherence of action sequences, and integrity of observations. Only demonstrations that meet all criteria are retained. This guarantees that VeriGUI provides reliable and verifiable supervision for long-horizon GUI agents.
人工演示收集。人工注释者根据给定的最终指令手动执行每个任务并记录完整轨迹演示，如图 3 右侧所示。执行前，注释者会优化子任务序列以确保可行性和操作流畅，并在交互过程中根据需要进行调整。演示通过屏幕录制工具记录，并包含详细注释，如操作日志、观察日志和子任务级目标。为确保高质量监督与准确基准测试，所有轨迹演示均经过严格质量控制，包含自动检查与人工复核，以验证子任务结果的正确性、操作序列的一致性和观察记录的完整性。仅满足所有标准的演示被保留。这保证了 VeriGUI 为长时程 GUI 智能体提供可靠且可验证的监督。


### 3.3 DATA STATISTICS
### 3.3 数据统计


To better understand the characteristics of the VeriGUI dataset, Figure 4 and Table 3 present statistical summaries of the collected web task trajectories. These statistics provide insights into the composition and structure of GUI-based tasks collected from a variety of real-world web environments. The domain distribution of task trajectories is shown in Fig. 4a, which demonstrates that the dataset covers a wide range of domains. This ensures broad coverage and diversity across real-world tasks. Each task is decomposed into a sequence of multiple subtasks, where each subtask corresponds to a verifiable goal. Figure $4\mathrm{\;b}$ and $4\mathrm{c}$ show the distribution of the number of subtasks per trajectory, typically ranging from 4 to 8 . This subtask-level structure allows agents to receive intermediate supervision, supporting more fine-grained evaluation and learning. VeriGUI further emphasizes long-chain complexity. Figure 4d illustrates the distribution of GUI action types, capturing a wide range of low-level behaviors such as clicks, scrolls, and drag operations. As shown in Fig. 4e and 4f, many tasks require executing hundreds of steps to reach completion. This highlights the need for agents to reason over long action sequences and handle extended workflows beyond simple UI manipulation. Overall, these statistics demonstrate that VeriGUI tasks are both subtask-verifiable and long-chain, offering a realistic and challenging benchmark for long-horizon planning and interaction in GUI environments.
为更好地理解 VeriGUI 数据集的特征，图 4 和表 3 给出了收集到的网页任务轨迹的统计摘要。这些统计数据揭示了从多种真实网页环境收集到的基于 GUI 任务的组成与结构。任务轨迹的领域分布见图 4a，表明数据集覆盖了广泛的领域，确保了真实任务的广泛性与多样性。每个任务被分解为多个子任务序列，每个子任务对应一个可验证的目标。图 $4\mathrm{\;b}$ 和 $4\mathrm{c}$ 展示了每条轨迹子任务数量的分布，通常在 4 到 8 个之间。这种子任务级结构使得代理能获得中间监督，支持更细粒度的评估与学习。VeriGUI 还强调长链复杂性。图 4d 描绘了 GUI 操作类型的分布，涵盖点击、滚动、拖拽等多种低级行为。正如图 4e 和 4f 所示，许多任务需要执行数百步才能完成，这凸显了代理在长动作序列上推理并处理超出简单界面操作的延展工作流的必要性。总体而言，这些统计表明 VeriGUI 任务既可按子任务验证又具备长链特性，为 GUI 环境中的长期规划与交互提供了现实且具有挑战性的基准。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_19_27_05deab.jpg"/>



Figure 4: The detailed data statistics of the proposed VeriGUI dataset based on 130 collected web task trajectories, with additional data including desktop task trajectories currently in process.
图 4：基于 130 条收集到的网页任务轨迹的 VeriGUI 数据集详细统计，另有包括桌面任务轨迹在内的附加数据正在处理中。


Table 3: The overall data statistics of VeriGUI based on 130 collected web task trajectories.
表 3：基于 130 条收集到的网页任务轨迹的 VeriGUI 总体数据统计。


<table><tr><td>Statistic</td><td>Value</td></tr><tr><td>Total number of tasks</td><td>130</td></tr><tr><td>Total number of subtasks</td><td>587</td></tr><tr><td>Average number of subtasks per task</td><td>4.5</td></tr><tr><td>Total number of steps</td><td>27,873</td></tr><tr><td>Average number of steps per task</td><td>214.4</td></tr><tr><td>Average number of steps per subtask</td><td>49.8</td></tr></table>
<table><tbody><tr><td>统计</td><td>数值</td></tr><tr><td>任务总数</td><td>130</td></tr><tr><td>子任务总数</td><td>587</td></tr><tr><td>每个任务的平均子任务数</td><td>4.5</td></tr><tr><td>步骤总数</td><td>27,873</td></tr><tr><td>每个任务的平均步骤数</td><td>214.4</td></tr><tr><td>每个子任务的平均步骤数</td><td>49.8</td></tr></tbody></table>


## 4 EXPERIMENTS
## 4 实验


To demonstrate the effectiveness of the proposed VeriGUI dataset in evaluating long-horizon reasoning and subtask-level verifiability in GUI tasks, we conduct experiments across a diverse set of agent frameworks and foundation models. The current experiments are based on the currently available 130 web task trajectories. Additional experiments with more comprehensive data, including ongoing desktop task trajectories, will be included in a future version of this work.
为了证明所提出的 VeriGUI 数据集在评估长程推理与子任务级可验证性方面的有效性，我们在多种代理框架和基础模型上进行了实验。当前实验基于现有的 130 条网页任务轨迹。包含更全面数据（包括正在收集的桌面任务轨迹）的附加实验将纳入本工作的未来版本。


### 4.1 EXPERIMENTAL SETTINGS
### 4.1 实验设置


Baselines. For web tasks in VeriGUI, we evaluate different web agents: (1) deep research agents: closed-source agents with built-in search capabilities, including OpenAI Deep Research (OpenAI, 2025) and Gemini Deep Research (Google, 2025). (2) search engine agents: different foundation models combined with an open-source search tool ${}^{2}$ based on the model context protocol ${}^{3}$ . (3) browser-use agents: different foundation models using the Browser-Use framework (Müller & Zunič, 2024). (4) multi-agent system: Camel OWL (Hu et al., 2025) with OpenAI-o3. We use the following foundation models across settings: OpenAI-o3, OpenAI-o4-mini, GPT-4.1, GPT- 40 (Hurst et al., 2024), Gemini-2.5-Pro, Gemini-2.5-Flash (Team et al., 2023), Claude-3.7-Sonnet, Claude-4.0-Sonnet (Anthropic, 2024), DeepSeek-Chat (Liu et al., 2024a), and Qwen-VL-Max (Bai et al., 2025). Note that except for the closed-source deep research agents, the remaining web agents follow two different interaction paradigms: (1) search engine agents use a search tool for retrieval without interacting with webpages and accept only text as input. (2) browser-use agents and OWL agents adopt web element operations as their action space and accept both visual and text inputs.
基线。对于 VeriGUI 中的网页任务，我们评估不同的网页代理：(1) 深度研究代理：具有内置搜索能力的闭源代理，包括 OpenAI Deep Research (OpenAI, 2025) 和 Gemini Deep Research (Google, 2025)。(2) 搜索引擎代理：不同的基础模型结合基于模型上下文协议的开源搜索工具 ${}^{2}$ ${}^{3}$。(3) 浏览器使用代理：不同基础模型使用 Browser-Use 框架 (Müller & Zunič, 2024)。(4) 多代理系统：使用 OpenAI-o3 的 Camel OWL (Hu et al., 2025)。我们在各设置中使用以下基础模型：OpenAI-o3、OpenAI-o4-mini、GPT-4.1、GPT-40 (Hurst et al., 2024)、Gemini-2.5-Pro、Gemini-2.5-Flash (Team et al., 2023)、Claude-3.7-Sonnet、Claude-4.0-Sonnet (Anthropic, 2024)、DeepSeek-Chat (Liu et al., 2024a) 和 Qwen-VL-Max (Bai et al., 2025)。注意，除闭源深度研究代理外，其余网页代理遵循两种不同的交互范式：(1) 搜索引擎代理使用搜索工具进行检索而不与网页交互，仅接受文本作为输入。(2) 浏览器使用代理和 OWL 代理采用网页元素操作作为其动作空间，并同时接受视觉和文本输入。


---



${}^{2}$ https://github.com/searxng/searxng-docker
${}^{2}$ https://github.com/searxng/searxng-docker


${}^{3}$ https://modelcontextprotocol.io/
${}^{3}$ https://modelcontextprotocol.io/


---



Table 4: Comparison of different agents on the VeriGUI benchmark based on 130 web tasks. SR denotes the task success rate, while CR denotes the task completion rate. Bold and underline mean the best and the second-best results in each column within agent type.
表 4：基于 130 条网页任务的 VeriGUI 基准上不同代理的比较。SR 表示任务成功率，CR 表示任务完成率。加粗与下划线表示每列中该代理类型下的最好与第二好结果。


<table><tr><td rowspan="2">Method</td><td colspan="2">Scientific</td><td colspan="2">Finance</td><td colspan="2">Technology</td><td colspan="2">Arts</td><td colspan="2">Social</td><td colspan="2">Average</td></tr><tr><td>SR (%)</td><td></td><td></td><td></td><td></td><td></td><td>SR (%)</td><td>CR (%)</td><td>SR (%)</td><td>CR (%)</td><td>SR (%)</td><td>CR (%)</td></tr><tr><td colspan="13">Deep Research Agent</td></tr><tr><td>OpenAI-o3</td><td>12.5</td><td>31.9</td><td>0.0</td><td>18.7</td><td>10.0</td><td>26.3</td><td>16.1</td><td>43.9</td><td>3.3</td><td>21.7</td><td>8.5</td><td>28.8</td></tr><tr><td>OpenAI-o4-mini</td><td>0.0</td><td>8.1</td><td>0.0</td><td>17.0</td><td>6.7</td><td>20.7</td><td>12.9</td><td>30.6</td><td>3.3</td><td>19.0</td><td>5.4</td><td>20.5</td></tr><tr><td>Gemini-2.5-Flash</td><td>6.2</td><td>19.4</td><td>0.0</td><td>14.3</td><td>3.3</td><td>16.7</td><td>16.1</td><td>41.0</td><td>6.7</td><td>17.7</td><td>6.9</td><td>22.6</td></tr><tr><td>Gemini-2.5-Pro</td><td>18.8</td><td>31.9</td><td>0.0</td><td>22.2</td><td>10.0</td><td>23.7</td><td>16.1</td><td>41.6</td><td>0.0</td><td>21.0</td><td>8.5</td><td>28.1</td></tr><tr><td colspan="13">Search Engine Agent</td></tr><tr><td>GPT-40</td><td>0.0</td><td>3.1</td><td>0.0</td><td>3.0</td><td>3.3</td><td>10.3</td><td>0.0</td><td>3.9</td><td>0.0</td><td>4.3</td><td>0.8</td><td>5.2</td></tr><tr><td>GPT-4.1</td><td>0.0</td><td>13.1</td><td>0.0</td><td>14.8</td><td>3.3</td><td>14.3</td><td>9.7</td><td>23.5</td><td>0.0</td><td>8.0</td><td>3.1</td><td>15.0</td></tr><tr><td>OpenAI-o3</td><td>0.0</td><td>5.0</td><td>0.0</td><td>13.5</td><td>10.0</td><td>19.0</td><td>12.9</td><td>35.2</td><td>0.0</td><td>11.0</td><td>5.4</td><td>18.3</td></tr><tr><td>Gemini-2.5-Flash</td><td>0.0</td><td>5.0</td><td>0.0</td><td>7.4</td><td>0.0</td><td>8.3</td><td>6.5</td><td>28.1</td><td>0.0</td><td>6.7</td><td>1.5</td><td>12.1</td></tr><tr><td>Gemini-2.5-Pro</td><td>0.0</td><td>4.4</td><td>0.0</td><td>8.7</td><td>3.3</td><td>12.0</td><td>12.9</td><td>28.1</td><td>0.0</td><td>7.7</td><td>3.8</td><td>13.3</td></tr><tr><td>Claude-3.7-Sonnet</td><td>0.0</td><td>8.1</td><td>0.0</td><td>10.9</td><td>13.3</td><td>23.7</td><td>9.7</td><td>30.0</td><td>0.0</td><td>8.0</td><td>5.4</td><td>17.4</td></tr><tr><td>Claude-4.0-Sonnet</td><td>0.0</td><td>11.9</td><td>0.0</td><td>11.3</td><td>6.7</td><td>13.7</td><td>12.9</td><td>21.9</td><td>0.0</td><td>11.0</td><td>4.6</td><td>14.4</td></tr><tr><td>Deepseek-Chat</td><td>0.0</td><td>4.4</td><td>0.0</td><td>2.2</td><td>3.3</td><td>10.7</td><td>12.9</td><td>24.8</td><td>0.0</td><td>4.7</td><td>3.8</td><td>10.4</td></tr><tr><td colspan="13">Browser-Use Agent</td></tr><tr><td>GPT-40</td><td>0.0</td><td>1.9</td><td>0.0</td><td>1.7</td><td>3.3</td><td>8.3</td><td>3.2</td><td>13.5</td><td>0.0</td><td>5.7</td><td>1.5</td><td>7.0</td></tr><tr><td>GPT-4.1</td><td>0.0</td><td>3.8</td><td>0.0</td><td>7.0</td><td>3.3</td><td>9.0</td><td>16.1</td><td>29.7</td><td>0.0</td><td>9.7</td><td>4.6</td><td>13.1</td></tr><tr><td>OpenAI-03</td><td>6.2</td><td>20.6</td><td>0.0</td><td>11.3</td><td>0.0</td><td>18.7</td><td>16.1</td><td>33.5</td><td>0.0</td><td>12.3</td><td>4.6</td><td>19.7</td></tr><tr><td>Gemini-2.5-Flash</td><td>0.0</td><td>1.9</td><td>0.0</td><td>6.1</td><td>0.0</td><td>2.0</td><td>0.0</td><td>19.7</td><td>0.0</td><td>7.3</td><td>0.0</td><td>8.2</td></tr><tr><td>Gemini-2.5-Pro</td><td>6.2</td><td>10.6</td><td>0.0</td><td>6.1</td><td>6.7</td><td>9.7</td><td>12.9</td><td>36.1</td><td>0.0</td><td>10.0</td><td>5.4</td><td>15.5</td></tr><tr><td>Claude-3.7-Sonnet</td><td>0.0</td><td>7.5</td><td>0.0</td><td>9.6</td><td>0.0</td><td>15.3</td><td>16.1</td><td>36.8</td><td>0.0</td><td>10.3</td><td>3.8</td><td>17.3</td></tr><tr><td>Claude-4.0-Sonnet</td><td>6.2</td><td>13.8</td><td>0.0</td><td>6.5</td><td>0.0</td><td>11.3</td><td>19.4</td><td>45.8</td><td>3.3</td><td>9.3</td><td>6.2</td><td>18.5</td></tr><tr><td>Owen-VL-Max</td><td>0.0</td><td>2.5</td><td>0.0</td><td>0.9</td><td>0.0</td><td>3.0</td><td>6.5</td><td>11.6</td><td>0.0</td><td>4.3</td><td>1.5</td><td>4.9</td></tr><tr><td colspan="13">Multi-Agent System</td></tr><tr><td>OWL with OpenAI-o3</td><td>6.2</td><td>18.8</td><td>0.0</td><td>6.5</td><td>3.3</td><td>11.3</td><td>16.1</td><td>32.3</td><td>6.7</td><td>16.3</td><td>6.9</td><td>17.5</td></tr></table>
<table><tbody><tr><td rowspan="2">方法</td><td colspan="2">科学</td><td colspan="2">金融</td><td colspan="2">技术</td><td colspan="2">艺术</td><td colspan="2">社会</td><td colspan="2">平均</td></tr><tr><td>成功率 (%)</td><td></td><td></td><td></td><td></td><td></td><td>成功率 (%)</td><td>覆盖率 (%)</td><td>成功率 (%)</td><td>覆盖率 (%)</td><td>成功率 (%)</td><td>覆盖率 (%)</td></tr><tr><td colspan="13">深度研究代理</td></tr><tr><td>OpenAI-o3</td><td>12.5</td><td>31.9</td><td>0.0</td><td>18.7</td><td>10.0</td><td>26.3</td><td>16.1</td><td>43.9</td><td>3.3</td><td>21.7</td><td>8.5</td><td>28.8</td></tr><tr><td>OpenAI-o4-mini</td><td>0.0</td><td>8.1</td><td>0.0</td><td>17.0</td><td>6.7</td><td>20.7</td><td>12.9</td><td>30.6</td><td>3.3</td><td>19.0</td><td>5.4</td><td>20.5</td></tr><tr><td>Gemini-2.5-Flash</td><td>6.2</td><td>19.4</td><td>0.0</td><td>14.3</td><td>3.3</td><td>16.7</td><td>16.1</td><td>41.0</td><td>6.7</td><td>17.7</td><td>6.9</td><td>22.6</td></tr><tr><td>Gemini-2.5-Pro</td><td>18.8</td><td>31.9</td><td>0.0</td><td>22.2</td><td>10.0</td><td>23.7</td><td>16.1</td><td>41.6</td><td>0.0</td><td>21.0</td><td>8.5</td><td>28.1</td></tr><tr><td colspan="13">搜索引擎代理</td></tr><tr><td>GPT-40</td><td>0.0</td><td>3.1</td><td>0.0</td><td>3.0</td><td>3.3</td><td>10.3</td><td>0.0</td><td>3.9</td><td>0.0</td><td>4.3</td><td>0.8</td><td>5.2</td></tr><tr><td>GPT-4.1</td><td>0.0</td><td>13.1</td><td>0.0</td><td>14.8</td><td>3.3</td><td>14.3</td><td>9.7</td><td>23.5</td><td>0.0</td><td>8.0</td><td>3.1</td><td>15.0</td></tr><tr><td>OpenAI-o3</td><td>0.0</td><td>5.0</td><td>0.0</td><td>13.5</td><td>10.0</td><td>19.0</td><td>12.9</td><td>35.2</td><td>0.0</td><td>11.0</td><td>5.4</td><td>18.3</td></tr><tr><td>Gemini-2.5-Flash</td><td>0.0</td><td>5.0</td><td>0.0</td><td>7.4</td><td>0.0</td><td>8.3</td><td>6.5</td><td>28.1</td><td>0.0</td><td>6.7</td><td>1.5</td><td>12.1</td></tr><tr><td>Gemini-2.5-Pro</td><td>0.0</td><td>4.4</td><td>0.0</td><td>8.7</td><td>3.3</td><td>12.0</td><td>12.9</td><td>28.1</td><td>0.0</td><td>7.7</td><td>3.8</td><td>13.3</td></tr><tr><td>Claude-3.7-Sonnet</td><td>0.0</td><td>8.1</td><td>0.0</td><td>10.9</td><td>13.3</td><td>23.7</td><td>9.7</td><td>30.0</td><td>0.0</td><td>8.0</td><td>5.4</td><td>17.4</td></tr><tr><td>Claude-4.0-Sonnet</td><td>0.0</td><td>11.9</td><td>0.0</td><td>11.3</td><td>6.7</td><td>13.7</td><td>12.9</td><td>21.9</td><td>0.0</td><td>11.0</td><td>4.6</td><td>14.4</td></tr><tr><td>Deepseek-Chat</td><td>0.0</td><td>4.4</td><td>0.0</td><td>2.2</td><td>3.3</td><td>10.7</td><td>12.9</td><td>24.8</td><td>0.0</td><td>4.7</td><td>3.8</td><td>10.4</td></tr><tr><td colspan="13">浏览器使用代理</td></tr><tr><td>GPT-40</td><td>0.0</td><td>1.9</td><td>0.0</td><td>1.7</td><td>3.3</td><td>8.3</td><td>3.2</td><td>13.5</td><td>0.0</td><td>5.7</td><td>1.5</td><td>7.0</td></tr><tr><td>GPT-4.1</td><td>0.0</td><td>3.8</td><td>0.0</td><td>7.0</td><td>3.3</td><td>9.0</td><td>16.1</td><td>29.7</td><td>0.0</td><td>9.7</td><td>4.6</td><td>13.1</td></tr><tr><td>OpenAI-03</td><td>6.2</td><td>20.6</td><td>0.0</td><td>11.3</td><td>0.0</td><td>18.7</td><td>16.1</td><td>33.5</td><td>0.0</td><td>12.3</td><td>4.6</td><td>19.7</td></tr><tr><td>Gemini-2.5-Flash</td><td>0.0</td><td>1.9</td><td>0.0</td><td>6.1</td><td>0.0</td><td>2.0</td><td>0.0</td><td>19.7</td><td>0.0</td><td>7.3</td><td>0.0</td><td>8.2</td></tr><tr><td>Gemini-2.5-Pro</td><td>6.2</td><td>10.6</td><td>0.0</td><td>6.1</td><td>6.7</td><td>9.7</td><td>12.9</td><td>36.1</td><td>0.0</td><td>10.0</td><td>5.4</td><td>15.5</td></tr><tr><td>Claude-3.7-Sonnet</td><td>0.0</td><td>7.5</td><td>0.0</td><td>9.6</td><td>0.0</td><td>15.3</td><td>16.1</td><td>36.8</td><td>0.0</td><td>10.3</td><td>3.8</td><td>17.3</td></tr><tr><td>Claude-4.0-Sonnet</td><td>6.2</td><td>13.8</td><td>0.0</td><td>6.5</td><td>0.0</td><td>11.3</td><td>19.4</td><td>45.8</td><td>3.3</td><td>9.3</td><td>6.2</td><td>18.5</td></tr><tr><td>Owen-VL-Max</td><td>0.0</td><td>2.5</td><td>0.0</td><td>0.9</td><td>0.0</td><td>3.0</td><td>6.5</td><td>11.6</td><td>0.0</td><td>4.3</td><td>1.5</td><td>4.9</td></tr><tr><td colspan="13">多代理系统</td></tr><tr><td>OWL 与 OpenAI-o3</td><td>6.2</td><td>18.8</td><td>0.0</td><td>6.5</td><td>3.3</td><td>11.3</td><td>16.1</td><td>32.3</td><td>6.7</td><td>16.3</td><td>6.9</td><td>17.5</td></tr></tbody></table>


Evaluation Metrics. To comprehensively evaluate agent performance, we consider three complementary metrics4. (1) The task Success Rate (SR) measures whether the agent achieves the overall task goal. (2) The task Completion Rate (CR) measures the extent to which the agent achieves the overall task goal. Since our tasks often involve multiple subtasks, CR estimates the completion level by calculating the proportion of correct elements in the output. For example, if the expected result contains ten keywords and the agent outputs one correctly, the CR is 10%. We introduce this metric because the tasks are highly challenging, and using only SR makes it difficult to distinguish the performance differences between agents. (3) The Action Efficiency $\left( {AE}\right)$ quantifies the planning effectiveness of agents by measuring the number of steps required to arrive at the final answer. Note that AE is only defined for tasks that are successfully completed. Moreover, AE is not directly comparable across different interaction paradigms due to their inherently distinct action spaces. For both the SR and the CR, we report the LLM-as-a-Judge score (Gu et al., 2024). Specifically, we utilize GPT-4.1 as the judge to semantically evaluate the correctness of the agents' final answers. Without further clarification, each experiment is conducted once to obtain the final results. Detailed prompts are provided in Appendix B.
评估指标。为全面评估智能体的性能，我们考虑了三个互补的指标4。（1）任务成功率（SR）衡量智能体是否达成了整体任务目标。（2）任务完成率（CR）衡量智能体达成整体任务目标的程度。由于我们的任务通常包含多个子任务，CR 通过计算输出中正确元素的比例来估算完成水平。例如，如果预期结果包含十个关键词，而智能体正确输出了一个，那么 CR 就是 10%。我们引入这个指标是因为任务极具挑战性，仅使用 SR 难以区分不同智能体的性能差异。（3）行动效率 $\left( {AE}\right)$ 通过衡量得出最终答案所需的步骤数来量化智能体的规划有效性。请注意，AE 仅针对成功完成的任务定义。此外，由于不同交互范式的动作空间本质上不同，AE 无法在不同交互范式之间直接比较。对于 SR 和 CR，我们报告了以大语言模型作为评判者的得分（Gu 等人，2024）。具体来说，我们使用 GPT - 4.1 作为评判者，从语义上评估智能体最终答案的正确性。除非另有说明，每个实验均进行一次以获得最终结果。详细的提示信息见附录 B。


---



${}^{4}$ We further introduce a metric that treats a subtask as a valid starting point,allowing agent evaluation at different task stages. Specifically,the task Success Rate under $k$ -subtask oracle $\left( {{SR}@k}\right)$ indicates whether the agent can achieve the overall task goal when provided with the goal outcomes of the first $k$ subtasks. Experiments on this will be presented in a future version.
${}^{4}$ 我们进一步引入一项指标，将子任务视为有效的起点，允许在不同任务阶段对智能体进行评估。具体而言，基于 $k$ 子任务预言机 $\left( {{SR}@k}\right)$ 下的任务成功率表示在提供前 $k$ 个子任务的目标结果时，智能体是否能实现整体任务目标。相关实验将在后续版本中呈现。


---



### 4.2 MAIN RESULTS
### 4.2 主要结果


Table 4 summarizes the performance of all evaluated agents on the VeriGUI benchmark across five web domains, measuring both task success rate and completion rate. Notably, across all agent types and foundation models, no configuration achieves an average success rate above 10% or a completion rate above ${30}\%$ . This consistently low performance highlights the challenging nature of the VeriGUI tasks, which require long-horizon planning, multi-step reasoning, and complex decision-making under diverse web scenarios. We analyze the results from three perspectives: foundation model capability, interaction paradigm, and domain-specific behavior.
表 4 总结了在五个网页域上所有评估智能体在 VeriGUI 基准上的表现，衡量了任务成功率和完成率。值得注意的是，在所有智能体类型和基础模型中，没有任何配置的平均成功率超过 10%，或完成率超过 ${30}\%$ 。这种持续偏低的表现凸显了 VeriGUI 任务的挑战性，这些任务需要长时程规划、多步推理以及在多样化网页场景下的复杂决策。我们从三方面分析结果：基础模型能力、交互范式和域特定行为。


Foundation Model Comparison. We observe notable differences in agent performance across foundation models. Within the deep research agent setting, OpenAI-o3 and Gemini-2.5-Pro achieve the highest average SRs at ${8.5}\%$ ,with CRs of 28.8% and 28.1%,respectively. These results suggest that both models possess relatively stronger reasoning capabilities and better generalization across tasks. In contrast, OpenAI-o4-mini performs worst in this setting, indicating limitations in handling complex web tasks despite being a reasoning model. In the search engine and browser-use settings, where most models are shared, we observe similar model-wise trends. OpenAI-o3, Claude-3.7- Sonnet, and Claude-4.0-Sonnet demonstrate stronger completion rates across both settings. GPT- 40 shows consistently low SRs (0.8-1.5%) and CRs (5.2-7.0%) across both settings, indicating limitations in handling complex multi-step tasks. Although GPT-4.1 performs slightly better, it still lags behind Claude and Gemini. Besides, Deepseek-Chat and Qwen-VL-Max also show weaker performance. These results suggest that foundation model differences continue to play a key role in determining agent effectiveness.
基础模型比较。我们观察到不同基础模型间的智能体表现存在显著差异。在深度研究智能体设置中，OpenAI-o3 与 Gemini-2.5-Pro 达到最高的平均 SR，为 ${8.5}\%$ ，其 CR 分别为 28.8% 和 28.1%。这些结果表明两者具有较强的推理能力和更好的任务泛化能力。相比之下，OpenAI-o4-mini 在此设置中表现最差，表明其在处理复杂网页任务时存在局限，尽管其为推理模型。在搜索引擎和浏览器使用设置中（大多数模型相同），我们观察到类似的模型间趋势。OpenAI-o3、Claude-3.7-Sonnet 与 Claude-4.0-Sonnet 在两种设置中均表现出较强的完成率。GPT-40 在两种设置中持续表现出较低的 SR（0.8–1.5%）和 CR（5.2–7.0%），显示在处理复杂多步任务时的局限性。尽管 GPT-4.1 表现略好，但仍落后于 Claude 和 Gemini。此外，Deepseek-Chat 与 Qwen-VL-Max 的表现也较弱。这些结果表明，基础模型差异仍然是决定智能体有效性的关键因素。


Impact of Interaction Paradigms. The design of the interaction paradigm has a substantial impact on agent performance. Agents using the search engine paradigm achieve the weakest results across both SR and CR metrics. Most models under this setting have average SRs between 0.8-5.4% and CRs below 18.3%. This is likely due to their reliance on passive text-based retrieval without the ability to interact directly with web page structures. In comparison, agents using the browser-use paradigm generally obtain slightly higher scores. While the improvements in SR are often modest, the average CR is higher for several models. For instance, Claude-4.0-Sonnet improves from 14.4% CR in the search engine setting to 18.5% in the browser setting, and Gemini-2.5-Pro improves from 13.3% to 15.5%. These gains suggest that having access to page-level structure and the ability to simulate user actions can provide meaningful advantages, especially for tasks involving dynamic interfaces or multiple steps. The deep research agent setting achieves the highest completion rates overall, with top models reaching over 28% average CR. Although their interaction mechanisms are less transparent, the results suggest that strong built-in retrieval, summarization, or planning capabilities may contribute to their relative success. The multi-agent system also performs competitively, achieving an SR of 6.9% with OWL and OpenAI-o3, indicating that orchestrated agent collaboration can be beneficial in certain task types.
交互范式的影响。交互范式的设计对代理性能有重大影响。使用搜索引擎范式的代理在 SR 和 CR 指标上表现最弱。在此设置下，大多数模型的平均 SR 在 0.8-5.4% 之间，CR 低于 18.3%。这可能是因为它们依赖被动的基于文本的检索，无法直接与网页结构交互。相比之下，使用浏览器操作范式的代理通常得分略高。虽然 SR 的提升往往有限，但若干模型的平均 CR 更高。例如，Claude-4.0-Sonnet 在搜索引擎设置下的 CR 为 14.4%，在浏览器设置下提升至 18.5%；Gemini-2.5-Pro 则从 13.3% 提升至 15.5%。这些提升表明，访问页面级结构并模拟用户操作的能力可以带来实质性优势，尤其在涉及动态界面或多步骤的任务中。深度研究代理设置总体上取得最高的完成率，顶尖模型的平均 CR 超过 28%。尽管其交互机制不太透明，结果表明强大的内建检索、摘要或规划能力可能有助于其相对成功。多代理系统也具有竞争力，使用 OWL 和 OpenAI-o3 时达成了 6.9% 的 SR，表明在某些任务类型中，协同代理的编排可能有益。


Performance Across Domains. We also analyze performance across the five domains in the VeriGUI to explore how content type influences agent effectiveness. Tasks in arts and entertainment generally achieved the highest success and completion rates, likely due to more structured and predictable data formats such as lists or summaries. For example, the browser-use agent with Claude-4.0-Sonnet reaches 19.4% SR and 45.8% CR in this domain. In contrast, domains like finance and economics and social policy and sustainability proved more challenging, often requiring the agent to extract fragmented, abstract information from less standardized content. Most models show SRs near $0\%$ and CRs below ${20}\%$ in these domains. The scientific and academic research and technology and innovation domains showed intermediate difficulty, frequently involving dense technical descriptions or multi-attribute reasoning. These trends suggest that the complexity of information presentation is the key factor influencing agent success in web-based GUI tasks.
各领域表现。我们还分析了 VeriGUI 中五个领域的表现，以探究内容类型如何影响代理效能。艺术与娱乐领域的任务通常取得最高的成功率和完成率，可能因为该类数据格式更结构化且可预测，如列表或摘要。例如，使用 Claude-4.0-Sonnet 的浏览器代理在此领域达到 19.4% 的 SR 和 45.8% 的 CR。相比之下，金融与经济以及社会政策与可持续性等领域更具挑战性，常需代理从更不规范的内容中提取零散、抽象的信息。在这些领域，大多数模型的 SR 接近 $0\%$，CR 低于 ${20}\%$。科学与学术研究以及技术与创新领域呈现中等难度，经常包含密集的技术描述或多属性推理。这些趋势表明，信息呈现的复杂性是影响代理在基于网页 GUI 任务中成功的关键因素。


### 4.3 ANALYSIS
### 4.3 分析


Analysis of Task Difficulty. To better understand the intrinsic difficulty of tasks in the VeriGUI-Web benchmark, we conduct a fine-grained statistical analysis of SR and CR distributions across all tasks, comparing results from different agent frameworks. The distribution curves in Fig. 5 reveal that for both agent types, the majority of tasks yield low SR and CR values, with a long tail of near-zero success, underscoring the challenge posed by VeriGUI's multi-step reasoning requirements.
任务难度分析。为更好地理解 VeriGUI-Web 基准中任务的内在难度，我们对所有任务的 SR 和 CR 分布进行了细粒度统计分析，并比较了不同代理框架的结果。图 5 的分布曲线显示，对于两种代理类型，大多数任务的 SR 和 CR 值较低，并存在一个接近零成功率的长尾，强调了 VeriGUI 多步骤推理要求所带来的挑战。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_19_27_2a4a88.jpg"/>



Figure 5: Distribution of task success rate (SR) and completion rate (CR) across 130 web tasks.
图 5：130 个网页任务的任务成功率（SR）和完成率（CR）分布。


To systematically categorize task difficulty, we define five levels based on the average SR and CR across all models and agents: (1) Level 1 includes tasks with SR above $0\%$ ,indicating they are relatively tractable for current agents. (2) Level 2 includes tasks with zero SR but CR above 20%. (3) Level 3 includes tasks with zero SR but CR between 5% and 20%. (4) Level 4 includes tasks with zero SR but CR between 0% and 5%. (5) Level 5 includes tasks where both SR and CR are zero, indicating no model was able to make progress. The results in Fig. 6 show that the majority of VeriGUI tasks fall into Level 2-5 with zero SR, highlighting the prevalence of high-complexity, partially achievable tasks. Only a small fraction of tasks fall into Level 1, indicating that few tasks are straightforward for current agents. This categorization provides a practical framework for future benchmarking and curriculum design in GUI agent training.
为系统地对任务难度进行分类，我们基于所有模型与代理的平均 SR 和 CR 定义了五个等级：（1）等级 1 包含 SR 高于 $0\%$ 的任务，表明当前代理相对可解；（2）等级 2 包含 SR 为零但 CR 高于 20% 的任务；（3）等级 3 包含 SR 为零但 CR 在 5% 至 20% 之间的任务；（4）等级 4 包含 SR 为零但 CR 在 0% 至 5% 之间的任务；（5）等级 5 包含 SR 和 CR 均为零的任务，表明没有模型能够取得进展。图 6 的结果显示，大多数 VeriGUI 任务属于等级 2-5 且 SR 为零，凸显出高复杂度且部分可达成任务的普遍性。仅有少部分任务属于等级 1，表明当前代理能轻松解决的任务很少。该分类为未来的基准测试和 GUI 代理训练的课程设计提供了实用框架。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_19_27_ff95d9.jpg"/>



Figure 6: Task Difficulty Level.
图 6：任务难度等级。


Analysis of Action Efficiency. The analysis of action efficiency reveals clear differences in the planning capabilities of browser-use agents powered by various foundation models. Notably, in the browser-use setting, agents operate over web elements as their action space rather than GUI operations, making their action counts not directly comparable to those in human demonstrations. As shown in Tab. 5, models such as GPT-4.1 and Claude-3.7-Sonnet generally required more actions, suggesting a more exploratory or cautious execution style. In contrast, models like OpenAI-40 and Claude-4.0-Sonnet completed tasks with fewer steps, indicating more direct strategies. However, lower action counts did not always align with better outcomes, as some models exhibited brittle reasoning despite efficient execution. Conversely, higher action counts sometimes reflected more thorough exploration, particularly in tasks with complex or ambiguous goals. These results suggest that while action efficiency provides insight into planning behavior, it must be considered alongside success rate to fully assess agent performance.
行动效率分析。对行动效率的分析揭示了由不同基础模型驱动的浏览器使用代理在规划能力上的明显差异。值得注意的是，在浏览器使用场景中，代理以网页元素作为其动作空间而非 GUI 操作，因此其动作计数不能与人类示范直接比较。如表 5 所示，诸如 GPT-4.1 和 Claude-3.7-Sonnet 的模型通常需要更多动作，表明其执行风格更具探索性或谨慎性。相反，OpenAI-40 和 Claude-4.0-Sonnet 等模型以更少步骤完成任务，显示出更直接的策略。然而，更少的动作并不总是意味着更好的结果，因为一些模型尽管执行高效却推理脆弱。相反，更多的动作有时反映了更彻底的探索，尤其是在目标复杂或模糊的任务中。这些结果表明，尽管行动效率能提供关于规划行为的洞见，但必须与成功率结合考虑以全面评估代理性能。


Table 5: Comparison of the average action efficiency for browser-use agents with different foundation Models on the VeriGUI benchmark.
表 5：在 VeriGUI 基准上，不同基础模型的浏览器使用代理平均行动效率比较。


<table><tr><td>Method</td><td>Average AE</td></tr><tr><td colspan="2">Browser-Use Agent</td></tr><tr><td>OpenAI-03</td><td>29.7</td></tr><tr><td>GPT-40</td><td>22.8</td></tr><tr><td>GPT-4.1</td><td>36.0</td></tr><tr><td>Gemini-2.5-Pro</td><td>41.2</td></tr><tr><td>Gemini-2.5-Flash</td><td>65.7</td></tr><tr><td>Claude-3.7-Sonnet</td><td>35.7</td></tr><tr><td>Claude-4.0-Sonnet</td><td>24.7</td></tr><tr><td>Qwen-VL-Max</td><td>29.8</td></tr></table>
<table><tbody><tr><td>方法</td><td>平均 AE</td></tr><tr><td colspan="2">浏览器使用代理</td></tr><tr><td>OpenAI-03</td><td>29.7</td></tr><tr><td>GPT-40</td><td>22.8</td></tr><tr><td>GPT-4.1</td><td>36.0</td></tr><tr><td>Gemini-2.5-Pro</td><td>41.2</td></tr><tr><td>Gemini-2.5-Flash</td><td>65.7</td></tr><tr><td>Claude-3.7-Sonnet</td><td>35.7</td></tr><tr><td>Claude-4.0-Sonnet</td><td>24.7</td></tr><tr><td>Qwen-VL-Max</td><td>29.8</td></tr></tbody></table>


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_19_27_86785e.jpg"/>



Figure 7: Case studies of agent performance on two web tasks in VeriGUI.
图7：VeriGUI 中两个网页任务上代理性能的案例研究。


### 4.4 CASE STUDIES
### 4.4 案例研究


To better understand the behaviors and limitations of different agent types in long-horizon GUI reasoning tasks, we present two representative cases from the VeriGUI benchmark. These examples illustrate retrieval fidelity, multi-step reasoning quality, and typical failure modes across four defined error types: Misinformation, Incomplete Result, Retrieval Failure, and Irrelevant Result.
为更好地理解不同类型代理在长时程 GUI 推理任务中的行为与局限，我们展示了 VeriGUI 基准中的两个代表性案例。这些示例说明了检索忠实度、多步推理质量及四类典型失败模式：错误信息、结果不完整、检索失败和无关结果。


For Task 1 in the left part of Fig. 7, the agent must identify the global streaming service with the highest subscriber growth and provide detailed metadata about its highest-budget original series. The Deep Research Agent (OpenAI-o3) achieves a relatively high Completion Rate, correctly identifying Netflix, Stranger Things, and most relevant metadata. However, it exhibits two key errors. First, it commits misinformation by reporting an approximate subscriber growth of 39 million instead of the exact 38.64 million, due to being misled by media reports and mistakenly recording Q4 2023 as 260.8 million instead of the official 260.28 million. Second, it demonstrates an incomplete result by mentioning only one VFX company, while omitting six others that contributed significantly.
在图7 左侧的任务1中，代理需识别订阅用户增长最多的全球流媒体服务，并提供其最高预算原创剧集的详细元数据。深度研究代理（OpenAI-o3）达成率较高，正确识别出 Netflix、《怪奇物语》及大部分相关元数据。但它表现出两处关键错误。首先，它提供了错误信息，报告的订阅增长约为 3900 万而非精确的 3864 万，原因是被媒体报道误导并错误记录了 2023 年第四季度为 260.8 万而非官方的 260.28 万。其次，它呈现出结果不完整，仅提到了一家视觉特效公司，却遗漏了另外六家也有重大贡献的公司。


For Task 2 in the right part of Fig. 7, the agent is asked to identify the first city to implement a congestion charge and extract key policy details. The Browser-Use Agent (GPT-40) correctly identifies Singapore and the implementation year 1975, but fails in other aspects. It encounters a retrieval failure by not providing any specific value for the congestion charge, instead returning vague descriptions. Additionally, it provides an irrelevant result by discussing average traffic speeds rather than reporting the required percentage reduction in traffic during the first year. These issues suggest that although browser-based agents can navigate webpages, they still struggle with precise data extraction and generating structured, goal-oriented output, leading to a lower completion rate.
在图7 右侧的任务2中，代理被要求识别第一个实施拥堵收费的城市并提取关键政策细节。浏览器使用代理（GPT-40）正确识别出新加坡及实施年份 1975，但在其他方面失败。它出现了检索失败，未提供任何具体的收费数值，而是返回了模糊描述。此外，它给出了无关结果，讨论了平均车速而非报告第一年交通量所需的百分比减少。这些问题表明，尽管基于浏览器的代理可以在网页上导航，但在精确数据提取和生成结构化、目标导向输出方面仍存在困难，导致较低的完成率。


Beyond individual examples, our experiments reveal several systemic limitations. First, many chat-based agents demonstrate shallow search behavior: they invoke tools only a few times before prematurely terminating the output, even when the task clearly requires deeper investigation. This limits their ability to perform comprehensive, multi-hop retrieval in complex GUI environments. Second, browser agents often formulate web queries using full natural language sentences instead of distilled keywords. While sentence-level inputs may appear more natural, they frequently lead to suboptimal search results, reducing the likelihood of retrieving exact information needed for task completion.
除个别示例外，我们的实验还揭示了若干系统性局限。首先，许多基于对话的代理表现出浅层搜索行为：在明显需要更深层次调查的任务中，它们仅调用工具几次便过早结束输出。这限制了它们在复杂 GUI 环境中执行全面、多跳检索的能力。其次，浏览器代理常用完整自然语言句子而非提炼后的关键词来构造网页查询。尽管句子级输入看似更自然，但它们经常导致次优的搜索结果，降低检索到完成任务所需精确信息的可能性。


## 5 DISCUSSION
## 5 讨论


The current experimental results in VeriGUI are based on a limited subset of 130 web tasks, most of which focus on information-seeking scenarios. Interestingly, we observe that deep research agents generally outperform browser-use agents in this setting. This raises an important question: should we prioritize the development of deep research agents, or does the GUI agent paradigm still hold promise for broader and more powerful generalist capabilities? We believe the latter remains highly compelling, and this observation should be interpreted from several perspectives.
目前 VeriGUI 的实验结果基于 130 个网页任务的有限子集，其中大部分聚焦信息检索场景。有趣的是，我们观察到深度研究代理在此设置下一般优于浏览器使用代理。这提出了一个重要问题：我们应优先发展深度研究代理，还是 GUI 代理范式在更广泛、更强大的通用能力方面仍有前景？我们认为后者仍然非常有吸引力，这一观察应从多个角度来解读。


GUI Agents Excel in Interactive Tasks. The nature of the tasks strongly influences performance. Most of the current web tasks in VeriGUI emphasize multi-hop information retrieval and factual synthesis, which align closely with the strengths of deep research agents. However, for many practical tasks involving interface manipulation, such as uploading files and logging into accounts, deep research agents are fundamentally limited. These agents lack the ability to interact with the visual layout of interfaces, which is essential for completing such tasks. In contrast, GUI agents are built to operate over both the visual and structural components of the environment, enabling them to tackle interactive workflows that go beyond passive information extraction. Thus, future versions of VeriGUI will include a broader set of web tasks that emphasize GUI interaction.
GUI 代理在交互性任务中表现出色。任务的性质强烈影响表现。VeriGUI 中当前的大多数网页任务强调多跳信息检索和事实综合，这与深度研究代理的优势高度一致。然而，对于许多涉及界面操作的实际任务（如上传文件和登录账户），深度研究代理从根本上受限：它们缺乏与界面视觉布局交互的能力，而这对完成此类任务至关重要。相比之下，GUI 代理旨在同时操作环境的视觉和结构组件，使其能够处理超越被动信息提取的交互式工作流。因此，未来版本的 VeriGUI 将包含更多强调 GUI 交互的网页任务。


GUI Agent Performance is Underestimated. Most existing browser-based GUI agents rely on general-purpose multimodal models and relatively basic execution frameworks. They have not yet benefited from the same degree of domain-specific optimization or tool integration that powers deep research systems. As the field progresses, we expect that advances in environment modeling, long-horizon planning, multimodal understanding, and training with fine-grained subtask supervision as provided in VeriGUI will significantly improve the reasoning, robustness, and decision-making capabilities of GUI agents. The performance gap we see today should not be seen as a fundamental limitation, but rather as a reflection of the early stages of a promising technology.
GUI 代理的性能被低估。现有大多数基于浏览器的 GUI 代理依赖通用多模态模型与相对基础的执行框架，尚未受益于推动深度研究系统的那种领域特定优化或工具集成。随着该领域的发展，我们预计环境建模、长时程规划、多模态理解以及像 VeriGUI 提供的细粒度子任务监督训练方面的进步将显著提升 GUI 代理的推理、鲁棒性与决策能力。我们今天看到的性能差距不应被视为根本性限制，而应被看作是这一有前途技术处于早期阶段的反映。


GUI Agents may Offer a Path Toward Generalist Agents. One of the most exciting prospects for GUI agents is their potential to serve as a foundational tool in the development of more generalist AI systems. While deep research agents are currently focused on web-based tasks, GUI agents have the inherent ability to generalize across multiple computing environments, including web and desktop platforms. Their ability to interact with graphical interfaces makes them versatile, capable of performing tasks such as browsing, document editing, system configuration, and data entry, all without the need for domain-specific rules or pipelines. This extensibility and flexibility provide a promising path towards building generalist models that can seamlessly navigate and execute tasks across diverse digital environments. By offering a unified approach to task execution, GUI agents may become a critical enabler for the development of truly general-purpose interactive agents.
GUI 代理可能为通用代理铺路。GUI 代理最令人振奋的前景之一是其作为开发更通用 AI 系统的基础工具的潜力。尽管深度研究代理目前专注于基于网络的任务，GUI 代理天生能够在多种计算环境（包括网页和桌面平台）间泛化。它们与图形界面的交互能力使其用途广泛，能够执行浏览、文档编辑、系统配置和数据录入等任务，而无需领域特定的规则或流程。这样的可扩展性和灵活性为构建能在多样数字环境中无缝导航和执行任务的通用模型提供了有希望的路径。通过提供统一的任务执行方法，GUI 代理可能成为开发真正通用交互代理的关键推动者。


It is important to note that the current evaluation only reflects a portion of what VeriGUI aims to capture. We are actively expanding the dataset to include more web tasks with interactive requirements, as well as a significant number of desktop tasks involving complex software operations. Future experiments on this expanded data will enable a more balanced and complete understanding of GUI agent capabilities across task types and environments.
需要注意的是，目前的评估仅反映了 VeriGUI 旨在捕捉内容的一部分。我们正积极扩展数据集，以包含更多具有交互要求的网页任务，以及大量涉及复杂软件操作的桌面任务。对这些扩展数据的未来实验将有助于更平衡、更完整地理解 GUI 代理在不同任务类型和环境中的能力。


## 6 CONCLUSION
## 6 结论


In this work, we introduce VeriGUI, a large-scale, human-annotated dataset designed to address the growing need for verifiable, long-horizon benchmarks in GUI agent research. Unlike prior datasets that focus on short-term interactions and outcome-only validation, VeriGUI emphasizes long-chain complexity and subtask-level verifiability, supporting the development and evaluation of agent capabilities in real-world GUI workflows. Our comprehensive experiments across a range of leading agent models highlight persistent challenges in long-horizon task decomposition and execution, underscoring the importance of datasets like VeriGUI in pushing the frontier of generalist agent intelligence. We have open-sourced the dataset and will continue to update it. We hope VeriGUI serves as a valuable resource for the community, fostering further research into GUI agents.
在本工作中，我们介绍了 VeriGUI——一个大规模、人工标注的数据集，旨在应对 GUI 代理研究中对可验证、长时程基准日益增长的需求。不同于以往关注短期交互和仅基于结果验证的数据集，VeriGUI 强调长链复杂性和子任务级可验证性，支持在真实 GUI 工作流中开发和评估代理能力。我们对一系列领先代理模型进行的全面实验凸显了在长时程任务分解和执行中的持续挑战，强调了像 VeriGUI 这类数据集在推进通用代理智能前沿方面的重要性。我们已将数据集开源，并将持续更新。希望 VeriGUI 能成为社区的宝贵资源，促进对 GUI 代理的进一步研究。


---



## 7 CONTRIBUTORS
## 7 贡献者


## Project Leaders
## 项目负责人


	- Shunyu Liu, Nanyang Technological University
	- 刘舜宇，南洋理工大学


	- Minghao Liu, 2077AI, M-A-P
	- 刘明豪，2077AI，M-A-P


## Core Contributors
## 核心贡献者


	- Huichi Zhou, 2077AI
	- 周惠池，2077AI


	- Zhenyu Cui, Zhejiang University
	- 崔振宇，浙江大学


	- Yang Zhou, Zhejiang University
	- 周扬，浙江大学


	- Yuhao Zhou, Shanghai AI Lab
	- 周宇浩，上海人工智能实验室


## Contributors
## 贡献者


	- Wendong Fan, Camel AI
	- 樊文东，Camel AI


	- Ge Zhang, M-A-P
	- 张戈，M-A-P


	- Jiajun Shi, M-A-P
	- 石嘉峻，M-A-P


	- Weihao Xuan, The University of Tokyo
	- 宣威豪，东京大学


	- Jiaxing Huang, Nanyang Technological University
	- 黄佳兴，南洋理工大学


	- Shuang Luo, Zhejiang University
	- 罗爽，浙江大学


	- Fang Wu, Stanford University
	- 吴方，斯坦福大学


	- Heli Qi, Waseda University
	- 祁和丽，早稻田大学


	- Qingcheng Zeng, Northwestern University
	- 曾青城，西北大学


	- Ziqi Ren, 2077AI, Zhejiang University
	- 任子齐，2077AI，浙江大学


	- Jialiang Gao, 2077AI, Abaka AI
	- 高嘉亮，2077AI，Abaka AI


	- Jindi Lv, Sichuan University
	- 吕金迪，四川大学


	- Junjie Wang, Tsinghua University, 2077AI
	- 王俊杰，清华大学，2077AI


	- Aosong Feng, Yale University
	- 冯奥松，耶鲁大学


	- Heng Zhou, Shanghai AI Lab
	- 周恒，上海人工智能实验室


## Advisors
## 顾问


	- Wangchunshu Zhou, OPPO
	- 周望春树，OPPO


	- Zhenfei Yin, Shanghai AI Lab
	- 尹振飞，上海人工智能实验室


	- Wenlong Zhang, Shanghai AI Lab
	- 张文龙，上海人工智能实验室


	- Guohao Li, Camel AI
	- 李国豪，Camel AI


	- Wenhao Yu, Tencent AI Lab
	- 俞文豪，腾讯人工智能实验室


	- Irene Li, The University of Tokyo
	- 李艾琳，东京大学


	- Lei Ma, The University of Tokyo
	- 马磊，东京大学


	- Lei Bai, Shanghai AI Lab
	- 白磊，上海人工智能实验室


	- Qunshu Lin, Abaka AI, Zhejiang University
	- 林群淑，Abaka AI，浙江大学


## Corresponding Authors
## 通信作者


	- Mingli Song, Zhejiang University
	- 宋明礼，浙江大学


	- Dacheng Tao, Nanyang Technological University
	- 祁成涛，南洋理工大学


---



## REFERENCES
## 参考文献


Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, 等. Gpt-4 技术报告. arXiv 预印本 arXiv:2303.08774, 2023.


Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like a human. arXiv preprint arXiv:2410.08164, 2024.
Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, 和 Xin Eric Wang. Agent s：一个开放的代理框架，像人类一样使用计算机. arXiv 预印本 arXiv:2410.08164, 2024.


Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. URL https://www.anthropic.com.
Anthropic. Claude 3 模型家族：Opus、Sonnet、Haiku, 2024. URL https://www.anthropic.com.


Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.
白帅, 陈可勤, 刘雪静, 王嘉临, 葛文斌, 宋思博, 邓凯, 王鹏, 王世杰, 唐俊, 等. Qwen2.5-vl 技术报告. arXiv 预印本 arXiv:2502.13923, 2025.


Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, et al. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264, 2024.
Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, 等. Windows agent arena：大规模评估多模态操作系统代理. arXiv 预印本 arXiv:2409.08264, 2024.


Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Huichi Zhou, Qihui Zhang, Zhigang He, Yilin Bai, Chujie Gao, Liuyi Chen, et al. Gui-world: A video benchmark and dataset for multimodal gui-oriented understanding. In ICLR, 2025.
陈东平, 黄岳, 吴思源, 汤靖宇, 周慧驰, 张启辉, 何志刚, 白艺琳, 高楚杰, 陈六易, 等. Gui-world：一个用于多模态 GUI 理解的视频基准与数据集. 在 ICLR, 2025.


Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. Secelick: Harnessing gui grounding for advanced visual gui agents. In ACL, pp. 9313-9332, 2024.
程坤志, 孙秋实, 楚友刚, 徐方志, 李彦涛, 张建兵, 以及吴志勇. Secelick：利用 GUI 锚定构建先进视觉 GUI 代理. 在 ACL, 第 9313-9332 页, 2024.


Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. In NeurIPS, volume 36, pp. 28091-28114, 2023.
邓翔, 顾宇, 郑泊源, 陈世杰, 萨姆·史蒂文斯, 王博石, 孙欢, 及苏宇. Mind2web：迈向通用网页代理. 在 NeurIPS, 卷 36, 第 28091-28114 页, 2023.


Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, et al. Assistgui: Task-oriented pc graphical user interface automation. In CVPR, pp. 13289-13298, 2024.
高迪飞, 纪磊, 白泽晨, 欧阳明煜, 李沛然, 毛东兴, 吴沁晨, 张维宸, 王佩怡, 郭翔武, 等. Assistgui：面向任务的 PC 图形用户界面自动化. 在 CVPR, 第 13289-13298 页, 2024.


Google. Gemini deep research, 2025. URL https://gemini.google/overview/deep-research.
Google. Gemini deep research, 2025. URL https://gemini.google/overview/deep-research.


Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024.
苟博宇, 王若涵, 郑泊源, 谢雅楠, 常成, 舒一恒, 孙欢, 与苏宇. 像人类一样导航数字世界：面向 GUI 代理的通用视觉锚定. arXiv 预印本 arXiv:2410.05243, 2024.


Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. A survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024.
顾嘉炜, 蒋旭辉, 史智超, Tan Hexiang, 翟学浩, 徐成进, 李伟, 沈英翰, 马胜杰, 刘宏浩, 等. 关于 LLM 作为裁判的综述. arXiv 预印本 arXiv:2411.15594, 2024.


Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. In ACL, 2024.
何洪亮, 姚文林, 马凯欣, 余文豪, 戴勇, 张宏明, 兰振中, 及余东. Webvoyager：用大型多模态模型构建端到端网页代理. 在 ACL, 2024.


Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents. In CVPR, pp. 14281- 14290, 2024.
洪文毅, 王伟翰, 吕庆松, 徐家政, 余文萌, 纪君辉, 王岩, 王子涵, 董宇霄, 丁鸣, 等. Cogagent：面向 GUI 代理的视觉语言模型. 在 CVPR, 第 14281-14290 页, 2024.


Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, et al. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation. arXiv preprint arXiv:2505.23885, 2025.
胡梦康, 周宇航, 樊文东, 聂宇舟, 夏博炜, 孙涛, 叶子瑜, 金昭轩, 李映儒, 陈启光, 等. Owl：面向现实任务自动化的通用多代理协助的优化劳动力学习. arXiv 预印本 arXiv:2505.23885, 2025.


Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et al. Os agents: A survey on mllm-based agents for computer, phone and browser use. OpenReview, 2024.
胡雪瑜, 熊涛, 易彪, 魏子书, 肖瑞轩, 陈雨润, 叶嘉晟, 陶美玲, 周祥欣, 赵紫宇, 等. OS agents：关于基于 MLLM 的计算机、手机与浏览器代理的综述. OpenReview, 2024.


Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-40 system card. arXiv preprint arXiv:2410.21276, 2024.
Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, 等. GPT-40 系统说明. arXiv 预印本 arXiv:2410.21276, 2024.


Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025.
金博文, 曾汉思, 岳振瑞, 尹振松, Sercan Arik, 王东, Hamed Zamani, 及韩家炜. Search-r1：用强化学习训练 LLM 推理并利用搜索引擎. arXiv 预印本 arXiv:2503.09516, 2025.


Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. In ACL, 2024.
Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, 及 Daniel Fried. Visualwebarena：在真实视觉网页任务上评估多模态代理. 在 ACL, 2024.


Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025.
Kaixin Li、Ziyang Meng、Hongzhan Lin、Ziyang Luo、Yuchen Tian、Jing Ma、Zhiyong Huang 和 Tat-Seng Chua。Screenspot-pro：面向专业高分辨率计算机使用的 GUI 定位。arXiv 预印本 arXiv:2504.07981，2025 年。


Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Stan Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. In CVPR, pp. 19498-19508, 2025.
Kevin Qinghong Lin、Linjie Li、Difei Gao、Zhengyuan Yang、Shiwei Wu、Zechen Bai、Stan Weixian Lei、Lijuan Wang 和 Mike Zheng Shou。Showui：一种用于 GUI 可视代理的一体化视觉-语言-动作模型。载于 CVPR，页 19498-19508，2025 年。


Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a.
Aixin Liu、Bei Feng、Bing Xue、Bingxuan Wang、Bochao Wu、Chengda Lu、Chenggang Zhao、Chengqi Deng、Chenyu Zhang、Chong Ruan 等。Deepseek-v3 技术报告。arXiv 预印本 arXiv:2412.19437，2024a 年。


Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802, 2018.
Evan Zheran Liu、Kelvin Guu、Panupong Pasupat、Tianlin Shi 和 Percy Liang。在网页界面上使用工作流引导探索的强化学习。arXiv 预印本 arXiv:1802.08802，2018 年。


Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, volume 36, pp. 34892-34916, 2023.
Haotian Liu、Chunyuan Li、Qingyang Wu 和 Yong Jae Lee。视觉指令微调。载于 NeurIPS，第 36 卷，页 34892-34916，2023 年。


Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visu-alwebbench: How far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955, 2024b.
Junpeng Liu、Yifan Song、Bill Yuchen Lin、Wai Lam、Graham Neubig、Yuanzhi Li 和 Xiang Yue。Visu-alwebbench：多模态大模型在网页理解与定位上发展到何种程度？arXiv 预印本 arXiv:2404.05955，2024b 年。


Xing Han Lù, Zdeněk Kasner, and Siva Reddy. Weblinx: Real-world website navigation with multi-turn dialogue. In ICML, 2024.
Xing Han Lù、Zdeněk Kasner 和 Siva Reddy。Weblinx：具有多轮对话的真实网站导航。载于 ICML，2024 年。


Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: A generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025.
Run Luo、Lu Wang、Wanwei He 和 Xiaobo Xia。Gui-r1：面向 GUI 代理的一般化 R1 风格视觉-语言-动作模型。arXiv 预印本 arXiv:2504.10458，2025 年。


Magnus Müller and Gregor Zunič. Browser use: Enable ai to control your browser, 2024. URL https: //github.com/browser-use/browser-use.
Magnus Müller 和 Gregor Zunič。Browser use：使 AI 能够控制你的浏览器，2024。网址 https: //github.com/browser-use/browser-use。


Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. Gui agents: A survey. arXiv preprint arXiv:2412.13501, 2024.
Dang Nguyen、Jian Chen、Yu Wang、Gang Wu、Namyong Park、Zhengmian Hu、Hanjia Lyu、Junda Wu、Ryan Aponte、Yu Xia 等。GUI 代理：综述。arXiv 预印本 arXiv:2412.13501，2024 年。


Liangbo Ning, Ziran Liang, Zhuohang Jiang, Haohao Qu, Yujuan Ding, Wenqi Fan, Xiao-yong Wei, Shanru Lin, Hui Liu, Philip S Yu, et al. A survey of webagents: Towards next-generation ai agents for web automation with large foundation models. arXiv preprint arXiv:2503.23350, 2025.
Liangbo Ning、Ziran Liang、Zhuohang Jiang、Haohao Qu、Yujuan Ding、Wenqi Fan、Xiao-yong Wei、Shanru Lin、Hui Liu、Philip S Yu 等。Webagents 调查：面向以大基础模型驱动的下一代网页自动化 AI 代理。arXiv 预印本 arXiv:2503.23350，2025 年。


OpenAI. Deep research system card, 2025. URL https://cdn.openai.com/ deep-research-system-card.pdf.
OpenAI。深度研究系统说明，2025 年。网址 https://cdn.openai.com/ deep-research-system-card.pdf。


Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, et al. Webcanvas: Benchmarking web agents in online environments. arXiv preprint arXiv:2406.12373, 2024.
Yichen Pan、Dehan Kong、Sida Zhou、Cheng Cui、Yifei Leng、Bing Jiang、Hangyu Liu、Yanyi Shang、Shuyan Zhou、Tongshuang Wu 等。Webcanvas：在线环境下网页代理的基准测试。arXiv 预印本 arXiv:2406.12373，2024 年。


Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025.
Yujia Qin、Yining Ye、Junjie Fang、Haoming Wang、Shihao Liang、Shizuo Tian、Junda Zhang、Jiahao Li、Yunxin Li、Shijue Huang 等。Ui-tars：开创性地用原生代理实现的自动化 GUI 交互。arXiv 预印本 arXiv:2501.12326，2025 年。


Dingfeng Shi, Jingyi Cao, Qianben Chen, Weichen Sun, Weizhen Li, Hongxuan Lu, Fangchen Dong, Tian-rui Qin, King Zhu, Minghao Liu, Jian Yang, Ge Zhang, Jiaheng Liu, Changwang Zhang, Jun Wang, Yuchen Eleanor Jiang, and Wangchunshu Zhou. Taskcraft: Automated generation of agentic tasks. arXiv preprint arXiv:2506.10055, 2025.
Dingfeng Shi、Jingyi Cao、Qianben Chen、Weichen Sun、Weizhen Li、Hongxuan Lu、Fangchen Dong、Tian-rui Qin、King Zhu、Minghao Liu、Jian Yang、Ge Zhang、Jiaheng Liu、Changwang Zhang、Jun Wang、Yuchen Eleanor Jiang 和 Wangchunshu Zhou。Taskcraft：任务型代理任务的自动生成。arXiv 预印本 arXiv:2506.10055，2025 年。


Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In ICML, pp. 3135-3144, 2017.
石天林、Andrej Karpathy、樊林溪、Jonathan Hernandez 与 Percy Liang。World of bits：一个面向基于网页的代理的开放域平台。发表于 ICML，页 3135-3144，2017。


Huatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian Min, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher++: Incentivizing the dynamic knowledge acquisition of llms via reinforcement learning. arXiv preprint arXiv:2505.17005, 2025.
宋华桐、江金豪、田文清、陈志鹏、吴雨欢、赵家豪、闵英倩、赵新文、方磊 与 文继荣。R1-searcher++：通过强化学习激励大模型的动态知识获取。arXiv 预印本 arXiv:2505.17005，2025。


Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, et al. Cradle: Empowering foundation agents towards general computer control. arXiv preprint arXiv:2403.03186, 2024.
谭伟豪、张文韬、徐欣润、夏昊崇、丁子洛、李博宇、周博涵、岳俊鹏、江杰川、李野文 等。Cradle：增强基础代理以实现通用计算机控制。arXiv 预印本 arXiv:2403.03186，2024。


Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalk-wyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
Gemini 团队、Rohan Anil、Sebastian Borgeaud、Jean-Baptiste Alayrac、余佳辉、Radu Soricut、Johan Schalk-wyk、Andrew M Dai、Anja Hauth、Katie Millican 等。Gemini：一系高度能力的多模态模型。arXiv 预印本 arXiv:2312.11805，2023。


Shuai Wang, Weiwen Liu, Jingxuan Chen, Yuqi Zhou, Weinan Gan, Xingshan Zeng, Yuhan Che, Shuai Yu, Xinlong Hao, Kun Shao, et al. Gui agents with foundation models: A comprehensive survey. arXiv preprint arXiv:2411.04890, 2024.
王帅、刘伟文、陈景轩、周宇琦、甘微楠、曾兴山、车雨涵、余帅、郝新龙、邵昆 等。带有基础模型的 GUI 代理：一项全面综述。arXiv 预印本 arXiv:2411.04890，2024。


Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572, 2025.
吴嘉龙、尹文彪、姜勇、王正林、席泽坤、方润南、张林海、何玉兰、周德宇、谢鹏军 等。Webwalker：评测大模型在网页遍历中的表现。arXiv 预印本 arXiv:2501.07572，2025。


Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: A foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024.
吴志勇、吴振宇、徐方志、王奕安、孙秋实、贾承佑、程坎志、丁紫晨、陈励恒、Paul Pu Liang 等。OS-Atlas：面向通用 GUI 代理的基础动作模型。arXiv 预印本 arXiv:2410.23218，2024。


Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634, 2023.
谢天宝、周凡、程周俊、史鹏、翁洛轩、刘一涛、Toh Jing Hua、赵俊宁、刘茜、刘彻 等。OpenAgents：面向真实环境中语言代理的开放平台。arXiv 预印本 arXiv:2310.10634，2023。


Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In NeurIPS, volume 37, pp. 52040-52094, 2024.
谢天宝、张丹阳、陈继轩、李晓川、赵思衡、曹睿升、Toh J Hua、程周俊、申东灿、雷方宇 等。OSWorld：在真实计算机环境中为开放式任务评测多模态代理。发表于 NeurIPS，第 37 卷，页 52040-52094，2024。


Jingqi Yang, Zhilong Song, Jiawei Chen, Mingli Song, Sheng Zhou, Xiaogang Ouyang, Chun Chen, Can Wang, et al. Gui-robust: A comprehensive dataset for testing gui agent robustness in real-world anomalies. arXiv preprint arXiv:2506.14477, 2025.
Jingqi Yang, Zhilong Song, Jiawei Chen, Mingli Song, Sheng Zhou, Xiaogang Ouyang, Chun Chen, Can Wang, et al. Gui-robust：用于测试图形界面代理在真实世界异常情况下鲁棒性的综合数据集。arXiv 预印本 arXiv:2506.14477, 2025。


Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In NeurIPS, volume 35, pp. 20744-20757, 2022.
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop：面向可扩展真实世界网页交互的有根语言代理。载于 NeurIPS，卷 35，页 20744-20757, 2022。


Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile ui understanding with multimodal llms. In ECCV, pp. 240-255, 2024.
Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui：使用多模态大模型的落地移动界面理解。载于 ECCV，页 240-255, 2024。


Bofei Zhang, Zirui Shang, Zhi Gao, Wang Zhang, Rui Xie, Xiaojian Ma, Tao Yuan, Xinxiao Wu, Song-Chun Zhu, and Qing Li. Tongui: Building generalized gui agents by learning from multimodal web tutorials. arXiv preprint arXiv:2504.12679, 2025.
Bofei Zhang, Zirui Shang, Zhi Gao, Wang Zhang, Rui Xie, Xiaojian Ma, Tao Yuan, Xinxiao Wu, Song-Chun Zhu, and Qing Li. Tongui：通过从多模态网络教程中学习构建通用图形界面代理。arXiv 预印本 arXiv:2504.12679, 2025。


Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents: A survey. arXiv preprint arXiv:2411.18279, 2024a.
Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. 大语言模型驱动的图形界面代理：综述。arXiv 预印本 arXiv:2411.18279, 2024a。


Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: A ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024b.
Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. UFO：面向 Windows 操作系统交互的界面聚焦代理。arXiv 预印本 arXiv:2402.07939, 2024b。


Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. Mm-llms: Recent advances in multimodal large language models. arXiv preprint arXiv:2401.13601, 2024c.
Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. Mm-llms：多模态大语言模型的最新进展。arXiv 预印本 arXiv:2401.13601, 2024c。


Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Xinbei Ma, Muyun Yang, Tiejun Zhao, and Min Zhang. Dynamic planning for llm-based graphical user interface automation. In EMNLP Findings, pp. 1304-1320, 2024d.
Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Xinbei Ma, Muyun Yang, Tiejun Zhao, and Min Zhang. 基于大模型的图形用户界面自动化的动态规划。载于 EMNLP Findings，页 1304-1320, 2024d。


Henry Hengyuan Zhao, Difei Gao, and Mike Zheng Shou. Worldgui: An interactive benchmark for desktop gui automation from any starting point. arXiv preprint arXiv:2502.08047, 2025.
Henry Hengyuan Zhao, Difei Gao, and Mike Zheng Shou. Worldgui：一个从任意起点开始的桌面界面自动化交互基准。arXiv 预印本 arXiv:2502.08047, 2025。


Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, and Shuicheng Yan. Agentstudio: A toolkit for building general virtual agents. arXiv preprint arXiv:2403.17918, 2024.
Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, and Shuicheng Yan. Agentstudio：构建通用虚拟代理的工具包。arXiv 预印本 arXiv:2403.17918, 2024。


Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deep-researcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025.
Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deep-researcher：通过在真实世界环境中用强化学习扩展深度研究。arXiv 预印本 arXiv:2504.03160, 2025。


Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023a.
Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena：用于构建自主代理的真实网页环境。arXiv 预印本 arXiv:2307.13854, 2023a。


Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang, Hua-jun Chen, Peng Cui, and Mrinmaya Sachan. Agents: An open-source framework for autonomous language agents. arXiv preprint arXiv:2309.07870, 2023b.
Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang, Hua-jun Chen, Peng Cui, and Mrinmaya Sachan. Agents：一个用于自主语言代理的开源框架。arXiv 预印本 arXiv:2309.07870, 2023b。


Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, and Yuchen Eleanor Jiang. Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532, 2024.
Wangchunshu Zhou、Yixin Ou、Shengwei Ding、Long Li、Jialong Wu、Tiannan Wang、Jiamin Chen、Shuai Wang、Xiaohua Xu、Ningyu Zhang、Huajun Chen 和 Yuchen Eleanor Jiang。符号化学习使自我进化代理成为可能。arXiv 预印本 arXiv:2406.18532，2024。


This work is still in progress and additional data will be included in a future version.
这项工作仍在进行中，未来版本将包含更多数据。


Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinqlin Jia, et al. Gui-gl: Understanding rl-zero-like training for visual grounding in gui agents. arXiv preprint arXiv:2505.15810, 2025.
Yuqi Zhou、Sunhao Dai、Shuai Wang、Kaiwen Zhou、Qinqlin Jia 等。Gui-gl：理解用于 GUI 代理视觉定位的类似 rl-zero 的训练。arXiv 预印本 arXiv:2505.15810，2025。


He Zhu, Tianrui Qin, King Zhu, Heyuan Huang, Yeyi Guan, Jinxiang Xia, Yi Yao, Hanhao Li, Ningning Wang, Pai Liu, Tianhao Peng, Xin Gui, Xiaowan Li, Yuhui Liu, Yuchen Eleanor Jiang, Jun Wang, Changwang Zhang, Xiangru Tang, Ge Zhang, Jian Yang, Minghao Liu, Xitong Gao, Jiaheng Liu, and Wangchunshu Zhou. Oagents: An empirical study of building effective agents. arXiv preprint arXiv:2506.15741, 2025a.
He Zhu、Tianrui Qin、King Zhu、Heyuan Huang、Yeyi Guan、Jinxiang Xia、Yi Yao、Hanhao Li、Ningning Wang、Pai Liu、Tianhao Peng、Xin Gui、Xiaowan Li、Yuhui Liu、Yuchen Eleanor Jiang、Jun Wang、Changwang Zhang、Xiangru Tang、Ge Zhang、Jian Yang、Minghao Liu、Xitong Gao、Jiaheng Liu 和 Wangchunshu Zhou。Oagents：构建高效代理的实证研究。arXiv 预印本 arXiv:2506.15741，2025a。


King Zhu, Hanhao Li, Siwei Wu, Tianshun Xing, Dehua Ma, Xiangru Tang, Minghao Liu, Jian Yang, Jiaheng Liu, Yuchen Eleanor Jiang, Changwang Zhang, Chenghua Lin, Jun Wang, Ge Zhang, and Wangchunshu Zhou. Scaling test-time compute for llm agents. arXiv preprint arXiv:2506.12928, 2025b.
King Zhu、Hanhao Li、Siwei Wu、Tianshun Xing、Dehua Ma、Xiangru Tang、Minghao Liu、Jian Yang、Jiaheng Liu、Yuchen Eleanor Jiang、Changwang Zhang、Chenghua Lin、Jun Wang、Ge Zhang 和 Wangchunshu Zhou。在推理时扩展 llm 代理的计算规模测试。arXiv 预印本 arXiv:2506.12928，2025b。


## Appendix
## 附录


TABLE OF CONTENTS
目录


A Action Space 19
A 行动空间 19


A. 1 Action Space for Web Task 19
A.1 网页任务的行动空间 19


B Experimental Settings 19
B 实验设置 19


B. 1 Agent Prompt for Web Task 19
B.1 网页任务的代理提示 19


B.2 LLM-as-a-Judge Prompt for Web Task 20
B.2 网页任务的 LLM 作为裁判 的提示 20


C Task Example 21
C 任务示例 21


C. 1 Web Task Example 21
C.1 网页任务示例 21


## A ACTION SPACE
## A 行动空间


### A.1 ACTION SPACE FOR WEB TASK
### A.1 网页任务的行动空间


We develop a screen capture tool to support human annotators in collecting detailed task trajectories. Each recorded trajectory logs all mouse and keyboard events, which can be systematically mapped to the predefined GUI action space. The mapping is as follows:
我们开发了一个屏幕录制工具，帮助人工标注者采集详尽的任务轨迹。每条记录的轨迹都会记录所有鼠标和键盘事件，这些事件可以系统地映射到预定义的 GUI 动作空间。映射如下：


- Scroll wheel events (WHEEL) are mapped to the scroll action.
- 滚轮事件 (WHEEL) 映射为 scroll 动作。


- Key press events (KEY_DOWN) are mapped to the key_down action.
- 按键按下事件 (KEY_DOWN) 映射为 key_down 动作。


- Text input events (INPUT) are mapped to the input action.
- 文本输入事件 (INPUT) 映射为 input 动作。


- Text output events (RESULT_STATE) are mapped to the result_state action.
- 文本输出事件 (RESULT_STATE) 映射为 result_state 动作。


- Right-click context menu events (CONTEXT_MENU) are mapped to the right_click action.
- 右键上下文菜单事件 (CONTEXT_MENU) 映射为 right_click 动作。


- Tab switching events (TAB_CHANGE) are interpreted to the left_click action at the corresponding coordinates.
- 选项卡切换事件 (TAB_CHANGE) 解释为在对应坐标的 left_click 动作。


- Mouse drag actions (MOUSE_DRAG) are mapped to the drag action.
- 鼠标拖拽动作 (MOUSE_DRAG) 映射为 drag 动作。


- If a MOUSE_DOWN event is not followed by a MOUSE_DRAG event, it is interpreted as the left_click action.
- 若 MOUSE_DOWN 事件后未跟随 MOUSE_DRAG 事件，则解释为 left_click 动作。


- Additionally, MOUSE_UP events are recorded to help determine the end of drag actions or validate click completions, although they are not directly mapped to any action in the defined space.
- 此外，记录 MOUSE_UP 事件以帮助确定拖拽动作的结束或验证点击完成，尽管它们未被直接映射到已定义的动作空间中的任何动作。


This mapping ensures consistency between the raw recorded interactions and the unified action space $\mathcal{A}$ ,enabling accurate interpretation and reproduction of user behaviors by the model during both training and inference.
该映射确保原始记录交互与统一动作空间 $\mathcal{A}$ 之间的一致性，使模型在训练和推理期间能够准确解释和重现用户行为。


## B EXPERIMENTAL SETTINGS
## B 实验设置


B.1 AGENT PROMPT FOR WEB TASK
B.1 网页任务的代理提示


The agent prompt for different agent in web tasks is shown below:
不同代理在网页任务中的代理提示如下所示：


Deep Research Agent Prompt
深入研究代理提示


\{question\}
\{question\}


## Search Engine Agent Prompt
## 搜索引擎代理提示


You are the EXECUTOR agent. You will receive one task description at a time. Your role is to complete the task efficiently, using available tools via function calls when necessary.
你是 EXECUTOR 代理。你将一次收到一个任务描述。你的职责是高效完成任务，在必要时通过函数调用使用可用工具。


Guidelines:
指南：


- Always think step by step before responding.
- 在回复前务必逐步思考。


- Provide concise answers.
- 提供简明回答。


- If a tool is needed, respond only with the function call - no extra text.
- 如果需要工具，只回复函数调用——不要添加额外文字。


- When the task is complete, respond with: FINAL ANSWER: [your answer here]
- 任务完成后，回复：FINAL ANSWER: [your answer here]


## Browser-Use Agent Prompt
## 浏览器使用代理提示


We follow the official agent prompt from Browser-Use (Müller & Zunič, 2024).
我们遵循 Browser-Use 的官方代理提示（Müller & Zunič, 2024）。


## OWL Agent Prompt
## OWL 代理提示


You are a helpful assistant that can search the web, extract webpage content, simulate browser actions, and provide relevant information to solve the given task.
你是一个能搜索网络、提取网页内容、模拟浏览器操作并提供相关信息以解决给定任务的助理。


You are now working in 'working_dir'. All your work related to local operations should be done in that directory.
你现在在 'working_dir' 工作。所有与本地操作相关的工作应在该目录完成。


## ###Mandatory Instructions
## ###强制性指示


1. **Take Detailed Notes**: You MUST use the 'append_note' tool to record your findings. Ensure notes are detailed, well-organized, and include source URLs. Do not overwrite notes unless summarizing; append new information. Your notes are crucial for the Document Agent.
1. **做详细笔记**：你必须使用 'append_note' 工具记录你的发现。确保笔记详尽、有条理，并包含来源 URL。除非在做摘要，否则不要覆盖笔记；要追加新信息。你的笔记对文档代理至关重要。


## ###Web Search Workflow
## ###网络搜索工作流程


1. **Initial Search**: Start with a search engine like 'search_google' or 'search_bing' to get a list of relevant URLs for your research if available.
1. **初始搜索**：如果可行，先用类似 'search_google' 或 'search_bing' 的搜索引擎获取与你研究相关的 URL 列表。


2. **Browser-Based Exploration**: Use the rich browser toolset to investigate websites.
2. **基于浏览器的探索**：使用丰富的浏览器工具集来调查网站。


- **Navigation**: Use 'visit_page' to open a URL. Navigate with 'click', 'back', and 'forward'. Manage multiple pages with 'switch_tab'.
- **导航**：使用 'visit_page' 打开 URL。通过 'click'、'back' 和 'forward' 导航。用 'switch_tab' 管理多个页面。


- **Analysis**: Use 'get_som_screenshot' to understand the page layout and identify interactive elements. Since this is a heavy operation, only use it when visual analysis is necessary. - **Interaction**: Use 'type' to fill out forms and 'enter' to submit.
- **分析**：使用 'get_som_screenshot' 理解页面布局并识别交互元素。由于这是一个开销较大的操作，仅在需要视觉分析时使用。- **交互**：使用 'type' 填写表单，使用 'enter' 提交。


3. **Detailed Content Extraction**: Prioritize using the scraping tools from 'Crawl4AIToolkit' for in-depth information gathering from awebpage.
3. **详细内容提取**：优先使用 'Crawl4AIToolkit' 的爬取工具从网页获取深入信息。


###Guidelines and Best Practices
### 指南与最佳实践


- **URL Integrity**: You MUST only use URLs from trusted sources (e.g., search engine results or links on visited pages). NEVER invent or guess URLs.
- **URL 完整性**：你必须仅使用来自可信来源的 URL（例如搜索引擎结果或已访问页面上的链接）。绝不可捏造或臆测 URL。


- **Thoroughness**: If a search query is complex, break it down. If a snippet is unhelpful but the URL seems authoritative, visit the page. Check subpages for more information.
- **彻底性**：如果搜索查询复杂，将其拆分。如果摘要无用但 URL 看起来权威，访问该页面。检查子页面以获取更多信息。


- **Persistence**: If one method fails, try another. Combine search, scraper, and browser tools for comprehensive information gathering.
- **坚持性**：如果一种方法失败，尝试另一种。结合搜索、爬虫和浏览器工具以全面收集信息。


- **Collaboration**: Communicate with other agents using 'send_message' when you need help. Use 'list_available_agents' to see who is available.
- **协作**：需要帮助时使用 'send_message' 与其他代理沟通。使用 'list_available_agents' 查看谁可用。


- **Clarity**: In your response, you should mention the URLs you have visited and processed.
- **清晰性**：在你的回应中，应提及你已访问并处理的 URL。


### B.2 LLM-AS-A-JUDGE PROMPT FOR WEB TASK
### B.2 面向网络任务的 LLM 作为裁判 提示


For web tasks, the goal is defined as obtaining a correct textual answer through multi-turn information retrieval and reasoning. Thus, we use GPT-4.1 as a judge to semantically evaluate the correctness of agents' final answers based on the question, ground truth, and model response, and report the LLM-as-a-Judge score. The detailed evaluation prompt is provided as follows:
对于网络任务，目标是通过多轮信息检索和推理获得正确的文字答案。因此，我们使用 GPT-4.1 作为裁判，根据问题、真实答案和模型响应对代理最终答案的语义正确性进行评估，并报告 LLM 作为裁判的分数。详细的评估提示如下：


## LLM-as-a-Judge Prompt for Web Task
## 面向网络任务的 LLM 作为裁判 提示


You are a strict evaluator assessing answer correctness. You must score the model's prediction on a scale from 0 to 10 , where 0 represents an entirely incorrect answer and 10 indicates a highly correct answer.
你是一个严格的评估者，评判答案正确性。你必须按 0 到 10 的尺度为模型预测打分，0 表示完全错误，10 表示高度正确。


#Input
#输入


Question:
问题：


...



\{question\}
\{question\}


Ground Truth Answer:
参考答案：


...



\{answer\}
\{answer\}


Model Prediction:
模型预测：


\{pred\}
\{pred\}


#Evaluation Rules
#评估规则


- The model prediction may contain the reasoning process, you should spot the final answer from it.
- 模型预测中可能包含推理过程，你应从中找出最终答案。


- Assign a high score if the prediction matches the answer semantically, considering variations in format.
- 若预测在语义上与答案一致（考虑格式差异），应给高分。


- Deduct points for partially correct answers or those with incorrect additional information.
- 对部分正确或包含错误附加信息的回答扣分。


- Ignore minor differences in formatting, capitalization, or spacing since the model may explain in a different way.
- 忽略因解释方式不同而导致的轻微格式、大小写或空格差异。


- Treat numerical answers as correct if they match within reasonable precision
- 若为数值答案，只要在合理精度内匹配即视为正确。


- For questions requiring units, both value and unit must be correct
- 对要求单位的问题，数值和单位必须都正确。


#Scoring Guide
#评分指南


Provide a single integer from 0 to 10 to reflect your judgment of the answer's correctness.
给出一个从0到10的整数，反映你对答案正确性的判断。


#Strict Output format example
#严格输出格式示例


4



## C TASK EXAMPLE
## C 任务示例


### C.1 WEB TASK EXAMPLE
### C.1 网络任务示例


The web tasks focus on deep research requiring multi-turn information retrieval and reasoning. In VeriGUI, these tasks span five key thematic domains: scientific and academic research; finance and economics; technology and innovation; arts and entertainment; and social policy and sustainability. Below are some examples of web tasks.
这些网络任务侧重于需要多轮信息检索与推理的深入研究。在 VeriGUI 中，任务涵盖五大主题领域：科学与学术研究；金融与经济；技术与创新；艺术与娱乐；以及社会政策与可持续发展。以下为一些网络任务示例。


## Web Task Example - Scientific and Academic Research
## 网络任务示例——科学与学术研究


## Task Instruction
## 任务说明


Identify the earliest known warship that sank on its maiden voyage. Provide the vessel's commonly accepted name, estimated sinking year or century, salvage year, location described by sea area and the exclusive economic zone of the country it lies in, as well as the museum currently displaying the wreck and the official name of any anchor-related artifacts from it in the museum's collection.
确定已知最早在处女航中沉没的军舰。提供该舰的通用名称、估计沉没年份或世纪、打捞年份、以海域和该残骸所在国专属经济区描述的位置、以及目前展示该残骸的博物馆和博物馆藏品中任何与锚相关文物的官方名称。


## Task Goal
## 任务目标


Shipwreck: Vasa
沉船：瓦萨号


Year of sinking: 1628
沉没年份：1628


Salvage year: 1961
打捞年份：1961


Location: Stockholm, Sweden
位置：瑞典 斯德哥尔摩


Museum: Vasa Museum
博物馆：瓦萨博物馆


Artifact: Ankarstock
文物：Ankarstock


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_19_27_6f5c92.jpg"/>



Figure 8: Human demonstration screenshots for the scientific and academic research web task example.
图 8：用于科学与学术研究的网络任务示例的人类演示截图。


## Subtask 1 Instruction
## 子任务 1 说明


Compile a list of major shipwreck discoveries where the primary search and identification technology used was multibeam sonar.
汇编一份重大发现沉船的清单，这些沉船的主要搜索与识别技术为多波束声纳。


## Subtask 1 Goal
## 子任务 1 目标


<table><tr><td>Name</td><td>Year</td></tr><tr><td>USS Kittiwake</td><td>2011</td></tr><tr><td>C-50 Naufragio Vicente Palacio Riva Ship</td><td>2000</td></tr><tr><td>The Vasa</td><td>1961</td></tr><tr><td>The Lusitania</td><td>1915</td></tr></table>
<table><tbody><tr><td>名称</td><td>年份</td></tr><tr><td>美国海军基蒂维克号</td><td>2011</td></tr><tr><td>C-50 Vicente Palacio Riva 船只残骸</td><td>2000</td></tr><tr><td>瓦萨号</td><td>1961</td></tr><tr><td>卢西塔尼亚号</td><td>1915</td></tr></tbody></table>


## Subtask 2 Instruction
## 子任务 2 指令


For each shipwreck on the list, find its estimated sinking date (year or century). Identify the wreck with the earliest sinking date.
对于列表中的每艘沉船，找出其估计沉没日期（年份或世纪）。确定沉没日期最早的沉船。


## Subtask 2 Goal
## 子任务 2 目标


<table><tr><td>Name</td><td>Year</td></tr><tr><td>USS Kittiwake <br> C-50 Naufragio Vicente Palacio Riva Ship <br> The Vasa <br> The Lusitania</td><td>1994 <br> 2000 <br> 1628 (oldest) 1906</td></tr></table>
<table><tbody><tr><td>姓名</td><td>年份</td></tr><tr><td>USS Kittiwake <br/> C-50 Naufragio Vicente Palacio Riva 船只 <br/> 瓦萨号 <br/> 卢西塔尼亚号</td><td>1994 <br/> 2000 <br/> 1628（最古） 1906</td></tr></tbody></table>


## Subtask 3 Instruction
## 子任务3 指令


Confirm the full name of the organization, or institution responsible for the discovery of the Vasa.
确认发现瓦萨号的组织或机构的全称。


## Subtask 3 Goal
## 子任务3 目标


Organization: Vasa Museum
组织：瓦萨博物馆


## Subtask 4 Instruction
## 子任务4 指令


Determine The Vasa's location, specifying the sea or ocean body and the Exclusive Economic Zone (EEZ) of the relevant coastal nation.
确定瓦萨号的位置，说明所属海域或洋区，以及相关沿海国家的专属经济区（EEZ）。


## Subtask 4 Goal
## 子任务4 目标


Location: Stockholm, Sweden
位置：瑞典 斯德哥尔摩


## Subtask 5 Instruction
## 子任务5 指令


Search museum databases and archaeological reports to find a museum that currently exhibits The Vasa.
在博物馆数据库和考古报告中查找目前展出瓦萨号的博物馆。


Subtask 5 Goal
Subtask 5 Goal


Museum: Vasa Museum
Museum: Vasa Museum


## Subtask 6 Instruction
## 子任务6 指令


From the Vasa Museum's official collection catalog or website, find the official name of the specific artifact related to the anchor stock in the museum's collection.
从瓦萨博物馆的官方藏品目录或网站中，查找博物馆藏品中与锚杆相关的该文物的官方名称。


Subtask 6 Goal
Subtask 6 Goal


Name: Ankarstock
Name: Ankarstock


## Web Task Example - Finance and Economics
## 网络任务示例 - 金融与经济


## Task Instruction
## 任务说明


Among all Chinese banks listed in Hong Kong from 2022 to 2023, list the bank with the highest increase in net interest margin ranking, and provide: (1) bank name, (2) net interest margin values before and after the increase, (3) stock code, (4) total asset growth rate, and (5) chairman's name.
在所有于 2022 至 2023 年在香港上市的中资银行中，列出净利差排名提升最多的银行，并提供：(1) 银行名称，(2) 提升前后净利差数值，(3) 股票代码，(4) 总资产增长率，(5) 董事长姓名。


Task Goal
任务目标


Bank Name: Hang Seng Bank
银行名称：恒生银行


Net Interest Margin Values Before and After the Increase: 1.75%, 2.30%
提升前后净利差数值：1.75%、2.30%


Stock Code: 0011.HK
股票代码：0011.HK


Total Asset Growth Rate: -8.75%
总资产增长率：-8.75%


Chairman's Name: Irene Lee
董事长姓名：Irene Lee


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_19_27_f3e775.jpg"/>



Figure 9: Human demonstration screenshots for the finance and economics web task example.
图 9：金融与经济网络任务示例的人类示范截图。


## Subtask 1 Instruction
## 子任务 1 说明


Collect the list of the top 10 licensed banks in Hong Kong by total assets in 2024 and their respective annual net interest margin (NIM) data.
收集 2024 年按总资产排名的香港十大持牌银行名单及其各自的年度净利差（NIM）数据。


## Subtask 1 Goal
## 子任务 1 目标


Hongkong and Shanghai Banking Corporation Limited (The): 10,500,393 HK\$ million
香港上海汇丰银行有限公司（The）：10,500,393 HK$ 百万


Bank of China (Hong Kong) Limited: 3,685,578 HK\$ million
中国银行（香港）有限公司：3,685,578 HK$ 百万


Standard Chartered Bank (Hong Kong) Limited: 2,534,695 HK\$ million
渣打银行（香港）有限公司：2,534,695 HK$ 百万


Hang Seng Bank, Limited: 1,692,094 HK\$ million
恒生银行有限公司：1,692,094 港元百万


Industrial and Commercial Bank of China (Asia) Limited: 915,960 HK\$ million
中国工商银行（亚洲）有限公司：915,960 港元百万


Bank of East Asia, Limited (The): 860,361 HK\$ million
东亚银行有限公司（The）：860,361 港元百万


Nanyang Commercial Bank, Limited: 555,149 HK\$ million
南洋商业银行有限公司：555,149 港元百万


China Construction Bank (Asia) Corporation Limited: 493,858 HK\$ million
中国建设银行（亚洲）有限公司：493,858 港元百万


China CITIC Bank International Limited: 470,387 HK\$ million
中信银行国际有限公司：470,387 港元百万


DBS Bank (Hong Kong) Limited: 467,621 HK\$ million
星展银行（香港）有限公司：467,621 港元百万


## Subtask 2 Instruction
## 子任务2 指示


Collect the list of the top 10 licensed banks in Hong Kong from 2022 to 2023 and their annual net interest margin data, and calculate the increase in net interest margin for each bank and identify the bank with the largest increase.
收集2022至2023年香港前10家持牌银行及其年度净利差数据，计算各行净利差的增加幅度并识别增加最多的银行。


Subtask 2 Goal
子任务2 目标


Bank: Hang Seng Bank
银行：恒生银行


NIM Increase: 55bp
净利差增加：55个基点


## Subtask 3 Instruction
## 子任务3 指示


Find the following for the bank: (1) name, (2) specific net interest margin values for 2022 and 2023, (3) stock code.
为该银行查找：(1) 名称，(2) 2022年和2023年的具体净利差数值，(3) 股票代码。


Subtask 3 Goal
子任务3 目标


Name: Hang Seng Bank
名称：恒生银行


2022 NIM: 1.75%
2022 净息差(NIM)：1.75%


2023 NIM: 2.30%
2023 净息差(NIM)：2.30%


Stock Code: 0011.HK
股票代码：0011.HK


## Subtask 4 Instruction
## 子任务4 说明


Find the bank's(Hang Seng Bank) total asset data for 2022-2024 and calculate the total asset growth rate.
查找该行（恒生银行）2022–2024 年的总资产数据并计算总资产增长率。


Subtask 4 Goal
子任务4 目标


2022 asset data: 1,854.4 HK\$bn
2022 年资产数据：1,854.4 HK\$bn


2023 asset data: 1,692.1 HK\$bn
2023 年资产数据：1,692.1 HK\$bn


Growth rate: -8.75%
增长率：-8.75%


## Subtask 5 Instruction
## 子任务5 说明


Find the current chairman's name of the bank(Hang Seng Bank).
查找该行（恒生银行）现任董事长姓名。


## Subtask 5 Goal
## 子任务5 目标


Chairman: Irene Lee
董事长：Irene Lee


## Web Task Example - Technology and Innovation
## 网页任务示例 - 技术与创新


## Task Instruction
## 任务说明


Identify the pharmaceutical company that had the FDA-approved new molecular entities (NMEs) between 2020 and 2024, where at least one of these drugs achieved blockbuster status (over \$1 billion in annual sales) within 24 months of approval. List the company name, total number of NMEs approved, the name and indication of the fastest blockbuster drug, its peak annual sales figure, and the name and specialization of the lead scientist credited with its discovery.
识别在2020至2024年间获得 FDA 批准的新分子实体（NME）的制药公司，其中至少有一种药物在获批后24个月内成为重磅药（年销售额超过 \$1 billion）。列出公司名称、获批的 NME 总数、最迅速成为重磅药的药物名称及适应症、其最高年销售额，以及被认为发现该药的首席科学家的姓名和专长。


## Task Goal
## 任务目标


Company Name: Pfizer Inc.
公司名称：辉瑞公司（Pfizer Inc.）


Total NME Approvals: 9
新药（NME）获批总数：9


Details of the company with the largest number of approvals: Approval date drug trade name drug generic name 2021-11-05 Paxlovid™ nirmatrelvir/ritonavir, 2022-05-25 Cibinqo™ abrocitinib, 2023-01-30 Zavzpret® zavegepant, 2023-05-25 Paxlovid nirmatrelvir/ritonavir, 2023-06-05 Litfulo ritlegepitinib, 2023-08-22 Penbraya™ pentavalent meningococcal, 2023-10-12 Velsipity ${}^{\mathrm{{TM}}}$ etrasimod, 2024-03-14 Rezdiffra* resmetirom, 2023-03-09 Za-vzepant* zavegepant
获批数量最多的公司详情：批准日期 药品商品名 药品通用名 2021-11-05 Paxlovid™ nirmatrelvir/ritonavir, 2022-05-25 Cibinqo™ abrocitinib, 2023-01-30 Zavzpret® zavegepant, 2023-05-25 Paxlovid nirmatrelvir/ritonavir, 2023-06-05 Litfulo ritlegepitinib, 2023-08-22 Penbraya™ pentavalent meningococcal, 2023-10-12 Velsipity ${}^{\mathrm{{TM}}}$ etrasimod, 2024-03-14 Rezdiffra* resmetirom, 2023-03-09 Za-vzepant* zavegepant


Fastest Blockbuster Drug: Paxlovid (nirmatrelvir/ritonavir)
增长最快的重磅药：Paxlovid（nirmatrelvir/ritonavir）


Indication: treatment of mild-to-moderate COVID-19 in adults and pediatric patients (12 years of age and older weighing at least ${40}\mathrm{\;{kg}}$ ) who are at high risk for progression to severe COVID-19
适应症：用于治疗轻至中度 COVID-19 的成人和儿科患者（12 岁及以上且体重至少为 ${40}\mathrm{\;{kg}}$）中有进展为重症高风险者


Peak Annual Sales: \$18.933 billion (2022)
年峰值销售额：189.33 亿美元（2022）


Lead Scientist: Dafydd Owen
首席科学家：Dafydd Owen


Specialization: medicinal chemist in the design and synthesis of drug-like molecules
专长：药物化学家，专注于类药分子的设计与合成


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_19_27_88b24d.jpg"/>



Figure 10: Human demonstration screenshots for the technology and innovation web task example.
图 10：技术与创新网络任务示例的人类演示截图。


## Subtask 1 Instruction
## 子任务 1 说明


Compile a statistical summary of all FDA-approved NMEs (2020-2024), identify the company with the highest number of approvals, and report its approved drugs with both brand and generic names.
汇编所有 FDA 批准的新药（2020-2024）统计摘要，找出获批数量最多的公司，并报告其获批药物的商品名与通用名。


## Subtask 1 Goal
## 子任务 1 目标


The ranking of the number of NME approvals of the company:
公司新药获批数量排名：


<table><tr><td></td><td>Company name</td><td>Approved quantity</td></tr><tr><td>1</td><td>Pfizer Inc.</td><td>9</td></tr><tr><td>2</td><td>Novartis Pharmaceuticals</td><td>7</td></tr><tr><td>3</td><td>Bristol Myers Squibb</td><td>6</td></tr><tr><td>4</td><td>Merck Sharp & Dohme</td><td>5</td></tr><tr><td>5</td><td>Takeda Pharmaceuticals</td><td>4</td></tr><tr><td>6</td><td>Eli Lilly and Company</td><td>3</td></tr></table>
<table><tbody><tr><td></td><td>公司名称</td><td>批准数量</td></tr><tr><td>1</td><td>辉瑞公司</td><td>9</td></tr><tr><td>2</td><td>诺华制药</td><td>7</td></tr><tr><td>3</td><td>百时美施贵宝</td><td>6</td></tr><tr><td>4</td><td>默克夏普与多姆</td><td>5</td></tr><tr><td>5</td><td>武田制药</td><td>4</td></tr><tr><td>6</td><td>礼来公司</td><td>3</td></tr></tbody></table>


Company Name: Pfizer Inc.
公司名称：Pfizer Inc.


Total NME Approvals: 9
新药（NME）批准总数：9


Details of the company with the largest number of approvals:
批准数量最多的公司的详细信息：


<table><tr><td>Approval date</td><td>drug Trade Name</td><td>Generic Name</td></tr><tr><td>2021-11-05</td><td>Paxlovidi™</td><td>nirmatrelvir/ritonavir</td></tr><tr><td>2022-05-25</td><td>Cibinqo™</td><td>abrocitinib</td></tr><tr><td>2023-01-30</td><td>Zavzpret®</td><td>zavegepant</td></tr><tr><td>2023-05-25</td><td>Paxlovid</td><td>nirmatrelvir/ritonavir</td></tr><tr><td>2023-06-05</td><td>Litfulo</td><td>ritlegepitinib</td></tr><tr><td>2023-08-22</td><td>Penbraya™</td><td>pentavalent meningococcal</td></tr><tr><td>2023-10-12</td><td>Velsipity™</td><td>etrasimod</td></tr><tr><td>2024-03-14</td><td>Rezdiffra*</td><td>resmetirom</td></tr><tr><td>2023-03-09</td><td>Zavzepant*</td><td>zavegepant</td></tr></table>
<table><tbody><tr><td>批准日期</td><td>药品商品名</td><td>通用名</td></tr><tr><td>2021-11-05</td><td>Paxlovidi™</td><td>nirmatrelvir/ritonavir</td></tr><tr><td>2022-05-25</td><td>Cibinqo™</td><td>abrocitinib</td></tr><tr><td>2023-01-30</td><td>Zavzpret®</td><td>zavegepant</td></tr><tr><td>2023-05-25</td><td>Paxlovid</td><td>nirmatrelvir/ritonavir</td></tr><tr><td>2023-06-05</td><td>Litfulo</td><td>ritlegepitinib</td></tr><tr><td>2023-08-22</td><td>Penbraya™</td><td>五价脑膜炎球菌疫苗</td></tr><tr><td>2023-10-12</td><td>Velsipity™</td><td>etrasimod</td></tr><tr><td>2024-03-14</td><td>Rezdiffra*</td><td>resmetirom</td></tr><tr><td>2023-03-09</td><td>Zavzepant*</td><td>zavegepant</td></tr></tbody></table>


## Subtask 2 Instruction
## 子任务 2 指示


Among qualifying companies, identify Pfizer Inc. with the most FDA-approved NMEs and find which of their drugs reached blockbuster status fastest after approval and its peak annual sales figure.
在符合条件的公司中，确定拥有最多 FDA 批准 NME 的公司辉瑞公司，并找出其哪种药物在获批后最快达到重磅药地位及其年度最高销售额。


Subtask 2 Goal
子任务 2 目标


Fastest Blockbuster Drug: Paxlovid (nirmatrelvir/ritonavir)
最快成为重磅药的药物：Paxlovid（奈玛特韦/利托那韦）


Peak Annual Sales: \$18.933 billion (2022)
年度最高销售额：\$18.9330 十亿（2022）


## Subtask 3 Instruction
## 子任务 3 指示


Find the primary indication for Paxlovid (nirmatrelvir/ritonavir).
查找 Paxlovid（奈玛特韦/利托那韦）的主要适应症。


## Subtask 3 Goal
## 子任务 3 目标


Indication: treatment of mild-to-moderate COVID-19 in adults and pediatric patients (12 years of age and older weighing at least ${40}\mathrm{\;{kg}}$ ) who are at high risk for progression to severe COVID-19
适应症：用于治疗轻至中度 COVID-19 成人及 12 岁及以上且体重至少为 ${40}\mathrm{\;{kg}}$ 的儿科患者，这些患者有发展为重症 COVID-19 的高风险


## Subtask 4 Instruction
## 子任务 4 指示


Search for the lead scientist or principal investigator credited with discovering Paxlovid (nirmatrelvir/ritonavir), including their full name and area of specialization.
查找被记为发现 Paxlovid（奈玛特韦/利托那韦）的首席科学家或主要研究者，包括其全名和专长领域。


Subtask 4 Goal
子任务 4 目标


Lead Scientist: Dafydd Owen
首席科学家：Dafydd Owen


Specialization: medicinal chemist in the design and synthesis of drug-like molecules
专长：药物样分子的设计与合成方向的药物化学家


## Web Task Example - Arts and Entertainment
## 网络任务示例 - 艺术与娱乐


## Task Instruction
## 任务指示


Identify the film with the highest production cost return ratio (box office/production cost) among all movies that grossed over \$1 billion worldwide between 2020 and 2024, and list its title, director, production cost, global box office, main filming location, as well as the name of the highest-level film award it received and the city where the award ceremony was held.
在 2020 至 2024 年间全球票房超过 \$10 亿的所有电影中，找出制作成本回报率（票房/制作成本）最高的影片，并列出其片名、导演、制作成本、全球票房、主要拍摄地，以及其获得的最高级别电影奖项名称和颁奖城市。


## Task Goal
## 任务目标


Title: The Super Mario Bros. Movie
片名：超级马里奥兄弟大电影


Director: Aaron Horvath
导演：Aaron Horvath


Production cost: \$100,000,000
制作成本：\$100,000,000


Global box office: \$1,360,847,665
全球票房：\$1,360,847,665


Main filming location: Paris, France
主要拍摄地：法国，巴黎


The name of the highest-level film award it received and the city where the award ceremony was held: Festival Film Bandung - Film Impor Terpuji / Commendable Imported Film, Bandung, Indonesia
其获得的最高级别电影奖项名称及颁奖城市：Festival Film Bandung - Film Impor Terpuji / Commendable Imported Film，印度尼西亚，万隆


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_19_27_c23fe2.jpg"/>



Figure 11: Human demonstration screenshots for the arts and entertainment web task example.
图 11：艺术与娱乐网络任务示例的人类演示截图。


## Subtask 1 Instruction
## 子任务 1 指令


Collect a list of all films worldwide with box office earnings exceeding \$1 billion from 2020
收集自 2020 年起全球所有票房超过 \$10 亿的电影名单


to 2024, along with their box office data.
及其票房数据。


Subtask 1 Goal
子任务 1 目标


Avatar: The Way of Water: \$2,320,250,281
阿凡达：水之道：\$2,320,250,281


Inside Out 2: \$1,698,863,816
头脑特工队 2：\$1,698,863,816


Spider-Man: No Way Home: \$1,922,598,800
蜘蛛侠：英雄无归：\$1,922,598,800


Top Gun: Maverick: \$1,495,696,292
壮志凌云：独行侠：$1,495,696,292


Barbie: \$1,447,038,421
芭比：$1,447,038,421


The Super Mario Bros. Movie: \$1,360,847,665
超级马力欧兄弟大电影：$1,360,847,665


Deadpool & Wolverine: \$1,338,073,645
死侍与金刚狼：$1,338,073,645


Moana 2: \$1,059,242,164
莫阿娜2：$1,059,242,164


## Subtask 2 Instruction
## 子任务2 指令


Search the production cost of each film, and calculate the ratio of box office to production cost to identify the film with the highest return on investment. List only the highest-rated movies and their ratios.
搜索每部影片的制作成本，计算票房与制作成本的比率，找出投资回报率最高的影片。仅列出最高的影片及其比率。


## Subtask 2 Goal
## 子任务2 目标


The Super Mario Bros. Movie, 13.61
超级马力欧兄弟大电影，13.61


## Subtask 3 Instruction
## 子任务3 指令


Find the director's name of The Super Mario Bros. Movie, the specific production cost, and the exact global box office revenue.
找出《超级马力欧兄弟大电影》的导演姓名、具体制作成本和确切的全球票房收入。


## Subtask 3 Goal
## 子任务3 目标


Aaron Horvath, \$100,000,000, \$1,360,847,665
Aaron Horvath，$100,000,000，$1,360,847,665


## Subtask 4 Instruction
## 子任务4 指令


Search for the main filming locations of The Super Mario Bros. Movie.
搜索《超级马力欧兄弟大电影》的主要拍摄地。


## Subtask 4 Goal
## 子任务4 目标


Paris, France
巴黎，法国


## Subtask 5 Instruction
## 子任务 5 指令


Find all the film awards that The Super Mario Bros. Movie has received, identify the highest-level award among them, and find the host city of the corresponding award ceremony.
找出《超级玛丽兄弟大电影》获得的所有影展/电影奖项，确定其中级别最高的奖项，并找出该颁奖/典礼的主办城市。


## Subtask 5 Goal
## 子任务 5 目标


Festival Film Bandung - Film Impor Terpuji / Commendable Imported Film, Bandung, Indonesia
Festival Film Bandung - Film Impor Terpuji / 值得表扬的进口影片，印度尼西亚，万隆


## Web Task Example - Social Policy and Sustainability
## 网络任务示例 - 社会政策与可持续性


## Task Instruction
## 任务说明


Identify the G20 country that achieved the largest percentage decrease in CO2 emissions per capita between 2015 and 2023 while simultaneously recording a real GDP growth of over 20% in the same period. List the country's name, its official head of government as of year-end 2023, the primary renewable energy source by installed capacity, and the official title of its most recent Nationally Determined Contribution (NDC) report submitted to the UNFCCC.
确定在 2015 到 2023 年间人均二氧化碳排放量下降比例最大的 G20 国家，并且在同一期间实际 GDP 增长超过 20%。列出该国名称、截至 2023 年底的实际政府首脑、按装机容量计的主要可再生能源来源，以及其最近一次向 UNFCCC 提交的国家自主贡献（NDC）报告的正式标题。


## Task Goal
## 任务目标


The country's name: UK
该国名称：英国


UK's official head of government as of year-end 2023: Rishi Sunak
截至 2023 年底该国的实际政府首脑：里希·苏纳克


The primary renewable energy source by installed capacity: wind sources
按装机容量计的主要可再生能源来源：风能


The official title of its most recent Nationally Determined Contribution (NDC) report sub-
其最近一次向 UNFCCC 提交的国家自主贡献（NDC）报告的正式标题（部分）：


mitted to the UNFCCC: United Kingdom of Great Britain and Northern Ireland's 2035 Nationally Determined Contribution
提交给 UNFCCC 的标题：大不列颠及北爱尔兰联合王国 2035 年国家自主贡献


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_19_27_5c0264.jpg"/>



Figure 12: Human demonstration screenshots for the social policy and sustainability web task example.
图 12：社会政策与可持续性网络任务示例的人类示范屏幕截图。


## Subtask 1 Instruction
## 子任务 1 指令


Identify the G20 country that achieved a real GDP growth of over 20% between 2015 and 2023.
确定在2015年至2023年间实际 GDP 增长超过20%的 G20 国家。


## Subtask 1 Goal
## 子任务1 目标


USA, China, india, UK, Brazil, Russia, Canada, Mexico, Indonesia, Turkey, Saudi Arabia, Argentina
美国, 中国, 印度, 英国, 巴西, 俄罗斯, 加拿大, 墨西哥, 印度尼西亚, 土耳其, 沙特阿拉伯, 阿根廷


## Subtask 2 Instruction
## 子任务2 指令


Identify the country with the largest percentage decrease in CO2 emissions per capita.
找出人均二氧化碳排放量下降幅度最大的国家。


## Subtask 2 Goal
## 子任务2 目标


The country name: UK
国家名称：英国


## Subtask 3 Instruction
## 子任务3 指令


Find the full name of the head of government for UK, who was in office on December 31, 2023.
查找在2023年12月31日仍在任的英国政府首脑的全名。


## Subtask 3 Goal
## 子任务3 目标


The full name: Rishi Sunak
全名：Rishi Sunak


## Subtask 4 Instruction
## 子任务4 指令


Research the energy profile of UK to determine its primary renewable energy source based on the latest available data for installed capacity (in MW or GW).
研究英国的能源结构，以已安装容量（MW 或 GW）最新可得数据为准，确定其主要可再生能源来源。


## Subtask 4 Goal
## 子任务4 目标


UK's primary renewable energy source: wind sources
英国的主要可再生能源来源：风能


## Subtask 5 Instruction
## 子任务5 指令


Search the official UNFCCC registry or UK's national environmental ministry website to find its most recently submitted Nationally Determined Contribution (NDC) report. Record its full official title and the year it was published/submitted.
在 UNFCCC 官方登记处或英国国家环境部网站上查找其最近提交的《国家自主贡献（NDC）》报告。记录其完整官方标题及发布/提交年份。


## Subtask 5 Goal
## 子任务 5 目标


The official title of its most recent Nationally Determined Contribution (NDC) report submitted to the UNFCCC: United Kingdom of Great Britain and Northern Ireland's 2035 Nationally Determined Contribution
其最近一次提交给 UNFCCC 的国家自主贡献（NDC）报告的官方标题：大不列颠及北爱尔兰联合王国 2035 年国家自主贡献