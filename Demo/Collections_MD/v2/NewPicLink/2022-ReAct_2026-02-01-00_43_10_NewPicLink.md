# REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS
# ReAct：在语言模型中协同推理与行动


Shunyu Yao ${}^{*,1}$ , Jeffrey Zhao ${}^{2}$ , Dian Yu ${}^{2}$ , Nan Du ${}^{2}$ , Izhak Shafran ${}^{2}$ , Karthik Narasimhan ${}^{1}$ , Yuan Cao ${}^{2}$
Shunyu Yao ${}^{*,1}$ , Jeffrey Zhao ${}^{2}$ , Dian Yu ${}^{2}$ , Nan Du ${}^{2}$ , Izhak Shafran ${}^{2}$ , Karthik Narasimhan ${}^{1}$ , Yuan Cao ${}^{2}$


${}^{1}$ Department of Computer Science,Princeton University ${}^{2}$ Google Research,Brain team
${}^{1}$ 普林斯顿大学计算机科学系 ${}^{2}$ Google Research, Brain 团队


${}^{1}\{$ shunyuy,karthikn $\}$ @princeton.edu
${}^{1}\{$ shunyuy,karthikn $\}$ @princeton.edu


${}^{2}$ \{jeffreyzhao,dianyu,dunan,izhak,yuancao\}@google.com
${}^{2}$ \{jeffreyzhao,dianyu,dunan,izhak,yuancao\}@google.com


## ABSTRACT
## 摘要


While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of ${34}\%$ and ${10}\%$ respectively,while being prompted with only one or two in-context examples.
尽管大语言模型（LLM）在语言理解和交互式决策任务中表现出色，但其推理能力（如思维链提示）和行动能力（如行动计划生成）主要作为独立课题研究。本文探讨了利用 LLM 以交替方式生成推理轨迹和特定任务行动，从而实现两者的协同：推理轨迹帮助模型诱导、跟踪和更新行动计划并处理异常，而行动则使其能够与外部源（如知识库或环境）交互并收集额外信息。我们将这种命名为 ReAct 的方法应用于多样的语言和决策任务，证明其效果优于最先进的基线方法，并提高了人类可解释性和可信度。具体而言，在问答（HotpotQA）和事实核查（Fever）中，ReAct 通过与简单的维基百科 API 交互，克服了思维链推理中普遍存在的幻觉和错误传播问题，生成的类人任务解决轨迹比无推理轨迹的基线更具可解释性。此外，在两个交互式决策基准（ALFWorld 和 WebShop）中，ReAct 仅通过一两个上下文示例提示，其成功率就分别以 ${34}\%$ 和 ${10}\%$ 的绝对优势超过了模仿学习和强化学习方法。


## 1 INTRODUCTION
## 1 引言


A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with verbal reasoning (or inner speech, Alderson-Day & Fernyhough 2015), which has been theorized to play an important role in human cognition for enabling self-regulation or strategization (Vygotsky, 1987; Luria, 1965; Fernyhough 2010) and maintaining a working memory (Baddeley, 1992). Consider the example of cooking up a dish in the kitchen. Between any two specific actions, we may reason in language in order to track progress ("now that everything is cut, I should heat up the pot of water"), to handle exceptions or adjust the plan according to the situation ("I don't have salt, so let me use soy sauce and pepper instead"), and to realize when external information is needed ("how do I prepare dough? Let me search on the Internet"). We may also act (open a cookbook to read the recipe, open the fridge, check ingredients) to support the reasoning and to answer questions ("What dish can I make right now?"). This tight synergy between "acting" and "reasoning" allows humans to learn new tasks quickly and perform robust decision making or reasoning, even under previously unseen circumstances or facing information uncertainties.
人类智能的一个独特特征是能将导向任务的行动与语言推理（或内部语言，Alderson-Day &amp; Fernyhough 2015）无缝结合。理论认为，这在人类认知中发挥着重要作用，通过自我调节或策略化（Vygotsky, 1987; Luria, 1965; Fernyhough 2010）以及维持工作记忆（Baddeley, 1992）来实现。以在厨房做饭为例：在任意两个具体行动之间，我们可能会用语言进行推理，以跟踪进度（“既然切好了，我该烧锅水了”），处理异常或根据情况调整计划（“没有盐了，我用酱油和胡椒代替”），以及意识到何时需要外部信息（“怎么发面？我上网搜搜”）。我们也会采取行动（打开食谱阅读配方、打开冰箱、检查食材）来支持推理并回答问题（“我现在能做什么菜？”）。这种“行动”与“推理”之间的紧密协同，使人类能够快速学习新任务，并在从未见过的环境或面临信息不确定性时，做出稳健的决策或推理。


Recent results have hinted at the possibility of combining verbal reasoning with interactive decision making in autonomous systems. On one hand, properly prompted large language models (LLMs) have demonstrated emergent capabilities to carry out several steps of reasoning traces to derive answers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al. 2022). However, this "chain-of-thought" reasoning is a static black box, in that the model uses its own internal representations to generate thoughts and is not grounded in the external world, which limits its ability to reason reactively or update its knowledge. This can lead to issues like fact hallucination and error propagation over the reasoning process (Figure 1(1b)). On the other hand, recent work has explored the use of pre-trained language models for planning and acting in interactive environments (Ahn et al. 2022; Nakano et al. 2021; Yao et al. 2020; Huang et al. 2022a), with a focus on predicting actions via language priors. These approaches usually convert multi-modal observations into text, use a language model to generate domain-specific actions or plans, and then use a controller to choose or execute them. However, they do not employ language models to reason abstractly about high-level goals or maintain a working memory to support acting, barring Huang et al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the current state. Beyond such simple embodied tasks to interact with a few blocks, there have not been studies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic benefits compared to reasoning or acting alone.
最近的研究结果暗示了在自主系统中结合语言推理与交互式决策的可能性。一方面，经过适当提示的大语言模型（LLMs）已展现出涌现能力，能够通过多步推理迹从算术、常识和符号推理任务的问题中推导出答案（Wei et al. 2022）。然而，这种“思维链”推理是一个静态黑盒，模型使用其内部表示来生成想法，且未与外部世界挂钩，这限制了其进行反应式推理或更新知识的能力。这可能导致事实幻觉和推理过程中的错误传播等问题（图1(1b)）。另一方面，最近的工作探索了使用预训练语言模型在交互式环境中进行规划和行动（Ahn et al. 2022; Nakano et al. 2021; Yao et al. 2020; Huang et al. 2022a），重点是通过语言先验预测动作。这些方法通常将多模态观测转化为文本，利用语言模型生成特定领域的动作或计划，然后通过控制器进行选择或执行。然而，它们并没有利用语言模型对高层目标进行抽象推理，或维持工作记忆以支持行动，除了 Huang et al. (2022b) 曾执行过一种有限形式的语言推理来重申当前状态的空间事实。除了这类与少量方块交互的简单具身任务外，目前尚未有研究探讨在通用任务解决中如何以协同方式结合推理与行动，以及这种结合相比单纯的推理或行动是否能带来系统性的收益。


---



*Work during Google internship. Projet page with code: https://react-lm.github.io/
*在 Google 实习期间的工作。包含代码的项目页面：https://react-lm.github.io/


---



<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_16_23_140d63.jpg"/>



Figure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought (CoT, Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al. 2018) question. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar et al. 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task solving trajectories generated by the model (Act, Thought) and the environment (Obs).
图 1：(1) 四种提示方法的比较：(a) 标准，(b) 思维链（CoT，仅推理），(c) 仅行动，以及 (d) ReAct（推理+行动），解决一个 HotpotQA (Yang et al. 2018) 问题。(2) 解决 AlfWorld (Shridhar et al. 2020b) 游戏时，(a) 仅行动与 (b) ReAct 提示的比较。在这两个领域中，我们省略了提示中的上下文示例，仅展示由模型（动作、想法）和环境（观测）生成的任务解决轨迹。


In this work, we present ReAct, a general paradigm to combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).
在这项工作中，我们提出了 ReAct，一种利用语言模型将推理与行动相结合的通用范式，用于解决多样化的语言推理和决策任务（图 1）。ReAct 提示 LLMs 以交错的方式生成与任务相关的语言推理迹和动作，这使模型能够执行动态推理，以创建、维护和调整高层的行动计划（推理以行动），同时与外部环境（如维基百科）交互，将额外信息纳入推理（行动以推理）。


We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks: question answering (HotPotQA, Yang et al. 2018), fact verification (Fever, Thorne et al. 2018), text-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao et al. 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact with, ReAct outperforms vanilla action generation models while being competitive with chain-of-thought reasoning (CoT) (Wei et al. 2022). The best approach overall is a combination of ReAct and $\mathrm{C} \circ  \mathrm{T}$ that allows for the use of both internal knowledge and externally obtained information during reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able to outperform imitation or reinforcement learning methods trained with ${10}^{3} \sim  {10}^{5}$ task instances, with an absolute improvement of ${34}\%$ and ${10}\%$ in success rates respectively. We also demonstrate the importance of sparse, versatile reasoning in decision making by showing consistent advantages over controlled baselines with actions only. Besides general applicability and performance boost, the combination of reasoning and acting also contributes to model interpretability, trustworthiness, and diagnosability across all domains, as humans can readily distinguish information from model's internal knowledge versus external environments, as well as inspect reasoning traces to understand the decision basis of model actions.
我们在四个不同的基准测试中对 ReAct 和最先进的基准模型进行了实证评估：问答（HotPotQA, Yang et al. 2018）、事实核查（Fever, Thorne et al. 2018）、文本游戏（ALFWorld, Shridhar et al., 2020b）和网页导航（WebShop, Yao et al. 2022）。对于 HotPotQA 和 Fever，通过访问可交互的维基百科 API，ReAct 的表现优于纯动作生成模型，同时与思维链推理（CoT） (Wei et al. 2022) 具有竞争力。整体最佳方法是 ReAct 与 $\mathrm{C} \circ  \mathrm{T}$ 的结合，这允许在推理过程中同时使用内部知识和外部获取的信息。在 ALFWorld 和 WebShop 上，仅需两步甚至一步的 ReAct 提示就能超越使用 ${10}^{3} \sim  {10}^{5}$ 任务实例训练的模仿学习或强化学习方法，成功率分别实现了 ${34}\%$ 和 ${10}\%$ 的绝对提升。我们还通过展示其相对于仅含动作的受控基准模型的持续优势，证明了稀疏、通用的推理在决策中的重要性。除了通用适用性和性能提升外，推理与行动的结合还有助于提升模型在所有领域中的可解释性、可信度和可诊断性，因为人类可以轻易区分来自模型内部知识与外部环境的信息，并检查推理迹以理解模型动作的决策依据。


To summarize, our key contributions are the following: (1) we introduce ReAct, a novel prompt-based paradigm to synergize reasoning and acting in language models for general task solving; (2) we perform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a few-shot learning setup over prior approaches that perform either reasoning or action generation in isolation; (3) we present systematic ablations and analysis to understand the importance of acting in reasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the prompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial finetuning experiments showing the potential of ReAct to improve with additional training data. Scaling up ReAct to train and operate on more tasks and combining it with complementary paradigms like reinforcement learning could further unlock the potential of large language models.
总而言之，我们的主要贡献如下：(1) 我们引入了 ReAct，这是一种新颖的基于提示的范式，旨在通用任务解决中协同语言模型的推理与行动；(2) 我们在不同基准测试中进行了广泛实验，展示了在少样本学习设置下，ReAct 相比以往孤立进行推理或动作生成的策略具有明显优势；(3) 我们进行了系统性的消融实验和分析，以理解行动在推理任务中的重要性以及推理在交互式任务中的重要性；(4) 我们分析了 ReAct 在提示设置下的局限性（即对推理和行动行为的支持有限），并进行了初步的微调实验，展示了 ReAct 通过额外训练数据提升性能的潜力。扩展 ReAct 以在更多任务上训练和运行，并将其与强化学习等互补范式相结合，可以进一步释放大语言模型的潜力。


## 2 REACT: SYNERGIZING REASONING + ACTING
## 2 REACT：协同推理 + 行动


Consider a general setup of an agent interacting with an environment for task solving. At time step $t$ ,an agent receives an observation ${o}_{t} \in  \mathcal{O}$ from the environment and takes an action ${a}_{t} \in  \mathcal{A}$ following some policy $\pi \left( {{a}_{t} \mid  {c}_{t}}\right)$ ,where ${c}_{t} = \left( {{o}_{1},{a}_{1},\cdots ,{o}_{t - 1},{a}_{t - 1},{o}_{t}}\right)$ is the context to the agent. Learning a policy is challenging when the mapping ${c}_{t} \mapsto  {a}_{t}$ is highly implicit and requires extensive computation. For example, the agent shown in Figure 1(1c) is unable to generate the correct final action (Act 4) to finish the QA task as it requires complex reasoning over the trajectory context (Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the context that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.
考虑智能体与环境交互以解决任务的通用设置。在时间步 $t$，智能体从环境接收观察结果 ${o}_{t} \in  \mathcal{O}$，并根据某种策略 ${a}_{t} \in  \mathcal{A}$ 采取动作 $\pi \left( {{a}_{t} \mid  {c}_{t}}\right)$，其中 ${c}_{t} = \left( {{o}_{1},{a}_{1},\cdots ,{o}_{t - 1},{a}_{t - 1},{o}_{t}}\right)$ 是智能体所处的上下文。当映射 ${c}_{t} \mapsto  {a}_{t}$ 极其隐晦且需要大量计算时，学习策略极具挑战性。例如，图 1(1c) 所示的智能体无法生成正确的最终动作（Act 4）来完成问答任务，因为它需要对轨迹上下文（问题、Act 1-3、Obs 1-3）进行复杂推理。同样，图 1(2a) 所示的智能体无法从上下文中理解洗手盆 1 中不包含调味瓶 1，从而不断产生幻觉动作。


The idea of ReAct is simple: we augment the agent’s action space to $\widehat{\mathcal{A}} = \mathcal{A} \cup  \mathcal{L}$ ,where $\mathcal{L}$ is the space of language. An action ${\widehat{a}}_{t} \in  \mathcal{L}$ in the language space,which we will refer to as a thought or a reasoning trace, does not affect the external environment, thus leading to no observation feedback. Instead,a thought ${\widehat{a}}_{t}$ aims to compose useful information by reasoning over the current context ${c}_{t}$ . and update the context ${c}_{t + 1} = \left( {{c}_{t},{\widehat{a}}_{t}}\right)$ to support future reasoning or acting. As shown in Figure 1 there could be various types of useful thoughts, e.g. decomposing task goals and create action plans (2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1), extracting important parts from observations (1d, Thought2, 4), track progress and transit action plans (2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.
ReAct 的思想很简单：我们将智能体的动作空间扩展为 $\widehat{\mathcal{A}} = \mathcal{A} \cup  \mathcal{L}$，其中 $\mathcal{L}$ 是语言空间。语言空间中的动作 ${\widehat{a}}_{t} \in  \mathcal{L}$（我们称之为想法或推理轨迹）不会影响外部环境，因此不会产生观察反馈。相反，想法 ${\widehat{a}}_{t}$ 旨在通过对当前上下文 ${c}_{t}$ 进行推理来组合有用信息，并更新上下文 ${c}_{t + 1} = \left( {{c}_{t},{\widehat{a}}_{t}}\right)$ 以支持后续的推理或行动。如图 1 所示，可能存在各种类型的有用想法，例如：分解任务目标并制定行动计划（2b, Act 1; 1d, Thought 1）、注入与任务解决相关的常识知识（2b, Act 1）、从观察中提取重要部分（1d, Thought 2, 4）、追踪进度并转换行动计划（2b, Act 8）、处理异常并调整行动计划（1d, Thought 3）等等。


However,as the language space $\mathcal{L}$ is unlimited,learning in this augmented action space is difficult and requires strong language priors. In this paper, we mainly focus on the setup where a frozen large language model, PaLM-540B (Chowdhery et al. 2022) examples to generate both domain-specific actions and free-form language thoughts for task solving (Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision making tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to appear sparsely in the most relevant positions of a trajectory, so we let the language model decide the asynchronous occurrence of thoughts and actions for itself.
然而，由于语言空间 $\mathcal{L}$ 是无限的，在这个增强的动作空间中进行学习非常困难，且需要强大的语言先验。在本文中，我们主要关注使用冻结的大语言模型 PaLM-540B (Chowdhery et al. 2022) 示例来生成特定领域的动作和自由形式的语言想法以解决任务的设置（图 1 (1d), (2b)）。每个上下文示例都是由人类编写的解决任务实例的动作、想法和环境观察轨迹（参见附录 C）。对于推理至关重要的任务（图 1(1)），我们交替生成想法和动作，使任务解决轨迹由多个“想法-动作-观察”步骤组成。相比之下，对于可能涉及大量动作的决策任务（图 1(2)），想法只需稀疏地出现在轨迹中最相关的部分，因此我们让语言模型自行决定想法和动作的异步发生。


---



${}^{1}$ We show some GPT-3 (Brown et al. 2020) results in Appendix A.1 which outperforms PaLM-540B.
${}^{1}$ 我们在附录 A.1 中展示了一些 GPT-3 (Brown et al. 2020) 的结果，其表现优于 PaLM-540B。


---



Since decision making and reasoning capabilities are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is straightforward as human annotators just type down their thoughts in language on top of their actions taken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail prompt design for each task in Sections 3 and 4 B) General and flexible: Due to the flexible thought space and thought-action occurrence format, ReAct works for diverse tasks with distinct action spaces and reasoning needs, including but not limited to QA, fact verification, text game, and web navigation. C) Performant and robust: ReAct shows strong generalization to new task instances while learning solely from one to six in-context examples, consistently outperforming baselines with only reasoning or acting across different domains. We also show in Section 3 additional benefits when finetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections. D) Human aligned and controllable: ReAct promises an interpretable sequential decision making and reasoning process where humans can easily inspect reasoning and factual correctness. Moreover, humans can also control or correct the agent behavior on the go by thought editing, as shown in Figure 5 in Section 4
由于决策和推理能力被整合到大型语言模型中，ReAct 具有几个独特的特点：A) 直观且易于设计：设计 ReAct 提示非常直接，因为人类标注者只需在采取的行动之上用语言写下他们的想法。本文不使用特定的格式选择、思维设计或示例筛选。我们在第 3 和第 4 节详细介绍了每个任务的提示设计。B) 通用且灵活：由于灵活的思维空间和“思维-行动”发生格式，ReAct 适用于具有不同行动空间和推理需求的多种任务，包括但不限于问答、事实核实、文本游戏和网页导航。C) 高性能且稳健：ReAct 在仅从一到六个上下文示例中学习时，对新任务实例表现出强大的泛化能力，在不同领域中始终优于仅推理或仅行动的基线。我们还在第 3 节展示了启用微调时的额外收益，并在第 4 节展示了 ReAct 性能对提示选择的稳健性。D) 符合人类直觉且可控：ReAct 承诺了一个可解释的序列决策和推理过程，人类可以轻松检查其推理和事实的正确性。此外，人类还可以通过编辑思维随时控制或纠正智能体的行为，如第 4 节图 5 所示。


## 3 KNOWLEDGE-INTENSIVE REASONING TASKS
## 3 知识密集型推理任务


We begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact verification. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to retrieve information to support reasoning, while also use reasoning to target what to retrieve next, demonstrating a synergy of reasoning and acting.
我们从知识密集型推理任务开始，如多跳问答和事实核实。如图 1(1d) 所示，通过与 Wikipedia API 交互，ReAct 能够检索信息以支持推理，同时利用推理来确定下一步检索的目标，展示了推理与行动的协同作用。


### 3.1 SETUP
### 3.1 实验设置


Domains We consider two datasets challenging knowledge retrieval and reasoning: (1) Hot-PotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning over two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact verification benchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based on if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only setup for both tasks, where models only receive the question/claim as input without access to support paragraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with an external environment to support reasoning.
领域 我们考虑两个具有挑战性的知识检索和推理数据集：(1) HotpotQA (Yang et al., 2018)，一个需要对两个或多个 Wikipedia 段落进行推理的多跳问答基准；(2) FEVER (Thorne et al., 2018)，一个事实核实基准，每个主张根据是否存在可验证该主张的 Wikipedia 段落被标注为“支持”、“反驳”或“信息不足”。在这项工作中，我们对这两个任务均采用“仅问题”设置，模型仅接收问题/主张作为输入，无法访问支撑段落，必须依靠其内部知识或通过与外部环境交互检索知识来支持推理。


Action Space We design a simple Wikipedia web API with three types of actions to support interactive information retrieval: (1) search[entity], which returns the first 5 sentences from the corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the Wikipedia search engine, (2) lookup[string], which would return the next sentence in the page containing string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which would finish the current task with answer. We note that this action space mostly can only retrieve a small part of a passage based on exact passage name, which is significantly weaker than state-of-the-art lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia, and force models to retrieve via explicit reasoning in language.
行动空间 我们设计了一个简单的 Wikipedia 网页 API，包含三种类型的行动以支持交互式信息检索：(1) search[entity]，如果对应的实体百科页面存在，则返回前 5 句，否则从 Wikipedia 搜索引擎推荐前 5 个相似实体；(2) lookup[string]，返回页面中包含字符串的下一句，模拟浏览器上的 Ctrl+F 功能；(3) finish[answer]，以答案结束当前任务。我们注意到，该行动空间大多只能根据确切的段落名称检索段落的一小部分，其性能显著弱于目前最先进的词法或神经检索器。其目的是模拟人类与 Wikipedia 的交互方式，并迫使模型通过显式的语言推理进行检索。


### 3.2 METHODS
### 3.2 方法


ReAct Prompting For HotpotQA and Fever,we randomly select 6 and 3 cases ${}^{2}$ from the training set and manually compose ReAct -format trajectories to use as few-shot exemplars in the prompts. Similar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense thought), where free-form thoughts are used for various purposes. Specifically, we use a combination of thoughts that decompose questions ("I need to search x, find y, then find z"), extract information from Wikipedia observations ("x was started in 1844", "The paragraph does not tell x"), perform commonsense ("x is not y, so z must instead be...") or arithmetic reasoning ("1844 < 1989"), guide search reformulation ("maybe I can search/look up x instead"), and synthesize the final answer ("...so the answer is x"). See Appendix C for more details.
ReAct 提示 对于 HotpotQA 和 Fever，我们从训练集中随机选择 6 个和 3 个案例 ${}^{2}$，并手动编写 ReAct 格式的轨迹，作为提示中的少样本示例。与图 1(d) 类似，每条轨迹由多个“思维-行动-观察”步骤（即密集思维）组成，其中自由格式的思维被用于各种目的。具体而言，我们结合使用了多种思维：分解问题（“我需要搜索 x，找到 y，然后找到 z”）、从 Wikipedia 观察中提取信息（“x 始于 1844 年”、“该段落没有提到 x”）、进行常识（“x 不是 y，所以 z 必须是……”）或算术推理（“1844 < 1989”）、指导搜索重构（“也许我可以改为搜索/查找 x”）以及综合最终答案（“……所以答案是 x”）。详见附录 C。


---



${}^{2}$ We find more examples do not improve performance.
${}^{2}$ 我们发现更多示例并不能提高性能。


---



<table><tr><td>Prompt Method</td><td>HotpotQA (EM)</td><td>Fever (Acc)</td></tr><tr><td>Standard</td><td>28.7</td><td>57.1</td></tr><tr><td>CoT Wei et al., 2022)</td><td>29.4</td><td>56.3</td></tr><tr><td>CoT-SC Wang et al., 2022a)</td><td>33.4</td><td>60.4</td></tr><tr><td>Act</td><td>25.7</td><td>58.9</td></tr><tr><td>ReAct</td><td>27.4</td><td>60.9</td></tr><tr><td>CoT-SC $\rightarrow$ ReAct</td><td>34.2</td><td>64.6</td></tr><tr><td>ReAct $\rightarrow$ CoT-SC</td><td>35.1</td><td>62.0</td></tr><tr><td>Supervised SoTA ${}^{b}$</td><td>67.5</td><td>89.5</td></tr></table>
<table><tbody><tr><td>提示方法</td><td>HotpotQA (EM)</td><td>Fever (Acc)</td></tr><tr><td>标准</td><td>28.7</td><td>57.1</td></tr><tr><td>CoT (Wei et al., 2022)</td><td>29.4</td><td>56.3</td></tr><tr><td>CoT-SC (Wang et al., 2022a)</td><td>33.4</td><td>60.4</td></tr><tr><td>Act</td><td>25.7</td><td>58.9</td></tr><tr><td>ReAct</td><td>27.4</td><td>60.9</td></tr><tr><td>CoT-SC $\rightarrow$ ReAct</td><td>34.2</td><td>64.6</td></tr><tr><td>ReAct $\rightarrow$ CoT-SC</td><td>35.1</td><td>62.0</td></tr><tr><td>有监督 SoTA ${}^{b}$</td><td>67.5</td><td>89.5</td></tr></tbody></table>


Table 1: PaLM-540B prompting results on HotpotQA and Fever.
表 1：PaLM-540B 在 HotpotQA 和 Fever 上的提示结果。


${}^{a}$ HotpotQA EM is 27.1,28.9,33.8 for Standard,CoT, CoT-SC in Wang et al. 2022b
${}^{a}$ 在 Wang 等人 2022b 的研究中，HotpotQA 的标准提示、CoT、CoT-SC 的 EM 分别为 27.1、28.9、33.8


${}^{b}$ Zhu et al. 2021 Lewis et al. 2020
${}^{b}$ Zhu 等人 2021；Lewis 等人 2020


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_16_23_5d4805.jpg"/>



Figure 2: PaLM-540B prompting results with respect to number of CoT-SC samples used.
图 2：PaLM-540B 提示结果随 CoT-SC 样本数量的变化。


Baselines We systematically ablate ReAct trajectories to build prompts for multiple baselines (with formats as Figure 1(1a-1c)): (a) Standard prompting (Standard), which removes all thoughts, actions, observations in ReAct trajectories. (b) Chain-of-thought prompting (CoT) (Wei et al. 2022), which removes actions and observations and serve as a reasoning-only baseline. We also build a self-consistency baseline (CoT-SC) (Wang et al., 2022a b) by sampling 21 CoT trajectories with decoding temperature 0.7 during inference and adopting the majority answer, which is found to consistently boost performance over CoT. (c) Acting-only prompt (Act), which removes thoughts in ReAct trajectories, loosely resembling how WebGPT (Nakano et al. 2021) interacts with the Internet to answer questions, though it operates on a different task and action space, and uses imitation and reinforcement learning instead of prompting.
基准方法 我们系统地消融 ReAct 轨迹，为多个基准方法构建提示（格式如图 1(1a-1c) 所示）：(a) 标准提示 (Standard)，移除 ReAct 轨迹中所有的思考、动作和观察。(b) 思维链提示 (CoT) (Wei 等人 2022)，移除动作和观察，作为纯推理基准。我们还通过在推理过程中以 0.7 的解码温度采样 21 条 CoT 轨迹并采用多数投票答案，构建了自一致性基准 (CoT-SC) (Wang 等人 2022a b)，该方法被发现能持续提升 CoT 的性能。(c) 纯动作提示 (Act)，移除 ReAct 轨迹中的思考，这与 WebGPT (Nakano 等人 2021) 通过互联网交互回答问题的方式略有相似，尽管它在不同的任务和动作空间上运行，且使用模仿学习和强化学习而非提示。


Combining Internal and External Knowledge As will be detail in Section 3.3, we observe that the problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT is more accurate in formulating reasoning structure but can easily suffer from hallucinated facts or thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide when to switch to the other method based on the following heuristics: A) ReAct $\rightarrow$ CoT-SC: when ReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for HotpotQA and FEVER respectively as we find more steps will not improve ReAct performance ${}^{3}$ B) CoT-SC $\rightarrow$ ReAct: when the majority answer among $n$ CoT-SC samples occurs less than $n/2$ times (i.e. internal knowledge might not support the task confidently), back off to ReAct.
结合内部和外部知识 正如 3.3 节所述，我们观察到 ReAct 展示的解题过程更具事实性和落地性，而 CoT 在构建推理结构上更准确，但容易受到事实幻觉或错误思考的影响。因此，我们建议结合 ReAct 和 CoT-SC，并让模型根据以下启发式策略决定何时切换到另一种方法：A) ReAct $\rightarrow$ CoT-SC：当 ReAct 未能在给定步数内返回答案时，退回到 CoT-SC。我们将 HotpotQA 和 FEVER 分别设置为 7 步和 5 步，因为我们发现增加步数并不能提高 ReAct 的性能 ${}^{3}$ B) CoT-SC $\rightarrow$ ReAct：当 $n$ 个 CoT-SC 样本中的多数答案出现次数少于 $n/2$ 次时（即内部知识可能无法自信地支撑任务），退回到 ReAct。


Finetuning Due to the challenge of manually annotating reasoning traces and actions at scale, we consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories with correct answers generated by ReAct (also for other baselines) to finetune smaller language models (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on input questions/claims. More details are in Appendix B.1
微调 由于大规模手动标注推理轨迹和动作具有挑战性，我们考虑采用类似于 Zelikman 等人 (2022) 的自举方法，利用 ReAct 生成的 3,000 条包含正确答案的轨迹（同样适用于其他基准方法）来微调较小的语言模型 (PaLM-8/62B)，使其能够根据输入的问项/主张解码出轨迹（所有的思考、动作、观察）。更多详情见附录 B.1


### 3.3 RESULTS AND OBSERVATIONS
### 3.3 结果与观察


ReAct outperforms Act consistently Table 1 shows HotpotQA and Fever results using PaLM- 540B as the base model with different prompting methods. We note that ReAct is better than Act on both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the final answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also confirm the benefit of reasoning traces for more informed acting.
ReAct 持续优于 Act 表 1 显示了使用 PaLM-540B 作为基础模型在不同提示方法下的 HotpotQA 和 Fever 结果。我们注意到 ReAct 在这两个任务上都优于 Act，证明了通过推理引导动作的价值，尤其是在综合最终答案方面，如图 1 (1c-d) 所示。微调结果 3 也证实了推理轨迹对更具信息量的动作大有裨益。


---



${}^{3}$ Of all trajectories with correct final answers,those with 7 steps on HotpotQA and 5 steps on FEVER only take up 0.84% and 1.33% respectively.
${}^{3}$ 在所有获得正确最终答案的轨迹中，HotpotQA 达到 7 步和 FEVER 达到 5 步的轨迹分别仅占 0.84% 和 1.33%。


---



<table><tr><td></td><td>Type</td><td>Definition</td><td>ReAct</td><td>CoT</td></tr><tr><td rowspan="2">Success</td><td>True positive</td><td>Correct reasoning trace and facts</td><td>94%</td><td>86%</td></tr><tr><td>False positive</td><td>Hallucinated reasoning trace or facts</td><td>6%</td><td>14%</td></tr><tr><td rowspan="4">Failure</td><td>Reasoning error</td><td>Wrong reasoning trace (including failing to recover from repetitive steps)</td><td>47%</td><td>16%</td></tr><tr><td>Search result error</td><td>Search return empty or does not contain useful information</td><td>23%</td><td>-</td></tr><tr><td>Hallucination</td><td>Hallucinated reasoning trace or facts</td><td>0%</td><td>56%</td></tr><tr><td>Label ambiguity</td><td>Right prediction but did not match the label precisely</td><td>29%</td><td>28%</td></tr></table>
<table><tbody><tr><td></td><td>类型</td><td>定义</td><td>ReAct</td><td>CoT</td></tr><tr><td rowspan="2">成功</td><td>真阳性</td><td>推理路径与事实均正确</td><td>94%</td><td>86%</td></tr><tr><td>假阳性</td><td>存在幻觉推理路径或事实</td><td>6%</td><td>14%</td></tr><tr><td rowspan="4">失败</td><td>推理错误</td><td>推理路径错误（包括未能从重复步骤中恢复）</td><td>47%</td><td>16%</td></tr><tr><td>搜索结果错误</td><td>搜索结果为空或不包含有用信息</td><td>23%</td><td>-</td></tr><tr><td>幻觉</td><td>存在幻觉推理路径或事实</td><td>0%</td><td>56%</td></tr><tr><td>标签歧义</td><td>预测正确但与标签不完全匹配</td><td>29%</td><td>28%</td></tr></tbody></table>


Table 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their percentages in randomly selected examples studied by human.
表 2：ReAct 和 CoT 在 HotpotQA 上的成功与失败模式类型，以及人工研究随机样本中的占比。


ReAct vs. CoT On the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly lags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only differ by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge is vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we randomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct and CoT respectively (thus 200 examples in total), and manually labeled their success and failure modes in Table 2 Some key observations are as follows:
ReAct 与 CoT：另一方面，ReAct 在 Fever 上的表现优于 CoT（60.9 vs. 56.3），而在 HotpotQA 上略逊于 CoT（27.4 vs. 29.4）。Fever 中关于 支持/反驳 的声明可能仅有细微差别（见附录 D.1），因此通过行动获取准确且最新的知识至关重要。为了更好地理解 ReAct 和 CoT 在 HotpotQA 上的行为差异，我们分别从 ReAct 和 CoT 中随机抽取了 50 条包含正确和错误答案（由 EM 判定）的轨迹（总计 200 个样本），并在表 2 中手动标注了它们的成功与失败模式。一些关键观察如下：


A) Hallucination is a serious problem for CoT, resulting in much higher false positive rate than ReAct (14% vs. 6%) in success mode, and make up its major failure mode (56%). In contrast, the problem solving trajectory of ReAct is more grounded, fact-driven, and trustworthy, thanks to the access of an external knowledge base.
A) 幻觉是 CoT 的严重问题，导致其在成功模式下的假阳性率远高于 ReAct（14% vs. 6%），且构成了其主要的失败模式（56%）。相比之下，由于可以访问外部知识库，ReAct 的解题轨迹更加务实、由事实驱动且更值得信赖。


B) While interleaving reasoning, action and observation steps improves ReAct's groundedness and trustworthiness, such a structural constraint also reduces its flexibility in formulating reasoning steps,leading to more reasoning error rate than $\mathrm{C} \circ  \mathrm{T}$ . we note that there is one frequent error pattern specific to ReAct, in which the model repetitively generates the previous thoughts and actions, and we categorize it as part of "reasoning error" as the model fails to reason about what the proper next action to take and jump out of the loop ${}^{4}$
B) 虽然交替进行推理、行动和观察步骤提高了 ReAct 的可靠性和可信度，但这种结构化限制也降低了其制定推理步骤的灵活性，导致推理错误率高于 $\mathrm{C} \circ  \mathrm{T}$。我们注意到 ReAct 特有一种频繁出现的错误模式，即模型重复生成之前的想法和行动，我们将其归类为“推理错误”，因为模型无法推理出下一步应采取的恰当行动并跳出循环 ${}^{4}$。


C) For ReAct, successfully retrieving informative knowledge via search is critical. Noninformative search, which counts for 23% of the error cases, derails the model reasoning and gives it a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between factuality and flexibility, which motivates our proposed strategies of combining two methods.
C) 对于 ReAct 而言，通过搜索成功检索到信息丰富的知识至关重要。无信息搜索占错误案例的 23%，它会干扰模型的推理，使其难以恢复和重构思路。这或许是事实性与灵活性之间预期的权衡，也激发了我们提出结合两种方法的策略。


We provide examples for each success and failure modes in Appendix E.1. We also find some HotpotQA questions may contain outdated answer labels, see Figure 4 for example.
我们在附录 E.1 中提供了每种成功和失败模式的示例。我们还发现某些 HotpotQA 问题可能包含过时的答案标签，示例见图 4。


ReAct + CoT-SC perform best for prompting LLMs Also shown in Table 1 the best prompting method on HotpotQA and Fever are ReAct $\rightarrow$ CoT-SC and CoT-SC $\rightarrow$ ReAct respectively. Furthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC samples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both significantly and consistently outperform CoT-SC across different number of samples, reaching CoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of properly combining model internal knowledge and external knowledge for reasoning tasks.
ReAct + CoT-SC 在提示 LLM 方面表现最佳：如表 1 所示，在 HotpotQA 和 Fever 上最佳的提示方法分别是 ReAct $\rightarrow$ CoT-SC 和 CoT-SC $\rightarrow$ ReAct。此外，图 2 显示了不同方法在不同 CoT-SC 样本数量下的表现。虽然两种 ReAct + CoT-SC 方法各在项任务中占优，但它们在不同样本量下均显著且稳定地优于 CoT-SC，仅需 3-5 个样本即可达到 CoT-SC 使用 21 个样本时的性能。这些结果表明，在推理任务中妥善结合模型内部知识与外部知识具有重要价值。


ReAct performs best for fine-tuning Figure 3 shows the scaling effect of prompting/finetuning four methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, prompting ReAct performs worst among four methods due to the difficulty to learn both reasoning and acting from in-context examples. However, when finetuned with just 3,000 examples, ReAct becomes the best method among the four, with PaLM-8B finetuned ReAct outperforming all PaLM-62B prompting methods, and PaLM-62B finetuned ReAct outperforming all 540B prompting methods. In contrast, finetuning Standard or CoT is significantly worse than finetuning ReAct or Act for both PaLM- 8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge facts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a more generalizable skill for knowledge reasoning. As all prompting methods are still significantly far from domain-specific state-of-the-art approaches (Table 1), we believe finetuning with more human-written data might be a better way to unleash the power of ReAct.
ReAct 在微调方面表现最佳：图 3 展示了在 HotpotQA 上对四种方法（Standard, CoT, Act, ReAct）进行提示/微调的扩展效应。对于 PaLM-8/62B，由于难以从上下文示例中同时学习推理和行动，提示 ReAct 在四种方法中表现最差。然而，仅通过 3,000 个示例进行微调后，ReAct 成为四种方法中效果最好的，微调后的 PaLM-8B ReAct 优于所有 PaLM-62B 提示方法，微调后的 PaLM-62B ReAct 优于所有 540B 提示方法。相比之下，对于 PaLM-8/62B，微调 Standard 或 CoT 的效果显著差于微调 ReAct 或 Act，因为前者本质上是教模型记忆（可能存在幻觉的）知识事实，而后者教模型如何通过（推理和）行动从维基百科获取信息，这是一种更通用的知识推理技能。由于所有提示方法仍与领域特定的先进方法（表 1）有显著差距，我们认为使用更多人工编写的数据进行微调可能是释放 ReAct 能力的更好途径。


---



${}^{4}$ We suspect that this could be due to the sub-optimal greedy decoding procedure,and future work using better decoding (e.g. beam search) might help address this issue.
${}^{4}$ 我们怀疑这可能是由于次优的贪婪解码过程造成的，未来工作使用更好的解码方式（如束搜索）可能有助于解决此问题。


---



<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_16_23_41e189.jpg"/>



Figure 3: Scaling results for prompting and finetuning on HotPotQA with ReAct (ours) and baselines.
图 3：在 HotPotQA 上使用 ReAct（本研究方法）和基准方法进行提示和微调的扩展结果。


## 4 DECISION MAKING TASKS
## 4 决策任务


We also test ReAct on two language-based interactive decision-making tasks, ALFWorld and WebShop, both of which feature complex environments that require agents to act over long horizons with sparse rewards, warranting the need for reasoning to act and explore effectively.
我们还在 ALFWorld 和 WebShop 这两个基于语言的交互式决策任务上对 ReAct 进行了测试。这两个任务都具有复杂的环境，要求智能体在稀疏奖励下进行长期操作，因此需要通过推理来有效地行动和探索。


ALFWorld ALFWorld (Shridhar et al. 2020b) (Figure 1(2)) is a synthetic text-based game designed to align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of tasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by navigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take paper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy more than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore systematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into ALFWorld is the need to determine likely locations for common household items (e.g. desklamps will likely be on desks, shelfs, or dressers), making this environment a good fit for LLMs to exploit their pretrained commonsense knowledge. To prompt ReAct, we randomly annotate three trajectories from the training set for each task type, where each trajectory includes sparse thoughts that (1) decompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via commonsense where to find an object and what to do with it. We show prompts used for ALFWorld in Appendix C.4 Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games in a task-specific setup. For robustness, we construct 6 prompts for each task type through each permutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using the same trajectories, but without thoughts - since task instances are randomly chosen from the training set, it favors neither ReAct nor Act and provides a fair and controlled comparison to test the importance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation learning agent trained on ${10}^{5}$ expert trajectories for each task type
ALFWorld ALFWorld (Shridhar et al. 2020b) (图 1(2)) 是一款合成的文本游戏，旨在与具身智能基准 ALFRED (Shridhar et al., 2020a) 保持一致。它包含 6 类任务，智能体需要通过文本动作（如：go to coffeetable 1, take paper 2, use desklamp 1）在模拟家庭环境中导航并交互，以实现高层目标（如：在台灯下检查纸张）。一个任务实例可能包含超过 50 个地点，且专家策略需执行 50 步以上才能解决，这对智能体规划和追踪子目标以及系统性探索（如：逐一检查所有桌子以寻找台灯）提出了挑战。特别是 ALFWorld 内置的一项挑战是需要确定常见家庭用品的可能位置（如：台灯通常在桌子、架子或梳妆台上），这使得该环境非常适合大语言模型（LLM）利用其预训练的常识知识。为了提示 ReAct，我们为每种任务类型从训练集中随机标注了三个轨迹，每个轨迹包含稀疏的思考，用于：(1) 分解目标，(2) 追踪子目标完成情况，(3) 确定下一个子目标，以及 (4) 通过常识推理在哪里寻找物体以及如何处理。我们在附录 C.4 中展示了用于 ALFWorld 的提示词。遵循 Shridhar et al. (2020b) 的方法，我们在特定任务设置下的 134 个未见过的评估游戏上进行评估。为了保证鲁棒性，我们通过对 3 条标注轨迹中的每 2 条进行排列组合，为每种任务类型构建了 6 个提示词。Act 提示词使用相同的轨迹构建，但不包含思考——由于任务实例是从训练集中随机选择的，它既不偏向 ReAct 也不偏向 Act，为测试稀疏思考的重要性提供了公平且受控的比较。对于基准模型，我们使用了 BUTLER (Shridhar et al., 2020b)，这是一种针对每种任务类型在 ${10}^{5}$ 条专家轨迹上训练的模仿学习智能体。


WebShop Can ReAct also interact with noisy real-world language environments for practical applications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping website environment with ${1.18}\mathrm{M}$ real-world products and ${12}\mathrm{k}$ human instructions. Unlike ALFWorld, Webshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions, and options crawled from Amazon), and requires an agent to purchase a product based on a user instruction (e.g. "I am looking for a nightstand with drawers. It should have a nickel finish, and priced lower than \$140") through web interactions (e.g. search "nightstand drawers", choose buttons such as "color: modern-nickel-white" or "back to search"). This task is evaluated by average score (percentage of desired attributes covered by the chosen product averaged across all episodes) and success rate (percentage of episodes where the chosen product satisfies all requirements) on 500 test instructions. We formulate Act prompts with actions to search, choose product, choose options, and buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy, and what products options are relevant to the instruction. See Table 6 for an example prompt, and Table 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method
WebShop ReAct 是否也能与真实的嘈杂语言环境交互以进行实际应用？我们调查了 WebShop (Yao et al., 2022)，这是一个最近提出的在线购物网站环境，包含 ${1.18}\mathrm{M}$ 个真实世界产品和 ${12}\mathrm{k}$ 条人类指令。与 ALFWorld 不同，Webshop 包含大量多样的结构化和非结构化文本（如从亚马逊爬取的产品标题、描述和选项），并要求智能体根据用户指令（如：“我正在寻找一个带抽屉的床头柜。它应该是镍色饰面，价格低于 140 美元”），通过网络交互（如：搜索 “nightstand drawers”，选择 “color: modern-nickel-white” 或 “back to search” 等按钮）购买产品。该任务通过 500 条测试指令的平均得分（所选产品包含的目标属性百分比在所有回合中的平均值）和成功率（所选产品满足所有要求的次数占比）进行评估。我们构建了包含搜索、选择产品、选择选项和购买等动作的 Act 提示词，而 ReAct 提示词则额外增加了推理，以确定探索内容、购买时机以及哪些产品选项与指令相关。示例提示词见表 6，模型预测结果见附录表 10。我们将其与模仿学习 (IL) 方法进行了比较。


---



Micheli & Fleuret (2021) finetuned a GPT-2 model on 3553 task instances and achieved a much improved performance than BUTLER, but it is trained on all task types, thus not included as a baseline.
Micheli &amp; Fleuret (2021) 在 3553 个任务实例上微调了 GPT-2 模型，并取得了比 BUTLER 更好的性能，但由于它是在所有任务类型上训练的，因此未被列为基准模型。


---



<table><tr><td>Method</td><td>Pick</td><td>Clean</td><td>Heat</td><td>Cool</td><td>Look</td><td>Pick 2</td><td>All</td></tr><tr><td>Act (best of 6)</td><td>88</td><td>42</td><td>74</td><td>67</td><td>72</td><td>41</td><td>45</td></tr><tr><td>ReAct (avg)</td><td>65</td><td>39</td><td>83</td><td>76</td><td>55</td><td>24</td><td>57</td></tr><tr><td>ReAct (best of 6)</td><td>92</td><td>58</td><td>96</td><td>86</td><td>78</td><td>41</td><td>71</td></tr><tr><td>ReAct-IM (avg)</td><td>55</td><td>59</td><td>60</td><td>55</td><td>23</td><td>24</td><td>48</td></tr><tr><td>ReAct-IM (best of 6)</td><td>62</td><td>68</td><td>87</td><td>57</td><td>39</td><td>33</td><td>53</td></tr><tr><td>${\text{ BUTLER }}_{q}$ (best of 8)</td><td>33</td><td>26</td><td>70</td><td>76</td><td>17</td><td>12</td><td>22</td></tr><tr><td>BUTLER (best of 8)</td><td>46</td><td>39</td><td>74</td><td>100</td><td>22</td><td>24</td><td>37</td></tr></table>
<table><tbody><tr><td>方法</td><td>挑选</td><td>清洗</td><td>加热</td><td>冷却</td><td>观察</td><td>挑选 2</td><td>全部</td></tr><tr><td>Act (6次取最佳)</td><td>88</td><td>42</td><td>74</td><td>67</td><td>72</td><td>41</td><td>45</td></tr><tr><td>ReAct (平均)</td><td>65</td><td>39</td><td>83</td><td>76</td><td>55</td><td>24</td><td>57</td></tr><tr><td>ReAct (6次取最佳)</td><td>92</td><td>58</td><td>96</td><td>86</td><td>78</td><td>41</td><td>71</td></tr><tr><td>ReAct-IM (平均)</td><td>55</td><td>59</td><td>60</td><td>55</td><td>23</td><td>24</td><td>48</td></tr><tr><td>ReAct-IM (6次取最佳)</td><td>62</td><td>68</td><td>87</td><td>57</td><td>39</td><td>33</td><td>53</td></tr><tr><td>${\text{ BUTLER }}_{q}$ (8次取最佳)</td><td>33</td><td>26</td><td>70</td><td>76</td><td>17</td><td>12</td><td>22</td></tr><tr><td>BUTLER (8次取最佳)</td><td>46</td><td>39</td><td>74</td><td>100</td><td>22</td><td>24</td><td>37</td></tr></tbody></table>


<table><tr><td>Method</td><td>Score</td><td>SR</td></tr><tr><td>Act</td><td>62.3</td><td>30.1</td></tr><tr><td>ReAct</td><td>66.6</td><td>40.0</td></tr><tr><td>IL</td><td>59.9</td><td>29.1</td></tr><tr><td>IL+RL</td><td>62.4</td><td>28.7</td></tr><tr><td>Human Expert</td><td>82.1</td><td>59.6</td></tr></table>
<table><tbody><tr><td>方法</td><td>评分</td><td>SR</td></tr><tr><td>Act</td><td>62.3</td><td>30.1</td></tr><tr><td>ReAct</td><td>66.6</td><td>40.0</td></tr><tr><td>IL</td><td>59.9</td><td>29.1</td></tr><tr><td>IL+RL</td><td>62.4</td><td>28.7</td></tr><tr><td>人类专家</td><td>82.1</td><td>59.6</td></tr></tbody></table>


Table 3: AlfWorld task-specific success rates (%). BUTLER and ${\text{ BUTLER }}_{q}$ results are from Table 4 of Shridhar et al. (2020b). All methods use greedy decoding, except that BUTLER uses beam search.
表 3：AlfWorld 特定任务成功率 (%)。BUTLER 与 ${\text{ BUTLER }}_{q}$ 的结果来自 Shridhar 等人 (2020b) 的表 4。除 BUTLER 使用束搜索外，所有方法均使用贪婪解码。


Table 4: Score and success rate (SR) on Web-shop. IL/IL+RL taken from Yao et al. (2022). trained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL) method additionally trained with 10,587 training instructions.
表 4：Web-shop 上的得分与成功率 (SR)。IL/IL+RL 取自 Yao 等人 (2022)，前者使用 1,012 条人类标注轨迹训练，模仿+强化学习 (IL+RL) 方法额外使用了 10,587 条训练指令进行训练。


Results ReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On ALFWorld, the best ReAct trial achieves an average success rate of 71%, significantly outperforming the best Act (45%) and BUTLER (37%) trials. In fact, even the worse ReAct trial (48%) beats the best trial of both methods. Moreover, the advantage of ReAct over Act is consistent across six controlled trials, with relative performance gain ranging from 33% to 90% and averaging 62%. Qualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals into smaller subgoals, or loses track of the current state of the environment. Example trajectories comparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2
结果：ReAct 在 ALFWorld（表 3）和 Webshop（表 4）上均优于 Act。在 ALFWorld 上，ReAct 的最佳试验达到 71% 的平均成功率，显著优于 Act (45%) 和 BUTLER (37%) 的最佳试验。事实上，即使是较差的 ReAct 试验 (48%) 也优于这两种方法的最佳试验。此外，ReAct 相对于 Act 的优势在六次对照试验中保持一致，相对性能提升在 33% 到 90% 之间，平均为 62%。定性来看，我们发现 Act 在没有任何思考的情况下，无法正确将目标分解为较小的子目标，或者会丢失对环境当前状态的追踪。比较 ReAct 和 Act 的示例轨迹可见附录 D.2.1 和 D.2.2。


On Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With additional sparse reasoning, ReAct achieves significantly better performance, with an absolute 10% improvement over the previous best success rate. By checking examples, we find that ReAct is more likely to identify instruction-relevant products and options by reasoning to bridge the gap between noisy observations and actions (e.g. "For 'space-saving ottoman bench for living room', the item has options '39x18x18inch' and 'blue' and seems good to buy."). However, existing methods are still far from the performance of expert humans (Table 4), who perform significantly more product explorations and query re-formulations that are still challenging for prompting-based methods.
在 Webshop 上，单样本 Act 提示的表现已与 IL 和 IL+RL 方法相当。通过额外的稀疏推理，ReAct 取得了显著更优的性能，成功率较此前最佳水平绝对提升了 10%。通过检查案例，我们发现 ReAct 更有可能通过推理来桥接噪声观测与动作之间的差距，从而识别出与指令相关的产品和选项（例如，“对于‘客厅节省空间的奥斯曼长凳’，该商品有‘39x18x18英寸’和‘蓝色’选项，看起来值得购买”）。然而，现有方法距离专家级人类的表现（表 4）仍有差距，人类进行了更多的产品探索和查询重构，这对于基于提示的方法仍然具有挑战性。


On the value of internal reasoning vs. external feedback To our knowledge, ReAct is the first demonstration of combined reasoning and action using an LLM applied to an interactive environment within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang et al. (2022b), in which actions from an embodied agent are motivated by an eponymous "inner monologue". However, IM's "inner monologue" is limited to observations of the environment state and what needs to be completed by the agent for the goal to be satisfied. In contrast, the reasoning traces in ReAct for decision making is flexible and sparse, allowing diverse reasoning types (see Section 2) to be induced for different tasks.
关于内部推理与外部反馈的价值：据我们所知，ReAct 是首次在闭环系统内将结合推理与动作的 LLM 应用于交互式环境的演示。或许最接近的前期工作是 Huang 等人 (2022b) 的内省 (Inner Monologue, IM)，其中具身智能体的动作由同名的“内省”驱动。然而，IM 的“内省”仅限于对环境状态的观察以及智能体为实现目标所需完成的内容。相比之下，ReAct 中用于决策的推理轨迹是灵活且稀疏的，允许针对不同任务诱导多样化的推理类型（见第 2 节）。


To demonstrate the differences between ReAct and IM, and to highlight the importance of internal reasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought pattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially outperforms IM-style prompting (ReAct-IM) (71 vs. 53 overall success rate), with consistent advantages on five out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakes in identifying when subgoals were finished, or what the next subgoal should be, due to a lack of high-level goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where an item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning. Both shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in Appendix B.2 An example prompt for ReAct-IM can be found in Appendix C.4, and an example trajectory in Appendix D.2.3
为了展示 ReAct 与 IM 的区别，并强调内部推理相对于简单外部反馈反应的重要性，我们使用由类 IM 密集外部反馈组成的思考模式进行了消融实验。如表 3 所示，ReAct 显著优于 IM 风格提示 (ReAct-IM)（总成功率 71 对 53），且在六项任务中的五项具有一致优势。定性来看，我们观察到 ReAct-IM 经常在识别子目标何时完成或下一个子目标应是什么时犯错，这是由于缺乏高层目标分解。此外，许多 ReAct-IM 轨迹由于缺乏常识推理，难以确定物品在 ALFWorld 环境中可能出现的位置。这两个缺点都可以在 ReAct 范式中得到解决。关于 ReAct-IM 的更多细节见附录 B.2，ReAct-IM 的示例提示见附录 C.4，示例轨迹见附录 D.2.3。


## 5 RELATED WORK
## 5 相关工作


Language model for reasoning Perhaps the most well-known work of using LLMs for reasoning is Chain-of-Thought (CoT) (Wei et al. 2022), which reveals the ability of LLMs to formulate their own "thinking procedure" for problem solving. Several follow-up works have since been performed, including least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shot-CoT (Kojima et al. 2022), and reasoning with self-consistency (Wang et al. 2022a). Recently, (Madaan & Yazdanbakhsh 2022) systematically studied the formulation and structure of CoT, and observed that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other work has also been extended to more sophisticated reasoning architecture beyond simple prompting. For example Selection-Inference (Creswell et al. 2022) divides the reasoning process into two steps of "selection" and "inference". STaR (Zelikman et al. 2022) bootstraps the reasoning process by finetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswell & Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated LM respectively. Similar approaches like Scratchpad (Nye et al. 2021), which finetunes a LM on intermediate computation steps, also demonstrate improvement on multi-step computation problems. In contrast to these methods, ReAct performs more than just isolated, fixed reasoning, and integrates model actions and their corresponding observations into a coherent stream of inputs for the model to reason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).
用于推理的语言模型 使用LLM进行推理最著名的工作或许是思维链（CoT）（Wei et al. 2022），它揭示了LLM为解决问题制定自身“思考过程”的能力。此后开展了多项后续工作，包括用于解决复杂任务的由易到难提示（Zhou et al., 2022）、零样本CoT（Kojima et al. 2022）以及基于自一致性的推理（Wang et al. 2022a）。最近，(Madaan & Yazdanbakhsh 2022)系统地研究了CoT的表述与结构，并观察到符号、模式和文本的存在对CoT的有效性至关重要。其他工作也已扩展到简单提示之外更复杂的推理架构。例如，“选择-推理”（Creswell et al. 2022）将推理过程分为“选择”和“推理”两个步骤。STaR（Zelikman et al. 2022）通过在模型自身生成的正确理由上进行微调来引导推理过程。忠实推理（Creswell & Shanahan, 2022）将多步推理分解为三个步骤，每步分别由专门的LM执行。类似的方法如Scratchpad（Nye et al. 2021），通过在中间计算步骤上微调LM，也展示了在多步计算问题上的提升。与这些方法相比，ReAct不仅执行孤立、固定的推理，还将模型动作及其相应的观察结果整合到连贯的输入流中，使模型能够更准确地推理并处理推理之外的任务（例如交互式决策）。


Language model for decision making The strong capability of LLMs has enabled them to perform tasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a policy model for decision making, especially in interactive environments. WebGPT (Nakano et al. 2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to complicated questions from ELI5 (Fan et al. 2019). In comparison to ReAct, WebGPT does not explicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for reinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b) and Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl et al. 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly consider the reasoning procedure either, and also relies on expensive datasets and human feedback collections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the decision making process only requires language description of the reasoning procedure ${}^{6}$
用于决策的语言模型 LLM的强大能力使其能够执行语言生成之外的任务，利用LLM作为决策策略模型（尤其是在交互式环境中）正变得越来越流行。WebGPT（Nakano et al. 2021）使用LM与网络浏览器交互、浏览网页，并根据ELI5（Fan et al. 2019）推断复杂问题的答案。与ReAct相比，WebGPT没有显式地建模思考和推理过程，而是依赖昂贵的人类反馈进行强化学习。在对话建模中，像BlenderBot（Shuster et al., 2022b）和Sparrow（Glaese et al., 2022）这样的聊天机器人，以及像SimpleTOD（Hosseini-Asl et al. 2020）这样的任务导向型对话系统，也训练LM来做出API调用决策。与ReAct不同，它们同样没有显式考虑推理过程，并且依赖昂贵的数据集和人类反馈收集来进行策略学习。相比之下，ReAct学习策略的方式要廉价得多，因为决策过程仅需要推理过程的语言描述 ${}^{6}$


LLMS have also been increasingly employed in interactive and embodied environments for planning and decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al. 2022) and Inner Monologue (Huang et al. 2022b), which use LLMs for robotic action planning and decision making. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which is then reranked by an affordance model grounded on the visual environments for final prediction. Inner Monologue made further improvements by adding the eponymous "inner monologue", which is implemented as injected feedback from the environment. To our knowledge, Inner Monologue is the first work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue that Inner Monologue does not truly comprise of inner thoughts - this is elaborated in Section 4 . We also note that leveraging language as semantically-rich inputs in the process of interactive decision making has been shown to be successful under other settings (Abramson et al., 2020, Karamcheti et al., 2021; Huang et al., 2022a) Li et al., 2022). It is becoming more evident that with the help of LLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and decision making. What is more, progress in LLMs has also inspired the development of versatile and generalist agents like Reed et al. (2022).
LLM也越来越多地被应用于交互式和具身环境中的规划与决策。在这方面，与ReAct最相关的或许是SayCan（Ahn et al. 2022）和Inner Monologue（Huang et al. 2022b），它们将LLM用于机器人动作规划和决策。在SayCan中，LLM被提示直接预测机器人可以采取的可能动作，然后由基于视觉环境的可供性模型进行重排序以完成最终预测。Inner Monologue通过增加同名的“内心独白”（实现为从环境注入的反馈）做了进一步改进。据我们所知，Inner Monologue是第一个展示这种闭环系统的工作，ReAct以此为基础。然而，我们认为Inner Monologue并不真正包含内心想法——这将在第4节详述。我们还注意到，在交互式决策过程中利用语言作为语义丰富的输入，在其他设置下已被证明是成功的（Abramson et al., 2020, Karamcheti et al., 2021; Huang et al., 2022a) Li et al., 2022）。越来越明显的是，在LLM的帮助下，语言作为一种基础的认知机制，将在交互和决策中发挥关键作用。此外，LLM的进步也启发了像Reed et al. (2022) 这样多功能和通用型智能体的开发。


## 6 CONCLUSION
## 6 结论


We have proposed ReAct - a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experiments on multi-hop question-answering, fact checking, and interactive decision-making tasks, we show that ReAct leads to superior performance with interpretable decision traces. Despite the simplicity of our method, complex tasks with large action spaces require more demonstrations to learn well, which unfortunately can easily go beyond the input length limit of in-context learning. We explore the fine-tuning approach on HotpotQA with initial promising results, but learning from more high-quality human annotations will be the desiderata to further improve the performance. Scaling up ReAct with multi-task training and combining it with complementary paradigms like reinforcement learning could result in stronger agents that further unlock the potential of LLMs for more applications.
我们提出了 ReAct——一种在大型语言模型中协同推理与行动的简便且有效的方法。通过在多跳问答、事实核查及交互式决策任务上的一系列实验，我们证明了 ReAct 能够以可解释的决策轨迹实现卓越的性能。尽管我们的方法很简单，但具有大动作空间的复杂任务需要更多演示才能良好学习，这遗憾地容易超出上下文学习的输入长度限制。我们在 HotpotQA 上探索了微调方法，初步结果令人鼓舞，但从更多高质量的人类标注中学习将是进一步提升性能的必要条件。通过多任务训练扩大 ReAct 的规模，并将其与强化学习等互补范式相结合，可能会产生更强大的智能体，从而进一步释放 LLM 在更多应用中的潜力。


---



${}^{6}$ Human feedback can also be incorporated in a complementary manner but we leave it for future work.
${}^{6}$ 人类反馈也可以通过互补的方式结合进来，但我们将其留给未来的工作。


---



## ACKNOWLEDGMENTS
## 致谢


We thank the support and feedback of many people from Google Brain team and Princeton NLP Group. This work was supported in part by the National Science Foundation under Grant No. 2107048. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
我们感谢来自 Google Brain 团队和普林斯顿 NLP 小组许多人的支持与反馈。这项工作得到了美国国家科学基金会第 2107048 号资助项目的部分支持。本材料中表达的所有观点、发现、结论或建议均为作者个人观点，不一定反映美国国家科学基金会的观点。


## REPRODUCIBILITY STATEMENT
## 复现性声明


Our main experiments are done on PaLM (Chowdhery et al. 2022), which is not an openly accessible model yet. To increase reproducibility, we have included all used prompts in Appendix C, additional experiments using GPT-3 (Brown et al. 2020) in Appendix A.1 and associated GPT-3 ReAct prompting code at https://anonymous.4open.science/r/ReAct-2268/
我们的主要实验是在 PaLM (Chowdhery et al. 2022) 上完成的，该模型目前尚未公开。为了提高复现性，我们在附录 C 中包含了所有使用的提示词，在附录 A.1 中包含了使用 GPT-3 (Brown et al. 2020) 的额外实验，并在 https://anonymous.4open.science/r/ReAct-2268/ 提供了相关的 GPT-3 ReAct 提示词代码。


## ETHICS STATEMENT
## 伦理声明


ReAct prompts large language models to generate more human interpretable, diagnosable, and controllable task-solving trajectories than previous methods. However, hooking up a large language model with an action space to interact with external environments (e.g. the web, physical environments) has potential dangers, e.g. looking up inappropriate or private information, or taking harmful actions in an environment. Our experiments minimize such risks by limiting the interactions to specific websites (Wikipedia or WebShop) that are free of private information, without any dangerous actions in the action space design (i.e. models cannot really buy products on WebShop the research benchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing more extensive experiments in the future.
与以往的方法相比，ReAct 促使大型语言模型生成更具人类可解释性、可诊断性和可控性的任务解决轨迹。然而，将具有动作空间的大型语言模型与外部环境（如网络、物理环境）连接起来存在潜在风险，例如查询不当或私人信息，或在环境中采取有害行动。我们的实验通过将交互限制在不含私人信息的特定网站（Wikipedia 或 WebShop）来最小化此类风险，且在动作空间设计中不包含任何危险动作（即模型无法在 WebShop 研究基准上真正购买商品，也无法编辑 Wikipedia）。我们认为研究人员在未来设计更广泛的实验之前应意识到此类风险。


## REFERENCES
## 参考文献


Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathewson, Soňa Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne, Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence, 2020.URLhttps://arxiv.org/abs/2012.05672
Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathewson, Soňa Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne, Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence, 2020. URL https://arxiv.org/abs/2012.05672


Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL https://arxiv.org/abs/2204.01691
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL https://arxiv.org/abs/2204.01691


Ben Alderson-Day and Charles Fernyhough. Inner speech: development, cognitive functions, phenomenology, and neurobiology. Psychological bulletin, 141(5):931, 2015.
Ben Alderson-Day and Charles Fernyhough. Inner speech: development, cognitive functions, phenomenology, and neurobiology. Psychological bulletin, 141(5):931, 2015.


---



Alan Baddeley. Working memory. Science, 255(5044):556-559, 1992.
Alan Baddeley. Working memory. Science, 255(5044):556-559, 1992.


---



Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, 等. 语言模型是少样本学习者. Advances in neural information processing systems, 33:1877-1901, 2020.


Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, 等. PaLM：通过 Pathways 扩展语言建模. arXiv preprint arXiv:2204.02311, 2022.


Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL https://arxiv.org/abs/2208.14271
Antonia Creswell 和 Murray Shanahan. 使用大型语言模型进行忠实推理, 2022. URL https://arxiv.org/abs/2208.14271


Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning, 2022. URL https://arxiv.org/abs/ 2205.09712
Antonia Creswell, Murray Shanahan, 和 Irina Higgins. 选择-推理：利用大型语言模型进行可解释的逻辑推理, 2022. URL https://arxiv.org/abs/2205.09712


Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3558-3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/ P19-1346
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, 和 Michael Auli. ELI5：长篇问答. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3558-3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/P19-1346


Charles Fernyhough. Vygotsky, luria, and the social brain. Self and social regulation: Social interaction and the development of social understanding and executive functions, pp. 56-79, 2010.
Charles Fernyhough. 维果茨基、鲁利亚与社会大脑. Self and social regulation: Social interaction and the development of social understanding and executive functions, pp. 56-79, 2010.


Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soña Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements, 2022. URL https://storage.googleapis.com/deepmind-media/DeepMind com/Authors-Notes/sparrow/sparrow-final.pdf
Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soña Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, 和 Geoffrey Irving. 通过定向人类判断改善对话智能体的对齐, 2022. URL https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf


Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple language model for task-oriented dialogue. Advances in Neural Information Processing Systems, 33:20179-20191, 2020.
Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, 和 Richard Socher. 一种用于任务导向对话的简单语言模型. Advances in Neural Information Processing Systems, 33:20179-20191, 2020.


Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022a.
Wenlong Huang, Pieter Abbeel, Deepak Pathak, 和 Igor Mordatch. 语言模型作为零样本规划器：为具身智能体提取可操作知识. arXiv preprint arXiv:2201.07207, 2022a.


Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022b.
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, 等. 内部独白：通过语言模型规划进行具身推理. arXiv preprint arXiv:2207.05608, 2022b.


Siddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. Lila: Language-informed latent actions. In CoRL, pp. 1379-1390, 2021. URL https://proceedings.mlr.press/ v164/karamcheti22a.html
Siddharth Karamcheti, Megha Srivastava, Percy Liang, 和 Dorsa Sadigh. LILA：语言告知的潜在动作. In CoRL, pp. 1379-1390, 2021. URL https://proceedings.mlr.press/v164/karamcheti22a.html


Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, 和 Yusuke Iwasawa. 大型语言模型是零样本推理者. arXiv preprint arXiv:2205.11916, 2022.


Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-augmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022.
Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, 和 Nikolai Grigorev. 通过少样本提示进行互联网增强的语言模型以用于开放域问答. arXiv preprint arXiv:2203.05115, 2022.


Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 9459-9474, 2020.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 知识密集型 NLP 任务的检索增强生成。神经信息处理系统进展 (NeurIPS), 33: 9459-9474, 2020.


Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke Zhu. Pre-trained language models for interactive decision-making, 2022. URL https: //arxiv.org/abs/2202.01771
Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke Zhu. 用于交互式决策的预训练语言模型, 2022. URL https://arxiv.org/abs/2202.01771


Aleksandr Romanovich Luria. Ls vygotsky and the problem of localization of functions. Neuropsy-chologia, 3(4):387-392, 1965.
Aleksandr Romanovich Luria. 维果茨基与功能定位问题。神经心理学 (Neuropsychologia), 3(4):387-392, 1965.


Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango, 2022. URLhttps://arxiv.org/abs/2209.07686
Aman Madaan and Amir Yazdanbakhsh. 文本与模式：有效的思维链需要双管齐下, 2022. URL https://arxiv.org/abs/2209.07686


Vincent Micheli and François Fleuret. Language models are few-shot butlers. arXiv preprint arXiv:2104.07972, 2021.
Vincent Micheli and François Fleuret. 语言模型是少样本管家。arXiv 预印本 arXiv:2104.07972, 2021.


Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback, 2021. URL https://arxiv org/abs/2112.09332
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT：结合人类反馈的浏览器辅助问答, 2021. URL https://arxiv.org/abs/2112.09332


Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. URL https://arxiv.org/abs/2112.00114
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 展示你的工作：语言模型中间计算的草稿本, 2021. URL https://arxiv.org/abs/2112.00114


Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https: //arxiv.org/abs/2205.06175.
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. 通才智能体, 2022. URL https://arxiv.org/abs/2205.06175.


Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10740-10749, 2020a.
Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED：解释日常任务落地指令的基准。收录于 IEEE/CVF 计算机视觉与模式识别会议论文集 (CVPR), pp. 10740-10749, 2020a.


Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020b.
Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld：为交互式学习对齐文本与具身环境。arXiv 预印本 arXiv:2010.03768, 2020b.


Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. Language models that seek for knowledge: Modular search & generation for dialogue and prompt completion. arXiv preprint arXiv:2203.13224, 2022a.
Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. 寻求知识的语言模型：用于对话和提示补全的模块化搜索与生成。arXiv 预印本 arXiv:2203.13224, 2022a.


Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage, 2022b. URL https://arxiv.org/abs/2208.03188
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. BlenderBot 3：一个可持续学习负责任互动的已部署对话智能体, 2022b. URL https://arxiv.org/abs/2208.03188


James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018.
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER：用于证据提取与验证的大规模数据集。arXiv 预印本 arXiv:1803.05355, 2018.


Lev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky, 1:39-285, 1987.
Lev S Vygotsky. 思维与语言. LS Vygotsky 全集, 1:39-285, 1987.


Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2022a.URLhttps://arxiv.org/abs/2203.11171
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-ery, 以及 Denny Zhou. 自一致性提升了语言模型中的思维链推理, 2022a. URL https://arxiv.org/abs/2203.11171


Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, 以及 Denny Zhou. 语言模型中的原理解释增强集成. arXiv 预印本 arXiv:2207.00747, 2022b.


Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, 以及 Denny Zhou. 思维链提示诱导了大型语言模型中的推理. arXiv 预印本 arXiv:2201.11903, 2022.


Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, 以及 Christopher D Manning. Hotpotqa：一个用于多样化、可解释多跳问答的数据集. arXiv 预印本 arXiv:1809.09600, 2018.


Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore: Language models for action generation in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8736-8754, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704. URLhttps://aclanthology.org/2020.emnlp-main.704.
Shunyu Yao, Rohan Rao, Matthew Hausknecht, 以及 Karthik Narasimhan. 保持 CALM 并探索：用于文本游戏动作生成的语言模型. 收录于 2020 年自然语言处理实证方法会议 (EMNLP) 论文集, pp. 8736-8754, 在线, 2020 年 11 月. 计算语言学协会. doi: 10.18653/v1/2020.emnlp-main.704. URL https://aclanthology.org/2020.emnlp-main.704.


Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206, 2022.
Shunyu Yao, Howard Chen, John Yang, 以及 Karthik Narasimhan. Webshop：面向具备接地语言智能体的可扩展现实世界网络交互. arXiv 预印本 arXiv:2207.01206, 2022.


Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465
Eric Zelikman, Yuhuai Wu, Jesse Mu, 以及 Noah D. Goodman. Star：通过推理自我引导推理, 2022. URL https://arxiv.org/abs/2203.14465


Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2022. URL https://arxiv.org/abs/2205.10625.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, 以及 Ed Chi. 从易到难的提示开启了大型语言模型中的复杂推理, 2022. URL https://arxiv.org/abs/2205.10625.


Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information seeking for open-domain question answering. arXiv preprint arXiv:2109.06747, 2021.
Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, 以及 Xueqi Cheng. 面向开放域问答的自适应信息检索. arXiv 预印本 arXiv:2109.06747, 2021.


## A ADDITIONAL RESULTS
## A 附加结果


### A.1 GPT-3 EXPERIMENTS
### A.1 GPT-3 实验


<table><tr><td></td><td>PaLM-540B</td><td>GPT-3</td></tr><tr><td>HotpotQA (exact match)</td><td>29.4</td><td>30.8</td></tr><tr><td>ALFWorld (success rate %)</td><td>70.9</td><td>78.4</td></tr></table>
<table><tbody><tr><td></td><td>PaLM-540B</td><td>GPT-3</td></tr><tr><td>HotpotQA (完全匹配)</td><td>29.4</td><td>30.8</td></tr><tr><td>ALFWorld (成功率 %)</td><td>70.9</td><td>78.4</td></tr></tbody></table>


Table 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding). On HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all 134 unseen validation task instances, and use the best prompt set according to PaLM-540B.
表 5：PaLM-540B 与 GPT-3 (text-davinci-002, 贪婪解码) 的 ReAct 提示结果。在 HotpotQA 上，我们随机抽取了 500 个验证问题子集。在 ALFWorld 上，我们使用了全部 134 个未见过的验证任务实例，并采用了 PaLM-540B 的最佳提示集。


We run additional GPT-3 (Brown et al. 2020) experiments to confirm ReAct prompting performance is general across different large language models. As shown in Table 5, GPT-3 (text-davinci-002, greedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly because it is finetuned with human instruction following. This indicates ReAct prompting is effective across different large language models on different tasks. The code for these experiments are at https://react-lm.github.io/
我们运行了额外的 GPT-3 (Brown et al. 2020) 实验，以确认 ReAct 提示性能在不同的语言大模型中具有通用性。如表 5 所示，GPT-3 (text-davinci-002, 贪婪解码) 在 HotpotQA 和 ALFWorld 上的表现一致优于 PaLM-540B，这可能是因为它经过了遵循人类指令的微调。这表明 ReAct 提示在不同任务上的不同语言大模型中均有效。这些实验的代码见 https://react-lm.github.io/


A. 2 REACT OBTAINS UP-TO-DATE KNOWLEDGE ON HOTPOTQA
A. 2 REACT 在 HOTPOTQA 上获取最新知识


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_16_23_3d35e5.jpg"/>



Figure 4: Another example HotpotQA question, where the original label is outdated. Only ReAct is able to obtain the up-to-date answer thanks to real-world web interaction plus reasoning.
图 4：另一个 HotpotQA 问题示例，其中原始标签已过时。得益于真实的 Web 交互及推理，只有 ReAct 能够获得最新的答案。


During trajectory inspection, we also find that sometimes ReAct does not agree with dataset labels as the labels themselves could be outdated. For example, as shown in Figure 4, the question asks about the size of a hotel, which increased from the HotpotQA construction time. While Standard and CoT give wrong answers due to hallucination, Act fails despite the access of real-world web interaction, due to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to retrieve up-to-date information from the Internet and provide a reasonable answer. Therefore, better incorporation of reasoning abilities might benefit recent Internet-augmented language models (Nakano et al. 2021, Lazaridou et al. 2022, Shuster et al. 2022a) for up-to-date task solving.
在轨迹检查期间，我们还发现 ReAct 有时与数据集标签不一致，因为标签本身可能已过时。例如，如图 4 所示，问题询问一家酒店的大小，该酒店自 HotpotQA 构建以来有所扩建。虽然 Standard 和 CoT 由于幻觉给出错误答案，但 Act 尽管可以访问真实的 Web 交互，却由于缺乏引导如何与互联网进行问答交互的推理而失败。只有 ReAct 能够从互联网检索最新信息并提供合理的答案。因此，更好地融合推理能力可能有益于近期基于互联网增强的语言模型 (Nakano et al. 2021, Lazaridou et al. 2022, Shuster et al. 2022a) 以解决最新任务。


### A.3 HUMAN-IN-THE-LOOP BEHAVIOR CORRECTION ON ALFWORLD
### A.3 ALFWORLD 上的“人在回路”行为修正


We also explore human-in-the-loop interaction with ReAct, to allow a human to inspect and edit ReAct's reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act 17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align with these human thought edits and succeed in the task. From a human perspective, solving such a task becomes significantly easier, from typing tens of actions to only editing a couple of thoughts, which enables new forms of human-machine collaboration. We note that such a policy edit on-the-go
我们还探索了与 ReAct 的“人在回路”交互，允许人类检查和编辑 ReAct 的推理轨迹。图 5 显示，只需删除 Act 17 中的一句幻觉并增加 Act 23 中的一些提示，即可使 ReAct 大幅改变其行为以匹配这些人工思维编辑并成功完成任务。从人类的角度来看，解决此类任务变得显著简单，从输入数十个动作变为仅编辑几个思维，这开启了人机协作的新形式。我们注意到，这种实时的策略编辑


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_16_23_30b1ee.jpg"/>



Figure 5: A human-in-the-loop behavior correction example with ReAct in AlfWorld. (a) ReAct trajectory fails due to a hallucinating thought (Act 17). (b) By a human simply editing two thoughts (Act 17, 23), the ReAct trajectory produces desirable reasoning traces and actions and succeeds.
图 5：ReAct 在 AlfWorld 中的一个“人在回路”行为修正示例。(a) ReAct 轨迹由于思维幻觉 (Act 17) 而失败。(b) 通过人类简单编辑两个思维 (Act 17, 23)，ReAct 轨迹产生了理想的推理迹象和动作并取得成功。


is difficult for Act and previous RL methods, as a human cannot change the model parameters, and changing a few actions might not edit the rest of the model behavior. This paradigm is also more than human dialogue to update the goal or subgoal as in Huang et al. (2022b) - while editing ReAct thoughts can do these, it can also modify the model's internal belief, reasoning styles, or anything the flexible thought space supports, for better task solving. We believe this is an exciting direction for human alignment and leave more systematic study as future work.
对于 Act 和之前的 RL 方法来说是困难的，因为人类无法更改模型参数，且更改少量动作可能无法修改其余的模型行为。这种范式也超越了 Huang et al. (2022b) 中用于更新目标或子目标的人类对话——虽然编辑 ReAct 思维可以做到这些，但它还可以修改模型的内部信念、推理风格或灵活思维空间支持的任何内容，以更好地解决任务。我们认为这是人类对齐的一个令人兴奋的方向，并将更系统的研究留作未来工作。


## B EXPERIMENT DETAILS
## B 实验细节


### B.1 HotPOTQA FINETUNING DETAILS
### B.1 HOTPOTQA 微调细节


For all finetuning we use a batch size of 64. On PaLM-8B, we finetune ReAct and Act methods for 4,000 steps and Standard and CoT methods for 2,000 steps. On PaLM-62B, we finetune ReAct and Act methods for 4,000 steps and Standard and CoT methods for 1,000 steps. We find ReAct and Act methods generally benefit from more training steps (and more training data), while Standard and CoT methods degrade soon after finetuning.
对于所有微调，我们使用 64 的批处理大小。在 PaLM-8B 上，我们将 ReAct 和 Act 方法微调 4,000 步，Standard 和 CoT 方法微调 2,000 步。在 PaLM-62B 上，我们将 ReAct 和 Act 方法微调 4,000 步，Standard 和 CoT 方法微调 1,000 步。我们发现 ReAct 和 Act 方法通常受益于更多的训练步骤（和更多的训练数据），而 Standard 和 CoT 方法在微调后不久性能就会下降。


### B.2 ALFWORLD IM-STYLE DETAILS
### B.2 ALFWORLD IM 风格细节


For the IM-style ablation, the same expert trajectories used in ReAct are reannotated with dense external feedback thoughts within these trajectories, that limit ReAct-IM to only think about (1) decomposing the current goal and (2) the current subgoal that needs to be completed. In particular, ReAct-IM lacks thoughts that (1) determine when a subgoal is completed (2) determine what the next subgoal should be (3) inducing the LLM to refer to its internal pretraining knowledge to identify where items can be within the environment.
对于 IM 风格的消融实验，ReAct 中使用的相同专家轨迹在这些轨迹内被重新标注了密集的外部反馈思维，这将 ReAct-IM 限制为仅思考 (1) 分解当前目标和 (2) 需要完成的当前子目标。特别是，ReAct-IM 缺乏用于以下目的的思维：(1) 确定子目标何时完成；(2) 确定下一个子目标应该是什么；(3) 引导 LLM 参考其内部预训练知识以识别物品在环境中的位置。


## C PROMPTS
## C 提示词


### C.1 HOTPOTQA
### C.1 HOTPOTQA


<table><tr><td colspan="2">Hotpot QA Prompts</td></tr><tr><td>Original</td><td></td></tr><tr><td>Question <br> Answer</td><td>What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into? <br> 1,800 to 7,000 ft</td></tr><tr><td>Question <br> Answer</td><td>Musician and satirist Allie Goertz wrote a song about the "The Simpsons" character Milhouse, who Matt Groening named after who? Richard Nixon</td></tr><tr><td>Question</td><td>Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?</td></tr><tr><td>Answer</td><td>The Saimaa Gesture</td></tr><tr><td>Question Answer</td><td>What profession does Nicholas Ray and Elia Kazan have in common? director, screenwriter, actor</td></tr><tr><td>Question Answer</td><td>Which magazine was started first Arthur's Magazine or First for Women? Arthur's Magazine</td></tr><tr><td>Question Answer</td><td>Were Pavel Urysohn and Leonid Levin known for the same type of work? Yes</td></tr><tr><td colspan="2">Act</td></tr><tr><td>Question</td><td>What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td></tr><tr><td>Action 1 <br> Observation 1</td><td>Search [Colorado orogeny] <br> The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.</td></tr><tr><td>Action 2 Observation 2 <br> Action 3 Observation 3 Action 4</td><td>Lookup [eastern sector] <br> (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny. <br> Search [High Plains] <br> High Plains refers to one of two distinct land regions: <br> Search [High Plains (United States)]</td></tr><tr><td>Observation 4 <br> Action 5</td><td>The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m). [3] <br> Finish [1,800 to 7,000 ft]</td></tr><tr><td>Question <br> Action 1</td><td>Musician and satirist Allie Goertz wrote a song about the "The Simpsons" character Milhouse, who Matt Groening named after who? Search [Milhouse]</td></tr><tr><td>Observation 1</td><td>Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.</td></tr><tr><td>Action 2 Observation 2 <br> Action 3</td><td>Lookup [named after] <br> (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous. <br> Finish[Richard Nixon]</td></tr><tr><td>Question</td><td>Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?</td></tr><tr><td>Action 1 Observation 1</td><td>Search [Adam Clayton Powell] <br> Could not find [Adam Clayton Powell]. Similar: ['Adam Clayton Powell III', 'Seventh Avenue (Manhattan)', 'Adam Clayton Powell Jr. State Office Building', 'Isabel Washington Powell', 'Adam Powell', 'Adam Clayton Powell (film)', 'Giancarlo Esposito'].</td></tr><tr><td>Action 2 Observation 2 <br> Action 3</td><td>Search [Adam Clayton Powell (film)] <br> Adam Clayton Powell is a 1989 American documentary film directed by Richard Kilberg. The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr. [3][4] It was later aired as part of the PBS series The American Experience. <br> Finish [The Saimaa Gesture]</td></tr><tr><td>Question <br> Action 1</td><td>What profession does Nicholas Ray and Elia Kazan have in common? Search[Nicholas Ray]</td></tr><tr><td>Observation 1</td><td>Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.</td></tr><tr><td>Action 2 Observation 2</td><td>Search [Elia Kazan] <br> Elia Kazan was an American film and theatre director, producer, screenwriter and actor.</td></tr></table>
<table><tbody><tr><td colspan="2">Hotpot QA 提示词</td></tr><tr><td>原文</td><td></td></tr><tr><td>问题 <br/> 回答</td><td>科罗拉多造山带东段延伸区域的海拔范围是多少？ <br/> 1,800 至 7,000 英尺</td></tr><tr><td>问题 <br/> 回答</td><td>音乐家兼讽刺作家艾莉·格茨写了一首关于《辛普森一家》角色米尔豪斯的歌，马特·格勒宁是以谁的名字为该角色命名的？理查德·尼克松</td></tr><tr><td>问题</td><td>哪部纪录片是关于芬兰摇滚乐队的，《亚当·克莱顿·鲍威尔》还是《塞马湖手势》？</td></tr><tr><td>回答</td><td>《塞马湖手势》</td></tr><tr><td>问题 回答</td><td>尼古拉斯·雷和伊利亚·卡赞有什么共同职业？导演、编剧、演员</td></tr><tr><td>问题 回答</td><td>哪本杂志创刊更早，《亚瑟杂志》还是《女性第一》？《亚瑟杂志》</td></tr><tr><td>问题 回答</td><td>帕维尔·乌雷松和列昂尼德·列文是否以同类型的工作闻名？是的</td></tr><tr><td colspan="2">行动</td></tr><tr><td>问题</td><td>科罗拉多造山带东段延伸区域的海拔范围是多少？</td></tr><tr><td>行动 1 <br/> 观察 1</td><td>搜索 [科罗拉多造山运动] <br/> 科罗拉多造山运动是发生在科罗拉多及其周边地区的一次造山事件。</td></tr><tr><td>行动 2 观察 2 <br/> 行动 3 观察 3 行动 4</td><td>查找 [东段] <br/> （结果 1 / 1）东段延伸至高地平原，被称为中平原造山运动。 <br/> 搜索 [高地平原] <br/> 高地平原指两个不同的地理区域之一： <br/> 搜索 [高地平原（美国）]</td></tr><tr><td>观察 4 <br/> 行动 5</td><td>高地平原是大平原的一个子区域。从东到西，高地平原的海拔从约 1,800 英尺上升到 7,000 英尺（550 至 2,130 米）。[3] <br/> 结束 [1,800 至 7,000 英尺]</td></tr><tr><td>问题 <br/> 行动 1</td><td>音乐家兼讽刺作家艾莉·格茨写了一首关于《辛普森一家》角色米尔豪斯的歌，马特·格勒宁是以谁的名字为该角色命名的？搜索 [米尔豪斯]</td></tr><tr><td>观察 1</td><td>米尔豪斯·墨索里尼·范·霍滕是福克斯动画系列剧《辛普森一家》中的常驻角色，由帕梅拉·海登配音，马特·格勒宁创作。</td></tr><tr><td>行动 2 观察 2 <br/> 行动 3</td><td>查找 [以...命名] <br/> （结果 1 / 1）米尔豪斯是以美国总统理查德·尼克松的名字命名的，尼克松的中间名是米尔豪斯。 <br/> 结束 [理查德·尼克松]</td></tr><tr><td>问题</td><td>哪部纪录片是关于芬兰摇滚乐队的，《亚当·克莱顿·鲍威尔》还是《塞马湖手势》？</td></tr><tr><td>行动 1 观察 1</td><td>搜索 [亚当·克莱顿·鲍威尔] <br/> 未找到 [亚当·克莱顿·鲍威尔]。相似结果：['亚当·克莱顿·鲍威尔三世', '第七大道（曼哈顿）', '小亚当·克莱顿·鲍威尔州办公大楼', '伊莎贝尔·华盛顿·鲍威尔', '亚当·鲍威尔', '亚当·克莱顿·鲍威尔（电影）', '詹卡洛·埃斯波西托']。</td></tr><tr><td>行动 2 观察 2 <br/> 行动 3</td><td>搜索 [亚当·克莱顿·鲍威尔（电影）] <br/> 《亚当·克莱顿·鲍威尔》是一部 1989 年由理查德·基尔伯格执导的美国纪录片。该片讲述了极具影响力的非裔美国政治家小亚当·克莱顿·鲍威尔的兴衰。[3][4] 此后作为 PBS 系列剧《美国印象》的一部分播出。 <br/> 结束 [《塞马湖手势》]</td></tr><tr><td>问题 <br/> 行动 1</td><td>尼古拉斯·雷和伊利亚·卡赞有什么共同职业？搜索 [尼古拉斯·雷]</td></tr><tr><td>观察 1</td><td>尼古拉斯·雷（原名小雷蒙德·尼古拉斯·金兹尔，1911年8月7日 - 1979年6月16日）是美国电影导演、编剧和演员，以 1955 年的电影《无因的反叛》最为著名。</td></tr><tr><td>行动 2 观察 2</td><td>搜索 [伊利亚·卡赞] <br/> 伊利亚·卡赞是美国电影和戏剧导演、制片人、编剧及演员。</td></tr></tbody></table>


Continued on next page
续下页


<table><tr><td></td><td>Hotpot QA Prompts - Continued from previous page</td></tr><tr><td>Action 3 <br> Question</td><td>Finish [director, screenwriter, actor] <br> Which magazine was started first Arthur's Magazine or First for Women?</td></tr><tr><td>Action 1 <br> Observation 1 <br> Action 2</td><td>Search [Arthur's Magazine] <br> Arthur's Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century. <br> Search[First for Women]</td></tr><tr><td>Observation 2 <br> Action 3 <br> Question Action 1</td><td>First for Women is a woman's magazine published by Bauer Media Group in the USA. [1] The magazine was started in 1989. <br> Finish[Arthur's Magazine] <br> Were Pavel Urysohn and Leonid Levin known for the same type of work? Search [Pavel Urysohn]</td></tr><tr><td>Observation 1 <br> Action 2 Observation 2 <br> Action 3</td><td>Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet mathematician who is best known for his contributions in dimension theory. Search [Leonid Levin] <br> Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist. <br> Finish [yes]</td></tr><tr><td colspan="2">CoT</td></tr><tr><td>Question <br> Thought</td><td>What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into? <br> Let's think step by step. The eastern sector of Colorado orogeny extends <br> into the High Plains. High Plains rise in elevation from around 1,800 to</td></tr><tr><td>Answer <br> Question <br> Thought <br> Answer</td><td>7,000 ft, so the answer is 1,800 to 7,000 ft. <br> 1,800 to 7,000 ft <br> Musician and satirist Allie Goertz wrote a song about the "The Simpsons" character Milhouse, who Matt Groening named after who? <br> Let's think step by step. Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon. <br> Richard Nixon</td></tr><tr><td>Question <br> Thought <br> Answer</td><td>Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture? <br> Let's think step by step. Adam Clayton Powell (film) is a documentary about an African-American politician, not Finnish rock groups. So the documentary about Finnish rock groups must instead be The Saimaa Gesture. <br> The Saimaa Gesture</td></tr><tr><td>Question Thought <br> Answer</td><td>What profession does Nicholas Ray and Elia Kazan have in common? <br> Let's think step by step. Professions of Nicholas Ray are director, <br> screenwriter, and actor. Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor. <br> director, screenwriter, actor</td></tr><tr><td>Question Thought <br> Answer</td><td>Which magazine was started first Arthur's Magazine or First for Women? Let's think step by step. Arthur's Magazine was started in 1844. First for Women was started in 1989. 1844 (Arthur's Magazine) < 1989 (First for Women), so Arthur's Magazine was started first. <br> Arthur's Magazine</td></tr><tr><td>Question Thought <br> Answer</td><td>Were Pavel Urysohn and Leonid Levin known for the same type of work? Let's think step by step. Pavel Urysohn is a mathematician. Leonid Levi is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work. <br> Yes</td></tr><tr><td colspan="2">ReAct</td></tr><tr><td>Question</td><td>What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td></tr><tr><td>Thought 1</td><td>I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.</td></tr><tr><td>Action 1 Observation 1 <br> Thought 2</td><td>Search [Colorado orogeny] <br> The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas. <br> It does not mention the eastern sector. So I need to look up eastern sector.</td></tr><tr><td>Action 2 Observation 2 <br> Thought 3 <br> Action 3</td><td>Lookup [eastern sector] <br> (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny. <br> The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range. <br> Search [High Plains]</td></tr><tr><td>Observation 3 <br> Thought 4 <br> Action 4</td><td>High Plains refers to one of two distinct land regions <br> I need to instead search High Plains (United States). <br> Search[High Plains (United States)]</td></tr></table>
<table><tbody><tr><td></td><td>Hotpot QA 提示词 - 接上页</td></tr><tr><td>动作 3 <br/> 问题</td><td>完成 [导演, 编剧, 演员] <br/> Arthur's Magazine 和 First for Women 哪本杂志创刊更早？</td></tr><tr><td>动作 1 <br/> 观察 1 <br/> 动作 2</td><td>搜索 [Arthur's Magazine] <br/> Arthur's Magazine (1844-1846) 是 19 世纪在费城出版的一份美国文学期刊。 <br/> 搜索 [First for Women]</td></tr><tr><td>观察 2 <br/> 动作 3 <br/> 问题 动作 1</td><td>First for Women 是由鲍尔媒体集团在美国出版的一本女性杂志。[1] 该杂志创刊于 1989 年。 <br/> 完成 [Arthur's Magazine] <br/> Pavel Urysohn 和 Leonid Levin 是因同类型的工作而闻名吗？搜索 [Pavel Urysohn]</td></tr><tr><td>观察 1 <br/> 动作 2 观察 2 <br/> 动作 3</td><td>Pavel Samuilovich Urysohn (1898年2月3日 - 1924年8月17日) 是一位苏联数学家，以在维数理论方面的贡献而闻名。搜索 [Leonid Levin] <br/> Leonid Anatolievich Levin 是一位苏联裔美国数学家和计算机科学家。 <br/> 完成 [是]</td></tr><tr><td colspan="2">CoT</td></tr><tr><td>问题 <br/> 想法</td><td>科罗拉多造山带东段延伸区域的海拔范围是多少？ <br/> 让我们一步步思考。科罗拉多造山带的东段延伸 <br/> 至高地平原。高地平原的海拔从大约 1,800 上升至</td></tr><tr><td>回答 <br/> 问题 <br/> 想法 <br/> 回答</td><td>7,000 英尺，所以答案是 1,800 到 7,000 英尺。 <br/> 1,800 到 7,000 英尺 <br/> 音乐家兼讽刺作家 Allie Goertz 写了一首关于《辛普森一家》角色 Milhouse 的歌，马特·格勒宁是以谁的名字为他命名的？ <br/> 让我们一步步思考。Milhouse 是以美国总统理查德·尼克松的名字命名的，所以答案是理查德·尼克松。 <br/> 理查德·尼克松</td></tr><tr><td>问题 <br/> 想法 <br/> 回答</td><td>哪部纪录片是关于芬兰摇滚乐队的，是《Adam Clayton Powell》还是《The Saimaa Gesture》？ <br/> 让我们一步步思考。《Adam Clayton Powell》（电影）是一部关于非裔美国政治家的纪录片，不是关于芬兰摇滚乐队的。因此，关于芬兰摇滚乐队的纪录片必然是《The Saimaa Gesture》。 <br/> The Saimaa Gesture</td></tr><tr><td>问题 想法 <br/> 回答</td><td>Nicholas Ray 和 Elia Kazan 有什么共同职业？ <br/> 让我们一步步思考。Nicholas Ray 的职业是导演、 <br/> 编剧和演员。Elia Kazan 的职业是导演、制片人、编剧和演员。所以 Nicholas Ray 和 Elia Kazan 的共同职业是导演、编剧和演员。 <br/> 导演, 编剧, 演员</td></tr><tr><td>问题 想法 <br/> 回答</td><td>Arthur's Magazine 和 First for Women 哪本杂志创刊更早？让我们一步步思考。Arthur's Magazine 创刊于 1844 年。First for Women 创刊于 1989 年。1844 (Arthur's Magazine) < 1989 (First for Women)，所以 Arthur's Magazine 创刊更早。 <br/> Arthur's Magazine</td></tr><tr><td>问题 想法 <br/> 回答</td><td>Pavel Urysohn 和 Leonid Levin 是因同类型的工作而闻名吗？让我们一步步思考。Pavel Urysohn 是一位数学家。Leonid Levin 是一位数学家和计算机科学家。所以 Pavel Urysohn 和 Leonid Levin 从事同类型的工作。 <br/> 是</td></tr><tr><td colspan="2">ReAct</td></tr><tr><td>问题</td><td>科罗拉多造山带东段延伸区域的海拔范围是多少？</td></tr><tr><td>想法 1</td><td>我需要搜索科罗拉多造山带，找到科罗拉多造山带东段延伸的区域，然后找到该区域的海拔范围。</td></tr><tr><td>动作 1 观察 1 <br/> 想法 2</td><td>搜索 [科罗拉多造山带] <br/> 科罗拉多造山运动是科罗拉多州及周边地区的一次造山活动。 <br/> 其中没有提到东段。所以我需要查找东段。</td></tr><tr><td>动作 2 观察 2 <br/> 想法 3 <br/> 动作 3</td><td>查找 [东段] <br/> (结果 1 / 1) 东段延伸至高地平原，被称为中平原造山带。 <br/> 科罗拉多造山带的东段延伸至高地平原。所以我需要搜索高地平原并找到其海拔范围。 <br/> 搜索 [高地平原]</td></tr><tr><td>观察 3 <br/> 想法 4 <br/> 动作 4</td><td>High Plains 指的是两个不同陆地区域之一 <br/> 我需要改为搜索 High Plains (United States)。 <br/> Search[High Plains (United States)]</td></tr></tbody></table>


Continued on next page
续下页


<table><tr><td></td><td>Hotpot QA Prompts - Continued from previous page</td></tr><tr><td>Observation 4 <br> Thought 5 <br> Action 5</td><td>The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m). [3] <br> High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft. <br> Finish [1,800 to 7,000 ft] <br> Musician and satirist Allie Goertz wrote a song about the "The Simpsons"</td></tr><tr><td>Question <br> Thought 1 <br> Action 1 Observation 1</td><td>character Milhouse, who Matt Groening named after who? <br> The question simplifies to "The Simpsons" character Milhouse is named after who. I only need to search Milhouse and find who it is named after. Search [Milhouse] <br> Milhouse Mussolini Van Houten is a recurring character in the Fox animated <br> television series The Simpsons voiced by Pamela Hayden and created by Matt <br> Groening.</td></tr><tr><td>Thought 2</td><td>The paragraph does not tell who Milhouse is named after, maybe I can look up <br> "named after".</td></tr><tr><td>Action 2 <br> Observation 2 <br> Thought 3</td><td>Lookup [named after] <br> (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose <br> middle name was Milhous. <br> Milhouse was named after U.S. president Richard Nixon, so the answer is <br> Richard Nixon.</td></tr><tr><td>Action 3 <br> Question</td><td>Finish[Richard Nixon] <br> Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?</td></tr><tr><td>Thought 1 <br> Action 1</td><td>I need to search Adam Clayton Powell and The Saimaa Gesture, and find which documentary is about Finnish rock groups.</td></tr><tr><td>Observation 1</td><td>Search [Adam Clayton Powell] <br> Could not find [Adam Clayton Powell]. Similar: ['Adam Clayton Powell III', 'Seventh Avenue (Manhattan)', 'Adam Clayton Powell Jr. State Office Building', 'Isabel Washington Powell', 'Adam Powell', 'Adam Clayton Powell (film)', 'Giancarlo Esposito'].</td></tr><tr><td>Thought 2 Action 2 Observation 2 <br> Thought 3</td><td>To find the documentary, I can search Adam Clayton Powell (film). Search [Adam Clayton Powell (film)] <br> Adam Clayton Powell is a 1989 American documentary film directed by Richard Kilberg. The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the PBS series The American Experience. <br> Adam Clayton Powell (film) is a documentary about an African-American politician, not Finnish rock groups. So the documentary about Finnish rock groups must instead be The Saimaa Gesture.</td></tr><tr><td>Action 3 <br> Question Thought 1 <br> Action 1</td><td>Finish [The Saimaa Gesture] <br> What profession does Nicholas Ray and Elia Kazan have in common? <br> I need to search Nicholas Ray and Elia Kazan, find their professions, then find the profession they have in common. <br> Search [Nicholas Ray]</td></tr><tr><td>Observation 1</td><td>Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.</td></tr><tr><td>Thought 2 <br> Action 2</td><td>Professions of Nicholas Ray are director, screenwriter, and actor. I need to search Elia Kazan next and find his professions. <br> Search [Elia Kazan]</td></tr><tr><td>Observation 2</td><td>Elia Kazan was an American film and theatre director, producer, screenwriter <br> and actor.</td></tr><tr><td>Thought 3 <br> Action 3</td><td>Professions of Elia Kazan are director, producer, screenwriter, and actor. <br> So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor. <br> Finish [director, screenwriter, actor]</td></tr><tr><td>Question <br> Thought 1</td><td>Which magazine was started first Arthur's Magazine or First for Women? <br> I need to search Arthur's Magazine and First for Women, and find which was started first.</td></tr><tr><td>Action 1 <br> Observation 1 <br> Thought 2</td><td>Search [Arthur's Magazine] <br> Arthur's Magazine (1844-1846) was an American literary periodical published <br> in Philadelphia in the 19th century. <br> Arthur's Magazine was started in 1844. I need to search First for Women next.</td></tr><tr><td>Action 2</td><td>Search [First for Women]</td></tr><tr><td>Observation 2 <br> Thought 3 <br> Action 3</td><td>First for Women is a woman's magazine published by Bauer Media Group in the USA. [1] The magazine was started in 1989. <br> First for Women was started in 1989. 1844 (Arthur's Magazine) < 1989 (First for Women), so Arthur's Magazine was started first. <br> Finish [Arthur's Magazine]</td></tr><tr><td>Question Thought 1 <br> Action 1</td><td>Were Pavel Urysohn and Leonid Levin known for the same type of work? <br> I need to search Pavel Urysohn and Leonid Levin, find their types of work, then find if they are the same. <br> Search [Pavel Urysohn]</td></tr></table>
<table><tbody><tr><td></td><td>Hotpot QA 提示词 - 接上页</td></tr><tr><td>观察 4 <br/> 思考 5 <br/> 行动 5</td><td>高地平原是大大平原的一个次级区域。自东向西，高地平原的海拔从约 1,800 英尺升至 7,000 英尺（550 至 2,130 米）。[3] <br/> 高地平原海拔从约 1,800 英尺升至 7,000 英尺，因此答案是 1,800 到 7,000 英尺。 <br/> 完成 [1,800 到 7,000 英尺] <br/> 音乐家兼讽刺作家 Allie Goertz 写了一首关于《辛普森一家》的歌</td></tr><tr><td>问题 <br/> 思考 1 <br/> 行动 1 观察 1</td><td>角色米尔豪斯，马特·格勒宁是以谁的名字命名的？ <br/> 问题简化为《辛普森一家》中的角色米尔豪斯是以谁的名字命名的。我只需要搜索米尔豪斯并找出其命名来源。搜索 [Milhouse] <br/> 米尔豪斯·墨索里尼·范霍滕是福克斯动画 <br/> 电视剧《辛普森一家》中的常驻角色，由帕梅拉·海登配音，马特·格勒宁 <br/> 创作。</td></tr><tr><td>思考 2</td><td>段落中没有说明米尔豪斯是以谁的名字命名的，也许我可以查找 <br/> “命名自”。</td></tr><tr><td>行动 2 <br/> 观察 2 <br/> 思考 3</td><td>查找 [named after] <br/> （结果 1 / 1）米尔豪斯是以美国总统理查德·尼克松的名字命名的，他的 <br/> 中间名是米尔豪斯。 <br/> 米尔豪斯是以美国总统理查德·尼克松的名字命名的，所以答案是 <br/> 理查德·尼克松。</td></tr><tr><td>行动 3 <br/> 问题</td><td>完成 [理查德·尼克松] <br/> 哪部纪录片是关于芬兰摇滚乐队的，《亚当·克莱顿·鲍威尔》还是《塞马湖姿态》？</td></tr><tr><td>思考 1 <br/> 行动 1</td><td>我需要搜索亚当·克莱顿·鲍威尔和《塞马湖姿态》，并找出哪部纪录片是关于芬兰摇滚乐队的。</td></tr><tr><td>观察 1</td><td>搜索 [Adam Clayton Powell] <br/> 无法找到 [Adam Clayton Powell]。相似项：['Adam Clayton Powell III', 'Seventh Avenue (Manhattan)', 'Adam Clayton Powell Jr. State Office Building', 'Isabel Washington Powell', 'Adam Powell', 'Adam Clayton Powell (film)', 'Giancarlo Esposito']。</td></tr><tr><td>思考 2 行动 2 观察 2 <br/> 思考 3</td><td>为了找到这部纪录片，我可以搜索 Adam Clayton Powell (film)。搜索 [Adam Clayton Powell (film)] <br/> 《亚当·克莱顿·鲍威尔》是一部 1989 年的美国纪录片，由理查德·基尔伯格执导。该片讲述了有影响力的非裔美国政治家小亚当·克莱顿·鲍威尔的兴衰。[3][4] 该片后来作为 PBS 系列节目《美国印象》的一部分播出。 <br/> 《亚当·克莱顿·鲍威尔》（电影）是一部关于非裔美国政治家的纪录片，不是关于芬兰摇滚乐队的。因此，关于芬兰摇滚乐队的纪录片必然是《塞马湖姿态》。</td></tr><tr><td>行动 3 <br/> 问题 思考 1 <br/> 行动 1</td><td>完成 [The Saimaa Gesture] <br/> 尼古拉斯·雷和伊利亚·卡赞有什么共同职业？ <br/> 我需要搜索尼古拉斯·雷和伊利亚·卡赞，找到他们的职业，然后找到他们的共同职业。 <br/> 搜索 [Nicholas Ray]</td></tr><tr><td>观察 1</td><td>尼古拉斯·雷（原名 Raymond Nicholas Kienzle Jr.，1911年8月7日 - 1979年6月16日）是美国电影导演、编剧和演员，以 1955 年的电影《无因的反叛》而闻名。</td></tr><tr><td>思考 2 <br/> 行动 2</td><td>尼古拉斯·雷的职业是导演、编剧和演员。接下来我需要搜索伊利亚·卡赞并找到他的职业。 <br/> 搜索 [Elia Kazan]</td></tr><tr><td>观察 2</td><td>伊利亚·卡赞是美国电影和戏剧导演、制片人、编剧 <br/> 和演员。</td></tr><tr><td>思考 3 <br/> 行动 3</td><td>伊利亚·卡赞的职业是导演、制片人、编剧和演员。 <br/> 因此尼古拉斯·雷和伊利亚·卡赞共同的职业是导演、编剧和演员。 <br/> 完成 [导演, 编剧, 演员]</td></tr><tr><td>问题 <br/> 思考 1</td><td>《亚瑟杂志》和《女性首选》哪本创刊更早？ <br/> 我需要搜索《亚瑟杂志》和《女性首选》，找出哪本创刊更早。</td></tr><tr><td>行动 1 <br/> 观察 1 <br/> 思考 2</td><td>搜索 [Arthur's Magazine] <br/> 《亚瑟杂志》（1844-1846）是 19 世纪在费城出版的 <br/> 美国文学期刊。 <br/> 《亚瑟杂志》创刊于 1844 年。接下来我需要搜索《女性首选》。</td></tr><tr><td>行动 2</td><td>搜索 [First for Women]</td></tr><tr><td>观察 2 <br/> 思考 3 <br/> 行动 3</td><td>First for Women 是由美国鲍尔媒体集团出版的女性杂志。[1] 该杂志创刊于 1989 年。<br/> First for Women 创刊于 1989 年。1844 年（Arthur's Magazine）< 1989 年（First for Women），因此 Arthur's Magazine 创刊更早。<br/> 结束 [Arthur's Magazine]</td></tr><tr><td>问题 思考 1 <br/> 行动 1</td><td>Pavel Urysohn 和 Leonid Levin 是否因同类工作而闻名？<br/> 我需要搜索 Pavel Urysohn 和 Leonid Levin，找到他们的工作类型，然后判断是否相同。<br/> 搜索 [Pavel Urysohn]</td></tr></tbody></table>


<table><tr><td>Observation 1</td><td>Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet <br> mathematician who is best known for his contributions in dimension theory.</td></tr><tr><td>Thought 2</td><td>Pavel Urysohn is a mathematician. I need to search Leonid Levin next and <br> find its type of work.</td></tr><tr><td>Action 2</td><td>Search [Leonid Levin]</td></tr><tr><td>Observation 2</td><td>Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.</td></tr><tr><td>Thought 3</td><td>Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.</td></tr><tr><td>Action 3</td><td>Finish [yes]</td></tr></table>
<table><tbody><tr><td>观察 1</td><td>帕维尔·萨穆伊洛维奇·乌雷松（1898年2月3日-1924年8月17日）是一位苏联<br/>数学家，以其在维数理论方面的贡献而闻名。</td></tr><tr><td>思考 2</td><td>帕维尔·乌雷松是一位数学家。我接下来需要搜索列昂尼德·列文并<br/>寻找他的工作类型。</td></tr><tr><td>操作 2</td><td>搜索 [Leonid Levin]</td></tr><tr><td>观察 2</td><td>列昂尼德·阿纳托利耶维奇·列文是一位苏联裔美国数学家和计算机科学家。</td></tr><tr><td>思考 3</td><td>列昂尼德·列文是一位数学家和计算机科学家。因此帕维尔·乌雷松和列昂尼德·列文的工作类型相同。</td></tr><tr><td>操作 3</td><td>完成 [是]</td></tr></tbody></table>


### C.2 Fever
### C.2 发热


<table><tr><td></td><td>FEVER Prompts</td></tr><tr><td>Original</td><td>Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td></tr><tr><td>Claim Answer</td><td>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. SUPPORTS</td></tr><tr><td>Claim Answer</td><td>Stranger Things is set in Bloomington, Indiana. REFUTES</td></tr><tr><td>Claim Answer</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003. NOT ENOUGH INFO</td></tr><tr><td></td><td>Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td></tr><tr><td>Claim Action2 Observation 1</td><td>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. Search[Nikolaj Coster-Waldau] <br> Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and producer. He graduated from the Danish National School of Performing Arts in Copenhagen in 1993, [1] and had his breakthrough role in Denmark with the film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy drama series Game of Thrones, for which he received two Primetime Emmy Award nominations for Outstanding Supporting Actor in a Drama Series.. Coster-Waldau has appeared in numerous films in his native Denmark and Scandinavia, including Headhunters (2011) and A Thousand Times Good Night (2013). In the U.S, his debut film role was in the war film Black Hawk Down (2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a detective in the short-lived Fox television series New Amsterdam (2008), and appeared in the 2009 Fox television film Virtuality, originally intended as a pilot.</td></tr><tr><td>Action 2</td><td>Finish [SUPPORTS]</td></tr><tr><td></td><td></td></tr><tr><td>Claim <br> Action 1</td><td>Stranger Things is set in Bloomington, Indiana. <br> Search [Stranger Things]</td></tr><tr><td>Observation 1 <br> Action 2</td><td>Stranger Things is an American science fiction horror drama television series created by the Duffer Brothers. Set in the 1980s, primarily in the fictional town of Hawkins, Indiana, the series centers on a number of mysteries and supernatural events occurring around the town and their impact on an ensemble of child and adult characters. <br> Finish [REFUTES]</td></tr><tr><td></td><td></td></tr><tr><td>Claim <br> Action 1</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003. <br> Search [Beautiful]</td></tr><tr><td>Observation 1</td><td>Could not find [Beautiful]. Similar: ['Beautiful', 'Beautiful', Beautiful', 'A Beautiful Mind (film)', 'Beautiful (Christina Aguilera song)', 'Life Is Beautiful'].</td></tr><tr><td>Action 2 Observation 2</td><td>Search [Beautiful (Christina Aguilera song) <br> "Beautiful" is a song recorded by American singer Christina Aguilera for her fourth studio album, Stripped (2002).</td></tr><tr><td>Action 3 <br> Observation 3 <br> Action 4</td><td>Lookup [Billboard Hot 100] <br> (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the United States, where it was certified Gold for 500,000 units shipped. Finish[NOT ENOUGH INFO]</td></tr><tr><td>CoT</td><td>Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td></tr><tr><td>Claim Thought <br> Answer</td><td>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. <br> Nikolaj William Coster-Waldau appeared in the 2009 Fox television film <br> Virtuality, so he has worked with the Fox Broadcasting Company. <br> SUPPORTS</td></tr><tr><td>Claim Thought <br> Answer</td><td>Stranger Things is set in Bloomington, Indiana. <br> Stranger Things is in the fictional town of Hawkins, Indiana, not in Bloomington, Indiana. <br> REFUTES</td></tr></table>
<table><tbody><tr><td></td><td>FEVER 提示词</td></tr><tr><td>原文</td><td>判定是否存在支持 (SUPPORTS) 或反驳 (REFUTES) 某项主张的观察结果，或者是否存在信息不足 (NOT ENOUGH INFORMATION)。</td></tr><tr><td>主张 答案</td><td>尼古拉·科斯特-瓦尔道曾与福斯广播公司合作。支持</td></tr><tr><td>主张 答案</td><td>《怪奇物语》的背景设定在印第安纳州的布卢明顿。反驳</td></tr><tr><td>主张 答案</td><td>《Beautiful》在 2003 年登上了公告牌百强单曲榜第二名。信息不足</td></tr><tr><td></td><td>判定是否存在支持 (SUPPORTS) 或反驳 (REFUTES) 某项主张的观察结果，或者是否存在信息不足 (NOT ENOUGH INFORMATION)。</td></tr><tr><td>主张 操作2 观察1</td><td>尼古拉·科斯特-瓦尔道曾与福斯广播公司合作。搜索[尼古拉·科斯特-瓦尔道] <br/> 尼古拉·威廉·科斯特-瓦尔道（生于 1970 年 7 月 27 日）是丹麦演员和制片人。他于 1993 年毕业于哥本哈根的丹麦国家表演艺术学院，[1] 并凭借电影《看夜守》(1994) 在丹麦取得了突破。他在 HBO 奇幻剧集《权力的游戏》中饰演詹姆·兰尼斯特，并因此两次获得黄金时段艾美奖剧情类最佳男配角提名。科斯特-瓦尔道曾出演过家乡丹麦及斯堪的纳维亚的多部电影，包括《猎头游戏》(2011) 和《一千次晚安》(2013)。在美国，他的首部电影作品是战争片《黑鹰坠落》(2001)，饰演荣誉勋章获得者加里·戈登。[2] 随后，他在福斯电视短剧《永恒传说》(2008) 中饰演一名侦探，并出演了 2009 年的福斯电视电影《虚拟现实》，该片最初拟作为试播集。</td></tr><tr><td>操作 2</td><td>完成 [支持]</td></tr><tr><td></td><td></td></tr><tr><td>主张 <br/> 操作 1</td><td>《怪奇物语》的背景设定在印第安纳州的布卢明顿。 <br/> 搜索 [怪奇物语]</td></tr><tr><td>观察 1 <br/> 操作 2</td><td>《怪奇物语》是由达菲兄弟创作的美国科幻恐怖剧集。该剧背景设定在 20 世纪 80 年代，主要发生在一个虚构的印第安纳州小镇霍金斯，围绕该镇发生的一系列神秘和超自然事件及其对一群孩子和成人角色的影响展开。 <br/> 完成 [反驳]</td></tr><tr><td></td><td></td></tr><tr><td>主张 <br/> 操作 1</td><td>《Beautiful》在 2003 年登上了公告牌百强单曲榜第二名。 <br/> 搜索 [Beautiful]</td></tr><tr><td>观察 1</td><td>无法找到 [Beautiful]。相似项：['Beautiful', 'Beautiful', Beautiful', '美丽心灵 (电影)', 'Beautiful (克里斯蒂娜·阿奎莱拉歌曲)', '美丽人生']。</td></tr><tr><td>操作 2 观察 2</td><td>搜索 [Beautiful (克里斯蒂娜·阿奎莱拉歌曲) <br/> “Beautiful”是美国歌手克里斯蒂娜·阿奎莱拉为其第四张录音室专辑《裸》(2002) 录制的一首歌曲。</td></tr><tr><td>操作 3 <br/> 观察 3 <br/> 操作 4</td><td>查找 [公告牌百强单曲榜] <br/> (结果 1 / 3) 这首歌在美国公告牌百强单曲榜上最高排名第二，因出货量达 50 万张被认证为金唱片。完成[信息不足]</td></tr><tr><td>思维链</td><td>判定是否存在支持 (SUPPORTS) 或反驳 (REFUTES) 某项主张的观察结果，或者是否存在信息不足 (NOT ENOUGH INFORMATION)。</td></tr><tr><td>主张 思考 <br/> 答案</td><td>尼古拉·科斯特-瓦尔道曾与福斯广播公司合作。 <br/> 尼古拉·威廉·科斯特-瓦尔道曾出演 2009 年福斯电视电影 <br/> 《虚拟现实》，因此他曾与福斯广播公司合作。 <br/> 支持</td></tr><tr><td>主张 思考 <br/> 答案</td><td>《怪奇物语》的背景设定在印第安纳州的布卢明顿。 <br/> 《怪奇物语》设定在虚构的印第安纳州小镇霍金斯，而非印第安纳州布卢明顿。 <br/> 反驳</td></tr></tbody></table>


Continued on next page
续下页


<table><tr><td></td><td>Fever Prompts - Continued from previous page</td></tr><tr><td>Claim Thought <br> Answer</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003. <br> The song peaked at number two on the Billboard Hot 100 in the United States, but not sure if it was in 2003. <br> NOT ENOUGH INFO</td></tr><tr><td>ReAct</td><td>Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td></tr><tr><td colspan="2"></td></tr><tr><td>Claim Thought 1</td><td>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. <br> I need to search Nikolaj Coster-Waldau and find if he has worked with the Fox Broadcasting Company.</td></tr><tr><td>Action 1 Observation 1</td><td>Search [Nikolaj Coster-Waldau] <br> Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and producer. He graduated from the Danish National School of Performing Arts in Copenhagen in 1993, [1] and had his breakthrough role in Denmark with the film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy drama series Game of Thrones, for which he received two Primetime Emmy Award nominations for Outstanding Supporting Actor in a Drama Series.. Coster-Waldau has appeared in numerous films in his native Denmark and Scandinavia, including Headhunters (2011) and A Thousand Times Good Night (2013). In the U.S, his debut film role was in the war film Black Hawk Down (2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a detective in the short-lived Fox television series New Amsterdam (2008), and appeared in the 2009 Fox television film Virtuality, originally intended as a pilot.</td></tr><tr><td>Thought 2 <br> Action 2</td><td>Because he "appeared in the 2009 Fox television film Virtuality", he should have worked with the Fox Broadcasting Company. <br> Finish [SUPPORTS]</td></tr><tr><td></td><td></td></tr><tr><td>Claim Thought 1</td><td>Stranger Things is set in Bloomington, Indiana. <br> I should search for Stranger Things, and see if it is set in Bloomington, Indiana.</td></tr><tr><td>Action 1</td><td>Search [Stranger Things]</td></tr><tr><td>Observation 1</td><td>Stranger Things is an American science fiction horror drama television <br> series created by the Duffer Brothers. Set in the 1980s, primarily in <br> the fictional town of Hawkins, Indiana, the series centers on a number of <br> mysteries and supernatural events occurring around the town and their impact <br> on an ensemble of child and adult characters.</td></tr><tr><td>Thought 2</td><td>The observation says that it is set in a "fictional town of Hawkins, Indiana", so it is not set in Bloomington.</td></tr><tr><td>Action 2</td><td>Finish [REFUTES]</td></tr><tr><td>Claim</td><td>Beautiful reached number two on the Billboard Hot 100 in 2003.</td></tr><tr><td>Thought 1</td><td>I need to search the song Beautiful and find if it reached number two on the Billboard Hot 100 in 2003.</td></tr><tr><td>Action 1</td><td>Search [Beautiful]</td></tr><tr><td>Observation 1</td><td>Could not find [Beautiful]. Similar: ['Beautiful', 'Beautiful, Beautiful', 'A Beautiful Mind (film)', 'Beautiful (Christina Aguilera song)', 'Life Is Beautiful'].</td></tr><tr><td>Thought 2</td><td>From suggestions, I should search "Beautiful (Christina Aguilera song)" to find the song.</td></tr><tr><td>Action 2</td><td>Search[Beautiful (Christina Aguilera song)]</td></tr><tr><td>Observation 2</td><td>"Beautiful" is a song recorded by American singer Christina Aguilera for her fourth studio album, Stripped (2002).</td></tr><tr><td>Thought 3</td><td>It does not mention Billboard, so I need to look up "Billboard Hot 100" to find if it reached number two on it in 2003.</td></tr><tr><td>Action 3</td><td>Lookup [Billboard Hot 100]</td></tr><tr><td>Observation 3</td><td>(Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the United States, where it was certified Gold for 500,000 units shipped.</td></tr><tr><td>Thought 4</td><td>It only says the song peaked at number two on the Billboard Hot 100, but not if it was in 2003. I am not sure if this claim is true or not.</td></tr><tr><td>Action 4</td><td>Finish [NOT ENOUGH INFO]</td></tr></table>
<table><tbody><tr><td></td><td>Fever 提示词 - 续前页</td></tr><tr><td>主张 思维 <br/> 回答</td><td>《Beautiful》在 2003 年位列公告牌百强单曲榜第二名。 <br/> 该曲在美国公告牌百强单曲榜最高排名第二，但不确定是否是在 2003 年。 <br/> 信息不足</td></tr><tr><td>ReAct</td><td>判断是否存在支持或反驳主张的观察结果，或者信息不足。</td></tr><tr><td colspan="2"></td></tr><tr><td>主张 思维 1</td><td>尼古拉·科斯特-瓦尔道曾就职于福克斯广播公司。 <br/> 我需要搜索尼古拉·科斯特-瓦尔道，查看他是否曾就职于福克斯广播公司。</td></tr><tr><td>操作 1 观察 1</td><td>搜索 [尼古拉·科斯特-瓦尔道] <br/> 尼古拉·威廉·科斯特-瓦尔道（生于 1970 年 7 月 27 日）是丹麦演员兼制片人。他 1993 年毕业于哥本哈根丹麦国家表演艺术学院，[1] 凭借电影《看门人》（1994 年）在丹麦实现突破。他在 HBO 奇幻剧集《权力的游戏》中饰演詹姆·兰尼斯特，并因此获得两项黄金时段艾美奖剧情类最佳男配角提名。科斯特-瓦尔道曾出演家乡丹麦及斯堪的纳维亚的多部电影，包括《猎头游戏》（2011 年）和《一千次晚安》（2013 年）。在美国，他的银幕处女作是战争片《黑鹰坠落》（2001 年），饰演荣誉勋章获得者加里·戈登。[2] 随后，他在福克斯短播电视剧《新阿姆斯特丹》（2008 年）中饰演一名侦探，并出演了 2009 年福克斯电视电影《虚拟现实》，该片原计划作为试播集。</td></tr><tr><td>思维 2 <br/> 操作 2</td><td>由于他“出演了 2009 年福克斯电视电影《虚拟现实》”，他应该曾就职于福克斯广播公司。 <br/> 结束 [支持]</td></tr><tr><td></td><td></td></tr><tr><td>主张 思维 1</td><td>《怪奇物语》的背景设定在印第安纳州布卢明顿。 <br/> 我应该搜索《怪奇物语》，查看其背景是否设定在印第安纳州布卢明顿。</td></tr><tr><td>操作 1</td><td>搜索 [怪奇物语]</td></tr><tr><td>观察 1</td><td>《怪奇物语》是由达菲兄弟创作的美国科幻恐怖剧情电视剧。 <br/> 该剧背景设定在 20 世纪 80 年代，主要发生在印第安纳州一个名为霍金斯的虚构小镇， <br/> 故事围绕该镇发生的一系列神秘和超自然事件及其对 <br/> 一群儿童和成人角色的影响展开。</td></tr><tr><td>思维 2</td><td>观察结果显示其背景设定在“印第安纳州虚构的霍金斯小镇”，因此并非设定在布卢明顿。</td></tr><tr><td>操作 2</td><td>结束 [反驳]</td></tr><tr><td>主张</td><td>《Beautiful》在 2003 年位列公告牌百强单曲榜第二名。</td></tr><tr><td>思维 1</td><td>我需要搜索歌曲《Beautiful》，查看它是否在 2003 年位列公告牌百强单曲榜第二名。</td></tr><tr><td>操作 1</td><td>搜索 [Beautiful]</td></tr><tr><td>观察 1</td><td>未找到 [Beautiful]。相似结果：['Beautiful', 'Beautiful, Beautiful', '美丽心灵 (电影)', 'Beautiful (克里斯蒂娜·阿奎莱拉歌曲)', '美丽人生']。</td></tr><tr><td>思维 2</td><td>根据建议，我应该搜索“Beautiful (Christina Aguilera song)”来寻找这首歌。</td></tr><tr><td>操作 2</td><td>搜索 [Beautiful (Christina Aguilera song)]</td></tr><tr><td>观察 2</td><td>《Beautiful》是美国歌手克里斯蒂娜·阿奎莱拉为她的第四张录音室专辑《Stripped》（2002 年）录制的歌曲。</td></tr><tr><td>思维 3</td><td>文中未提及公告牌，所以我需要查阅“公告牌百强单曲榜”，查看它是否在 2003 年位列该榜单第二名。</td></tr><tr><td>操作 3</td><td>查阅 [公告牌百强单曲榜]</td></tr><tr><td>观察 3</td><td>(结果 1 / 3) 该曲在美国公告牌百强单曲榜最高排名第二，并因出货量达 50 万张获得金唱片认证。</td></tr><tr><td>思维 4</td><td>文中仅提到该曲在公告牌百强单曲榜最高排名第二，但未说明是否在 2003 年。我不确定这一主张是否真实。</td></tr><tr><td>操作 4</td><td>结束 [信息不足]</td></tr></tbody></table>


Table 6: Prompts used for the Webshop experiments in Section 4. We use the prompt on the left for Act, and use the prompt on the right (add reasoning) for ReAct.
表 6：第 4 节中 Webshop 实验所使用的提示词。左侧为 Act 的提示词，右侧（添加了推理）为 ReAct 的提示词。


Instruction: i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars
指令：我想要一瓶 3 盎司的明亮柑橘味敏感肌止汗剂，且价格低于 50.00 美元


Act ReAct
Act ReAct


Action: search [3 ounce bright citrus deodorant sensitive skin] Action: search 3 ounce bright citrus deodorant sensitive skin
动作：搜索 [3 盎司 明亮柑橘味 止汗剂 敏感肌] 动作：搜索 3 盎司 明亮柑橘味 止汗剂 敏感肌


Observation: Observation:
观察： 观察：


Back to Search Back to Search
返回搜索 返回搜索


Page 1 (Total results: 50) Page 1 (Total results: 50)
第 1 页（总结果：50） 第 1 页（总结果：50）


[Next] [Next]
[下一页] [下一页]


[B078GWRC1J] [B078GWRC1J]
[B078GWRC1J] [B078GWRC1J]


Bright Citrus Deodorant by Earth Mama | Natural and Safe for Bright Citrus Deodorant by Earth Mama | Natural and Safe for
Earth Mama 明亮柑橘止汗剂 | 天然安全，适用于 Earth Mama 明亮柑橘止汗剂 | 天然安全，适用于


Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic
敏感肌、孕期及哺乳期，含有机金盏花 3 盎司 敏感肌、孕期及哺乳期，含有机


Calendula 3-Ounce
金盏花 3 盎司


\$10.99 \$10.99



[B078GTKVXY]
[B078GTKVXY]


[B078GTKVXY]
[B078GTKVXY]


Ginger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce
Earth Mama 鲜姜止汗剂 | 天然安全，适用于敏感肌、孕期及哺乳期，含有机金盏花 3 盎司


Ginger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic
Earth Mama 鲜姜止汗剂 | 天然安全，适用于敏感肌、孕期及哺乳期，含有机


Calendula 3-Ounce
金盏花 3盎司


\$10.99 \$10.99



[B08KBVJ4XN] [B08KBVJ4XN]
[B08KBVJ4XN] [B08KBVJ4XN]


Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men , Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar & Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)
Barrel and Oak - 无铝防臭剂，男士除臭剂，植物精油香型，24小时异味防护，雪松与藿香香调，温和呵护敏感肌（高山鼠尾草，2.7盎司，2支装）


Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men , Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar & Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)
Barrel and Oak - 无铝防臭剂，男士除臭剂，植物精油香型，24小时异味防护，雪松与藿香香调，温和呵护敏感肌（高山鼠尾草，2.7盎司，2支装）


\$15.95 \$15.95



Action: think[B078GWRC1J and B078GTKVXY are bright citrus deodorant less then 50 dollars. I can check B078GWRC1J first. Observation: OK. Action: click [B078GWRC1J]
行动：思考[B078GWRC1J和B078GTKVXY是低于50美元的清新柑橘除臭剂。我可以先检查B078GWRC1J。观察：好。行动：点击 [B078GWRC1J]


Observation:
观察：


[Back to Search]
[返回搜索]


Prev
上一页


scent [assorted scents][bright citrus][calming lavender][ginger [fresh] simply non-scents] Action: click [B078GWRC1J]
香型 [多种香型][清新柑橘][舒缓薰衣草][生姜 [清爽] 纯净无香] 行动：点击 [B078GWRC1J]


Observation:
观察：


size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2 -pack)] [Back to Search]
规格 [旅行装 (4支装)][3盎司 (1支装)][3盎司 (2支装)] [返回搜索]


Prev
上一页


size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2 -pack)]
规格 [旅行装 (4支装)][3盎司 (1支装)][3盎司 (2支装)]


Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce
Earth Mama 清新柑橘除臭剂 | 天然安全，适用于敏感肌、孕期及哺乳期，含有机金盏花 3盎司


scent [assorted scents][bright citrus][calming lavender][ginger [fresh] simply non-scents]
香型 [多种香型][清新柑橘][舒缓薰衣草][生姜 [清爽] 纯净无香]


Price: \$10.99
价格：$10.99


Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce Rating: N.A.
Earth Mama 亮泽柑橘除臭剂 | 敏感肌、孕期及哺乳期天然安全之选，含有机金盏花 3盎司 评分：暂无


[Description]
[描述]


[Features]
[特性]


[Reviews] Price: \$10.99
[评论] 价格：$10.99


[Buy Now] Rating: N.A.
[立即购买] 评分：暂无


[Description]
[描述]


Action: click[bright citrus] [Features]
操作：点击[亮泽柑橘] [特性]


Observation: You have clicked bright citrus. [Reviews]
观察：你已点击亮泽柑橘。[评论]


[Buy Now]
[立即购买]


Action: click[3 ounce (pack of 1)]
操作：点击[3盎司（1支装）]


Observation: You have clicked 3 ounce (pack of 1). Action: think[For 3 ounce bottle of bright citrus deodorant
观察：你已点击3盎司（1支装）。操作：思考[针对3盎司瓶装亮泽柑橘除臭剂


Action: click[Buy Now]
操作：点击[立即购买]


for sensitive skin, the item has options 'bright citrus' and '3 ounce (pack of 1)' and seems good to buy.]
适用于敏感肌，该商品有“亮泽柑橘”和“3盎司（1支装）”选项，看起来值得购买。]


Observation: OK.
观察：好的。


Action: click[bright citrus]
操作：点击[亮泽柑橘]


Observation: You have clicked bright citrus.
观察：你已点击亮泽柑橘。


Action: click[3 ounce (pack of 1)]
操作：点击[3盎司（1瓶装）]


Observation: You have clicked 3 ounce (pack of 1).
观察：你点击了3盎司（1瓶装）。


Action: click[Buy Now]
操作：点击[立即购买]


C. 4 ALFWORLD
C. 4 ALFWORLD


---



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You are in the middle of a room. Looking quickly around you, you see a cabinet
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你位于房间中央。快速环顾四周，你看到了橱柜


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;13，橱柜12，橱柜11，橱柜10，橱柜9，橱柜8，橱柜


7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet
7，橱柜6，橱柜5，橱柜4，橱柜3，橱柜2，橱柜


&nbsp;&nbsp;&nbsp;&nbsp;1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a
&nbsp;&nbsp;&nbsp;&nbsp;1，咖啡机1，台面1，餐桌1，抽屉1，冰箱1，


&nbsp;&nbsp;&nbsp;&nbsp;garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a
&nbsp;&nbsp;&nbsp;&nbsp;垃圾桶1，微波炉1，架子3，架子2，架子1，水槽1，


stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.
炉灶4，炉灶3，炉灶2，炉灶1，以及烤面包机1。


&nbsp;&nbsp;&nbsp;&nbsp;Your task is to: put a clean lettuce in diningtable.
&nbsp;&nbsp;&nbsp;&nbsp;你的任务是：将一颗干净的生菜放入餐桌。


&nbsp;&nbsp;&nbsp;&nbsp;> go to fridge 1
&nbsp;&nbsp;&nbsp;&nbsp;> 前往冰箱 1


&nbsp;&nbsp;&nbsp;&nbsp;The fridge 1 is closed.
&nbsp;&nbsp;&nbsp;&nbsp;冰箱1是关着的。


&nbsp;&nbsp;&nbsp;&nbsp;> open fridge 1
&nbsp;&nbsp;&nbsp;&nbsp;> 打开冰箱 1


You open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a
你打开了冰箱1。冰箱1已打开。在里面，你看到了杯子3，鸡蛋2，


&nbsp;&nbsp;&nbsp;&nbsp;potato 3, and a potato 2.
&nbsp;&nbsp;&nbsp;&nbsp;土豆3，以及土豆2。


&nbsp;&nbsp;&nbsp;&nbsp;> go to diningtable 1
&nbsp;&nbsp;&nbsp;&nbsp;> 前往 diningtable 1


On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a
在 diningtable 1 上，你看到了 apple 1，bread 1，butterknife 2，cup 2，以及一个


fork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a
fork 2，knife 2，knife 1，ladle 1，lettuce 1，mug 2，mug 1，pan 2，以及一个


&nbsp;&nbsp;&nbsp;&nbsp;peppershaker 1, a spatula 3, a tomato 2, and a tomato 1.
&nbsp;&nbsp;&nbsp;&nbsp;peppershaker 1，spatula 3，tomato 2 和 tomato 1。


> take lettuce 1 from diningtable 1
> 从 diningtable 1 拿取 lettuce 1


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You pick up the lettuce 1 from the diningtable 1.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你从 diningtable 1 上拿起了 lettuce 1。


&nbsp;&nbsp;&nbsp;&nbsp;> go to sinkbasin 1
&nbsp;&nbsp;&nbsp;&nbsp;> 前往 sinkbasin 1


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;On the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在 sinkbasin 1 上，你看到了 apple 2，ladle 2，spoon 1 和 tomato 3。


&nbsp;&nbsp;&nbsp;&nbsp;> clean lettuce 1 with sinkbasin 1
&nbsp;&nbsp;&nbsp;&nbsp;> 用 sinkbasin 1 清洗 lettuce 1


&nbsp;&nbsp;&nbsp;&nbsp;You clean the lettuce 1 using the sinkbasin 1.
&nbsp;&nbsp;&nbsp;&nbsp;你使用 sinkbasin 1 清洗了 lettuce 1。


> think: Now I clean a lettuce (1). Next, I need to put it in/on diningtable 1.
> 思考：现在我清洗了 lettuce (1)。下一步，我需要把它放入/放在 diningtable 1 上。


OK.



&nbsp;&nbsp;&nbsp;&nbsp;> go to diningtable 1
&nbsp;&nbsp;&nbsp;&nbsp;> 前往 diningtable 1


On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork
在 diningtable 1 上，你看到了 apple 1，bread 1，butterknife 2，cup 2，fork


2 , a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a
2，knife 2，knife 1，ladle 1，mug 2，mug 1，pan 2，peppershaker 1，以及一个


&nbsp;&nbsp;&nbsp;&nbsp;spatula 3, a tomato 2, and a tomato 1.
&nbsp;&nbsp;&nbsp;&nbsp;spatula 3，tomato 2 和 tomato 1。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;> put lettuce 1 in/on diningtable 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;> 将 lettuce 1 放入/放在 diningtable 1


You put the lettuce 1 in/on the diningtable 1.
你把生菜 1 放在餐桌 1 上/里。


---



Table 8: An ReAct prompt on the ALFWorld clean task.
表 8：ALFWorld 清洁任务中的 ReAct 提示词。


---



You are in the middle of a room. Looking quickly around you, you see a cabinet
你在一间房子的中央。快速环顾四周，你看到橱柜


13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet
13、橱柜 12、橱柜 11、橱柜 10、橱柜 9、橱柜 8、橱柜


7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet
7、橱柜 6、橱柜 5、橱柜 4、橱柜 3、橱柜 2、橱柜


1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a
1、咖啡机 1、台面 1、餐桌 1、抽屉 1、冰箱 1、


garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a
垃圾桶 1、微波炉 1、架子 3、架子 2、架子 1、水槽 1、


stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.
炉灶 4、炉灶 3、炉灶 2、炉灶 1，以及烤面包机 1。


Your task is to: put a clean lettuce in diningtable.
你的任务是：将一颗干净的生菜放在餐桌上。


> think: To solve the task, I need to find and take a lettuce, then clean it with
> 思考：为了完成任务，我需要找到并拿到一颗生菜，然后用


sinkbasin, then put it in diningtable.
水槽清洗它，最后把它放在餐桌上。


> think: First I need to find a lettuce. A lettuce is more likely to appear in
> 思考：首先我需要找到生菜。生菜更有可能出现在


&nbsp;&nbsp;&nbsp;&nbsp;fridge (1), diningtable (1), sinkbasin (1), stoveburner (1-3), cabinet (1-13). I
&nbsp;&nbsp;&nbsp;&nbsp;冰箱 (1)、餐桌 (1)、水槽 (1)、炉灶 (1-3)、橱柜 (1-13)。我


&nbsp;&nbsp;&nbsp;&nbsp;can check one by one, starting with fridge 1 .
&nbsp;&nbsp;&nbsp;&nbsp;可以逐一检查，从冰箱 1 开始。


OK.



$>$ go to fridge 1
$>$ 前往冰箱 1


The fridge 1 is closed.
冰箱 1 是关着的。


> open fridge 1
> 打开冰箱 1


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你打开了冰箱 1。冰箱 1 已打开。在里面，你看到了杯子 3、鸡蛋 2、


&nbsp;&nbsp;&nbsp;&nbsp;potato 3, and a potato 2.
&nbsp;&nbsp;&nbsp;&nbsp;土豆 3 和土豆 2。


&nbsp;&nbsp;&nbsp;&nbsp;> go to diningtable 1
&nbsp;&nbsp;&nbsp;&nbsp;> 前往餐桌 1


On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a
在餐桌 1 上，你看到了苹果 1、面包 1、黄油刀 2、杯子 2、


fork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a
叉子 2、刀 2、刀 1、长柄勺 1、生菜 1、马克杯 2、马克杯 1、平底锅 2、


peppershaker 1, a spatula 3, a tomato 2, and a tomato 1.
胡椒粉瓶 1、铲子 3、西红柿 2 和西红柿 1。


&nbsp;&nbsp;&nbsp;&nbsp;> think: Now I find a lettuce (1). Next, I need to take it.
&nbsp;&nbsp;&nbsp;&nbsp;> 思考：现在我找到了生菜 (1)。接下来，我需要拿起它。


OK.



> take lettuce 1 from diningtable 1
> 从餐桌 1 拿起生菜 1


&nbsp;&nbsp;&nbsp;&nbsp;You pick up the lettuce 1 from the diningtable 1.
&nbsp;&nbsp;&nbsp;&nbsp;你从餐桌 1 上拿起了生菜 1。


> think: Now I take a lettuce (1). Next, I need to go to sinkbasin (1) and clean
> 思考：现在我拿到了生菜 (1)。接下来，我需要前往洗手池 (1) 并清洗它。


it.



OK.



> go to sinkbasin 1
> 前往洗手池 1


&nbsp;&nbsp;&nbsp;&nbsp;On the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.
&nbsp;&nbsp;&nbsp;&nbsp;在洗手池 1 上，你看到了苹果 2、长柄勺 2、勺子 1 和西红柿 3。


&nbsp;&nbsp;&nbsp;&nbsp;> clean lettuce 1 with sinkbasin 1
&nbsp;&nbsp;&nbsp;&nbsp;> 在洗手池 1 清洗生菜 1


&nbsp;&nbsp;&nbsp;&nbsp;You clean the lettuce 1 using the sinkbasin 1.
&nbsp;&nbsp;&nbsp;&nbsp;你利用洗手池 1 清洗了生菜 1。


> think: Now I clean a lettuce (1). Next, I need to put it in/on diningtable 1.
> 思考：现在我清洗了生菜 (1)。接下来，我需要把它放入/放在餐桌 1 上。


OK.



$>$ go to diningtable 1
$>$ 去餐桌 1


On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork
在餐桌 1 上，你看到了苹果 1、面包 1、黄油刀 2、杯子 2、叉子


2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a
2、刀 2、刀 1、长柄勺 1、马克杯 2、马克杯 1、平底锅 2、胡椒瓶 1、


&nbsp;&nbsp;&nbsp;&nbsp;spatula 3, a tomato 2, and a tomato 1.
&nbsp;&nbsp;&nbsp;&nbsp;刮铲 3、番茄 2 和番茄 1。


&nbsp;&nbsp;&nbsp;&nbsp;> put lettuce 1 in/on diningtable 1
&nbsp;&nbsp;&nbsp;&nbsp;> 将生菜 1 放入/放在餐桌 1 上


&nbsp;&nbsp;&nbsp;&nbsp;You put the lettuce 1 in/on the diningtable 1.
&nbsp;&nbsp;&nbsp;&nbsp;你将生菜 1 放入/放在了餐桌 1 上。


---



Table 9: An ReAct-IM prompt on the ALFWorld clean task.
表 9：ALFWorld 清洁任务上的一个 ReAct-IM 提示。


---



You are in the middle of a room. Looking quickly around you, you see a cabinet
你在房间中央。快速环顾四周，你看到了橱柜


13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet
13、橱柜 12、橱柜 11、橱柜 10、橱柜 9、橱柜 8、橱柜


7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet
7、橱柜 6、橱柜 5、橱柜 4、橱柜 3、橱柜 2、橱柜


1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a
1、咖啡机 1、台面 1、餐桌 1、抽屉 1、冰箱 1、


garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a
垃圾桶 1、微波炉 1、架子 3、架子 2、架子 1、水槽 1、


stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.
炉灶火口 4、炉灶火口 3、炉灶火口 2、炉灶火口 1 以及烤面包机 1。


Your task is to: put a clean lettuce in diningtable.
你的任务是：将一颗洗净的生菜放在餐桌上。


> think: To solve the task, I need to find and take a lettuce, then clean it with
> 思考：为了完成任务，我需要找到并拿到一颗生菜，然后用


sinkbasin, then put it in diningtable.
水槽清洗它，最后把它放在餐桌上。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;> think: First I need to find a lettuce.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;> 思考：首先我需要找到生菜。


&nbsp;&nbsp;&nbsp;&nbsp;OK.



&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to fridge 1
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往冰箱 1


&nbsp;&nbsp;&nbsp;&nbsp;The fridge 1 is closed.
&nbsp;&nbsp;&nbsp;&nbsp;冰箱 1 是关着的。


&nbsp;&nbsp;&nbsp;&nbsp;> think: First I need to find a lettuce.
&nbsp;&nbsp;&nbsp;&nbsp;> 思考：首先我需要找到生菜。


&nbsp;&nbsp;&nbsp;&nbsp;OK.



&nbsp;&nbsp;&nbsp;&nbsp;> open fridge 1
&nbsp;&nbsp;&nbsp;&nbsp;> 打开冰箱 1


You open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a
你打开了冰箱 1。冰箱 1 已打开。在里面，你看到了杯子 3、鸡蛋 2、


&nbsp;&nbsp;&nbsp;&nbsp;potato 3, and a potato 2.
&nbsp;&nbsp;&nbsp;&nbsp;土豆 3 和土豆 2。


&nbsp;&nbsp;&nbsp;&nbsp;> think: First I need to find a lettuce.
&nbsp;&nbsp;&nbsp;&nbsp;> 思考：首先我需要找到生菜。


&nbsp;&nbsp;&nbsp;&nbsp;OK.



&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to diningtable 1
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往餐桌 1


On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a
在餐桌 1 上，你看到了苹果 1、面包 1、餐刀 2、杯子 2、


fork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a
叉子 2、刀子 2、刀子 1、长柄勺 1、生菜 1、马克杯 2、马克杯 1、平底锅 2、


&nbsp;&nbsp;&nbsp;&nbsp;peppershaker 1, a spatula 3, a tomato 2, and a tomato 1.
&nbsp;&nbsp;&nbsp;&nbsp;胡椒瓶 1、铲子 3、西红柿 2 和西红柿 1。


&nbsp;&nbsp;&nbsp;&nbsp;> think: First I need to find a lettuce.
&nbsp;&nbsp;&nbsp;&nbsp;> 思考：首先我需要找到生菜。


&nbsp;&nbsp;&nbsp;&nbsp;OK.



> take lettuce 1 from diningtable 1
> 从餐桌 1 拿走生菜 1


&nbsp;&nbsp;&nbsp;&nbsp;You pick up the lettuce 1 from the diningtable 1.
&nbsp;&nbsp;&nbsp;&nbsp;你从餐桌 1 上拿起了生菜 1。


&nbsp;&nbsp;&nbsp;&nbsp;> think: I need to clean this lettuce (1) using sinkbasin 1.
&nbsp;&nbsp;&nbsp;&nbsp;> 思考：我需要用洗手池 1 清洗这颗生菜 (1)。


&nbsp;&nbsp;&nbsp;&nbsp;OK.



&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to sinkbasin 1
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往洗手池 1


&nbsp;&nbsp;&nbsp;&nbsp;On the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.
&nbsp;&nbsp;&nbsp;&nbsp;在洗手池 1 上，你看到了苹果 2、长柄勺 2、勺子 1 和西红柿 3。


> think: I need to clean this lettuce (1) using sinkbasin 1.
> 思考：我需要利用洗手池 1 清洗这颗生菜 (1)。


&nbsp;&nbsp;&nbsp;&nbsp;OK.



> clean lettuce 1 with sinkbasin 1
> 用洗手池 1 清洗生菜 1


&nbsp;&nbsp;&nbsp;&nbsp;You clean the lettuce 1 using the sinkbasin 1.
&nbsp;&nbsp;&nbsp;&nbsp;你利用洗手池 1 清洗了生菜 1。


&nbsp;&nbsp;&nbsp;&nbsp;> think: I need to put this cleaned lettuce (1) in/on diningtable 1.
&nbsp;&nbsp;&nbsp;&nbsp;> 思考：我需要把这颗洗净的生菜 (1) 放在餐桌 1 上。


&nbsp;&nbsp;&nbsp;&nbsp;OK.



&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to diningtable 1
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往餐桌 1


On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork
在餐桌 1 上，你看到了苹果 1、面包 1、黄油刀 2、杯子 2、叉子


2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a
2、餐刀 2、餐刀 1、长柄勺 1、马克杯 2、马克杯 1、平底锅 2、胡椒粉瓶 1、


&nbsp;&nbsp;&nbsp;&nbsp;spatula 3, a tomato 2, and a tomato 1.
&nbsp;&nbsp;&nbsp;&nbsp;铲子 3、西红柿 2 和西红柿 1。


> think: I need to put this cleaned lettuce (1) in/on diningtable 1.
> 思考：我需要把这颗洗净的生菜 (1) 放在餐桌 1 上。


&nbsp;&nbsp;&nbsp;&nbsp;OK.



&nbsp;&nbsp;&nbsp;&nbsp;> put lettuce 1 in/on diningtable 1
&nbsp;&nbsp;&nbsp;&nbsp;> 将生菜 1 放在餐桌 1 上


&nbsp;&nbsp;&nbsp;&nbsp;You put the lettuce 1 in/on the diningtable 1.
&nbsp;&nbsp;&nbsp;&nbsp;你将生菜 1 放在了餐桌 1 上。


---



D TRAJECTORIES
D 轨迹


### D.1 FEVER TRAJECTORIES
### D.1 FEVER 轨迹


We provide some trajectories produced by ReAct, CoTand Acton several randomly selected examples from the FEVER dev set below. Search results are omitted to space.
我们在下方提供了由 ReAct、CoT 和 Act 在 FEVER 开发集中随机选取的示例所生成的轨迹。出于空间考虑，搜索结果已省略。


<table><tr><td></td><td>Example 2491 (gt: REFUTES)</td></tr><tr><td>ReAct <br> Claim:</td><td>Bermuda Triangle is a loosely-defined region in the Pacific Ocean.</td></tr><tr><td>Thought 1: <br> Action 1: <br> Observation 1: <br> Thought 2:</td><td>I need to search Bermuda Triangle and find if it is in the Pacific Ocean. Search [Bermuda Triangle] <br> Reportedly haunted locations... <br> The observation says that it is in the western part of the North Atlantic</td></tr><tr><td>Action 2:</td><td>Ocean, so it is not in the Pacific Ocean. <br> Finish [REFUTES]</td></tr><tr><td>Observation 2:</td><td>Episode finished</td></tr><tr><td>Act <br> Claim: <br> Action 1 <br> Observation 1 <br> Action 2: <br> Observation 2:</td><td>Bermuda Triangle is a loosely-defined region in the Pacific Ocean. <br> : Search [Bermuda Triangle] <br> : Reportedly haunted locations:... <br> Finish [REFUTES] <br> Episode finished</td></tr><tr><td>CoT <br> Claim: <br> Thought: <br> Answer:</td><td>Bermuda Triangle is a loosely-defined region in the Pacific Ocean. <br> Let's think step by step. The Bermuda Triangle is a loosely-defined region in the Atlantic Ocean. The Pacific Ocean is not the Atlantic Ocean. REFUTES</td></tr></table>
<table><tbody><tr><td></td><td>示例 2491 (gt: REFUTES)</td></tr><tr><td>ReAct <br/> 索赔：</td><td>百慕大三角是太平洋中一个定义模糊的区域。</td></tr><tr><td>思考 1：<br/> 行动 1：<br/> 观察 1：<br/> 思考 2：</td><td>我需要搜索百慕大三角，看看它是否在太平洋。搜索 [百慕大三角] <br/> 据报道有鬼魂出没的地点... <br/> 观察结果显示它位于北大西洋西部</td></tr><tr><td>行动 2：</td><td>洋，所以它不在太平洋。 <br/> 完成 [REFUTES]</td></tr><tr><td>观察 2：</td><td>回合结束</td></tr><tr><td>Act <br/> 索赔：<br/> 行动 1 <br/> 观察 1 <br/> 行动 2：<br/> 观察 2：</td><td>百慕大三角是太平洋中一个定义模糊的区域。 <br/> ：搜索 [百慕大三角] <br/> ：据报道有鬼魂出没的地点：... <br/> 完成 [REFUTES] <br/> 回合结束</td></tr><tr><td>CoT <br/> 索赔：<br/> 思考：<br/> 回答：</td><td>百慕大三角是太平洋中一个定义模糊的区域。 <br/> 让我们逐步思考。百慕大三角是大西洋中一个定义模糊的区域。太平洋不是大西洋。REFUTES</td></tr></tbody></table>


<table><tr><td>ReAct <br> Claim: <br> Thought 1: <br> Action 1: <br> Observation 1: <br> Thought 2: <br> Action 2: <br> Observation 2:</td><td>Princess Mononoke is a film. <br> I need to search Princess Mononoke and find if it is a film. <br> Search [Princess Mononoke] <br> Princess Mononoke ... <br> From the observation, it says that Princess Mononoke is a film. <br> Finish [SUPPORTS] <br> Episode finished</td></tr><tr><td>Act. <br> Claim: <br> Action 1: <br> Observation 1: <br> Action 2: <br> Observation 2:</td><td>Princess Mononoke is a film. <br> Search[Princess Mononoke] <br> Princess Mononoke ... <br> Finish [SUPPORTS] <br> Episode finished</td></tr><tr><td>CoT</td><td></td></tr><tr><td>Claim:</td><td>Princess Mononoke is a film.</td></tr><tr><td>Thought:</td><td>Let's think step by step. Princess Mononoke is a movie, and movies are films.</td></tr><tr><td>Answer:</td><td>SUPPORTS</td></tr></table>
<table><tbody><tr><td>ReAct <br/> 结论： <br/> 思考 1： <br/> 行动 1： <br/> 观察 1： <br/> 思考 2： <br/> 行动 2： <br/> 观察 2：</td><td>《幽灵公主》是一部电影。 <br/> 我需要搜索《幽灵公主》并确认它是否为电影。 <br/> 搜索 [幽灵公主] <br/> 幽灵公主 ... <br/> 根据观察，结果显示《幽灵公主》是一部电影。 <br/> 完成 [支持] <br/> 环节结束</td></tr><tr><td>行动。 <br/> 结论： <br/> 行动 1： <br/> 观察 1： <br/> 行动 2： <br/> 观察 2：</td><td>《幽灵公主》是一部电影。 <br/> 搜索 [幽灵公主] <br/> 幽灵公主 ... <br/> 完成 [支持] <br/> 环节结束</td></tr><tr><td>CoT</td><td></td></tr><tr><td>结论：</td><td>《幽灵公主》是一部电影。</td></tr><tr><td>思考：</td><td>让我们逐步思考。《幽灵公主》是一部 movie，而 movie 即为电影。</td></tr><tr><td>答案：</td><td>支持</td></tr></tbody></table>


<table><tr><td></td><td>Example 1951 (gt: REFUTES)</td></tr><tr><td>ReAct</td><td></td></tr><tr><td>Claim:</td><td>Soyuz was part of the American space program.</td></tr><tr><td>Thought 1:</td><td>I need to search Soyuz and find if it was part of the American space program.</td></tr><tr><td>Action 1:</td><td>Search [Soyuz]</td></tr><tr><td>Observation 1:</td><td>Soyuz is a transliteration...</td></tr><tr><td>Thought 2:</td><td>It does not mention the American space program, so I need to search <br> "American space program" to find if it was part of it.</td></tr><tr><td>Action 2:</td><td>Search [American space program]</td></tr><tr><td>Observation 2:</td><td>The space policy...</td></tr><tr><td>Thought 3:</td><td>It does not mention Soyuz, so I am not sure if this claim is true or not.</td></tr><tr><td>Action 3:</td><td>Finish[NOT ENOUGH INFO]</td></tr><tr><td>Observation 3:</td><td>Episode finished</td></tr><tr><td colspan="2">Act</td></tr><tr><td>Claim:</td><td>Soyuz was part of the American space program.</td></tr><tr><td>Action 1:</td><td>Search [Soyuz]</td></tr><tr><td>Observation 1:</td><td>Soyuz is a transliteration...</td></tr><tr><td>Action 2:</td><td>Finish [NOT ENOUGH INFO]</td></tr><tr><td>Observation 2:</td><td>Episode finished</td></tr><tr><td colspan="2">CoT</td></tr><tr><td>Claim:</td><td>Soyuz was part of the American space program.</td></tr><tr><td>Thought:</td><td>Let's think step by step. Soyuz is a Russian spacecraft. The American space program is NASA. NASA and Russia have worked together on the International Space Station.</td></tr><tr><td>Answer:</td><td>SUPPORTS</td></tr></table>
<table><tbody><tr><td></td><td>示例 1951 (gt: REFUTES)</td></tr><tr><td>ReAct</td><td></td></tr><tr><td>断言：</td><td>联盟号是美国航天计划的一部分。</td></tr><tr><td>思考 1：</td><td>我需要搜索“联盟号”，查看它是否属于美国航天计划。</td></tr><tr><td>行动 1：</td><td>搜索 [联盟号]</td></tr><tr><td>观察 1：</td><td>联盟号是一个音译词……</td></tr><tr><td>思考 2：</td><td>此处未提及美国航天计划，因此我需要搜索 <br/> “美国航天计划”来确认它是否包含其中。</td></tr><tr><td>行动 2：</td><td>搜索 [美国航天计划]</td></tr><tr><td>观察 2：</td><td>航天政策……</td></tr><tr><td>思考 3：</td><td>此处未提及联盟号，因此我不确定该断言是否属实。</td></tr><tr><td>行动 3：</td><td>完成 [信息不足]</td></tr><tr><td>观察 3：</td><td>流程结束</td></tr><tr><td colspan="2">Act</td></tr><tr><td>断言：</td><td>联盟号是美国航天计划的一部分。</td></tr><tr><td>行动 1：</td><td>搜索 [联盟号]</td></tr><tr><td>观察 1：</td><td>联盟号是一个音译词……</td></tr><tr><td>行动 2：</td><td>完成 [信息不足]</td></tr><tr><td>观察 2：</td><td>流程结束</td></tr><tr><td colspan="2">CoT</td></tr><tr><td>断言：</td><td>联盟号是美国航天计划的一部分。</td></tr><tr><td>思考：</td><td>让我们逐步思考。联盟号是俄罗斯航天器。美国航天计划由 NASA 负责。NASA 和俄罗斯在国际空间站项目上有过合作。</td></tr><tr><td>回答：</td><td>SUPPORTS</td></tr></tbody></table>


<table><tr><td colspan="2">Example 3208 (gt: REFUTES)</td></tr><tr><td colspan="2">ReAct</td></tr><tr><td>Claim:</td><td>Reign Over Me is an American film made in 2010.</td></tr><tr><td>Thought 1:</td><td>I need to search for Reign Over Me and find if it is an American film made in 2010.</td></tr><tr><td>Action 1:</td><td>Search [Reign Over Me]</td></tr><tr><td>Observation 1:</td><td>Reign Over Me ...</td></tr><tr><td>Thought 2:</td><td>The observation says that it is an American film made in 2007, so it is not made in 2010.</td></tr><tr><td>Action 2:</td><td>Finish [REFUTES]</td></tr><tr><td>Observation 2:</td><td>Episode finished</td></tr><tr><td colspan="2">Act</td></tr><tr><td>Claim:</td><td>Reign Over Me is an American film made in 2010.</td></tr><tr><td>Action 1:</td><td>Search [Reign Over Me]</td></tr><tr><td>Action 2:</td><td>Finish [SUPPORTS]</td></tr><tr><td>Observation 2:</td><td>Episode finished</td></tr><tr><td colspan="2">CoT</td></tr><tr><td>Claim:</td><td>Reign Over Me is an American film made in 2010.</td></tr><tr><td>Thought:</td><td>Let's think step by step. First, is Reign Over Me an American film? Yes, it is. Second, was it made in 2010? Yes, it was.</td></tr><tr><td>Answer:</td><td>SUPPORTS</td></tr></table>
<table><tbody><tr><td colspan="2">示例 3208 (gt: REFUTES)</td></tr><tr><td colspan="2">ReAct</td></tr><tr><td>断言：</td><td>《从心开始》是一部 2010 年制作的美国电影。</td></tr><tr><td>思考 1：</td><td>我需要搜索《从心开始》，并确认它是否为一部 2010 年制作的美国电影。</td></tr><tr><td>操作 1：</td><td>搜索 [Reign Over Me]</td></tr><tr><td>观察 1：</td><td>《从心开始》...</td></tr><tr><td>思考 2：</td><td>观察结果显示这是一部 2007 年制作的美国电影，所以它不是在 2010 年制作的。</td></tr><tr><td>操作 2：</td><td>完成 [REFUTES]</td></tr><tr><td>观察 2：</td><td>环节结束</td></tr><tr><td colspan="2">Act</td></tr><tr><td>断言：</td><td>《从心开始》是一部 2010 年制作的美国电影。</td></tr><tr><td>操作 1：</td><td>搜索 [Reign Over Me]</td></tr><tr><td>操作 2：</td><td>完成 [SUPPORTS]</td></tr><tr><td>观察 2：</td><td>环节结束</td></tr><tr><td colspan="2">CoT</td></tr><tr><td>断言：</td><td>《从心开始》是一部 2010 年制作的美国电影。</td></tr><tr><td>思考：</td><td>让我们逐步思考。首先，《从心开始》是一部美国电影吗？是的。其次，它是 2010 年制作的吗？是的。</td></tr><tr><td>回答：</td><td>SUPPORTS</td></tr></tbody></table>


D. 2 ALFWORLD TRAJECTORIES
D. 2 ALFWORLD 轨迹


We provide an example trajectory of ReAct, Act, and ReAct-IM on the same ALFWorld game, pick_clean_then_place_in_recep-Knife-None-CounterTop-10/ trial_T20190909_110445_675754.
我们提供了 ReAct、Act 和 ReAct-IM 在同一个 ALFWorld 游戏（试运行：pick_clean_then_place_in_recep-Knife-None-CounterTop-10/ trial_T20190909_110445_675754）中的示例轨迹。


D.2.1 REACT TRAJECTORY
D.2.1 REACT 轨迹


ReAct is able to correctly find the knife, clean it, and then place it on a countertop.
ReAct 能够正确地找到刀，将其清洗干净，然后放置在台面上。


## ReAct ALFWorld trajectory
## ReAct ALFWorld 轨迹


---



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You are in the middle of a room. Looking quickly around you, you see a cabinet 6, a
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你位于房间中央。快速环顾四周，你看到了橱柜 6、


cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a
橱柜 5、橱柜 4、橱柜 3、橱柜 2、橱柜 1、咖啡机 1、


countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a
台面 3、台面 2、台面 1、抽屉 3、抽屉 2、抽屉 1、


fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin
冰箱 1、垃圾桶 1、微波炉 1、架子 3、架子 2、架子 1、水槽


1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster
1、炉灶火口 4、炉灶火口 3、炉灶火口 2、炉灶火口 1 以及烤面包机 1。


1.



&nbsp;&nbsp;&nbsp;&nbsp;Your task is to: put a clean knife in countertop.
&nbsp;&nbsp;&nbsp;&nbsp;你的任务是：将一把干净的刀放在台面上。


> think: To solve the task, I need to find and take a knife, then clean it with
> think: 为了完成任务，我需要找到并拿到一把刀，然后在水槽


&nbsp;&nbsp;&nbsp;&nbsp;sinkbasin, then put it in countertop.
&nbsp;&nbsp;&nbsp;&nbsp;中清洗，最后放在台面上。


OK.



> think: First I need to find a knife. A knife is more likely to appear in cabinet
> think: 首先我需要找到一把刀。刀更有可能出现在橱柜


(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),
(1-6)、抽屉 (1-3)、台面 (1-3)、冰箱 (1)、垃圾桶 (1)、架子 (1-3)、


sinkbasin (1), stoveburner (1-4), toaster (1). I can check one by one, starting with
水槽 (1)、炉灶火口 (1-4)、烤面包机 (1)。我可以从第一个开始逐一检查。


cabinet 1 .
橱柜 1。


OK.



> go to cabinet 1
> 前往橱柜 1


On the cabinet 1, you see a bowl 1.
在橱柜 1 上，你看到一个碗 1。


&nbsp;&nbsp;&nbsp;&nbsp;> go to cabinet 2
&nbsp;&nbsp;&nbsp;&nbsp;> 前往橱柜 2


&nbsp;&nbsp;&nbsp;&nbsp;The cabinet 2 is closed.
&nbsp;&nbsp;&nbsp;&nbsp;橱柜 2 是关着的。


&nbsp;&nbsp;&nbsp;&nbsp;> go to cabinet 3
&nbsp;&nbsp;&nbsp;&nbsp;> 前往橱柜 3


&nbsp;&nbsp;&nbsp;&nbsp;On the cabinet 3, you see a glassbottle 1.
&nbsp;&nbsp;&nbsp;&nbsp;在橱柜 3 上，你看到一个玻璃瓶 1。


> go to cabinet 4
> 前往橱柜 4


&nbsp;&nbsp;&nbsp;&nbsp;On the cabinet 4, you see a mug 1.
&nbsp;&nbsp;&nbsp;&nbsp;在橱柜 4 上，你看到一个马克杯 1。


> go to cabinet 5
> 前往橱柜 5


&nbsp;&nbsp;&nbsp;&nbsp;The cabinet 5 is closed.
&nbsp;&nbsp;&nbsp;&nbsp;橱柜 5 是关着的。


&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to cabinet 6
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往橱柜 6


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The cabinet 6 is closed.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;橱柜 6 是关着的。


&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to drawer 1
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往抽屉 1


&nbsp;&nbsp;&nbsp;&nbsp;The drawer 1 is closed.
&nbsp;&nbsp;&nbsp;&nbsp;抽屉 1 是关着的。


&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to drawer 2
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往抽屉 2


&nbsp;&nbsp;&nbsp;&nbsp;The drawer 2 is closed.
&nbsp;&nbsp;&nbsp;&nbsp;抽屉 2 是关着的。


> go to drawer 3
> 去抽屉 3


The drawer 3 is closed.
抽屉 3 是关着的。


$>$ go to countertop 1
$>$ 去台面 1


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在台面 1 上，你看到了生菜 2、马克杯 2、胡椒粉瓶 1 和勺子 2。


> go to countertop 2
> 去台面 2


On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a
在台面 2 上，你看到了杯子 1、洗碗海绵 1、玻璃瓶 3、小刀 1、


plate 2, a potato 3, and a statue 1.
盘子 2、土豆 3 和雕像 1。


> think: Now I find a knife (1). Next, I need to take it.
> 思考：现在我找到了一把小刀 (1)。下一步，我需要拿起它。


OK.



> take knife 1 from countertop 2
> 从台面 2 拿起小刀 1


&nbsp;&nbsp;&nbsp;&nbsp;You pick up the knife 1 from the countertop 2.
&nbsp;&nbsp;&nbsp;&nbsp;你从台面 2 拿起了小刀 1。


> think: Now I take a knife (1). Next, I need to go to sinkbasin (1) and clean it.
> 思考：现在我拿到了小刀 (1)。下一步，我需要去水槽 (1) 清洗它。


OK.



> go to sinkbasin 1
> 去水槽 1


On the sinkbasin 1, you see a fork 3, a lettuce 3, and a spatula 2.
在水槽 1 上，你看到了叉子 3、生菜 3 和铲子 2。


> clean knife 1 with sinkbasin 1
> 在水槽 1 清洗小刀 1


You clean the knife 1 using the sinkbasin 1.
你用水槽 1 清洗了小刀 1。


> think: Now I clean a knife (1). Next, I need to put it in/on countertop 1.
> think: 现在我清洗一把刀 (1)。接下来，我需要把它放在台面 1 上。


OK.



> go to countertop 1
> 前往台面 1


&nbsp;&nbsp;&nbsp;&nbsp;On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
&nbsp;&nbsp;&nbsp;&nbsp;在台面 1 上，你看到了生菜 2、马克杯 2、胡椒瓶 1 和勺子 2。


> put knife 1 in/on countertop 1
> 将刀 1 放在台面 1 上


&nbsp;&nbsp;&nbsp;&nbsp;You put the knife 1 in/on the countertop 1.
&nbsp;&nbsp;&nbsp;&nbsp;你把刀 1 放在了台面 1 上。


---



#### D.2.2 ACT TRAJECTORY
#### D.2.2 ACT 轨迹


Act is able to correctly find the knife, but tries to clean it using a sinkbasin before actually going to the sinkbasin. From there on, the model is unable to recover, and is stuck repeating the same sequence of commands. With ReAct, an existing thought would have concretely identified that the knife had been taken, and that the next subgoal was to go to a sinkbasin to clean it.
Act 能够正确找到刀，但在实际前往水槽之前就尝试清洗它。从此往后，模型无法恢复，陷入了重复相同命令序列的困境。若使用 ReAct，现有的思考过程本可以具体识别出刀已被拿走，且下一个子目标是前往水槽清洗它。


## Act ALFWorld trajectory
## Act ALFWorld 轨迹


---



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You are in the middle of a room. Looking quickly around you, you see a cabinet 6, a
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你在一间房子的中央。快速环顾四周，你看到了橱柜 6、


cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a
橱柜 5、橱柜 4、橱柜 3、橱柜 2、橱柜 1、咖啡机 1、


countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a
台面 3、台面 2、台面 1、抽屉 3、抽屉 2、抽屉 1、


fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin
冰箱 1、垃圾桶 1、微波炉 1、架子 3、架子 2、架子 1、水槽


1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster
1、炉灶火口 4、炉灶火口 3、炉灶火口 2、炉灶火口 1 以及烤面包机 1。


&nbsp;&nbsp;&nbsp;&nbsp;1.



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Your task is to: put a clean knife in countertop.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你的任务是：将一把干净的刀放在台面上。


&nbsp;&nbsp;&nbsp;&nbsp;> go to countertop 1
&nbsp;&nbsp;&nbsp;&nbsp;> 前往台面 1


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在台面 1 上，你看到了生菜 2、马克杯 2、胡椒瓶 1 和勺子 2。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;> go to countertop 2
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;> 前往 2 号台面


On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a
在 2 号台面上，你看到了杯子 1、洗碗海绵 1、玻璃瓶 3、刀子 1、


&nbsp;&nbsp;&nbsp;&nbsp;plate 2, a potato 3, and a statue 1.
&nbsp;&nbsp;&nbsp;&nbsp;盘子 2、土豆 3 和雕像 1。


&nbsp;&nbsp;&nbsp;&nbsp;> take knife 1 from countertop 2
&nbsp;&nbsp;&nbsp;&nbsp;> 从 2 号台面拿取刀子 1


&nbsp;&nbsp;&nbsp;&nbsp;You pick up the knife 1 from the countertop 2.
&nbsp;&nbsp;&nbsp;&nbsp;你从 2 号台面上拿起了刀子 1。


&nbsp;&nbsp;&nbsp;&nbsp;> go to countertop 1
&nbsp;&nbsp;&nbsp;&nbsp;> 前往 1 号台面


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在 1 号台面上，你看到了生菜 2、马克杯 2、胡椒粉瓶 1 和勺子 2。


&nbsp;&nbsp;&nbsp;&nbsp;> clean knife 1 with sinkbasin 1
&nbsp;&nbsp;&nbsp;&nbsp;> 在 1 号洗手池清洗刀子 1


&nbsp;&nbsp;&nbsp;&nbsp;Nothing happens.
&nbsp;&nbsp;&nbsp;&nbsp;什么也没发生。


&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to countertop 3
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往 3 号台面


On the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard
在 3 号台面上，你看到了面包 3、黄油刀 2、手机 1、信用卡


1 , a fork 2 , a houseplant 1 , a knife 2 , a spatula 1 , a statue 3 , a tomato 3 , a tomato
1、叉子 2、室内植物 1、刀子 2、铲子 1、雕像 3、西红柿 3、西红柿


&nbsp;&nbsp;&nbsp;&nbsp;2 , a tomato 1 , and a vase 2 .
&nbsp;&nbsp;&nbsp;&nbsp;2、西红柿 1 和花瓶 2。


&nbsp;&nbsp;&nbsp;&nbsp;> take knife 2 from countertop 3
&nbsp;&nbsp;&nbsp;&nbsp;> 从 3 号台面拿取刀子 2


&nbsp;&nbsp;&nbsp;&nbsp;Nothing happens.
&nbsp;&nbsp;&nbsp;&nbsp;什么也没发生。


&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to countertop 2
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往 2 号台面


On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a
在台面 2 上，你看到了杯子 1、洗碗海绵 1、玻璃瓶 3、盘子 2、


&nbsp;&nbsp;&nbsp;&nbsp;potato 3, and a statue 1.
&nbsp;&nbsp;&nbsp;&nbsp;土豆 3 和雕像 1。


&nbsp;&nbsp;&nbsp;&nbsp;> take knife 1 from countertop 2
&nbsp;&nbsp;&nbsp;&nbsp;> 从台面 2 拿取刀 1


&nbsp;&nbsp;&nbsp;&nbsp;Nothing happens.
&nbsp;&nbsp;&nbsp;&nbsp;毫无反应。


&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to countertop 1
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往台面 1


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在台面 1 上，你看到了生菜 2、马克杯 2、胡椒瓶 1 和勺子 2。


&nbsp;&nbsp;&nbsp;&nbsp;> clean knife 1 with sinkbasin 1
&nbsp;&nbsp;&nbsp;&nbsp;> 用水槽 1 清洗刀 1


Nothing happens.
毫无反应。


&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to countertop 3
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往台面 3


On the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard
在台面 3 上，你看到了面包 3、黄油刀 2、手机 1、信用卡


1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato
1、叉子 2、室内植物 1、刀 2、刮铲 1、雕像 3、西红柿 3、西红柿


&nbsp;&nbsp;&nbsp;&nbsp;2, a tomato 1, and a vase 2.
&nbsp;&nbsp;&nbsp;&nbsp;2、西红柿 1 和花瓶 2。


&nbsp;&nbsp;&nbsp;&nbsp;> take knife 2 from countertop 3
&nbsp;&nbsp;&nbsp;&nbsp;> 从台面 3 拿取刀 2


&nbsp;&nbsp;&nbsp;&nbsp;Nothing happens.
&nbsp;&nbsp;&nbsp;&nbsp;毫无反应。


&nbsp;&nbsp;&nbsp;&nbsp;> go to countertop 2
&nbsp;&nbsp;&nbsp;&nbsp;> 前往台面 2


On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a
在台面 2 上，你看到了杯子 1、洗碗海绵 1、玻璃瓶 3、盘子 2、


&nbsp;&nbsp;&nbsp;&nbsp;potato 3, and a statue 1.
&nbsp;&nbsp;&nbsp;&nbsp;土豆 3，以及雕像 1。


&nbsp;&nbsp;&nbsp;&nbsp;$>$ take knife 1 from countertop 2
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 从台面 2 拿走小刀 1


&nbsp;&nbsp;&nbsp;&nbsp;Nothing happens.
&nbsp;&nbsp;&nbsp;&nbsp;什么也没发生。


&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to countertop 1
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往台面 1


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在台面 1 上，你看到了生菜 2、马克杯 2、胡椒瓶 1 和勺子 2。


> clean knife 1 with sinkbasin 1
> 用洗手池 1 清洗小刀 1


&nbsp;&nbsp;&nbsp;&nbsp;Nothing happens.
&nbsp;&nbsp;&nbsp;&nbsp;什么也没发生。


&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to countertop 3
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往台面 3


On the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard
在台面 3 上，你看到了面包 3、黄油刀 2、手机 1、信用卡


1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato
1、叉子 2、盆栽 1、小刀 2、刮铲 1、雕像 3、西红柿 3、西红柿


&nbsp;&nbsp;&nbsp;&nbsp;2 , a tomato 1 , and a vase 2 .
&nbsp;&nbsp;&nbsp;&nbsp;2、西红柿 1 和花瓶 2。


&nbsp;&nbsp;&nbsp;&nbsp;> take knife 2 from countertop 3
&nbsp;&nbsp;&nbsp;&nbsp;> 从台面 3 拿走小刀 2


&nbsp;&nbsp;&nbsp;&nbsp;Nothing happens.
&nbsp;&nbsp;&nbsp;&nbsp;什么也没发生。


$>$ go to countertop 2
$>$ 前往台面 2


On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a
在台面 2 上，你看到了杯子 1、洗碗海绵 1、玻璃瓶 3、盘子 2、


&nbsp;&nbsp;&nbsp;&nbsp;potato 3, and a statue 1.
&nbsp;&nbsp;&nbsp;&nbsp;土豆 3 和雕像 1。


&nbsp;&nbsp;&nbsp;&nbsp;> take knife 1 from countertop 2
&nbsp;&nbsp;&nbsp;&nbsp;> 从台面 2 拿取刀 1


&nbsp;&nbsp;&nbsp;&nbsp;Nothing happens.
&nbsp;&nbsp;&nbsp;&nbsp;无事发生。


&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to countertop 1
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往台面 1


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在台面 1 上，你看到了一颗生菜 2、一个马克杯 2、一个胡椒瓶 1 和一把勺子 2。


&nbsp;&nbsp;&nbsp;&nbsp;> clean knife 1 with sinkbasin 1
&nbsp;&nbsp;&nbsp;&nbsp;> 用水槽 1 清洗刀 1


Nothing happens.
无事发生。


---



#### D.2.3 REACT-IM TRAJECTORY
#### D.2.3 REACT-IM 轨迹


ReAct-IM is able to correctly find the knife, but is unable to clean it as well. The LLM then gets stuck trying to place this knife on a countertop. One possible issue is the incorrect thought think: I need to find a clean knife., which seems to trick the LLM in believing that the knife is already cleaned. In ReAct, this sequence of subgoals would be concretely captured by each thought.
ReAct-IM 能够正确找到刀，但同样无法清洗它。随后 LLM 陷入了试图将这把刀放在台面上的困境。一个可能的问题是错误的思考 think: 我需要找到一把干净的刀。，这似乎误导了 LLM，使其认为刀已经被清洗过了。在 ReAct 中，这一系列子目标会被每个思考具体地记录下来。


ReAct-IM ALFWorld trajectory
ReAct-IM ALFWorld 轨迹


---



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You are in the middle of a room. Looking quickly around you, you see a cabinet 6, a
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你在房间中央。快速环顾四周，你看到了橱柜 6、


cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a
橱柜 5、橱柜 4、橱柜 3、橱柜 2、橱柜 1、咖啡机 1、


countertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a
台面 3、台面 2、台面 1、抽屉 3、抽屉 2、抽屉 1、


fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin
冰箱 1、垃圾桶 1、微波炉 1、架子 3、架子 2、架子 1、水槽


1 , a stoveburner 4 , a stoveburner 3 , a stoveburner 2 , a stoveburner 1 , and a toaster
1、炉灶 4、炉灶 3、炉灶 2、炉灶 1 以及烤面包机 1。


1.



&nbsp;&nbsp;&nbsp;&nbsp;Your task is to: put a clean knife in countertop.
&nbsp;&nbsp;&nbsp;&nbsp;你的任务是：将一把干净的刀放在台面上。


> think: To solve the task, I need to find and take a clean knife, then put it in
> think: 为了完成任务，我需要找到并拿取一把干净的刀，然后将其放入


countertop.
台面。


OK.



> think: I need to find a clean knife.
> 思考：我需要找一把干净的刀。


OK.



$>$ go to countertop 1
$>$ 前往 1 号台面


On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
在 1 号台面上，你看到了生菜 2、马克杯 2、胡椒粉瓶 1 和勺子 2。


> think: I need to find a clean knife.
> 思考：我需要找一把干净的刀。


OK.



$>$ go to countertop 2
$>$ 前往 2 号台面


&nbsp;&nbsp;&nbsp;&nbsp;On the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a
&nbsp;&nbsp;&nbsp;&nbsp;在 2 号台面上，你看到了杯子 1、洗碗海绵 1、玻璃瓶 3、刀 1、


&nbsp;&nbsp;&nbsp;&nbsp;plate 2, a potato 3, and a statue 1.
&nbsp;&nbsp;&nbsp;&nbsp;盘子 2、土豆 3 和雕像 1。


&nbsp;&nbsp;&nbsp;&nbsp;> think: I need to find a clean knife.
&nbsp;&nbsp;&nbsp;&nbsp;> 思考：我需要找一把干净的刀。


OK.



> take knife 1 from countertop 2
> 从 2 号台面拿取刀 1


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You pick up the knife 1 from the countertop 2.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你从 2 号台面上拿起了刀 1。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;> think: I need to put this knife (1) in/on countertop 1.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;> 思考：我需要把这把刀 (1) 放入/放在 1 号台面。


OK.



&nbsp;&nbsp;&nbsp;&nbsp;$>$ go to countertop 1
&nbsp;&nbsp;&nbsp;&nbsp;$>$ 前往 1 号台面


On the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.
在 1 号台面上，你看到了生菜 2、马克杯 2、胡椒粉瓶 1 和勺子 2。


&nbsp;&nbsp;&nbsp;&nbsp;> think: I need to put this knife (1) in/on countertop 1.
&nbsp;&nbsp;&nbsp;&nbsp;> 思考：我需要把这把刀 (1) 放入/放在 1 号台面。


OK.



&nbsp;&nbsp;&nbsp;&nbsp;> put knife 1 in/on countertop 1
&nbsp;&nbsp;&nbsp;&nbsp;> 将刀 1 放入/放在 1 号台面


&nbsp;&nbsp;&nbsp;&nbsp;You put the knife 1 in/on the countertop 1.
&nbsp;&nbsp;&nbsp;&nbsp;你把小刀 1 放进/放在了台面 1 上。


> think: I need to put this knife (1) in/on countertop 1.
> think: 我需要把这把刀 (1) 放到 countertop 1 上。


OK.



$>$ put knife 1 in/on countertop 1
$>$ 将刀具 1 放入/放在台面 1 上


Nothing happens.
什么也没发生。


> think: I need to put this knife (1) in/on countertop 1.
> think: 我需要把这把小刀 (1) 放在 countertop 1 上。


OK.



$>$ put knife 1 in/on countertop 1
$>$ 将刀 1 放在台面 1 上


Nothing happens.
什么也没有发生。


> think: I need to put this knife (1) in/on countertop 1.
> think: 我需要把这把刀 (1) 放到 countertop 1 上。


OK.



> put knife 1 in/on countertop 1
> 将刀子 1 放在台面 1 上


Nothing happens.
什么也没有发生。


> think: I need to put this knife (1) in/on countertop 1.
> think: 我需要把这把小刀 (1) 放在台面 1 上。


OK.



> put knife 1 in/on countertop 1
> 将 knife 1 放入/放在 countertop 1 上


Nothing happens.
什么都没有发生。


&nbsp;&nbsp;&nbsp;&nbsp;> think: I need to put this knife (1) in/on countertop 1.
&nbsp;&nbsp;&nbsp;&nbsp;> think: 我需要把这把小刀 (1) 放到厨台 1 上。


&nbsp;&nbsp;&nbsp;&nbsp;OK.



&nbsp;&nbsp;&nbsp;&nbsp;> put knife 1 in/on countertop 1
&nbsp;&nbsp;&nbsp;&nbsp;> 将 菜刀 1 放在 台面 1 上


&nbsp;&nbsp;&nbsp;&nbsp;Nothing happens.
&nbsp;&nbsp;&nbsp;&nbsp;什么都没有发生。


&nbsp;&nbsp;&nbsp;&nbsp;> think: I need to put this knife (1) in/on countertop 1.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;> 思考：我需要把这把刀 (1) 放入/放在柜台 1 上。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OK.



---



Table 10: Example trajectories for Webshop predicted by Act and ReAct. Compared to Act, ReAct uses reasoning to find products that satisfy all target attributes.
表 10：Act 和 ReAct 预测的 Webshop 示例轨迹。与 Act 相比，ReAct 使用推理来寻找满足所有目标属性的产品。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_16_23_9a65f3.jpg"/>



## E MORE ANALYSIS
## E 更多分析


### E.1 SUCCESS AND FAILURE MODES ANALYSIS
### E.1 成功与失败模式分析


We provide some examples corresponding to the success and error mode analysis given in Sec. 3.3 Search results and non-representative steps are omitted to reduce space.
我们提供了与 3.3 节中成功和错误模式分析相对应的示例。为节省空间，省略了搜索结果和非代表性步骤。


<table><tr><td></td><td>Success: True positive</td></tr><tr><td colspan="2">ReAct</td></tr><tr><td>Question:</td><td>Author David Chanoff has collaborated with a U.S. Navy admiral who served as the ambassador to the United Kingdom under which President?</td></tr><tr><td>Thought 1:</td><td>I need to search David Chanoff and find the U.S. Navy admiral he collaborated with.</td></tr><tr><td>Action 1:</td><td>Search [David Chanoff]</td></tr><tr><td>Observation 1: <br> Action 2:</td><td>David Chanoff is a noted author... <br> Search [William J. Crowe]</td></tr><tr><td>Observation 2:</td><td>William J. Crowe is...</td></tr><tr><td>Thought 3:</td><td>William J. Crowe served as the ambassador to the United Kingdom under <br> President Bill Clinton. So the answer is Bill Clinton.</td></tr><tr><td>Action 3:</td><td>Finish[Bill Clinton]</td></tr><tr><td>Observation 3:</td><td>Episode finished</td></tr><tr><td colspan="2">CoT</td></tr><tr><td>Question:</td><td>What was the name of the team that represented an Arizona university in the conference that covers a broad expanse of the western United States, including Arizona, California, and New Mexico?</td></tr><tr><td>Thought:</td><td>Let's think step by step. The name of the team that represented an Arizona university in the conference that covers a broad expanse of the western United States, including Arizona, California, and New Mexico is Arizona State Sun Devils.</td></tr><tr><td>Answer:</td><td>Arizona State Sun Devils</td></tr></table>
<table><tbody><tr><td></td><td>成功：真阳性</td></tr><tr><td colspan="2">ReAct</td></tr><tr><td>问题：</td><td>作者 David Chanoff 曾与一位担任过驻英国大使的美国海军上将合作，请问该大使是在哪位总统任期内任职的？</td></tr><tr><td>思考 1：</td><td>我需要搜索 David Chanoff，找到与他合作过的美国海军上将。</td></tr><tr><td>操作 1：</td><td>搜索 [David Chanoff]</td></tr><tr><td>观察 1：<br/> 操作 2：</td><td>David Chanoff 是一位著名的作者……<br/> 搜索 [William J. Crowe]</td></tr><tr><td>观察 2：</td><td>William J. Crowe 是……</td></tr><tr><td>思考 3：</td><td>William J. Crowe 在比尔·克林顿总统任期内担任驻英国大使。所以答案是比尔·克林顿。</td></tr><tr><td>操作 3：</td><td>结束 [比尔·克林顿]</td></tr><tr><td>观察 3：</td><td>流程结束</td></tr><tr><td colspan="2">CoT</td></tr><tr><td>问题：</td><td>哪支队伍代表亚利桑那州的大学参加了涵盖美国西部广大地区（包括亚利桑那州、加利福尼亚州和新墨西哥州）的联盟赛事？</td></tr><tr><td>思考：</td><td>让我们一步步思考。代表亚利桑那州的大学参加涵盖美国西部广大地区（包括亚利桑那州、加利福尼亚州和新墨西哥州）联盟赛事的队伍名称是亚利桑那州立大学太阳魔队。</td></tr><tr><td>回答：</td><td>亚利桑那州立大学太阳魔队</td></tr></tbody></table>


<table><tr><td></td><td>Success: False positive</td></tr><tr><td>ReAct <br> Question: <br> Thought 1: <br> Action 1: <br> Observation 1: <br> Thought 2: <br> Action 2: <br> Observation 2: <br> Thought 3: <br> Action 3: <br> Observation 3:</td><td>Did the board game San Marco or About Time come out first? <br> I need to search San Marco and About Time, and find which came out first. Search[San Marco] <br> San Marco is one of the ... The was built in venice. <br> San Marco is a sestiere of Venice. I need to search About Time next. Search [About Time] <br> Could not find [About Time]. Similar: [...]. <br> About Time is a 2013 film. So San Marco came out first. <br> Finish[San Marco] <br> Episode finished</td></tr><tr><td>CoT <br> Question: Thought: <br> Answer:</td><td>Was Vanderbilt University or Emory University founded first? <br> Let's think step by step. Vanderbilt University was founded in 1873. Emory University was founded in 1836. 1873 (Vanderbilt University) > 1836 (Emory University), so Vanderbilt University was founded first. <br> Emory University</td></tr></table>
<table><tbody><tr><td></td><td>成功：误报</td></tr><tr><td>ReAct <br/> 问题： <br/> 思考 1： <br/> 行动 1： <br/> 观察 1： <br/> 思考 2： <br/> 行动 2： <br/> 观察 2： <br/> 思考 3： <br/> 行动 3： <br/> 观察 3：</td><td>桌游《圣马可》(San Marco) 和《时空恋旅人》(About Time) 哪个先推出？ <br/> 我需要搜索《圣马可》和《时空恋旅人》，找出哪个先推出。搜索 [圣马可] <br/> 《圣马可》是……之一。它建于威尼斯。 <br/> 圣马可是威尼斯的一个行政区。接下来我需要搜索《时空恋旅人》。搜索 [About Time] <br/> 未能找到 [About Time]。相似条目：[...]。 <br/> 《时空恋旅人》是一部 2013 年的电影。所以《圣马可》先推出。 <br/> 完成 [圣马可] <br/> 回合结束</td></tr><tr><td>CoT <br/> 问题：思考： <br/> 回答：</td><td>范德堡大学和埃默里大学哪所大学先创立？ <br/> 让我们一步步思考。范德堡大学成立于 1873 年。埃默里大学成立于 1836 年。1873 年（范德堡大学） > 1836 年（埃默里大学），所以范德堡大学成立更晚。 <br/> 埃默里大学</td></tr></tbody></table>


<table><tr><td></td><td>Failure: Reasoning error</td></tr><tr><td>ReAct</td><td></td></tr><tr><td>Question:</td><td>What was the middle name of the actress who was cast alongside Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laur Bell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur in Hairspray?</td></tr><tr><td>Thought 1:</td><td>I need to search Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur, find the actress that was cast alongside them in Hairspray, then find its middle name.</td></tr><tr><td>Action 1:</td><td>Search [Harvey Fierstein]</td></tr><tr><td>Observation 1:</td><td>Harvey Forbes Fierstein ...</td></tr><tr><td>Thought 2:</td><td>Harvey Fierstein is an actor, playwright, and screenwriter. I need to search Linda Hart next...</td></tr><tr><td>Action 2:</td><td>Search[Linda Hart]...</td></tr><tr><td>CoT</td><td></td></tr><tr><td>Question: Thought:</td><td>Who is older Danny Green or James Worthy? <br> Let's think step by step. Danny Green was born in 1987. James Worthy was born in 1961. 1987 (Danny Green) > 1961 (James Worthy), so Danny Green is older.</td></tr><tr><td>Answer:</td><td>Danny Green</td></tr></table>
<table><tbody><tr><td></td><td>失败：推理错误</td></tr><tr><td>ReAct</td><td></td></tr><tr><td>问题：</td><td>在《发胶星梦》中，与哈维·费斯特恩、琳达·哈特、迪克·拉泰萨、克拉克·索雷尔、玛丽·邦德·戴维斯、劳拉·贝尔·邦迪、马修·莫里森、科里·雷诺兹和玛丽莎·贾特·威诺克共同出演的女演员中间名叫什么？</td></tr><tr><td>思考 1：</td><td>我需要搜索哈维·费斯特恩、琳达·哈特、迪克·拉泰萨、克拉克·索雷尔、玛丽·邦德·戴维斯、劳拉·贝尔·邦迪、马修·莫里森、科里·雷诺兹和玛丽莎·贾特·威诺克，找到在《发胶星梦》中与他们共同出演的女演员，然后找到她的中间名。</td></tr><tr><td>动作 1：</td><td>搜索 [Harvey Fierstein]</td></tr><tr><td>观察 1：</td><td>哈维·福布斯·费斯特恩 ...</td></tr><tr><td>思考 2：</td><td>哈维·费斯特恩是一位演员、剧作家和编剧。接下来我需要搜索琳达·哈特……</td></tr><tr><td>动作 2：</td><td>搜索 [Linda Hart]……</td></tr><tr><td>思维链 (CoT)</td><td></td></tr><tr><td>问题：思考：</td><td>丹尼·格林和詹姆斯·沃西谁的年龄更大？<br/> 让我们逐步思考。丹尼·格林出生于 1987 年。詹姆斯·沃西出生于 1961 年。1987 年 (丹尼·格林) > 1961 年 (詹姆斯·沃西)，所以丹尼·格林更年轻（原文逻辑有误，此处按原文意译：丹尼·格林更大）。</td></tr><tr><td>回答：</td><td>丹尼·格林</td></tr></tbody></table>


<table><tr><td></td><td>Failure: Search error</td></tr><tr><td>ReAct</td><td></td></tr><tr><td>Question:</td><td>What does the goddess associated with the goddess frigg consists of what tales?</td></tr><tr><td>Thought 1: <br> Action 1:</td><td>I need to search goddess frigg and find its associated goddess. <br> Search [goddess frigg]</td></tr><tr><td>Observation 1:</td><td>Could not find goddess frigg. Similar: ['Frigg', 'Frigg and Freyja common <br> origin hypothesis', 'Freyja (section Relation to Frigg and other goddesses <br> and figures)', 'Eir (redirect from Eir (goddess))', 'Baldr'].</td></tr><tr><td>Thought 2:</td><td>I need to search Frigg instead...</td></tr></table>
<table><tbody><tr><td></td><td>失败：搜索错误</td></tr><tr><td>ReAct</td><td></td></tr><tr><td>问题：</td><td>与女神弗丽嘉相关的女神由哪些故事组成？</td></tr><tr><td>思考 1：<br/> 行动 1：</td><td>我需要搜索女神弗丽嘉并找到与其相关的女神。<br/> 搜索 [女神弗丽嘉]</td></tr><tr><td>观察 1：</td><td>未找到女神弗丽嘉。相似项：['弗丽嘉', '弗丽嘉与芙蕾雅共同起源假说', '芙蕾雅（章节：与弗丽嘉及其他女神和人物的关系）', '埃尔（重定向自埃尔（女神））', '巴德尔']。</td></tr><tr><td>思考 2：</td><td>我需要改为搜索“弗丽嘉”...</td></tr></tbody></table>


<table><tr><td colspan="2">Failure: Hallucination</td></tr><tr><td>CoT <br> Question: <br> Thought: <br> Answer:</td><td>What was the first year a scientific journal published by an organization located in the Bronx was published? <br> Let's think step by step. The first year a scientific journal published by an organization located in the Bronx was published was 1916. <br> 1916 (Label: 1909)</td></tr></table>
<table><tbody><tr><td colspan="2">失败：幻觉</td></tr><tr><td>CoT <br/> 问题：<br/> 思考：<br/> 回答：</td><td>由位于布朗克斯的机构出版的科学期刊最早是哪一年出版的？<br/> 让我们循序渐进地思考。由位于布朗克斯的机构出版的第一本科学期刊出版于1916年。<br/> 1916 (标签：1909)</td></tr></tbody></table>


<table><tr><td></td><td>Failure: Label ambiguity</td></tr><tr><td colspan="2">ReAct</td></tr><tr><td>Question:</td><td>What nationality is the man who developed the dictator game?</td></tr><tr><td>Thought 1:</td><td>I need to search dictator game, find the man who developed it, then find his nationality.</td></tr><tr><td>Action 1:</td><td>Search [dictator qame]</td></tr><tr><td>Observation 1:</td><td>The dictator game is a popular experimental instrument in social psychology and economics...</td></tr><tr><td>Action 3:</td><td>Search [Amos Tversky]</td></tr><tr><td>Observation 3:</td><td>Amos Nathan Tversky...</td></tr><tr><td>Thought 4:</td><td>Amos Tversky is an Israeli cognitive and mathematical psychologist. So the man who developed the dictator game is Israeli.</td></tr><tr><td>Action 4:</td><td>Finish[Israeli] (label: Israel-American)</td></tr><tr><td colspan="2">CoT</td></tr><tr><td>Question:</td><td>Kurt Fuller from Stockton, CA appeared in which American Detective Comedy Drama television series created by Steve Franks?</td></tr><tr><td>Thought:</td><td>Let's think step by step. Kurt Fuller from Stockton, CA appeared in Psych. <br> Psych is an American Detective Comedy Drama television series created by</td></tr><tr><td></td><td>Steve Franks.</td></tr><tr><td>Answer:</td><td>Psych (label: Psych is an American detective comedy-drama)</td></tr></table>
<table><tbody><tr><td></td><td>失败：标签歧义</td></tr><tr><td colspan="2">ReAct</td></tr><tr><td>问题：</td><td>开发独裁者游戏的人是什么国籍？</td></tr><tr><td>思考 1：</td><td>我需要搜索独裁者游戏，找到其开发者，然后查询他的国籍。</td></tr><tr><td>操作 1：</td><td>搜索 [独裁者游戏]</td></tr><tr><td>观察 1：</td><td>独裁者游戏是社会心理学和经济学中常用的实验工具……</td></tr><tr><td>操作 3：</td><td>搜索 [阿莫斯·特沃斯基]</td></tr><tr><td>观察 3：</td><td>阿莫斯·内森·特沃斯基……</td></tr><tr><td>思考 4：</td><td>阿莫斯·特沃斯基是一位以色列认知心理学家和数学心理学家。因此，开发独裁者游戏的人是以色列人。</td></tr><tr><td>操作 4：</td><td>结束[以色列人]（标签：以色列裔美国人）</td></tr><tr><td colspan="2">CoT</td></tr><tr><td>问题：</td><td>来自加州斯托克顿的库尔特·富勒出演了哪部由史蒂夫·弗兰克斯创作的美国侦探喜剧剧集？</td></tr><tr><td>思考：</td><td>让我们一步步思考。来自加州斯托克顿的库尔特·富勒出演了《灵异妙探》。<br/>《灵异妙探》是一部美国侦探喜剧剧集，由以下作者创作：</td></tr><tr><td></td><td>史蒂夫·弗兰克斯。</td></tr><tr><td>回答：</td><td>《灵异妙探》（标签：灵异妙探是一部美国侦探喜剧）</td></tr></tbody></table>