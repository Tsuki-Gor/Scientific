# OSWORLD: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments
# OSWORLD：用于在真实计算机环境中对开放任务进行多模态代理基准测试


Tianbao Xie ${}^{h}$ Danyang Zhang ${}^{h}$ Jixuan Chen ${}^{h}$ Xiaochuan Li ${}^{h}$ Siheng Zhao ${}^{h}$ Ruisheng Cao ${}^{h}$ Toh Jing Hua ${}^{h}$ Zhoujun Cheng ${}^{h}$ Dongchan Shin ${}^{h}$ Fangyu Lei ${}^{h}$ Yitao Liu ${}^{h}$ Yiheng Xu ${}^{h}$ Shuyan Zhou ${}^{c}$ Silvio Savarese ${}^{s}$ Caiming Xiong ${}^{s}$ Victor Zhong ${}^{w}$ Tao Yu ${}^{h}$
Tianbao Xie ${}^{h}$ Danyang Zhang ${}^{h}$ Jixuan Chen ${}^{h}$ Xiaochuan Li ${}^{h}$ Siheng Zhao ${}^{h}$ Ruisheng Cao ${}^{h}$ Toh Jing Hua ${}^{h}$ Zhoujun Cheng ${}^{h}$ Dongchan Shin ${}^{h}$ Fangyu Lei ${}^{h}$ Yitao Liu ${}^{h}$ Yiheng Xu ${}^{h}$ Shuyan Zhou ${}^{c}$ Silvio Savarese ${}^{s}$ Caiming Xiong ${}^{s}$ Victor Zhong ${}^{w}$ Tao Yu ${}^{h}$


${}^{h}$ The University of Hong Kong ${}^{c}$ Carnegie Mellon University
${}^{h}$ 香港大学 ${}^{c}$ 卡内基梅隆大学


${}^{s}$ Salesforce Research ${}^{w}$ University of Waterloo
${}^{s}$ Salesforce Research ${}^{w}$ 滑铁卢大学


## Abstract
## 摘要


Autonomous agents that accomplish complex computer tasks with minimal human interventions can significantly enhance accessibility and productivity of human-computer interactions. Existing benchmarks either lack interactive environments or are limited to specific applications/domains, failing to reflect the diversity and complexity of real-world computer use and limiting agent scalability. We introduce OSWORLD, the first-of-its-kind scalable real computer environment for multimodal agents, supporting task setup, interactive learning, and execution-based evaluation of open-ended computer tasks across arbitrary applications in Ubuntu, Windows, and macOS. Using OSWORLD, we create a benchmark of 369 tasks involving real web and desktop apps in open domains, OS file I/O, and multi-app workflows. Each example derives from real-world use cases and includes detailed setup and execution-based evaluation for reproducibility. Extensive evaluation of state-of-the-art LLM/VLM agents on OSWORLD reveals deficiencies in their ability to serve as computer assistants. While humans accomplish 72.4% of the tasks, the best agents achieve <12.2%, struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWORLD provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Implementation and experiments are athttps://os-world.github.io
能够在最少人工干预下完成复杂计算机任务的自主代理，可以显著提升人机交互的可访问性和生产力。现有基准要么缺乏交互环境，要么仅限于特定应用/领域，无法真实反映现实世界计算使用的多样性与复杂性，从而限制代理的可扩展性。我们提出 OSWORLD——首个可扩展的真实计算机多模态代理环境，支持任务设置、交互式学习和基于执行的开放式计算任务评估，涵盖在 Ubuntu、Windows 和 macOS 上的任意应用。利用 OSWORLD，我们创建了一个包含 369 项真实世界任务的基准，涉及开放域的真实网页和桌面应用、操作系统文件输入输出以及多应用工作流。每个示例均源自真实使用场景，包含可重复的详细设置和基于执行的评估。对最先进的大语言模型/大视觉模型代理在 OSWORLD 上的广泛评估揭示了它们作为计算机助手的能力不足。人类完成 72.4% 的任务，而最佳代理不足 12.2%，在 GUI 定位和操作知识方面存在困难。通过 OSWORLD 的综合分析为开发多模态通用代理提供了宝贵见解，这在以往基准中是无法实现的。实现与实验请访问 https://os-world.github.io


## 1 Introduction
## 1 介绍


Humans interact with computers to perform essential tasks in the digital realm, including web browsing, video editing, file management, data analysis, and software development. These task workflows often involve multiple applications through graphical user interfaces (GUI) and command line interfaces (CLI). Autonomous agents powered by large vision-language models (VLMs) can revolutionize how we interact with computer environments [32, 48, 1]. By following natural language instructions, these agents can make computers more accessible and vastly increase human productivity.
人类与计算机交互，以在数字领域执行基本任务，如网页浏览、视频编辑、文件管理、数据分析和软件开发。这些任务工作流通常通过图形用户界面（GUI）和命令行界面（CLI）涉及多种应用。由大型视觉-语言模型（VLM）驱动的自主代理可以彻底改变我们与计算机环境的交互方式[32, 48, 1]。通过遵循自然语言指令，这些代理可以使计算机更加易于访问，并大幅提高人类的生产力。


A major challenge in developing multimodal agents is the absence of a benchmark that covers interactive, diverse, and complex real-world computer use across operating systems, interfaces, and applications. Prior benchmarks that provide demonstration datasets without executable environments [12, 44, 25] assume a single solution for each task and limit potential research in interactive learning and real-world exploration. Prior work with executable environments simplify the observation and action spaces of agents and limit task scopes to specific applications/domains such as navigation of specific websites [48, 34, 63, 71], coding [62] and the combination [36, 59, 38]. These restricted environments do not fully reflect real-world computer use, as they do not evaluate scenarios that require navigating between applications and interfaces in open domains (Fig. 1).
开发多模态代理的一个主要挑战是缺乏覆盖跨操作系统、界面和应用的交互性、多样性和复杂性的真实世界计算使用的基准。以往的基准提供演示数据集而缺乏可执行环境[12, 44, 25]，假设每个任务只有一种解决方案，限制了交互学习和现实世界探索的潜在研究。以往具有可执行环境的工作简化了代理的观测和动作空间，并将任务范围限制在特定应用/领域，如特定网站导航[48, 34, 63, 71]、编码[62]及组合[36, 59, 38]。这些受限的环境未能完全反映现实世界的计算使用，因为它们并不评估在开放域中需要在应用与界面之间切换的场景（图1）。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_5e556f.jpg"/>



Figure 1: OSWORLD is a first-of-its-kind scalable real computer env for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across systems. It is a unified environment for evaluating open-ended computer tasks that involve arbitrary apps. Using OSWORLD, we create a benchmark of 369 real-world tasks with reproducible setup and evaluation scripts.
图1：OSWORLD 是首个可扩展的真实计算机环境，用于多模态代理，支持跨系统的任务设置、基于执行的评估以及交互式学习。它是一个评估涉及任意应用的开放式计算任务的统一环境。利用 OSWORLD，我们创建了一个包含 369 个真实世界任务的基准，具有可重复的设置和评估脚本。


To address this gap, we introduce OSWORLD, the first-of-its-kind scalable, real computer environment for developing multimodal agents capable of executing a wide range of real computer tasks beyond isolated interfaces and applications. This executable environment allows free-form raw keyboard and mouse control of real computer applications and supports initial task state configuration, execution-based evaluation, and interactive learning across mainstream operating systems (Ubuntu, Windows, macOS). OSWORLD enables evaluation of open-ended computer tasks that involve arbitrary applications, ranging from image viewing to software functionality integration and programming (Fig. 1). OSWORLD serves as a unified, real computer environment that allows users to define their agent tasks without the need to build application/domain-specific simulated environments.
为填补这一空白，我们推出 OSWORLD——首个可扩展的现实计算机环境，用于开发能够执行超出单一界面与应用的广泛真实计算任务的多模态代理。该可执行环境允许对真实计算机应用的自由形式原始键鼠控制，并支持初始任务状态配置、基于执行的评估，以及跨主流操作系统（Ubuntu、Windows、macOS）的交互学习。OSWORLD 使对涉及任意应用的开放式计算任务的评估成为可能，从图像查看到软件功能集成与编程（图1）。OSWORLD 作为一个统一的真实计算机环境，允许用户在不需要构建特定应用/领域的仿真环境的情况下定义代理任务。


Building upon OSWORLD, we create a benchmark with 369 real-world tasks that involve widely-used web and desktop apps in open domains, OS file I/O, and multi-app workflows through both GUI and CLI. Each example is based on real-world use cases and often requires interactions with multiple applications and interfaces. To ensure reliable, reproducible evaluation, 9 authors with computer science backgrounds carefully annotate each example with an initial state setup configuration to simulate in-progress human work and a custom execution-based evaluation script to verify task completion. Our benchmark has 134 unique evaluation functions - significantly larger than prior work [71] — showcasing the complexity, diversity, and evaluation challenges of tasks in our benchmark. The human performance study indicates that task examples from OSWORLD are more time-consuming and challenging compared to those in prior work.
在 OSWORLD 的基础上，我们创建了一个包含 369 个真实世界任务的基准，涉及在开放域中的广泛使用的网页和桌面应用、操作系统文件 I/O，以及通过 GUI 和 CLI 的多应用工作流。每个示例都基于真实世界场景，Often 需要与多个应用和接口交互。为确保可可靠、可重复的评估，来自计算机科学背景的 9 位作者对每个示例进行了初始状态设置配置的细致标注，以模拟进行中的人工工作，并提供一个基于自定义执行的评估脚本来验证任务完成情况。我们的基准具有 134 个独特的评估函数——显著大于之前的工作 [71] — 展现了我们基准中任务的复杂性、多样性及评估挑战。人类性能研究表明，OSWORLD 的任务示例比先前工作中的任务更耗时且更具挑战性。


We evaluate state-of-the-art LLM and VLM agent baselines, including GPT-4V [43], Gemini [54, 45], Claude-3 Opus [3] and Qwen-Max [5], as well as Mixtral [23], Llama-3 [39] and CogAgent [21] from the open-source community. The performance of these experiments ranges from 0.99% to 12.24%,with subsets of applications reaching $0\%$ ,for workflow tasks that involve cooperation from multiple apps, the highest performance of the baseline agent is 6.57%. This indicates that current LLMs and VLMs are far from capable of serving as computer assistants (§4.2). Results also show that while additional knowledge such as the accessibility tree and Set-of-Mark (§4.1) can be helpful, it can also lead to potential misguidance and varies across models. Finally, we find that VLM-based agents struggle to ground screenshots to predict precise coordinates for actions, tend to predict repetitive actions, are unable to handle noise from unexpected application windows and exhibit limited knowledge of basic GUI interactions and domain-specific features of apps (\$5, \$D.4).
我们评估了最先进的 LLM 和 VLM 代理基线，包括 GPT-4V [43]、Gemini [54, 45]、Claude-3 Opus [3] 和 Qwen-Max [5]，以及 Mixtral [23]、Llama-3 [39] 和 CogAgent [21] 等开源社区工具。上述实验的表现范围为 0.99% 至 12.24%，其中部分应用子集达到 $0\%$，对于需要多应用协作的工作流任务，基线代理的最高表现为 6.57%。这表明当前的 LLMs 与 VLMs 还远不能胜任作为计算机助手 (§4.2) 的角色。结果还显示，尽管可用的额外知识如可访问性树和 Set-of-Mark (§4.1) 可能有帮助，但也可能带来潜在的误导并因模型而异。最后，我们发现基于 VLM 的代理难以将截图定位到动作的精确坐标，倾向于预测重复动作，无法处理来自意外应用窗口的噪声，并且对 GUI 的基本交互和应用的领域特定特征知识有限 (\$5、\$D.4)。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_63f6f6.jpg"/>



Figure 2: Overview of OSWORLD infrastructure. The environment uses a config file for initializing tasks (red), agent interaction, post-processing upon agent completion (orange), retrieving files and information (yellow), and executing the evaluation function (highlighted in green). Environments can run in parallel on a single host machine for learning or evaluation. Headless operation is supported.
图 2：OSWORLD 基础设施概览。环境使用一个配置文件来初始化任务（红色）、代理交互、代理完成后的后处理（橙色）、检索文件和信息（黄色），以及执行评估函数（绿色高亮）。环境可以在单一主机上并行运行，用于学习或评估。支持无头模式。


## 2 OSWORLD Environment
## 2 OSWORLD 环境


In this section, we will introduce the task definition of autonomous agents, the components and implementation of the OSWORLD environment, and the supported observation and action spaces.
在本节中，我们将介绍自主代理的任务定义、OSWORLD 环境的组成和实现，以及支持的观测与行动空间。


### 2.1 Task Definition
### 2.1 任务定义


An autonomous digital agent task can be formalized as a Goal-Augmented Partially Observable Markov Decision Process (GA-POMDP) $\left( {\mathcal{S},\mathcal{O},\mathcal{A},\mathcal{T},\Omega ,r,\gamma ,{\rho }_{0},\mathcal{G},{p}_{g},\phi }\right)$ where $\mathcal{S}$ is the full state space (including hidden system states), $\mathcal{O}$ is the observation space (§2.3,what’s visible or accessible to the agent), $\mathcal{A}$ is the action space (§2.4), $\mathcal{T} : \mathcal{S} \times  \mathcal{A} \rightarrow  \mathcal{S}$ is the transition function, $\Omega$ is the observation function, $r : \mathcal{S} \times  \mathcal{A} \times  \mathcal{G} \rightarrow  \mathbb{R}$ is the reward function, $\gamma$ is the discount factor, ${\rho }_{0}$ is the initial state distribution, $\mathcal{G}$ is the space of goals (instructions in our case), ${p}_{g}$ is the distribution of desired goals (instructions),and $\phi  : \mathcal{O} \rightarrow  \mathcal{G}$ is a mapping function from observations to goals. Given current observation ${o}_{t} \in  \mathcal{O}$ (a natural language instruction observation and a screenshot,accessibility a11y tree, or their combination according to facilities available), an agent generates executable action ${a}_{t} \in  \mathcal{A}$ (e.g.,clicking on the certain pixel of the screen — .click (300,540,button=‘right’), press key combination - .hotkey ('ctrl', 'alt', 't')),which results in a new state ${s}_{t + 1} \in  \mathcal{S}$ (e.g.,current computer state) and a new partial observation ${o}_{t + 1} \in  \mathcal{O}$ . The interaction loop repeats until an action that marks termination (DONE or FAIL, see Sec. 2.4) is generated or the agent reaches the max number of steps (e.g., 15 in our experiments). OSWORLD implements an execution-based reward function $r : \mathcal{S} \times  \mathcal{A} \times  \mathcal{G} \rightarrow  \left\lbrack  {0,1}\right\rbrack$ . The reward function awards a value of 1 or a positive decimal under 1 at the final step if the state transitions meet the expectations of the task objective (i.e., the goal is successfully achieved or partially achieved), or if the agent accurately predicts failure for an infeasible task. In all other scenarios, it returns 0.
自主数字智能体任务可以形式化为目标增强的部分可观测马尔可夫决策过程（GA - POMDP）$\left( {\mathcal{S},\mathcal{O},\mathcal{A},\mathcal{T},\Omega ,r,\gamma ,{\rho }_{0},\mathcal{G},{p}_{g},\phi }\right)$，其中$\mathcal{S}$是完整状态空间（包括隐藏的系统状态），$\mathcal{O}$是观测空间（§2.3，智能体可见或可访问的内容），$\mathcal{A}$是动作空间（§2.4），$\mathcal{T} : \mathcal{S} \times  \mathcal{A} \rightarrow  \mathcal{S}$是转移函数，$\Omega$是观测函数，$r : \mathcal{S} \times  \mathcal{A} \times  \mathcal{G} \rightarrow  \mathbb{R}$是奖励函数，$\gamma$是折扣因子，${\rho }_{0}$是初始状态分布，$\mathcal{G}$是目标空间（在我们的案例中即指令），${p}_{g}$是期望目标（指令）的分布，$\phi  : \mathcal{O} \rightarrow  \mathcal{G}$是从观测到目标的映射函数。给定当前观测${o}_{t} \in  \mathcal{O}$（根据可用工具，可能是自然语言指令观测、屏幕截图、无障碍访问（a11y）树或它们的组合），智能体生成可执行动作${a}_{t} \in  \mathcal{A}$（例如，点击屏幕上的特定像素 — .click (300,540,button='right')，按下组合键 - .hotkey ('ctrl', 'alt', 't')），这会产生一个新状态${s}_{t + 1} \in  \mathcal{S}$（例如，当前计算机状态）和一个新的部分观测${o}_{t + 1} \in  \mathcal{O}$。交互循环会一直重复，直到生成标记为终止的动作（DONE 或 FAIL，见第 2.4 节），或者智能体达到最大步数（例如，在我们的实验中为 15 步）。OSWORLD 实现了基于执行的奖励函数$r : \mathcal{S} \times  \mathcal{A} \times  \mathcal{G} \rightarrow  \left\lbrack  {0,1}\right\rbrack$。如果状态转移符合任务目标的预期（即目标成功实现或部分实现），或者智能体准确预测了不可行任务的失败，奖励函数会在最后一步给出 1 或小于 1 的正小数的值。在所有其他情况下，返回 0。


### 2.2 Real Computer Environment Infrastructure
### 2.2 真实计算机环境基础设施


OSWORLD is an executable and controllable environment that supports task initialization, execution-based evaluation, and interactive learning in real operating systems (e.g., Ubuntu, Windows, macOS) using virtual machines (VMs), shown in Fig. 2, VMs offer safe, isolated, and resettable/reversible (via snapshotting) environments that prevent irreversible damage to the real host machine.
OSWORLD 是一个可执行且可控制的环境，它使用虚拟机（VM）在真实操作系统（如 Ubuntu、Windows、macOS）中支持任务初始化、基于执行的评估和交互式学习，如图 2 所示。虚拟机提供了安全、隔离且可重置/可逆（通过快照）的环境，可防止对真实主机造成不可逆的损坏。


Initialization Prior to agent interactions, we initializing the VM environment via config file. This includes downloading files, opening software, and adjusting interface layout. Many real-world assistance scenarios occur not at the beginning of digital activities, such as right after launching an application or starting the computer, but rather at intermediate stages, such as when certain software is already open or the computer has experienced a crash. Therefore, we reproduce these intermediate states during the initialization to replicate real-world scenarios. See B.5 for more details.
初始化 在智能体交互之前，我们通过配置文件初始化虚拟机环境。这包括下载文件、打开软件和调整界面布局。许多现实世界的辅助场景并非发生在数字活动的开始阶段，例如应用程序启动或计算机开机后不久，而是发生在中间阶段，例如某些软件已经打开或计算机崩溃之后。因此，我们在初始化过程中重现这些中间状态，以模拟现实世界的场景。更多详细信息见 B.5。


Table 1: Examples of annotated evaluation scripts, which involve retrieving data from configuration files, environment, and cloud, and executing functions to obtain results and assess correctness.
表 1：带注释的评估脚本示例，这些脚本涉及从配置文件、环境和云端检索数据，并执行函数以获取结果并评估正确性。


<table><tr><td>Initial State</td><td>Task Instruction</td><td>Evaluation Script (Simplified)</td></tr><tr><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_92a829.jpg"/></td><td>Can you help me clean up my computer by getting rid of all the cookies that Amazon might have saved?</td><td>cookie_data = get_cookie_data(env) rule = \{"type":"domains", "domains":[".amazon.com"]\} is_cookie_deleted(cookie_data, rule)</td></tr><tr><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_01d664.jpg"/></td><td>Rename "Sheet 1" to "LARS Resources". Then make a copy of it. Place the copy before "Sheet 2" and rename it by appending a suffix “(Backup)”, ...</td><td>result = get_file(env) <br> expected = get_file(cloud) <br> rules = [\{"type":"sheet_name"\}, <br> \{"type":"sheet_data", <br> "sheet_idx0":0, <br> "sheet_idx1":1\}...] <br> compare_table(result, expected, rules)</td></tr><tr><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_da8c42.jpg"/></td><td>I've drafted an e-mail reminder for those who haven't paid tuition. Please help me to check out their e-mails from the payment record and add to the receiver field.</td><td>tree = get_ally_tree(env) <br> rules = [\{"selectors": <br> ["tool-bar[attr|id=MsgHeadersToolbar] label [name=To] <br> [attr|class=\\"address-pill\\"]> <br> label[attr|class=\\"pill-label\\"] <br> [name*=\\"fox@someuniversity.edu...] check_ally_tree(tree, rules)</td></tr></table>
<table><tbody><tr><td>初始状态</td><td>任务指令</td><td>评估脚本（简化版）</td></tr><tr><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_92a829.jpg"/></td><td>你能帮我清理电脑吗，删除 Amazon 可能保存的所有 cookie？</td><td>cookie_data = get_cookie_data(env) rule = {"type":"domains","domains":[".amazon.com"]} is_cookie_deleted(cookie_data, rule)</td></tr><tr><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_01d664.jpg"/></td><td>将 "Sheet 1" 重命名为 "LARS Resources"。然后对其进行复制。将副本放在 "Sheet 2" 之前，并通过追加后缀 "(Backup)" 进行重命名，...</td><td>result = get_file(env) <br/> expected = get_file(cloud) <br/> rules = [{"type":"sheet_name"}, <br/> {"type":"sheet_data", <br/> "sheet_idx0":0, <br/> "sheet_idx1":1}...] <br/> compare_table(result, expected, rules)</td></tr><tr><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_da8c42.jpg"/></td><td>我已经为那些尚未缴费的学生起草了一封电子邮件提醒。请帮助我从缴费记录中检查他们的电子邮件，并将其添加到收件人字段。</td><td>tree = get_ally_tree(env) <br/> rules = [{"selectors": <br/> ["tool-bar[attr|id=MsgHeadersToolbar] label [name=To] <br/> [attr|class=\\"address-pill\\"]> <br/> label[attr|class=\\"pill-label\\"] <br/> [name*=\\"fox@someuniversity.edu...] check_ally_tree(tree, rules)</td></tr></tbody></table>


Evaluation After agent interactions, we post-process the environment during the evaluation phase. This includes activating certain windows, saving some files for easy retrieval of information, and acquiring files and information for evaluation such as the final spreadsheet file for spreadsheet tasks, cookies for Chrome tasks. Finally, we apply the appropriate evaluation functions and parameters. We construct a vast collection of functions that make final wrangling and retrieve files and data information of varying types, categories, and granularities from the cloud and software from virtual machines as well as evaluation functions covering different aspects and their combinations, inputting this information as parameters to assess the outcomes. Tab. 1 illustrates evaluation processes including extracting cookie data, fetching files from both virtual machines and cloud services, retrieving the current interface's accessibility tree, and validating success by checking cookie deletions, table accuracy, and interface access. See more in B.6
评估 在代理交互后，我们在评估阶段对环境进行后处理。这包括激活某些窗口、保存部分文件以便信息的快速检索、以及获取用于评估的文件和信息，如用于表格任务的最终电子表格文件、Chrome 任务的 cookies。最后，我们应用适当的评估函数和参数。我们构建了大量函数，用于最终整理并从云端、虚拟机中的软件以及覆盖不同方面及其组合的评估函数中检索不同类型、类别和粒度的文件和数据信息，并将这些信息作为参数输入以评估结果。表1 展示了评估过程，包括提取 cookie 数据、从虚拟机和云服务获取文件、检索当前界面的可访问性树，以及通过检查 cookie 删除、表格准确性和界面访问性来验证成功。参见 B.6


### 2.3 Observation Space
### 2.3 观测空间


The observation space in OSWORLD contains the same desktop screenshot that human users perceive. This includes the mouse's position and shape, application windows, files, and folders that are opened in different sizes and orders. Also, similar to previous agent-building web and mobile research [34, 31] [12, 71] that provide and support the use of the webpage's DOM and app's view hierarchy, OSWORLD also provides accessibility (a11y) tree which can support additional information for modeling. These raw observations allow rich interactions between multiple applications but induce challenges in long-horizon decision-making from high-resolution images (e.g., 4k screenshots) and structured long text (e.g., accessibility trees). A.2 describes observation space in more detail.
OSWORLD 的观测空间包含人类用户所感知的相同桌面截图。这包括鼠标的位置与形状、应用窗口、以不同尺寸和顺序打开的文件和文件夹。类似于先前关于代理构建的网页与移动端研究 [34, 31] [12, 71]，这些研究提供并支持使用网页的 DOM 和应用视图层级，OSWORLD 还提供可访问性（a11y）树，以支持对建模的额外信息。这些原始观测允许多应用之间进行丰富的交互，但也在从高分辨率图像（如 4K 截图）和结构化长文本（如可访问性树）进行长时决策时带来挑战。A.2 详述观测空间的更多细节。


### 2.4 Action Space
### 2.4 动作空间


Action space $\mathcal{A}$ in OSWORLD encompasses all mouse and keyboard actions,including movement, clicks (left-key, right-key, multiple clicks), dragging, keystrokes, hotkeys, and others, covering all human-computer action space. Some action examples are shown on the left and the complete action list can be found in A.3 Timing is also crucial, as highlighted in previous studies on mobile devices [55], as well as the ability to determine whether a task is infeasible or completed. Therefore, we add three special actions named WAIT, FAIL, and DONE to enhance the aforementioned action spaces.
OSWORLD 的动作空间 $\mathcal{A}$ 包含所有鼠标和键盘操作，包括移动、点击（左键、右键、多次点击）、拖拽、按键、热键等，覆盖了人机交互的全部动作空间。左侧展示了一些动作示例，完整动作列表可参见 A.3。时序同样关键，如前人在移动设备上的研究所强调的 [55]，以及判断任务是否不可行或已完成的能力。因此，我们添加了三个特殊动作，命名为 WAIT、FAIL 和 DONE，以增强上述动作空间。


Previous efforts towards creating domain-specific agents, such as MiniWoB++ [48, 34], CC-Net [22], and WebArena [71, 26], have defined action spaces that include clicks and typing, as well as some actions specially designed for web browsing. However, they do not model all possible actions on a computer, leading to limitations when attempting actions like right-clicking and clicking with the ctrl key held to select items. This imposes an upper bound on agent learning capabilities.
此前为了创建领域特定代理所做的努力，如 MiniWoB++ [48, 34]、CC-Net [22] 和 WebArena [71, 26]，定义了包含点击和输入以及为网页浏览专门设计的一些动作的行动空间。然而，它们并未对计算机上的所有可能动作进行建模，在尝试诸如右键单击以及在按住 Ctrl 键的情况下选中项目等操作时会产生限制。这对代理的学习能力设定了上限。


Table 2: Example mouse and keyboard actions $\mathcal{A}$ in OSWORLD. See App. A.3 for the complete list.
表2：OSWORLD 中的示例鼠标和键盘动作 $\mathcal{A}$。完整列表请参见附录 A.3。


<table><tr><td>Function</td><td>Description</td></tr><tr><td>moveTo(x, y)</td><td>Moves the mouse to the specified coordinates.</td></tr><tr><td>click(x, y)</td><td>Clicks at the specified coordinates.</td></tr><tr><td>write('text')</td><td>Types the specified text at the current cursor location.</td></tr><tr><td>press('enter')</td><td>Presses the Enter key.</td></tr><tr><td>hotkey(‘ctrl’, ‘c’)</td><td>Performs the Ctrl+C hotkey combination (copy).</td></tr><tr><td>scroll(200)</td><td>Scrolls up by 200 units.</td></tr><tr><td>dragTo(x, y)</td><td>Drags the mouse to the specified coordinates.</td></tr><tr><td>keyDown('shift')</td><td>Holds down the Shift key.</td></tr><tr><td>keyUp(‘shift’)</td><td>Releases the Shift key.</td></tr><tr><td>WAIT</td><td>Agent decides it should wait.</td></tr><tr><td>FAIL</td><td>Agent decides the task is infeasible.</td></tr><tr><td>DONE</td><td>Agent decides the task is finished.</td></tr></table>
<table><tbody><tr><td>功能</td><td>描述</td></tr><tr><td>moveTo(x, y)</td><td>将鼠标移动到指定坐标。</td></tr><tr><td>click(x, y)</td><td>在指定坐标处点击。</td></tr><tr><td>write('text')</td><td>在当前光标位置输入指定文本。</td></tr><tr><td>press('enter')</td><td>按下回车键。</td></tr><tr><td>hotkey(‘ctrl’, ‘c’)</td><td>执行 Ctrl+C 热键组合（复制）。</td></tr><tr><td>scroll(200)</td><td>向上滚动 200 个单位。</td></tr><tr><td>dragTo(x, y)</td><td>将鼠标拖动到指定坐标。</td></tr><tr><td>keyDown('shift')</td><td>按住 Shift 键。</td></tr><tr><td>keyUp(‘shift’)</td><td>释放 Shift 键。</td></tr><tr><td>等待</td><td>代理决定应该等待。</td></tr><tr><td>失败</td><td>代理判定任务不可行。</td></tr><tr><td>完成</td><td>代理判定任务已完成。</td></tr></tbody></table>


## 3 OSWORLD Benchmark
## 3 OSWORLD 基准测试


The OSWORLD benchmark encompasses 369 real computing tasks defined and executed on Ubuntu, as well as 43 tasks on Windows. The environment preparation, annotation process, data statistics, and human performance are described in this section.
OSWORLD 基准测试包含在 Ubuntu 上定义并执行的 369 个真实计算任务，以及在 Windows 上的 43 个任务。本节将介绍环境准备、标注过程、数据统计和人类执行情况。


### 3.1 Operating System and Software Environments
### 3.1 操作系统和软件环境


OSWORLD supports the development of automated computer agents across real operating systems like Windows, macOS, and Ubuntu, focusing on Ubuntu for its open-source advantages and accessible APIs for comprehensive example creation and task evaluation. For Windows, we provide annotated examples targeting similar application functionalities. This framework is designed for open-domain tasks involving multiple applications and interfaces, such as GUIs and CLIs. It prioritizes a balanced benchmark across eight key applications: Chrome, VLC, Thunderbird, VS Code, LibreOffice suite (Calc, Writer, Impress), GIMP, and essential OS utilities (terminal, file manager, image viewer, PDF viewer), highlighting the need for varied operational skills, including commonsense reasoning, software navigation, and precise input control. Further details are available in B.1 and B.2
OSWORLD 支持在 Windows、macOS 和 Ubuntu 等真实操作系统上开发自动化计算机代理，鉴于 Ubuntu 的开源优势和便于创建综合示例及评估任务的可访问 API，重点关注 Ubuntu。对于 Windows，我们提供针对类似应用功能的带注释示例。该框架专为涉及多个应用程序和界面（如 GUI 和 CLI）的开放域任务而设计。它着重对八个关键应用程序进行均衡基准测试：Chrome、VLC、Thunderbird、VS Code、LibreOffice 套件（Calc、Writer、Impress）、GIMP 以及基本的操作系统实用程序（终端、文件管理器、图像查看器、PDF 查看器），强调需要多种操作技能，包括常识推理、软件导航和精确输入控制。更多详细信息见 B.1 和 B.2


### 3.2 Tasks
### 3.2 任务


We create a suite of 369 real-world computer tasks on Ubuntu from diverse sources such as forums, tutorials, and guidelines to demonstrate open-ended task creation within OSWORLD. Each example is carefully annotated with a natural language instruction, a setup configuration with corresponding files and setup actions for environment initialization, and a manually crafted evaluation script to check if the task is successfully executed. We also adapt 43 tasks from the Ubuntu set for analytic usage on Windows. Overall, it took 9 computer science students (all student authors) over 3 months, consuming approximately 1800 man-hours (650 hours on single-app tasks, 750 hours on workflow tasks and 400 hours for double-checking).
我们从论坛、教程和指南等不同来源在 Ubuntu 上创建了一套 369 个真实世界的计算机任务，以展示 OSWORLD 中开放式任务的创建。每个示例都精心标注了自然语言指令、包含相应文件和环境初始化设置操作的配置，以及一个手动编写的评估脚本，用于检查任务是否成功执行。我们还将 Ubuntu 任务集中的 43 个任务进行调整，用于 Windows 上的分析。总体而言，9 名计算机科学专业学生（均为学生作者）花费了 3 个多月时间，约 1800 个工时（单应用任务 650 小时，工作流任务 750 小时，复核 400 小时）。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_e06c2c.jpg"/>



Figure 3: Distribution of task instructions in OS-WORLD by app domains and operation types.
图 3：OS - WORLD 中按应用领域和操作类型划分的任务指令分布。


Task instructions and scenarios We collect realistic examples from diverse sources including official guidelines & tutorials, video pieces giving tips and tutorials on the Internet (TikTok,
任务指令和场景 我们从各种来源收集真实示例，包括官方指南和教程、互联网上提供技巧和教程的视频片段（TikTok


YouTube), how-to websites (WikiHow), Q&A forums ( Reddit, Quora, Superuser, StackOverflow), formal video courses (Coursera, Udemy), and publicly-available personal blogs & guidelines. B. 3 details resources used in our benchmark. We select examples by their popularity, helpfulness, and diversity, revealed by view counts and votes. While descriptions of single-application tasks are easily found, those of tasks that involve the coordination of multiple applications are scarce. Therefore, we authors combine existing examples and designed examples inspired by daily-life scenarios to compile the tasks. The instructions and task-related files are then crafted from these real-world guidelines and questions by the authors. After selection, each example is cross-checked by two other authors on the feasibility, ambiguity, and alignment with the source. We not only collect feasible tasks, but also tasks inherently infeasible due to feature deprecation or hallucinated features raised by real users, which results in 30 infeasible examples in our benchmark. Additionally, to demonstrate the unification ability of OSWORLD for the creation of open-ended computer tasks, we also integrate 84 examples from other benchmarks focusing on single-application or domain-specific environments such as NL2Bash [33], Mind2Web [12], SheetCopilot [29], PPTC [18], and GAIA [40]. Refer to B.4 for more details and B. 7 for sampled examples for the showcase. A total of about 400 man-hours were spent to collect these examples. §2.2 outlines the procedure for creating config and evaluation for tasks. Initial state design took 1 man-hour per example and is detailed in B.5 Evaluation design took two man-hour per example and is detailed in B.6
YouTube）、操作指南网站（WikiHow）、问答论坛（Reddit、Quora、Superuser、StackOverflow）、正规视频课程（Coursera、Udemy）以及公开的个人博客和指南。B.3 详细介绍了我们基准测试中使用的资源。我们根据示例的受欢迎程度、实用性和多样性（通过浏览量和投票数体现）进行选择。虽然单应用任务的描述很容易找到，但涉及多个应用程序协调的任务描述却很少。因此，我们作者结合现有示例和受日常生活场景启发设计的示例来编写任务。然后作者根据这些真实世界的指南和问题编写指令和与任务相关的文件。选择后，每个示例由另外两名作者就可行性、模糊性以及与来源的一致性进行交叉检查。我们不仅收集可行的任务，还收集由于功能弃用或真实用户提出的虚构功能而本质上不可行的任务，这导致我们的基准测试中有 30 个不可行的示例。此外，为了展示 OSWORLD 创建开放式计算机任务的统一能力，我们还整合了来自其他专注于单应用或特定领域环境的基准测试的 84 个示例，如 NL2Bash [33]、Mind2Web [12]、SheetCopilot [29]、PPTC [18] 和 GAIA [40]。更多详细信息见 B.4，展示的示例样本见 B.7。收集这些示例总共花费了约 400 个工时。§2.2 概述了为任务创建配置和评估的过程。每个示例的初始状态设计花费 1 个工时，详情见 B.5；每个示例的评估设计花费 2 个工时，详情见 B.6


Quality control After annotation, each example is attempted by two authors who did not participate in its annotation, acting as agents to complete the task. This process evaluates the current example's quality and provides feedback to the annotators (such as unclear instructions or inability to complete the task, crashes in corner cases, serious instances of false positives and negatives, etc.), and involves joint revisions and supplements. During experiments for human performance and baselines, we further fixed examples found to have issues, dedicating over 400 man-hours for four rounds of checks.
质量控制 标注完成后，由未参与该示例标注的两名作者作为代理尝试完成每个示例任务。此过程评估当前示例的质量，并向标注人员提供反馈（如指令不清晰、无法完成任务、极端情况下崩溃、严重的误报和漏报等），并进行联合修订和补充。在人类执行情况和基线实验期间，我们进一步修正了发现有问题的示例，四轮检查共投入 400 多个工时。


### 3.3 Data Statistics
### 3.3 数据统计


Statistics To facilitate analysis, we cluster the examples into software categories. Specifically, these categories include OS, Office (LibreOffice Calc, Impress, Writer), Daily (Chrome, VLC Player, Thunderbird), Professional (VS Code and GIMP), and Workflow (tasks involving multiple apps). The main statistics of OSWORLD are presented in Tab. 3 and Fig. 3 showcasing the outline and a broad spectrum of tasks. Specifically, OSWORLD contains a total of 369 tasks (and an additional 43 tasks on Windows for analysis), with the majority (268 tasks or 72.6%) aiming at single application functionalities and a section of workflow-related tasks (101 tasks or 27.4%). We also consider infeasible examples, totaling 30 tasks or 8.1% of the dataset. Additionally, a total of 84 tasks (22.8%) are integrated from related datasets. The final dataset incorporates 302 distinct initial states and 134 different evaluation scripts, underscoring the comprehensive approach towards evaluating the tasks' complexity and requirements. More statistic details are available in B.4
为分析便利，我们将示例聚类到软件类别中。具体而言，这些类别包括操作系统（OS）、办公软件（LibreOffice Calc、Impress、Writer）、日常应用（Chrome、VLC Player、Thunderbird）、专业应用（VS Code 与 GIMP）以及工作流相关任务（涉及多应用程序的任务）。OSWORLD 的主要统计数据在表 3 和图 3 中展示，概述及广泛的任务范围。具体而言，OSWORLD 总共包含 369 项任务（分析中在 Windows 还额外有 43 项任务），其中大多数（268 项，占 72.6%）针对单一应用功能，另有一部分与工作流程相关的任务（101 项，占 27.4%）。我们也考虑不可行的示例，总计 30 项任务，占数据集的 8.1%。此外，总计有 84 项任务（22.8%）来自相关数据集的整合。最终数据集包含 302 个不同的初始状态和 134 种不同的评估脚本，强调对任务复杂性和需求的全面评估方法。更多统计细节请参见 B.4


Table 3: OSWORLD statistic. Supp. refers to Windows tasks that are usable only after activation due to copyright.
表 3：OSWORLD 统计。Supp. 指的是仅在激活后由于版权原因才能使用的 Windows 任务。


<table><tr><td>Statistic</td><td>Number</td></tr><tr><td>Total tasks (Ubuntu)</td><td>369 (100%)</td></tr><tr><td>- Multi-App Workflow</td><td>101 (27%)</td></tr><tr><td>- Single-App</td><td>268 (73%)</td></tr><tr><td>- Integrated</td><td>84 (23%)</td></tr><tr><td>- Infeasible</td><td>30 (8%)</td></tr><tr><td>Supp. tasks (Windows)</td><td>43</td></tr><tr><td>Initial States</td><td>302</td></tr><tr><td>Eval. Scripts</td><td>134</td></tr></table>
<table><tbody><tr><td>统计</td><td>数量</td></tr><tr><td>总任务（Ubuntu）</td><td>369 (100%)</td></tr><tr><td>- 多应用工作流</td><td>101 (27%)</td></tr><tr><td>- 单应用</td><td>268 (73%)</td></tr><tr><td>- 集成</td><td>84 (23%)</td></tr><tr><td>- 不可行</td><td>30 (8%)</td></tr><tr><td>Supp. 任务（Windows）</td><td>43</td></tr><tr><td>初始状态</td><td>302</td></tr><tr><td>评估脚本</td><td>134</td></tr></tbody></table>


Comparison with existing benchmarks Tab. 4 compares OSWORLD to prior benchmarks. First, instead of focusing on specific computer applications such as a browser [71, 12], OSWORLD utilizes raw multimodal observations and keyboard/mouse actions used by humans, which are universal across different applications and allows the development of generalizable agents. Second, instead of providing static demonstrations, OSWORLD executable environment supports agent exploration during learning and evaluation - behavior critical in generalizing to new applications. Third, instead of focusing on interactions within a single application, OSWORLD considers cross-app interactions found in real-world computer usage. Fourth, instead of limiting to a single task type with a success definition, OSWORLD provides example-wise, execution-based evaluation for tasks. Specifically, the total of 134 unique execution-based evaluation functions in our benchmark is significantly more than previous work, demonstrating the complexity, diversity, and evaluation challenges of OSWORLD tasks. Finally, instead of focusing on clean initialization, OSWORLD tasks require operation from intermediate initialization, as is typical in realistic computer usage.
与现有基准的比较 Tab. 4 将 OSWORLD 与此前的基准进行比较。首先，与专注于浏览器等特定计算机应用程序不同，OSWORLD 使用人类使用的原始多模态观测和键盘/鼠标操作，这些在不同应用中通用且有助于开发可泛化的智能体。第二，与提供静态演示不同，OSWORLD 的可执行环境在学习和评估阶段支持代理的探索——这一行为对于在新应用中的泛化至关重要。第三，与仅关注单一应用内的交互不同，OSWORLD 考虑在真实世界计算机使用中存在的跨应用交互。第四，与仅限于单一任务类型及其成功定义不同，OSWORLD 提供逐样例、基于执行的任务评估。具体而言，我们基准中总共 134 种独特的基于执行的评估函数远多于以往工作，展示了 OSWORLD 任务的复杂性、多样性与评估挑战。最后，与注重干净初始化不同，OSWORLD 任务要求从中间初始化开始运行，这在现实计算机使用中是典型的。


Table 4: Comparison of different environments for benchmarking digital agents.
Table 4: 对用于基准数字智能体的不同环境的比较。


<table><tr><td></td><td>#Instances (# Templates)</td><td>Control. Exec. Env.?</td><td>Environment Scalability?</td><td>Multimodal Support?</td><td>Cross-App？</td><td>Intermediate Init. State?</td><td>#Exec.-based Eval. Func.</td></tr><tr><td>GAIA [40]</td><td>466</td><td>✘</td><td>-</td><td>✘</td><td>✘</td><td>✘</td><td>0</td></tr><tr><td>Mind2WEB 12</td><td>2350</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✓</td><td>0</td></tr><tr><td>WEBLINX 37</td><td>2337</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✓</td><td>0</td></tr><tr><td>PixelHelp 31</td><td>187</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✘</td><td>0</td></tr><tr><td>METAGUI 52</td><td>1125</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✘</td><td>0</td></tr><tr><td>AITW 44</td><td>30k</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✓</td><td>0</td></tr><tr><td>SCREENAGENT 42</td><td>70</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✓</td><td>0</td></tr><tr><td>OMNIACT 25</td><td>9802</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✓</td><td>0</td></tr><tr><td>AGENTBENCH 36</td><td>1091</td><td>Multi-isolated</td><td>✘</td><td>✘</td><td>✘</td><td>✘</td><td>7</td></tr><tr><td>INTERCODE 62</td><td>1350 (3)</td><td>Code</td><td>✘</td><td>✘</td><td>✘</td><td>✘</td><td>3</td></tr><tr><td>MINIWOB++ 34</td><td>125</td><td>Web</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>125</td></tr><tr><td>WEBSHOP 63</td><td>12k (1)</td><td>Web</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>1</td></tr><tr><td>WEBARENA [71]</td><td>812 (241)</td><td>Web</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>5</td></tr><tr><td>VWEBARENA 26</td><td>910 (314)</td><td>Web</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>6</td></tr><tr><td>WORKARENA [13]</td><td>23k (29)</td><td>Web</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>7</td></tr><tr><td>WIKIHOW 66</td><td>150 (16)</td><td>Mobile</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>16</td></tr><tr><td>Assistable</td><td>100</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>2</td></tr><tr><td>OSWORLD</td><td>369</td><td>Computer</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>134</td></tr></table>
<table><tbody><tr><td></td><td>#实例 (# 模板)</td><td>控制。执行环境？</td><td>环境扩展性？</td><td>多模态支持？</td><td>跨应用？</td><td>中间初始化状态？</td><td>基于执行的评估函数。</td></tr><tr><td>GAIA [40]</td><td>466</td><td>✘</td><td>-</td><td>✘</td><td>✘</td><td>✘</td><td>0</td></tr><tr><td>Mind2WEB 12</td><td>2350</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✓</td><td>0</td></tr><tr><td>WEBLINX 37</td><td>2337</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✓</td><td>0</td></tr><tr><td>PixelHelp 31</td><td>187</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✘</td><td>0</td></tr><tr><td>METAGUI 52</td><td>1125</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✘</td><td>0</td></tr><tr><td>AITW 44</td><td>30k</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✓</td><td>0</td></tr><tr><td>SCREENAGENT 42</td><td>70</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✓</td><td>0</td></tr><tr><td>OMNIACT 25</td><td>9802</td><td>✘</td><td>-</td><td>✓</td><td>✘</td><td>✓</td><td>0</td></tr><tr><td>AGENTBENCH 36</td><td>1091</td><td>多重隔离</td><td>✘</td><td>✘</td><td>✘</td><td>✘</td><td>7</td></tr><tr><td>INTERCODE 62</td><td>1350 (3)</td><td>代码</td><td>✘</td><td>✘</td><td>✘</td><td>✘</td><td>3</td></tr><tr><td>MINIWOB++ 34</td><td>125</td><td>网页</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>125</td></tr><tr><td>WEBSHOP 63</td><td>12k (1)</td><td>网页</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>1</td></tr><tr><td>WEBARENA [71]</td><td>812 (241)</td><td>网页</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>5</td></tr><tr><td>VWEBARENA 26</td><td>910 (314)</td><td>网页</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>6</td></tr><tr><td>WORKARENA [13]</td><td>23k (29)</td><td>网页</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>7</td></tr><tr><td>WIKIHOW 66</td><td>150 (16)</td><td>移动</td><td>✘</td><td>✓</td><td>✘</td><td>✘</td><td>16</td></tr><tr><td>可辅助的</td><td>100</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td><td>✓</td><td>2</td></tr><tr><td>OSWORLD</td><td>369</td><td>计算机</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>134</td></tr></tbody></table>


### 3.4 Human Performance
### 3.4 人类表现


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_0fc2ef.jpg"/>



Figure 4: Human operation time and accuracy on OSWORLD and WebArena.
图 4：人类在 OSWORLD 和 WebArena 上的操作时间与准确率。


We conduct human evaluations on each example in our dataset, with annotators being computer science major college students who possess basic software usage skills but have not been exposed to the samples or software before. We recorded the time required to complete each example and whether their completion of the example was correct. For comparison, we also sampled 100 examples from WebArena [71] under the same evaluation setup.
我们对数据集中的每个示例进行了人类评估，标注人员为计算机科学专业的大学生，他们具备基本的软件使用技能，但此前未接触过这些样本或软件。我们记录了完成每个示例所需的时间，以及他们完成示例的情况是否正确。为作比较，我们还在相同的评估设置下从 WebArena [71] 中抽取了 100 个示例。


As illustrated, tasks from our dataset generally required more time to complete, with a median completion time of 111.94 seconds (compared to 35.38 seconds in WebArena), and a significant number of examples distributed at 900 seconds or even more. In terms of accuracy, the human performance on our tasks was approximately 72.36%, significantly lower than the 88% observed on the pure web task dataset. These findings highlight the complexity and challenge of tasks in our dataset, which demand more time and effort. The lower accuracy rate further indicates that our tasks require a higher level of understanding and proficiency, underscoring the need for advanced models and techniques to tackle them effectively.
如图所示，我们数据集中的任务通常需要更多时间来完成，中位完成时间为 111.94 秒（相比之下，WebArena 为 35.38 秒），并且有相当数量的示例完成时间分布在 900 秒甚至更久。在准确率方面，人类在我们任务上的表现约为 72.36%，显著低于在纯网络任务数据集上观察到的 88%。这些发现凸显了我们数据集中任务的复杂性和挑战性，需要更多的时间和精力。较低的准确率进一步表明，我们的任务需要更高水平的理解和熟练程度，强调了有效解决这些任务需要先进的模型和技术。


## 4 Benchmarking LLM and VLM Agent Baselines
## 4 对大语言模型（LLM）和视觉语言模型（VLM）智能体基线进行基准测试


In this section, we present the implementation details and experimental settings for several state-of-the-art LLM and VLM agent baselines on OSWORLD benchmark, as well as their performance.
在本节中，我们介绍了几种最先进的大语言模型（LLM）和视觉语言模型（VLM）智能体基线在 OSWORLD 基准测试中的实现细节和实验设置，以及它们的性能表现。


### 4.1 LLM and VLM Agent Baselines
### 4.1 大语言模型（LLM）和视觉语言模型（VLM）智能体基线


We evaluate state-of-the-art open-source LLMs and VLMs such as Mixtral [23] and Llama-3 [39], and closed-source ones such as GPT, Gemini, Claude on OSWORLD. We also explore methods such as the Set-of-Marks aided approach [61, 14], which has been demonstrated to improve spatial capabilities for visual reasoning. For each method, we provide the 3 most recent observation-action pairs and generate actions with the temperature of 1.0 and top-p of 0.9 . The prompts used in the experiments are provided in C.1 We request the agents to complete the tasks within a max step limit of 15, which is enough for most tasks. We present a summary of the results in Tab. 5 and analysis in
我们在 OSWORLD 上评估了最先进的开源大语言模型（LLM）和视觉语言模型（VLM），如 Mixtral [23] 和 Llama - 3 [39]，以及闭源模型，如 GPT、Gemini、Claude。我们还探索了诸如标记集辅助方法 [61, 14] 等方法，该方法已被证明可提高视觉推理的空间能力。对于每种方法，我们提供 3 个最新的观察 - 动作对，并以温度为 1.0 和核采样概率（top - p）为 0.9 来生成动作。实验中使用的提示见 C.1。我们要求智能体在最多 15 步的限制内完成任务，这对大多数任务来说是足够的。我们在表 5 中总结了结果并进行分析


Table 5: Success rates of baseline LLM and VLM agents on OSWORLD, grouped by task categories: OS, Office (LibreOffice Calc, Impress, Writer), Daily (Chrome, VLC Player, Thunderbird), Professional (VS Code and GIMP) and Workflow (tasks involving multiple apps), for gaining insights from interfaces and operation logic. See C.1 and C.6 for more details.
表 5：大语言模型（LLM）和视觉语言模型（VLM）智能体基线在 OSWORLD 上的成功率，按任务类别分组：操作系统（OS）、办公软件（LibreOffice Calc、Impress、Writer）、日常软件（Chrome、VLC 播放器、Thunderbird）、专业软件（VS Code 和 GIMP）和工作流（涉及多个应用程序的任务），以从界面和操作逻辑中获取见解。更多详情见 C.1 和 C.6。


<table><tr><td rowspan="2">Inputs</td><td rowspan="2">Model</td><td colspan="6">Success Rate (↑)</td></tr><tr><td>OS</td><td>Office</td><td>Daily</td><td>Profess.</td><td>Workflow</td><td>Overall</td></tr><tr><td rowspan="8">A11y tree</td><td>Mixtral-8x7B</td><td>12.50%</td><td>1.01%</td><td>4.79%</td><td>6.12%</td><td>0.09%</td><td>2.98%</td></tr><tr><td>Llama-3-70B</td><td>4.17%</td><td>1.87%</td><td>2.71%</td><td>0.00%</td><td>0.93%</td><td>1.61%</td></tr><tr><td>GPT-3.5</td><td>4.17%</td><td>4.43%</td><td>2.71%</td><td>0.00%</td><td>1.62%</td><td>2.69%</td></tr><tr><td>GPT-4</td><td>20.83%</td><td>3.58%</td><td>25.64%</td><td>26.53%</td><td>2.97%</td><td>12.24%</td></tr><tr><td>Gemini-Pro</td><td>4.17%</td><td>1.71%</td><td>3.99%</td><td>4.08%</td><td>0.63%</td><td>2.37%</td></tr><tr><td>Gemini-Pro-1.5</td><td>12.50%</td><td>2.56%</td><td>7.83%</td><td>4.08%</td><td>3.60%</td><td>4.81%</td></tr><tr><td>Qwen-Max</td><td>29.17%</td><td>3.58%</td><td>8.36%</td><td>10.20%</td><td>2.61%</td><td>6.87%</td></tr><tr><td>GPT-40</td><td>20.83%</td><td>6.99%</td><td>16.81%</td><td>16.33%</td><td>7.56%</td><td>11.36%</td></tr><tr><td rowspan="6">Screenshot</td><td>CogAgent</td><td>4.17%</td><td>0.85%</td><td>2.71%</td><td>0.00%</td><td>0.00%</td><td>1.11%</td></tr><tr><td>GPT-4V</td><td>12.50%</td><td>1.86%</td><td>7.58%</td><td>4.08%</td><td>6.04%</td><td>5.26%</td></tr><tr><td>Gemini-ProV</td><td>8.33%</td><td>3.58%</td><td>6.55%</td><td>16.33%</td><td>2.08%</td><td>5.80%</td></tr><tr><td>Gemini-Pro-1.5</td><td>12.50%</td><td>6.99%</td><td>2.71%</td><td>6.12%</td><td>3.60%</td><td>5.40%</td></tr><tr><td>Claude-3-Opus</td><td>4.17%</td><td>1.87%</td><td>2.71%</td><td>2.04%</td><td>2.61%</td><td>2.42%</td></tr><tr><td>GPT-40</td><td>8.33%</td><td>3.58%</td><td>6.07%</td><td>4.08%</td><td>5.58%</td><td>5.03%</td></tr><tr><td rowspan="6">Screenshot + Ally tree</td><td>CogAgent</td><td>4.17%</td><td>0.85%</td><td>2.71%</td><td>0.62%</td><td>0.09%</td><td>1.32%</td></tr><tr><td>GPT-4V</td><td>16.66%</td><td>6.99%</td><td>24.50%</td><td>18.37%</td><td>4.64%</td><td>12.17%</td></tr><tr><td>Gemini-ProV</td><td>4.17%</td><td>4.43%</td><td>6.55%</td><td>0.00%</td><td>1.52%</td><td>3.48%</td></tr><tr><td>Gemini-Pro-1.5</td><td>12.50%</td><td>3.58%</td><td>7.83%</td><td>8.16%</td><td>1.52%</td><td>5.10%</td></tr><tr><td>Claude-3-Opus</td><td>12.50%</td><td>3.57%</td><td>5.27%</td><td>8.16%</td><td>1.00%</td><td>4.41%</td></tr><tr><td>GPT-40</td><td>41.67%</td><td>6.16%</td><td>12.33%</td><td>14.29%</td><td>7.46%</td><td>11.21%</td></tr><tr><td rowspan="6">Set-of-Mark</td><td>CogAgent</td><td>4.17%</td><td>0.00%</td><td>2.71%</td><td>0.00%</td><td>0.53%</td><td>0.99%</td></tr><tr><td>GPT-4V</td><td>8.33%</td><td>8.55%</td><td>22.84%</td><td>14.28%</td><td>6.57%</td><td>11.77%</td></tr><tr><td>Gemini-ProV</td><td>4.17%</td><td>1.01%</td><td>1.42%</td><td>0.00%</td><td>0.63%</td><td>1.06%</td></tr><tr><td>Gemini-Pro-1.5</td><td>16.67%</td><td>5.13%</td><td>12.96%</td><td>10.20%</td><td>3.60%</td><td>7.79%</td></tr><tr><td>Claude-3-Opus</td><td>12.50%</td><td>2.72%</td><td>14.24%</td><td>6.12%</td><td>4.49%</td><td>6.72%</td></tr><tr><td>GPT-40</td><td>20.83%</td><td>3.58%</td><td>3.99%</td><td>2.04%</td><td>3.60%</td><td>4.59%</td></tr><tr><td colspan="2">Human Performance</td><td>75.00%</td><td>71.79%</td><td>70.51%</td><td>73.47%</td><td>73.27%</td><td>72.36%</td></tr></table>
<table><tbody><tr><td rowspan="2">输入</td><td rowspan="2">模型</td><td colspan="6">成功率 (↑)</td></tr><tr><td>操作系统</td><td>办公软件</td><td>日常</td><td>专业</td><td>工作流</td><td>总体</td></tr><tr><td rowspan="8">无障碍树</td><td>Mixtral-8x7B</td><td>12.50%</td><td>1.01%</td><td>4.79%</td><td>6.12%</td><td>0.09%</td><td>2.98%</td></tr><tr><td>Llama-3-70B</td><td>4.17%</td><td>1.87%</td><td>2.71%</td><td>0.00%</td><td>0.93%</td><td>1.61%</td></tr><tr><td>GPT-3.5</td><td>4.17%</td><td>4.43%</td><td>2.71%</td><td>0.00%</td><td>1.62%</td><td>2.69%</td></tr><tr><td>GPT-4</td><td>20.83%</td><td>3.58%</td><td>25.64%</td><td>26.53%</td><td>2.97%</td><td>12.24%</td></tr><tr><td>Gemini-Pro</td><td>4.17%</td><td>1.71%</td><td>3.99%</td><td>4.08%</td><td>0.63%</td><td>2.37%</td></tr><tr><td>Gemini-Pro-1.5</td><td>12.50%</td><td>2.56%</td><td>7.83%</td><td>4.08%</td><td>3.60%</td><td>4.81%</td></tr><tr><td>Qwen-Max</td><td>29.17%</td><td>3.58%</td><td>8.36%</td><td>10.20%</td><td>2.61%</td><td>6.87%</td></tr><tr><td>GPT-40</td><td>20.83%</td><td>6.99%</td><td>16.81%</td><td>16.33%</td><td>7.56%</td><td>11.36%</td></tr><tr><td rowspan="6">截图</td><td>认知智能体</td><td>4.17%</td><td>0.85%</td><td>2.71%</td><td>0.00%</td><td>0.00%</td><td>1.11%</td></tr><tr><td>GPT-4V</td><td>12.50%</td><td>1.86%</td><td>7.58%</td><td>4.08%</td><td>6.04%</td><td>5.26%</td></tr><tr><td>Gemini-ProV</td><td>8.33%</td><td>3.58%</td><td>6.55%</td><td>16.33%</td><td>2.08%</td><td>5.80%</td></tr><tr><td>Gemini-Pro-1.5</td><td>12.50%</td><td>6.99%</td><td>2.71%</td><td>6.12%</td><td>3.60%</td><td>5.40%</td></tr><tr><td>Claude-3-Opus</td><td>4.17%</td><td>1.87%</td><td>2.71%</td><td>2.04%</td><td>2.61%</td><td>2.42%</td></tr><tr><td>GPT-40</td><td>8.33%</td><td>3.58%</td><td>6.07%</td><td>4.08%</td><td>5.58%</td><td>5.03%</td></tr><tr><td rowspan="6">截图 + 无障碍树</td><td>认知智能体</td><td>4.17%</td><td>0.85%</td><td>2.71%</td><td>0.62%</td><td>0.09%</td><td>1.32%</td></tr><tr><td>GPT-4V</td><td>16.66%</td><td>6.99%</td><td>24.50%</td><td>18.37%</td><td>4.64%</td><td>12.17%</td></tr><tr><td>Gemini-ProV</td><td>4.17%</td><td>4.43%</td><td>6.55%</td><td>0.00%</td><td>1.52%</td><td>3.48%</td></tr><tr><td>Gemini-Pro-1.5</td><td>12.50%</td><td>3.58%</td><td>7.83%</td><td>8.16%</td><td>1.52%</td><td>5.10%</td></tr><tr><td>Claude-3-Opus</td><td>12.50%</td><td>3.57%</td><td>5.27%</td><td>8.16%</td><td>1.00%</td><td>4.41%</td></tr><tr><td>GPT-40</td><td>41.67%</td><td>6.16%</td><td>12.33%</td><td>14.29%</td><td>7.46%</td><td>11.21%</td></tr><tr><td rowspan="6">标记集</td><td>认知智能体</td><td>4.17%</td><td>0.00%</td><td>2.71%</td><td>0.00%</td><td>0.53%</td><td>0.99%</td></tr><tr><td>GPT-4V</td><td>8.33%</td><td>8.55%</td><td>22.84%</td><td>14.28%</td><td>6.57%</td><td>11.77%</td></tr><tr><td>Gemini-ProV</td><td>4.17%</td><td>1.01%</td><td>1.42%</td><td>0.00%</td><td>0.63%</td><td>1.06%</td></tr><tr><td>Gemini-Pro-1.5</td><td>16.67%</td><td>5.13%</td><td>12.96%</td><td>10.20%</td><td>3.60%</td><td>7.79%</td></tr><tr><td>Claude-3-Opus</td><td>12.50%</td><td>2.72%</td><td>14.24%</td><td>6.12%</td><td>4.49%</td><td>6.72%</td></tr><tr><td>GPT-40</td><td>20.83%</td><td>3.58%</td><td>3.99%</td><td>2.04%</td><td>3.60%</td><td>4.59%</td></tr><tr><td colspan="2">人类表现</td><td>75.00%</td><td>71.79%</td><td>70.51%</td><td>73.47%</td><td>73.27%</td><td>72.36%</td></tr></tbody></table>


Sec. 4.2. We implement the following four types of input settings on LLM and VLM: Accessibility tree, Screenshot, Screenshot + accessibility tree, and Set-of-Marks. Details see App. C. 3
Sec. 4.2. 我们在 LLM 和 VLM 上实现以下四种输入设置：可访问性树、屏幕截图、屏幕截图 + 可访问性树，以及一组标记。详细见附录 C.3


### 4.2 Results
### 4.2 结果


LLMs and VLMs are still far from being digital agents on real computers. Table 5 shows that screenshots-only agents that generate keyboard/mouse actions via pyautogui achieve 5.26% to 5.80% success rate (VLMs GPT-4V, Gemini-Pro-vision) while the text-only agents using using a11y tree as input achieve 2.37% to 12.24% (LLMs GPT-4, GPT-4o). These results from state-of-the-art VLMs and LLMs significantly trail the performance of humans not familiar with the software (72.36%), which indicates further research is required to develop capable digital assistants. While Claude-3 Opus is competitive with GPT-4V on common benchmarks [2], it underperforms GPT-4V significantly as a digital agent in OSWORLD. D. 4 present qualitative analysis and infer reasons.
LLMs 和 VLMs 仍远未成为在真实计算机上的数字代理。表5显示，仅屏幕截图的代理通过 pyautogui 生成键盘/鼠标操作的成功率为 5.26% 到 5.80%（VLMs GPT-4V、Gemini-Pro-vision），而以文本为输入且使用 a11y 树的代理的成功率为 2.37% 到 12.24%（LLMs GPT-4、GPT-4o）。这些来自最先进 VLM 和 LLM 的结果显著落后于对软件不熟悉的人类（72.36%），这表明需要进一步研究以开发有能力的数字助手。尽管 Claude-3 Opus 在常见基准测试 [2] 上与 GPT-4V 具有竞争力，但在 OSWORLD 作为数字代理时显著劣于 GPT-4V。D.4 给出定性分析并推断原因。


Agents have much higher variance than humans in different types of computer tasks. Tab. 5 shows that agent performance varies significantly across different software types, performing better on CLI-oriented interfaces (such as OS-type tasks) compared to GUI-oriented interfaces (such as Office tasks involving clicks on spreadsheet interfaces and document processing). Moreover, the CLI vs. GUI gap between models and settings is inconsistent, with some >20%. Similarly, performance on workflow-type tasks involving multiple software (<5%) significantly trails single software performance. Unlike agent performance, human performance is consistent across these tasks,fluctuating around ${70}\%$ with $< 5\%$ deviation. This suggests that the way humans understand and complete tasks may differ significantly from the current logic and methods based on LLMs and VLMs.
代理在不同类型的计算任务中比人类有更高的方差。表5 显示代理在不同软件类型上的表现差异显著，在面向 CLI 的界面（如 OS 类型任务）上表现优于面向 GUI 的界面（如涉及对电子表格界面和文档处理的点击的 Office 任务）。此外，模型与设置之间的 CLI 与 GUI 差距并不一致，有些超过 20%。同样，涉及多种软件的工作流型任务的表现（<5%）显著落后于单一软件的表现。与代理表现不同，人类在这些任务上的表现是一致的，围绕 ${70}\%$ 波动，具有 $< 5\%$ 的偏差。这表明人类理解和完成任务的方式可能与基于 LLMs 和 VLMs 的当前逻辑和方法有显著差异。


A11y tree and SoM's effectiveness varies by models. The a11y tree contains some attribute information of visible elements, including window position and size, as well as some semantic labels of the window. The performance gap illustrated in Table 5 between GPT-4V and Claude-3 with additional a11y tree information and under a pure screenshot setup suggests that it still has significant room for improvement in accurately perceiving and reasoning GUI elements. Conclusions are reversed for Gemini-Pro. While applying SoM setting, there is a decline for GPT-4V in performance compared to directly providing the model with screenshots and a 11 yre ie inputs, which contradicts the widely shown effectiveness of SoM in classic image understanding tasks [61], as well as in application areas like web agents [70, 20]. We speculate that this is due to the tasks performed within operating systems having higher resolution and much more elements, (e.g., the cells in a spread table), leading to a significant amount of noise that counteracts the auxiliary role of bounding boxes. Some tasks also require coordinate-level operations, which cannot be modeled by SoM bounding boxes.
可访问性树和 SoM 的有效性因模型而异。a11y 树包含可见元素的一些属性信息，包括窗口位置和大小，以及窗口的一些语义标签。表5 中 GPT-4V 与 Claude-3 在额外 a11y 树信息及纯屏幕截图设定下的性能差距，表明在准确感知和推理 GUI 元素方面仍有显著改进空间。对 Gemini-Pro 的结论则相反。在应用 SoM 设置时，与直接向模型提供屏幕截图及 11 yre ie 输入相比，GPT-4V 的性能有所下降，这与 SoM 在经典图像理解任务 [61] 以及在网络代理等应用领域的广泛证明的有效性相悖 [70, 20]。我们推测这是因为操作系统内执行的任务具有更高分辨率和更多元素（例如电子表格中的单元格），导致大量噪声抵消了边界框的辅助作用。一些任务还需要坐标级操作，SoM 边界框无法建模。


VLM agents with screenshot-only setting show lower performance, but it should be the ultimate configuration in the long run. The setting that relies solely on screenshots exhibits the lowest performance, at only 5.26%, among all. Despite the performance, it is worth mentioning that this is the only configuration that does not require additional information, such as an a11y tree, making it concise and in alignment with intuitive human perception since the a11y tree may not be well-supported across all software or cannot be obtained under noisy conditions (e.g., when the agent is restricted to viewing the computer through peripheral screens), and the massive amount of tokens contained in the a11y tree (even just the leaf nodes can have tens of thousands of tokens) can also impose an additional inference burden on the model. Future work on purely vision-based agents could lead to stronger generalization capabilities, efficiency, and, ultimately, the potential for integration with the physical world on a larger scale.
仅屏幕截图设置的 VLM 代理表现较低，但从长远来看应是最终配置。仅依赖屏幕截图的设置在所有设置中表现最低，只有 5.26%。尽管如此，需要指出的是这是唯一一种不需要额外信息（如 a11y 树）的配置，使其简洁且符合直觉的人类感知，因为 a11y 树在所有软件上可能并不被良好支持或在噪声条件下无法获取（如代理仅通过外设屏幕查看计算机时），并且 a11y 树中包含的大量标记（即使仅叶节点也可能有数万标记）也会给模型带来额外的推断负担。未来仅基于视觉的代理工作可能带来更强的泛化能力、效率，并最终有望更大规模地与物理世界整合。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_851149.jpg"/>



## 5 Analysis
## 5 分析


Higher resolution typically improves performance Despite the availability of high-res displays, most VLMs are trained on far lower resolutions. We evaluate performance using screenshot-only and SoM by down-sampling the original resolution by 0.2-0.8 (Figure for 10% of examples on the right). The output coordinates of the model for the screenshot setting are still expected to align with the original resolution (i.e., 1080P). Resolution impact on performance is shown on the right (for a subset of 10% of examples). Screenshot-only performance improves with higher resolution, which may arise from the discrepancy between the downsampled input resolution and the coordinates of the output (which is for the original resolution). In contrast for SoM, a reduction to 768×432 (down-sampling 0.4) improves performance and further reduction in resolution to 0.2 noticeably degrades performance.
分辨率通常提升性能 尽管有高分辨率显示，但大多数 VLMs 是在远低于此分辨率的条件下训练的。我们通过将原始分辨率下采样 0.2-0.8（右图显示 10% 例子）来使用截图仅限以及 SoM 评估性能。截图设置下模型的输出坐标仍应与原始分辨率对齐（即 1080P）。分辨率对性能的影响在右侧对 10% 的样本给出子集。截图仅限性能随着更高分辨率而提升，这可能源自下采样输入分辨率与输出坐标（原始分辨率）之间的差异。相比之下，对 SoM，降至 768×432（下采样 0.4）可提升性能，进一步降至 0.2 的分辨率会显著降低性能。


Longer text-based trajectory history context improves performance, unlike screenshot-only history, but poses efficiency challenges We include current and past N rounds of observations and actions in the constructed prompts (see App. C. 1 for more details) to explore the impact of context length on agent performance. We set N to 1, 2, 3, and all where we put as much context as we can. The results (on 10% of examples) on the right show the performance increase with more history context for SoM. Future work on constructing models with enhanced capabilities for longer context support and understanding reasoning, improving model efficiency, and designing new agent architectures for efficient memory storage will have a significant impact on digital agents. However, we also note that the inclusion of additional trajectory history does not enhance performance under the pure screenshot setting. This suggests that contemporary advanced VLMs might not be as adept at extracting robust contextual information from images as they are from textual data.
比起仅文本历史，较长的文本轨迹历史上下文能提升性能，但会带来效率挑战 我们在构造的提示中包含当前轮次及前 N 轮的观测和动作（见附录 C.1 以了解更多），以探究上下文长度对代理性能的影响。我们将 N 设置为 1、2、3 以及尽量多的上下文。右侧对 10% 样本的结果显示，SoM 的历史上下文越多，性能提升越明显。未来在构建具备更长上下文支持与理解推理能力、提升模型效率、设计新型代理架构以实现高效记忆存储方面的工作，将对数字代理产生重大影响。然而，我们也注意到，在纯截图设置下，增加额外的轨迹历史并不能提升性能。这表明当代先进的 VLMs 可能在从图像中提取稳健上下文信息方面不如从文本数据中获取信息的能力。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_b763d3.jpg"/>



VLM agents struggle with perturbation of application windows size/position and irrelevant information We consider the best SoM setting and a subset of 28 OS-WORLD tasks that agents perform well on (with a success rate of 50.79%). At the beginning of each task, we introduce disturbances to the windows by 1) changing the position of the window; 2) changing the size of the window to the minimal; 3) opening some irrelevant software and maximizing them to clutter the screen. We find that current agents are not robust to these superficial changes,
VLM 代理在应用窗口大小/位置的扰动和无关信息面前难以应对 我们在最佳 SoM 设置下，挑选一组代理表现较好的 28 个 OS-WORLD 任务（成功率为 50.79%）。在每个任务开始时，我们通过以下方式对窗口造成干扰：1）改变窗口位置；2）将窗口大小改到最小；3）打开一些无关的软件并将其最大化以使屏幕变得混乱。我们发现当前代理对这些表层变化不具鲁棒性，


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_08ad83.jpg"/>



which lead to performance drops of ${60}\%$ and even ${80}\%$ . Surprisingly,agents are able to switch windows to a certain degree but fail to maximize the window as an intermediate step. This suggests that while agents possess some capability to navigate between windows, they lack a comprehensive strategy for managing window states effectively.
这导致 ${60}\%$ 的性能下降，甚至 ${80}\%$ 的下降。令人惊讶的是，代理在一定程度上能够切换窗口，但作为中间步骤却未能将窗口最大化。这表明虽然代理具备在窗口间导航的某些能力，但在有效管理窗口状态方面缺乏综合策略。


## 6 Related Work
## 6 相关工作


Benchmarks for multimodal agents Recent research has made significant progress in evaluating multimodal agents, including physical world [49, 11, 8] and digital world [48, 15]. In the digital realm, interactive evaluation of agents primarily spans coding, web scenarios, and mobile applications. Prior work on coding provides frameworks and datasets for evaluating agents across programming languages and software engineering activities [62, 24, 28, 50]. Prior work on web agents develop platforms for interacting with websites through keyboard and mouse actions, as well as datasets focusing on open-ended web tasks and realistic web navigation [48, 34, 63, 12, 71, 26, 13]. Mobile device interaction research develop simulators for mobile UI interactions and platforms dedicated to InfoUI tasks [31, 52, 56, 55, 44, 66, 58, 65, 57]. Further, environments connecting to real computers and datasets for GUI grounding, albeit without interactive capability, have emerged [17, 10, 42, 25, 53]. Comprehensive task evaluation across different aspects also sees innovations [36, 40]. In contrast to prior work that address specific domains/applications/datasets, OSWORLD facilitates the development of general-purpose digital agents that openly interact with OS. See Tab. 4 for comparison.
多模态代理基准 最近的研究在评估多模态代理方面取得了显著进展，包括物理世界 [49, 11, 8] 与数字世界 [48, 15]。在数字领域，代理的交互评估主要涵盖编码、网页场景和移动应用。早期关于编码的工作提供了跨编程语言和软件工程活动评估代理的框架和数据集 [62, 24, 28, 50]。早期关于网页代理的工作开发了通过键盘和鼠标操作与网站交互的平台，以及聚焦开放性网页任务和真实网页导航的数据集 [48, 34, 63, 12, 71, 26, 13]。移动设备交互研究开发了用于移动 UI 交互的仿真器和专注于 InfoUI 任务的平台 [31, 52, 56, 55, 44, 66, 58, 65, 57]。此外，连通真实计算机的环境和用于 GUI 接地的数据集虽然没有交互能力，但已出现 [17, 10, 42, 25, 53]。跨不同方面的全面任务评估也出现了创新 [36, 40]。与以往工作只解决特定领域/应用/数据集相比，OSWORLD 有助于开发可泛化的数字代理，使其能够与操作系统进行开放式交互。见表 4 的对比。


Vision-language models for multi-modal agents Many existing works on GUI interaction utilize structured data such as HTML, accessibility trees, and view hierarchies as a grounding source [12, 19] 31, 41, 69, 51, 67, 71]. However, source code often tends to be verbose, non-intuitive, or inaccessible, which necessitates multi-modal/visual understanding. Prior work on multimodal models consider screenshots for interaction with websites [4, 16, 22, 27, 47] and mobile devices [21, 68]. Additionally, general-purpose foundation models [5, 30, 35, 72, 9] also demonstrate potential for multi-modal digital agents. Prompt-based reasoning methods [17, 20, 60, 70] have further improved digital agents for web pages, mobile apps, and desktops. This work evaluates state-of-the-art text, vision, and multi-modal methods, demonstrating that existing multi-modal models are far from capable computer agents.
将多模态智能体的视觉-语言模型用于多模态任务 现有许多关于 GUI 交互的工作将 HTML、可访问性树和视图层级等结构化数据作为锚定源 [12, 19] 31, 41, 69, 51, 67, 71]。然而，源代码往往冗长、非直观或不可获取，因此需要多模态/视觉理解。此前的多模态模型工作将网站的交互视为截图 [4, 16, 22, 27, 47]，以及移动设备 [21, 68] 的交互。此外，通用基础模型 [5, 30, 35, 72, 9] 也展现了多模态数字代理的潜力。基于提示的方法 [17, 20, 60, 70] 进一步提升了用于网页、移动应用和桌面的数字代理。本研究对最先进的文本、视觉和多模态方法进行了评估，表明现有的多模态模型与真正的计算代理相去甚远。


## 7 Conclusion
## 7 结论


OSWORLD addresses critical gaps in existing interactive learning environments to advance the development of autonomous digital agents. By providing a rich, realistic setting that spans multiple operating systems, interfaces, and applications, OSWORLD broadens the scope of tasks digital agents can perform, and enhances their potential for real-world applications. Despite the promise shown by advancements in vision-language models, evaluations within OSWORLD reveal notable challenges in agents' abilities, particularly in GUI understanding and operational knowledge, pointing to essential areas for future research and development.
OSWORLD 弥补了现有交互式学习环境中的关键空白，推动自主数字代理的发展。通过提供跨越多操作系统、界面与应用的丰富且真实的环境，OSWORLD 扩大了数字代理可执行任务的范围，并提升其在现实世界应用中的潜力。尽管视觉-语言模型的进展备受期待，但在 OSWORLD 的评估中，代理的能力仍面临显著挑战，尤其在 GUI 理解和操作知识方面，指向未来研究与开发的关键领域。


## Acknowledgements
## 致谢


We thank Sida Wang, Peter Shaw, Alane Suhr, Luke Zettlemoyer, Chen Henry Wu, Pengcheng Yin, Shunyu Yao, Xing Han Lu, Siva Reddy, Ruoxi Sun, Zhiyuan Zeng, Chengyou Jia, Haoyuan Wu, Jiaqi Deng, Yuhao Yang and Lei Li for their helpful feedback on this work.
感谢 Sida Wang、Peter Shaw、Alane Suhr、Luke Zettlemoyer、Chen Henry Wu、Pengcheng Yin、Shunyu Yao、Xing Han Lu、Siva Reddy、Ruoxi Sun、Zhiyuan Zeng、Chengyou Jia、Haoyuan Wu、Jiaqi Deng、Yuhao Yang 及 Lei Li 对本工作提出的宝贵意见。


## References
## 参考文献


[1] Adept. ACT-1: Transformer for Actions. https://www.adept.ai/act.2022.
[1] Adept. ACT-1: Transformer for Actions. https://www.adept.ai/act.2022.


[2] Anthropic. Introducing the next generation of claude. https://www.anthropic.com/news/ claude-3-family 2023. Accessed: 2024-03-26.
[2] Anthropic. Introducing the next generation of claude. https://www.anthropic.com/news/ claude-3-family 2023. Accessed: 2024-03-26.


[3] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, 2024.
[3] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, 2024.


[4] Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Cärbune, Jason Lin, Jindong Chen, and Abhanshu Sharma. Screenai: A vision-language model for ui and infographics understanding. arXiv preprint arXiv:2402.04615, 2024.
[4] Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Cärbune, Jason Lin, Jindong Chen, and Abhanshu Sharma. Screenai: A vision-language model for ui and infographics understanding. arXiv preprint arXiv:2402.04615, 2024.


[5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.
[5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.


[6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.
[6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.


[7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choro-manski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.
[7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choro-manski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.


[8] Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, and Baobao Chang. Pca-bench: Evaluating multimodal large language models in perception-cognition-action chain. arXiv preprint arXiv:2402.15527, 2024.
[8] Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, and Baobao Chang. Pca-bench: Evaluating multimodal large language models in perception-cognition-action chain. arXiv preprint arXiv:2402.15527, 2024.


[9] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, 2024. URL https://arxiv.org/abs/2404.16821
[9] 何陈、汪为云、田浩、叶胜龙、郜章伟、崔尔斐、童文文、胡控治、罗家鹏、马正、马 Ji、王佳齐、董晓逸、闫航、郭和炜、何钟辉、石博天、金振江、徐超、王斌、魏兴健、张文贞、张博、蔡品龙、温力承、阎向超、豆敏、卢乐伟、徐周珠、龙同、林达华、乔羽、代济峰、王文海。我们离 GPT-4V 还有多远？用开源套件缩小与商用多模态模型之间的差距，2024。URL https://arxiv.org/abs/2404.16821


[10] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024.
[10] 程康之、孙秋实、朱有刚、徐方至、李炎涛、张简建、吴志勇。Seeclick：利用 GUI 定位实现高级可视化图形用户界面代理。arXiv 预印本 arXiv:2401.10935，2024。


[11] Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping Liu, and Yang Liu. Can vision-language models think from a first-person perspective? arXiv preprint arXiv:2311.15596, 2023.
[11] 程思杰、郭智成、吴静文、方克辰、李鹏、刘华平、刘阳。视觉-语言模型能否以第一人称视角思考？arXiv 预印本 arXiv:2311.15596，2023。


[12] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070, 2023.
[12] 戴翔、顾宇、郑博源、陈实杰、塞缪尔·斯蒂文斯、王博石、孙欢、余宇。Mind2web：迈向网页通用代理。arXiv 预印本 arXiv:2306.06070，2023。


[13] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H Laradji, Manuel Del Verme, Tom Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, et al. Workarena: How capable are web agents at solving common knowledge work tasks? arXiv preprint arXiv:2403.07718, 2024.
[13] 多米尼克·德鲁安、马克西姆·加塞、马西莫·卡西亚、Issam H Laradji、Manuel Del Verme、Tom Marty、Léo Boisvert、Megh Thakkar、Quentin Cappart、David Vazquez 等。Workarena：网络代理在解决常识性工作任务方面有多强？arXiv 预印本 arXiv:2403.07718，2024。


[14] D. Dupont. GPT-4V-Act: GPT-4 Variant for Active Learning. GitHub repository, 2023. URL https://github.com/ddupont808/GPT-4V-Act
[14] D. Dupont。GPT-4V-Act：用于主动学习的 GPT-4 变体。GitHub 仓库，2023。URL https://github.com/ddupont808/GPT-4V-Act


[15] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:18343-18362, 2022.
[15] 樊林玺、王冠志、姜云帆、Ajay Mandlekar、杨云聪、朱浩毅、安德鲁·坦、黄德安、朱毓、Anima Anandkumar。MineDojo：构建具有互联网规模知识的开放式具身代理。神经信息处理系统进展，35:18343-18362，2022。


[16] Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854, 2023.
[16] 庫·布鲁塔、Ofir Nachum、Kuang-Huei Lee、松尾祐、Shane Gu、Izzeddin Gur。带指令微调基础模型的多模态网络导航。arXiv 预印本 arXiv:2305.11854，2023。


[17] Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, et al. Assistgui: Task-oriented desktop graphical user interface automation. arXiv preprint arXiv:2312.13108, 2023.
[17] 高非飞、季蕾、白泽辰、欧阳铭宇、李培议、茂东兴、吴钦辰、郭翔宇、等。AssistGUI：面向任务的桌面图形用户界面自动化。arXiv 预印本 arXiv:2312.13108，2023。


[18] Yiduo Guo, Zekai Zhang, Yaobo Liang, Dongyan Zhao, and Duan Nan. Pptc benchmark: Evaluating large language models for powerpoint task completion. arXiv preprint arXiv:2311.01767, 2023.
[18] 郭一多、张则凯、梁耀博、赵东彦、段南。PPTC 基准：评估大型语言模型的 PowerPoint 任务完成能力。arXiv 预印本 arXiv:2311.01767，2023。


[19] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023.
[19] 伊兹丁·古尔、福鲁塔、奥斯汀·黄、Mustafa Safdari、松尾裕、道格拉斯·艾克、阿莱克斯·福斯特。具备计划、长上下文理解和程序合成的真实世界网页代理。arXiv 预印本 arXiv:2307.12856，2023。


[20] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024.
[20] 何红亮、姚文林、马开欣、俞文浩、戴永、张宏明、蓝振中、余栋。Webvoyager：使用大模态模型构建端到端网页代理。arXiv 预印本 arXiv:2401.13919，2024。


[21] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023.
[21] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: 一个用于 GUI 代理的可视语言模型。 arXiv 预印本 arXiv:2312.08914, 2023.


[22] Peter C Humphreys, David Raposo, Tobias Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Adam Santoro, and Timothy Lillicrap. A data-driven approach for learning to control computers. In International Conference on Machine Learning, pages 9466-9482. PMLR, 2022.
[22] Peter C Humphreys, David Raposo, Tobias Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Adam Santoro, 与 Timothy Lillicrap. 一种数据驱动的学习控制计算机的方法。In International Conference on Machine Learning, 页面 9466-9482。PMLR, 2022.


[23] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.
[23] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 专家混合的 Mixtral。 arXiv 预印本 arXiv:2401.04088, 2024.


[24] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.
[24] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, 与 Karthik Narasimhan. Swe-bench: 语言模型能否解决真实世界的 GitHub 问题？ arXiv 预印本 arXiv:2310.06770, 2023.


[25] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. arXiv preprint arXiv:2402.17553, 2024.
[25] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, 与 Ruslan Salakhutdinov. Omniact: 一个数据集与基准，旨在使多模态通用自主代理用于桌面与网络。 arXiv 预印本 arXiv:2402.17553, 2024.


[26] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024.
[26] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, 与 Daniel Fried. Visualwebarena: 在现实视觉网页任务上评估多模态代理。 arXiv 预印本 arXiv:2401.13649, 2024.


[27] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisensch-los, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In International Conference on Machine Learning, pages 18893-18912. PMLR, 2023.
[27] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisensch-los, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, 与 Kristina Toutanova. Pix2struct: 将屏幕截图解析作为视觉语言理解的预训练。 In International Conference on Machine Learning, 页面 18893-18912。PMLR, 2023.


[28] Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, et al. Devbench: A comprehensive benchmark for software development. arXiv preprint arXiv:2403.08604, 2024.
[28] Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, et al. Devbench: 一个全面的软件开发基准。 arXiv 预印本 arXiv:2403.08604, 2024.


[29] Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, and Zhaoxiang Zhang. Sheetcopilot: Bringing software productivity to the next level through large language models. arXiv preprint arXiv:2305.19308, 2023.
[29] Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, 与 Zhaoxiang Zhang. Sheetcopilot: 通过大型语言模型将软件生产力提升到新水平。 arXiv 预印本 arXiv:2305.19308, 2023.


[30] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. Silkie: Preference distillation for large visual language models. arXiv preprint arXiv:2312.10665, 2023.
[30] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, 与 Lingpeng Kong. Silkie: 面向大型视觉语言模型的偏好蒸馏。 arXiv 预印本 arXiv:2312.10665, 2023.


[31] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile ui action sequences. arXiv preprint arXiv:2005.03776, 2020.
[31] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, 与 Jason Baldridge. 将自然语言指令映射到移动 UI 操作序列。 arXiv 预印本 arXiv:2005.03776, 2020.


[32] J. C. R. Licklider. Man-computer symbiosis. IRE Transactions on Human Factors in Electronics, HFE-1(1):4-11, 1960. doi: 10.1109/THFE2.1960.4503259.
[32] J. C. R. Licklider. 人机共生。IEEE 电子学人因数期刊 IRE Transactions on Human Factors in Electronics, HFE-1(1):4-11, 1960. doi: 10.1109/THFE2.1960.4503259.


[33] Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D Ernst. N12bash: A corpus and semantic parser for natural language interface to the linux operating system. arXiv preprint arXiv:1802.08979, 2018.
[33] Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, 与 Michael D Ernst. N12bash：一个自然语言界面对 Linux 操作系统的语料与语义解析器。 arXiv 预印本 arXiv:1802.08979, 2018.


[34] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. arXiv preprint arXiv: 1802.08802, 2018.
[34] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. 使用工作流引导探索的网页界面强化学习。 arXiv 预印本 arXiv: 1802.08802, 2018.


[35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.
[35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 可视化指令微调。 arXiv 预印本 arXiv:2304.08485, 2023.


[36] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023.
[36] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, 等. Agentbench：将 llms 作为代理进行评估。 arXiv 预印本 arXiv:2308.03688, 2023.


[37] Xing Han Lù, Zdeněk Kasner, and Siva Reddy. Weblinx: Real-world website navigation with multi-turn dialogue. arXiv preprint arXiv:2402.05930, 2024.
[37] Xing Han Lù, Zdeněk Kasner, and Siva Reddy. Weblinx：带多轮对话的现实世界网站导航。 arXiv 预印本 arXiv:2402.05930, 2024.


[38] Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. Agentboard: An analytical evaluation board of multi-turn llm agents. arXiv preprint arXiv:2401.13178, 2024.
[38] Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. Agentboard：多轮 llm 代理的分析评估板。 arXiv 预印本 arXiv:2401.13178, 2024.


[39] Meta AI. Introducing meta Llama 3: The most capable openly available LLM to date, April 2024. URL https://ai.meta.com/blog/meta-llama-3/ Accessed: 2024-04-18.
[39] Meta AI. 引入 meta Llama 3：迄今为止最强公开可用的大语言模型，2024 年 4 月。URL https://ai.meta.com/blog/meta-llama-3/ 访问日期：2024-04-18。


[40] Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants. arXiv preprint arXiv:2311.12983, 2023.
[40] Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia：一个通用 AI 助手基准。 arXiv 预印本 arXiv:2311.12983, 2023.


[41] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
[41] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, 等. Webgpt：带人类反馈的浏览器辅助问答。 arXiv 预印本 arXiv:2112.09332, 2021.


[42] Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan Leng, He Kong, Yi Chang, and Qi Wang. Screenagent: A vision language model-driven computer control agent. arXiv preprint arXiv:2402.07945, 2024.
[42] Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan Leng, He Kong, Yi Chang, and Qi Wang. Screenagent：一个由视觉语言模型驱动的计算机控制代理。 arXiv 预印本 arXiv:2402.07945, 2024.


[43] R OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2:13, 2023.
[43] R OpenAI. Gpt-4 技术报告。 arxiv 2303.08774。发表于：2:13，2023。


[44] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: A large-scale dataset for android device control. arXiv preprint arXiv:2307.10088, 2023.
[44] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild：一个用于安卓设备控制的大规模数据集。 arXiv 预印本 arXiv:2307.10088, 2023.


[45] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.
[45] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, 等. Gemini 1.5：在数百万上下文标记中解锁多模态理解。 arXiv 预印本 arXiv:2403.05530, 2024.


[46] Andrew Searles, Yoshimichi Nakatsuka, Ercan Ozturk, Andrew Paverd, Gene Tsudik, and Ai Enkoji. An empirical study & evaluation of modern \{CAPTCHAs\}. In 32nd USENIX Security Symposium (USENIX Security 23), pages 3081-3097, 2023.
[46] Andrew Searles, Yoshimichi Nakatsuka, Ercan Ozturk, Andrew Paverd, Gene Tsudik, and Ai Enkoji. 一项关于现代 CAPTCHA 的实证研究与评估。在 32 届 USENIX 安全会议（USENIX Security 23）中，页面 3081-3097，2023。


[47] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow instructions via graphical user interfaces. arXiv preprint arXiv:2306.00245, 2023.
[47] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. 从像素到 UI 动作：通过图形用户界面学习遵循指令。 arXiv 预印本 arXiv:2306.00245, 2023.


[48] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pages 3135-3144. PMLR, 2017.
[48] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. Bits 的世界：一个用于基于网络的代理的开放域平台。在国际机器学习大会上，页面 3135-3144。 PMLR，2017。


[49] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mot-taghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10740-10749, 2020.
[49] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mot-taghi, Luke Zettlemoyer, and Dieter Fox. Alfred: 一个用于解释日常任务的有 grounding 指令的基准。IEEE/CVF 计算机视觉与模式识别会议论文集，页码 10740-10749，2020。


[50] Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2code: How far are we from automating front-end engineering?, 2024.
[50] Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2code：我们距离实现前端工程自动化还差多远？，2024。


[51] Abishek Sridhar, Robert Lo, Frank F Xu, Hao Zhu, and Shuyan Zhou. Hierarchical prompting assists large language model on web navigation. arXiv preprint arXiv:2305.14257, 2023.
[51] Abishek Sridhar, Robert Lo, Frank F Xu, Hao Zhu, and Shuyan Zhou. 层级提示帮助大型语言模型完成网页导航。arXiv 预印本 arXiv:2305.14257，2023。


[52] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. Meta-gui: Towards multi-modal conversational agents on mobile gui. arXiv preprint arXiv:2205.11029, 2022.
[52] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. Meta-gui：面向移动端 GUI 的多模态对话代理。arXiv 预印本 arXiv:2205.11029，2022。


[53] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards general computer control: A multimodal agent for red dead redemption ii as a case study. arXiv preprint arXiv:2403.03186, 2024.
[53] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, 等。Towards general computer control: 以 Red Dead Redemption II 为案例研究的多模态代理。arXiv 预印本 arXiv:2403.03186，2024。


[54] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
[54] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, 等。Gemini：一系列高能力多模态模型。arXiv 预印本 arXiv:2312.11805，2023。


[55] Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: A reinforcement learning platform for android. arXiv preprint arXiv:2105.13231, 2021.
[55] Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv：一个用于安卓的强化学习平台。arXiv 预印本 arXiv:2105.13231，2021。


[56] Sagar Gubbi Venkatesh, Partha Talukdar, and Srini Narayanan. Ugif: Ui grounded instruction following. arXiv preprint arXiv:2211.07615, 2022.
[56] Sagar Gubbi Venkatesh, Partha Talukdar, and Srini Narayanan. Ugif：Ui 基于指令的跟随。arXiv 预印本 arXiv:2211.07615，2022。


[57] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024.
[57] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent：具有视觉感知的自主多模态移动设备代理。arXiv 预印本 arXiv:2401.16158，2024。


[58] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Empowering llm to use smartphone for intelligent task automation. arXiv preprint arXiv:2308.15272, 2023.
[58] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. 赋能大型语言模型使用智能手机实现智能任务自动化。arXiv 预印本 arXiv:2308.15272，2023。


[59] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, and Tao Yu. Openagents: An open platform for language agents in the wild. CoRR, abs/2310.10634, 2023. doi: 10.48550/ARXIV.2310.10634. URL https: //doi.org/10.48550/arXiv.2310.10634
[59] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, and Tao Yu. Openagents：野外语言代理的开放平台。CoRR，abs/2310.10634，2023。doi: 10.48550/ARXIV.2310.10634。URL https: //doi.org/10.48550/arXiv.2310.10634


[60] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023.
[60] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, 等。Wonderland 中的 GPT-4v：用于零样本智能手机 GUI 导航的大型多模态模型。arXiv 预印本 arXiv:2311.07562，2023。


[61] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023.
[61] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark 提示在 GPT-4v 中释放非凡的视觉定位能力。arXiv 预印本 arXiv:2310.11441，2023。


[62] John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. arXiv preprint arXiv:2306.14898, 2023.
[62] John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: 标准化并以执行反馈进行基准测试的交互式编码。arXiv 预印本 arXiv:2306.14898, 2023.


[63] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744-20757, 2022.
[63] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: 面向带有基础语言代理的可扩展现实世界网页交互。Advances in Neural Information Processing Systems, 35:20744-20757, 2022.


[64] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: A ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024.
[64] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, 等. Ufo: 一个面向界面的代理用于 windows 操作系统交互。arXiv 预印本 arXiv:2402.07939, 2024.


[65] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv e-prints, pages arXiv-2312, 2023.
[65] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: 将多模态代理作为智能手机用户。arXiv 电子稿，页面 arXiv-2312, 2023.


[66] Danyang Zhang, Lu Chen, and Kai Yu. Mobile-env: A universal platform for training and evaluation of mobile interaction. arXiv preprint arXiv:2305.08144, 2023.
[66] Danyang Zhang, Lu Chen, and Kai Yu. Mobile-env: 用于移动交互训练与评估的通用平台。arXiv 预印本 arXiv:2305.08144, 2023.


[67] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu. Large language models are semi-parametric reinforcement learning agents. Advances in Neural Information Processing Systems, 36, 2024.
[67] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu. 大语言模型是半参数化强化学习代理。Advances in Neural Information Processing Systems, 36, 2024.


[68] Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. arXiv e-prints, pages arXiv-2309, 2023.
[68] Zhuosheng Zhang and Aston Zhang. 你只看屏幕：多模态行动链代理。arXiv 电子稿，页面 arXiv-2309, 2023.


[69] Zihan Zhao, Lu Chen, Ruisheng Cao, Hongshen Xu, Xingyu Chen, and Kai Yu. Tie: Topological information enhanced structural reading comprehension on web pages. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1808-1821, 2022.
[69] Zihan Zhao, Lu Chen, Ruisheng Cao, Hongshen Xu, Xingyu Chen, and Kai Yu. Tie: 顶点信息增强的网页结构化阅读理解。收录于 Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 页码 1808-1821, 2022.


[70] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is a generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024.
[70] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (vision) 是一个在有 grounded 情况下的通用网页代理。arXiv 预印本 arXiv:2401.01614, 2024.


[71] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023.
[71] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, 等. Webarena: 构建自治代理的现实网页环境。arXiv 预印本 arXiv:2307.13854, 2023.


[72] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.
[72] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: 通过高级大语言模型提升视觉-语言理解。arXiv 预印本 arXiv:2304.10592, 2023.


## A Details of OSWORLD Environment
## OSWORLD 环境的细节


### A.1 Environment Infrastructure
### A.1 环境基础设施


As compared to core commonly used techniques like Docker T virtual machines can operate their own kernel and system, enabling compatibility with a wide variety of operating systems (such as Windows, macOS, Linux, etc.) across different CPU hardware types (x64, ARM, etc.), and supports training and evaluation in a multiprocess manner on both headless servers and personal computers.
与常用的核心技术如 Docker、虚拟机相比，后者可以自行操作内核与系统，确保跨多种操作系统（如 Windows、macOS、Linux 等）及不同 CPU 硬件类型（x64、ARM 等）兼容，并支持在无头服务器和个人电脑上以多进程方式进行训练与评估。


### A.2 Observation Space
### A.2 观测空间


We implement three kinds of observation: complete screenshot, accessibility tree and terminal output. We also implement a video recorder of the environment but don't put it into our modeling due to the agent's ability limitations. OSWORLD supports observation refactoring and extending if needed, such as getting data from certain opening applications that we want to focus on.
我们实现了三种观测：完整截图、可访问性树和终端输出。我们还实现了环境的视频记录器，但由于代理的能力受限未将其纳入建模。OSWORLD 支持在需要时进行观测重构和扩展，例如获取某些打开应用的数据。


#### A.2.1 Screenshot
#### A.2.1 屏幕截图


To align with the perception of a human user, we capture a screenshot of the entire computer screen. Including the mouse cursor also proves helpful in certain cases where mouse information is crucial. For screen resolution,we default to ${1920} \times  {1080}$ ,as it is the most commonly used screen resolution according to Internet Users Screen Resolution Realtime Statistics for ${2023}^{2}$ This resolution also offers a 16:9 aspect ratio. OSWORLD also supports modifying the resolution of virtual machines to avoid potential memorization of absolute pixel values and to assist studies on topics like generalization across different resolutions.
为符合人类用户的感知，我们截取整个计算机屏幕的截图。在鼠标信息至关重要的某些情况下，包含鼠标光标也很有帮助。对于屏幕分辨率，我们默认使用 ${1920} \times  {1080}$，因为根据 ${2023}^{2}$ 的互联网用户屏幕分辨率实时统计数据，这是最常用的屏幕分辨率。此分辨率还提供 16:9 的宽高比。OSWORLD 还支持修改虚拟机的分辨率，以避免潜在的绝对像素值记忆，并辅助进行跨不同分辨率的泛化等主题的研究。


#### A.2.2 Accessibility Tree
#### A.2.2 无障碍树


An accessibility tree (or a 11y tree, same logic to kubernetes and k8s), refers to an intricate structure generated by the browser or OS accessibility APIs that renders a representative model of the web content, providing a means of interaction for assistive technologies. Each node within the accessibility tree hosts important information about a UI element. This could range from the nature of the object (a button, checkbox, or paragraph of text), its current state (checked or unchecked, for checkboxes), and even its spatial orientation on the screen.
无障碍树（或 11y 树，与 Kubernetes 和 k8s 的逻辑相同）指的是由浏览器或操作系统无障碍 API 生成的复杂结构，它呈现网页内容的代表性模型，为辅助技术提供交互方式。无障碍树中的每个节点都包含有关 UI 元素的重要信息。这可能包括对象的类型（按钮、复选框或一段文本）、其当前状态（对于复选框，为已选中或未选中），甚至其在屏幕上的空间方向。


Different operating systems employ varied accessibility APIs and tools to construct and manipulate the accessibility tree. These include Microsoft Active Accessibility (MSAA) and User Interface Automation (UIA) for Windows, NSAccessibility Protocol and macOS Accessibility Inspector for macOS, and Assistive Technology Service Provider Interface (ATSPI) ${}^{3}$ for GNOME dekstop used on Ubuntu. We adopt pyatspi to get the accessibility tree on Ubuntu and pywinauto on Windows. We further convert it into XML format for message passing. Partial pieces of the XML formatted accessibility tree are shown in Figure 5
不同的操作系统使用不同的无障碍 API 和工具来构建和操作无障碍树。这些包括 Windows 的 Microsoft Active Accessibility (MSAA) 和用户界面自动化 (UIA)、macOS 的 NSAccessibility 协议和 macOS 无障碍检查器，以及用于 Ubuntu 上 GNOME 桌面的辅助技术服务提供商接口 (ATSPI) ${}^{3}$。我们采用 pyatspi 在 Ubuntu 上获取无障碍树，采用 pywinauto 在 Windows 上获取。我们进一步将其转换为 XML 格式以进行消息传递。XML 格式的无障碍树的部分片段如图 5 所示。


Tools such as Accerciser can help visualize the corresponding relationship of tree nodes and GUI components in the accessibility tree as shown in Figure 6
如图 6 所示，Accerciser 等工具可以帮助可视化无障碍树中树节点与 GUI 组件的对应关系。


### A.3 Action Space
### A.3 动作空间


We implement two kinds of action space: pyautogui and computer_13. We mainly use pyautogui action space, since it saves tokens for describing action space definition in prompting, compared with self-designed actions.
我们实现了两种动作空间：pyautogui 和 computer_13。我们主要使用 pyautogui 动作空间，因为与自行设计的动作相比，它在提示中描述动作空间定义时节省了令牌。


#### A.3.1 PYAUTOGUI
#### A.3.1 PYAUTOGUI


pyautogui is an open-source, cross-platform Python module utilized for programmatically controlling the mouse and keyboard. It can control simple movements, clicks, as well as keyboard inputs, and can provide screen capture features or locate where the screen-specific images reside that can be useful for GUI automation. Compared with other Python packages such as xdotool 4 mouse 5 and keyboard ${}^{6}$ ] pyautogui has better compatibility across different OSes as found in our early attempts, so that we adapt that as our core component of computer controlling as well as an official valid action space.
pyautogui 是一个开源的跨平台 Python 模块，用于以编程方式控制鼠标和键盘。它可以控制简单的移动、点击以及键盘输入，还可以提供屏幕截图功能或定位特定屏幕图像的位置，这对 GUI 自动化很有用。与其他 Python 包（如 xdotool 4 mouse 5 和 keyboard ${}^{6}$）相比，我们早期的尝试发现 pyautogui 在不同操作系统上具有更好的兼容性，因此我们将其用作计算机控制的核心组件以及官方有效的动作空间。


---



https://www.docker.com/



https://www.screenresolution.org/year-2023/



https://docs.gtk.org/atspi2/



---



<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_dd84f2.jpg"/>



Figure 5: Raw XML formatted accessibility tree visualization.
图 5：原始 XML 格式的无障碍树可视化。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_a1ff8e.jpg"/>



Figure 6: Use Accerciser to visualize components in accessibility tree. This example shows the corresponding relationship between the "Print" button (which is a printer icon on the bar of the LibreOffice app as shown on the left) and the "Print" tree node (which is the "Print" of the Accerciser Accessibility Explorer as shown on the right).
图 6：使用 Accerciser 可视化无障碍树中的组件。此示例显示了“打印”按钮（如左侧所示，是 LibreOffice 应用程序栏上的打印机图标）与“打印”树节点（如右侧所示，是 Accerciser 无障碍资源管理器中的“打印”）之间的对应关系。


Herein, we will demonstrate some use cases of pyautogui and illustrate how it can be wielded as an action space.
在此，我们将演示 pyautogui 的一些用例，并说明如何将其用作动作空间。


## Mouse Controlling Functions
## 鼠标控制函数


import pyautogui
导入 pyautogui


#move mouse to XY coordinates over num_second seconds
# 在 num_second 秒内将鼠标移动到 XY 坐标


pyautogui.moveTo(x, y, duration=num_seconds)
pyautogui.moveTo(x, y, duration=num_seconds)


#drag mouse to XY
#拖动鼠标到 XY


pyautogui.dragTo(x, y, duration=num_seconds)
pyautogui.dragTo(x, y, duration=num_seconds)


---



https://github.com/jordansissel/xdotool



https://github.com/boppreh/mouse



https://github.com/boppreh/keyboard



---



pyautogui.click(x=moveToX, y=moveToY, clicks=num_of_clicks, button='left')
pyautogui.click(x=moveToX, y=moveToY, clicks=num_of_clicks, button='left')


pyautogui.rightClick(x=moveToX, y=moveToY)
pyautogui.rightClick(x=moveToX, y=moveToY)


pyautogui.middleClick(x=moveToX, y=moveToY)
pyautogui.middleClick(x=moveToX, y=moveToY)


pyautogui.doubleClick(x=moveToX, y=moveToY)
pyautogui.doubleClick(x=moveToX, y=moveToY)


pyautogui.tripleClick(x=moveToX, y=moveToY)
pyautogui.tripleClick(x=moveToX, y=moveToY)


pyautogui.scroll(amount_to_scroll, x=moveToX, y=moveToY)
pyautogui.scroll(amount_to_scroll, x=moveToX, y=moveToY)


pyautogui.mouseDown(x=moveToX, y=moveToY, button='left')
pyautogui.mouseDown(x=moveToX, y=moveToY, button='left')


pyautogui.mouseUp(x=moveToX, y=moveToY, button='left')
pyautogui.mouseUp(x=moveToX, y=moveToY, button='left')


## Keyboard Controlling Functions
## 键盘控制函数


import pyautogui
import pyautogui


#useful for entering text, newline is Enter
#用于输入文本，换行是 Enter


pyautogui.typewrite('Hello world!\\n', interval=secs_between_keys)
pyautogui.typewrite('Hello world!\\n', interval=secs_between_keys)


pyautogui.typewrite(['a', 'b', 'c', 'left', 'backspace', 'enter', 'f1'], interval=secs_between_keys)
pyautogui.typewrite(['a', 'b', 'c', 'left', 'backspace', 'enter', 'f1'], interval=secs_between_keys)


pyautogui.hotkey('ctrl', 'c') # ctrl-c to copy
pyautogui.hotkey('ctrl', 'c') # 按下 ctrl-c 进行复制


pyautogui.hotkey('ctrl', 'v') # ctrl-v to paste
pyautogui.hotkey('ctrl', 'v') # 按下 ctrl-v 进行粘贴


pyautogui.keyDown(key_name)
pyautogui.keyDown(key_name)


pyautogui.keyUp(key_name)
pyautogui.keyUp(key_name)


pyautogui as an Action Space Given the various controls it provides, pyautogui can readily be used as an action space in building automation software or testing interfaces with minor adjustments. More formally, an action is within the action space when it meets the syntax of pyautogui or is one of three special actions WAIT, FAIL, and DONE. This might include actions like clicking at a certain location, entering text or key sequences, or even resting for a span (Pause). Each action could be mapped to an identifying label or number, forming a discrete action space. For example:
将 pyautogui 作为动作空间 鉴于 pyautogui 提供的各种控制功能，只需进行微小调整，它就可以很方便地在构建自动化软件或测试界面时用作动作空间。更正式地说，当一个动作符合 pyautogui 的语法，或者是三种特殊动作（WAIT、FAIL 和 DONE）之一时，该动作就属于这个动作空间。这可能包括在特定位置点击、输入文本或按键序列，甚至暂停一段时间（Pause）等动作。每个动作都可以映射到一个标识标签或数字，从而形成一个离散的动作空间。例如：


import pyautogui
import pyautogui


def perform_action(action):
def perform_action(action):


if action == 0:
if action == 0:


pyautogui.moveTo(100, 100)
pyautogui.moveTo(100, 100)


elif action == 1:
elif action == 1:


pyautogui.write('Hello world!', interval=0.25)
pyautogui.write('Hello world!', interval=0.25)


else:
else:


pyautogui.pause(1)
pyautogui.pause(1)


In this scheme, the "perform_action" function constitutes the action space, where each unique action is associated with a unique integer (its action ID). The function interprets these action IDs and performs the corresponding action, forming a rudimentary discrete action space.
在这个方案中，“perform_action”函数构成了动作空间，其中每个独特的动作都与一个独特的整数（其动作 ID）相关联。该函数会解读这些动作 ID 并执行相应的动作，从而形成一个基本的离散动作空间。


One interesting finding is that language models generate screenshot locate functions like:
一个有趣的发现是，语言模型会生成如下的屏幕截图定位函数：


pyautogui.locateOnScreen('Apple.png')
pyautogui.locateOnScreen('Apple.png')


When there is insufficient grounding evidence (such as when no screenshot is inputted, the accessibility tree lacks a specific element, or the multimodal model cannot comprehend the user interface), employing this function to retrieve the correct icon image could present an interesting method.
当缺乏足够的基础证据时（例如未输入截图、可访问性树缺少特定元素或多模态模型无法理解用户界面），使用此功能检索正确的图标图像可能是一种有趣的方法。


#### A.3.2 COMPUTER_13
#### A.3.2 COMPUTER_13


To facilitate potential reinforcement learning applications, we have created a variant of pyautogui, which we named computer_13. In this variant, we wrap pyautogui into a finite action class with parameterized enumeration, such that it features 13 action types, excluding three special ones for task process control. Utilizing this structured approach allows more effective reinforcement learning by providing a distinct and finite set of actions to be learned and optimized. As summarized in Table 6 each action type has certain parameters, detailed in the collection, confirming the type, range, and whether each parameter is optional for that action.
为了便于潜在的强化学习应用，我们创建了 pyautogui 的一个变体，名为 computer_13。在这个变体中，我们将 pyautogui 封装到一个具有参数化枚举的有限动作类中，使其具有 13 种动作类型，其中不包括用于任务流程控制的三种特殊动作类型。采用这种结构化方法，通过提供一组明确且有限的待学习和优化的动作，能够实现更有效的强化学习。如表 6 总结，每种动作类型都有特定的参数，这些参数的详细信息收录在集合中，明确了每种参数的类型、范围以及是否为该动作的可选参数。


Table 6: Action types and parameters defined in action space computer_13, a variance we created for the potential reinforcement learning research based on our environment.
表 6：动作空间 computer_13 中定义的动作类型和参数，这是我们基于自身环境为潜在的强化学习研究创建的一个变体。


<table><tr><td>Action Type</td><td>Parameters</td><td>Note</td></tr><tr><td>MOVE_TO</td><td>$x,y$</td><td>Move the cursor to the specified position</td></tr><tr><td>CLICK</td><td>button, <br> $x,y$ , <br> num_clicks</td><td>Click the left button if the button not specified, otherwise click the specified button; click at the current position if $x$ and $y$ are not specified, otherwise click at the specified position</td></tr><tr><td>MOUSE_DOWN</td><td>button</td><td>Press the left button if the button not specified, otherwise press the specified button</td></tr><tr><td>MOUSE_UP</td><td>button</td><td>Release the left button if the button not specified, otherwise release the specified button</td></tr><tr><td>RIGHT_CLICK</td><td>$x,y$</td><td>Right click at the current position if $x$ and $y$ are not specified, otherwise right click at the specified position</td></tr><tr><td>DOUBLE_CLICK</td><td>$x,y$</td><td>Double click at the current position if $x$ and $y$ are not specified, otherwise double click at the specified position</td></tr><tr><td>DRAG_TO</td><td>$x,y$</td><td>Drag the cursor to the specified position with the left button pressed</td></tr><tr><td>SCROLL</td><td>${dx},{dy}$</td><td>Scroll the mouse wheel up or down</td></tr><tr><td>TYPING</td><td>text</td><td>Type the specified text</td></tr><tr><td>PRESS</td><td>key</td><td>Press the specified key and release it</td></tr><tr><td>KEY_DOWN</td><td>key</td><td>Press the specified key</td></tr><tr><td>KEY_UP</td><td>key</td><td>Release the specified key</td></tr><tr><td>HOTKEY</td><td>keys</td><td>Press the specified key combination</td></tr><tr><td>WAIT</td><td>-</td><td>Wait until the next action</td></tr><tr><td>FAIL</td><td>-</td><td>Decide the task cannot be performed</td></tr><tr><td>DONE</td><td>-</td><td>Decide the task is done</td></tr></table>
<table><tbody><tr><td>操作类型</td><td>参数</td><td>备注</td></tr><tr><td>MOVE_TO</td><td>$x,y$</td><td>将光标移动到指定位置</td></tr><tr><td>CLICK</td><td>按钮，<br/> $x,y$，<br/> 点击次数</td><td>如果未指定按钮则单击左键；否则单击指定按钮；如果 $x$ 与 $y$ 未指定，则在当前位置单击，否则在指定位置单击</td></tr><tr><td>MOUSE_DOWN</td><td>按钮</td><td>如果未指定按钮则按下左键；否则按下指定按钮</td></tr><tr><td>MOUSE_UP</td><td>按钮</td><td>如果未指定按钮则释放左键；否则释放指定按钮</td></tr><tr><td>RIGHT_CLICK</td><td>$x,y$</td><td>如果 $x$ 与 $y$ 未指定，则在当前位置右击；否则在指定位置右击</td></tr><tr><td>DOUBLE_CLICK</td><td>$x,y$</td><td>如果 $x$ 与 $y$ 未指定，则在当前位置双击；否则在指定位置双击</td></tr><tr><td>DRAG_TO</td><td>$x,y$</td><td>在左键按下的状态下将光标拖到指定位置</td></tr><tr><td>SCROLL</td><td>${dx},{dy}$</td><td>向上或向下滚动鼠标滚轮</td></tr><tr><td>TYPING</td><td>文本</td><td>输入指定文本</td></tr><tr><td>PRESS</td><td>按键</td><td>按下指定按键并松开</td></tr><tr><td>KEY_DOWN</td><td>按键</td><td>按下指定按键</td></tr><tr><td>KEY_UP</td><td>按键</td><td>释放指定按键</td></tr><tr><td>HOTKEY</td><td>组合键</td><td>按下指定按键组合</td></tr><tr><td>WAIT</td><td>-</td><td>等待下一个动作</td></tr><tr><td>FAIL</td><td>-</td><td>判定任务无法执行</td></tr><tr><td>DONE</td><td>-</td><td>判定任务已完成</td></tr></tbody></table>


## B Details of OSWORLD Benchmark
## B OSWORLD 基准的详细信息


### B.1 Operating System Selection
### B.1 操作系统选择


As of 2023, the most popular desktop operating systems are Windows (69.5%), macOS (20.4%), ChromeOS (3.2%), and Linux (3.1%) T while Windows and macOS dominate the market share and boast the richest software ecosystems, their closed-source nature raises potential copyright concerns for direct usage. ChromeOS, being a web-based operating system, heavily depends on a Google account for its functionalities, rendering it less suitable for a public benchmark.
截至 2023 年，最流行的桌面操作系统是 Windows（69.5%）、macOS（20.4%）、ChromeOS（3.2%）和 Linux（3.1%）T 尽管 Windows 和 macOS 主导市场份额并拥有最丰富的软件生态系统，其闭源本质对直接使用可能带来版权问题。ChromeOS 作为基于网络的操作系统，极度依赖 Google 账户来实现功能，因此不太适合作为公开基准的选择。


In contrast, Linux desktops offer a wealth of open-source software for most daily tasks, supported by an active community for both basic and advanced use. Essential applications such as Office Suite, browsers, email clients, multimedia apps, and thousands of games and applications are either pre-installed or readily available through the software center of the distribution. Consequently, we select Ubuntu, the most representative Linux desktop OS, as the foundation for the main part of our benchmark intended for public use. Additionally, we have developed components to facilitate agent interaction on a Windows virtual machine and have created a relatively small set of examples focusing on the Microsoft Office suite, including Excel, PowerPoint, and Word. This serves as a counterpart to the LibreOffice suite available on Ubuntu. These components can be utilized in-house or officially with the purchase of a license. Regarding macOS, theoretically, it is illegal to install macOS on non-Apple devices, which leads us to refrain from developing our benchmark on this platform to avoid copyright issues.
相比之下，Linux 桌面提供丰富的开源软件，覆盖日常大多数任务，并由活跃社区支持基础与高级用法。诸如办公套件、浏览器、电子邮件客户端、多媒体应用以及成千上万的游戏与应用要么内置，要么可通过发行版的软件中心获取。因此，我们选择 Ubuntu 这一最具代表性的 Linux 桌面操作系统，作为面向公开使用的基准主部分的基础。此外，我们还开发了在 Windows 虚拟机上促进代理交互的组件，并创建了一个相对较小的示例集合，聚焦于 Microsoft Office 套件（包括 Excel、PowerPoint 和 Word），以作为 Ubuntu 上可用的 LibreOffice 套件的对照。这些组件可在内部使用，或在购买许可后正式使用。关于 macOS，理论上在非 Apple 设备上安装 macOS 属非法行为，因此我们避免在该平台上开发基准，以避免版权问题。


### B.2 Software Selection
### B.2 软件选择


Due to the high cost of obtaining operation and evaluation script annotation data, we have chosen a representative set of software for the examples of Ubuntu part. We adopt standards that consider: 1) Availability - the software must be available on Ubuntu 22.04; 2) Open-source - the software should be open-sourced with an appropriate license to prevent copyright issues; 3) Popularity - the software should take a high download number and frequency of recommendations in blogs and tutorials; 4) Strong user community and good support resources - it is preferable to have an active and robust user community as well as official documents, which can serve as ample resources for task collection and agent learning; 5) Diversity of categories - the software should be diverse to adequately represent and cover a wide range of real-world cases.
由于获得操作与评估脚本注释数据成本高昂，我们为 Ubuntu 部分的示例选取了具有代表性的软件集。我们采用的标准考虑：1) 可用性——软件必须在 Ubuntu 22.04 上可用；2) 开源性——软件应开源且有合适的许可证，以避免版权问题；3) 流行度——软件在下载量和博客、教程中的推荐频次应较高；4) 强大的用户社区与良好的支持资源——最好有活跃且健壮的用户社区以及官方文档，便于任务收集与代理学习的资源；5) 类别多样性——软件应具多样性，能够充分代表并覆盖真实世界的各种场景。


---



https://www.statista.com/statistics/218089/global-market-share-of-windows-7/



---



As a result, we have shortlisted the software into two categories: general usage and professional usage. For general usage, we have VLC for media playback, Chrome for web browsing, and Thunderbird for email management. For professional usage, we have VS Code as a coding IDE, and LibreOffice (Calc, Writer, and Impress) for handling spreadsheets, documents, and presentations respectively, along with GIMP for image editing. This brings our total to eight different types of software.
因此，我们将软件分成两类：通用途与专业用途。通用途方面，我们有用于媒体播放的 VLC、用于网页浏览的 Chrome，以及用于邮件管理的 Thunderbird。专业用途方面，我们有作为编码 IDE 的 VS Code，以及用于处理电子表格、文档和演示文稿的 LibreOffice（Calc、Writer、Impress），再加上用于图像编辑的 GIMP。总共形成八种不同类型的软件。


### B.3 Task Example Sources
### B.3 任务示例来源


We detail the task example sources in Table 7
我们在表 7 中详细列出任务示例来源


Table 7: Task Example Resources
表 7：任务示例资源


<table><tr><td>App</td><td>Resources</td><td>Link</td></tr><tr><td rowspan="5">OS</td><td>Ubuntu Documentations</td><td>https://help.ubuntu.com/</td></tr><tr><td>Ask Ubuntu</td><td>https://askubuntu.com/</td></tr><tr><td>Super User</td><td>https://superuser.com/</td></tr><tr><td>Stack Overflow</td><td>https://stackoverflow.com</td></tr><tr><td>YouTube</td><td>https://www.youtube.com/</td></tr><tr><td rowspan="10">Calc</td><td>LibreOffice Help</td><td>https://help.libreoffice.org/</td></tr><tr><td>Microsoft Tech Community</td><td>https://techcommunity.microsoft.com/</td></tr><tr><td>libreofficehelp.com</td><td>https://www.libreofficehelp.com/</td></tr><tr><td>Reddit r/LibreOfficeCal</td><td>https://www.reddit.com/r/LibreOfficeCalc/</td></tr><tr><td>Reddit r/Excel</td><td>https://www.reddit.com/r/Excel/</td></tr><tr><td>Super User</td><td>https://superuser.com/</td></tr><tr><td>Medium</td><td>https://medium.com/</td></tr><tr><td>Quora</td><td>https://www.quora.com/</td></tr><tr><td>YouTube</td><td>https://www.youtube.com/</td></tr><tr><td>Ryan and Debi & Toren Personal Site</td><td>https://www.ryananddebi.com/</td></tr><tr><td rowspan="9">Writer</td><td>LibreOffice Help</td><td>https://help.libreoffice.org/</td></tr><tr><td>LibreOffice Forum</td><td>https://ask.libreoffice.org/</td></tr><tr><td>libreofficehelp.com</td><td>https://www.libreofficehelp.com/</td></tr><tr><td>Super User</td><td>https://superuser.com/</td></tr><tr><td>Stack Overflow</td><td>https://stackoverflow.com</td></tr><tr><td>Ask Ubuntu</td><td>https://askubuntu.com/</td></tr><tr><td>Quora</td><td>https://www.quora.com/</td></tr><tr><td>YouTube</td><td>https://www.youtube.com/</td></tr><tr><td>SearchStar Personal Site</td><td>https://seekstar.github.io/</td></tr><tr><td rowspan="8">Impress</td><td>LibreOffice Help</td><td>https://help.libreoffice.org/</td></tr><tr><td>libreofficehelp.com</td><td>https://www.libreofficehelp.com/</td></tr><tr><td>Reddit r/LibreOffice</td><td>https://www.reddit.com/r/LibreOffice/</td></tr><tr><td>Super User</td><td>https://superuser.com/</td></tr><tr><td>Stack Overflow</td><td>https://stackoverflow.com</td></tr><tr><td>Technical Tips</td><td>https://technical-tips.com/</td></tr><tr><td>Just Click Here</td><td>https://justclickhere.co.uk/</td></tr><tr><td>TikTok</td><td>https://www.tiktok.com/</td></tr><tr><td rowspan="9">VLC</td><td>VLC Documentation</td><td>https://docs.videolan.me</td></tr><tr><td>VLCHelp.com</td><td>https://www.vlchelp.com/</td></tr><tr><td>VideoLAN's Wiki</td><td>https://wiki.videolan.org/</td></tr><tr><td>Ubuntu Documentations</td><td>https://help.ubuntu.com/</td></tr><tr><td>Reddit r/Fedora</td><td>https://www.reddit.com/r/Fedora/</td></tr><tr><td>Super User</td><td>https://superuser.com/</td></tr><tr><td>Medium</td><td>https://medium.com/</td></tr><tr><td>YouTube</td><td>https://www.youtube.com/</td></tr><tr><td>Dedoimedo</td><td>https://www.dedoimedo.com/index.html</td></tr><tr><td rowspan="8">Thunderbird</td><td>Thunderbird Support</td><td>https://support.mozilla.org/en-US/products/thunderbird</td></tr><tr><td>Reddit r/Thunderbird</td><td>https://www.reddit.com/r/Thunderbird/</td></tr><tr><td>Reddit r/Automation</td><td>https://www.reddit.com/r/automation/</td></tr><tr><td>Super User</td><td>https://superuser.com/</td></tr><tr><td>WikiHow</td><td>https://www.wikihow.com/</td></tr><tr><td>Quora</td><td>https://www.quora.com/</td></tr><tr><td>BitRecover</td><td>https://www.bitrecover.com/</td></tr><tr><td>AdSigner</td><td>https://www.adsigner.com/</td></tr><tr><td>Chrome</td><td>Google Chrome Help <br> Continued on next page</td><td>https://support.google.com/chrome</td></tr></table>
<table><tbody><tr><td>应用程序</td><td>资源</td><td>链接</td></tr><tr><td rowspan="5">操作系统</td><td>Ubuntu 文档</td><td>https://help.ubuntu.com/</td></tr><tr><td>Ask Ubuntu（提问 Ubuntu）</td><td>https://askubuntu.com/</td></tr><tr><td>超级用户</td><td>https://superuser.com/</td></tr><tr><td>Stack Overflow（技术问答社区）</td><td>https://stackoverflow.com</td></tr><tr><td>YouTube（视频平台）</td><td>https://www.youtube.com/</td></tr><tr><td rowspan="10">电子表格（Calc 为 LibreOffice 中的电子表格组件）</td><td>LibreOffice 帮助</td><td>https://help.libreoffice.org/</td></tr><tr><td>微软技术社区</td><td>https://techcommunity.microsoft.com/</td></tr><tr><td>libreofficehelp.com</td><td>https://www.libreofficehelp.com/</td></tr><tr><td>Reddit r/LibreOfficeCal（Reddit 上的 LibreOffice 电子表格板块）</td><td>https://www.reddit.com/r/LibreOfficeCalc/</td></tr><tr><td>Reddit r/Excel（Reddit 上的 Excel 板块）</td><td>https://www.reddit.com/r/Excel/</td></tr><tr><td>超级用户</td><td>https://superuser.com/</td></tr><tr><td>Medium（在线写作与分享平台）</td><td>https://medium.com/</td></tr><tr><td>Quora（问答平台）</td><td>https://www.quora.com/</td></tr><tr><td>YouTube（视频平台）</td><td>https://www.youtube.com/</td></tr><tr><td>瑞安和黛比及托伦个人网站</td><td>https://www.ryananddebi.com/</td></tr><tr><td rowspan="9">文字处理器（Writer 为 LibreOffice 中的文字处理组件）</td><td>LibreOffice 帮助</td><td>https://help.libreoffice.org/</td></tr><tr><td>LibreOffice 论坛</td><td>https://ask.libreoffice.org/</td></tr><tr><td>libreofficehelp.com</td><td>https://www.libreofficehelp.com/</td></tr><tr><td>超级用户</td><td>https://superuser.com/</td></tr><tr><td>Stack Overflow（技术问答社区）</td><td>https://stackoverflow.com</td></tr><tr><td>Ask Ubuntu（提问 Ubuntu）</td><td>https://askubuntu.com/</td></tr><tr><td>Quora（问答平台）</td><td>https://www.quora.com/</td></tr><tr><td>YouTube（视频平台）</td><td>https://www.youtube.com/</td></tr><tr><td>SearchStar 个人网站</td><td>https://seekstar.github.io/</td></tr><tr><td rowspan="8">演示文稿（Impress 为 LibreOffice 中的演示文稿组件）</td><td>LibreOffice 帮助</td><td>https://help.libreoffice.org/</td></tr><tr><td>libreofficehelp.com</td><td>https://www.libreofficehelp.com/</td></tr><tr><td>Reddit r/LibreOffice（Reddit 上的 LibreOffice 板块）</td><td>https://www.reddit.com/r/LibreOffice/</td></tr><tr><td>超级用户</td><td>https://superuser.com/</td></tr><tr><td>Stack Overflow（技术问答社区）</td><td>https://stackoverflow.com</td></tr><tr><td>技术小贴士</td><td>https://technical-tips.com/</td></tr><tr><td>只需点击此处</td><td>https://justclickhere.co.uk/</td></tr><tr><td>TikTok（短视频平台）</td><td>https://www.tiktok.com/</td></tr><tr><td rowspan="9">VLC（多媒体播放器）</td><td>VLC 文档</td><td>https://docs.videolan.me</td></tr><tr><td>VLCHelp.com</td><td>https://www.vlchelp.com/</td></tr><tr><td>VideoLAN 维基</td><td>https://wiki.videolan.org/</td></tr><tr><td>Ubuntu 文档</td><td>https://help.ubuntu.com/</td></tr><tr><td>Reddit r/Fedora（Reddit 上的 Fedora 板块）</td><td>https://www.reddit.com/r/Fedora/</td></tr><tr><td>超级用户</td><td>https://superuser.com/</td></tr><tr><td>Medium（在线写作与分享平台）</td><td>https://medium.com/</td></tr><tr><td>YouTube（视频平台）</td><td>https://www.youtube.com/</td></tr><tr><td>Dedoimedo（网站名）</td><td>https://www.dedoimedo.com/index.html</td></tr><tr><td rowspan="8">雷鸟（邮件客户端）</td><td>雷鸟支持</td><td>https://support.mozilla.org/en-US/products/thunderbird</td></tr><tr><td>Reddit r/Thunderbird（Reddit 上的雷鸟板块）</td><td>https://www.reddit.com/r/Thunderbird/</td></tr><tr><td>Reddit r/Automation（Reddit 上的自动化板块）</td><td>https://www.reddit.com/r/automation/</td></tr><tr><td>超级用户</td><td>https://superuser.com/</td></tr><tr><td>WikiHow（知识类网站）</td><td>https://www.wikihow.com/</td></tr><tr><td>Quora（问答平台）</td><td>https://www.quora.com/</td></tr><tr><td>比特恢复器（BitRecover）</td><td>https://www.bitrecover.com/</td></tr><tr><td>广告签名器（AdSigner）</td><td>https://www.adsigner.com/</td></tr><tr><td>谷歌浏览器（Chrome）</td><td>谷歌浏览器帮助 <br/> 续见下一页</td><td>https://support.google.com/chrome</td></tr></tbody></table>


Table 7 - continued from previous page
表 7 - 从上一页继续


<table><tr><td>App</td><td>Resources</td><td></td></tr><tr><td rowspan="9"></td><td>Reddit r/Chrome</td><td>https://www.reddit.com/r/Chrome/</td></tr><tr><td>Super User</td><td>https://superuser.com/</td></tr><tr><td>WikiHow</td><td>https://www.wikihow.com/</td></tr><tr><td>in5steps.com</td><td>https://in5stepstutorials.com/</td></tr><tr><td>How-To Geek</td><td>https://www.howtogeek.com/</td></tr><tr><td>Medium</td><td>https://medium.com/</td></tr><tr><td>Quora</td><td>https://www.quora.com/</td></tr><tr><td>YouTube</td><td>https://www.youtube.com/</td></tr><tr><td>Laptop Mag</td><td>https://www.laptopmag.com</td></tr><tr><td rowspan="5">VS Code</td><td>Super User</td><td>https://superuser.com/</td></tr><tr><td>Stack Overflow</td><td>https://stackoverflow.com</td></tr><tr><td>Quora</td><td>https://www.quora.com/</td></tr><tr><td>YouTube</td><td>https://www.youtube.com/</td></tr><tr><td>Campbell Muscle Lab GitHub</td><td>https://campbell-muscle-lab.github.io/</td></tr><tr><td rowspan="6">GIMP</td><td>Reddit r/GIMP</td><td>https://www.reddit.com/r/GIMP/</td></tr><tr><td>Super User</td><td>https://superuser.com/</td></tr><tr><td>Stack Overflow</td><td>https://stackoverflow.com</td></tr><tr><td>Quora</td><td>https://www.quora.com/</td></tr><tr><td>Make-Use-Of</td><td>https://www.makeuseof.com/</td></tr><tr><td>YouTube</td><td>https://www.youtube.com/</td></tr><tr><td rowspan="18">Workflow</td><td>UniPath Marketplace</td><td>https://marketplace.uipath.com/</td></tr><tr><td>sync.blue</td><td>https://www.sync.blue/</td></tr><tr><td>Device Tests</td><td>https://devicetests.com/</td></tr><tr><td>Make Tech Easier</td><td>https://www.maketecheasier.com/</td></tr><tr><td>Unix & Linux Stack Exchange</td><td>https://unix.stackexchange.com/</td></tr><tr><td>Geeks for Geeks</td><td>https://www.geeksforgeeks.org/</td></tr><tr><td>I Love Free Software</td><td>https://www.ilovefreesoftware.com/</td></tr><tr><td>The Geek Diary</td><td>https://www.thegeekdiary.com/</td></tr><tr><td>Zyxware</td><td>https://www.zyxware.com/</td></tr><tr><td>GNOME Discourse</td><td>https://discourse.gnome.org/</td></tr><tr><td>It's FOSS</td><td>https://itsfoss.com/</td></tr><tr><td>Super User</td><td>https://superuser.com/</td></tr><tr><td>Stack Overflow</td><td>https://stackoverflow.com</td></tr><tr><td>LibreOffice Forum</td><td>https://ask.libreoffice.org/</td></tr><tr><td>ImpressExtractNotes</td><td>https://github.com/danielrcollins1/ImpressExtractNotes</td></tr><tr><td>Medium</td><td>https://medium.com/</td></tr><tr><td>YouTube</td><td>https://www.youtube.com/</td></tr><tr><td>Kelvin Smith Library</td><td>https://case.edu/library/</td></tr></table>
<table><tbody><tr><td>应用</td><td>资源</td><td></td></tr><tr><td rowspan="9"></td><td>Reddit r/Chrome</td><td>https://www.reddit.com/r/Chrome/</td></tr><tr><td>超级用户</td><td>https://superuser.com/</td></tr><tr><td>WikiHow</td><td>https://www.wikihow.com/</td></tr><tr><td>in5steps.com</td><td>https://in5stepstutorials.com/</td></tr><tr><td>How-To Geek</td><td>https://www.howtogeek.com/</td></tr><tr><td>Medium</td><td>https://medium.com/</td></tr><tr><td>Quora</td><td>https://www.quora.com/</td></tr><tr><td>YouTube</td><td>https://www.youtube.com/</td></tr><tr><td>Laptop Mag</td><td>https://www.laptopmag.com</td></tr><tr><td rowspan="5">VS Code</td><td>超级用户</td><td>https://superuser.com/</td></tr><tr><td>Stack Overflow</td><td>https://stackoverflow.com</td></tr><tr><td>Quora</td><td>https://www.quora.com/</td></tr><tr><td>YouTube</td><td>https://www.youtube.com/</td></tr><tr><td>Campbell Muscle Lab GitHub</td><td>https://campbell-muscle-lab.github.io/</td></tr><tr><td rowspan="6">GIMP</td><td>Reddit r/GIMP</td><td>https://www.reddit.com/r/GIMP/</td></tr><tr><td>超级用户</td><td>https://superuser.com/</td></tr><tr><td>Stack Overflow</td><td>https://stackoverflow.com</td></tr><tr><td>Quora</td><td>https://www.quora.com/</td></tr><tr><td>Make-Use-Of</td><td>https://www.makeuseof.com/</td></tr><tr><td>YouTube</td><td>https://www.youtube.com/</td></tr><tr><td rowspan="18">Workflow</td><td>UniPath Marketplace</td><td>https://marketplace.uipath.com/</td></tr><tr><td>sync.blue</td><td>https://www.sync.blue/</td></tr><tr><td>Device Tests</td><td>https://devicetests.com/</td></tr><tr><td>Make Tech Easier</td><td>https://www.maketecheasier.com/</td></tr><tr><td>Unix & Linux Stack Exchange</td><td>https://unix.stackexchange.com/</td></tr><tr><td>Geeks for Geeks</td><td>https://www.geeksforgeeks.org/</td></tr><tr><td>I Love Free Software</td><td>https://www.ilovefreesoftware.com/</td></tr><tr><td>The Geek Diary</td><td>https://www.thegeekdiary.com/</td></tr><tr><td>Zyxware</td><td>https://www.zyxware.com/</td></tr><tr><td>GNOME Discourse</td><td>https://discourse.gnome.org/</td></tr><tr><td>It's FOSS</td><td>https://itsfoss.com/</td></tr><tr><td>超级用户</td><td>https://superuser.com/</td></tr><tr><td>Stack Overflow</td><td>https://stackoverflow.com</td></tr><tr><td>LibreOffice Forum</td><td>https://ask.libreoffice.org/</td></tr><tr><td>ImpressExtractNotes</td><td>https://github.com/danielrcollins1/ImpressExtractNotes</td></tr><tr><td>Medium</td><td>https://medium.com/</td></tr><tr><td>YouTube</td><td>https://www.youtube.com/</td></tr><tr><td>Kelvin Smith Library</td><td>https://case.edu/library/</td></tr></tbody></table>


### B.4 Task Examples Collection
### B.4 任务示例集合


Here we show the detailed statistics of OSWORLD benchmark, including the main set on Ubuntu (369 in total) and the analytic set on Windows (43 in total).
在这里我们展示 OSWORLD 基准的详细统计数据，包括在 Ubuntu 上的主集（共 369 条）和在 Windows 上的分析集（共 43 条）。


Table 8: Detailed statistics of OSWORLD benchmark suite about examples number, average instruction tokens, infeasible instructions and integrated instructions.
表 8：OSWORLD 基准集在示例数量、平均指令标记、不可行指令与集成指令方面的详细统计。


<table><tr><td></td><td>OS</td><td>Calc</td><td>Impress</td><td>Writer</td><td>VLC</td><td>Thunderbird</td><td>Chrome</td><td>VSCode</td><td>GIMP</td><td>Workflow</td><td>Overall</td></tr><tr><td>Examples</td><td>24</td><td>47</td><td>47</td><td>23</td><td>17</td><td>15</td><td>46</td><td>23</td><td>26</td><td>101</td><td>369</td></tr><tr><td>Avg. Inst. Tokens</td><td>22.38</td><td>33.30</td><td>25.19</td><td>35.30</td><td>35.82</td><td>34.07</td><td>22.07</td><td>20.78</td><td>16.23</td><td>51.24</td><td>33.36</td></tr><tr><td>#Infeasible</td><td>5</td><td>1</td><td>0</td><td>1</td><td>3</td><td>1</td><td>3</td><td>5</td><td>10</td><td>1</td><td>30</td></tr><tr><td>#Integrated</td><td>7</td><td>19</td><td>30</td><td>0</td><td>0</td><td>0</td><td>26</td><td>0</td><td>0</td><td>2</td><td>84</td></tr></table>
<table><tbody><tr><td></td><td>OS</td><td>Calc</td><td>Impress</td><td>Writer</td><td>VLC</td><td>Thunderbird</td><td>Chrome</td><td>VSCode</td><td>GIMP</td><td>Workflow</td><td>Overall</td></tr><tr><td>Examples</td><td>24</td><td>47</td><td>47</td><td>23</td><td>17</td><td>15</td><td>46</td><td>23</td><td>26</td><td>101</td><td>369</td></tr><tr><td>Avg. Inst. Tokens</td><td>22.38</td><td>33.30</td><td>25.19</td><td>35.30</td><td>35.82</td><td>34.07</td><td>22.07</td><td>20.78</td><td>16.23</td><td>51.24</td><td>33.36</td></tr><tr><td>#不可行</td><td>5</td><td>1</td><td>0</td><td>1</td><td>3</td><td>1</td><td>3</td><td>5</td><td>10</td><td>1</td><td>30</td></tr><tr><td>#集成</td><td>7</td><td>19</td><td>30</td><td>0</td><td>0</td><td>0</td><td>26</td><td>0</td><td>0</td><td>2</td><td>84</td></tr></tbody></table>


Table 9: Detailed statistics of Windows analytic set benchmark suite. This set contains no infeasible tasks and integrated tasks.
表 9：Windows 分析集基准套件的详细统计数据。该集合不包含不可行任务且为集成任务。


<table><tr><td></td><td>Excel</td><td>Word</td><td>PPT</td><td>Workflow</td><td>Overall</td></tr><tr><td>Examples</td><td>11</td><td>9</td><td>7</td><td>16</td><td>43</td></tr><tr><td>Avg. Inst. Tokens</td><td>19.45</td><td>21.44</td><td>21.86</td><td>47.57</td><td>32.48</td></tr></table>
<table><tbody><tr><td></td><td>Excel</td><td>Word</td><td>PPT</td><td>工作流</td><td>总体</td></tr><tr><td>示例</td><td>11</td><td>9</td><td>7</td><td>16</td><td>43</td></tr><tr><td>Avg. Inst. Tokens</td><td>19.45</td><td>21.44</td><td>21.86</td><td>47.57</td><td>32.48</td></tr></tbody></table>


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_ffdd3d.jpg"/>



Figure 7: Comparison of instructions distribution. All datasets are sampled to 300 to make a fair comparison. The hyper-parameters of t-SNE are randomly sampled for each plot.
图 7：指令分布的比较。为公平比较，所有数据集均采样至 300。每个图的 t-SNE 超参数随机抽取。


We also visualize the intent distribution (We obtain sentence embeddings for instructions using OpenAI's embedding model, and then apply t-SNE to reduce the dimensionality to two dimensions for visualization.) and compare it with other benchmarks which also focus on the digital agent. We randomly sample 300 examples from each dataset and randomly choose three different hyperparameters for t-SNE. Visualization results are shown in Figure 7 From the figure, we can observe that the semantic distribution of the instructions alone has reached the most comprehensive level. Additionally, our environment remains controllable and executable, offering a more reliable evaluation. It is also noticeable that the clustering centers of the other three are closely positioned, whereas the points in our distribution approaches are inconsistent with theirs, indicating that we can serve as a unique choice for a more comprehensive assessment of the capabilities of future intelligent agents.
我们还可视化意图分布（我们使用 OpenAI 的嵌入模型获得指令的句子嵌入，然后应用 t-SNE 将维度降至二维以便可视化），并将其与其他同样关注数字代理的基准进行比较。我们从每个数据集随机抽取 300 个样本，并随机选择三组不同的 t-SNE 超参数。可视化结果显示在图 7 中。从图中可以看出，单独指令的语义分布已达到最为全面的层级。此外，我们的环境仍然可控且可执行，提供更可靠的评估。还可注意到，其他三个聚类中心彼此接近，而我们分布的点与它们不一致，表明我们可以作为对未来智能代理能力更全面评估的独特选择。


### B.5 Initial State Setup Details
### B.5 初始状态设置细节


The setup of the initial state contains three stages: 1) Start emulator. The specified virtual machine is activated and automatically reverted to the corresponding snapshot, which records the initial system settings of the machine. 2) Prepare files (Optional). The file or software that specifies the initial state of the task to be executed is downloaded to the virtual machine and opened. The system is configured to first download the files to the host through a direct link and then upload them to the VM via a LAN connection. Specifically, some initial files are set up for OS-related tasks by manipulating the file system directly from the command line. 3) Execute reprocessing commands (Optional). For tasks that require additional preprocessing, task-specific operations are executed after the completion of the first two phases. For example, taking the currently open LibreOffice Impress file to page five, clicking in the center of the screen to return to the main interface, etc. We provide convenient APIs to configure initial conditions and world settings, standardizing our tasks to make this process user-friendly and easily extendable.
初始状态的设置分为三阶段：1) 启动仿真器。指定的虚拟机被激活并自动回放到对应的快照，该快照记录了机器的初始系统设置。2) 准备文件（可选）。要执行任务的初始状态所需的文件或软件被下载到虚拟机并打开。系统配置为先通过直接链接下载文件到主机，然后通过局域网连接上传到虚拟机。具体地，通过直接在命令行操作文件系统，某些初始文件被设置用于与操作系统相关的任务。3) 执行再处理命令（可选）。对于需要额外预处理的任务，在前两阶段完成后执行任务特定操作。例如，将当前打开的 LibreOffice Impress 文件跳转到第 5 页，在屏幕中心单击返回主界面等。我们提供方便的 API 来配置初始条件和世界设置，使任务标准化、用户友好且易于扩展。


### B.6 Evaluation Configuration Details
### B.6 评估配置细节


In this section, we will show details of preparations for the evaluation of the selected apps (LibreOffice - Calc, Writer and Impress, Thunderbird, VLC Media Player, Chrome, VS Code, GIMP) and OS (Ubuntu and Windows).
本节将展示所选应用（LibreOffice - Calc、Writer、Impress、Thunderbird、VLC 媒体播放器、Chrome、VS Code、GIMP）及操作系统（Ubuntu 与 Windows）评估准备工作的细节。


#### B.6.1 Ubuntu
#### B.6.1 Ubuntu


LibreOffice: Calc, Writer, and Impress LibreOffice is a popular open-source fully-featured office suite for Linux desktops. Our benchmark is built upon version 7.3.7.2, the version pre-installed in Ubuntu 22.04. Calc, Writer, and Impress are selected to build tasks on them. As the majority of tasks are to conduct a little revision to a file, we evaluate these tasks mainly by checking the final result file (in xlsx, docx, or pptx format). The check can be done by comparing the result file with a golden reference, or inspecting some particular attributes or settings of the file, e.g., page style, freezing, and locale. Usually, the xlsx, docx, and pptx files are mainly accessed through openpyx ${1}^{8}$ python-docx ${}^{9}$ and python-pptx ${}^{10}$ For some properties not supported by the current libraries, we also look them up directly via parsing the Office Open XML format
LibreOffice：Calc、Writer、Impress LibreOffice 是一套在 Linux 桌面广泛使用的开源全功能办公套件。我们的基准基于版本 7.3.7.2，亦即 Ubuntu 22.04 预装版本。选择 Calc、Writer、Impress 来构建任务。由于大多数任务仅对一个文件进行少量修改，我们主要通过对最终结果文件（xlsx、docx、pptx 格式）进行检查来评估这些任务。检查可以通过将结果文件与黄金参考进行比对，或检查某些特定属性或设置（如页面样式、冻结、区域设置）来完成。通常，xlsx、docx、pptx 文件主要通过 openpyxl ${1}^{8}$、python-docx ${}^{9}$ 和 python-pptx ${}^{10}$ 进行访问。对于当前库不支持的某些属性，我们也直接通过解析 Office Open XML 格式来查找。


Thunderbird Thunderbird is a popular open-source fully-featured email client for Linux desktops. Version 115.6.0 of Thunderbird is pre-installed in Ubuntu 22.04. We crafted an account profile to set up a feasible initial state. Evaluation for Thunderbird configurations is mainly performed by reading various configurations or data files in the profile folder. An open-source reverse engineering tool Firefox Decrypt is adopted to decrypt the stored account information for evaluation. The involved account information is just for examples and contains no information about the real person. Besides, there are tasks instructing to help to compose a new email. In these cases, the accessibility tree is leveraged to inspect the contents in the composing window before really sending it.
Thunderbird Thunderbird 是在 Linux 桌面上广受欢迎的开源全功能邮件客户端。Ubuntu 22.04 预装了 Thunderbird 115.6.0 版本。我们设计了一个账户配置来建立可行的初始状态。对 Thunderbird 配置的评估主要通过读取配置文件夹中的各种配置或数据文件来进行。采用一个开源的逆向工程工具 Firefox Decrypt 来解密用于评估的存储账户信息。涉及的账户信息仅为示例，不包含真实个人信息。此外，还有指导撰写新邮件的任务。在这些情况下，将使用可访问性树在实际发送前检查撰写窗口的内容。


---



${}^{8}$ https://openpyxl.readthedocs.io/en/stable/
${}^{8}$ https://openpyxl.readthedocs.io/en/stable/


https://github.com/python-openxml/python-docx



https://github.com/scanny/python-pptx



https://learn.microsoft.com/en-us/office/open-xml/about-the-open-xml-sdk



---



VLC Media Player VLC Media Player is a popular open-source cross-platform multimedia player and framework that plays most multimedia files. The evaluation for VLC Media Player is multifold, ranging from utilizing VLC HTTP interface ${}^{13}$ reading the VLC configuration file,comparing final result files,and leveraging accessibility tree to inspect the desired content.
VLC 媒体播放器 VLC 媒体播放器是一款广泛使用的开源跨平台多媒体播放器和框架，支持播放大多数多媒体文件。对 VLC 媒体播放器的评估是多方面的，包括使用 VLC HTTP 接口 ${}^{13}$、读取 VLC 配置文件、比较最终结果文件，以及利用无障碍性树来检查所需内容。


Chrome Google Chrome is one of the most popular and powerful cross-platform web browsers developed on Google's free and open-source software project Chromium. The evaluation of Chrome is mainly based on the utilization of Playwrigh ${}^{14}$ a browser automation library to control Chromium, Firefox, and WebKit with a single API. To connect Playwright running on host machine with Chrome running on virtual machine, port transferring tool socal ${}^{15}$ is leveraged. Additional information such as the HTML source codes of websites is also leveraged in the evaluation of some tasks.
Chrome 谷歌 Chrome 是基于 Google 的免费开源软件项目 Chromium 开发的最流行且强大的跨平台网页浏览器之一。对 Chrome 的评估主要基于使用 Playwrigh ${}^{14}$ 这一浏览器自动化库在单一 API 下控制 Chromium、Firefox 与 WebKit。为将主机上的 Playwright 与虚拟机上运行的 Chrome 连接起来，使用端口转发工具 socal ${}^{15}$。评估某些任务时还会利用网站的 HTML 源代码等额外信息。


VS Code VS Code is a popular open-source multi-functional cross-platform editor for source-code editing. The evaluation of VS Code tasks is primarily divided into two different categories. One subset of tasks is predominantly oriented towards file manipulation. In the context of these tasks, a comparative analysis is conducted between the resultant file and an anticipated reference gold file. Another subset of tasks is centered around how to utilize the intrinsic functionalities of the VS Code software itself, such as modifying color themes, initiating workspace sessions, and modifying settings. In these instances, it becomes important to extract relevant internal information and configurations from the VS Code environment.
VS Code VS Code 是一个流行的开源、跨平台的多功能代码编辑器。对 VS Code 任务的评估主要分为两类。一类任务偏向文件操作，在这些任务中，将结果文件与预期的参考金文件进行对比分析。另一类任务围绕如何利用 VS Code 软件本身的内在功能，如修改颜色主题、启动工作区会话、修改设置等。在这些情况下，提取 VS Code 环境中的相关内部信息和配置变得重要。


In the course of this research, we principally leverage the capabilities offered by the VS Code Extension AP ${\rbrack }^{16}$ and information in the settings JSON file ${}^{17}$ to obtain the requisite internal signal for the evaluation process. Our methodology involves the development of a custom VS Code extension, its installation within the VS Code software deployed on our virtual machine, and the subsequent invocation of the extension's command each time an evaluation is required, as well as checking whether the settings JSON has the correct value for a specific key.
在本研究过程中，我们主要利用 VS Code Extension AP ${\rbrack }^{16}$ 提供的能力以及 settings JSON 文件 ${}^{17}$ 中的信息，以获取评估过程所需的内部信号。我们的方法包括开发一个自定义的 VS Code 扩展，将其安装在部署在虚拟机上的 VS Code 软件中，并在每次需要评估时调用扩展的命令，以及检查某一键的 settings JSON 是否具有正确的值。


GIMP GIMP is an open-source raster graphics editor used for image manipulation, editing, free-form drawing, format transcoding, and more specialized tasks. The evaluation for GIMP tasks is also mainly divided into two different categories, just like the VS Code evaluation. One type of task is mainly oriented to file operations. In these tasks, the resulting files are compared and analyzed with the expected reference golden files, mainly relying on some powerful image processing libraries such as pillow ${}^{18}$ | Another category of tasks revolves around taking advantage of the inherent capabilities of the GIMP software itself. In these instances, we primarily read GIMP's configuration files to obtain internal information to evaluate the tasks.
GIMP GIMP 是一个用于图像处理、编辑、自由绘制、格式转码等的开源栅格图形编辑器。对 GIMP 任务的评估也大致分为两类，与 VS Code 的评估类似。一类任务主要面向文件操作，在这些任务中，生成的文件与期望的参考金文件进行比对和分析，主要依赖一些强大的图像处理库如 pillow ${}^{18}$ | 另一类任务则围绕利用 GIMP 软件本身的内在能力。在这些情况下，我们主要读取 GIMP 的配置文件以获取内部信息来评估任务。


#### B.6.2 Windows
#### B.6.2 Windows


Microsoft Office: Excel, Word, and PowerPoint Microsoft Office is the most popular office suite on Windows desktops. These three components share the same functions with the corresponding LibreOffice components by and large. They are used to edit xlsx, docx, and pptx files, respectively. Thus, the evaluation for LibreOffice tasks can be reused for Microsoft Office tasks.
Microsoft Office：Excel、Word 与 PowerPoint Microsoft Office 是在 Windows 台式机上最受欢迎的办公套件。这三大组件在很大程度上与相应的 LibreOffice 组件具有相同的功能。它们分别用于编辑 xlsx、docx 和 pptx 文件。因此，LibreOffice 任务的评估可以复用于 Microsoft Office 任务。


Thunderbird Thunderbird is a cross-platform email client. Only the structure of profile folder on Windows is sightly different from that on Linux. We thus revised the account profile and reuse it to set up the same initial state on Windows.
Thunderbird Thunderbird 是一个跨平台的电子邮件客户端。只有 Windows 上的配置文件夹结构与 Linux 稍有不同。因此，我们修订了账户配置文件并在 Windows 上重复使用，以设置相同的初始状态。


Chrome Chrome is a cross-platform web browser. To evaluate tasks on Chrome, only the port transferring tool needs to be replaced with Ncal ${}^{19}$ Other configurations and the evaluations can be shared with Linux-version tasks.
Chrome Chrome 是一个跨平台的网页浏览器。要在 Chrome 上评估任务，只需要将端口转发工具替换为 Ncal ${}^{19}$ 其他配置和评估可以与 Linux 版本的任务共享。


---



${}^{12}$ https://github.com/unode/firefox_decrypt
${}^{12}$ https://github.com/unode/firefox_decrypt


${}^{13}$ https://wiki.videolan.org/Control_VLC_via_a_browser/
${}^{13}$ https://wiki.videolan.org/Control_VLC_via_a_browser/


14https://playwright.dev/
14https://playwright.dev/


5http://www.dest-unreach.org/socat/https://linux.die.net/man/1/socat
5http://www.dest-unreach.org/socat/https://linux.die.net/man/1/socat


${}^{16}$ https://code.visualstudio.com/api
${}^{16}$ https://code.visualstudio.com/api


${}^{7h}\texttt{ ttps://code.visualstudio.com/docs/getstarted/settings\#\_settings }$ json
${}^{7h}\texttt{ ttps://code.visualstudio.com/docs/getstarted/settings\#\_settings }$ json


18https://pypi.org/project/pillow/
18https://pypi.org/project/pillow/


'http://www.dest-unreach.org/socat/
'http://www.dest-unreach.org/socat/


---



### B.7 More Task Examples
### B.7 更多任务示例


In this section, we curate a collection of examples from various app sets, each characterized by distinct operational logic and requiring different capabilities. These examples are carefully chosen to illustrate the diverse challenges and requirements encountered when interacting with different types of applications.
在本节中，我们整理了来自各应用集的示例集合，每个示例都具有明确定义的操作逻辑并需要不同的能力。这些示例经过精心挑选，旨在展示与不同类型应用交互时所遇到的各种挑战与要求。


Table 10: More Example Showcase from Each Subset of Domains.
表格10：来自每个子域集合的更多示例展示。


<table><tr><td>Related App(s)</td><td>Instruction(s)</td><td>Screenshot</td><td>Abilities Needed</td></tr><tr><td>OS</td><td>I want to install Spotify on my current system. Could you please help me?</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_4a0c95.jpg"/></td><td>knowledge of OS; omit distractions</td></tr><tr><td>Calc</td><td>Check the names in column "Names with duplicates" and put the unique ones in column "Unique Names". Keep the original order.</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_2fc27f.jpg"/></td><td>knowledge tricks or reasoning over long actions</td></tr><tr><td>Calc</td><td>I have a lookup table for the officers of each branch. Please, here is another table in which I need to fill with the officer names according the headoffice (i.e., the branch name). Help me to complete this.</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_635392.jpg"/></td><td>massive elements; knowledge of formulas and functions</td></tr><tr><td>Calc</td><td>Given a partial calendar, please highlight all the weekends (Saturday & Sunday) by setting the cell background as red (#ff0000).</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_6af23d.jpg"/></td><td>massive elements; commonsense reasoning; software tricks</td></tr><tr><td>Impress</td><td>I closed the slide pannel on the left and idk how to get it back please help</td><td></td><td>software knowledge; typos in instruction</td></tr></table>
<table><tbody><tr><td>相关应用</td><td>说明</td><td>屏幕截图</td><td>需要的能力</td></tr><tr><td>操作系统</td><td>我想在当前系统上安装 Spotify。你能帮忙吗？</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_4a0c95.jpg"/></td><td>对操作系统的知识；避免干扰</td></tr><tr><td>计算</td><td>检查“重复名称”列中的名称，保留唯一名称放在“唯一名称”列中。保持原有顺序。</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_2fc27f.jpg"/></td><td>知识性技巧或对长流程的推理</td></tr><tr><td>计算</td><td>我有一个各分支官员的查找表。请看另一张需要按总行（即分支名称）填写官员姓名的表，帮我完成。</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_635392.jpg"/></td><td>大量元素；公式与函数的知识</td></tr><tr><td>计算</td><td>给定一个部分日历，请将所有周末（周六和周日）高亮显示，单元格背景设为红色（#ff0000）。</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_6af23d.jpg"/></td><td>大量元素；常识性推理；软件技巧</td></tr><tr><td>给人深刻印象</td><td>我把左侧的幻灯片面板关上了，不知如何找回，请帮忙</td><td></td><td>软件知识；说明中的笔误</td></tr></tbody></table>


Continued on next page
续页继续


Table 10 - continued from previous page
表 10 - 继续自前一页


<table><tr><td>Related App(s)</td><td>Task Instruction</td><td>Screenshot of Initial State</td><td>Abilities Needed</td></tr><tr><td>Impress</td><td>On it Whenever I launch a Li-breOffice Impress, it uses both screens, one for current slide and next slide and another for actual presentation. What I want is to use only one monitor which shows presentation. I dont want the screen with Current slide and Next slide so that it can be used for other purposes. How should I achieve this?</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_3dbdf8.jpg"/></td><td>reason from unprofessional phenomenon expression</td></tr><tr><td>Writer</td><td>Copy the screenshot 1.png from the desktop to where my cursor is located</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_a4a2e6.jpg"/></td><td>locate the position of cursor; switch from desktop and app</td></tr><tr><td>Chrome</td><td>Can you help me clean up my computer by getting rid of all the tracking things that Amazon might have saved? I want to make sure my browsing is private and those sites don't remember me.</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_7ac4e9.jpg"/></td><td>understanding the unprofessional expression</td></tr><tr><td>VLC</td><td>I am reading lecture note in PDF while a music video is running in VLC media player: But I find I need to switch to the player every time I need to pause/start.Could you help me change the setting to allow pausing the video using keyboard shortcut without minimizing the PDF reader? I want to focus on the lecture note and don't be disturbed by the app switching.</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_313802.jpg"/></td><td>understanding the reference from unprofessional expression; software knowledge</td></tr><tr><td>VLC</td><td>Hey, could you turn this video the right way up for me? And once it's flipped around, could you save it for me with the name '1984_Apple.mp4' on the main screen where all my files are?</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_ae3d96.jpg"/></td><td>software knowledge; spatial judgment ability</td></tr></table>
<table><tbody><tr><td>相关应用</td><td>任务指令</td><td>初始状态截图</td><td>所需能力</td></tr><tr><td>给人留下深刻印象</td><td>关于它 无论何时我启动 Li-breOffice Impress，它使用两块屏幕，一块用于当前幻灯片和下一张幻灯片，另一块用于实际演示。我想只用一台显示演示的显示器。我不想要显示当前幻灯片和下一张幻灯片的屏幕，以便用于其他用途。我应该如何实现？</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_3dbdf8.jpg"/></td><td>来自不专业现象表达的原因</td></tr><tr><td>作者</td><td>将桌面截图 1.png 复制到光标所在的位置</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_a4a2e6.jpg"/></td><td>定位光标的位置；在桌面与应用之间切换</td></tr><tr><td>Chrome</td><td>你能帮我清理电脑，删除可能被亚马逊保存的所有跟踪项吗？我想确保我的浏览私密，这些站点不再记住我。</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_7ac4e9.jpg"/></td><td>理解不专业表达</td></tr><tr><td>VLC</td><td>我在 VLC 媒体播放器播放音乐视频时阅读 PDF 讲义：但我发现需要每次暂停/开始时都切换到播放器。你能帮我把设置改掉，以便通过键盘快捷键暂停视频而不最小化 PDF 阅读器吗？我想专注于讲义，不被应用切换打扰。</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_313802.jpg"/></td><td>理解来自不专业表达的参考；软件知识</td></tr><tr><td>VLC</td><td>嘿，你能把这个视频竖着翻过来吗？一旦翻转后，能否在我所有文件的主屏幕上以“1984_Apple.mp4”的名称保存给我？</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_ae3d96.jpg"/></td><td>软件知识；空间判断能力</td></tr></tbody></table>


Continued on next page
续见下页


<table><tr><td colspan="4">Table 10 - continued from previous page</td></tr><tr><td>Related App(s)</td><td>Task Instruction</td><td>Screenshot of Initial State</td><td>Abilities Needed</td></tr><tr><td>Thunderbird</td><td>Create a local folder called "Promotions" and create a filter to auto move the inbox emails whose subject contains "discount" to the new folder</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_74ef7b.jpg"/></td><td>software knowledge</td></tr><tr><td>Thunderbird</td><td>Attach the my AWS bill to the email. The bill is stored at /aws-bill.pdf. Don't close it or send it. I haven't finish all the contents.</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_9650b5.jpg"/></td><td>file management; extra requirement</td></tr><tr><td>Thunderbird</td><td>I've got a bunch of email accounts in Thunderbird, and it's a hassle to check them one by one. Can you show me how to set up a unified inbox so I can see all my emails in one place?</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_d59ac3.jpg"/></td><td>deep-hided feature, need to be explored even by human users; pop-up window</td></tr><tr><td>VS Code</td><td>Please modify VS Code's settings to disable error reporting for Python missing imports.</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_78ff25.jpg"/></td><td>software knowledge to deal with settings; reasoning to understand the cause and solution of the error</td></tr><tr><td>VS Code</td><td>Please help me install the autoDocstring extension in VS Code.</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_eb2723.jpg"/></td><td>software knowledge to deal with Extensions; reasoning to search and install the extension successfully</td></tr><tr><td>GIMP</td><td>Could you make the background of this image transparent for me?</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_77a4ec.jpg"/></td><td>precise and intricate operations</td></tr></table>
<table><tbody><tr><td colspan="4">Table 10 - 继续上一页</td></tr><tr><td>相关应用程序</td><td>任务说明</td><td>初始状态截图</td><td>所需能力</td></tr><tr><td>Thunderbird</td><td>创建本地文件夹名为 “Promotions”，并创建过滤器将主题包含 “discount” 的收件箱邮件自动移动到新文件夹</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_74ef7b.jpg"/></td><td>软件知识</td></tr><tr><td>Thunderbird</td><td>将我的 AWS 账单附在邮件中。账单存放在 /aws-bill.pdf。不要关闭它或发送它。我还没完成所有内容。</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_9650b5.jpg"/></td><td>文件管理；额外要求</td></tr><tr><td>Thunderbird</td><td>我在 Thunderbird 里有一堆邮箱账户，逐个查看很麻烦。你能给我演示如何设置统一收件箱，让我在一个地方看到所有邮件吗？</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_d59ac3.jpg"/></td><td>深藏功能，需要即使人类用户也要探索；弹出窗口</td></tr><tr><td>VS Code</td><td>请修改 VS Code 的设置，禁用 Python 缺少导入时的错误报告。</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_78ff25.jpg"/></td><td>处理设置的软件知识；推理以理解错误原因与解决方案</td></tr><tr><td>VS Code</td><td>请帮我在 VS Code 中安装 autoDocstring 扩展</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_eb2723.jpg"/></td><td>处理扩展的软件知识；推理以搜索并成功安装扩展</td></tr><tr><td>GIMP</td><td>你能为我把这张图片的背景变透明吗？</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_77a4ec.jpg"/></td><td>精确而复杂的操作</td></tr></tbody></table>


Continued on next page
续见下一页


Table 10 - continued from previous page
表 10 - 继续自上一页


<table><tr><td colspan="4">Table 10 — continued from previous page</td></tr><tr><td>Related App(s)</td><td>Task Instruction</td><td>Screenshot of Initial State</td><td>Abilities Needed</td></tr><tr><td>GIMP</td><td>Help me choose the yellow triangle and position it at the center of my picture.</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_132087.jpg"/></td><td>spatial perception and reasoning, as well as precise control of actions</td></tr><tr><td>Multiple (VLC+GIMP)</td><td>Could you help me create an Animated GIF from a video file using VLC and GIMP from the source of video "src.mp4", 5- second clip beginning at 00:03?</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_bc0aff.jpg"/></td><td>software knowledge to undergo sophisticated processes and ability to process multi-step procedure successfully</td></tr><tr><td>Multiple <br> (ThunderBird+ Writer+Chrome)</td><td>Help me export charts, graph or other images from docx files received in email "Lecture Document" in Notes folder and upload these png files to the figures/folder in Google Drive for later use (use numbers to name them).</td><td></td><td>ability to selectively export charts, graphs and images from docx file; software knowledge for google drive file upload</td></tr><tr><td>Multiple (Chrome+Calc)</td><td>Could you help me extract data in the table from a new invoice uploaded to my Google Drive, then export it to a Libreoffice calc .xlsx file in the desktop?</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_d351eb.jpg"/></td><td>ability to do table data extraction; export data to .xlsx</td></tr></table>
<table><tbody><tr><td colspan="4">Table 10 — 续前页</td></tr><tr><td>相关应用</td><td>任务指令</td><td>初始状态截图</td><td>所需能力</td></tr><tr><td>GIMP</td><td>请帮我在图片中选择黄色三角形并将其置于中心位置。</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_132087.jpg"/></td><td>空间感知与推理，以及对动作的精确控制</td></tr><tr><td>多项（VLC+GIMP）</td><td>你能否帮我用 VLC 和 GIMP 从视频源 "src.mp4" 创建一个动画 GIF，选择从 00:03 开始的5秒片段？</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_bc0aff.jpg"/></td><td>具备经历复杂流程的软件知识，以及成功完成多步骤流程的能力</td></tr><tr><td>多项 <br/>（ThunderBird+ Writer+Chrome）</td><td>请帮我导出电子邮件中收到的 docx 文件“Lecture Document”中的图表、图形或其他图片，并将这些 PNG 文件上传到 Google Drive 的 figures/ 文件夹以备后续使用（用数字命名）。</td><td></td><td>能够从 docx 文件中Selective 导出图表、图形和图片的能力；具备将文件上传至 Google Drive 的软件知识</td></tr><tr><td>多项（Chrome+Calc）</td><td>你能否帮我从上传到 Google Drive 的新发票中提取表格数据，然后导出为桌面 LibreOffice Calc 的 .xlsx 文件？</td><td><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_d351eb.jpg"/></td><td>具备表格数据提取的能力；导出数据为 .xlsx</td></tr></tbody></table>


## C Details of Baseline Methods
## Baseline方法的细节


### C.1 Hyper-Parameter of the Baseline Agents
### C.1 基线代理的超参数


We utilize the versions of gpt-3.5-turbo-16k, gpt-4-0125-preview, and gpt-4-vision-preview, respectively for GPT results, need to be noted that result could be changed from time since it is close-sourced. We also employ the gemini-pro and gemini-pro-vision versions for the Gemini models For all language models, we set the temperature parameter to 1.0, and top_p to 0.9, and the maximum number of tokens for generation is set to 1500 . We set the maximum steps of interaction to 15 and the maximum time limits to 30 minutes for all tasks since the agent could lead to a stuck environment under some unexpected cases.
我们分别使用 gpt-3.5-turbo-16k、gpt-4-0125-preview 和 gpt-4-vision-preview 的版本作为 GPT 结果，由于为闭源，结果可能随时间变化需注意。我们也为 Gemini 模型使用 gemini-pro 和 gemini-pro-vision 版本。对所有语言模型，温度参数设为 1.0，top_p 设为 0.9，生成的最大 token 数设为 1500。我们将所有任务的最大交互步数设为 15，最大时间限制设为 30 分钟，以防在某些意外情况下代理导致环境卡死。


### C.2 Prompt Details
### C.2 提示详情


C.2.1 Prompt for A11y Tree, Screenshot and their Combination Setting
C.2.1 A11y Tree、截图及其组合设置的提示


---



You are an agent which follow my instruction and perform desktop computer
你是一个按照我的指令执行桌面计算机任务的代理


&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ tasks as instructed.
&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 按指示完成任务。


&nbsp;&nbsp;&nbsp;&nbsp;You have good knowledge of computer and good internet connection and assume
&nbsp;&nbsp;&nbsp;&nbsp;你对计算机有良好知识、网络连接良好，并假设


$\hookrightarrow$ your code will run on a computer for controlling the mouse and keyboard.
$\hookrightarrow$ 你的代码将在一台计算机上运行，以控制鼠标和键盘。


&nbsp;&nbsp;&nbsp;&nbsp;For each step, you will get an observation of an image, which is the
&nbsp;&nbsp;&nbsp;&nbsp;每一步，你将得到一张图像的观测，即


$\hookrightarrow$ screenshot of the computer screen and you will predict the action of the
$\hookrightarrow$ 计算机屏幕的截图，你将基于图像预测操作


&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ computer based on the image.
&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 计算机的动作。


You are required to use `pyautogui` to perform the action grounded to the
你需要使用 `pyautogui` 来执行基于观测的操作，


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ observation,but DONOT use the `pyautogui.locateCenterOnScreen` function
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 但不使用 `pyautogui.locateCenterOnScreen` 函数


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ to locate the element you want to operate with since we have no image of
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 来定位你要操作的元素，因为我们没有要操作元素的图像。


&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ the element you want to operate with. DONOT USE `pyautogui.screenshot()`
&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 你要操作的元素。请勿使用 `pyautogui.screenshot()`


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ to make screenshot.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 以截屏。 


&nbsp;&nbsp;&nbsp;&nbsp;Return one line or multiple lines of python code to perform the action each
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;返回一行或多行 Python 代码以执行该操作


&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ time,be time efficient. When predicting multiple lines of code,make
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 次，节省时间。预测多行代码时，做


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ some small sleep like `time.sleep(0.5); `interval so that the machine
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 一些类似 time.sleep(0.5) 的小睡眠间隔，以便机器


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ could take; Each time you need to predict a complete code,no variables
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 可以获取；每次你需要预测一个完整的代码，没有变量


&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ or function can be shared from history
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 或函数可以从历史中共享


You need to to specify the coordinates of by yourself based on your
 你需要基于你的自身情况自行指定坐标


$\hookrightarrow$ observation of current observation,but you should be careful to ensure
 $\hookrightarrow$ 对当前观测的观察，但你应小心确保


&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ that the coordinates are correct.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 坐标是正确的。


&nbsp;&nbsp;&nbsp;&nbsp;You ONLY need to return the code inside a code block, like this:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你只需要返回代码块中的代码，如下所示：


	```python
	 ```python


&nbsp;&nbsp;&nbsp;&nbsp;#your code here
&nbsp;&nbsp;&nbsp;&nbsp;# #your code here


&nbsp;&nbsp;&nbsp;&nbsp;...



Specially, it is also allowed to return the following special code:
 特别地，也允许返回以下特殊代码：


When you think you have to wait for some time, return ``WAIT``;
 当你认为需要等待一段时间时，返回 ``WAIT``；


When you think the task can not be done, return ```FAIL```, don't easily say
 当你认为任务无法完成时，返回 ```FAIL```，不要轻易说出


$\hookrightarrow$ ```FAIL```, try your best to do the task;
 $\hookrightarrow$ ```FAIL```, 尽力完成任务；


When you think the task is done, return ```DONE```.
当你觉得任务完成时，请返回 ```DONE```。


My computer's password is 'password', feel free to use it when you need sudo
我的计算机密码是 'password'，需要 sudo 时可以随意使用


&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ rights.
&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 权限。


First give the current screenshot and previous things we did a short
先给出当前截图以及我们之前做过的内容简短回顾


$\hookrightarrow$ reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER
$\hookrightarrow$ 反思，然后把我要求的代码或特殊代码返回给我。永远


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ EVER RETURN ME ANYTHING ELSE.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 绝不返回给我其他任何东西。


---



For a11y tree setting and a11y tree + screenshot setting, the prompts are basically the same, just replace the screenshot words with a11y tree words.
对于无障碍树设定和无障碍树 + 截图设定，提示基本相同，只需要将截图相关词语替换为无障碍树词语。


#### C.2.2 Prompt for SoM Setting
#### C.2.2 SoM 设置提示


---



You are an agent which follow my instruction and perform desktop computer
你是一个遵循我的指令并执行桌面计算机任务的代理


$\hookrightarrow$ tasks as instructed.
$\hookrightarrow$ 按指示执行任务。


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You have good knowledge of computer and good internet connection and assume
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你具备良好的计算机知识和良好的互联网连接，并假设


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ your code will run on a computer for controlling the mouse and keyboard.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 你的代码将运行在用于控制鼠标和键盘的计算机上。


For each step, you will get an observation of the desktop by 1) a screenshot
对于每一步，你将通过1) 截图观察桌面，以及 2)


&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ with interact-able elements marked with numerical tags; and 2)
&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 具有可交互元素的标记的标签；以及 2) 


&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ accessibility tree,which is based on AT-SPI library. And you will
&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 无障碍树，这是基于 AT-SPI 库的。你将


$\hookrightarrow$ predict the action of the computer based on the image and text
$\hookrightarrow$ 根据图像和文本预测计算机的动作


$\hookrightarrow$ information.
$\hookrightarrow$ 信息。


You are required to use `pyautogui` to perform the action grounded to the
你需要使用 `pyautogui` 来执行与之相关的动作


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\rightarrow$ observation,but DONOT use the `pyautogui.locateCenterOnScreen` function
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\rightarrow$ 观察，但请勿使用 `pyautogui.locateCenterOnScreen` 函数


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ to locate the element you want to operate with since we have no image of
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 来定位你要操作的元素，因为我们没有该元素的图片


$\hookrightarrow$ the element you want to operate with. DONOT USE `pyautogui.screenshot()`
$\hookrightarrow$ 的元素你要操作。请勿使用 `pyautogui.screenshot()`


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ to make screenshot.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 来截图。


---



---



You can replace $x,y$ in the code with the tag of the element you want to
你可以在代码中用你要操作的元素标签替换 $x,y$


$\hookrightarrow$ operate with. such as:
$\hookrightarrow$ 操作对象。比如：


	```python
	```python


pyautogui.moveTo(tag_3)
pyautogui.moveTo(tag_3)


pyautogui.click(tag_2)
pyautogui.click(tag_2)


pyautogui.dragTo(tag_1, button='left')
pyautogui.dragTo(tag_1, button='left')


When you think you can directly output precise $x$ and $y$ coordinates or there
当你认为可以直接输出精确的 $x$ 和 $y$ 坐标，或者


$\hookrightarrow$ is no tag on which you want to interact,you can also use them directly.
$\hookrightarrow$ 上没有要交互的标签时，你也可以直接使用它们。


But you should be careful to ensure that the coordinates are correct.
但你应小心以确保坐标正确。


Return one line or multiple lines of python code to perform the action each
返回一行或多行的 Python 代码来执行该操作


$\hookrightarrow$ time,be time efficient. When predicting multiple lines of code,make
$\hookrightarrow$ 时间，time 高效利用时间。当预测多行代码时，确保


&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ some small sleep like `time.sleep(0.5); interval so that the machine
&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 一些像 `time.sleep(0.5);` 的小休眠间隔，以便机器


&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ could take; Each time you need to predict a complete code,no variables
&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 能够接手；每次需要预测完整代码时，不要携带变量


&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ or function can be shared from history
&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 或函数可以从历史中共享


You need to to specify the coordinates of by yourself based on your
你需要根据你自己的


$\hookrightarrow$ observation of current observation,but you should be careful to ensure
$\hookrightarrow$ 观测当前观测来指定坐标，但你应小心确保


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ that the coordinates are correct.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 坐标正确。


&nbsp;&nbsp;&nbsp;&nbsp;You ONLY need to return the code inside a code block, like this:
&nbsp;&nbsp;&nbsp;&nbsp;你只需返回代码块中的代码，如下所示：


		```python
		```python


#your code here
#你的代码在此


&nbsp;&nbsp;&nbsp;&nbsp;...



&nbsp;&nbsp;&nbsp;&nbsp;Specially, it is also allowed to return the following special code:
&nbsp;&nbsp;&nbsp;&nbsp;特别地，也可以返回以下特殊代码：


When you think you have to wait for some time, return ``WAIT``;
当你认为需要等待一段时间时，返回 ``WAIT``；


When you think the task can not be done, return "FAIL" , don't easily say
当你认为任务无法完成时，返回 "FAIL" ，不要轻易说


$\hookrightarrow$ ```FAIL```, try your best to do the task;
$\hookrightarrow$ ```FAIL```, 尽力完成任务；


When you think the task is done, return ```DONE```.
当你认为任务完成时，返回 ```DONE```。


My computer's password is 'password', feel free to use it when you need sudo
我的计算机密码是 'password'，需要 sudo 时随时使用也可以


&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ rights.
&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 权利。


First give the current screenshot and previous things we did a short
先给出当前截图和我们之前做过的简短说明


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 反思，然后把我要求的代码或特殊代码交给我。绝不


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ EVER RETURN ME ANYTHING ELSE.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\hookrightarrow$ 永远不要再给我其他东西。


---



### C.3 Input Settings
### C.3 输入设置


Accessibility tree We evaluate reasoning capabilities of LLM agents on textual descriptions of the observation in the form of accessibility trees. To make accessibility trees (usually millions of tokens) tractable for LLM contexts, we filter out non-essential elements by their tag, visibility, availability, etc as described in C.4 Only the tag, name, text, position, and size of the remaining elements are kept and concatenated by tab character in the input. As the raw coordinates are provided within the accessibility tree, the LLM is required to ground its action predictions to accurate coordinates.
无障碍树 我们在对观察的文字描述进行评估时，评估 LLM 代理的推理能力，使用可访问性树的形式。为了使可访问性树（通常为数百万个标记）对 LLM 上下文可处理，我们按标签、可见性、可用性等过滤掉非关键元素，具体如 C.4 所述。仅保留剩余元素的标签、名称、文本、位置和大小，并用制表符在输入中连接。由于可访问性树内提供了原始坐标，LLM 需要将其动作预测落到准确坐标上。


Screenshot To evaluate the visual reasoning ability of VLMs, we give the raw screenshot of the VM directly to the VLM, which is understand the screenshot and predict correct actions with precise coordinates. The raw resolution of the screen is set to ${1920} \times  {1080}$ . To investigate robustness to resolution changes,we perform ablation studies by manually downsampling the screenshot.
屏幕截图 为评估视觉推理能力，我们直接将虚拟机的原始屏幕截图提供给 VLM，使其理解截图并以精确坐标预测正确动作。屏幕的原始分辨率设置为 ${1920} \times  {1080}$ 。为研究对分辨率变化的鲁棒性，我们通过手动下采样截图进行消融研究。


Screenshot + accessibility tree We test whether the combination of screenshots with the accessibility tree can improve the capacity of VLM for spatial grounding.
截图 + 无障碍树 我们测试截图与无障碍树的组合是否能提升 VLM 的空间定位能力。


Set-of-Marks Set-of-Marks (SoM) [61] enhances the grounding capabilities of VLMs marking image segments with annotations like alphanumerics, masks, or boxes. We leverage the information from the filtered accessibility tree and mark the elements on the screenshot with a numbered bounding box. Following VisualWe-bArena [26] and UFO [64], we further combine the annotated screenshot with the text metadata from accessibility tree, including the index, tag, name, and text of the elements. Instead of predicting precise coordinates, the VLM is supposed to specify the action object by its number index, which will be mapped into our action space by post-processing. Ablation studies are also conducted with different resolutions for SoM setting.
一组标记 Set-of-Marks Set-of-Marks (SoM) [61] 提升 VLM 的定位能力，通过对图像片段进行标注，如字母数字、遮罩或框。我们利用经过筛选的可访问性树中的信息，对截图上的元素进行编号边界框标记。仿照 VisualWe-bArena [26] 与 UFO [64]，我们将带注释的截图与无障碍树的文本元数据（包括元素的索引、标签、名称和文本）结合起来。VLM 不直接预测精确坐标，而是将动作对象通过其数字索引来指定，经过后处理映射到我们的动作空间。也对 SoM 设置在不同分辨率下进行了消融研究。


### C.4 Accessibility Tree Filtering
### C.4 可访问性树过滤


Since the original tree is large (usually over 1 million tokens in XML format), we filter the accessibility tree nodes by their tags, visibilities, availabilities, etc. The concrete rules are illustrated in the following Table 11
由于原始树往往规模巨大（在 XML 格式下通常超过 100 万个标记），我们通过标签、可见性、可用性等对可访问性树节点进行筛选。具体规则在下表 11 中展示


Table 11: Criteria for keeping a11y tree nodes on Ubuntu and Windows platforms
表 11：在 Ubuntu 和 Windows 平台上保留 a11y 树节点的准则


<table><tr><td>Condition</td><td>Ubuntu</td><td>Windows</td></tr><tr><td>Node Tags</td><td>document*, *item, *button, *heading, *label, *scrollbar, *searchbox, *textbox, *link, *tabelement, *textfield, *textarea, *menu, alert, canvas, check-box, combo-box, entry, icon, image, paragraph, scroll-bar, section, slider, static, table-cell, terminal, text, netuiribontab, start, trayclockwclass, tray-dummysearchcontrol, uiimage, uiproperty, uiribboncommandbar</td><td>Same as Ubuntu</td></tr><tr><td>Showing</td><td>True</td><td>Not Applicable</td></tr><tr><td>Visible</td><td>True</td><td>True</td></tr><tr><td>Enabled or Editable or Expandable or Checkable</td><td>True</td><td>True</td></tr><tr><td>Has Name or Text or Image</td><td>True</td><td>True</td></tr><tr><td>Position</td><td>>= 0</td><td>>= 0</td></tr><tr><td>Size</td><td>$> 0$</td><td>> 0</td></tr></table>
<table><tbody><tr><td>条件</td><td>Ubuntu</td><td>Windows</td></tr><tr><td>节点标签</td><td>document*, *item, *button, *heading, *label, *scrollbar, *searchbox, *textbox, *link, *tabelement, *textfield, *textarea, *menu, 警报, 画布, 复选框, 组合框, 条目, 图标, 图像, 段落, 滚动条, 部分, 滑块, 静态, 表格单元, 终端, 文本, netuiribontab, start, trayclockwclass, tray-dummysearchcontrol, uiimage, uiproperty, uiribboncommandbar</td><td>与 Ubuntu 相同</td></tr><tr><td>正在显示</td><td>为真</td><td>不适用</td></tr><tr><td>可见</td><td>为真</td><td>为真</td></tr><tr><td>启用或可编辑或可展开或可选中</td><td>为真</td><td>为真</td></tr><tr><td>有名称或文本或图像</td><td>为真</td><td>为真</td></tr><tr><td>位置</td><td>>= 0</td><td>>= 0</td></tr><tr><td>大小</td><td>$> 0$</td><td>> 0</td></tr></tbody></table>


### C.5 Set-of-Mark Implementation Details
### C.5 Set-of-Mark 实现细节


Our methodology involves an initial transformation of the original observational image acquired from our environment into the standardized SoM format and be putted into VLMs together with a table of the marks with metadata information such as tags and names. This format consists of bounding boxes that bound the sub-images of interest, each associated with a corresponding integer mark. Notably, our approach incorporates the utilization of the accessibility tree to identify the bounding boxes associated with all clickable buttons within the current image observation, instead of using segmentation models like the original SoM paper. Some examples of screenshots after applying SoM are shown in Figure 8 We can observe the emergence of some low-quality, unmodelable tasks, and even misleading bounding boxes, depending on the level of support from the software ecosystem. This could be another reason for the poor performance of SoM. Future improvements could be made in this regard.
我们的方法包括将从环境中获取的原始观测图像进行初始转换，转化为标准化的 SoM 格式，并与带有元数据（如标签和名称）的标记表一起输入到 VLMs 中。该格式由限定子图像的边界盒组成，每个边界盒关联一个相应的整数标记。值得注意的是，我们的方法采用可及性树来识别当前图像观测中的所有可点击按钮所对应的边界盒，而不是使用像原始 SoM 论文那样的分割模型。应用 SoM 后的一些截图示例如图 8 所示。我们可以观察到，随着软件生态系统对支持程度的不同，可能会出现一些低质量、无法建模的任务，甚至出现误导性的边界盒。这也可能是 SoM 性能不佳的另一原因。未来在这方面可以进行改进。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_de9de7.jpg"/>



Figure 8: Showcase of example screenshots marked by SoM across different app GUI.
Figure 8: SoM 对不同应用 GUI 标注的示例截图展示。


Table 12: Success rates of more baseline LLM and VLM agents on OSWORLD, grouped by task categories: OS, Office (LibreOffice Calc, Impress, Writer), Daily (Chrome, VLC Player, Thunderbird), Professional (VS Code and GIMP) and Workflow (tasks involving multiple apps), for gaining insights from interfaces and operation logic. See C. 1 and C. 6 for more details.
Table 12: 在 OSWORLD 上更基线的 LLM 与 VLM 代理的成功率，按任务类别分组：OS、Office（LibreOffice Calc、Impress、Writer）、Daily（Chrome、VLC Player、Thunderbird）、Professional（VS Code 和 GIMP）以及 Workflow（涉及多个应用的任务），用于从界面和操作逻辑中获得洞见。更多细节见 C.1 与 C.6。


<table><tr><td rowspan="2">Inputs</td><td rowspan="2">Model</td><td colspan="6">Success Rate (↑)</td></tr><tr><td>OS</td><td>Office</td><td>Daily</td><td>Profess.</td><td>Workflow</td><td>Overall</td></tr><tr><td rowspan="4">Screenshot</td><td>GPT-40-mini</td><td>12.50%</td><td>3.58%</td><td>3.99%</td><td>4.08%</td><td>1.62%</td><td>3.77%</td></tr><tr><td>InternVL2</td><td>12.50%</td><td>1.87%</td><td>2.71%</td><td>8.16%</td><td>0.99%</td><td>3.33%</td></tr><tr><td>MiniCPM-V-2.6</td><td>8.33%</td><td>2.72%</td><td>1.42%</td><td>0.00%</td><td>0.63%</td><td>1.88%</td></tr><tr><td>Llava-OneVision</td><td>8.33%</td><td>2.72%</td><td>2.71%</td><td>0.00%</td><td>1.62%</td><td>2.42%</td></tr><tr><td colspan="2">Human Performance</td><td>75.00%</td><td>71.79%</td><td>70.51%</td><td>73.47%</td><td>73.27%</td><td>72.36%</td></tr></table>
<table><tbody><tr><td rowspan="2">输入</td><td rowspan="2">模型</td><td colspan="6">成功率（↑）</td></tr><tr><td>操作系统</td><td>办公</td><td>每日</td><td>教授</td><td>工作流</td><td>总体</td></tr><tr><td rowspan="4">截图</td><td>GPT-40-mini</td><td>12.50%</td><td>3.58%</td><td>3.99%</td><td>4.08%</td><td>1.62%</td><td>3.77%</td></tr><tr><td>InternVL2</td><td>12.50%</td><td>1.87%</td><td>2.71%</td><td>8.16%</td><td>0.99%</td><td>3.33%</td></tr><tr><td>MiniCPM-V-2.6</td><td>8.33%</td><td>2.72%</td><td>1.42%</td><td>0.00%</td><td>0.63%</td><td>1.88%</td></tr><tr><td>Llava-OneVision</td><td>8.33%</td><td>2.72%</td><td>2.71%</td><td>0.00%</td><td>1.62%</td><td>2.42%</td></tr><tr><td colspan="2">人类表现</td><td>75.00%</td><td>71.79%</td><td>70.51%</td><td>73.47%</td><td>73.27%</td><td>72.36%</td></tr></tbody></table>


### C.6 Full Results of Baseline Methods
### C.6 基线方法的完整结果


Here we show the break-down results of baseline methods from different LLMs and VLMs for follow-up reference.
在这里，我们展示了来自不同大语言模型（LLM）和视觉语言模型（VLM）的基线方法的分解结果，供后续参考。


We have also compiled the distribution of steps taken by the GPT-4V model under our four settings: Accessibility Tree (A11y Tree), Screenshot, Screenshot combined with Accessibility Tree (Screenshot+A11y Tree), and Set-of-Mark. This data (as shown in Fig. 9) provides potential clues for future work. Overall, there are observable differences in how many steps the agent chooses to execute and when it decides to terminate under different settings. More detailed control and analysis of these behaviors can be explored in subsequent research efforts.
我们还整理了在四种设定下 GPT-4V 模型所采取步骤的分布：可访问性树（A11y Tree）、截图、截图与可访问性树结合（Screenshot+A11y Tree）以及集合标记（Set-of-Mark）。该数据（如图 9 所示）为未来工作提供潜在线索。总体而言，在不同设定下，代理选择执行的步骤数量以及何时决定终止存在可观察的差异。对这些行为的更详细控制与分析可在后续研究中进一步探讨。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_c2b725.jpg"/>



Figure 9: Distribution of steps taken by the GPT-4V based agents across four different settings.
图 9：GPT-4V 基于代理在四种不同设定下所采取的步骤分布。


### C.7 Running Time and Cost
### C.7 运行时间与成本


We calculate and monitor the running time and cost for a portion of our experiments, which use the APIs of GPT-4V, Gemini-ProV, and Claude-3 Opus under different settings in April 2024. These results are shown in Table 14
我们对部分实验的运行时间与成本进行计算与监控，这些实验在 2024 年 4 月使用 GPT-4V、Gemini-ProV 和 Claude-3 Opus 的 API，在不同设定下进行。结果如表 14 所示


## D Further Analysis
## D 进一步分析


### D.1 Performance by Task Difficulty, Feasibility and App Involved
### D.1 按任务难度、可行性与所涉应用的性能


We analyze the success rate across several additional subsets of tasks, as summarized in Tab. 15 and will be discussed in the following sections.
我们分析若干额外子集任务的成功率，概括见表 15，后续章节将讨论。


Table 13: Detailed success rates of baseline LLM and VLM agents on OSWORLD, divided by apps (domains): OS, LibreOffice Calc, LibreOffice Impress, LibreOffice Writer, Chrome, VLC Player, Thunderbird, VS Code, GIMP and Workflow which is comprehensive with multiple apps, for gaining insights from interfaces and operation logics.
表 13：OSWORLD 上基线 LLM 与 VLM 代理的详细成功率，按应用（领域）划分：操作系统、LibreOffice Calc、LibreOffice Impress、LibreOffice Writer、Chrome、VLC 播放器、Thunderbird、VS Code、GIMP 及 Workflow（包含多应用的综合情景），以从界面与操作逻辑中获得洞见。


<table><tr><td rowspan="2">Inputs</td><td rowspan="2">Model</td><td colspan="11">Success Rate (↑)</td></tr><tr><td>OS</td><td>Calc</td><td>Impress</td><td>Writer</td><td>VLC</td><td>TB</td><td>ChromeVSC</td><td></td><td>GIMP</td><td>Workflow</td><td></td></tr><tr><td rowspan="8">A11y</td><td>Mixtral-8x7B</td><td>12.50</td><td>0.00</td><td>0.39</td><td>4.34</td><td>10.22</td><td>6.67</td><td>2.17</td><td>8.69</td><td>3.85</td><td>0.10</td><td></td></tr><tr><td>GPT-3.5</td><td>4.17</td><td>2.13</td><td>6.77</td><td>4.35</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>0.00</td><td>1.62</td><td></td></tr><tr><td>Gemini-Pro</td><td>4.17</td><td>0.00</td><td>2.13</td><td>4.35</td><td>12.41</td><td>0.00</td><td>2.17</td><td>0.00</td><td>7.69</td><td>0.63</td><td></td></tr><tr><td>GPT-4</td><td>20.83</td><td>0.00</td><td>6.77</td><td>4.35</td><td>23.53</td><td>26.67</td><td>26.09</td><td>30.43</td><td>23.08</td><td>2.97</td><td></td></tr><tr><td>Gemini-Pro-1.5</td><td>12.50</td><td>2.13</td><td>2.13</td><td>4.35</td><td>6.53</td><td>0.00</td><td>10.87</td><td>8.70</td><td>0.00</td><td>3.60</td><td></td></tr><tr><td>Llama-3-70B</td><td>4.17</td><td>0.00</td><td>0.39</td><td>8.70</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>0.00</td><td>0.63</td><td></td></tr><tr><td>GPT-40</td><td>20.83</td><td>6.38</td><td>6.77</td><td>8.69</td><td>12.41</td><td>20.00</td><td>17.39</td><td>21.74</td><td>11.54</td><td>7.56</td><td></td></tr><tr><td>Qwen-Max</td><td>29.17</td><td>0.00</td><td>2.52</td><td>13.04</td><td>8.95</td><td>0.00</td><td>10.87</td><td>8.70</td><td>11.54</td><td>2.61</td><td></td></tr><tr><td rowspan="6">Screen</td><td>CogAgent</td><td>4.17</td><td>0.00</td><td>0.00</td><td>4.34</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>0.00</td><td>0.00</td><td></td></tr><tr><td>Gemini-ProV</td><td>8.33</td><td>0.00</td><td>6.77</td><td>4.35</td><td>12.41</td><td>0.00</td><td>6.52</td><td>8.70</td><td>23.08</td><td>2.08</td><td></td></tr><tr><td>GPT-4V</td><td>12.50</td><td>0.00</td><td>2.52</td><td>4.35</td><td>18.34</td><td>0.00</td><td>6.52</td><td>0.00</td><td>7.69</td><td>6.04</td><td></td></tr><tr><td>Claude-3-Opus</td><td>4.17</td><td>0.00</td><td>2.52</td><td>4.34</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>3.84</td><td>2.61</td><td></td></tr><tr><td>Gemini-Pro-1.5</td><td>12.50</td><td>0.00</td><td>13.16</td><td>8.70</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>11.54</td><td>3.60</td><td></td></tr><tr><td>GPT-40</td><td>8.33</td><td>0.00</td><td>6.77</td><td>4.35</td><td>16.10</td><td>0.00</td><td>4.35</td><td>4.35</td><td>3.85</td><td>5.58</td><td></td></tr><tr><td rowspan="6">Screen + A11y</td><td>CogAgent</td><td>4.17</td><td>2.17</td><td>0.00</td><td>4.35</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>0.00</td><td>0.10</td><td></td></tr><tr><td>Gemini-ProV</td><td>4.17</td><td>2.13</td><td>6.77</td><td>4.35</td><td>18.30</td><td>0.00</td><td>4.35</td><td>0.00</td><td>0.00</td><td>1.52</td><td></td></tr><tr><td>GPT-4V</td><td>16.67</td><td>0.00</td><td>6.77</td><td>21.73</td><td>24.18</td><td>33.33</td><td>21.74</td><td>21.74</td><td>15.38</td><td>4.59</td><td></td></tr><tr><td>Claude-3-Opus</td><td>12.50</td><td>2.13</td><td>4.65</td><td>4.34</td><td>18.30</td><td>0.00</td><td>2.17</td><td>8.69</td><td>7.69</td><td>0.99</td><td></td></tr><tr><td>Gemini-Pro-1.5</td><td>12.50</td><td>0.00</td><td>4.65</td><td>8.70</td><td>12.41</td><td>0.00</td><td>8.70</td><td>4.35</td><td>11.54</td><td>1.56</td><td></td></tr><tr><td>GPT-40</td><td>41.67</td><td>4.26</td><td>6.81</td><td>8.70</td><td>9.50</td><td>6.67</td><td>15.22</td><td>30.43</td><td>0.00</td><td>7.46</td><td></td></tr><tr><td rowspan="6">SoM</td><td>CogAgent</td><td>4.17</td><td>2.17</td><td>0.00</td><td>4.34</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>0.00</td><td>0.00</td><td></td></tr><tr><td>Gemini-ProV</td><td>4.17</td><td>0.00</td><td>0.39</td><td>4.34</td><td>6.53</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.63</td><td></td></tr><tr><td>GPT-4V</td><td>8.33</td><td>8.51</td><td>6.38</td><td>13.04</td><td>10.66</td><td>33.33</td><td>23.91</td><td>17.39</td><td>11.54</td><td>6.57</td><td></td></tr><tr><td>Claude-3-Opus</td><td>12.50</td><td>2.13</td><td>0.39</td><td>8.70</td><td>6.53</td><td>13.33</td><td>17.39</td><td>0.00</td><td>11.54</td><td>4.49</td><td></td></tr><tr><td>Gemini-Pro-1.5</td><td>16.67</td><td>0.00</td><td>10.64</td><td>4.35</td><td>24.18</td><td>13.33</td><td>8.70</td><td>4.35</td><td>15.38</td><td>3.60</td><td></td></tr><tr><td>GPT-40</td><td>20.83</td><td>0.00</td><td>6.77</td><td>4.35</td><td>6.53</td><td>0.00</td><td>4.35</td><td>4.35</td><td>0.00</td><td>3.60</td><td></td></tr><tr><td colspan="2">Human Performance</td><td>75.00</td><td>61.70</td><td>80.85</td><td>73.91</td><td>70.59</td><td>46.67</td><td>78.26</td><td>73.91</td><td>73.08</td><td>73.27</td><td></td></tr></table>
<table><tbody><tr><td rowspan="2">输入</td><td rowspan="2">模型</td><td colspan="11">成功率 (↑)</td></tr><tr><td>系统</td><td>计算</td><td>印象</td><td>作者</td><td>VLC</td><td>TB</td><td>ChromeVSC</td><td></td><td>GIMP</td><td>工作流程</td><td></td></tr><tr><td rowspan="8">无障碍</td><td>Mixtral-8x7B</td><td>12.50</td><td>0.00</td><td>0.39</td><td>4.34</td><td>10.22</td><td>6.67</td><td>2.17</td><td>8.69</td><td>3.85</td><td>0.10</td><td></td></tr><tr><td>GPT-3.5</td><td>4.17</td><td>2.13</td><td>6.77</td><td>4.35</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>0.00</td><td>1.62</td><td></td></tr><tr><td>Gemini-Pro</td><td>4.17</td><td>0.00</td><td>2.13</td><td>4.35</td><td>12.41</td><td>0.00</td><td>2.17</td><td>0.00</td><td>7.69</td><td>0.63</td><td></td></tr><tr><td>GPT-4</td><td>20.83</td><td>0.00</td><td>6.77</td><td>4.35</td><td>23.53</td><td>26.67</td><td>26.09</td><td>30.43</td><td>23.08</td><td>2.97</td><td></td></tr><tr><td>Gemini-Pro-1.5</td><td>12.50</td><td>2.13</td><td>2.13</td><td>4.35</td><td>6.53</td><td>0.00</td><td>10.87</td><td>8.70</td><td>0.00</td><td>3.60</td><td></td></tr><tr><td>Llama-3-70B</td><td>4.17</td><td>0.00</td><td>0.39</td><td>8.70</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>0.00</td><td>0.63</td><td></td></tr><tr><td>GPT-40</td><td>20.83</td><td>6.38</td><td>6.77</td><td>8.69</td><td>12.41</td><td>20.00</td><td>17.39</td><td>21.74</td><td>11.54</td><td>7.56</td><td></td></tr><tr><td>Qwen-Max</td><td>29.17</td><td>0.00</td><td>2.52</td><td>13.04</td><td>8.95</td><td>0.00</td><td>10.87</td><td>8.70</td><td>11.54</td><td>2.61</td><td></td></tr><tr><td rowspan="6">屏幕</td><td>CogAgent</td><td>4.17</td><td>0.00</td><td>0.00</td><td>4.34</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>0.00</td><td>0.00</td><td></td></tr><tr><td>Gemini-ProV</td><td>8.33</td><td>0.00</td><td>6.77</td><td>4.35</td><td>12.41</td><td>0.00</td><td>6.52</td><td>8.70</td><td>23.08</td><td>2.08</td><td></td></tr><tr><td>GPT-4V</td><td>12.50</td><td>0.00</td><td>2.52</td><td>4.35</td><td>18.34</td><td>0.00</td><td>6.52</td><td>0.00</td><td>7.69</td><td>6.04</td><td></td></tr><tr><td>Claude-3-Opus</td><td>4.17</td><td>0.00</td><td>2.52</td><td>4.34</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>3.84</td><td>2.61</td><td></td></tr><tr><td>Gemini-Pro-1.5</td><td>12.50</td><td>0.00</td><td>13.16</td><td>8.70</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>11.54</td><td>3.60</td><td></td></tr><tr><td>GPT-40</td><td>8.33</td><td>0.00</td><td>6.77</td><td>4.35</td><td>16.10</td><td>0.00</td><td>4.35</td><td>4.35</td><td>3.85</td><td>5.58</td><td></td></tr><tr><td rowspan="6">屏幕 + 无障碍</td><td>CogAgent</td><td>4.17</td><td>2.17</td><td>0.00</td><td>4.35</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>0.00</td><td>0.10</td><td></td></tr><tr><td>Gemini-ProV</td><td>4.17</td><td>2.13</td><td>6.77</td><td>4.35</td><td>18.30</td><td>0.00</td><td>4.35</td><td>0.00</td><td>0.00</td><td>1.52</td><td></td></tr><tr><td>GPT-4V</td><td>16.67</td><td>0.00</td><td>6.77</td><td>21.73</td><td>24.18</td><td>33.33</td><td>21.74</td><td>21.74</td><td>15.38</td><td>4.59</td><td></td></tr><tr><td>Claude-3-Opus</td><td>12.50</td><td>2.13</td><td>4.65</td><td>4.34</td><td>18.30</td><td>0.00</td><td>2.17</td><td>8.69</td><td>7.69</td><td>0.99</td><td></td></tr><tr><td>Gemini-Pro-1.5</td><td>12.50</td><td>0.00</td><td>4.65</td><td>8.70</td><td>12.41</td><td>0.00</td><td>8.70</td><td>4.35</td><td>11.54</td><td>1.56</td><td></td></tr><tr><td>GPT-40</td><td>41.67</td><td>4.26</td><td>6.81</td><td>8.70</td><td>9.50</td><td>6.67</td><td>15.22</td><td>30.43</td><td>0.00</td><td>7.46</td><td></td></tr><tr><td rowspan="6">SoM</td><td>CogAgent</td><td>4.17</td><td>2.17</td><td>0.00</td><td>4.34</td><td>6.53</td><td>0.00</td><td>2.17</td><td>0.00</td><td>0.00</td><td>0.00</td><td></td></tr><tr><td>Gemini-ProV</td><td>4.17</td><td>0.00</td><td>0.39</td><td>4.34</td><td>6.53</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.63</td><td></td></tr><tr><td>GPT-4V</td><td>8.33</td><td>8.51</td><td>6.38</td><td>13.04</td><td>10.66</td><td>33.33</td><td>23.91</td><td>17.39</td><td>11.54</td><td>6.57</td><td></td></tr><tr><td>Claude-3-Opus</td><td>12.50</td><td>2.13</td><td>0.39</td><td>8.70</td><td>6.53</td><td>13.33</td><td>17.39</td><td>0.00</td><td>11.54</td><td>4.49</td><td></td></tr><tr><td>Gemini-Pro-1.5</td><td>16.67</td><td>0.00</td><td>10.64</td><td>4.35</td><td>24.18</td><td>13.33</td><td>8.70</td><td>4.35</td><td>15.38</td><td>3.60</td><td></td></tr><tr><td>GPT-40</td><td>20.83</td><td>0.00</td><td>6.77</td><td>4.35</td><td>6.53</td><td>0.00</td><td>4.35</td><td>4.35</td><td>0.00</td><td>3.60</td><td></td></tr><tr><td colspan="2">人类表现</td><td>75.00</td><td>61.70</td><td>80.85</td><td>73.91</td><td>70.59</td><td>46.67</td><td>78.26</td><td>73.91</td><td>73.08</td><td>73.27</td><td></td></tr></tbody></table>


Table 14: Summary of expected time and budget cost for different settings. Calculated in April 2024; these values may change with updates from the API providers.
表 14：不同设定的预期时间和预算成本摘要。于 2024 年 4 月计算；这些数值可能随 API 提供商的更新而变化。


<table><tr><td>Setting</td><td>Expected Time</td><td>Budget Cost (Full Test Set/Small Test Set)</td></tr><tr><td>GPT-4V (screenshot)</td><td>10h</td><td>\$100 (\$10)</td></tr><tr><td>Gemini-ProV (screenshot)</td><td>15h</td><td>\$0 (\$0)</td></tr><tr><td>Claude-3 Opus (screenshot)</td><td>15h</td><td>\$150 (\$15)</td></tr><tr><td>GPT-4V (a11y tree, SoM, etc.)</td><td>30h</td><td>\$500 (\$50)</td></tr></table>
<table><tbody><tr><td>设置</td><td>预计时间</td><td>预算成本（完整测试集/小型测试集）</td></tr><tr><td>GPT-4V（截图）</td><td>10h</td><td>\$100 (\$10)</td></tr><tr><td>Gemini-ProV（截图）</td><td>15h</td><td>\$0 (\$0)</td></tr><tr><td>Claude-3 Opus（截图）</td><td>15h</td><td>\$150 (\$15)</td></tr><tr><td>GPT-4V（无障碍树、SoM 等）</td><td>30h</td><td>\$500 (\$50)</td></tr></tbody></table>


Task difficulty We categorize the tasks based on the time required for human completion into three groups: $0 \sim  {60}\mathrm{\;s}$ (Easy), 60s <180s (Medium), and greater than 180 seconds (Hard), as an indicator of difficulty. Across these groups, the model's success rate drops as the required time increases, with tasks taking longer than 180 seconds becoming almost impossible to complete (considering we have infeasible examples for agent's luckiness), whereas human performance across these three groups is 84.91%, 81.08% and 49.57%, showing a slight decline of the same trend but not to the extent of being unachievable.
任务难度 我们将任务按人类完成时间分为三组：$0 \sim  {60}\mathrm{\;s}$（简单），60s <180s（中等），以及大于180秒（困难），作为难度的指示。跨越这三组，随着所需时间的增加，模型的成功率下降，完成时间超过180秒的任务几乎不可能完成（考虑到我们对代理运气的不可行情况示例），而人类在这三组的表现分别为84.91%、81.08%和49.57%，呈现同样趋势的略微下降，但不足以达到不可实现的程度。


Table 15: Success rate (SR) of GPT-4V (SoM) across different types of tasks.
表格 15：GPT-4V（SoM）在不同类型任务上的成功率（SR）。


<table><tr><td>Task Subset</td><td>% of Total</td><td>SR (↑)</td></tr><tr><td>Easy</td><td>28.72%</td><td>16.78%</td></tr><tr><td>Medium</td><td>40.11%</td><td>13.12%</td></tr><tr><td>Hard</td><td>30.17%</td><td>4.59%</td></tr><tr><td>Infeasible</td><td>8.13%</td><td>16.67%</td></tr><tr><td>Feasible</td><td>91.87%</td><td>13.34%</td></tr><tr><td>Single-App</td><td>72.63%</td><td>13.74%</td></tr><tr><td>Multi-App Workflow</td><td>27.37%</td><td>6.57%</td></tr></table>
<table><tbody><tr><td>任务子集</td><td>% of Total</td><td>SR (↑)</td></tr><tr><td>简单</td><td>28.72%</td><td>16.78%</td></tr><tr><td>中等</td><td>40.11%</td><td>13.12%</td></tr><tr><td>困难</td><td>30.17%</td><td>4.59%</td></tr><tr><td>不可行</td><td>8.13%</td><td>16.67%</td></tr><tr><td>可行</td><td>91.87%</td><td>13.34%</td></tr><tr><td>单应用</td><td>72.63%</td><td>13.74%</td></tr><tr><td>多应用工作流</td><td>27.37%</td><td>6.57%</td></tr></tbody></table>


Feasibility We also divide tasks into groups of tasks infeasible (e.g., deprecated features or hallucinated features)
可行性 我们还将任务分为不可行任务组（例如过时的特性或幻觉特性）与可行任务组


and tasks feasible, which requires the agents to have the ability to judge based on their own knowledge and exploration results. As shown in Tab. 15, we observe that agents currently perform slightly better in terms of infeasibility (16.67% to 13.34%), but overall, they are at a relatively low level. It is noteworthy that we also observe in some methods and settings (such as under the pure screenshot setting with the Gemini-Pro model), agents tend to easily output FAIL and refuse to continue trying. This situation leads to some false positives in infeasible tasks. The focus needs to be on improving overall performance.
以及需要代理基于自身知识和探索结果进行判断的可行任务。如表 15 所示，我们观察到代理在不可行方面的表现略好些（16.67% 降至 13.34%），但总体仍处于相对较低水平。值得注意的是，在某些方法与设置中（如在纯截图设定下使用 Gemini-Pro 模型时），代理往往容易输出 FAIL 并拒绝继续尝试。这种情况导致不可行任务出现一些假阳性。需要将重点放在提升总体表现上。


Number of apps involved We also examined the performance based on whether the task involved apps software or within a single app. As shown in Tab. 15 the average performance for tasks involving a single app is low, at 13.74%, but still more than double the 6.57% observed for subsets of tasks involving workflows across multiple apps. Within single-app scenarios, tasks involving GUI-intensive Office apps generally performed the worst, with subsets such as LibreOffice Calc often scoring zero (we show more detailed results in App. C.6) These findings highlight the need for improved collaboration capabilities between software and enhanced proficiency in specific scenarios.
涉及的应用数量 我们还基于任务是否涉及应用程序软件或在单一应用内来评估性能。如表 15 所示，涉及单一应用的任务平均绩效较低，为 13.74%，但仍是涉及跨多个应用的工作流子集的 6.57% 的两倍以上。在单一应用场景下，涉及图形用户界面密集的 Office 应用的任务通常表现最差，诸如 LibreOffice Calc 等子集经常得分为零（我们在附录 C.6 里给出更详细的结果）这些发现强调需要提升软件之间的协作能力以及在特定场景下的熟练度。


### D.2 A11y tree-based Observation Length Distribution
### D.2 基于可访问性树的观测长度分布


The main experiment revealed the decisive role of the a11y tree in performance within the current technological context. Even when we retain key attribute elements based on heuristic rules (keep nodes with tags of the document, item, button, heading, label, etc.), LLMs still require a sufficiently large context to process this information effectively. To further understand this, we sample some a11y tree observations from OSWORLD and conducted the statistical analysis, as shown in Figure 10 The analysis indicates that a context length of 6000 is needed to accommodate about ${90}\%$ of cases for a single observation.
主要实验揭示了在当前技术背景下，可访问性树对性能的决定性作用。即便我们基于启发式规则保留基于文档、项、按钮、标题、标签等标签的关键属性元素，LLM 仍然需要足够大的上下文才能有效处理这些信息。为进一步理解，我们从 OSWORLD 抽取了一些可访问性树的观测并进行了统计分析，如图 10 所示。分析表明，需上下文长度达到 6000 才能容纳大约 ${90}\%$ 的单次观测案例。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_d07ccc.jpg"/>



Figure 10: The length distribution of a11y tree as observation from sampled trajectories.
图 10：从抽样轨迹观测到的可访问性树的长度分布。


### D.3 Performance across Different Operating Systems
### D.3 不同操作系统上的性能差异


Another key challenge in building universal digital agents is ensuring that these agents can maintain efficient and consistent performance across different operating system environments. The differences between OS and their software ecosystems can significantly impact an agent's observation and action spaces, leading to performance uncertainties. Here, we explore and analyze the correlation between the success of agents in completing tasks on Windows after migrating from Ubuntu using examples from OSWORLD.
构建通用数字代理的另一个关键挑战是在不同操作系统环境中确保这些代理能够维持高效且一致的性能。操作系统及其软件生态间的差异会显著影响代理的观测与行动空间，进而带来性能不确定性。在这里，我们通过 OSWORLD 的示例，探索并分析从 Ubuntu 迁移到 Windows 后代理在完成任务方面的成功率相关性。


Table 16: Comparison of model performance and correlation across operating systems.
表 16：不同操作系统下模型性能与相关性比较。


<table><tr><td>OS</td><td>SR (%)</td><td>Correlation Coefficient</td></tr><tr><td>Ubuntu</td><td>4.88</td><td rowspan="2">0.7</td></tr><tr><td>Windows</td><td>2.55</td></tr></table>
<table><tbody><tr><td>OS</td><td>SR (%)</td><td>相关系数</td></tr><tr><td>Ubuntu</td><td>4.88</td><td rowspan="2">0.7</td></tr><tr><td>Windows</td><td>2.55</td></tr></tbody></table>


We enhance the functionality of the OSWORLD environment to support setting up initial experiment states, final evaluations, and obtaining observations such as the a11y tree and screenshots in Windows OS. Additionally, we have made example-wise fine-tuning modifications to the existing subset in OSWORLD for migration to Windows. We conduct evaluations using the GPT-4V screenshot-only method and present the correlation of performance across the two operating systems. As shown in Tab. 16 the model's performance on Ubuntu and Windows is 4.88% and 2.55%, respectively, with a correlation coefficient of 0.7, despite the differences in their observation spaces. This implies that insights and methodologies developed within the OSWORLD framework can be effectively transferred to Windows environments with a high degree of reliability.
我们增强了 OSWORLD 环境的功能，以支持设定初始实验状态、最终评估，并获取诸如 Windows OS 的 a11y 树和屏幕截图等观测结果。此外，我们对 OSWORLD 的现有子集进行了示例级微调修改，以便迁移到 Windows。我们使用 GPT-4V 仅截图的方法进行评估，并给出两种操作系统之间性能相关性。如表 16 所示，模型在 Ubuntu 与 Windows 的表现分别为 4.88% 和 2.55%，相关系数为 0.7，尽管它们的观测空间存在差异。这意味着在 OSWORLD 框架中发展出的洞见和方法能够在 Windows 环境中以较高的可靠性转移。


### D.4 Qualitative Analysis
### D.4 定性分析


In this section we highlight representative examples of success, failure, and surprising outcomes, alongside a comparative study between GPT-4V and Claude-3 agents, to elucidate the unique challenges and insights our environment introduces. See App. D. 5 for more details.
在本节中，我们突出展示了代表性的成功、失败和出乎意料的结果示例，并对 GPT-4V 与 Claude-3 代理进行了对比研究，以阐明我们的环境所带来的独特挑战和洞见。详见附录 D.5。 


Success and failure cases We find agents, particularly based on GPT-4V, can successfully solve tasks that involve complex problem-solving or creative thinking, showcasing the advanced understanding and processing capabilities of the model already. One successful task is shown in the first row of Figure 11 The agent is requested to extract subtitle files from the video stream and save them locally. The agent first divides the screen into two parts, with the VLC application window on the left and the terminal window open on the right, and uses the ffmpeg command twice. The first use removes the subtitles embedded in the original video, and the second use saves the extracted subtitles locally.
成功与失败案例 我们发现，尤其是基于 GPT-4V 的代理能够成功解决涉及复杂问题解决或创造性思维的任务，显示出模型已经具备的高级理解与处理能力。一个成功任务显示在图 11 的第一行：要求代理从视频流中提取字幕文件并本地保存。代理先将屏幕分成两部分，左边是 VLC 应用窗口，右边是已打开的终端窗口，并两次使用 ffmpeg 命令。第一次使用去除原始视频中嵌入的字幕，第二次使用将提取的字幕本地保存。 


Task Instruction: I downloaded an episode of Friends to practice listening, but I don't know how to remove the subtitles. Please help me remove the subtitles from the video and export it as "subtitles.srt" and store it in the same directory as the video.
任务指令：我下载了一集《Friends》来练习听力，但不知道如何去除字幕。请帮助我将字幕从视频中移除，并将其导出为“subtitles.srt”，并与视频放在同一目录中。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_de193b.jpg"/>



Figure 11: The agent successfully understood the complex task instructions, extracted the subtitle file from the video, and generated a pure video without embedded subtitles.
图 11：代理成功理解了复杂任务指令，从视频中提取字幕文件，并生成不含嵌字幕的纯视频。


Despite the successes, there are notable failures that highlight the limitations of current models. In the task of "center-aligning the title of the document" (Fig. 12 line 1), the agent fails to ground the relatively simple requirement of "center alignment of texts", performing many useless actions such as selecting irrelevant words, opening irrelevant menus, etc.
尽管取得了一些成功，但也存在显著的失败，凸显了当前模型的局限性。在任务“将文档标题居中对齐”（图 12 第 1 行）中，代理未能把“文本居中对齐”这一相对简单的要求落地，执行了许多无用操作，如选取无关词语、打开无关菜单等。


Moreover, we find that the agent lacks prior knowledge in using software, performing poorly in many specialized tasks (as shown in Fig. 13 with GIMP, LibreOffice Calc, and Chrome selected). Taking GIMP as an example, for the instruction "reduce brightness" the agent does not know which menu in the toolbar is for brightness adjustment and instead randomly tries until exhausting the maximum number of steps.
此外，我们发现代理对软件的使用缺乏先验知识，在许多专业任务中表现不佳（见图 13，选中的为 GIMP、LibreOffice Calc 和 Chrome）。以 GIMP 为例，对于指令“降低亮度”，代理不知道工具栏中的哪个菜单用于亮度调整，而是随机尝试，直到达到最大步骤数为止。


Common errors by GPT-4V agents Among the 550 failed examples from different settings in our sample, more than 75% exist mouse click inaccuracies, which is the most common error. The agent fails to click the correct coordinates despite planning detailed and accurate steps in their code comments, indicating strong planning but weak execution capabilities. Mouse click inaccuracies lead to two other frequent errors: repetitive clicks and environmental noise dilemma. Repetitive clicks occur when the agent repeatedly misclicks, adjusts, and fails, consuming too many steps. Environmental noise arises from clicking unintended objects, causing pop-ups, or opening unrelated applications. Due to a lack of prior knowledge about most professional software, it falls into a mismatch dilemma between the actions taken and the current state, and don't know how to get back to normal. Moreover, the agent lacks basic human-like cognition of web pages, such as not closing pop-ups in real-world web pages or being attracted by advertisement content, which affects its original correct judgment. Failures also arise from misinterpretation of instructions and visual oversight, highlighting the need for improvement in language and visual processing. See App. D.5.2 for the specific execution process.
GPT-4V 代理常见错误 在我们样本的不同设置中，550 个失败示例中超过 75% 存在鼠标点击不准确，这是最常见的错误。代理尽管在代码注释中计划了详细且准确的步骤，但未能点击正确的坐标，表明其计划能力强但执行能力弱。鼠标点击不准确导致的另外两类频繁错误是：重复点击和环境噪声困境。重复点击发生在代理反复误点、调整并失败，消耗了过多步骤。环境噪声源于点击到非目标对象，导致弹出窗口或打开无关应用。由于对大多数专业软件缺乏先验知识，它陷入所行动与当前状态之间的不匹配境地，不知道如何回到正常状态。此外，代理缺乏对网页的基本人类认知，例如在真实网页中不关闭弹窗，或被广告内容吸引，从而影响其原有正确判断。失败还源于对指令的误解和视觉监控的疏漏，凸显了语言与视觉处理方面需要改进的地方。详见附录 D.5.2 的具体执行过程。


Discrepancies in task difficulty between agent and human We identify notable disparities in the perceived difficulty of tasks between humans and AI agents. Tasks that are intuitively simple for humans often present substantial challenges to agents, and conversely, tasks that humans find demanding can be more straightforward for agents to execute. You can find more details in Fig. 16 and App. D. 5.3
任务难度的代理与人类差异 我们发现人类与 AI代理之间在感知任务难度上存在显著差异。对人类直观简单的任务，代理往往面临重大挑战；反之，人类认为困难的任务，对代理执行可能更为直接。更多细节见图 16 与附录 D.5.3


Tasks where humans outperform agents These tasks mainly involve text-based and design-related work, such as "bold the font on this slide and add notes" or "erase all the highlighted marks in this document" (Fig. 12 Line 2). Since the Internet lacks such fine-grained data as the software execution process, the agent also lacks the corresponding training process, so its grounding ability is not good enough. The lack of understanding of GUI logic also causes poor performance on operations like selecting and scrolling.
人类胜过代理的任务 这些任务主要涉及文本处理和设计相关工作，如“将此幻灯片的字体加粗并添加笔记”或“删除此文档中所有高亮标记”（图 12 第 2 行）。由于互联网缺乏软件执行过程等细粒度数据，代理也缺乏相应的训练过程，因此其落地能力不足。对 GUI 逻辑的理解不足也造成了在选择和滚动等操作上的表现不佳。 


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_79ee75.jpg"/>



Figure 12: Screenshots of the three examples mentioned in the quality analysis. The first line is an example of GPT-4V failing at a very simple task, the second line is one example where agents face more difficulty than humans, and the third line is one example that is more difficult for humans than for agents.
Figure 12: 质量分析中提及的三个示例的截图。第一行是 GPT-4V 在一个非常简单任务上的失败示例，第二行是代理比人类更困难的一个示例，第三行则是对人类来说比对代理更困难的一个示例。


Tasks where agents outperform humans Tasks that the agent considers simple but humans find difficult are concentrated in "code solvability tasks", such as "monitor the system CPU for 30s and output the results" and "force close a process". These tasks require little or no GUI interaction and can be completed by executing complex codes and instructions. It's worth noting that completing through code sometimes mismatches with human instructions. In the task "use GIMP to cut out the 2s to 4s part of a video,(Fig. 12 Line 3)" the agent used "ffmpeg" command to complete the video cropping, ignoring the "use GIMP" requirement in the instructions.
代理超越人类的任务 将代理认为简单但人类觉得困难的任务集中在“代码可解性任务”等领域，如“监控系统 CPU 30s 并输出结果”和“强制结束一个进程”。这些任务几乎不需要 GUI 交互，可以通过执行复杂代码和指令来完成。值得注意的是，通过代码完成有时与人类指令不匹配。在任务“用 GIMP 裁剪视频的 2s 至 4s 部分（Fig. 12 Line 3）”中，代理使用了“ffmpeg”命令完成视频裁剪，忽略了指令中的“使用 GIMP”要求。


Surprisingly, we discovered that agents are as prone to inefficiency in mechanically repetitive tasks, such as copying, pasting, and batch editing of Excel sheets, as humans. Humans frequently commit careless errors during execution. The shortcomings in agents stem either from the absence of an API or from insufficient training data related to the API, hindering their ability to efficiently process tasks in batches. Furthermore, sluggish response times can cause tasks to either time out or surpass the maximum allowed steps.
令人惊讶的是，我们发现代理在机械重复性任务（如复制、粘贴和批量编辑 Excel 表格）上的低效也与人类一样容易出现。人类在执行过程中也常常犯粗心错误。代理的不足来源要么是缺乏 API，要么是与该 API 相关的训练数据不足，阻碍它们高效批量处理任务。此外，响应时间较慢也可能导致任务超时或超过最大允许步数。


Comparative analysis: Claude-3 vs. GPT-4V Although Claude outperforms GPT-4 in many benchmarks such as GSM8K, HumanEval, etc., in our main experiment, we find that Claude has an average lower accuracy rate compared to GPT-4V by 2.84% to 7.76%. We find that Claude can provide satisfactory high-level solutions, but its grounding ability contains hallucinations in detail. For instance, Claude would interpret double-clicking a file as selecting it instead of opening it, treat column B in LibreOffice Calc software as column C, and enter text in the VS Code text replacement box without clicking on global replace. This shows that Claude can align well with human planning in problem-solving, but lacks excellent grounding ability when it comes to execution. Details can be seen in Fig. 17 and App. D.5.4
对比分析：Claude-3 与 GPT-4V 尽管 Claude 在 GSM8K、HumanEval 等多项基准测试中优于 GPT-4，但在我们的主要实验中，Claude 的平均准确率低于 GPT-4V 2.84% 至 7.76% 不等。我们发现 Claude 能提供令人满意的高层次解决方案，但其对细节的推理基础存在幻觉。例如，Claude 会将双击文件理解为选择而非打开；将 LibreOffice Calc 的列 B 视为列 C；在 VS Code 的文本替换框中输入文本而不点击全局替换。此现象表明 Claude 在解决问题时能与人类规划很好地对齐，但在执行阶段的把握能力不足。相关细节见图 17 和附录 D.5.4


### D.5 Qualitative Analysis Examples
### D.5 定性分析示例


Here we give the specific code execution process for the examples in the qualitative analysis in §D.4 and add more error cause analysis, trying to get insights for further improvement.
在 §D.4 的定性分析示例中，我们给出具体的代码执行过程，并增加更多错误原因分析，力求为进一步改进提供洞见。


#### D.5.1 Success and Failure Cases
#### D.5.1 成功与失败案例


Success Task: I downloaded an episode of Friends to practice listening, but I don't know how to remove the subtitles. Please help me remove the subtitles from the video and export it as "subtitles.srt" and store it in the same directory as the video.
成功任务：我下载了一集《Friends》来练习听力，但我不知道如何去除字幕。请帮我从视频中去除字幕，并导出为“subtitles.srt”并将其存放在与视频相同的目录中。


---



&nbsp;&nbsp;&nbsp;&nbsp;#Action 1
&nbsp;&nbsp;&nbsp;&nbsp;#行动 1


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import time
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import time


#Move the mouse to the top bar of the media player window to access the menu
#将鼠标移动到媒体播放器窗口的顶部栏以访问菜单


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.moveTo(800, 10)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.moveTo(800, 10)


&nbsp;&nbsp;&nbsp;&nbsp;#Assuming the menu is at the top of the screen
&nbsp;&nbsp;&nbsp;&nbsp;#假设菜单在屏幕顶部


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click()
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click()


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)
&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)


&nbsp;&nbsp;&nbsp;&nbsp;#Action 2
&nbsp;&nbsp;&nbsp;&nbsp;#动作 2


#Move the mouse to the Activities corner to open the GNOME Activities Overview
#将鼠标移到 Activities 角以打开 GNOME Activities 概览


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.moveTo(10, 10)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.moveTo(10, 10)


&nbsp;&nbsp;&nbsp;&nbsp;#Assuming the Activities corner is at the top-left
&nbsp;&nbsp;&nbsp;&nbsp;#假设 Activities 角在左上角


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click()
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click()


#Type `terminal' to search for the Terminal application
#输入 `terminal' 以搜索 Terminal 应用


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write('terminal')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write('terminal')


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)
&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)


&nbsp;&nbsp;&nbsp;&nbsp;#Press Enter to open the Terminal application
&nbsp;&nbsp;&nbsp;&nbsp;#按 Enter 打开 Terminal 应用


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press('enter')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press('enter')


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#动作 3


&nbsp;&nbsp;&nbsp;&nbsp;#Focus the Terminal window and type the command to extract subtitles
&nbsp;&nbsp;&nbsp;&nbsp;#聚焦 Terminal 窗口并输入命令以提取字幕


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(640, 360)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(640, 360)


&nbsp;&nbsp;&nbsp;&nbsp;#Clicking in the center of the screen to focus the Terminal
&nbsp;&nbsp;&nbsp;&nbsp;#在屏幕中心点击以聚焦 Terminal


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)
&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write('ffmpeg -i video.mp4 -map 0:s:0 subtitles.srt')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write('ffmpeg -i video.mp4 -map 0:s:0 subtitles.srt')


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press('enter')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press('enter')


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 4
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 4


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Focus the Terminal window and type the command
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#聚焦终端窗口并输入命令


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to create a new video without subtitles
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建一个无字幕的新视频


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(1100, 350)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(1100, 350)


&nbsp;&nbsp;&nbsp;&nbsp;#Clicking inside the Terminal window to focus it
&nbsp;&nbsp;&nbsp;&nbsp;#在终端窗口内点击以聚焦


pyautogui.write(`ffmpeg -i video.mp4 -c copy -sn no_subtitles_video.mp4')
pyautogui.write(`ffmpeg -i video.mp4 -c copy -sn no_subtitles_video.mp4')


&nbsp;&nbsp;&nbsp;&nbsp;Done!
&nbsp;&nbsp;&nbsp;&nbsp;完成！


---



Failure Task: Help me center align the heading in LibreOffice.
失败任务：请帮我在 LibreOffice 中将标题居中对齐。


---



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 1


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import time
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import time


#Move the mouse to the LibreOffice Writer document icon and double-click
#将鼠标移动到 LibreOffice Writer 文档图标并双击


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.moveTo(1100, 720, duration=1)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.moveTo(1100, 720, duration=1)


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.doubleClick()
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.doubleClick()


&nbsp;&nbsp;&nbsp;&nbsp;#Action 2
&nbsp;&nbsp;&nbsp;&nbsp;#Action 2


&nbsp;&nbsp;&nbsp;&nbsp;#Move the mouse to the beginning of the heading text
&nbsp;&nbsp;&nbsp;&nbsp;#将鼠标移到标题文本的开头


&nbsp;&nbsp;&nbsp;&nbsp;and click and drag to select the heading
&nbsp;&nbsp;&nbsp;&nbsp;并按住并拖动以选中标题


pyautogui.moveTo(200, 230, duration=1)
pyautogui.moveTo(200, 230, duration=1)


pyautogui.mouseDown()
pyautogui.mouseDown()


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.moveTo(800, 230, duration=1)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.moveTo(800, 230, duration=1)


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.mouseUp()
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.mouseUp()


#Move the mouse to the center align button in the toolbar and click
将鼠标移到工具栏中的居中对齐按钮并点击


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.moveTo(400, 150, duration=1)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.moveTo(400, 150, duration=1)


---



---



pyautogui.click()
pyautogui.click()


&nbsp;&nbsp;&nbsp;&nbsp;#Action 3
&nbsp;&nbsp;&nbsp;&nbsp;##动作 3


&nbsp;&nbsp;&nbsp;&nbsp;#Assuming the text is still selected,
&nbsp;&nbsp;&nbsp;&nbsp;#假设文本仍然被选中，


&nbsp;&nbsp;&nbsp;&nbsp;move the mouse to the center align button in the toolbar and click
&nbsp;&nbsp;&nbsp;&nbsp;将鼠标移到工具栏中的居中对齐按钮并点击


pyautogui.moveTo(400, 150, duration=1)
pyautogui.moveTo(400, 150, duration=1)


pyautogui.click()
pyautogui.click()


#Action 4
#动作 4


&nbsp;&nbsp;&nbsp;&nbsp;#Move the mouse to the center align button in the toolbar and click
&nbsp;&nbsp;&nbsp;&nbsp;#将鼠标移到工具栏中的居中对齐按钮并点击


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.moveTo(340, 80, duration=1)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.moveTo(340, 80, duration=1)


pyautogui.click()
pyautogui.click()


&nbsp;&nbsp;&nbsp;&nbsp;Failed.
&nbsp;&nbsp;&nbsp;&nbsp;失败。


---



<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_fc2673.jpg"/>



Figure 13: The figure shows examples of usage errors in three types of software. The first row is for GIMP, the second row is for LibreOffice Calc, and the third row is for Google Chrome. Detailed code execution processes are provided below.
图 13：该图展示三类软件中用法错误的示例。第一行为 GIMP，第二行为 LibreOffice Calc，第三行为 Google Chrome。下面提供了详细的代码执行过程。


GIMP Task: Could you tone down the brightness of my photo?
GIMP 任务：你能把我的照片亮度调低吗？


---



&nbsp;&nbsp;&nbsp;&nbsp;#Action 1
&nbsp;&nbsp;&nbsp;&nbsp;#操作 1


&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui
&nbsp;&nbsp;&nbsp;&nbsp;进口 pyautogui


&nbsp;&nbsp;&nbsp;&nbsp;import time
&nbsp;&nbsp;&nbsp;&nbsp;导入 time


#Coordinates for the image icon on the desktop
#桌面上图标的坐标


#Adding half of the width and height to get the center
#在宽度和高度上加取中点


&nbsp;&nbsp;&nbsp;&nbsp;image_icon_x, image_icon_y = 1793 + 60, 920 + 17
&nbsp;&nbsp;&nbsp;&nbsp;image_icon_x, image_icon_y = 1793 + 60, 920 + 17


&nbsp;&nbsp;&nbsp;&nbsp;#Double-click on the image icon
&nbsp;&nbsp;&nbsp;&nbsp;#对图标进行双击


pyautogui.doubleClick(image_icon_x, image_icon_y)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.doubleClick(image_icon_x, image_icon_y)


&nbsp;&nbsp;&nbsp;&nbsp;#Action 2
&nbsp;&nbsp;&nbsp;&nbsp;#操作 2


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Coordinates for the `Colors' menu
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#“颜色”菜单的坐标


#Adding half of the width and height to get the center
#在宽度和高度上加取中点


colors_menu_x, colors_menu_y = 344 + 28, 64 + 12
colors_menu_x, colors_menu_y = 344 + 28, 64 + 12


#Click on the `Colors' menu
点击“Colors”菜单


pyautogui.click(colors_menu_x, colors_menu_y)
pyautogui.click(colors_menu_x, colors_menu_y)


---



---



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 3


#Coordinates for the `Brightness-Contrast...' option in the `Colors' dropdown
在“Colors”下拉菜单中为“Brightness-Contrast...”选项的坐标


&nbsp;&nbsp;&nbsp;&nbsp;#Assuming each menu item is approximately 28 pixels apart
&nbsp;&nbsp;&nbsp;&nbsp;##假设每个菜单项相距大约 28 像素


brightness_contrast_option_x, brightness_contrast_option_y
亮度对比选项 x, 亮度对比选项 y


$= {344} + {28},{64} + {12} + {28} * 6$



&nbsp;&nbsp;&nbsp;&nbsp;#Click on the `Brightness-Contrast...' option
&nbsp;&nbsp;&nbsp;&nbsp;##点击“Brightness-Contrast...”选项


pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)
pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)


#Action 4
#操作 4


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Coordinates for the `Cancel' button in the `Shadows-Highlights` dialog
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Shadows-Highlights 对话框中“取消”按钮的坐标


&nbsp;&nbsp;&nbsp;&nbsp;#Adding half the width and height to get the center
&nbsp;&nbsp;&nbsp;&nbsp;##为获取中心点，将宽度和高度的一半相加


cancel_button_x, cancel_button_y = 375 + 42, 625 + 16
cancel_button_x, cancel_button_y = 375 + 42, 625 + 16


#Click the 'Cancel' button
#点击“取消”按钮


pyautogui.click(cancel_button_x, cancel_button_y)
pyautogui.click(cancel_button_x, cancel_button_y)


#Coordinates for the `Brightness-Contrast...' option in the `Colors' dropdown
#在 Colors 下拉菜单中“Brightness-Contrast...”选项的坐标


#Adjusted index for Brightness-Contrast option
Brightness-Contrast 选项的索引已调整


brightness_contrast_option_x, brightness_contrast_option_y
brightness_contrast_option_x, brightness_contrast_option_y


$= {344} + {28},{64} + {12} + {28} * 5$



pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)
pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)


#Action 5
#操作 5


&nbsp;&nbsp;&nbsp;&nbsp;#Coordinates for the 'Cancel' button in the 'Exposure' dialog
&nbsp;&nbsp;&nbsp;&nbsp;#Exposure 对话框中“取消”按钮的坐标


#Adding half the width and height to get the center
将宽度和高度的一半加上以获得中心


&nbsp;&nbsp;&nbsp;&nbsp;cancel_button_x, cancel_button_y
&nbsp;&nbsp;&nbsp;&nbsp;cancel_button_x, cancel_button_y


$= {375} + {42},{391} + {16}$



pyautogui.click(cancel_button_x, cancel_button_y)
pyautogui.click(cancel_button_x, cancel_button_y)


#Coordinates for the `Brightness-Contrast...' option in the `Colors' dropdown
#Colors 菜单中“Brightness-Contrast...”选项的坐标


#Adjusted index for Brightness-Contrast option
#Brightness-Contrast 选项的调整后的索引


brightness_contrast_option_x, brightness_contrast_option_y
brightness_contrast_option_x, brightness_contrast_option_y


= 344 + 28, 64 + 12 + 28 \$*\$ 6



pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)
pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)


#Action 6
#操作 6


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Coordinates for the `Cancel' button in the `Color Balance' dialog
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Color Balance 对话框中“Cancel”按钮的坐标


#Adding half the width and height to get the center
将宽度和高度的一半加上以获得中心


cancel_button_x, cancel_button_y
cancel_button_x, cancel_button_y


$= {375} + {42},{588} + {16}$



pyautogui.click(cancel_button_x, cancel_button_y)
pyautogui.click(cancel_button_x, cancel_button_y)


#Since the `Brightness-Contrast...' option was not correctly opened before,
由于之前并未正确打开“Brightness-Contrast...”选项，


we will use an estimated position based on the dropdown menu.
我们将基于下拉菜单的估算位置


#Assuming `Brightness-Contrast...' is the second option in the `Colors' menu.
假设“Brightness-Contrast...”是“Colors”菜单中的第二个选项


brightness_contrast_option_x, brightness_contrast_option_y
brightness_contrast_option_x, brightness_contrast_option_y


= colors_menu_x, colors_menu_y + 28 * 2
= colors_menu_x, colors_menu_y + 28 * 2


pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)
pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)


&nbsp;&nbsp;&nbsp;&nbsp;#Action 7
&nbsp;&nbsp;&nbsp;&nbsp;#操作 7


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Coordinates for the `Cancel' button in the `Color Temperature' dialog
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#“颜色温度”对话框中“取消”按钮的坐标


&nbsp;&nbsp;&nbsp;&nbsp;#Adding half the width and height to get the center
&nbsp;&nbsp;&nbsp;&nbsp;#将宽度和高度的一半相加以得到中心


&nbsp;&nbsp;&nbsp;&nbsp;cancel_button_x, cancel_button_y = 375 + 42, 403 + 16
&nbsp;&nbsp;&nbsp;&nbsp;cancel_button_x, cancel_button_y = 375 + 42, 403 + 16


pyautogui.click(cancel_button_x, cancel_button_y)
pyautogui.click(cancel_button_x, cancel_button_y)


#Since the `Brightness-Contrast...' option was not correctly opened before,
#由于之前未正确打开“亮度对比度...”选项，


we will use an estimated position based on the dropdown menu.
我们将基于下拉菜单使用一个估计位置。


#Assuming `Brightness-Contrast...' is the second option in the `Colors' menu.
#假设“亮度对比度...”是“颜色”菜单中的第二个选项。


brightness_contrast_option_x, brightness_contrast_option_y
brightness_contrast_option_x, brightness_contrast_option_y


= colors_menu_x, colors_menu_y + 28 * 2
= colors_menu_x, colors_menu_y + 28 * 2


#Click on the `Brightness-Contrast...' option
#点击“亮度对比度...”选项


pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)
pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)


#... More Wrong Duplicate Actions; Can not find the right coordination.
#... 更多错误重复操作；无法找到正确的坐标。


#Failed
#失败


---



Libreoffice calc Task: I now want to count the meeting cities of the three machine learning conferences in the past ten years from 2013 to 2019(including 2013 and 2019). I have listed the names and years of the conferences in excel. Please fill in the vacant locations.
Libreoffice calc 任务：现在我想统计过去十年（包含2013年到2019年）的三场机器学习会议的会议信息城市。我已将会议名称与年份列在 Excel 中，请填写空缺的城市。


---



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 1


#Move to cell C2 which is the first empty cell under City' for ICLR 2013
#移动到单元格 C2，即 ICLR 2013 的第一个空单元格的 City


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(319, 222)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(319, 222)


#Coordinates of cell C2
#单元格 C2 的坐标


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)
&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)


#Type the city name for ICLR 2013
#输入 ICLR 2013 的城市名称


pyautogui.write('Scottsdale')
pyautogui.write('Scottsdale')


&nbsp;&nbsp;&nbsp;&nbsp;#Press Enter to move to the next cell
&nbsp;&nbsp;&nbsp;&nbsp;#按回车移动到下一个单元格


pyautogui.press('enter')
pyautogui.press('enter')


&nbsp;&nbsp;&nbsp;&nbsp;#Action 2
&nbsp;&nbsp;&nbsp;&nbsp;#Action 2


&nbsp;&nbsp;&nbsp;&nbsp;#Move to cell C3 which is the city cell for ICML 2013
&nbsp;&nbsp;&nbsp;&nbsp;#移动到 C3 单元格，即 ICML 2013 的城市单元格


pyautogui.click(319, 268)
pyautogui.click(319, 268)


&nbsp;&nbsp;&nbsp;&nbsp;#Coordinates of cell C3
&nbsp;&nbsp;&nbsp;&nbsp;#C3 的坐标


#Type the city name for ICML 2013
#输入 ICML 2013 的城市名称


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write('Atlanta')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write('Atlanta')


#Press Enter to move to the next cell
按 Enter 以移动到下一个单元格


pyautogui.press('enter')
pyautogui.press('enter')


&nbsp;&nbsp;&nbsp;&nbsp;#Action 3
&nbsp;&nbsp;&nbsp;&nbsp;#Action 3


#Move to cell C4 which is the city cell for NeurIPS 2013
#移动到 C4 单元格，即 NeurIPS 2013 的城市单元格


pyautogui.click(319, 289)
pyautogui.click(319, 289)


&nbsp;&nbsp;&nbsp;&nbsp;#Coordinates of cell C4
&nbsp;&nbsp;&nbsp;&nbsp;#C4 的坐标


#Type the city name for NeurIPS 2013
#输入 NeurIPS 2013 的城市名称


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write(`Lake Tahoe')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write(`Lake Tahoe')


&nbsp;&nbsp;&nbsp;&nbsp;#Press Enter to move to the next cell
&nbsp;&nbsp;&nbsp;&nbsp;#按 Enter 以移动到下一个单元格


pyautogui.press('enter')"
pyautogui.press('enter')"


&nbsp;&nbsp;&nbsp;&nbsp;... More copy-and-paste operations
&nbsp;&nbsp;&nbsp;&nbsp;... 更多的复制粘贴操作


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 15
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 15


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Move to cell C16 which is the city cell for NeurIPS 2017
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#移动到 C16 单元格，即 NeurIPS 2017 的城市单元格


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(319, 541)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(319, 541)


#Coordinates of cell C16
#C16 的坐标


#Type the city name for NeurIPS 2017
#输入 NeurIPS 2017 的城市名称


pyautogui.write('Long Beach')
pyautogui.write('Long Beach')


#Press Enter to move to the next cell
按 Enter 键移动到下一个单元格


pyautogui.press('enter')
pyautogui.press('enter')


Failed.
失败。


---



Chrome Task: Can you make Bing the main search thingy when I look stuff up on the internet?
Chrome 任务：在我上网时能否把 Bing 设置为主要搜索引擎？


---



#Action 1
#行动 1


pyautogui.click(x=1280, y=50)
pyautogui.click(x=1280, y=50)


#Approximate coordinates for the `More` button
#近似的 More 按钮坐标


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.move(0, 150)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.move(0, 150)


#Moving the cursor down to `Settings' in the drop-down menu
#将光标移动到下拉菜单中的 Settings


pyautogui.click()
pyautogui.click()


#Clicking on `Settings'
#点击 Settings


&nbsp;&nbsp;&nbsp;&nbsp;#Action 2
&nbsp;&nbsp;&nbsp;&nbsp;#行动 2


pyautogui.click(tag_31)
pyautogui.click(tag_31)


#Click on the `Finish update' button to
点击“完成更新”按钮以


approximate the location of `More' button
近似 More 按钮的位置


pyautogui.move(-50, 0)
pyautogui.move(-50, 0)


#Move left from the `Finish update' button to approximate the `More' button
从“Finish update”按钮向左移动，接近“More”按钮


---



---



pyautogui.click()
pyautogui.click()


#Click on the `More` button
点击“More”按钮


time.sleep(0.5)
time.sleep(0.5)


pyautogui.move(0, 150)
pyautogui.move(0, 150)


#Move down to the 'Settings' option in the drop-down menu
向下移动到下拉菜单中的“Settings”选项


pyautogui.click()
pyautogui.click()


#Click on `Settings'
点击“Settings”


#Action 3
#Action 3


#Click on the `Customise Chrome' button which should be the `More' button
点击“Customise Chrome”按钮，应为“More”按钮


time.sleep(1)
time.sleep(1)


#Wait for the dropdown menu to appear
等待下拉菜单出现


#The `Settings' option is usually located towards the bottom of
“Settings”选项通常位于下拉菜单的底部附近


the dropdown menu, so I'll move the cursor down to reach it
因此我将光标向下移动以到达它


pyautogui.move(0, 200)
pyautogui.move(0, 200)


#Move down to the `Settings' option in the drop-down menu
向下移动到下拉菜单中的“Settings”选项


pyautogui.click()
pyautogui.click()


#Click on `Settings'
点击“Settings”


#Action 4
行动 4


pyautogui.click(tag_47)
pyautogui.click(tag_47)


#Click on the `Close' button to close the `Customise Chrome' menu
点击“Close”按钮以关闭“Customise Chrome”菜单


pyautogui.click(tag_25)
pyautogui.click(tag_25)


#Click on the `New Tab' button to open a new tab
点击“New Tab”按钮以打开新标签页


time.sleep(1)
time.sleep(1)


#Wait for the new tab to open
等待新标签页打开


pyautogui.typewrite('chrome://settings/searchEngines')
pyautogui.typewrite('chrome://settings/searchEngines')


#Type the direct URL to the search engine settings page
输入前往搜索引擎设置页面的直接 URL


pyautogui.press('enter')
pyautogui.press('enter')


#Action 5
行动 5


#Simulate pressing the `Tab' key multiple times to navigate through the options
模拟多次按下“Tab”键以在选项之间导航


for _ in range(10):
for _ in range(10):


#Assuming it might take around 10 tabs to reach the search engine list
#假设大约需要 10 次制表进入搜索引擎列表


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press('tab')
&nbsp;&nbsp;&nbsp;&nbsp;\tpyautogui.press('tab')


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)
&nbsp;&nbsp;&nbsp;&nbsp;\ttime.sleep(0.5)


#Wait for the focus to move to the next element
#等待焦点移动到下一个元素


#Simulate pressing the `Enter' key to select an option,
#模拟按下 `Enter' 键以选择一个选项，


which would be done once we reach the Bing option
在达到 Bing 选项后将执行


pyautogui.press('enter')
pyautogui.press('enter')


#Action 6
#操作 6


pyautogui.click(tag_42)
pyautogui.click(tag_42)


#Click on the `Cancel' button to exit the edit mode for the '360' search engine
#点击“取消”按钮以退出对 \'360\' 搜索引擎的编辑模式


Failed.
失败。


---



#### D.5.2 Common Errors by GPT-4V Agents
#### D.5.2 GPT-4V 代理的常见错误


In the following two figures (Figure 15 and Figure 14), we take the web page tasks as examples to show the two most common types of errors in GPT4-V: mouse click inaccuracies and inadequate handling of environmental noise.
在下图的两张图（图15和图14）中，我们以网页任务为例，展示 GPT4-V 的两种最常见错误类型：鼠标点击不准确和对环境噪声处理不足。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_4d1f8e.jpg"/>



Figure 14: The error shown in the figure is due to mouse click inaccuracies. The agent was supposed to click on the product category images of the Nike official store and follow the instructions to search for women's jerseys priced over 60 dollars. However, due to a coordinate calculation error, it mistakenly clicked on the favorite button, creating a significant discrepancy between the current state and the target state. Under these circumstances, the agent is unable to backtrack to the previous state and start over.
图14：图中错误是由于鼠标点击不准确所致。代理本应点击 Nike 官方商店的产品类别图片并按指示搜索价格超过 60 美元的女士球衣。但由于坐标计算错误，错误地点击了收藏按钮，导致当前状态与目标状态之间产生了显著差异。在这种情况下，代理无法回退到上一状态并重新开始。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_71b944.jpg"/>



Figure 15: The error shown in the figure is due to inadequate handling of environmental noise. When the agent encounters web pages with cookies or pop-up ads, its unfamiliarity with the structure of such pages leads to attempts to interact directly with the web page without bypassing the cookies or pop-ups. Since the pop-ups or cookie requests are not closed, all actions are ineffective.
图15：图中错误是由于对环境噪声处理不足。当代理遇到带有 Cookies 或弹出广告的网页时，其对此类网页结构的不熟悉导致直接与网页互动而不绕过 Cookies 或弹出广告。由于弹出窗口或 Cookies 请求未被关闭，所有操作都无效。


#### D.5.3 Discrepancies in Task Difficulty between Agent and Human
#### D.5.3 代理与人类在任务难度上的差异


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_da98c3.jpg"/>



Figure 16: Supplementary examples of tasks performed by humans and agents.
图16：人类与代理执行任务的补充示例。


Human > Agent Task: erase all the highlighted marks in this document
人类 > 代理任务：清除本文档中所有高亮标记


---



#Action 1
#动作 1


&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui
&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui


&nbsp;&nbsp;&nbsp;&nbsp;import time
&nbsp;&nbsp;&nbsp;&nbsp;import time


#Move to the `Character Highlighting Color' button and click
#移动到“字符高亮颜色”按钮并点击


pyautogui.moveTo(1020, 122)
pyautogui.moveTo(1020, 122)


pyautogui.click()
pyautogui.click()


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 2
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#动作 2


&nbsp;&nbsp;&nbsp;&nbsp;#Assuming the `No Color' option is the first option in the dropdown
&nbsp;&nbsp;&nbsp;&nbsp;#假设下拉菜单中的第一个选项为“无颜色”


#Move the mouse slightly below the 'Character Highlighting Color' button
#将鼠标略微向下移动到“字符高亮颜色”按钮下方


&nbsp;&nbsp;&nbsp;&nbsp;to click on `No Color'
&nbsp;&nbsp;&nbsp;&nbsp;点击“不显示颜色”


pyautogui.moveTo(1020, 180)
pyautogui.moveTo(1020, 180)


time.sleep(0.5)
time.sleep(0.5)


pyautogui.click()
pyautogui.click()


&nbsp;&nbsp;&nbsp;&nbsp;#Action 3
&nbsp;&nbsp;&nbsp;&nbsp;#动作 3


#Move to the `Save` button and click
前往“保存”按钮并点击


pyautogui.moveTo(184, 108)
pyautogui.moveTo(184, 108)


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)
&nbsp;&nbsp;&nbsp;&nbsp;\time.sleep(0.5)


pyautogui.click()
pyautogui.click()


#Failed.
失败。


---



Human > Agent Task: On page 2, add a note same to the title into the slide. Make the font of title bold.
Human > Agent Task: 在第2页，添加一个与标题相同的备注到幻灯片中。将标题字体设为粗体。


---



&nbsp;&nbsp;&nbsp;&nbsp;#Action 1
&nbsp;&nbsp;&nbsp;&nbsp;#\t#Action 1


#Click on the vertical scrollbar to scroll down and reveal more slides
#点击纵向滚动条向下滚动以显示更多幻灯片


pyautogui.click(tag_43)
pyautogui.click(tag_43)


&nbsp;&nbsp;&nbsp;&nbsp;#Action 2
&nbsp;&nbsp;&nbsp;&nbsp;#\t#Action 2


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Click on the second slide thumbnail to select it
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#                    #点击第二张幻灯片缩略图以选中它


---



---



pyautogui.click(tag_2)
pyautogui.click(tag_2)


&nbsp;&nbsp;&nbsp;&nbsp;Action 3
&nbsp;&nbsp;&nbsp;&nbsp;Action 3


#Click on the text box to add text
#点击文本框以添加文本


pyautogui.click(tag_42)
pyautogui.click(tag_42)


pyautogui.typewrite('Team Members', interval=0.1)
pyautogui.typewrite('Team Members', interval=0.1)


#Type the title
#输入标题


pyautogui.hotkey('ctrl', 'a')
pyautogui.hotkey('ctrl', 'a')


#Select all text
#全选文本


pyautogui.hotkey('ctrl', 'b')
pyautogui.hotkey('ctrl', 'b')


#Make the text bold
#将文本设为粗体


#Action 4
#操作 4


#Click on the Bold button to apply bold formatting
#点击“粗体”按钮以应用粗体格式


pyautogui.click(tag_68)
pyautogui.click(tag_68)


Failed.
失败。


---



Agent > Human Task: use GIMP to cut out the 2s to 4s part of a video
代理 > 人工任务：用 GIMP 将视频的 2s 至 4s 部分切出


---



#Action 1
#操作 1


&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui
&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import time
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import time


&nbsp;&nbsp;&nbsp;&nbsp;#Move the mouse to the assumed terminal icon position
&nbsp;&nbsp;&nbsp;&nbsp;##将鼠标移至假定的终端图标位置


&nbsp;&nbsp;&nbsp;&nbsp;at the bottom of the dock and click to open it
&nbsp;&nbsp;&nbsp;&nbsp;在 Dock 底部点击以打开它


#Assuming the terminal icon is at the bottom of the dock
#假设终端图标在 Dock 底部


pyautogui.moveTo(70, 1350, duration=1)
pyautogui.moveTo(70, 1350, duration=1)


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click()
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click()


&nbsp;&nbsp;&nbsp;&nbsp;#Type the command to use ffmpeg to trim the video
&nbsp;&nbsp;&nbsp;&nbsp;#输入使用 ffmpeg 修剪视频的命令


ffmpeg_command = `ffmpeg -ss 00:00:02 -to 00:00:04 -i /home/user/fullvideo.mp4
ffmpeg_command = `ffmpeg -ss 00:00:02 -to 00:00:04 -i /home/user/fullvideo.mp4


&nbsp;&nbsp;&nbsp;&nbsp;-c copy /home/user/trimmedvideo.mp4'
&nbsp;&nbsp;&nbsp;&nbsp;-c copy /home/user/trimmedvideo.mp4'


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write(ffmpeg_command, interval=0.05)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write(ffmpeg_command, interval=0.05)


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press('enter')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press('enter')


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 2
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#动作 2


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Close the search dialog
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#关闭搜索对话框


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press('esc')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press('esc')


#Open the terminal using a keyboard shortcut (Ctrl+Alt+T is a common shortcut
#使用键盘快捷键打开终端（Ctrl+Alt+T 是常见的快捷键


for opening a terminal in many Linux distributions)
用于在许多 Linux 发行版中打开终端）


pyautogui.hotkey('ctrl', 'alt', 't')
pyautogui.hotkey('ctrl', 'alt', 't')


&nbsp;&nbsp;&nbsp;&nbsp;#Action 3
&nbsp;&nbsp;&nbsp;&nbsp;#动作 3


#Move the mouse to the terminal and click to focus on the terminal window
#将鼠标移到终端并点击以聚焦终端窗口


pyautogui.moveTo(260 + 722/2, 103 + 434/2, duration=1)
pyautogui.moveTo(260 + 722/2, 103 + 434/2, duration=1)


pyautogui.click()
pyautogui.click()


#Type the command to use ffmpeg to trim the video
#输入命令以使用 ffmpeg 裁剪视频


pyautogui.write(ffmpeg_command, interval=0.05)
pyautogui.write(ffmpeg_command, interval=0.05)


pyautogui.press('enter')
pyautogui.press('enter')


#Done, but not follow the demanding in instruction.
#完成，但未遵循指令中的要求。


---



Agent > Human Task: I want to understand the resource usage of my Ubuntu system under normal workloads. Please use the 'sar' command in the 'sysstat' toolkit to monitor system activity, evaluate the status once every second for 30 seconds, output the results to "System Resources Report.txt" under Desktop.
Agent > Human Task: 我想了解在正常工作负载下我的 Ubuntu 系统的资源使用情况。请使用 sysstat 工具包中的 'sar' 命令监控系统活动，每秒输出一次结果，持续 30 秒，将结果输出到桌面的 "System Resources Report.txt"。


---



#Action 1
#操作 1


import pyautogui
import pyautogui


import time
import time


---



---



#Press the keyboard shortcut to open the terminal
#按下快捷键打开终端


pyautogui.hotkey('ctrl', 'alt', 't')
pyautogui.hotkey('ctrl', 'alt', 't')


time.sleep(1)
time.sleep(1)


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Wait a moment for the terminal to open
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#等待终端打开


#Action 2
#操作 2


#Click inside the terminal to focus on it
#在终端中点击以聚焦


pyautogui.click(600, 300)
pyautogui.click(600, 300)


#Assuming the terminal is in the upper left area of the screen
#假设终端在屏幕的左上角


n# Type the command to monitor system activity
n# 输入监控系统活动的命令


pyautogui.typewrite('sar 1 30 > "/Desktop/System Resources Report.txt')
pyautogui.typewrite('sar 1 30 > "/Desktop/System Resources Report.txt')


#Action 3
#操作 3


#Click on the 'System Resources Report.txt' file to select it
#单击“System Resources Report.txt”文件以选择


pyautogui.click(1850, 940)
pyautogui.click(1850, 940)


&nbsp;&nbsp;&nbsp;&nbsp;#Coordinates taken from the label position and size
&nbsp;&nbsp;&nbsp;&nbsp;#坐标取自标签位置和尺寸


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)
&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)


&nbsp;&nbsp;&nbsp;&nbsp;#Right-click to open the context menu
&nbsp;&nbsp;&nbsp;&nbsp;#右击以打开上下文菜单


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(1850, 940, button=`right')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(1850, 940, button=`right')


#Move to the `Open With Other Application' option in the context menu
#在上下文菜单中移动到“与其他应用程序打开”选项


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.move(0, 70)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.move(0, 70)


#Assuming 'Open With Other Application' is 70 pixels
#假设“与其他应用程序打开”距右击位置 70 像素


&nbsp;&nbsp;&nbsp;&nbsp;down from the right-click position
&nbsp;&nbsp;&nbsp;&nbsp;下移自右击位置


&nbsp;&nbsp;&nbsp;&nbsp;#Click on `Open With Other Application'
&nbsp;&nbsp;&nbsp;&nbsp;#单击“与其他应用程序打开”


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click()
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click()


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)
&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)


#Type `gedit' to select the text editor
#Type `gedit' to select the text editor


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.typewrite('gedit')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.typewrite('gedit')


&nbsp;&nbsp;&nbsp;&nbsp;#Press `Enter' to open the file with gedit
&nbsp;&nbsp;&nbsp;&nbsp;#Press `Enter' to open the file with gedit


pyautogui.press(`enter')
pyautogui.press(`enter')


#Action 4
#Action 4


&nbsp;&nbsp;&nbsp;&nbsp;#Click inside the terminal to ensure it's in focus
&nbsp;&nbsp;&nbsp;&nbsp;#Click inside the terminal to ensure it's in focus


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(500, 200)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(500, 200)


#Coordinates are estimated to be within the terminal window
#Coordinates are estimated to be within the terminal window


&nbsp;&nbsp;&nbsp;&nbsp;#Type the command to open the file with gedit
&nbsp;&nbsp;&nbsp;&nbsp;#Type the command to open the file with gedit


pyautogui.typewrite('gedit "/Desktop/System Resources Report.txt')
pyautogui.typewrite('gedit "/Desktop/System Resources Report.txt')


Done!
Done!


---



Agent > Human Task: Hey, my LibreOffice Writer seems to have frozen and I can't get it to close normally. Can you help me force quit the application from the command line? I'm on Ubuntu and I don't want to restart my computer or lose any other work I have open.
Agent > Human Task: 嘿，我的 LibreOffice Writer 似乎卡死了，无法正常关闭。你能不能帮我从命令行强制结束该应用？我在 Ubuntu 上，不想重启电脑或丢失其他打开的工作。


---



#Action 1
#Action 1


&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui
&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui


&nbsp;&nbsp;&nbsp;&nbsp;import time
&nbsp;&nbsp;&nbsp;&nbsp;import time


&nbsp;&nbsp;&nbsp;&nbsp;#Open a terminal window
&nbsp;&nbsp;&nbsp;&nbsp;#	打开一个终端窗口


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.hotkey('ctrl', 'alt', 't')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.hotkey('ctrl', 'alt', 't')


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(1)
&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(1)


#Type the command to kill all instances of LibreOffice Writer
&nbsp;&nbsp;&nbsp;&nbsp;#Type the command to kill all instances of LibreOffice Writer


pyautogui.typewrite('killall soffice.bin')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.typewrite('killall soffice.bin')


#Action 2
&nbsp;&nbsp;&nbsp;&nbsp;#Action 2


#Open a terminal window
&nbsp;&nbsp;&nbsp;&nbsp;#Open a terminal window


pyautogui.hotkey('ctrl', 'alt', 't')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.hotkey('ctrl', 'alt', 't')


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(1)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(1)


#Type the command to kill all instances of LibreOffice Writer
&nbsp;&nbsp;&nbsp;&nbsp;#Type the command to kill all instances of LibreOffice Writer


pyautogui.typewrite('killall soffice.bin')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.typewrite('killall soffice.bin')


#Action 3
&nbsp;&nbsp;&nbsp;&nbsp;#Action 3


&nbsp;&nbsp;&nbsp;&nbsp;#Ensure the terminal window is focused
&nbsp;&nbsp;&nbsp;&nbsp;#确保终端窗口获得焦点


pyautogui.click(500, 200)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(500, 200)


&nbsp;&nbsp;&nbsp;&nbsp;#Assuming the terminal is at this position
&nbsp;&nbsp;&nbsp;&nbsp;#假设终端在此位置


#Type the pkill command to forcefully kill the LibreOffice Writer application
&nbsp;&nbsp;&nbsp;&nbsp;#Type the pkill command to forcefully kill the LibreOffice Writer application


pyautogui.typewrite('pkill -9 soffice.bin')
pyautogui.typewrite('pkill -9 soffice.bin')


Done!
完成！


---



#### D.5.4 Comparative Analysis: Claude-3 vs. GPT-4V
#### D.5.4 比较分析：Claude-3 与 GPT-4V


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_5fa222.jpg"/>



Figure 17: Screenshots of error example analysis of Claude-3 Opus.
图 17：Claude-3 Opus 错误示例分析的截图。


Claude3 Failed Task1: I remember there is a file named "secret.docx" on this computer, but I can't remember where it is. Please find the path where this file is stored and copy it to the clipboard.
Claude3 失败任务 1：我记得这台电脑上有一个名为 "secret.docx" 的文件，但我不记得它的位置。请找出该文件的路径并将其复制到剪贴板。


---



#Action 1
#操作 1


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(tag_10)
&nbsp;&nbsp;&nbsp;&nbsp;\tpyautogui.click(tag_10)


&nbsp;&nbsp;&nbsp;&nbsp;#click the Files icon to open the file manager
&nbsp;&nbsp;&nbsp;&nbsp;#\t#点击 文件图标以打开文件管理器


&nbsp;&nbsp;&nbsp;&nbsp;#Action 2
&nbsp;&nbsp;&nbsp;&nbsp;##操作 2


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(tag_21)
&nbsp;&nbsp;&nbsp;&nbsp;\tpyautogui.click(tag_21)


&nbsp;&nbsp;&nbsp;&nbsp;#click the search icon to open the search bar
&nbsp;&nbsp;&nbsp;&nbsp;#\t#点击 搜索图标以打开搜索栏


&nbsp;&nbsp;&nbsp;&nbsp;#Action 3
&nbsp;&nbsp;&nbsp;&nbsp;##操作 3


pyautogui.typewrite('secret.docx')
pyautogui.typewrite('secret.docx')


#type the file name to search for
#输入要搜索的文件名


pyautogui.press('enter')
pyautogui.press('enter')


&nbsp;&nbsp;&nbsp;&nbsp;#press enter to start the search
&nbsp;&nbsp;&nbsp;&nbsp;#\t#按回车开始搜索


&nbsp;&nbsp;&nbsp;&nbsp;#Action 4
&nbsp;&nbsp;&nbsp;&nbsp;##动作 4


pyautogui.doubleClick(tag_52)
pyautogui.doubleClick(tag_52)


#double click the secret.docx file to select it
#双击 secret.docx 文件以选中


pyautogui.hotkey('ctrl', 'c')
pyautogui.hotkey('ctrl', 'c')


#press Ctrl+C to copy the selected file path to clipboard
#按 Ctrl+C 将所选文件路径复制到剪贴板


Failed.
失败。


---



Claude3 Failed Task2: copy the data in Cell B6 in this Libreoffice Calc file and search it in the Chrome browser.
Claude3 任务失败 2：将在这个 LibreOffice Calc 文件中选中的单元格 B6 的数据复制出来并在 Chrome 浏览器中搜索。


---



#Action 1
#动作 1


&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui
&nbsp;&nbsp;&nbsp;&nbsp;导入 pyautogui


import time
导入 time


&nbsp;&nbsp;&nbsp;&nbsp;#Scroll down to view more search results if needed
&nbsp;&nbsp;&nbsp;&nbsp;#如有需要，向下滚动以查看更多搜索结果


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press(`pagedown')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press(`pagedown')


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(1)
&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(1)


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press('pagedown')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press('pagedown')


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(2)
&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(2)


#The answer appears to be in the snippet under the first search result
#答案似乎出现在第一条搜索结果下的片段中


#`TV 31. u76f4u64adu4e2d.' which translates to `TV 31. Live.'
#`TV 31. u76f4u64adu4e2d.' 翻译为 `TV 31. Live.'


#So channel 31 in Hong Kong is RTHK TV 31
#所以香港的频道31是 RTHK TV 31


print(`Found the answer. Channel 31 in Hong Kong is RTHK TV 31.')
print(`Found the answer. Channel 31 in Hong Kong is RTHK TV 31.')


#The original task of copying cell B36 from the spreadsheet
#将单元格 B36 复制到电子表格中的原始任务


and searching it in the browser is complete
并在浏览器中搜索完成


print('Task complete.')
print('任务完成。')


#Action 2
#操作 2


&nbsp;&nbsp;&nbsp;&nbsp;#Activate the Chrome browser window
&nbsp;&nbsp;&nbsp;&nbsp;#激活 Chrome 浏览器窗口


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(tag_3)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(tag_3)


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(1)
&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(1)


#Click on the address/search bar
#点击地址/搜索栏


pyautogui.click(tag_30)
pyautogui.click(tag_30)


#Paste the copied value `Dewitt'
#粘贴已复制的值 `Dewitt'


pyautogui.hotkey('ctrl', 'v')
pyautogui.hotkey('ctrl', 'v')


#Press Enter to perform the search
#按回车执行搜索


pyautogui.press('enter')
pyautogui.press('enter')


print('Searched for `Dewitt' in the Chrome browser.')
在 Chrome 浏览器中搜索了 `Dewitt'。


#Action 3
#Action 3


print (`DONE')
print (`DONE`)


#Action 4
#Action 4


print (`DONE')
print (`DONE`)


Failed.
失败。


---



Claude3 Failed Task3: Please help me change all the places in this document that say 'text' to 'test'.
Claude3 任务3 失败：请帮我把文档中所有写着 'text' 的地方都改成 'test'。


---



&nbsp;&nbsp;&nbsp;&nbsp;#Action 1
&nbsp;&nbsp;&nbsp;&nbsp;#\t#Action 1


pyautogui.hotkey(`ctrl', `h')
pyautogui.hotkey(`ctrl', `h`)


pyautogui.write('text')
pyautogui.write('text')


pyautogui.press('tab')
pyautogui.press('tab')


pyautogui.write('test')
pyautogui.write('test')


pyautogui.press('enter')
pyautogui.press('enter')


&nbsp;&nbsp;&nbsp;&nbsp;#Action 2
&nbsp;&nbsp;&nbsp;&nbsp;#\t#Action 2


pyautogui.press(`enter')
pyautogui.press(`enter`)


#Action 3
#Action 3


pyautogui.hotkey('ctrl', 's')
pyautogui.热键('ctrl', 's')


Failed.
失败。


---



### D.6 Analysis on SoM Effectiveness
### D.6 对 SoM 效能的分析


Task w/o SoM: turning off the code checking function of the python plugin in vscode
无 SoM 的任务：在 vscode 中关闭 Python 插件的代码检查功能


---



#Action 1
# 操作 1


&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui
&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui


&nbsp;&nbsp;&nbsp;&nbsp;import time
&nbsp;&nbsp;&nbsp;&nbsp;import time


#Move the mouse to the gear icon (Manage) and click it
#将鼠标移动到齿轮图标（管理）并点击


pyautogui.moveTo(472, 1034, duration=0.5)
pyautogui.moveTo(472, 1034, duration=0.5)


pyautogui.click()
pyautogui.click()


&nbsp;&nbsp;&nbsp;&nbsp;#Action 2
&nbsp;&nbsp;&nbsp;&nbsp;#操作 2


&nbsp;&nbsp;&nbsp;&nbsp;#Move the mouse to the Settings" menu item and click it
&nbsp;&nbsp;&nbsp;&nbsp;#将鼠标移动到 设置 菜单项并点击


#A rough central point of the 'Settings' menu item
#“设置”菜单项的粗略中心点


pyautogui.moveTo(497 + 385/2, 832 + 25/2, duration=0.5)
pyautogui.moveTo(497 + 385/2, 832 + 25/2, duration=0.5)


pyautogui.click()
pyautogui.click()


&nbsp;&nbsp;&nbsp;&nbsp;#Action 3
&nbsp;&nbsp;&nbsp;&nbsp;#操作 3


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Move the mouse to the search settings bar and click it
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#将鼠标移动到搜索设置栏并点击


#A rough central point of the search settings bar
搜索设置栏的一个大致中心点


pyautogui.moveTo(783 + 587/2, 390 + 19/2, duration=0.5)
pyautogui.moveTo(783 + 587/2, 390 + 19/2, duration=0.5)


pyautogui.click()
pyautogui.click()


#Type the query to search for Python-related settings
输入查询以搜索与 Python 相关的设置


pyautogui.write('python', interval=0.05)
pyautogui.write('python', interval=0.05)


&nbsp;&nbsp;&nbsp;&nbsp;#Action 4
&nbsp;&nbsp;&nbsp;&nbsp;#操作 4


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Move the mouse to the `Edit in settings.json' button and click it
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#将鼠标移动到“在 settings.json 中编辑”按钮并点击


#A rough central point of the button
按钮的大致中心点


pyautogui.moveTo(990 + 116/2, 624 + 15/2, duration=0.5)
pyautogui.moveTo(990 + 116/2, 624 + 15/2, duration=0.5)


pyautogui.click()
pyautogui.click()


#Action 5
#操作 5


#Move the cursor to the correct position
#将光标移动到正确的位置


pyautogui.moveTo(889 + 1/2, 432 + 1/2, duration=0.5)
pyautogui.moveTo(889 + 1/2, 432 + 1/2, duration=0.5)


pyautogui.click()
pyautogui.click()


&nbsp;&nbsp;&nbsp;&nbsp;#Type the configuration to disable Python missing imports error reporting
&nbsp;&nbsp;&nbsp;&nbsp;#输入配置以禁用 Python 缺失导入错误的报告


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write('reportMissingImports: `none'', interval=0.05)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write('reportMissingImports: `none'', interval=0.05)


pyautogui.press('enter')
pyautogui.press('enter')


#Save the changes
#保存更改


pyautogui.hotkey('ctrl', 's')
pyautogui.hotkey('ctrl', 's')


#Done
#完成


---



Task w/ SoM: turning off the code checking function of the python plugin in vscode
带有 SoM 的任务：在 vscode 中关闭 Python 插件的代码检查功能


---



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Action 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#动作 1


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import pyautogui


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import time
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import time


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.hotkey('ctrl', `shift', 'p')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.hotkey('ctrl', `shift', 'p')


#Action 2
#动作 2


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write('Preferences: Open Settings (UI)', interval=0.05)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.write('Preferences: Open Settings (UI)', interval=0.05)


&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)
&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(0.5)


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press('enter')
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.press('enter')


&nbsp;&nbsp;&nbsp;&nbsp;#Action 3
&nbsp;&nbsp;&nbsp;&nbsp;##动作 3


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Click on the search settings input field
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#点击搜索设置输入框


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(x=600, y=175)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(x=600, y=175)


#These coordinates may need to be adjusted for your screen resolution
&nbsp;&nbsp;&nbsp;&nbsp;#这些坐标可能需要根据您的屏幕分辨率进行调整


&nbsp;&nbsp;&nbsp;&nbsp;#Type `Python' into the search field
&nbsp;&nbsp;&nbsp;&nbsp;#在搜索框输入 Python


pyautogui.write('Python', interval=0.05)
pyautogui.write('Python', interval=0.05)


---



---



pyautogui.press('enter')
pyautogui.press('enter')


#Action 4
&nbsp;&nbsp;&nbsp;&nbsp;#操作 4


#Click on the search settings bar and type in the query again
&nbsp;&nbsp;&nbsp;&nbsp;#点击搜索设置栏并重新输入查询


&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(x=800, y=150)
&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(x=800, y=150)


#Click on the search bar, the coordinates may need adjustment
&nbsp;&nbsp;&nbsp;&nbsp;#点击搜索栏，坐标可能需要调整


pyautogui.typewrite('Python missing imports', interval=0.25)
pyautogui.typewrite('Python missing imports', interval=0.25)


&nbsp;&nbsp;&nbsp;&nbsp;#... Can not find the right click area
&nbsp;&nbsp;&nbsp;&nbsp;#... 找不到右键区域


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Failed
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#失败


---



#### D.6.1 Analysis on SoM Effectiveness
#### D.6.1 关于 SoM 效果的分析


The SoM, effective for web tasks focusing only on labeled bounding boxes, did not excel in our diverse tasks that required more expertise and flexibility. It shortens the action space of the agent and thus hinders the agent's exploration and adaptability. For example in Figure 18 in the task of "turning off the code checking function of the python plugin." in VS Code, agents without SoM succeeded by editing the settings.json, whereas SoM-equipped agents struggled with finding and deselecting the checkbox in the settings. The latter has a longer action path and fails due to incorrect mouse clicks. Furthermore, SoM's labeling of elements in professional software also has such errors, that agents without SoM can use a11y tree to calculate and adjust coordinates, while the elements' blocks under SoM can be hard to change during the task.
SoM 对仅关注带标注边界框的网页任务有效，但在需要更多专长与灵活性的多样任务中并不出色。它会缩短智能体的行动空间，从而妨碍其探索与适应性。例如在任务“在 VS Code 中关闭 Python 插件的代码检查功能”的 Figure 18 中，未使用 SoM 的智能体通过修改 settings.json 成功，而装有 SoM 的智能体在查找并取消勾选设置中的复选框时遇到困难。后者的行动路径更长，因错误的鼠标点击而失败。此外，SoM 对专业软件中元素的标注也存在此类错误：没有 SoM 的智能体可以使用 a11y 树计算并调整坐标，而在 SoM 下的元素块在任务中可能难以更改。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_18_18_492087.jpg"/>



Figure 18: A task showcase where the SoM-equipped agent tried to find interactive settings, while the screen-allytree-equipped agents directly modified the value in the setting.json file.
Figure 18：一个任务展示，装有 SoM 的智能体尝试寻找交互设置，而屏幕同侪树（screen-allytree）装置的智能体直接修改 settings.json 文件中的值。


## E Limitations and Future Work
## E 局限性与未来工作


We identify several potential directions for community development and progress toward general-purpose agents for computer operation:
我们指出若干潜在的社区发展方向及向着通用操作代理的进展路径：


Enhancing VLM capabilities for efficient and robust GUI interactions For foundation model development, we need to boost the efficiency of our models, enabling them to process much longer contexts and perform inference computations efficiently, akin to the robotics community [6, 7] to better handle real-world cases. Enhancements in VLMs' GUI grounding capabilities that is robust to application windows changes and are also sought, focusing on the accurate understanding and generation of precise actions aligned with given instructions. Moreover, amplifying VLMs' ability to comprehend context in the form of images is a pivotal goal, since it is crucial to enable history encoding using images so that we can build memory and reflection upon that. These improvements may require more efforts in the upstream pre-training stage, downstream fine-tuning stage, and even in the model structure itself, as pointed out in previous work [12, 21, 37].
提升 VLM 能力，实现高效且鲁棒的 GUI 交互 对基础模型开发，我们需要提升模型效率，使其能够处理更长的上下文并高效地进行推理计算，类似机器人领域的做法以更好地处理现实世界的情况[6, 7]。对 VLM 在 GUI 绑定能力方面的提升也被看好，要求对应用窗口变化具有鲁棒性，聚焦于对给定指令的精确理解与行动的精准生成。此外，提升 VLM 理解以图像形式呈现的上下文的能力也是关键目标，因为这对实现以图像进行历史记忆编码、从而建立记忆与反思至关重要。这些改进可能需要在上游的预训练阶段、下游的微调阶段乃至模型结构本身投入更多努力，如以往工作[12, 21, 37]所指出的。 


Advancing agent methodologies for exploration, memory, and reflection The next-level approach encompasses designing more effective agent architectures that augment the agents' abilities to explore autonomously and synthesize their findings. The agents face challenges in leveraging lengthy raw observation and action records. It's fascinating to explore novel methods for encoding this history, incorporating efficient memory and reflection solutions to condense contextual information and aid the agent in extracting key information. Additionally, integrating knowledge grounding into (V)LLM agents through memory mechanisms is a promising avenue as well. Moreover, practice GUI assistants also require features of personalization and customization. These features rely on techniques such as user profiling and retaining memories from long-term user-assistant interactions. Additionally, crafting protocols specifically for digital agents operating within GUI and CLI interfaces aims at facilitating efficient actions is also an essential thing for the feasibility of general-purpose digital agents in the mid-short term.
为探索、记忆与反思而设计的代理方法学 下一阶段的方法包含设计更有效的代理架构，提升代理自主探索和综合发现的能力。代理在利用长时间原始观测与行动记录方面面临挑战。探索编码历史的新方法、结合高效的记忆与反思解决方案来浓缩上下文信息、帮助代理提取关键信息，具有吸引力。此外，通过记忆机制将知识绑定到 (V)LLM 代理中也是一个有前景的方向。此外，实践 GUI 助手也需要具备个性化与定制化的特征。这些特征依赖于用户画像等技术，以及从长期用户-助手互动中保留记忆的能力。此外，专门为在 GUI 与 CLI 界面中运行的数字代理制定协议，旨在促进高效行动，也是中短期内实现通用数字代理的关键要点。 


Addressing the safety challenges of agents in realistic environments The safety of agents is a critical issue if applying a built agent in fully realistic environments, the developed universal digital agent could potentially be used to bypass CAPTCHA systems in the future, as noted in [46]. However, due to the currently limited capabilities of agents, we have not observed any harmful and damaging behaviors during our experiments, an automatic agent has the opportunity to damage patent rights, abuse accounts, attempt to exploit software vulnerabilities to create viruses, or engage in attacks. Currently, we adopt virtual machines to make it difficult for developing digital agents to cause irreversible damage to our host machines. However, there still lacks a reliable metric to assess the safety of an agent developed in an isolated environment. The current evaluation functions mainly focus on the results closely regarding the task instructions, assess only the correctness of task completion, and pay little attention to potential unnecessary damaging actions of agents. Owing to the complexity of a complete computer environment, we didn't work out an efficient way to detect the latent side effects of the agent. Consequently, how to assess and control potential behaviors in open and real environments through environmental constraints and agent training is an important further direction of research.
应对现实环境中代理的安全挑战 如果将构建的代理应用于高度真实的环境，代理的安全性将是一个关键问题；如前文所述，在未来可能被用来绕过 CAPTCHA 系统的通用数字代理，尽管目前代理能力有限，我们尚未在实验中观察到有害行为；自动代理有可能侵犯专利权、滥用账户、试图利用软件漏洞制造病毒或进行攻击。目前，我们采用虚拟机以使开发数字代理难以对宿主机造成不可逆的损害。然而，仍然缺乏一个可靠的量化指标来评估在隔离环境中开发的代理的安全性。当前的评估函数主要关注任务指令相关的结果，评估仅限于任务完成的正确性，对潜在的不必要的有害行为几乎不予关注。由于完整计算环境的复杂性，我们尚未建立一种有效的方法来检测代理的潜在副作用。因此，如何通过环境约束和代理训练在开放与真实环境中评估与控制潜在行为，是一个重要且需要进一步研究的方向。 


Expanding and refining data and environments for agent development In terms of datasets and environments, we can broaden the scope to cover more specialized domains, including real-sector needs in healthcare, education, industry, transportation, and personalized requirements. Efforts can be made to ensure our environment's seamless deployment across various hardware and software settings. The variance of all ty tree quality across different applications is also noticed. Although the problem is not remarkable in the applications currently included, there is no guarantee of that the application developers obey the ally convention and offer clear and meaningful descriptions for GUI elements. More intelligent approaches to filter redundant all ty tree elements and to handle latently missing elements deserve careful investigation as well. We also highlight the necessity of a painless data collection method, allowing for the effortless acquisition of computer operation data and its transformation into agent capabilities.
扩展与优化代理开发的数据与环境 在数据集与环境方面，我们可以将范围扩展到更多专业领域，包括医疗、教育、工业、交通以及个性化需求等真实领域的需求。努力确保我们的环境能够在不同硬件与软件设置中无缝部署。注意到在不同应用中所有语义树（ty tree）质量的差异。尽管当前纳入的应用中问题并不突出，但不能保证应用开发者会遵循对 GUI 元素的清晰且有意义的描述的盟友约定。对过滤冗余的语义树元素、处理潜在缺失元素的更智能方法需要仔细研究。我们还强调数据收集方法无痛感的必要性，以便轻松获取计算机操作数据并将其转化为代理能力。 


## F Distribution & License
## F 分布与许可


The proposed OSWORLD platform and task set are open-sourced under the Apache-2.0 license and are available at https://os-world.github.io/ and https://github.com/xlang-ai/OSWorld The authors claim that OSWORLD will be permanently maintained for future environments.
提出的 OSWORLD 平台与任务集在 Apache-2.0 许可证下开源，地址为 https://os-world.github.io/ 与 https://github.com/xlang-ai/OSWorld。作者声称 OSWORLD 将永久维护以用于未来环境。


A non-exhaustive list of artifacts used in the development of OSWORLD environment includes: pyautogui ${}^{20}$ flask ${}^{21}$ python-pptx ${}^{22}$ python-docx ${}^{23}$ OpenCV ${}^{24}$ EasyOCR ${}^{25}$ etc. They are released under licenses BSD-3-Clause, MIT, and Apache-2.0. A non-exhaustive list of artifacts used in the experiments includes: openai-python ${}^{26}$ ] google-generativeai ${}^{27}$ ] groq-python ${}^{28}$ etc. They are released under licenses Apache-2.0. The authors claim that the usage completely obeys the licenses.
在 OSWORLD 环境开发中使用的非穷举清单包括：pyautogui ${}^{20}$ flask ${}^{21}$ python-pptx ${}^{22}$ python-docx ${}^{23}$ OpenCV ${}^{24}$ EasyOCR ${}^{25}$ 等。它们在 BSD-3-Clause、MIT 和 Apache-2.0 许可下发布。在实验中使用的非穷举清单包括：openai-python ${}^{26}$ ] google-generativeai ${}^{27}$ ] groq-python ${}^{28}$ 等。它们在 Apache-2.0 许可下发布。作者声明使用完全遵守许可协议。 


## G Datasheet
## G 数据表


### G.1 Motivation
### G.1 动机


- For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.
- 该数据集是为何目的创建的？是否有具体任务在脑海中？是否存在需要填补的特定空缺？请提供描述。


---



20https://github.com/asweigart/pyautogui
20https://github.com/asweigart/pyautogui


2 https://github.com/pallets/flask
2 https://github.com/pallets/flask


https://github.com/scanny/python-pptx



https://github.com/python-openxml/python-docx



${}^{24}$ https://github.com/opencv/opencv-python
${}^{24}$ https://github.com/opencv/opencv-python


${}^{25}$ https://github.com/JaidedAI/EasyOCR
${}^{25}$ https://github.com/JaidedAI/EasyOCR


26https://github.com/openai/openai-python
26https://github.com/openai/openai-python


${}^{27}$ https://github.com/GoogleCloudPlatform/generative-ai
${}^{27}$ https://github.com/GoogleCloudPlatform/generative-ai


${}^{28}$ https://github.com/groq/groq-python
${}^{28}$ https://github.com/groq/groq-python


---



The environment and dataset for OSWORLD are created to address a critical need for a comprehensive benchmark capable of evaluating the performance and capabilities of universal digital agents across real-world, operating system-level tasks. The motivation stemmed from the recognized gaps in existing benchmarks focusing predominantly on isolated applications or single-domain tasks. No existing dataset adequately represents the range of actions, coupled with real-world complexities, faced by operational software agents in a truly interactive operating system environment. This dataset enables the holistic assessment of agents in multi-application contexts and multi-modal interactions, with tasks requiring both comprehension and navigation of complex user interfaces across various commonly used software and operating systems.
为 OSWORLD 提供环境与数据集，是为了解决一个关键需求：建立一个全面基准，能够评估通用数字代理在真实世界、操作系统层级任务中的表现与能力。其动机源于现有基准在聚焦孤立应用或单一领域任务方面的公认不足。尚无现有数据集能够充分代表操作软件代理在真正交互式操作系统环境中所面对的多样化动作及真实世界的复杂性。该数据集使代理在多应用场景和多模态交互中进行全面评估，任务要求同时具备对复杂用户界面的理解与导航能力，覆盖各种常用软件与操作系统。


## Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?
## 数据集的创建者（例如，哪个团队、研究组）以及代表的实体（例如，哪个公司、机构、组织）？


Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu from the XLANG Lab ${}^{29}$ from University of Hong Kong, Carnegie Mellon University, Salesforce Research and University of Waterloo create the environment and the task set.
Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu from the XLANG Lab ${}^{29}$ from University of Hong Kong, Carnegie Mellon University, Salesforce Research and University of Waterloo create the environment and the task set.


### G.2 Composition
### G.2 组成


- What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.
数据集中的实例代表哪些内容（例如文档、照片、人物、国家）？是否存在多种类型的实例（例如电影、用户和评分；人物与他们之间的互动；节点与边）？请提供描述。


The instances within the OSWORLD dataset comprise a diverse collection of tasks simulated within real operating system environments, specifically focusing on Ubuntu and Windows systems. These tasks represent a variety of real-world computer usage scenarios, encompassing multiple applications and interfaces. Each instance includes detailed natural language instructions, setup configurations with corresponding files, and setup actions for environment initialization. The instances further encompass a variety of operation types such as document editing, file operations, email activities, and multimedia management. This design ensures a comprehensive evaluation of digital agents' ability to perform highly integrated, multi-modal interactions across different software applications and user interfaces. The dataset, therefore, offers a rich milieu of complex tasks requiring both cognitive understanding and interaction capabilities from the executing agents.
OSWORLD 数据集中的实例是在真实操作系统环境中模拟的多样任务，专注于 Ubuntu 与 Windows 系统。这些任务代表多种真实世界的计算机使用场景，涵盖多应用与多界面。每个实例包含详细的自然语言指令、带有对应文件的设置配置，以及环境初始化的设置操作。实例还涵盖了文档编辑、文件操作、邮件活动、多媒体管理等多种操作类型。设计旨在全面评估数字代理在不同软件应用与用户界面之间进行高度集成、跨模态交互的能力。因此，该数据集提供了一个丰富的复杂任务环境，要求执行代理具备认知理解与互动能力。


## How many instances are there in total (of each type, if appropriate)?
## 总共有多少个实例（如按类型分，若合适）？


There are a total of 369 tasks for the Ubuntu operating system and 43 tasks designed for Windows within the OSWORLD benchmark. These tasks encompass a variety of task types including multi-app workflow tasks, single-application tasks, and integrated tasks from related datasets. Specifically, the dataset includes 268 single-app tasks (72.6%), 101 multi-app workflow tasks (27.4%), and 30 infeasible tasks (8.1%). Additionally, 84 tasks (22.8%) are integrated from other datasets.
在 OSWORLD 基准中，Ubuntu 操作系统共有 369 个任务，Windows 任务设定为 43 个。这些任务涵盖多种任务类型，包括多应用工作流任务、单应用任务以及来自相关数据集的集成任务。具体而言，数据集包含 268 个单应用任务（占 72.6%）、101 个多应用工作流任务（占 27.4%），以及 30 个不可实现任务（占 8.1%）。此外，有 84 个任务（占 22.8%）是从其他数据集集成而来。


- Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).
- 数据集包含所有可能的实例还是来自更大集合的实例子集（不一定随机）？若数据集是一个样本，那么更大集合是什么？样本是否代表更大集合（如地理覆盖）？若是，请描述如何验证/核实这一代表性。如果它不代表更大集合，请描述原因（例如为了覆盖更广泛的实例、因为实例被隐藏或不可用）。


The dataset in OSWORLD is a curated sample from a larger set of potential real-world tasks. These samples were selected based on their popularity, helpfulness, and diversity, as judged by view counts and user votes on various platforms including official guidelines, tutorials, forums, and educational courses. This selection process intends to cover a broad spectrum of typical and critical tasks that users perform using computer systems across different operating systems and software environments. While this dataset aims to cover various practical and high-impact scenarios, it does not claim to represent all possible tasks due to the vastness and evolving nature of computer operations. It emphasizes tasks with significant educational and practical utility to ensure that the benchmark provides both comprehensive and challenging objectives for developing advanced digital agents. The representativeness has not been systematically verified statistically due to the qualitative selection criteria based on task relevance and utility rather than mere statistical representation.
OSWORLD 数据集是来自更大潜在现实世界任务集合的精选样本。选取基于它们在各平台（包括官方指南、教程、论坛和教育课程）上的浏览量、用户投票等来判断的流行性、实用性和多样性。此选取旨在覆盖用户在不同操作系统和软件环境下执行的典型且关键任务的广泛范围。尽管数据集旨在覆盖各种实际且高影响力的情景，但由于计算机操作的广阔性和不断演进的特性，它并不声称代表所有可能的任务。它强调具有显著教育与实用价值的任务，以确保基准测试对开发先进数字代理既全面又具挑战性。由于基于任务相关性与实用性而非单纯统计代表性的定性筛选标准，代表性尚未进行系统的统计验证。


- What data does each instance consist of? "Raw" data (e.g., unprocessed text or images) or features? In either case, please provide a description.
- 每个实例由哪些数据组成？“原始”数据（如未处理的文本或图像）还是特征？无论哪种情况，请提供描述。


Each instance within the OSWORLD dataset consists of both "raw" data and structured annotations. The main components of each task instance are:
OSWORLD 数据集中的每个实例都同时包含“原始”数据和结构化注释。每个任务实例的主要组成部分包括：


- Natural language instruction: Each task is accompanied by a natural language description that guides the agent on how to execute the task. These instructions are crafted from real-world guidelines and scenarios.
- 自然语言指令：每个任务都附有引导代理如何执行任务的自然语言描述。这些指令来自现实世界的指南与情景。


---



*https://www.xlang.ai/



---



- Setup configuration: This includes necessary files and setup actions required for initializing the task environment as the cases of the real world.
- 设置配置：包括初始化任务环境所需的必要文件和设置操作，与现实世界案例相似。


- Evaluation scripts: Scripts designed to programmatically assess whether the task has been successfully completed by the agent. These scripts function as objective measures of task performance.
- 评估脚本：用于以编程方式评估代理是否成功完成任务的脚本。这些脚本作为任务执行绩效的客观量度。


- Accessibility Trees and Screenshots: For GUI-based tasks, environments from instances include accessibility trees that provide structured data regarding the GUI elements, and screenshots that offer visual context. These components are crucial for agents that rely on visual and structural interpretations to navigate and interact within the tasks.
- 可访问性树与截图：对于基于GUI的任务，实例环境包含提供GUI元素结构化数据的可访问性树，以及提供视觉上下文的截图。这些组件对于依赖视觉与结构化解释来在任务中导航和交互的代理至关重要。


These data elements combine raw and processed formats to deliver comprehensive task-specific information that supports both the execution and the evaluation of automated agents across varied operating systems and interfaces.
这些数据要素将原始与处理后格式相结合，提供全面的任务特定信息，支持在多种操作系统和界面上对自动代理的执行与评估。


- Is there a label or target associated with each instance? If so, please provide a description.
- 是否每个实例都有标签或目标？若有，请描述。


Yes, each task in the OSWORLD benchmark is associated with a specific label describing the objective of the task. The label is a natural language instruction that indicates what the computer agent needs to achieve. Additionally, each task is accompanied by a setup configuration and an evaluation script, which are used to verify if the task has been accomplished successfully according to the predefined criteria. This structured approach ensures that agents are assessed accurately on their ability to perform a variety of real-world computing tasks.
是的，OSWORLD 基准中的每个任务都关联有描述任务目标的具体标签。标签是一个自然语言指令，指示计算机代理需要实现的目标。此外，每个任务还附有设置配置和评估脚本，用于按预定义标准验证任务是否成功完成。这种结构化方法确保对代理在执行各类现实世界计算任务能力的评估准确。


- Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.
- 是否有信息缺失于单个实例？若有，请描述原因（例如信息不可用）。这不包括明确移除的信息，可能包括被涂改的文本等。


Yes, some instances may lack certain details that were not available from the source materials. These sources include forums, tutorials, how-to websites, and video content from platforms like YouTube and TikTok. This missing information could be due to incomplete guidelines or advice given in the source materials, which were not exhaustively detailed. Additionally, although extensive efforts were made to create comprehensive annotations, some specifics might be inherently unobtainable due to the nature of the source content.
是的，某些实例可能缺少来自源材料不可用的特定细节。这些来源包括论坛、教程、操作指南网站，以及来自 YouTube、TikTok 等平台的视频内容。这些缺失信息可能是由于源材料中的指南或建议不完整或未详尽描述。此外，尽管进行了大量工作以创建全面的注释，但某些具体细节可能因源内容的性质而本质上无法获得。


- Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.
- 数据集中是否存在错误、噪声源或冗余？如果有，请描述。


Yes, our dataset may include various sources of noise or redundancies which typically arise from the variety of different real-world sources used for task examples, such as forums, tutorials, and guidelines. Additionally, the dataset includes deliberately collected infeasible tasks due to feature deprecation or hallucinated features, as highlighted in the design process. These instances are used to test the robustness and error-handling capabilities of digital agents but may be perceived as noise if not correctly accounted for in training. Redundancies may occur as multiple tasks may involve similar applications or operations, albeit with different specific instructions or end goals.
是的，我们的数据集可能包含来自不同现实来源（如论坛、教程和指南）所产生的各种噪声或冗余。此外，数据集还包含在设计过程中故意收集的因特征废弃或幻觉特征而导致的不可实现任务。这些实例用于测试数字代理的鲁棒性与错误处理能力，但在训练中若未正确处理，可能被视为噪声。冗余可能出现在多个任务涉及相似应用或操作，但具体指令或最终目标不同。


- Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.
- 数据集是自包含的吗，还是链接到或以其他方式依赖于外部资源（例如网站、推文、其他数据集）？如果链接或依赖外部资源，a) 是否有保证它们会存在并随时间保持不变；b) 是否有完整数据集的官方档案版本（即包括在数据集创建时存在的外部资源；c）是否存在可能适用于数据集用户的任何外部资源的限制（如许可、费用）？请提供所有外部资源及其相关限制的描述，以及适当时的链接或其他访问点。


The OSWORLD dataset is not entirely self-contained as it integrates tasks and guidelines derived from various external resources such as forums, tutorials, how-to websites, and public guidelines. These include websites like WikiHow, Reddit, Quora, StackOverflow, and video tutorials from platforms like YouTube and TikTok, among others.
OSWORLD 数据集并非完全自包含，因为它整合了来自论坛、教程、如何做网站以及公开指南等各种外部资源的任务和准则。这些资源包括 WikiHow、Reddit、Quora、StackOverflow 等网站，以及来自 YouTube、TikTok 等平台的视频教程等。


a) There are no guarantees that the external resources will exist indefinitely or remain constant over time, which could impact the dataset's reliability and the reproducibility of the tasks.
a) 外部资源并不能被保证无限期存在或随时间保持不变，这可能影响数据集的可靠性和任务的可重复性。


b) There are no official archival versions of the complete dataset that include external resources as they existed at the time the dataset was created. Consumers of the dataset will need to rely on the current versions of these resources, which may be subject to changes.
b) 没有包含在数据集创建时外部资源存在状态的官方档案版本。数据集的使用者需要依赖当前版本的这些资源，这些资源可能会发生变化。


c) Some of the external resources may have their own copyright or licensing restrictions which might apply to a dataset consumer. Each resource's terms of service and copyright laws should be reviewed prior to use to ensure compliance.
c) 一些外部资源可能具有自己的版权或许可限制，可能适用于数据集用户。在使用前应查阅各资源的服务条款与版权法以确保合规。


External resources vary in nature and can often have different terms of use; it is advised for users of the OSWORLD dataset to verify any potential restrictions or licensing requirements individually.
外部资源在性质上各不相同，且常有不同的使用条款；建议 OSWORLD 数据集的用户逐一核验潜在的限制或许可要求。


- Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)? If so, please provide a description.
- 数据集是否包含可能被视为保密的数据（例如受法律特权或医患保密保护、包含个人非公开通讯内容的数据）？若有，请提供描述。


No, the dataset does not contain any data that could be considered confidential. The tasks and examples within the OSWORLD dataset are created from widely accessible sources such as public forums, official tutorials, and openly available guidelines. All data in use, including task instructions and configurations, are derived from non-confidential, non-privileged, and public communications aimed at demonstrating real-world computer usage scenarios.
没有，数据集中不包含可被视为保密的任何数据。OSWORLD 数据集中的任务和示例来自广泛可获得的来源，如公开论坛、官方教程和公开指南。使用中的所有数据，包括任务指令和配置，均来自非保密、非特权、公开通信，旨在展示真实世界的计算机使用场景。


- Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.
- 数据集是否包含直接查看可能令人反感、侮辱性、威胁性，或可能引发焦虑的内容？若有，请描述原因。


No, the dataset primarily consists of tasks defined and executed on operating systems using data from tutorials, guides, and how-to information that is publicly available and widely considered to be educational and informational. Therefore, it is unlikely to contain content that could be viewed as offensive, insulting, or threatening.
不，数据集主要由在操作系统上定义和执行的任务组成，使用公开且普遍被认为是教育性和信息性的教程、指南、如何做信息中的数据。因此，不太可能包含被视为冒犯、侮辱或威胁的内容。


- Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.
- 数据集是否识别任何亚人群（例如按年龄、性别分组）？若有，请描述如何识别这些亚人群及它们在数据集中的分布。


No, the dataset does not identify or differentiate any subpopulations based on age, gender, or other demographic factors. The focus is on the diversity of tasks related to computer usage across different operating systems and software environments.
不，数据集不基于年龄、性别或其他人口统计因素来识别或区分任何亚人群。重点是关于跨不同操作系统和软件环境的计算任务多样性。


## Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how.
## 是否可以识别个人（即一个或多名自然人），直接或间接（结合其他数据）从数据集中识别？若有，请描述如何做。


No, it is not possible to identify individuals directly or indirectly from the dataset. The tasks and examples in OSWORLD are derived from general public sources such as forums, tutorials, and guidelines that are widely accessible and do not contain personal information. Furthermore, any task setup or descriptive data is specifically designed or curated to avoid inclusion of any personal identifiers or sensitive information that could lead to the re-identification of individuals.
不，无法从数据集中直接或间接识别个人。OSWORLD 的任务和示例来自一般公众来源，如论坛、教程和指南，这些来源广泛可得且不含个人信息。此外，任何任务设置或描述性数据都被特意设计或筛选，以避免包含任何个人身份信息或可能导致重新识别个人的敏感信息。


- Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.
- 数据集是否包含在任何方面可能被认为是敏感的数据（例如揭示种族或民族来源、性取向、宗教信仰、政治观点或工会成员资格，或地点；金融或健康数据；生物识别或遗传数据；政府身份形式，如社保号；犯罪记录）？若有，请提供描述。


The OSWORLD dataset does not contain data that might be considered sensitive. The dataset is constructed from publicly available sources such as forums, tutorials, and guidelines and solely focuses on operational tasks within computer environments. It does not include any personal data categories or information related to individuals. The tasks are generated and anonymized without incorporating any sensitive or personal data elements.
OSWORLD 数据集不包含可能被视为敏感的数据。数据集来自公开可用的来源，如论坛、教程和指南，专注于计算机环境中的操作任务。它不包含任何个人数据类别或与个人相关的信息。任务被生成并匿名化，未纳入任何敏感或个人数据要素。


### G.3 Collection Process
### G.3 Collection Process


- How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.
- 数据与每个实例相关联的方式是如何获取的？数据是直接可观测的（例如原始文本、影片评分）、由受试者报告（例如调查问卷答案），还是从其他数据推断/派生而来（例如词性标注、对年龄或语言的基于模型的猜测）？如果数据由受试者报告或间接从其他数据推断/派生，数据是否经过验证/核实？若有，请描述验证过程。


The data for each task in OSWORLD was meticulously collected from a variety of credible sources including forums, official tutorials, instructional video content from platforms like YouTube and TikTok, and Q&A websites such as Reddit and Quora. The tasks were inspired by real-world computer usage scenarios which are often discussed in these sources. Each example was carefully annotated with natural language instructions, and detailed setup configurations, and included manually crafted evaluation scripts to ensure the task accurately represents real-world functionalities. To validate the realism and feasibility of these tasks, every task was cross-verified by at least two other authors. In particular, these reviewers checked the instructions for clarity, the setup for correctness, and the evaluation scripts for comprehensiveness. Additionally, some tasks inherently deemed infeasible due to outdated features or user misconceptions were also included to represent the full spectrum of real-world challenges.
OSWORLD中每个任务的数据都来自多种可信来源的精细收集，包括论坛、官方教程、来自YouTube、TikTok等平台的教学视频内容，以及Reddit、Quora等问答网站。这些任务的灵感来自现实世界的计算机使用场景，这些场景常在上述来源中被讨论。每个示例都被仔细地用自然语言指令、详细的设置配置进行标注，并包含人工编写的评估脚本，以确保任务能够真实呈现现实世界的功能性。为验证这些任务的真实感和可行性，每个任务都至少由两位其他作者进行交叉核验。具体而言，这些评阅者检查了指令的清晰度、设置的正确性，以及评估脚本的全面性。此外，由于过时特征或用户误解导致本任务在某些方面本质上不可行的情况，也会包含在内，以代表现实世界挑战的全谱。


## What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated?
## 使用了哪些机制或程序来收集数据（例如硬件设备或传感器、手动人工筛选、软件程序、软件API）？这些机制或程序如何被验证？


Data collection for the OSWORLD benchmark involved a combination of manual human curation and usage of software programs. Specifically, task instructions and scenarios were gathered from diverse sources including official guidelines, online tutorials, how-to websites, public forums, and personal blogs. These were then annotated manually by the authors and supplemented with setup actions and evaluation scripts using software tools designed for task setup and verification.
OSWORLD基准的数据收集涉及手工人工筛选与使用软件程序的组合。具体而言，任务指令和场景来自多样来源，包括官方指南、在线教程、操作网站、公共论坛和个人博客。随后由作者手动标注，并辅以用于任务设置与验证的软件工具来补充设置操作和评估脚本。


The validation of these mechanisms and procedures was conducted through a rigorous peer-review process within the research team. Each task and its associated data were cross-checked by multiple authors to ensure feasibility, clarity, and alignment with the baseline sources. Additionally, the examples gathered were tested by independent evaluators not involved in the initial annotation, acting as agents to execute the tasks, and providing feedback which was used to refine the tasks and annotations further.
对这些机制与程序的验证通过研究团队内部的严格同行评审流程进行。每个任务及其相关数据由多位作者交叉核对，确保可行性、清晰度以及与基线来源的一致性。此外，收集的示例还由未参与初始标注的独立评估者进行测试，作为执行任务的代理并提供反馈，该反馈被用于进一步完善任务与标注。


## If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?
## 如果数据集是更大集合的一个样本，采样策略是什么（例如确定性、带有特定采样概率的概率抽样）？


The tasks in the OSWORLD represent a carefully curated subset of potential real-world computing tasks. The selection was made based on a combination strategy that included both deterministic and probabilistic elements. We chose examples based on their popularity, helpfulness, and diversity, with popularity assessed by view counts and votes on various platforms such as forums, tutorials, and online courses. This involved deterministic selection of application types to ensure coverage across a broad spectrum of daily, professional, and workflow scenarios. Additionally, we incorporated tasks that were inherently challenging or infeasible due to software limitations or feature deprecations, enhancing the dataset's practical relevance and complexity.
OSWORLD的任务代表了潜在现实世界计算任务的精心筛选子集。选择基于结合策略，既包含确定性元素，也包含概率性元素。我们基于受欢迎度、帮助性和多样性来选择示例，受欢迎度通过在论坛、教程、在线课程等平台的观看量和投票数评估。这涉及对应用类型的确定性选择，以确保覆盖广泛的日常、专业以及工作流场景。此外，我们还纳入了在软件限制或特征废弃等原因本质上具有挑战性或不可行的任务，以提升数据集的实际相关性与复杂性。


## Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?
## 数据收集过程中谁参与了（例如学生、众包工作者、承包商）？他们的报酬如何（例如众包工作者的薪酬水平）？


All the development of platform, app data collection, and task definition creation are completed by the authors.
平台开发、应用数据收集与任务定义创建均由作者完成。


- Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.
- 数据收集覆盖了多长时间框架？该时间框架是否与与实例相关数据的创建时间框架一致（例如对旧新闻的最近抓取）？如果不一致，请描述与实例相关数据创建的时间框架。


## Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?
## 您是直接从相关个人处收集数据，还是通过第三方或其他来源（例如网站）获取？


The data for OSWORLD were primarily collected via third parties and other sources. This included a diverse array of platforms such as official guidelines and tutorials, video tutorials on TikTok and YouTube, how-to websites like WikiHow, Q&A forums such as Reddit, Quora, Superuser, and StackOverflow, formal online courses from Coursera and Udemy, and publicly available personal blogs and guidelines.
OSWORLD的数据主要通过第三方及其他来源收集。这包括多种平台，如官方指南和教程、TikTok和YouTube上的视频教程、WikiHow等操作网站、Reddit、Quora、Superuser和StackOverflow等问答论坛、Coursera和Udemy等在线课程，以及公开可用的个人博客与指南。


- Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.
- 是否通知了相关个人数据收集的对象？如果是，请描述（或通过截图或其他信息展示）通知的方式，并提供通知原文的链接或其他入口，或以其他方式再现通知的确切语言。


No, the nature of the data collected for the OSWORLD benchmark did not involve any individual-based data that would require the notification of individuals. The tasks and environment setups used in OSWORLD were sourced from publicly available materials such as forums, tutorials, and guidelines, which do not contain personally identifiable information. Thus, notification to individuals was not applicable in this context.
不，OSWORLD基准所收集的数据性质不涉及需要通知个人的个人基数据。OSWORLD使用的任务和环境设置来自公开材料，如论坛、教程和指南，并不包含可识别个人身份的信息。因此，在此背景下不需要向个人发出通知。


- Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.
- 以上人员是否同意对其数据进行收集和使用？如果同意，请描述（或通过截图或其他信息显示）如何请求并获取同意，以及提供与当事人同意的具体语言相同的链接或其他访问点，或以其他方式复现该语言。


The data collected for the OSWORLD includes tasks sourced from publicly accessible forums, tutorials, and community-driven platforms, which inherently involve contributions from public individuals. However, the paper does not specify that these individuals provided direct consent for the use of such data in the context of creating the benchmark. Typically, these public platforms have their own terms of service that users agree to, which may cover data usage for research, but exact consent for this specific study was not detailed. Hence, more explicit consent mechanisms would potentially enhance ethical compliance.
OSWORLD 所收集的数据包含来自公开可访问的论坛、教程和社区驱动平台的任务，这些本质上涉及公开人物的贡献。然而，论文并未具体说明这些个人是否就用于创建基准的数据获得了直接同意。通常，这些公开平台有自己的服务条款，用户会同意，可能涉及研究数据使用，但并未详细说明就本研究的确切同意。因此，更明确的同意机制可能有助于提升伦理合规性。


- If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).
- 如果获得了同意，是否为同意者提供了未来撤回同意或对特定用途撤回的机制？若有，请提供描述，以及该机制的链接或其他访问入口（如适用）。


As the data collection involved publicly available sources and no specific documentation of direct consent is provided, there is also no mention of a mechanism for the individuals to revoke their consent in the future or for certain uses of the data. Implementing a consent revocation mechanism would be an important step towards enhancing the ethical handling of the data used in OSWORLD.
由于数据收集涉及公开可用的来源，且未提供直接同意的具体文档，因此也未提及未来撤回同意或对数据特定用途撤回同意的机制。实施撤回同意机制将是提升 OSWORLD 数据伦理处理的重要一步。


### G.4 Uses
### G.4 使用


- Has the dataset been used for any tasks already? If so, please provide a description.
- 数据集是否已被用于任何任务？若有，请提供描述。


Yes, the OSWORLD dataset has been utilized to support the development and evaluation of automated computer agents across various operating systems. It includes a set of 369 tasks on Ubuntu and 43 tasks on Windows, which are designed to assess various capabilities such as commonsense reasoning, software navigation, and precise input control. These tasks have been employed in comprehensive testing and performance evaluation scenarios, which have involved comparing different LLM and VLM agents' abilities to handle multi-app workflows and other operation types within real computing environments. Consequently, the dataset has facilitated research into creating more effective and versatile digital agents. Detailed results and analyses stemming from these uses are documented in the paper.
是的，OSWORLD 数据集已被用于支持跨不同操作系统的自动化计算机代理的开发与评估。它包括在 Ubuntu 上的 369 项任务和在 Windows 上的 43 项任务，设计用于评估常识推理、软件导航和精确输入控制等能力。这些任务已在综合测试和性能评估场景中使用，比较了不同的 LLM 和 VLM 代理在处理多应用工作流和其他操作类型时在真实计算环境中的能力。因此，该数据集促进了关于打造更高效、更具通用性的数字代理的研究。关于这些用途的详细结果和分析已在论文中记载。


- Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.
- 是否存在链接到使用该数据集的论文或系统的仓库？若有，请提供链接或其他访问入口。


Yes, the OSWORLD dataset is accompanied by a repository that includes links to all papers and systems utilizing the dataset for research and development purposes. It serves as a comprehensive resource for tracing the impact and applications of the dataset across various studies and implementations. The repository can be accessed via https://github.com/xlang-ai/OSWorld
是的，OSWORLD 数据集附带一个存储库，其中包含用于研究和开发目的的所有论文与系统的链接。它是追踪数据集在各类研究与实现中的影响与应用的综合资源。该存储库可通过 https://github.com/xlang-ai/OSWorld 访问


- Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other risks or harms (e.g., legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms?
- 数据集的组成、收集与预处理/清洗/标注方式是否有可能影响未来的使用？例如，数据集使用者是否需要了解的事项，以避免造成对个人或群体的不公待遇（如刻板印象、服务质量问题）或其他风险或伤害（如法律风险、经济损失）？若有，请提供描述。数据集使用者是否可以采取措施来降低这些风险或伤害？


No.



### G.5 Distribution
### G.5 发行


- Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.
- 数据集是否会以对数据创建主体之外的第三方（如公司、机构、组织）名义进行分发？若是，请提供描述。


Yes, the dataset created as part of the OSWORLD project is made available publicly in https: //github.com/xlang-ai/OSWorld It is open-sourced under the Apache-2.0 license, and it can be accessed via the official project URLs provided on GitHub and the OSWORLD website. This facilitates wide accessibility and utilization across different sectors and by various entities beyond the originating organization, promoting broader research and development in digital agent technology.
是的，作为 OSWORLD 项目的一部分创建的数据集公开发布在 https: //github.com/xlang-ai/OSWorld，遵循 Apache-2.0 许可证开源，可通过 GitHub 官方项目链接与 OSWORLD 官网提供的访问路径访问。这促进了跨不同领域和多方实体的广泛可访问性与利用，推动数字代理技术的更广泛研究与开发。


- How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?
- 数据集将如何分发（例如网站上的打包文件、API、GitHub）？数据集是否有数字对象标识符（DOI）？


The platform and dataset are open-sourced at GitHub. We do not apply for a DOI.
平台与数据集在 GitHub 开源。我们不申请 DOI。


- When will the dataset be distributed?
- 数据集何时分发？


Both the environment and the task set have already been made public.
环境与任务集均已公开。


- Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.
- 数据集是否在版权或其他知识产权（IP）许可下分发，或在适用的使用条款（ToU）下分发？如是，请描述该许可与/或 ToU，并提供相关许可条款或 ToU 的链接或访问入口，或以其他方式重现相关条款，以及与此等限制相关的任何费用。


Both the environment and dataset are open-sourced under Apache-2.0 license.
环境与数据集在 Apache-2.0 许可下开源。


- Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.
- 是否有任何第三方对实例相关数据施加基于 IP 的或其他限制？如有，请描述这些限制，并提供链接或访问入口，或以其他方式重现相关许可条款，以及与这些限制相关的任何费用。


No third-party IP-based or other restrictions are imposed on the primary data utilized by the OS-WORLD benchmark. The components comprising the tasks are derived from widely accessible and openly available sources, such as forums, tutorials, and free-to-use websites. All efforts have been made to ensure the tasks do not infringe upon proprietary data. However, the platform for virtual machines and tasks adapted for Windows require activation due to copyright considerations post-implementation, which can involve associated fees, although optional. For more detailed licensing terms and specific conditions, refer to the original sources and legal advice where relevant.
对 OS-WORLD 基准所使用的主要数据没有第三方 IP 或其他限制。构成任务的组件来自广泛可获取且公开可用的来源，如论坛、教程与免费可用的网站。已尽力确保任务不侵犯专有数据。然而，适用于 Windows 的虚拟机平台和为 Windows 调整的任务在实现后需激活，出于版权考虑，可能涉及费用，尽管是可选的。有关更详细的许可条款与特定条件，请在相关来源与法律意见处查阅。


- Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.
- 是否对数据集或单独实例存在出口管制或其他监管限制？如有，请描述这些限制，并提供链接或访问入口，或以其他方式重现相关凭证。


No.



### G.6 Maintenance
### G.6 维护


- Who will be supporting/hosting/maintaining the dataset?
- 谁来支持/托管/维护数据集？


The authors will support, host, and maintain permanently.
作者将提供支持、托管并永久维护。


## How can the owner/curator/manager of the dataset be contacted (e.g., email address)?
数据集所有者/馆长/管理员的联系途径（如电子邮件地址）如何获取？


Issues and discussions on GitHub and Hugging Face are welcome. One can also seek help from Tianbao Xie (tbxie@cs.hku.hk), Danyang Zhang (zhang-dy20@sjtu.edu.cn), and Tao Yu (taoyds@hku.hk).
欢迎在 GitHub 和 Hugging Face 上提出问题和进行讨论。也可以向谢天宝（tbxie@cs.hku.hk）、张丹阳（zhang-dy20@sjtu.edu.cn）以及于涛（taoyds@hku.hk）寻求帮助。


- Is there an erratum? If so, please provide a link or other access point.
- 是否存在勘误？若有，请提供链接或其他获取入口。


Currently, no. Errata will be announced if there is any in the future.
目前没有。若未来有勘误，将予以宣布。


- Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (e.g., mailing list, GitHub)?
- 数据集是否会更新（如纠正标签错误、添加新样本、删除样本）？如会，请描述更新频率、由谁进行以及如何通知数据集用户（如邮件列表、GitHub）？


Yes, the OSWORLD dataset will be periodically updated to correct any labeling errors, add new instances, and delete outdated or incorrect instances as necessary. The updates will be conducted annually by the core team of maintainers comprising computer science researchers and student contributors. Notice of updates, including details of changes made, will be communicated to dataset consumers through a dedicated mailing list and updates will be posted on the project's GitHub repository at https://github.com/xlang-ai/OSWorld
是的，OSWORLD 数据集将定期更新，以纠正任何标签错误、添加新样本、在必要时删除过时或不正确的样本。更新将由由计算机科学研究人员与学生贡献者组成的维护核心团队每年执行。更新通知及变更细节将通过专用邮件列表告知数据集用户，更新也将发布在项目的 GitHub 仓库 https://github.com/xlang-ai/OSWorld


- If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.
- 如果数据集涉及人员，是否存在关于与实例相关数据保留的适用限制（例如是否已告知涉事个人其数据将被保留一定时间后再删除）？如有，请描述这些限制并解释如何执行。


No.



- Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers. Older versions of the OSWORLD dataset will be available for use, but they may not receive updates or support once a new version is released. We aim to keep archival versions available for research reproducibility and comparison purposes. However, these archival versions will be hosted without active maintenance, meaning that issues or bugs identified in these versions will only be addressed in new releases. Obsolescence of any version will be communicated through our official website and version-release notes associated with the OSWORLD dataset.
- 是否继续支持/托管/维护数据集的早期版本？如果是，请描述方式；若否，请描述将如何向数据集用户传达其过时性。OSWORLD 的较旧版本将可用于研究可复现性与比较，但在新版本发布后可能不会获得更新或支持。我们旨在保持档案版本可用于研究复现与比较目的。然而，这些档案版本将以无主动维护的方式托管，意味着在这些版本中发现的问题或错误只会在新版本中得到解决。任何版本的过时性将通过我们的官方网站和与 OSWORLD 数据集相关的版本发行说明进行传达。


- If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.
- 如果其他人想扩展/增强/基于数据集进行构建/贡献，是否存在相应机制？如有，请提供描述。这些贡献是否会被验证/确认？若是，请描述方式；如否，请说明原因？是否存在将这些贡献通知/分发给数据集用户的流程？如有，请提供描述。


Yes, the OSWORLD framework encourages the academic and developer community to extend, augment, and contribute to the dataset. Contributions can be made via GitHub, where the project and its datasets are hosted. Contributors can submit pull requests with their enhancements or new task datasets. All contributions undergo a rigorous review process by the existing project maintainers to ensure that they meet the quality standards of the dataset and are consistent with the goals of OSWORLD. This review process includes verifying the accuracy of the information, the relevance, and the usability of the tasks. Once approved and merged, contributions are incorporated into the main branch of the dataset repository and are made available for immediate access and use. Updates to the dataset are regularly communicated through the project's website and GitHub repository, as well as through mailing lists and social media to ensure that dataset consumers and contributors stay informed. Additionally, periodic releases of the dataset include detailed changelogs that summarize new additions, changes, and improvements, thus ensuring transparency and ease of access for all users interested in building upon or utilizing the OSWORLD dataset.
是的，OSWORLD 框架鼓励学术界和开发者社区扩展、增强并贡献于数据集。贡献可以通过 GitHub 提交，在该项目及其数据集托管的地方进行。贡献者可以提交包含改进或新任务数据集的拉取请求。所有贡献都将经过现有项目维护者的严格评审，以确保符合数据集的质量标准并与 OSWORLD 的目标保持一致。该评审过程包括核实信息的准确性、相关性及任务的可用性。一旦获准并合并，贡献将被并入数据集仓库的主分支，并可立即访问使用。数据集的更新将通过项目网站、GitHub 仓库、邮件列表和社交媒体等渠道进行定期通知，以确保数据集用户和贡献者及时了解最新信息。此外，数据集的定期版本发布将包含详细的变更日志，概述新增内容、变更与改进，从而确保透明性并便于所有希望基于 OSWORLD 构建或使用的用户访问。}