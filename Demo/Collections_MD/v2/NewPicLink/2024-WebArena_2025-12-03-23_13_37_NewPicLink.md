# WEBARENA: A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS
# WEBARENA: 用于构建自主代理的逼真网页环境


Shuyan Zhou* Frank F. Xu*
周树言* 许方峰*


Hao Zhu ${}^{ \dagger  }$ Xuhui Zhou ${}^{ \dagger  }$ Robert Lo ${}^{ \dagger  }$ Abishek Sridhar ${}^{ \dagger  }$ Xianyi Cheng
朱浩 ${}^{ \dagger  }$ 周旭辉 ${}^{ \dagger  }$ 罗伯特·洛 ${}^{ \dagger  }$ 阿比塞克·斯里达尔 ${}^{ \dagger  }$ 程宪逸


Tianyue Ou Yonatan Bisk Daniel Fried Uri Alon Graham Neubig
欧天悦 约纳坦·比斯克 丹尼尔·弗里德 于里·阿隆 格雷厄姆·诺伊比格


Carnegie Mellon University
卡内基梅隆大学


\{shuyanzh, fangzhex, gneubig\}@cs.cmu.edu
\{shuyanzh, fangzhex, gneubig\}@cs.cmu.edu


## ABSTRACT
## 摘要


With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress. Our code, data, environment reproduction resources, and video demonstrations are publicly available at https://webarena.dev/
随着生成式AI的发展，自主代理通过自然语言命令处理日常任务成为可能。然而，目前的代理主要在简化的合成环境中构建和测试，与现实场景存在脱节。本文构建了一个高度逼真且可复现的语言引导代理环境。具体而言，我们聚焦于在网页上执行任务的代理，创建了包含四大常见领域（电商、社交论坛讨论、协作软件开发和内容管理）的功能齐全网站。我们的环境还配备了工具（例如地图）和外部知识库（例如用户手册），以鼓励类人问题解决。基于该环境，我们发布了一组基准任务，侧重评估任务完成的功能正确性。基准任务多样、长程且旨在模拟人们在互联网中常做的操作。我们对若干基线代理进行了实验，整合了诸如先推理后行动等近期技术。结果表明解决复杂任务具有挑战性：我们最佳的基于 GPT-4 的代理端到端任务成功率仅为14.41%，远低于人类的78.24%。这些结果凸显了进一步发展鲁棒代理的必要性，表明当前最先进的大型语言模型在这些真实任务中的表现仍远未完美，并且 WebArena 可用于衡量此类进展。我们的代码、数据、环境复现资源和视频演示已在 https://webarena.dev/ 公布。


## 1 INTRODUCTION
## 1 引言


Autonomous agents that perform everyday tasks via human natural language commands could significantly augment human capabilities, improve efficiency, and increase accessibility. Nonetheless, to fully leverage the power of autonomous agents, it is crucial to understand their behavior within an environment that is both authentic and reproducible. This will allow measurement of the ability of agents on tasks that human users care about in a fair and consistent manner.
能够通过人类自然语言命令执行日常任务的自主代理，可显著增强人类能力、提高效率并提升可及性。但要充分发挥自主代理的潜力，关键是要在既真实又可复现的环境中理解其行为。这将使得以公平一致的方式衡量代理在用户关心任务上的能力成为可能。


Current environments for evaluate agents tend to over-simplify real-world situations. As a result, the functionality of many environments is a limited version of their real-world counterparts, leading to a lack of task diversity (Shi et al. 2017; Anderson et al. 2018; Gordon et al. 2018; Misra et al. 2016; Shridhar et al. 2020; 2021; Yao et al. 2022a). In addition, these simplifications often lower the complexity of tasks as compared to their execution in the real world (Puig et al. 2018; Shridhar et al. 2020; Yao et al. 2022a). Finally, some environments are presented as a static resource (Shi et al. 2017; Deng et al. 2023) where agents are confined to accessing only those states that were previously cached during data collection, thus limiting the breadth and diversity of exploration. For evaluation, many environments focus on comparing the textual surface form of the predicted
当前用于评估代理的环境往往过度简化现实情境，导致许多环境的功能只是现实对应物的受限版本，从而缺乏任务多样性（Shi et al. 2017；Anderson et al. 2018；Gordon et al. 2018；Misra et al. 2016；Shridhar et al. 2020；2021；Yao et al. 2022a）。此外，这些简化常常降低了任务相较于现实执行时的复杂性（Puig et al. 2018；Shridhar et al. 2020；Yao et al. 2022a）。最后，某些环境以静态资源呈现（Shi et al. 2017；Deng et al. 2023），代理仅能访问在数据收集期间已缓存的那些状态，因而限制了探索的广度和多样性。在评估方面，许多环境侧重于将预测的动作序列与参考动作序列在文本表面形式上进行比较，而忽视执行的功能正确性和可能的替代解决方案（Puig et al., 2018；Jernite et al., 2019；Xu et al., 2021；Li et al., 2020；Deng et al., 2023）。这些限制常导致模拟环境与现实之间的差异，可能影响AI代理在复杂现实情境中成功理解、适应和操作的泛化能力。


---



*Lead contributors.
*主要贡献者。


${}^{ \dagger  }$ Equal contribution.
${}^{ \dagger  }$ 同等贡献。


---



<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_43_3ac076.jpg"/>



Figure 1: WebArena is a standalone, self-hostable web environment for building autonomous agents. WebArena creates websites from four popular categories with functionality and data mimicking their real-world equivalents. To emulate human problem-solving, WebArena also embeds tools and knowledge resources as independent websites. WebArena introduces a benchmark on interpreting high-level realistic natural language command to concrete web-based interactions. We provide validators to programmatically validate the functional correctness of each task.
图1：WebArena 是一个独立且可自托管的网页环境，用于构建自主代理。WebArena 创建四类流行网站，其功能和数据模仿现实对应物。为模拟人类问题解决，WebArena 还将工具和知识资源作为独立网站嵌入。WebArena 提出一个将高级现实自然语言命令解释为具体网页交互的基准。我们提供验证器以编程方式验证每个任务的功能正确性。


action sequences with reference action sequences, disregarding the functional correctness of the executions and possible alternative solutions (Puig et al., 2018; Jernite et al., 2019; Xu et al., 2021; Li et al., 2020; Deng et al., 2023). These limitations often result in a discrepancy between simulated environments and the real world, and can potentially impact the generalizability of AI agents to successfully understand, adapt, and operate within complex real-world situations.
动作序列与参考动作序列的文本对比忽视了执行的功能正确性和可能的替代方案（Puig et al., 2018；Jernite et al., 2019；Xu et al., 2021；Li et al., 2020；Deng et al., 2023）。这些限制常导致模拟环境与现实之间的差异，可能影响AI代理在复杂现实情境中成功理解、适应和操作的泛化能力。


We introduce WebArena, a realistic and reproducible web environment designed to facilitate the development of autonomous agents capable of executing tasks (§2). An overview of WebArena is in Figure 1 Our environment comprises four fully operational, self-hosted web applications, each representing a distinct domain prevalent on the internet: online shopping, discussion forums, collaborative development, and business content management. Furthermore, WebArena incorporates several utility tools, such as map, calculator, and scratchpad, to best support possible human-like task executions. Lastly, WebArena is complemented by an extensive collection of documentation and knowledge bases that vary from general resources like English Wikipedia to more domain-specific references, such as manuals for using the integrated development tool (Fan et al. 2022). The content populating these websites is extracted from their real-world counterparts, preserving the authenticity of the content served on each platform. We deliver the hosting services using Docker containers with gym-APIs (Brockman et al. 2016), ensuring both the usability and the reproducibility of WebArena.
我们提出 WebArena，一个现实且可复现的网页环境，旨在促进能够执行任务的自主代理的开发（§2）。图1 展示了 WebArena 的概览。我们的环境包含四个完全可运行、自主托管的 web 应用，每个代表互联网上常见的不同领域：在线购物、讨论论坛、协同开发和企业内容管理。此外，WebArena 集成了若干实用工具，如地图、计算器和草稿本，以尽可能支持类人任务执行。最后，WebArena 配备了大量文档和知识库，涵盖从通用资源如英文维基百科到更具领域针对性的参考资料，例如集成开发工具的使用手册（Fan et al. 2022）。这些网站的内容来源于对应的真实世界平台，保留了各平台内容的真实性。我们通过带有 gym-APIs（Brockman et al. 2016）的 Docker 容器提供托管服务，确保 WebArena 的可用性和可复现性。


Along with WebArena, we release a ready-to-use benchmark with 812 long-horizon web-based tasks (§3). Each task is described as a high-level natural language intent, emulating the abstract language usage patterns typically employed by humans (Bisk et al. 2019). Two example intents are shown in the upper left of Figure 1. We focus on evaluating the functional correctness of these tasks, i.e., does the result of the execution actually achieve the desired goal (§3.2). For instance, to evaluate the example in Figure 2, our evaluation method verifies the concrete contents in the designated repository. This evaluation is not only more reliable (Zhong et al. 2017) Chen et al. 2021, Wang et al. 2022) than comparing the textual surface-form action sequences (Puig et al., 2018; Deng et al. 2023) but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks.
随同 WebArena，我们发布了一个可直接使用的基准，包含 812 项长时程的基于网页的任务（§3）。每个任务以高级自然语言意图描述，模拟人类通常采用的抽象语言使用模式（Bisk et al. 2019）。图1 左上角展示了两个示例意图。我们侧重于评估这些任务的功能正确性，即执行结果是否真正达成期望目标（§3.2）。例如，为评估图2 中的示例，我们的评估方法会验证指定代码仓库中的具体内容。该评估方法不仅比对文本表面动作序列（Puig et al., 2018; Deng et al. 2023）更可靠（Zhong et al. 2017; Chen et al. 2021; Wang et al. 2022），而且能容纳多种可能的有效路径来达成相同目标——这是在足够复杂任务中普遍存在的现象。


We use this benchmark to evaluate several agents that can follow NL command and perform web-based tasks (§4). These agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as GPT-4 and PALM-2. Experiment results show that the best GPT-4 agent performance is somewhat limited, with an end-to-end task success rate of only 14.41%, while the human performance is 78.24%. We hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks (§5.1). These outcomes underscore the necessity for further development towards robust and effective agents (LeCun 2022) in WebArena.
我们使用该基准评估了几种能遵循自然语言指令并执行网页任务的代理（§4）。这些代理以少样本上下文学习的方式实现，基于强大的大语言模型（LLMs），如 GPT-4 和 PALM-2。实验结果表明，表现最佳的 GPT-4 代理仍然受限，端到端任务成功率仅为 14.41%，而人类表现为 78.24%。我们假设当前 LLM 性能受限的原因在于缺乏关键能力，如主动探索和失败恢复，导致无法成功完成复杂任务（§5.1）。这些结果强调了在 WebArena 中进一步开发健壮且高效代理的必要性（LeCun 2022）。


## 2 WEBARENA: WEBSITES AS AN ENVIRONMENT FOR AUTONOMOUS AGENTS
## 2 WEBARENA：将网站作为自主代理的环境


Our goal is to create a realistic and reproducible web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical
我们的目标是创建一个现实且可复现的网页环境。我们通过使环境独立运行而不依赖实时网站来实现可复现性，从而规避了诸如机器人需应对 CAPTCHA、内容不可预测的修改和配置变更等技术难题，这些问题会阻碍不同系统随时间进行公平比较。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_43_12b1a4.jpg"/>



Figure 2: A high-level task that can be fully executed in WebArena. Success requires sophisticated, long-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), and (3) update the README file in the appropriate repository with the planned route.
图2：一个可以在 WebArena 中完整执行的高级任务。成功需要复杂的长期规划与推理。为达成目标（顶部），代理需要（1）在维基百科上查找匹兹堡的艺术博物馆，（2）在地图上确定其位置并优化行程，及（3）在相应仓库中更新 README 文件以记录规划路线。


challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts.
我们通过使用许多流行类别中实际使用的网站所基于的开源库，并从其真实世界对应物导入数据，为环境实现现实性。


### 2.1 CONTROLLING AGENTS THROUGH HIGH-LEVEL NATURAL LANGUAGE
### 2.1 通过高级自然语言控制代理


The WebArena environment is denoted as $\mathcal{E} = \langle \mathcal{S},\mathcal{A},\mathcal{O},\mathcal{T}\rangle$ with state space $\mathcal{S}$ ,action space $\mathcal{A}\left( \sqrt[5]{2.4}\right)$ and observation space $\mathcal{O}\left( \sqrt[8]{2.3}\right)$ . The transition function $\mathcal{T} : \mathcal{S} \times  \mathcal{A} \rightarrow  \mathcal{S}$ is deterministic,and it is defined by the underlying implementation of each website in the environment. Given a task described as a natural language intent $\mathbf{i}$ ,an agent issues an action ${a}_{t} \in  \mathcal{A}$ based on intent $\mathbf{i}$ ,the current observation ${o}_{t} \in  \mathcal{O}$ ,the action history ${\mathbf{a}}_{1}^{t - 1}$ and the observation history ${\mathbf{o}}_{1}^{t - 1}$ . Consequently, the action results in a new state ${s}_{t + 1} \in  \mathcal{S}$ and its corresponding observation ${o}_{t + 1} \in  \mathcal{O}$ . We propose a reward function $r\left( {{\mathbf{a}}_{1}^{T},{\mathbf{s}}_{1}^{T}}\right)$ to measure the success of a task execution,where ${\mathbf{a}}_{1}^{T}$ represents the sequence of actions from start to the end time step $T$ ,and ${\mathbf{s}}_{1}^{T}$ denotes all intermediate states. This reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent's actions, such as checking the correctness of the predicted answer.
WebArena 环境记为 $\mathcal{E} = \langle \mathcal{S},\mathcal{A},\mathcal{O},\mathcal{T}\rangle$，其状态空间为 $\mathcal{S}$，动作空间为 $\mathcal{A}\left( \sqrt[5]{2.4}\right)$，观测空间为 $\mathcal{O}\left( \sqrt[8]{2.3}\right)$。转移函数 $\mathcal{T} : \mathcal{S} \times  \mathcal{A} \rightarrow  \mathcal{S}$ 为确定性，由环境中每个网站的底层实现定义。给定以自然语言意图 $\mathbf{i}$ 描述的任务，代理基于意图 $\mathbf{i}$、当前观测 ${o}_{t} \in  \mathcal{O}$、动作历史 ${\mathbf{a}}_{1}^{t - 1}$ 和观测历史 ${\mathbf{o}}_{1}^{t - 1}$ 发出动作 ${a}_{t} \in  \mathcal{A}$。因此，动作导致新的状态 ${s}_{t + 1} \in  \mathcal{S}$ 及其相应的观测 ${o}_{t + 1} \in  \mathcal{O}$。我们提出了奖励函数 $r\left( {{\mathbf{a}}_{1}^{T},{\mathbf{s}}_{1}^{T}}\right)$ 来衡量任务执行的成功，其中 ${\mathbf{a}}_{1}^{T}$ 表示从起始到终止时间步 $T$ 的动作序列，${\mathbf{s}}_{1}^{T}$ 表示所有中间状态。该奖励函数评估状态转移是否符合意图的预期。例如，对下单意图，会验证是否已下单。此外，它还评估代理动作的准确性，例如检查预测答案的正确性。


### 2.2 WEBSITE SELECTION
### 2.2 网站选择


To decide which categories of websites to use, we first analyzed approximately 200 examples from the authors' actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories. We then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities (e.g., Amazon, eBay), (2) social forum platforms for opinion exchanges (e.g., Reddit, StackExchange), (3) collaborative development platforms for software development (e.g., GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (e.g., online store management).
为决定使用哪些类别的网站，我们首先分析了约 200 个来自作者真实浏览器历史的示例。每位作者检查各自的浏览历史，总结浏览会话特定片段的目标。基于此，我们将访问的网站归类为抽象类别。然后我们确定了四个最显著的类别，并基于该分析为每个类别实现了一个实例：（1）支持在线购物的电子商务平台（如 Amazon、eBay），（2）用于意见交流的社交论坛平台（如 Reddit、StackExchange），（3）用于软件开发的协作开发平台（如 GitLab），以及（4）管理数字内容创建与修订的内容管理系统（CMS）（如在线商店管理）。


In addition to these platforms, we selected three utility-style tools that are frequently used in web-based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes. As information-seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into WebArena. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals.
除了这些平台外，我们还挑选了三种常见于网络任务的工具型应用：（1）用于导航和搜索兴趣点（POI）信息（如机构或地点）的地图，（2）计算器，以及（3）用于记笔记的草稿板。鉴于信息检索与知识获取在基于网络的任务中至关重要，我们还将各种知识资源纳入 WebArena。这些资源涵盖通用信息中心，如英文维基百科，到更专业的知识库，如网站用户手册。


Implementation We leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website (OneStopShop), GitLab, Reddit, an online store content management system (CMS), a map, and an English Wikipedia. Then we imported sampled data from their real-world counterparts. As an example, our version of GitLab was developed based on the actual GitLab project 1 We carefully emulated the features of a typical code repository by including both popular projects with many issues and pull requests and smaller, personal projects. Details of all websites in WebArena can be found in Appendix A.1. We deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix A.2).
实现 我们利用与各类别相关的开源库构建了自己的电子商务网站（OneStopShop）、GitLab、Reddit、在线商店内容管理系统（CMS）、地图和英文维基百科的版本。然后我们从其真实对应物中导入了采样数据。例如，我们的 GitLab 版本基于真实的 GitLab 项目开发。我们通过包含既有大量 issue 和合并请求的热门项目，又有较小的个人项目，来仔细模拟典型代码仓库的特性。WebArena 中所有网站的详细信息见附录 A.1。我们以 docker 形式提供该环境，并提供将环境重置到确定性初始状态的脚本（见附录 A.2）。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_43_7bb02d.jpg"/>



Figure 3: We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right). The content of the middle and right figures are trimmed to save space.
图 3：我们将观测设计为网页的 URL 和内容，可选择将内容表示为截图（左）、HTML DOM 树（中）和可访问性树（右）。中图和右图的内容被裁剪以节省空间。


### 2.3 OBSERVATION SPACE
### 2.3 观测空间


We design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs , and the web page content of the focused tab. WebArena is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. The multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab. We provide flexible configuration to render the page content in many modes: (see Figure 3 for an example): (1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work (Shi et al. 2017) Deng et al. 2023; Li et al. 2020); (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page ${}^{2}$ The accessibility tree is a subset of the DOM tree with elements that are relevant and useful for displaying the contents of a web page. Every element is represented as its role (e.g., a link), its text content, and its properties (e.g., whether it is focusable). Accessibility trees largely retain the structured information of a web page while being more compact than the DOM representation.
我们设计观测空间以大致模拟网页浏览器体验：网页 URL、已打开的标签页，以及当前聚焦标签页的网页内容。WebArena 是首个考虑多标签网页任务的环境，以促进工具使用、跨标签页的直接比较与引用等功能。多标签功能比在单一标签页中维持一切更真实地复现了人类的网页浏览习惯。我们提供灵活配置以多种模式渲染页面内容：（参见图 3 示例）：(1) 原始网页 HTML，由文档对象模型（DOM）树组成，常见于过去工作（Shi et al. 2017；Deng et al. 2023；Li et al. 2020）；(2) 截图，即以像素为基础的表示，将当前网页表示为 RGB 数组；(3) 网页的可访问性树 ${}^{2}$ 可访问性树是 DOM 树的一个子集，仅包含显示网页内容相关且有用的元素。每个元素以其角色（如链接）、文本内容及属性（如是否可聚焦）表示。可访问性树在保留网页结构信息的同时比 DOM 表示更紧凑。


We provide an option to limit the content to the contents within a viewport for all modes. This ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements.
我们提供一个选项，将所有模式的内容限制在视口内。这确保观测可输入到上下文长度受限的文本模型或对图像大小/分辨率有要求的图像模型中。


### 2.4 ACTION SPACE
### 2.4 动作空间


Following previous work on navigation and operation in web and embodied environments (Shi et al. 2017; Liu et al. 2018), we design a compound action space that emulates the keyboard and mouse operations available on web pages. Figure 4 lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history.
继承先前在网页和具身环境中关于导航与操作的工作（Shi et al. 2017；Liu et al. 2018），我们设计了一个复合动作空间，模拟网页上的键盘与鼠标操作。图 4 列出所有可用动作，分为三类。第一类为元素操作，如点击、悬停、输入和组合键按下。第二类为标签页相关动作，如打开、关闭和切换标签页。第三类为 URL 导航动作，如访问特定 URL 或在浏览历史中前进与后退。


Building on these actions, WebArena provides agents with the flexibility to refer to elements for operation in different ways. An element can be selected by its on-screen coordinates, $\left( {x,y}\right)$ ,or by a unique element ID that is prepended to each element. This ID is generated when traversing the Document Object Model (DOM) or accessibility tree. With element IDs, the element selection is transformed into an $n$ -way classification problem,thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. For example, issuing the action click [1582] clicks the button given the observation of [1582] Add to Cart. This flexible element selection allows WebArena to support agents designed in various ways (e.g., accepting input from different modalities) without compromising fair comparison metrics such as step count.
基于这些动作，WebArena 赋予代理以不同方式引用元素进行操作的灵活性。元素可以通过其屏幕坐标、$\left( {x,y}\right)$ 或通过预置于每个元素前的唯一元素 ID 来选择。该 ID 在遍历文档对象模型（DOM）或可访问性树时生成。使用元素 ID 后，元素选择转化为一个 $n$ 路分类问题，从而消除了代理或底层实现所需的任何消歧义工作。例如，发出动作 click [1582] 会在观测到 [1582] Add to Cart 时点击该按钮。这种灵活的元素选择使 WebArena 能够支持以多种方式设计的代理（例如接受不同模态输入）而不损害诸如步数等公平比较指标。


---



https://gitlab.com/gitlab-org/gitlab



https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree



---



<table><tr><td>Action Type</td><td>Description</td></tr><tr><td>noop</td><td>Do nothing</td></tr><tr><td>click (elem)</td><td>Click at an element</td></tr><tr><td>hover (elem)</td><td>Hover on an element</td></tr><tr><td>type(elem, text)</td><td>Type to an element</td></tr><tr><td>press (key_comb)</td><td>Press a key comb</td></tr><tr><td>scroll (dir)</td><td>Scroll up and down</td></tr><tr><td>tab_focus(index)</td><td>focus on $i$ -th tab</td></tr><tr><td>new_tab</td><td>Open a new tab</td></tr><tr><td>tab_close</td><td>Close current tab</td></tr><tr><td>go_back</td><td>Visit the last URL</td></tr><tr><td>go_forward</td><td>Undo go_back</td></tr><tr><td>goto (URL)</td><td>Go to URL</td></tr></table>
<table><tbody><tr><td>操作类型</td><td>描述</td></tr><tr><td>无操作</td><td>什么也不做</td></tr><tr><td>点击（elem）</td><td>在元素上点击</td></tr><tr><td>悬停（elem）</td><td>在元素上悬停</td></tr><tr><td>输入（elem, text）</td><td>向元素输入</td></tr><tr><td>按下（key_comb）</td><td>按下按键组合</td></tr><tr><td>滚动（dir）</td><td>上下滚动</td></tr><tr><td>标签聚焦(index)</td><td>聚焦第 $i$ 个标签</td></tr><tr><td>新标签</td><td>打开新标签页</td></tr><tr><td>关闭标签</td><td>关闭当前标签</td></tr><tr><td>后退</td><td>访问上一个 URL</td></tr><tr><td>前进</td><td>撤销后退</td></tr><tr><td>前往（URL）</td><td>前往 URL</td></tr></tbody></table>


Figure 4: Action Space of WebArena
图4：WebArena 的动作空间


<table><tr><td>Category</td><td>Example</td></tr><tr><td rowspan="2">Information Seeking</td><td>When was the last time I bought shampoo</td></tr><tr><td>Compare walking and driving time from AMC Waterfront to Randyland</td></tr><tr><td rowspan="2">Site Navigation</td><td>Checkout merge requests assigned to me</td></tr><tr><td>Show me the ergonomic chair with the best rating</td></tr><tr><td rowspan="2">Content & Config</td><td>Post to ask "whether I need a car in NYC"</td></tr><tr><td>Delete the reviews from the scammer Yoke</td></tr></table>
<table><tbody><tr><td>类别</td><td>示例</td></tr><tr><td rowspan="2">信息查询</td><td>我上次买洗发水是什么时候</td></tr><tr><td>比较从 AMC Waterfront 到 Randyland 步行和驾车所需时间</td></tr><tr><td rowspan="2">网站导航</td><td>查看分配给我的合并请求</td></tr><tr><td>显示评分最高的人体工学椅</td></tr><tr><td rowspan="2">内容与配置</td><td>发帖询问“在纽约市我是否需要一辆车”</td></tr><tr><td>删除来自骗子 Yoke 的评论</td></tr></tbody></table>


Figure 5: Example intents from three categories.
图 5：来自三类的示例意图。


User Role Simulation Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories. We emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix A.3
用户角色模拟 同一网站的用户由于角色、权限和交互历史不同，常有截然不同的体验。我们通过在每个平台上生成独特的用户配置来模拟该情形。详情见附录 A.3


## 3 BENCHMARK SUITE OF WEB-BASED TASKS
## 3 基于网页任务的基准套件


We provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in WebArena. Each example has a metric to evaluate the functional correctness of the task execution. In this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark.
我们提供了一个在 WebArena 中将高级自然语言指令落地为交互行为的 812 个测试示例基准。每个示例都配有用于评估任务执行功能正确性的度量。在本节中，我们先形式化定义通过自然语言控制自治代理的任务，然后介绍基准的标注流程。


### 3.1 INTENT COLLECTION
### 3.1 意图收集


We focus on curating realistic intents to carry out complex and creative tasks within WebArena. To start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites' content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites.
我们专注于策划用于在 WebArena 中执行复杂且富有创造性的真实意图。首先，标注人员被引导花几分钟浏览网站以熟悉其内容和功能。由于我们的大多数网站与其开放网络对应站点几乎相同，尽管使用了采样数据，大多数标注人员能迅速理解这些网站。


Next, we instructed the annotators to formulate intents based on the following criteria:
接着，我们指示标注人员按以下标准制定意图：


(1) The intent should be abstract and high-level, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of "click the science subreddit", we encouraged annotators to come up with something more complex like "post a greeting message on science subreddit", which involves performing multiple actions.
（1）意图应抽象且高层次，意味着该任务不能仅凭一两步操作完成。例如，我们鼓励标注人员不要写“点击 science 子版块”，而应提出更复杂的任务，如“在 science 子版块发布一条问候信息”，这将涉及多步操作。


(2) The intent should be creative. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (e.g., "create a Reddit account identical to my GitLab one") to make the intents more unique.
（2）意图应具有创造性。诸如创建账户之类的常见任务易于想到，我们鼓励标注人员加入约束（例如“创建一个与我的 GitLab 相同的 Reddit 账号”）以使意图更独特。


(3) The intent should be formulated as a template by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent "create a Reddit account identical to my GitLab one" can be converted into "create a \{\{site1\}\} account identical to my \{\{site2\}\} one", with an instantiation like "\{site1: Reddit, site2: GitLab\}" and another like "\{site1: GitLab, site2: OneStopShopping/". Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation.
（3）意图应以模板形式制定，将可替换元素设为变量。标注人员还需为每个变量开发若干实例化。例如，意图“创建一个与我的 GitLab 相同的 Reddit 账号”可转换为“创建一个 \{\{site1\}\} 账号，与我的 \{\{site2\}\} 相同”，并可有如“\{site1: Reddit, site2: GitLab\}”或“\{site1: GitLab, site2: OneStopShopping\}”等实例。值得注意的是，源自同一模板的任务可能有不同的执行轨迹，相似性主要体现在高层语义而非具体实现。


We also provided a prompt for the annotators to use with ChatGPT ${}^{3}$ for inspiration,that contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites. Furthermore, we offered a curated list of examples for annotators to reference.
我们还为标注人员提供了一个用于与 ChatGPT ${}^{3}$ 一起使用以获取灵感的提示，提示包含每个网站的概览并指示模型描述在这些站点上可执行的潜在任务。此外，我们提供了经过整理的示例供标注人员参考。


Intent Analysis In total, we curated 241 templates and 812 instantiated intents. On average, each template is instantiated to 3.3 examples. The intent distribution is shown in Figure 6. Furthermore, we classify the intents into three primary categories with examples shown in Figure 5
意图分析 我们共策划了 241 个模板和 812 个实例化意图。平均每个模板被实例化为 3.3 个示例。意图分布见图 6。此外，我们将意图划分为三大类，示例见图 5


---



https://chat.openai.com/



---



(1) Information-seeking tasks expect a textual response. Importantly, these tasks in WebArena often require navigation across multiple pages or focus on user-centric content. This makes them distinct from open-domain question-answering (Yang et al., 2018; Kwiatkowski et al., 2019), which focuses on querying general knowledge with a simple retrieval step. For instance, to answer "When was the last time I bought the shampoo", an agent traverses the user's purchase history, checking order details to identify the most recent shampoo purchase.
（1）信息检索任务期望文本型响应。重要的是，WebArena 中的这些任务常需跨多页导航或聚焦用户中心内容，使其不同于侧重通过简单检索步骤查询通用知识的开放域问答（Yang et al., 2018；Kwiatkowski et al., 2019）。例如，为回答“我上次什么时候买过洗发水”，代理需遍历用户购买历史，检查订单详情以识别最近一次购买洗发水的时间。


(2) Site navigation: This category is composed of tasks that require navigating through web pages using a variety of interactive elements such as search functions and links. The objective is often to locate specific information or navigate to a particular section of a site.
（2）网站导航：此类由需要通过搜索功能和链接等各种交互元素在网页间导航的任务组成。目标通常是定位特定信息或导航至网站的特定部分。


(3) Content and configuration operation: This category encapsulates tasks that require operating in the web environment to create, revise, or configure content or settings. This includes adjusting settings, managing accounts, performing online transactions, generating new web content, and modifying existing content. Examples range from updating a social media status or README file to conducting online purchases and configuring privacy settings.
（3）内容与配置操作：此类涵盖需在网页环境中创建、修改或配置内容或设置的任务，包括调整设置、管理账户、执行在线交易、生成新网页内容和修改现有内容。示例从更新社交媒体状态或 README 文件到进行在线购物和配置隐私设置不等。


### 3.2 EVALUATION ANNOTATION
### 3.2 评估标注


Evaluating Information Seeking Tasks To measure the correctness of information-seeking tasks where a textual answer is expected,we provide the annotated answer ${a}^{ * }$ for each intent. The ${a}^{ * }$ is further compared with the predicted answer $\widehat{a}$ with one of the following scoring functions ${r}_{\text{ info }}\left( {\widehat{a},{a}^{ * }}\right)$ .
评估信息检索任务 为了衡量期望文本答案的信息检索任务的正确性，我们为每个意图提供带注释的答案 ${a}^{ * }$。然后将 ${a}^{ * }$ 与预测答案 $\widehat{a}$ 使用下列评分函数之一 ${r}_{\text{ info }}\left( {\widehat{a},{a}^{ * }}\right)$ 进行比较。


First,we define exact_match where only $\widehat{a}$ that is identical with ${a}^{ * }$ receives a score of one. This function is primarily applicable to intent types whose responses follow a more standardized format, similar to the evaluation on question answering literature (Rajpurkar et al. 2016, Yang et al. 2018).
首先，我们定义 exact_match，只有与 ${a}^{ * }$ 完全相同的 $\widehat{a}$ 才得一分。该函数主要适用于回答格式更为标准化的意图类型，类似于问答评估文献中的做法（Rajpurkar et al. 2016, Yang et al. 2018）。


Second,we create must_include where any $\widehat{a}$ containing ${a}^{ * }$ receives a score of one. This function is primarily used in when an unordered list of text is expected or where the emphasis of evaluation is on certain key concepts. In the second example in Table 1, we expect both the correct name and the email address to be presented, irrespective of the precise wording used to convey the answer.
其次，我们设立 must_include，只要 $\widehat{a}$ 包含 ${a}^{ * }$ 即得一分。该函数主要用于预期为无序文本列表或评估侧重某些关键概念的场景。在表1的第二个示例中，我们期望同时呈现正确的姓名和电子邮件地址，而不要求具体措辞一致。


Finally,we introduce fuzzy_match where we utilize a language model to assess whether $\widehat{a}$ is semantically equivalent to ${a}^{ * }$ . Specifically,in this work,we use gpt-4-0613 to perform this evaluation. The corresponding prompt details are provided in Appendix A.7 The fuzzy_match function applies to situations where the format of the answer is diverse. For instance, in responding to "Compare the time for walking and driving route from AMC Waterfront to Randyland", it is essential to ensure that driving time and walking time are accurately linked with the correct terms. The fuzzy_match function could also flexibly match the time "2h58min" with different forms such as "2 hour 58 minutes", "2:58" and others. We demonstrate a language model can achieve nearly perfect performance on this task in §A.8
最后，我们引入 fuzzy_match，利用语言模型判断 $\widehat{a}$ 与 ${a}^{ * }$ 在语义上是否等价。具体地，在本工作中我们使用 gpt-4-0613 来执行该评估，相关提示详见附录 A.7。fuzzy_match 适用于答案格式多样的情况。例如，响应“Compare the time for walking and driving route from AMC Waterfront to Randyland”时，必须确保驾驶时间和步行时间与正确术语准确对应。fuzzy_match 还可灵活匹配“2h58min”与“2 hour 58 minutes”“2:58”等不同表述。我们在 §A.8 展示了语言模型在此任务上几乎能达到完美的表现。


Evaluating Site Navigation and Content & Config Tasks The tasks in these categories require accessing web pages that meet certain conditions or performing operations that modify the underlying data storage of the respective websites. To assess these,we establish reward functions ${r}_{\text{ prog }}\left( \mathbf{s}\right)$ that programmatically examine the intermediate states $\mathbf{s}$ within an execution trajectory to ascertain whether the outcome aligns with the intended result. These intermediate states are often the underlying databases of the websites, the status, and the content of a web page at each step of the execution.
评估站点导航与内容及配置任务 这些类别的任务需要访问满足特定条件的网页或执行修改相应网站底层数据存储的操作。为此，我们建立奖励函数 ${r}_{\text{ prog }}\left( \mathbf{s}\right)$，以编程方式检查执行轨迹中的中间状态 $\mathbf{s}$，以判断结果是否与预期一致。这些中间状态通常是网站的底层数据库、每一步执行时网页的状态和内容。


Evaluating each instance involves two components. First, we provide a locator, tasked with retrieving the critical content pertinent to each intent. The implementation of this locator varies from a database query, a website-supported API call, to a JavaScript element selection on the relevant web page, depending on implementation feasibility. For example, the evaluation process for the intent of the fifth example in Table 1 first obtains the URL of the latest post by examining the last state in the state sequence s. Then it navigates to the corresponding post page and obtains the post's content by running the Javascript "document.querySelector('.submission_inner').outerText".
评估每个实例包括两部分。首先，我们提供一个定位器，用于检索与每个意图相关的关键内容。该定位器的实现视可行性而异，可为数据库查询、网站支持的 API 调用或相关网页上的 JavaScript 元素选择。例如，表1 第五个示例的评估过程先通过检查状态序列 s 的最后状态获取最新帖子的 URL，然后导航到相应帖子页面并通过运行 JavaScript "document.querySelector('.submission_inner').outerText" 获取帖子的内容。


Subsequently, we annotate keywords that need to exist within the located content. For example, the evaluation verifies if the post is correctly posted in the "nyc" subreddit by examining the URL of the post and if the post contains the requested content by examining the post content. We reuse the exact_match and must_include functions from information-seeking tasks for this purpose.
随后，我们标注需要在定位到的内容中存在的关键词。例如，评估通过检查帖子的 URL 来验证帖子是否正确发布在 "nyc" 子版块，并通过检查帖子内容来验证是否包含所请求的内容。为此我们重用信息检索任务中的 exact_match 和 must_include 函数。


<table><tr><td>Function</td><td>ID</td><td>Intent</td><td>Eval Implementation</td></tr><tr><td rowspan="3">${r}_{\text{ info }}\left( {{a}^{ * },\widehat{a}}\right)$</td><td>1</td><td>Tell me the name of the customer who has the most cancellations in the history</td><td>exact_match( $\widehat{a}$ ,"Samantha Jones")</td></tr><tr><td>2</td><td>Find the customer name and email with phone number 8015551212</td><td>must_include( $\widehat{a}$ ,"Sean Miller") must_include( $\widehat{a}$ ,"sean@gmail.com")</td></tr><tr><td>3</td><td>Compare walking and driving time from AMC Waterfront to Randyland</td><td>fuzzy_match( $\widehat{a}$ ,"walking: 2h58min") fuzzy_match( $\widehat{a}$ ,"driving: 21min")</td></tr><tr><td rowspan="2">${r}_{\text{ prog }}\left( \mathbf{s}\right)$</td><td>4</td><td>Checkout merge requests assigned to me</td><td>url=locate_current_url(s) exact_match (URL, “gitlab.com/merge_ requests?assignee_username=byteblaze")</td></tr><tr><td>5</td><td>Post to ask "whether I need a car in NYC"</td><td>url=locate_latest_post_url(s) body=locate_latest_post_body(s) must_include(URL, "/f/nyc") must_include(body, "a car in NYC")</td></tr></table>
<table><tbody><tr><td>功能</td><td>ID</td><td>意图</td><td>评估实现</td></tr><tr><td rowspan="3">${r}_{\text{ info }}\left( {{a}^{ * },\widehat{a}}\right)$</td><td>1</td><td>告诉我历史上取消次数最多的客户姓名</td><td>exact_match( $\widehat{a}$ ,"Samantha Jones")</td></tr><tr><td>2</td><td>查找电话号码为 8015551212 的客户姓名和电子邮件</td><td>must_include( $\widehat{a}$ ,"Sean Miller") must_include( $\widehat{a}$ ,"sean@gmail.com")</td></tr><tr><td>3</td><td>比较从 AMC Waterfront 到 Randyland 的步行和驾车时间</td><td>fuzzy_match( $\widehat{a}$ ,"walking: 2h58min") fuzzy_match( $\widehat{a}$ ,"driving: 21min")</td></tr><tr><td rowspan="2">${r}_{\text{ prog }}\left( \mathbf{s}\right)$</td><td>4</td><td>查看分配给我的合并请求</td><td>url=locate_current_url(s) exact_match (URL, “gitlab.com/merge_ requests?assignee_username=byteblaze")</td></tr><tr><td>5</td><td>发布询问“我在纽约市需要车吗”</td><td>url=locate_latest_post_url(s) body=locate_latest_post_body(s) must_include(URL, "/f/nyc") must_include(body, "a car in NYC")</td></tr></tbody></table>


Table 1: We introduce two evaluation approaches. ${r}_{\text{ info }}$ (top) measures the correctness of performing information-seeking tasks. It compares the predicted answer $\widehat{a}$ with the annotated reference ${a}^{ * }$ with three implementations. ${r}_{\text{ prog }}$ (bottom) programmatically checks whether the intermediate states during the executions possess the anticipated properties specified by the intent.
表 1：我们引入两种评估方法。${r}_{\text{ info }}$（上）衡量执行信息检索任务的正确性。它将预测答案 $\widehat{a}$ 与带注释的参考答案 ${a}^{ * }$ 通过三种实现方式进行比较。${r}_{\text{ prog }}$（下）以程序化方式检查执行过程中的中间状态是否具备意图所指定的预期属性。


Unachievable Tasks Due to constraints such as inadequate evidence, user permissions (\$A.3), or the absence of necessary functional support on the website, humans may ask for tasks that are not possible to complete. Inspired by previous work on evaluating question-answering models on unanswerable questions (Rajpurkar et al. 2018), we design unachievable tasks in WebArena. For instance, fulfilling an intent like "Tell me the contact number of OneStopShop" is impracticable in WebArena, given that the website does not provide such contact information. We label such instances as "N/A" and expect an agent to produce an equivalent response. These examples allow us to assess an agent's ability to avoid making unfounded claims and its adherence to factual accuracy.
因约束而无法完成的任务 由于证据不足、用户权限（\$A.3）或网站缺乏必要功能支持等原因，人类可能会提出无法完成的任务。受此前在不可答问题上评估问答模型工作的启发（Rajpurkar et al. 2018），我们在 WebArena 中设计了不可完成的任务。例如，在 WebArena 中实现“告诉我 OneStopShop 的联系电话”这样的意图是不可能的，因为该网站未提供此类联系信息。我们将此类实例标注为“N/A”，并期望代理给出等效响应。这些例子使我们能够评估代理避免做无根据断言的能力及其对事实准确性的遵守情况。


Annotation Process The intents were contributed by the authors following the annotation guideline in §3.1 Every author has extensive experience with web-based tasks. The reference answers to the information-seeking tasks were curated by the authors and an external annotator. To ensure consistency and accuracy, each question was annotated twice. If the two annotators disagreed, a third annotator finalized the annotation. The programs to evaluate the remaining examples were contributed by three of the authors who are proficient in JavaScript programming. Difficult tasks were often discussed collectively to ensure the correctness of the annotation. The annotation required the annotator to undertake the full execution and scrutinize the intermediate states.
标注过程 意图由作者根据 §3.1 的标注指南贡献。每位作者在基于网页的任务方面都有丰富经验。信息检索任务的参考答案由作者和一名外部标注者整理。为确保一致性和准确性，每个问题由两人标注；若两位标注者意见不一致，则由第三位标注者定稿。用于评估其余示例的程序由三位精通 JavaScript 编程的作者贡献。对难题常进行集体讨论以确保标注正确。标注要求标注者进行完整执行并仔细审查中间状态。


Human Performance We sample one task from each of the 170 templates and ask five computer science graduate students to perform these tasks. The human performance is on the right. Overall, the human annotators complete 78.24% of the tasks, with lower performance on information-seeking tasks. Through examining the recorded trajectories, we found that ${50}\%$ of the failures are due to misinterpreting the intent (e.g.,providing travel distance when asked for travel time), incomplete answers (e.g., providing only name when asked for name and email), and incomplete executions (e.g., partially filling the product information), while the remaining instances have more severe failures, where the executions are off-target. More discussions on human annotations can be found in §A.5
人工表现 我们从 170 个模板中各抽取一个任务，并请五名计算机科学研究生执行这些任务。人工表现见右侧。总体上，人工标注者完成了 78.24% 的任务，信息检索任务的表现较低。通过检查记录的轨迹，我们发现 ${50}\%$ 的失败是由于误解意图（例如在被问及旅行时间时提供了旅行距离）、答案不完整（例如在被问及姓名和邮箱时只提供了姓名）和执行不完整（例如部分填写了产品信息），其余实例则为更严重的失败，执行偏离目标。关于人工标注的更多讨论见 §A.5


<table><tr><td>Avg. Time</td><td>110s</td></tr><tr><td>Success Rate ${}_{\text{ info }}$</td><td>74.68%</td></tr><tr><td>Success Rate ${}_{\text{ others }}$</td><td>81.32%</td></tr><tr><td>Success Rate ${}_{\text{ all }}$</td><td>78.24%</td></tr></table>
<table><tbody><tr><td>平均时间</td><td>110s</td></tr><tr><td>成功率 ${}_{\text{ info }}$</td><td>74.68%</td></tr><tr><td>成功率 ${}_{\text{ others }}$</td><td>81.32%</td></tr><tr><td>成功率 ${}_{\text{ all }}$</td><td>78.24%</td></tr></tbody></table>


## 4 BASELINE WEB AGENTS
## 4 基线网页代理


We experiment with three LLMs using two prompting strategies, both with two examples in the context. In the first setting, we ask the LLM to directly predict the next action given the current observation, the intent and the previously performed action. In the second setting, with the same information, the model first performs chain-of-thought reasoning steps in the text before the action prediction (CoT, Wei et al. (2022); Yao et al. (2022b)). Before the examples, we provide a detailed overview of the browser environment, the allowed actions, and many rules. To make the model aware of the unachievable tasks, the instruction explicitly asks the agent to stop if it believes the task is impossible to perform. We refer to this directive as Unachievable hint, or UA hint. This introduction is largely identical to the guidelines we presented to human annotators to ensure a fair comparison. We use an accessibility tree with element IDs as the observation space. The agent can identify which element to interact with by the ID of the element. For instance, the agent can issue click [1582] to click the "Add to Cart" button with the ID of 1582. The full prompts can be found in Appendix A.9 The detailed configurations of each model can be found in Appendix A.6
我们在三种大型语言模型上用两种提示策略进行实验，且每种策略在上下文中都给出两个示例。在第一种设置中，我们要求模型在给定当前观察、意图和先前执行动作的情况下直接预测下一个动作。在第二种设置中，在相同信息下，模型先在文本中进行链式思考推理步骤，然后再预测动作（CoT，Wei 等，2022；Yao 等，2022b）。在示例之前，我们提供了浏览器环境、允许的动作和许多规则的详细概述。为了让模型意识到不可达成的任务，指令明确要求代理在认为任务不可能完成时停止。我们将此指令称为不可达提示，或 UA 提示。该介绍在很大程度上与我们提供给人工标注者的指南相同，以确保公平比较。我们使用带有元素 ID 的可访问性树作为观察空间。代理可以通过元素的 ID 来识别要交互的元素。例如，代理可以发出 click [1582] 来点击 ID 为 1582 的“加入购物车”按钮。完整提示见附录 A。9 各模型的详细配置见附录 A.6


## 5 RESULTS
## 5 结果


The main results are shown on the top of Table 2, GPT-4 (OpenAI, 2023) with CoT prompting achieves a modest end-to-end task success rate of 11.70%, which is significantly lower than the human performance of 78.24%. GPT-3.5 (OpenAI, 2022) with CoT prompting is only able to successfully perform 8.75% of the tasks. The explicit reasoning procedure is somewhat helpful, it brings 2.34% improvement over the version without it. Further, TEXT-BISON-001 (Anil et al. 2023) underperforms GPT-3.5, with a success rate of 5.05%. These results underline the inherent challenges and complexities of executing tasks that span long horizons, particularly in realistic environments such as WebArena.
主要结果见表 2 上方，GPT-4（OpenAI，2023）在 CoT 提示下的端到端任务成功率为 11.70%，明显低于人类的 78.24%。GPT-3.5（OpenAI，2022）在 CoT 提示下仅能成功完成 8.75% 的任务。显式推理过程有一定帮助，相较于无该过程的版本带来 2.34% 的提升。此外，TEXT-BISON-001（Anil 等，2023）表现不及 GPT-3.5，成功率为 5.05%。这些结果突显了在如 WebArena 这类真实环境中执行长程任务的内在挑战与复杂性。


<table><tr><td>CoT</td><td>UA Hint</td><td>Model</td><td>SR</td><td>${\mathrm{{SR}}}_{\mathrm{{AC}}}$</td><td>SRUA</td></tr><tr><td>✓</td><td>✓</td><td>TEXT-BISON-001</td><td>5.05</td><td>4.00</td><td>27.78</td></tr><tr><td>✘</td><td>✓</td><td>GPT-3.5</td><td>6.41</td><td>4.90</td><td>38.89</td></tr><tr><td>✓</td><td>✓</td><td>GPT-3.5</td><td>8.75</td><td>6.44</td><td>58.33</td></tr><tr><td>✓</td><td>✓</td><td>GPT-4</td><td>11.70</td><td>8.63</td><td>77.78</td></tr><tr><td>✘</td><td>✘</td><td>GPT-3.5</td><td>5.10</td><td>4.90</td><td>8.33</td></tr><tr><td>✓</td><td>✘</td><td>GPT-3.5</td><td>6.16</td><td>6.06</td><td>8.33</td></tr><tr><td>✓</td><td>✘</td><td>GPT-4</td><td>14.41</td><td>13.02</td><td>44.44</td></tr><tr><td>-</td><td>✓</td><td>Human</td><td>78.24</td><td>77.30</td><td>100.00</td></tr></table>
<table><tbody><tr><td>CoT</td><td>UA 提示</td><td>模型</td><td>SR</td><td>${\mathrm{{SR}}}_{\mathrm{{AC}}}$</td><td>SRUA</td></tr><tr><td>✓</td><td>✓</td><td>TEXT-BISON-001</td><td>5.05</td><td>4.00</td><td>27.78</td></tr><tr><td>✘</td><td>✓</td><td>GPT-3.5</td><td>6.41</td><td>4.90</td><td>38.89</td></tr><tr><td>✓</td><td>✓</td><td>GPT-3.5</td><td>8.75</td><td>6.44</td><td>58.33</td></tr><tr><td>✓</td><td>✓</td><td>GPT-4</td><td>11.70</td><td>8.63</td><td>77.78</td></tr><tr><td>✘</td><td>✘</td><td>GPT-3.5</td><td>5.10</td><td>4.90</td><td>8.33</td></tr><tr><td>✓</td><td>✘</td><td>GPT-3.5</td><td>6.16</td><td>6.06</td><td>8.33</td></tr><tr><td>✓</td><td>✘</td><td>GPT-4</td><td>14.41</td><td>13.02</td><td>44.44</td></tr><tr><td>-</td><td>✓</td><td>人类</td><td>78.24</td><td>77.30</td><td>100.00</td></tr></tbody></table>


Table 2: The end-to-end task success rate (SR %) on WebArena with different prompting strategies. CoT: the model performs step-by-step reasoning before issuing the action. UA hint: ask the model to stop when encountering unachievable questions.
表 2：在 WebArena 上不同提示策略下的端到端任务成功率（SR %）。CoT：模型在执行动作前进行逐步推理。UA hint：提示模型在遇到不可实现的问题时停止。


### 5.1 ANALYSIS
### 5.1 分析


Do models know when to stop? In our error analysis of the execution trajectories, we observe a prevalent error pattern of early stopping due to the model's conclusion of unachievability. For instance, GPT-4 erroneously identifies 54.9% of feasible tasks as impossible. This issue primarily stems from the UA hint in the instruction, while this hint allows models to identify unachievable tasks, it also hinders performance on achievable tasks. To address this, we conduct an ablation study where we remove this hint. We then break down the success rate for both achievable and unachievable tasks. As shown in Table 2, eliminating this instruction led to a performance boost in achievable tasks, enhancing the overall task success rate of GPT-4 to 14.41%. Despite an overall decline in identifying unachievable tasks, GPT-4 retains the capacity to recognize 44.44% of such tasks. It does so by generating reasons of non-achievability, even without explicit instructions. On the other hand, GPT-3.5 rarely exhibits this level of reasoning. Instead, it tends to follow problematic patterns such as hallucinating incorrect answers, repeating invalid actions, or exceeding the step limits. This result suggests that even subtle differences in instruction design can significantly influence the behavior of a model in performing interactive tasks in complex environments.
模型知道何时停止吗？在对执行轨迹的错误分析中，我们观察到一种普遍的早停错误模式，原因是模型判定任务不可实现。例如，GPT-4 错误地将 54.9% 的可行任务认定为不可能。该问题主要源自指令中的 UA hint；虽然该提示使模型能够识别不可实现的任务，但也妨碍了对可实现任务的表现。为了解决这一点，我们进行了消融研究，移除了该提示，并分别统计了可实现与不可实现任务的成功率。如表 2 所示，删除该指令提升了可实现任务的表现，使 GPT-4 的总体任务成功率提升到 14.41%。尽管识别不可实现任务的总体能力有所下降，GPT-4 仍能识别 44.44% 的此类任务，且即便没有明确指示，也会生成不可实现的理由。另一方面，GPT-3.5 很少表现出这种程度的推理；相反，它倾向于出现幻觉性错误答案、重复无效动作或超出步骤限制等问题。该结果表明，即使是指令设计的细微差别，也会显著影响模型在复杂交互环境中执行任务的行为。


Can a model maintain consistent performance across similar tasks? Tasks that originate from the same template usually follow similar reasoning and planning processes, even though their observations and executions will differ. We plot a histogram of per-template success rates for our models in Table 3. Of the 61 templates, GPT-4 manages to achieve a 100% task success rate on only four templates, while GPT-3.5 fails to achieve full task completion for any of the templates. In many cases, the models are only able to complete one task variation with a template. These observations indicate that even when tasks are derived from the same template, they can present distinct challenges. For instance, while "Fork metaseq" can be a straightforward task, "Fork all repos from Facebook" derived from the same template requires more repetitive operations, hence increasing its complexity. Therefore, WebArena provide a testbed to evaluate more sophisticated methods. In particular, those that incorporate memory components,
模型能在相似任务间保持一致的表现吗？源自同一模板的任务通常遵循类似的推理与规划过程，尽管它们的观测与执行会不同。我们在表 3 中绘制了各模板成功率的直方图。在 61 个模板中，GPT-4 仅在 4 个模板上达到 100% 的任务成功率，而 GPT-3.5 在任何模板上都未能实现全部任务完成。在许多情况下，模型只能完成某模板的一个任务变体。这些观察表明，即便任务来源于相同模板，它们仍可能带来不同的挑战。例如，“Fork metaseq” 可能是一个简单任务，但同一模板衍生的 “Fork all repos from Facebook” 需要更多重复操作，从而增加了复杂性。因此，WebArena 提供了评估更复杂方法的测试场景，特别是那些包含记忆组件的方法，


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_43_429a57.jpg"/>



Table 3: Distribution of success rates on templates with $\geq  1$ successful executions on GPT models (no UA hint).
表 3：在 GPT 模型上模板成功率分布，$\geq  1$ 次成功执行（无 UA hint）。


<table><tr><td colspan="2">Benchmark</td><td>Dynamic Interaction?</td><td>Realistic Environment?</td><td>Diverse Human Tasks?</td><td>Functional Correctness?</td></tr><tr><td>Mind2Web</td><td>Deng et al. 2023)</td><td>✘</td><td>✓</td><td>✓</td><td>✘</td></tr><tr><td>Form/QAWoB</td><td>Shi et al. 2017)</td><td>✘</td><td>✓</td><td>✓</td><td>✘</td></tr><tr><td>MiniWoB++</td><td>Liu et al. 2018)</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td></tr><tr><td>Webshop</td><td>Yao et al. 2022a</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td></tr><tr><td>ALFRED</td><td>Shridhar et al. 2020</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td></tr><tr><td>VirtualHome</td><td>Pug et al. 2018)</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>AndroidEnv</td><td>Toyama et al. (2021)</td><td>✓</td><td>✓</td><td>✘</td><td>✘</td></tr><tr><td colspan="2">WebArena</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr></table>
<table><tbody><tr><td colspan="2">基准测试</td><td>动态交互？</td><td>真实环境？</td><td>多样化的人类任务？</td><td>功能正确性？</td></tr><tr><td>Mind2Web</td><td>Deng 等，2023)</td><td>✘</td><td>✓</td><td>✓</td><td>✘</td></tr><tr><td>表单/QAWoB</td><td>Shi 等，2017)</td><td>✘</td><td>✓</td><td>✓</td><td>✘</td></tr><tr><td>MiniWoB++</td><td>Liu 等，2018)</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td></tr><tr><td>Webshop</td><td>Yao 等，2022a</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td></tr><tr><td>ALFRED</td><td>Shridhar 等，2020</td><td>✓</td><td>✘</td><td>✘</td><td>✓</td></tr><tr><td>VirtualHome</td><td>Pug 等，2018)</td><td>✘</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>AndroidEnv</td><td>Toyama 等，（2021）</td><td>✓</td><td>✓</td><td>✘</td><td>✘</td></tr><tr><td colspan="2">WebArena</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr></tbody></table>


Table 4: The comparison between our benchmark and existing benchmarks on grounding natural language instructions to concrete executions. Our benchmark is implemented in our fully interactable highly-realistic environment. It features diverse tasks humans may encounter in their daily routines. We design evaluation metrics to assess the functional correctness of task executions.
表 4：我们基准与现有将自然语言指令绑定到具体执行的基准比较。我们的基准在可完全交互的高度真实环境中实现，包含人们日常可能遇到的多样任务，并设计了评估任务执行功能正确性的指标。


enabling the reuse of successful strategies from past experiments Zhou et al. (2022a); Wang et al. (2023). More error analysis with examples can be found in Appendix A.10
使得可复用先前实验中成功策略 Zhou et al. (2022a); Wang et al. (2023)。更多带示例的错误分析见附录 A.10


## 6 RELATED WORK
## 6 相关工作


Benchmarks for Controlling Agents through Natural Language Controlling agents via natural language in the digital world have been studied in the literature (Branavan et al. 2009; Shi et al. 2017) Liu et al. 2018; Toyama et al. 2021, Deng et al. 2023; Li et al. 2020; Xu et al. 2021). However, the balance between functionality, authenticity, and support for environmental dynamics remains a challenge. Existing benchmarks often compromise these aspects, as shown in Table 4 . Some works rely on static states, limiting agents' explorations and functional correctness evaluation (Shi et al. 2017, Deng et al. 2023), while others simplify real-world complexities, restricting task variety (Yao et al. 2022a; Liu et al. 2018). While AndroidEnv (Toyama et al. 2021) replicates an Android setup, it does not guarantee the reproducibility since live Android applications are used. (Kolve et al. 2017) Shridhar et al. 2020, Puig et al. 2018) and extends to gaming environments (Fan et al. 2022) Küttler et al. 2020), where the environment mechanisms often diverge from human objectives.
通过自然语言控制智能体的基准 在数字世界中通过自然语言控制智能体已有大量研究(Branavan et al. 2009; Shi et al. 2017) Liu et al. 2018; Toyama et al. 2021, Deng et al. 2023; Li et al. 2020; Xu et al. 2021)。然而，功能性、真实性与对环境动态支持之间的平衡仍是挑战。现有基准常在这些方面做出妥协，如表 4 所示。有些工作依赖静态状态，限制了智能体的探索与功能正确性评估(Shi et al. 2017, Deng et al. 2023)，而另一些简化了现实复杂性，限制了任务多样性(Yao et al. 2022a; Liu et al. 2018)。尽管 AndroidEnv (Toyama et al. 2021) 重现了 Android 环境，但由于使用真实 Android 应用，无法保证可复现性。(Kolve et al. 2017) Shridhar et al. 2020, Puig et al. 2018) 并扩展至游戏环境 (Fan et al. 2022) Küttler et al. 2020)，这些环境机制往往与人类目标存在偏离。


Interactive Decision-Making Agents Nakano et al. (2021) introduce WebGPT which searches the web and reads the search results to answer questions. Gur et al. (2023) propose a web agent that synthesizes Javascript code for the task executions. Adding a multi-modal dimension, Lee et al. (2023) and Shaw et al. (2023) develop agents that predict actions based on screenshots of web pages rather than relying on the text-based DOM trees. Performing tasks in interactive environments requires the agents to exhibit several capabilities including hierarchical planning, state tracking, and error recovery. Existing works (Huang et al., 2022, Madaan et al., 2022). Li et al., 2023) observe LLMs could break a task into more manageable sub-tasks (Zhou et al., 2022b). This process can be further refined by representing task executions as programs, a technique that aids sub-task management and skill reuse (Zhou et al., 2022a, Liang et al., 2023, Wang et al., 2023, Gao et al., 2023). Meanwhile, search and backtracking methods introduce a more structured approach to planning while also allowing for decision reconsideration (Yao et al. 2023, Long 2023). Existing works also incorporate failure recovery, self-correction (Shinn et al., 2023; Kim et al., 2023), observation summarization (Sridhar et al. 2023) to improve execution robustness. The complexity of WebArena presents a unique challenge and opportunity for further testing and improvement of these methods.
交互式决策智能体 Nakano et al. (2021) 提出 WebGPT，通过搜索网络并阅读结果来回答问题。Gur et al. (2023) 提出一个为任务执行合成 Javascript 代码的网络智能体。加入多模态维度后，Lee et al. (2023) 和 Shaw et al. (2023) 开发了基于网页截图而非文本 DOM 树预测动作的智能体。在交互环境中执行任务要求智能体具备层级规划、状态跟踪和错误恢复等能力。现有工作(Huang et al., 2022, Madaan et al., 2022). Li et al., 2023) 观察到大型语言模型可将任务分解为更易处理的子任务(Zhou et al., 2022b)。将任务执行表示为程序可进一步优化此过程，有助于子任务管理和技能复用(Zhou et al., 2022a, Liang et al., 2023, Wang et al., 2023, Gao et al., 2023)。同时，搜索与回溯方法为规划引入了更结构化的手段并允许决策重考(Yao et al. 2023, Long 2023)。现有工作还结合了故障恢复、自我纠正(Shinn et al., 2023; Kim et al., 2023)、观察摘要(Sridhar et al. 2023) 以提升执行鲁棒性。WebArena 的复杂性为进一步测试和改进这些方法提出了独特的挑战与机遇。


## 7 CONCLUSION
## 7 结论


We present WebArena, a highly-realistic, standalone, and reproducible web environment designed for the development and testing of autonomous agents. WebArena includes fully functional web applications and organic data from popular domains. Additionally, we curate a comprehensive benchmark consisting of 812 examples that focus on mapping high-level natural language intents into concrete web interactions. We also offer outcome-based evaluation that programmatically validate the tasks success. Our experiments show that even GPT-4 only achieves a limited end-to-end task success rate of 14.41%, significantly lagging behind the human performance of 78.24%. These findings underscore the need for future research to focus on enhancing the robustness and efficacy of autonomous agents within WebArena environment.
我们提出 WebArena，一种高度真实、独立且可复现的网页环境，用于自主智能体的开发和测试。WebArena 包含功能完整的网络应用和来自热门领域的真实数据。我们还整理了包含 812 个示例的综合基准，聚焦将高层自然语言意图映射为具体网页交互，并提供基于结果的评估以程序化验证任务是否成功。实验证明即使 GPT-4 的端到端任务成功率也仅为 14.41%，远低于人类的 78.24%。这些发现强调了未来研究需着重提升 WebArena 环境中自主智能体的鲁棒性与效能。


## ACKNOWLEDGEMENT
## 致谢


We would like to thank Emmy Liu, Zhiruo Wang, Zhitong Guo for examining our annotations, Shunyu Yao for providing the raw Amazon product data in Webshop, Pengfei Liu, Zaid Sheikh and Aman Madaan for the helpful discussions. We are also grateful to the Center for AI Safety for providing computational resources. This material is partly based on research sponsored in part by the Air Force Research Laboratory under agreement number FA8750-19-2-0200. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government. This project was also partially supported by a gift from AWS AI.
我们感谢 Emmy Liu、Zhiruo Wang、Zhitong Guo 对我们注释的审阅，感谢 Shunyu Yao 提供 Webshop 中的原始亚马逊产品数据，感谢 Pengfei Liu、Zaid Sheikh 和 Aman Madaan 的有益讨论。我们也感谢 Center for AI Safety 提供计算资源。本材料部分基于由空军研究实验室（协议编号 FA8750-19-2-0200）部分资助的研究。尽管有任何版权标注，美国政府被授权为政府用途复制和分发重印件。文中观点和结论为作者个人观点，不应被解释为必然代表空军研究实验室或美国政府的官方政策或认可（无论明示或暗示）。该项目也部分由 AWS AI 的一笔捐赠支持。


## REFERENCES
## REFERENCES


Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian D. Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18- 22, 2018, pp. 3674-3683. IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.00387. URL http://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_ Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.html
Peter Anderson、Qi Wu、Damien Teney、Jake Bruce、Mark Johnson、Niko Sünderhauf、Ian D. Reid、Stephen Gould 和 Anton van den Hengel。视觉与语言导航：在真实环境中解释以视觉为基础的导航指令。见 2018 年 IEEE 计算机视觉与模式识别会议（CVPR 2018），美国犹他州盐湖城，2018 年 6 月 18-22 日，页 3674-3683。IEEE Computer Society, 2018。doi: 10.1109/CVPR.2018.00387。网址 http://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_ Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.html


Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.
Rohan Anil、Andrew M. Dai、Orhan Firat、Melvin Johnson、Dmitry Lepikhin、Alexandre Passos、Siamak Shakeri、Emanuel Taropa、Paige Bailey、Zhifeng Chen、Eric Chu、Jonathan H. Clark、Laurent El Shafey、Yanping Huang、Kathy Meier-Hellstern、Gaurav Mishra、Erica Moreira、Mark Omernick、Kevin Robinson、Sebastian Ruder、Yi Tay、Kefan Xiao、Yuanzhong Xu、Yujing Zhang、Gustavo Hernandez Abrego、Junwhan Ahn、Jacob Austin、Paul Barham、Jan Botha、James Bradbury、Siddhartha Brahma、Kevin Brooks、Michele Catasta、Yong Cheng、Colin Cherry、Christopher A. Choquette-Choo、Aakanksha Chowdhery、Clément Crepy、Shachi Dave、Mostafa Dehghani、Sunipa Dev、Jacob Devlin、Mark Díaz、Nan Du、Ethan Dyer、Vlad Feinberg、Fangxiaoyu Feng、Vlad Fienber、Markus Freitag、Xavier Garcia、Sebastian Gehrmann、Lucas Gonzalez、Guy Gur-Ari、Steven Hand、Hadi Hashemi、Le Hou、Joshua Howland、Andrea Hu、Jeffrey Hui、Jeremy Hurwitz、Michael Isard、Abe Ittycheriah、Matthew Jagielski、Wenhao Jia、Kathleen Kenealy、Maxim Krikun、Sneha Kudugunta、Chang Lan、Katherine Lee、Benjamin Lee、Eric Li、Music Li、Wei Li、YaGuang Li、Jian Li、Hyeontaek Lim、Hanzhao Lin、Zhongtao Liu、Frederick Liu、Marcello Maggioni、Aroma Mahendru、Joshua Maynez、Vedant Misra、Maysam Moussalem、Zachary Nado、John Nham、Eric Ni、Andrew Nystrom、Alicia Parrish、Marie Pellat、Martin Polacek、Alex Polozov、Reiner Pope、Siyuan Qiao、Emily Reif、Bryan Richter、Parker Riley、Alex Castro Ros、Aurko Roy、Brennan Saeta、Rajkumar Samuel、Renee Shelby、Ambrose Slone、Daniel Smilkov、David R. So、Daniel Sohn、Simon Tokumine、Dasha Valter、Vijay Vasudevan、Kiran Vodrahalli、Xuezhi Wang、Pidong Wang、Zirui Wang、Tao Wang、John Wieting、Yuhuai Wu、Kelvin Xu、Yunhan Xu、Linting Xue、Pengcheng Yin、Jiahui Yu、Qiao Zhang、Steven Zheng、Ce Zheng、Weikang Zhou、Denny Zhou、Slav Petrov 和 Yonghui Wu。Palm 2 技术报告，2023。


Yonatan Bisk, Jan Buys, Karl Pichotta, and Yejin Choi. Benchmarking hierarchical script knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4077-4085, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1412.URLhttps://aclanthology.org/N19-1412
Yonatan Bisk、Jan Buys、Karl Pichotta 和 Yejin Choi。基准化分层脚本知识。载于 2019 年北美计算语言学协会年会：人类语言技术会议论文集，第1卷（长短论文），第 4077–4085 页，明尼阿波利斯，明尼苏达，2019。计算语言学协会。doi: 10.18653/v1/N19-1412.URLhttps://aclanthology.org/N19-1412


S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pp. 82-90, Suntec, Singapore, 2009. Association for Computational Linguistics. URL https://aclanthology.org/P09-1010
S.R.K. Branavan、Harr Chen、Luke Zettlemoyer 和 Regina Barzilay。将指令映射到动作的强化学习。载于第 47 届 ACL 年会与第 4 届 AFNLP 国际联合自然语言处理会议联合会会议论文集，第 82–90 页，新加坡 Suntec，2009。计算语言学协会。URL https://aclanthology.org/P09-1010


Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016. URL https://arxiv.org/abs/1606.01540
Greg Brockman、Vicki Cheung、Ludwig Pettersson、Jonas Schneider、John Schulman、Jie Tang 和 Wojciech Zaremba。Openai gym，2016。URL https://arxiv.org/abs/1606.01540


Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374
Mark Chen、Jerry Tworek、Heewoo Jun、Qiming Yuan、Henrique Ponde、Jared Kaplan、Harri Edwards、Yura Burda、Nicholas Joseph、Greg Brockman 等。评估在代码上训练的大型语言模型。ArXiv 预印本，abs/2107.03374，2021。URL https://arxiv.org/abs/2107.03374


Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web, 2023.
Xiang Deng、Yu Gu、Boyuan Zheng、Shijie Chen、Samuel Stevens、Boshi Wang、Huan Sun 和 Yu Su。Mind2web：迈向通用网络代理，2023。


Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum? id=rc8o_j8I8PX
Linxi Fan、Guanzhi Wang、Yunfan Jiang、Ajay Mandlekar、Yuncong Yang、Haoyi Zhu、Andrew Tang、De-An Huang、Yuke Zhu 和 Anima Anandkumar。Minedojo：用互联网级知识构建开放式具身代理。载于第三十六届神经信息处理系统会议 数据集与基准轨，2022。URL https://openreview.net/forum? id=rc8o_j8I8PX


Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764-10799. PMLR, 2023.
Luyu Gao、Aman Madaan、Shuyan Zhou、Uri Alon、Pengfei Liu、Yiming Yang、Jamie Callan 和 Graham Neubig。PAL：程序辅助语言模型。载于国际机器学习大会，页 10764–10799。PMLR，2023。


Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. IQA: visual question answering in interactive environments. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 4089-4098. IEEE Computer Society, 2018. doi: 10.1109/CVPR. 2018.00430.URLhttp://openaccess.thecvf.com/content_cvpr_2018/html/ Gordon_IQA_Visual_Question_CVPR_2018_paper.html
Daniel Gordon、Aniruddha Kembhavi、Mohammad Rastegari、Joseph Redmon、Dieter Fox 和 Ali Farhadi。IQA：交互环境中的视觉问答。载于 2018 年 IEEE 计算机视觉与模式识别会议（CVPR 2018），美国犹他州盐湖城，2018 年 6 月 18–22 日，第 4089–4098 页。IEEE 计算机学会，2018。doi: 10.1109/CVPR. 2018.00430.URLhttp://openaccess.thecvf.com/content_cvpr_2018/html/ Gordon_IQA_Visual_Question_CVPR_2018_paper.html


Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023.
Izzeddin Gur、Hiroki Furuta、Austin Huang、Mustafa Safdari、Yutaka Matsuo、Douglas Eck 和 Aleksandra Faust。具有规划、长上下文理解和程序综合的真实世界网络代理。arXiv 预印本 arXiv:2307.12856，2023。


Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 9118-9147. PMLR, 2022. URL https://proceedings.mlr.press/v162/huang22a.html
Wenlong Huang、Pieter Abbeel、Deepak Pathak 和 Igor Mordatch。将语言模型作为零样本规划器：为具身代理提取可执行知识。载于 Kamalika Chaudhuri、Stefanie Jegelka、Le Song、Csaba Szepesvári、Gang Niu 与 Sivan Sabato（编），国际机器学习大会 ICML 2022，2022 年 7 月 17–23，巴尔的摩，马里兰，美国，Proceedings of Machine Learning Research，第 162 卷，第 9118–9147 页。PMLR，2022。URL https://proceedings.mlr.press/v162/huang22a.html


Yacine Jernite, Kavya Srinet, Jonathan Gray, and Arthur Szlam. CraftAssist Instruction Parsing: Semantic Parsing for a Minecraft Assistant. ArXiv preprint, abs/1905.01978, 2019. URL https: //arxiv.org/abs/1905.01978
Yacine Jernite、Kavya Srinet、Jonathan Gray 和 Arthur Szlam。CraftAssist 指令解析：用于 Minecraft 助手的语义解析。ArXiv 预印本，abs/1905.01978，2019。URL https: //arxiv.org/abs/1905.01978


Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. ArXiv preprint, abs/2303.17491, 2023. URL https://arxiv.org/abs/2303.17491
Geunwoo Kim、Pierre Baldi 和 Stephen McAleer。语言模型可以解决计算机任务。ArXiv 预印本，abs/2303.17491，2023。URL https://arxiv.org/abs/2303.17491


Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv, 2017.
Eric Kolve、Roozbeh Mottaghi、Winson Han、Eli VanderBilt、Luca Weihs、Alvaro Herrasti、Daniel Gordon、Yuke Zhu、Abhinav Gupta 和 Ali Farhadi。AI2-THOR：用于视觉 AI 的交互式 3D 环境。arXiv，2017。


Heinrich Küttler, Nantas Nardelli, Alexander H. Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, and Tim Rocktäschel. The nethack learning environment. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 569ff987c643b4bedf504efda8f786c2-Abstract.html
Heinrich Küttler、Nantas Nardelli、Alexander H. Miller、Roberta Raileanu、Marco Selvatici、Edward Grefenstette 和 Tim Rocktäschel。nethack 学习环境。见 Hugo Larochelle、Marc'Aurelio Ranzato、Raia Hadsell、Maria-Florina Balcan 和 Hsuan-Tien Lin（编），《Advances in Neural Information Processing Systems 33：NeurIPS 2020 年年会论文集》，2020 年 12 月 6-12 日，线上，2020。URL https://proceedings.neurips.cc/paper/2020/hash/ 569ff987c643b4bedf504efda8f786c2-Abstract.html


Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026
Tom Kwiatkowski、Jennimaria Palomaki、Olivia Redfield、Michael Collins、Ankur Parikh、Chris Alberti、Danielle Epstein、Illia Polosukhin、Jacob Devlin、Kenton Lee、Kristina Toutanova、Llion Jones、Matthew Kelcey、Ming-Wei Chang、Andrew M. Dai、Jakob Uszkoreit、Quoc Le 和 Slav Petrov。Natural Questions：一个面向问答研究的基准。Transactions of the Association for Computational Linguistics，7：452-466，2019。doi: 10.1162/tacl_a_00276。URL https://aclanthology.org/Q19-1026


Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62, 2022.
Yann LeCun。一条通往自主机器智能的路径 0.9 版。2，2022-06-27。Open Review，62，2022。


Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In International Conference on Machine Learning, pp. 18893-18912. PMLR, 2023.
Kenton Lee、Mandar Joshi、Iulia Raluca Turc、Hexiang Hu、Fangyu Liu、Julian Martin Eisenschlos、Urvashi Khandelwal、Peter Shaw、Ming-Wei Chang 和 Kristina Toutanova。Pix2struct：将截图解析作为视觉语言理解的预训练。载于国际机器学习大会，页 18893-18912。PMLR，2023。


Xinze Li, Yixin Cao, Muhao Chen, and Aixin Sun. Take a break in the middle: Investigating subgoals towards hierarchical script generation. ArXiv preprint, abs/2305.10907, 2023. URL https://arxiv.org/abs/2305.10907
Xinze Li、Yixin Cao、Muhao Chen 和 Aixin Sun。中途休息：探讨分目标以实现分层脚本生成。ArXiv 预印本，abs/2305.10907，2023。URL https://arxiv.org/abs/2305.10907


Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile UI action sequences. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8198-8210, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.729. URL https: //aclanthology.org/2020.acl-main.729
Yang Li、Jiacong He、Xin Zhou、Yuan Zhang 和 Jason Baldridge。将自然语言指令映射为移动 UI 操作序列。载于第 58 届计算语言学协会年会论文集，页 8198-8210，线上，2020。计算语言学协会。doi: 10.18653/v1/2020.acl-main.729。URL https: //aclanthology.org/2020.acl-main.729


Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 9493-9500. IEEE, 2023.
Jacky Liang、Wenlong Huang、Fei Xia、Peng Xu、Karol Hausman、Brian Ichter、Pete Florence 和 Andy Zeng。将代码作为策略：用于具身控制的语言模型程序。载于 2023 年 IEEE 国际机器人与自动化会议（ICRA），页 9493-9500。IEEE，2023。


Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id= ryTp3f-0-
Evan Zheran Liu、Kelvin Guu、Panupong Pasupat、Tianlin Shi 和 Percy Liang。在网页界面上使用工作流引导探索的强化学习。载于第 6 届国际表征学习会议 ICLR 2018，温哥华，BC，加拿大，2018 年 4 月 30 日 - 5 月 3 日，会议论文集。OpenReview.net，2018。URL https://openreview.net/forum?id= ryTp3f-0-


Jieyi Long. Large language model guided tree-of-thought. ArXiv preprint, abs/2305.08291, 2023. URLhttps://arxiv.org/abs/2305.08291
Jieyi Long。大语言模型引导的思维树。ArXiv 预印本，abs/2305.08291，2023。URLhttps://arxiv.org/abs/2305.08291


Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1384-1403, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. URL https://aclanthology.org/ 2022.emnlp-main.90
Aman Madaan、Shuyan Zhou、Uri Alon、Yiming Yang 和 Graham Neubig。《代码的语言模型是少样本常识学习者》。载于 2022 年实证方法在自然语言处理会议论文集，页 1384-1403，阿布扎比，阿拉伯联合酋长国，2022。计算语言学协会。URL https://aclanthology.org/ 2022.emnlp-main.90


Dipendra K Misra, Jaeyong Sung, Kevin Lee, and Ashutosh Saxena. Tell me dave: Context-sensitive grounding of natural language to manipulation instructions. The International Journal of Robotics Research, 35(1-3):281-300, 2016.
Dipendra K Misra、Jaeyong Sung、Kevin Lee 和 Ashutosh Saxena。《告诉我，Dave：将自然语言在上下文中映射为操控指令》。国际机器人研究杂志，35(1-3):281-300，2016。


Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
Reiichiro Nakano、Jacob Hilton、Suchir Balaji、Jeff Wu、Long Ouyang、Christina Kim、Christopher Hesse、Shantanu Jain、Vineet Kosaraju、William Saunders 等。《WebGPT：带有人类反馈的浏览器辅助问答》。arXiv 预印本 arXiv:2112.09332，2021。


OpenAI. Chatgpt: Optimizing language models for dialogue. 2022.
OpenAI。《ChatGPT：为对话优化语言模型》。2022。


OpenAI. Gpt-4 technical report. arXiv, pp. 2303-08774, 2023.
OpenAI。《GPT-4 技术报告》。arXiv，页 2303-08774，2023。


Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730-27744, 2022.
Long Ouyang、Jeffrey Wu、Xu Jiang、Diogo Almeida、Carroll Wainwright、Pamela Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray 等。《用人类反馈训练语言模型以遵循指令》。神经信息处理系统进展，35: 27730-27744，2022。


Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 8494-8502. IEEE Computer Society, 2018. doi: 10.1109/CVPR. 2018.00886.URLhttp://openaccess.thecvf.com/content_cvpr_2018/html/ Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.html
Xavier Puig、Kevin Ra、Marko Boben、Jiaman Li、Tingwu Wang、Sanja Fidler 和 Antonio Torralba。《VirtualHome：通过程序模拟家庭活动》。载于 2018 年 IEEE 计算机视觉与模式识别会议，CVPR 2018，盐湖城，犹他州，美国，2018 年 6 月 18-22 日，页 8494-8502。IEEE 计算机学会，2018。doi: 10.1109/CVPR.2018.00886.URLhttp://openaccess.thecvf.com/content_cvpr_2018/html/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.html


Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383-2392, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URLhttps://aclanthology.org/D16-1264
Pranav Rajpurkar、Jian Zhang、Konstantin Lopyrev 和 Percy Liang。《SQuAD：面向机器文本理解的 100,000+ 个问题》。载于 2016 年实证方法在自然语言处理会议论文集，页 2383-2392，奥斯汀，德克萨斯，2016。计算语言学协会。doi: 10.18653/v1/D16-1264。URLhttps://aclanthology.org/D16-1264


Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784-789, Melbourne, Australia, 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://aclanthology org/P18-2124
Pranav Rajpurkar、Robin Jia 和 Percy Liang。《知道自己不知道：SQuAD 的不可回答问题》。载于第 56 届计算语言学协会年会（第 2 卷：短文集），页 784-789，墨尔本，澳大利亚，2018。计算语言学协会。doi: 10.18653/v1/P18-2124。URL https://aclanthology org/P18-2124


Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow instructions via graphical user interfaces. arXiv preprint arXiv:2306.00245, 2023.
Peter Shaw、Mandar Joshi、James Cohan、Jonathan Berant、Panupong Pasupat、Hexiang Hu、Urvashi Khandelwal、Kenton Lee 和 Kristina Toutanova。《从像素到界面操作：通过图形用户界面学习遵循指令》。arXiv 预印本 arXiv:2306.00245，2023。


Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 3135- 3144.PMLR, 2017. URLhttp://proceedings.mlr.press/v70/shi17a.html
Tianlin Shi、Andrej Karpathy、Linxi Fan、Jonathan Hernandez 和 Percy Liang。《World of Bits：一个面向网页代理的开放域平台》。载于 Doina Precup 和 Yee Whye Teh（编），第 34 届国际机器学习会议论文集，ICML 2017，悉尼，新南威尔士，澳大利亚，2017 年 8 月 6-11，Proceedings of Machine Learning Research 第 70 卷，页 3135-3144。PMLR，2017。URLhttp://proceedings.mlr.press/v70/shi17a.html


Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. ArXiv preprint, abs/2303.11366, 2023. URL https://arxiv.org/abs/2303.11366
Noah Shinn、Beck Labash 和 Ashwin Gopinath。《Reflexion：具有动态记忆和自我反思的自主代理》。ArXiv 预印本，abs/2303.11366，2023。URL https://arxiv.org/abs/2303.11366


Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: A benchmark for interpreting grounded instructions for everyday tasks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 10737-10746. IEEE, 2020. doi: 10. 1109/CVPR42600.2020.01075. URL https://doi.org/10.1109/CVPR42600.2020.01075
Mohit Shridhar、Jesse Thomason、Daniel Gordon、Yonatan Bisk、Winson Han、Roozbeh Mottaghi、Luke Zettlemoyer 和 Dieter Fox。ALFRED：用于解释日常任务中有根指令的基准。载于 2020 年 IEEE/CVF 计算机视觉与模式识别会议，CVPR 2020，西雅图，华盛顿，美国，2020 年 6 月 13-19 日，页 10737-10746。IEEE，2020。doi: 10.1109/CVPR42600.2020.01075。URL https://doi.org/10.1109/CVPR42600.2020.01075


Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew J. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id= 01OX0YcCdTn
Mohit Shridhar、Xingdi Yuan、Marc-Alexandre Côté、Yonatan Bisk、Adam Trischler 和 Matthew J. Hausknecht。Alfworld：将文本与具身环境对齐以进行交互式学习。载于第 9 届学习表征国际会议，ICLR 2021，线上会议，奥地利，2021 年 5 月 3-7 日。OpenReview.net，2021。URL https://openreview.net/forum?id=01OX0YcCdTn


Abishek Sridhar, Robert Lo, Frank F Xu, Hao Zhu, and Shuyan Zhou. Hierarchical prompting assists large language model on web navigation. arXiv preprint arXiv:2305.14257, 2023.
Abishek Sridhar、Robert Lo、Frank F Xu、Hao Zhu 和 Shuyan Zhou。分层提示帮助大型语言模型进行网页导航。arXiv 预印本，arXiv:2305.14257，2023。


Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: A reinforcement learning platform for android. ArXiv preprint, abs/2105.13231, 2021. URL https://arxiv.org/abs/2105.13231
Daniel Toyama、Philippe Hamel、Anita Gergely、Gheorghe Comanici、Amelia Glaese、Zafarali Ahmed、Tyler Jackson、Shibl Mourad 和 Doina Precup。Androidenv：面向 Android 的强化学习平台。ArXiv 预印本，abs/2105.13231，2021。URL https://arxiv.org/abs/2105.13231


Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. ArXiv preprint, abs/2305.16291, 2023. URL https://arxiv.org/abs/2305.16291
Guanzhi Wang、Yuqi Xie、Yunfan Jiang、Ajay Mandlekar、Chaowei Xiao、Yuke Zhu、Linxi Fan 和 Anima Anandkumar。Voyager：基于大型语言模型的开放式具身代理。ArXiv 预印本，abs/2305.16291，2023。URL https://arxiv.org/abs/2305.16291


Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. Execution-based evaluation for open-domain code generation. ArXiv preprint, abs/2212.10481, 2022. URL https://arxiv.org/abs/2212.10481
Zhiruo Wang、Shuyan Zhou、Daniel Fried 和 Graham Neubig。面向开放域代码生成的基于执行的评估。ArXiv 预印本，abs/2212.10481，2022。URL https://arxiv.org/abs/2212.10481


Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.
Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Fei Xia、Ed Chi、Quoc V Le、Denny Zhou 等。连锁思维提示在大型语言模型中引出推理。Advances in Neural Information Processing Systems，35:24824-24837，2022。


Nancy Xu, Sam Masling, Michael Du, Giovanni Campagna, Larry Heck, James Landay, and Monica Lam. Grounding open-domain instructions to automate web support tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1022-1032, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.80. URL https://aclanthology.org/ 2021.naacl-main.80
Nancy Xu、Sam Masling、Michael Du、Giovanni Campagna、Larry Heck、James Landay 和 Monica Lam。将开放域指令落地以自动化网页支持任务。载于 2021 年北美计算语言学会年会（NAACL）论文集：人类语言技术，页 1022-1032，线上，2021。计算语言学协会。doi: 10.18653/v1/2021.naacl-main.80。URL https://aclanthology.org/2021.naacl-main.80


Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369-2380, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.
Zhilin Yang、Peng Qi、Saizheng Zhang、Yoshua Bengio、William Cohen、Ruslan Salakhutdinov 和 Christopher D. Manning。HotpotQA：用于多样且可解释的多跳问答的数据集。载于 2018 年经验方法自然语言处理会议论文集，页 2369-2380，比利时布鲁塞尔，2018。计算语言学协会。doi: 10.18653/v1/D18-1259。URL https://aclanthology.org/D18-1259


Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. volume abs/2207.01206, 2022a. URL https://arxiv.org/abs/2207.01206
Shunyu Yao、Howard Chen、John Yang 和 Karthik Narasimhan。Webshop：朝着具根语言代理可扩展的真实世界网页交互迈进。卷 abs/2207.01206，2022a。URL https://arxiv.org/abs/2207.01206


Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv preprint, abs/2210.03629, 2022b.URLhttps://arxiv.org/abs/2210.03629
Shunyu Yao、Jeffrey Zhao、Dian Yu、Nan Du、Izhak Shafran、Karthik Narasimhan 和 Yuan Cao。React：在语言模型中协同推理与行动。ArXiv 预印本，abs/2210.03629，2022b。URL https://arxiv.org/abs/2210.03629


Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv preprint, abs/2305.10601, 2023. URL https://arxiv.org/abs/2305.10601
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv preprint, abs/2305.10601, 2023. URL https://arxiv.org/abs/2305.10601


Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. arxiv 2017. ArXiv preprint, abs/1709.00103, 2017. URLhttps://arxiv.org/abs/1709.00103
Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. arxiv 2017. ArXiv preprint, abs/1709.00103, 2017. URLhttps://arxiv.org/abs/1709.00103


Shuyan Zhou, Pengcheng Yin, and Graham Neubig. Hierarchical control of situated agents through natural language. In Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI), pp. 67-84, Seattle, USA, 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.suki-1.8. URL https://aclanthology.org/2022.suki-1.8
Shuyan Zhou, Pengcheng Yin, and Graham Neubig. Hierarchical control of situated agents through natural language. In Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI), pp. 67-84, Seattle, USA, 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.suki-1.8. URL https://aclanthology.org/2022.suki-1.8


Shuyan Zhou, Li Zhang, Yue Yang, Qing Lyu, Pengcheng Yin, Chris Callison-Burch, and Graham Neubig. Show me more details: Discovering hierarchies of procedures from semi-structured web data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2998-3012, Dublin, Ireland, 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.214. URL https://aclanthology.org/ 2022.acl-long.214
Shuyan Zhou, Li Zhang, Yue Yang, Qing Lyu, Pengcheng Yin, Chris Callison-Burch, and Graham Neubig. Show me more details: Discovering hierarchies of procedures from semi-structured web data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2998-3012, Dublin, Ireland, 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.214. URL https://aclanthology.org/ 2022.acl-long.214


## A APPENDIX
## A 附录


### A.1 WEBSITE IMPLEMENTATION
### A.1 网站实现


Given the selected websites described in §2.2, we make the best attempt to reproduce the functionality of commonly used sites in a reproducible way. To achieve this, we utilized open-source frameworks for the development of the websites across various categories and imported data from their real-world counterparts. For the E-commerce category, we constructed a shopping website with approximately ${90k}$ products,including the prices,options,detailed product descriptions,images,and reviews, spanning over 300 product categories. This website is developed using Adobe Magento, an open-source e-commerce platform ${}^{4}$ ] Data resources were obtained from data from actual online sites,such as that included in the Webshop data dump Yao et al. (2022a). As for the social forum platform, we deployed an open-source software Postmill ${}^{5}$ the open-sourced counterpart of Reddit ${}^{6}$ We sampled from the top 50 subreddits ${}^{7}$ We then manually selected many subreddit for northeast US cities as well as subreddit for machine learning and deep learning-related topics. This manual selection encourages cross-website tasks such as seeking information related to the northeast US on both Reddit and the map. In total, we have 95 subreddits, 127390 posts, and 661781 users. For the collaborative software development platform, we choose GitLab ${}^{8}$ We heuristically simulate the code repository characteristics by sampling at least ten repositories for every programming language: ${80}\%$ of them are sampled from the set of top 90 percentile wrt stars repos using a discrete probability distribution weighted proportional to their number of stars; the remaining are sampled from the bottom ten percentile set using similar weighted distribution. This is done to ensure fair representation of repos of all kinds, from popular projects with many issues and pull requests to small personal projects. In total, we have 300 repositories and more than 1000 accounts with at least one commit to a repository. For the content management system, we adapted Adobe Magento's admin portal, deploying the sample data provided in the official guide. We employ OpenStreetMap ${}^{9}$ for map service implementation, confining our focus to the northeast US region due to data storage constraints. We implement a calculator and a scratchpad ourselves.
鉴于 §2.2 中描述的所选网站，我们尽力以可复现的方式重现常用站点的功能。为此，我们使用开源框架开发各类别网站，并从其真实对应物导入数据。对电子商务类别，我们构建了一个约有 ${90k}$ 款产品的购物网站，包含价格、选项、详细产品描述、图片和评论，覆盖超过 300 个产品类别。该网站使用开源电商平台 Adobe Magento 开发 ${}^{4}$ ] 数据资源来自实际在线站点的数据，如 Webshop 数据转储 Yao et al. (2022a) 中所包含的。至于社交论坛平台，我们部署了开源软件 Postmill ${}^{5}$，这是 Reddit 的开源对应项目 ${}^{6}$。我们从前 50 个 subreddit 中抽样 ${}^{7}$，并手工选择了许多东北美国产城的 subreddit 以及与机器学习和深度学习相关的话题 subreddit。此手工选择促成了跨站点任务，例如在 Reddit 和地图上同时查找与东北美国产城相关的信息。总体上，我们有 95 个 subreddit、127390 篇帖子和 661781 名用户。对于协作软件开发平台，我们选择了 GitLab ${}^{8}$。我们通过启发式方法模拟代码仓库特征，对每种编程语言至少抽样十个仓库：其中 ${80}\%$ 个来自按 star 数位于前 90 百分位的仓库集合，使用与 star 数成比例的离散概率分布抽样；其余来自位于后 10 百分位的集合，采用类似的加权分布抽样。这样做以确保各类仓库公平代表，从拥有大量 issue 和 pull request 的热门项目到小型个人项目。总体上，我们有 300 个仓库和超过 1000 个至少对某个仓库有一次提交的账户。对于内容管理系统，我们改编了 Adobe Magento 的管理员门户，部署了官方指南提供的示例数据。地图服务实现使用 OpenStreetMap ${}^{9}$，由于数据存储限制，我们将关注范围限定在美国东北部地区。计算器和草稿板由我们自行实现。


Lastly, we configure the knowledge resources as individual websites, complemented with search functionality for efficient information retrieval. Specifically, we utilize Kiwix ${}^{10}$ to host an offline version of English Wikipedia with a knowledge cutoff of May 2023. The user manuals for GitLab and Adobe Commerce Merchant documentation are scraped from the official websites.
最后，我们将知识资源配置为独立网站，并配备搜索功能以提高信息检索效率。具体地，我们使用 Kiwix ${}^{10}$ 托管截止 2023 年 5 月的离线英语维基百科。GitLab 和 Adobe Commerce 商家文档的用户手册则从官方网站抓取。


### A.2 ENVIRONMENT DELIVERY AND RESET
### A.2 环境交付与重置


One goal for our evaluation environment is ease of use and reproducibility. As a result, we deploy our websites in separate Docker images ${}^{11}$ , one per website. The Docker images are fully self-contained with all the code of the website, database, as well as any other software dependencies. They also do not rely on external volume mounts to function, as the data of the websites are also part of the docker image. This way, the image is easy to distribution containing all the pre-populated websites for reproducible evaluation. End users can download our packaged Docker images and run them on their systems and re-deploy the exact websites together with the data used in our benchmarks for their local benchmarking.
我们的评估环境目标之一是易用且可复现。因此，我们将每个网站部署在单独的 Docker 镜像中 ${}^{11}$ 。这些 Docker 镜像完全自包含网站的所有代码、数据库以及其他软件依赖。它们也不依赖外部挂载卷来运行，因为网站的数据也是镜像的一部分。这样，镜像便于分发，包含所有预先填充的网站以实现可复现的评估。终端用户可以下载我们打包的 Docker 镜像，在其系统上运行，并重新部署与基准测试中使用的数据完全相同的网站以进行本地基准测试。


Since some evaluation cases may require the agent to modify the data contained in the website, e.g., creating a new user, deleting a post, etc., it is crucial to be able to easily reset the website environment to its initial state. With Docker images, the users could stop and delete the currently running containers for that website and start the container from our original image again to fully reset the environment to the initial state. Depending on the website, this process may take from a few seconds to one minute. However, not all evaluation cases would require an environment reset, as many of the intents are information gathering and are read-only for the website data. Also, combined with the inference time cost for the agent LLMs, we argue that this environment reset method, through restarting Docker containers from the original images, will have a non-negligible but small impact on evaluation time.
由于某些评估场景可能需要代理修改网站中的数据，例如创建新用户、删除帖子等，因此能够轻松将网站环境重置为初始状态至关重要。使用 Docker 镜像，用户可以停止并删除当前运行的该网站容器，然后从我们的原始镜像重新启动容器，以将环境完全重置为初始状态。根据网站不同，此过程可能需要数秒到一分钟。然而，并非所有评估场景都需要重置环境，因为许多意图是信息收集性质，对网站数据为只读。此外，结合代理大模型的推理时间成本，我们认为通过从原始镜像重启 Docker 容器的这种环境重置方法会对评估时间产生不可忽略但较小的影响。


---



https://github.com/magento/magento2



https://postmill.xyz/



https://www.reddit.com/



https://redditlist.com/sfw.html



https://gitlab.com/gitlab-org/gitlab



https://www.openstreetmap.org/



https://www.kiwix.org/en/



https://www.docker.com/



---



<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_43_ea5a7b.jpg"/>



Figure 6: The intent distribution across different websites. Cross-site intents necessitate interacting with multiple websites. Notably, regardless of the website, all user intents require interactions with multiple web pages.
图 6：不同网站间的意图分布。跨站意图需要与多个网站交互。值得注意的是，无论网站如何，所有用户意图都需要与多个网页交互。


### A.3 USER ROLES SIMULATION
### A.3 用户角色模拟


Users of the same website often have disparate experiences due to their distinct roles, permissions, and interaction histories. For instance, within an E-commerce CMS, a shop owner might possess full read and write permissions across all content, whereas an employee might only be granted write permissions for products but not for customer data. We aim to emulate this scenario by generating unique user profiles on each platform.
相同网站的用户由于角色、权限和交互历史不同，常有截然不同的体验。例如，在电商内容管理系统中，店主可能对所有内容拥有完全的读写权限，而员工可能仅对商品具有写权限但不能访问客户数据。我们旨在通过在每个平台上生成独特的用户档案来模拟这种情形。


On the shopping site, we created a customer profile that has over 35 orders within a span of two years. On GitLab, we selected a user who maintains several popular open-source projects with numerous merge requests and issues. This user also manages a handful of personal projects privately. On Reddit, our chosen profile was a user who actively participates in discussions, with many posts and comments. Lastly, on our E-commerce CMS, we set up a user profile for a shop owner who has full read-and-write access to all system contents.
在购物网站上，我们创建了一个在两年内有超过 35 笔订单的顾客档案。在 GitLab 上，我们选择了一位维护多个流行开源项目并有大量合并请求和 issue 的用户，该用户也管理着少量私人项目。在 Reddit 上，我们选择的档案是一个积极参与讨论、拥有大量帖子和评论的用户。最后，在我们的电商 CMS 中，我们为一位对系统所有内容拥有完全读写权限的店主设置了用户档案。


All users are automatically logged into their accounts using a pre-cached cookie. To our best knowledge, this is the first publicly available agent evaluation environment to implement such a characteristic. Existing literature typically operates under the assumption of universally identical user roles Shi et al. (2017); Liu et al. (2018); Deng et al. (2023).
所有用户都使用预缓存的 cookie 自动登录他们的帐户。据我们所知，这是首个公开可用的代理评估环境实现此特性的工作。现有文献通常假定用户角色在各处均相同 Shi et al. (2017); Liu et al. (2018); Deng et al. (2023)。


### A.4 INTENT DISTRIBUTION
### A.4 意图分布


The distribution of intents across the websites are shown in Figure 6
各网站间的意图分布如图 6 所示


### A.5 HUMAN PERFORMANCE
### A.5 人类表现


We acknowledge that there may be a difference in human performance when annotators with different demographics are involved. In fact, many tasks in our dataset require domain-specific knowledge. For instance, an average user may not know what a git merge request is; or how to create a product in a complex content management system. We aim to design tasks that have easy-to-imagine outcomes (e.g., a new product page is created) rather than those that are easily performed by an average user without significant domain knowledge.
我们承认当注释者人口统计不同会导致人类表现差异。事实上，本数据集中许多任务需要领域特定知识。例如，普通用户可能不知道 git 合并请求是什么；或如何在复杂的内容管理系统中创建产品。我们旨在设计那些结果易于想象（例如创建了一个新产品页面）的任务，而非那些普通用户在没有显著领域知识下就能轻松完成的任务。


<table><tr><td>CoT UA Hint Model SR</td></tr><tr><td>✓ ✘ GPT-3.5 6.28</td></tr></table>
<table><tbody><tr><td>CoT UA 提示模型 SR</td></tr><tr><td>✓ ✘ GPT-3.5 6.28</td></tr></tbody></table>


Table 5: The task success rate (SR %) of GPT-3.5-TURBO-16K-0613 with temperature 0.0.
表5：温度为0.0时 GPT-3.5-TURBO-16K-0613 的任务成功率（SR %）。


<table><tr><td>Dataset</td><td>gpt-4-0613</td><td>gpt-4-1106-preview</td></tr><tr><td>Date (900 examples)</td><td>100</td><td>100</td></tr><tr><td>Time duration (900 examples)</td><td>100</td><td>100</td></tr></table>
<table><tbody><tr><td>数据集</td><td>gpt-4-0613</td><td>gpt-4-1106-preview</td></tr><tr><td>日期（900 个示例）</td><td>100</td><td>100</td></tr><tr><td>时间时长（900 个示例）</td><td>100</td><td>100</td></tr></tbody></table>


Table 6: The accuracy (%) of two versions of GPT-4 on judging if dates and time duration of different formats are equivalent.
表 6：两种版本 GPT-4 在判断不同格式的日期和时间时长是否等价时的准确率（％）。


### A.6 EXPERIMENT CONFIGURATIONS
### A.6 实验配置


We experiment with GPT-3.5-TURBO-16K-0613, GPT-4-0613, and TEXT-BISON-001 with a temperature of 1.0 and a top- $p$ parameter of 0.9 . The maximum number of state transitions is set to 30. We halt execution if the same action is repeated more than three times on the same observation or if the agent generates three consecutive invalid actions. These situations typically indicate a high likelihood of execution failure and hence warrant early termination. For TEXT-BISON-001, we additionally allow ten retries until it generates a valid action.
我们使用 GPT-3.5-TURBO-16K-0613、GPT-4-0613 和 TEXT-BISON-001，温度设为 1.0，top- $p$ 参数为 0.9。最大状态转移次数设为 30。如果在相同观测上同一动作重复超过三次，或代理连续生成三次无效动作，我们将停止执行。这些情况通常表明执行失败的概率很高，因而需要提前终止。对于 TEXT-BISON-001，我们额外允许十次重试，直到其生成有效动作。


Primarily, we use a high temperature of 1.0 to encourage the exploration. To aid replicating the results, we provide the results of GPT-3.5-TURBO-16K-0613 with temperature 0.0 in Table 5 and the execution trajectories in our code repository.
主要我们使用较高的温度 1.0 来鼓励探索。为便于复现结果，我们在表 5 中提供了温度为 0.0 的 GPT-3.5-TURBO-16K-0613 的结果，并在代码仓库中提供执行轨迹。


---



### A.7 PROMPT FOR FUZZY_MATCH
### A.7 FUZZY_MATCH 的提示词


---



Help a teacher to grade the answer of a student given a question. Keep in mind that the student may use different phrasing or wording to answer the question. The goal is to evaluate whether the answer is semantically equivalent to the reference answer.
帮助教师根据题目批改学生的答案。请记住学生可能使用不同措辞或表述来回答问题。目标是评估答案在语义上是否等价于参考答案。


question: \{\{intent\}\}
question: {{intent}}


reference answer: \{\{reference answer\}\}
reference answer: {{reference answer}}


all the string 'N/A' that you see is a special sequence that means 'not achievable' student answer: \{\{prediction\}\}
所有你看到的字符串 'N/A' 是表示“不可达”（not achievable）的特殊序列 student answer: {{prediction}}


Conclude the judgement by correct/incorrect/partially correct.
用 correct/incorrect/partially correct 给出判定。


Predictions that are judged as "correct" will receive a score of one, while all other predictions will receive a score of zero.
被判定为“correct”的预测得分为一，其它所有预测得分为零。


### A.8 THE ACCURACY OF FUZZY MATCH FUNCTION
### A.8 FUZZY MATCH 函数的准确率


To evaluate this, we manually checked 40 examples and found that 39 of them are identical to our human judgment. In addition, among the 82 examples that require using GPT-4 for evaluation, the answer of 49 (60%) examples is a date (e.g., 10/23/2022) or time duration (e.g., 15 minutes). In these cases, GPT-4 is only used to judge the different format of the answers. We quantitatively evaluate the correctness of GPT-4 in this case by generating different formats of a date and time duration programmatically. We randomly sample negative examples. For instance, Nov 3, 2022, November 3, 2022, 3rd November 2022, 3 Nov 2022, 2022-11-03, and 3rd of November, 2022 are all correct variances of 2022/11/03. The accuracy of GPT-4 is shown in Table 6 . We can see that two versions of GPT-4 are extremely accurate, both achieving 100% accuracy.
为评估此项，我们人工检查了 40 个示例，发现其中 39 个与我们的人工判断一致。此外，在需要使用 GPT-4 评估的 82 个示例中，有 49 个（60%）的答案是日期（例如 10/23/2022）或时间时长（例如 15 minutes）。在这些情况下，GPT-4 仅用于判断答案的不同格式。我们通过以程序方式生成日期和时间时长的不同格式来定量评估 GPT-4 在此情形下的正确性。我们随机采样负例。例如，Nov 3, 2022、November 3, 2022、3rd November 2022、3 Nov 2022、2022-11-03 和 3rd of November, 2022 都是 2022/11/03 的正确变体。GPT-4 的准确率如表 6 所示。可以看到，两种版本的 GPT-4 极为准确，均达到 100% 的准确率。


### A.9 THE PROMPTS OF THE BASELINE WEB AGENTS
### A.9 基线网页代理的提示词


The system message of the reasoning agent for both GPT-3.5 and GPT-4 is in Figure 7, and two examples are in Figure 8 The system message of the direct agent for GPT-3.5 is in Figure 9 and the two examples are in Figure 10. UA hint refers to the instruction of "If you believe the task is
用于 GPT-3.5 和 GPT-4 的推理代理的 system message 在图 7，两则示例在图 8。用于 GPT-3.5 的直接代理的 system message 在图 9，两则示例在图 10。UA 提示指的是“如果你认为该任务是”这条指令


---



								You are an autonomous intelligent agent tasked with navigating a web browser. You will be given
								你是一个自治智能代理，负责在网页浏览器上导航。你将收到


									web-based tasks. These tasks will be accomplished through the use of specific actions you can issue.
									基于网页的任务。这些任务将通过你可以发出的特定操作来完成。


											Here's the information you'll have:
											以下是你将拥有的信息：


											The user's objective: This is the task you're trying to complete.
											用户的目标：这是你要完成的任务。


											The current web page's accessibility tree: This is a simplified representation of the webpage, providing
											当前网页的可访问性树：这是网页的简化表示，提供


												key information.
												关键信息。


										The current web page's URL: This is the page you're currently navigating.
										当前网页的 URL：这是你当前访问的页面。


												The open tabs: These are the tabs you have open.
												已打开的标签页：这些是你已打开的标签页。


											The previous action: This is the action you just performed. It may be helpful to track your progress.
											上一个操作：这是你刚执行的操作。它可能有助于跟踪你的进展。


											The actions you can perform fall into several categories:
											你可以执行的操作分为几类：


												Page Operation Actions
												页面操作


														`click [id]`: This action clicks on an element with a specific id on the webpage.
														`click [id]`：该操作单击网页上具有特定 id 的元素。


														`type [id] [content] [press_enter_after=0|1]`: Use this to type the content into the field with id. By
														`type [id] [content] [press_enter_after=0|1]`：用于在具有该 id 的字段中输入内容。默认情况下，输入后会按“回车”，除非将 press_enter_after 设置为 0。


												default, the "Enter" key is pressed after typing unless press_enter_after is set to 0.
												默认情况下，输入后会按“回车”，除非将 press_enter_after 设置为 0。


														`hover [id]`: Hover over an element with id.
														`hover [id]`：将鼠标悬停在具有 id 的元素上。


															`press [key_comb]`: Simulates the pressing of a key combination on the keyboard (e.g., Ctrl+v).
															`press [key_comb]`：模拟按下键盘组合键（例如 Ctrl+v）。


																scroll [direction=downlup]`: Scroll the page up or down.
																`scroll [direction=downlup]`：向上或向下滚动页面。


												Tab Management Actions:
												选项卡管理操作：


														`new_tab`: Open a new, empty browser tab.
														`new_tab`: 打开一个新的空浏览器选项卡。


														`tab_focus [tab_index]`: Switch the browser's focus to a specific tab using its index.
														`tab_focus [tab_index]`: 使用索引切换浏览器焦点到指定选项卡。


																`close_tab`: Close the currently active tab.
																`close_tab`: 关闭当前活动的选项卡。


													URL Navigation Actions:
													URL 导航操作：


																`goto [url]`: Navigate to a specific URL.
																`goto [url]`: 导航到指定的 URL。


														` go_back`: Navigate to the previously viewed page.
														` go_back`: 返回到之前查看的页面。


																		` go_forward`: Navigate to the next page (if a previous
																		` go_forward`: 前进到下一页（如果之前执行过


															`go_back` action was performed).
															`go_back` 操作）。


												Completion Action:
												完成操作：


														`stop [answer]`: Issue this action when you believe the task is complete. If the objective is to find
														`stop [answer]`: 当你认为任务完成时执行此操作。如果目标是找到


											a text-based answer, provide the answer in the bracket. If you believe the task is impossible to
											基于文本的答案，请在括号中提供答案。如果你认为任务不可能


											complete, provide the answer as "N/A" in the bracket.
											完成，请在括号中提供 "N/A"。


												Homepage:
												主页：


											If you want to visit other websites, check out the homepage at http://homepage.com.It has a list of
											如果你想访问其他网站，请查看主页 http://homepage.com。它列出了一些


											websites you can visit.
											你可以访问的网站。


											http://homepage.com/password.html lists all the account names and passwords for the websites. You
											http://homepage.com/password.html 列出了所有网站的账户名和密码。你


										can use them to log in to the websites.
										可以用它们登录这些网站。


											To be successful, it is very important to follow the following rules:
											要成功，遵守以下规则非常重要：


											1. You should only issue an action that is valid given the current observation
											1. 你只应在当前观察下发出有效的操作


										2. You should only issue one action at a time.
										2. 你每次只应发出一个操作。


									3. You should follow the examples to reason step by step and then issue the next action.
									3. 你应按示例逐步推理，然后发出下一个操作。


										4. Generate the action in the correct format. Start with a "In summary, the next action I will perform
										4. 以正确格式生成操作。以“In summary, the next action I will perform


										is" phrase, followed by action inside ___ . For example, "In summary, the next action I will
										is”短语开头，后接 ___ 内的操作。例如，“In summary, the next action I will


											perform is ```click [1234]```”.
											perform is ```click [1234]```”。


										5. Issue stop action when you think you have achieved the objective. Don't generate anything after
										5. 当你认为已达成目标时发出 stop 操作。停后不要生成任何内容


										stop."""
										stop。"""


Figure 7: The system message of the reasoning agent. This message has a general explanation of
图 7：推理代理的系统消息。该消息对任务、可用操作及避免常见错误的注意事项作了总体说明。


the task, the available actions, and some notes on avoiding common failures.
该消息包含任务、可用操作以及关于避免常见失败的一些说明。


---



impossible to complete, provide the answer as "N/A" in the bracket.". We remove this sentence in our ablation studies.
若不可能完成，则在括号中提供答案为 "N/A"。我们在消融研究中移除了这句话。


example_user
example_user


OBSERVATION:
OBSERVATION:


[1744] link 'HP CB782A#ABA 640 Inkjet Fax Machine (Renewed)'
[1744] 链接 'HP CB782A#ABA 640 喷墨传真机（翻新）'


[1749] StaticText '\$279.49'
[1749] 静态文本 '\$279.49'


[1757] button 'Add to Cart'
[1757] 按钮 '加入购物车'


[1760] button 'Add to Wish List'
[1760] 按钮 '加入愿望清单'


[1761] button 'Add to Compare'
[1761] 按钮 '加入比较'


URL: http://onestopmarket.com/office-products/office-electronics.html
URL: http://onestopmarket.com/office-products/office-electronics.html


OBJECTIVE: What is the price of HP Inkjet Fax Machine
目标：HP 喷墨传真机的价格是多少


PREVIOUS ACTION: None
先前操作：无


## example_assistant
## example_assistant


Let's think step-by-step. This page lists the information of HP Inkjet Fax Machine, which is the product identified in the objective. Its price is \$279.49. I think I have achieved the objective. I will issue the stop action with the answer. In summary, the next action I will perform is ```stop [\$279.49]```
让我们逐步思考。该页面列出了 HP 喷墨传真机的信息，正是目标中识别的产品。其价格为 \$279.49。我认为我已完成目标。我将以答案发出停止操作。总之，我接下来要执行的操作是 ```stop [\$279.49]```


example_user
example_user


OBSERVATION:
观察：


[164] textbox 'Search' focused: True required: False
[164] 文本框 '搜索' 获得焦点：是 必需：否


[171] button 'Go'
[171] 按钮 '前往'


[174] link 'Find directions between two points'
[174] 链接 '在两点之间查找路线'


[212] heading 'Search Results'
[212] 标题 '搜索结果'


[216] button 'Close'
[216] 按钮 'Close'


URL: http://openstreetmap.org
URL: http://openstreetmap.org


OBJECTIVE: Show me the restaurants near ABC
目标：显示我在 ABC 附近的餐馆


PREVIOUS ACTION: None
先前操作：无


## example_assistant
## example_assistant


Let's think step-by-step. This page has a search box whose ID is [164]. According to the nominatim rule of openstreetmap, I can search for the restaurants near a location by restaurants near. I can submit my typing by pressing the Enter afterwards. In summary, the next action I will perform is ```type [164] [restaurants near ABC] [1]```
让我们一步步思考。此页面有一个 ID 为 [164] 的搜索框。根据 openstreetmap 的 nominatim 规则，我可以通过 "restaurants near" 来搜索某地附近的餐馆。之后我可以按回车提交输入。总之，我接下来要执行的操作是 ```type [164] [restaurants near ABC] [1]```


Figure 8: The two examples provided as example_user and example_assistant for the reasoning agent. Before issuing the action, the agent first perform reasoning.
图 8：为推理代理提供的两个示例，分别作为 example_user 和 example_assistant。在发出操作之前，代理先进行推理。


---



									You are an autonomous intelligent agent tasked with navigating a web browser. You will be given
									你是一个负责操控网页浏览器的自治智能代理。你将被赋予


										web-based tasks. These tasks will be accomplished through the use of specific actions you can issue.
										基于网页的任务。通过你可以发出的特定操作来完成这些任务。


											Here's the information you'll have:
											你将获得以下信息：


											The user's objective: This is the task you're trying to complete.
											用户的目标：这是你要完成的任务。


										The current web page's accessibility tree: This is a simplified representation of the webpage, providing
										当前网页的可访问性树：这是网页的简化表示，提供


											key information.
											关键信息。


											The current web page's URL: This is the page you're currently navigating.
											当前网页的 URL：这是你当前正在浏览的页面。


											The open tabs: These are the tabs you have open.
											打开的标签页：这是你打开的标签页。


											The previous action: This is the action you just performed. It may be helpful to track your progress.
											先前的操作：这是你刚执行的操作。它可能有助于跟踪你的进度。


												The actions you can perform fall into several categories:
												你可以执行的操作分为几类：


												Page Operation Actions
												页面操作：


														`click [id]`: This action clicks on an element with a specific id on the webpage.
														`click [id]`：点击网页上具有特定 id 的元素。


														`type [id] [content] [press_enter_after=0|1]`: Use this to type the content into the field with id. By
														`type [id] [content] [press_enter_after=0|1]`：在具有该 id 的字段中输入内容。


												default, the "Enter" key is pressed after typing unless press_enter_after is set to 0.
												默认情况下，输入后会按下“Enter”键，除非将 press_enter_after 设为 0。


														`hover [id]`: Hover over an element with id.
														`hover [id]`：将鼠标悬停在具有该 id 的元素上。


															`press [key_comb]`: Simulates the pressing of a key combination on the keyboard (e.g., Ctrl+v).
															`press [key_comb]`：模拟按下键盘组合键（例如 Ctrl+v）。


															`scroll [direction=downlup]`: Scroll the page up or down.
															`scroll [direction=downlup]`：向上或向下滚动页面。


												Tab Management Actions:
												标签页管理：


														`new_tab`: Open a new, empty browser tab.
														`new_tab`：打开一个新的空白浏览器标签页。


												`tab_focus [tab_index]`: Switch the browser's focus to a specific tab using its index.
												`tab_focus [tab_index]`：使用索引将浏览器焦点切换到特定标签页。


														`close_tab`: Close the currently active tab.
														`close_tab`：关闭当前活动的标签页。


													URL Navigation Actions:
													URL 导航：


															`goto [url]`: Navigate to a specific URL.
															`goto [url]`：导航到指定的 URL。


																` go_back`: Navigate to the previously viewed page.
																` go_back`：返回到之前查看的页面。


																			` go_forward`: Navigate to the next page (if a previous
																			` go_forward`：前进到下一页（如果有之前的前进记录）。


															`go_back` action was performed).
															`go_back` 动作已执行)。


												Completion Action:
												完成动作：


													`stop [answer]`: Issue this action when you believe the task is complete. If the objective is to find
													`stop [answer]`：当你认为任务完成时发出此动作。如果目标是找到


												a text-based answer, provide the answer in the bracket. If you believe the task is impossible to
												基于文本的答案，请在括号内提供答案。如果你认为任务不可能完成，


											complete, provide the answer as "N/A" in the bracket.
											请在括号内提供 "N/A" 作为答案。


												Homepage:
												主页：


										If you want to visit other websites, check out the homepage at http://homepage.com.It has a list of
										如需访问其他网站，请查看主页 http://homepage.com。上面有可访问


											websites you can visit.
											的网站列表。


											http://homepage.com/password.html lists all the account name and password for the websites. You
											http://homepage.com/password.html 列出了所有网站的账号和密码，你


											can use them to log in to the websites.
											可以使用它们登录这些网站。


										To be successful, it is very important to follow the following rules:
										要取得成功，遵守以下规则非常重要：


										To be successful, it is very important to follow the following rules:
										要取得成功，遵守以下规则非常重要：


										1. You should only issue an action that is valid given the current observation
										1. 你应仅在当前观察下发出有效的动作


										2. You should only issue one action at a time.
										2. 你一次只应发出一个动作。


											3. Generate the action in the correct format. Always put the action inside a pair of ```. For example,
											3. 以正确格式生成动作。始终将动作置于一对 ``` 之间。例如，


										```click [1234]```
										```click [1234]```


									4. Issue stop action when you think you have achieved the objective. Don't generate anything after
									4. 当你认为已达目标时，发出 stop 操作。之后不要生成任何内容


										stop."""
										stop.\"\"\"


Figure 9: The system message of the direct agent. This message has the general explanation of the
图 9：直接代理的系统消息。该消息包含任务的一般说明、可用操作以及避免常见失败的注意事项。


task, the available actions and some notes on avoiding common failures.
任务说明、可用动作及一些关于避免常见错误的说明。


---



---



example_user
example_user


	OBSERVATION:
	观测：


	[1744] link 'HP CB782A#ABA 640 Inkjet Fax Machine (Renewed)'
	[1744] 链接 'HP CB782A#ABA 640 喷墨传真机（翻新）'


															[1749] StaticText '\$279.49'
															[1749] 静态文本 '\$279.49'


														[1757] button 'Add to Cart'
														[1757] 按钮 '加入购物车'


														[1760] button 'Add to Wish List'
														[1760] 按钮 '加入愿望清单'


																[1761] button 'Add to Compare'
																[1761] 按钮 '加入对比'


URL: http://onestopmarket.com/office-products/office-electronics.html
网址: http://onestopmarket.com/office-products/office-electronics.html


OBJECTIVE: What is the price of HP Inkjet Fax Machine
目标：HP 喷墨传真机的价格是多少


PREVIOUS ACTION: None
之前的操作：无


example_assistant
example_assistant


			```stop [\$279.49]```
			```stop [\$279.49]```


example_user
example_user


OBSERVATION:
观察：


	[164] textbox 'Search' focused: True required: False
	[164] 文本框 'Search' 聚焦: True 必需: False


[171] button 'Go'
[171] 按钮 'Go'


	[174] link 'Find directions between two points'
	[174] 链接 'Find directions between two points'


	[212] heading 'Search Results'
	[212] 标题 'Search Results'


	[216] button 'Close'
	[216] 按钮 'Close'


URL: http://openstreetmap.org
URL: http://openstreetmap.org


OBJECTIVE: Show me the restaurants near ABC
目标: 显示 ABC 附近的餐馆


PREVIOUS ACTION: None
先前操作: 无


example_assistant
example_assistant


```type [164] [restaurants near ABC] [1]```
```type [164] [restaurants near ABC] [1]```


---



Figure 10: The two examples provided as example_user and example_assistant for the direct agent. The agent directly emits the next action given the observation.
图10：为直接代理提供的两个示例，分别由 example_user 和 example_assistant 给出。该代理在观察到信息后直接发出下一步动作。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_43_667e80.jpg"/>



Figure 11: Two examples where the GPT-4 agent failed, along with their screenshot and the accessibility tree of the relevant sections (grey). On the left, the agent fails to proceed to the "Users" section to accomplish the task of "Fork all Facebook repos"; on the right, the agent repeats entering the same search query even though the observation indicates the input box is filled.
图11：两个 GPT-4 代理失败的示例，附有它们的截图及相关区域的无障碍树（灰色）。左侧，代理未能进入“Users”部分以完成“Fork all Facebook repos”任务；右侧，尽管观察显示输入框已填充，代理仍重复输入相同的搜索查询。


### A.10 ADDITIONAL ERROR ANALYSIS
### A.10 附加错误分析


Observation Bias Realistic websites frequently present information on similar topics across various sections to ensure optimal user accessibility. However, a GPT-4 agent often demonstrates a tendency to latch onto the first related piece of information it encounters without sufficiently verifying its relevance or accuracy. For instance, the homepage of the E-Commerce CMS displays the best-selling items based on recent purchases, while historical best-seller data is typically accessed via a separate report. Presented with the task of "What is the top-1 best-selling product in 2022", the GPT-4 agent defaults to leveraging the readily available information on the homepage, bypassing the necessary step of generating the report to obtain the accurate data.
观察偏差 现实网站常在不同版块呈现相似主题的信息以优化用户可达性。然而，GPT-4 代理常倾向于抓住它遇到的第一条相关信息，而未充分验证其相关性或准确性。例如，某电商 CMS 的主页展示基于近期购买的畅销商品，而历史畅销数据通常通过单独报告获得。面对“2022 年排名第一的畅销产品是什么”这样的任务时，GPT-4 代理常直接使用主页上现成的信息，跳过生成报告以获取准确数据的必要步骤。


Failures in Observation Interpretation Interestingly, while GPT-4 is capable of summarizing the observations, it occasionally overlooks more granular information, such as the previously entered input. As in the right-hand example of Figure 11, [5172] StaticText indicates that the search term "DMV area" has already been entered. However, the agent disregards this detail and continuously issues the command type [2430] [DMV area] until it reaches the maximum step limit. Furthermore, the agent often neglects the previous action information that is provided alongside the observation.
在观察解释上的失误 有趣的是，尽管 GPT-4 能总结观察内容，但有时会忽略更细粒度的信息，例如之前输入的内容。如图 11 右侧示例中，[5172] StaticText 指出搜索词 "DMV area" 已被输入。然而，代理忽视了这一细节并不断发出命令类型 [2430] [DMV area]，直到达到最大步数限制。此外，代理常常忽略随观察一并提供的先前动作信息。


We hypothesize that these observed failures are related to the current pretraining and supervised fine-tuning on dialogues employed in GPT models Ouyang et al. (2022). These models are primarily trained to execute instructions given immediate observations (i.e., the dialogue history); thereby, they may exhibit a lack of explorations. Furthermore, in dialogue scenarios, subtle differences in NL expressions often have less impact on the overall conversation. As a result, models may tend to overlook minor variations in their observations.
我们推测这些观察到的失败与当前在 GPT 模型中使用的预训练和基于对话的有监督微调有关 Ouyang et al. (2022)。这些模型主要被训练在给定即时观察（即对话历史）的情况下执行指令；因此，它们可能表现出缺乏探索性。此外，在对话场景中，自然语言表达的微小差异通常对整体对话影响较小，导致模型可能倾向于忽视观察中的细微变化。