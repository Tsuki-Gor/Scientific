# HI-AGENT: HIERARCHICAL VISION-LANGUAGE AGENTS FOR MOBILE DEVICE CONTROL
# HI-AGENT：用于移动设备控制的分层视觉语言智能体


Zhe Wu ${}^{1}$ , Hongjin Lu ${}^{1}$ , Junliang Xing ${}^{1}$ ; Changhao Zhang ${}^{1}$ , Yin Zhu ${}^{1}$ , Yuhao Yang ${}^{2}$ , Yuheng Jing ${}^{3}$ , Kai Li ${}^{3}$ , Kun Shao ${}^{2}$ , Jianye Hao ${}^{2}$ , Jun Wang ${}^{4}$ , Yuanchun Shi ${}^{1}$
吴哲${}^{1}$，鲁鸿金${}^{1}$，邢军亮${}^{1}$；张昌昊${}^{1}$，朱垠${}^{1}$，杨宇豪${}^{2}$，景宇衡${}^{3}$，李凯${}^{3}$，邵坤${}^{2}$，郝建业${}^{2}$，汪军${}^{4}$，史元春${}^{1}$


${}^{1}$ Tsinghua University ${}^{2}$ Huawei Noah’s Ark Lab
${}^{1}$清华大学${}^{2}$华为诺亚方舟实验室


${}^{3}$ Institute of Automation,Chinese Academy of Sciences ${}^{4}$ University College London
${}^{3}$中国科学院自动化研究所${}^{4}$伦敦大学学院


## ABSTRACT
## 摘要


Building agents that autonomously operate mobile devices has attracted increasing attention. While Vision-Language Models (VLMs) show promise, most existing approaches rely on direct state-to-action mappings, which lack structured reasoning and planning, and thus generalize poorly to novel tasks or unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical vision-language agent for mobile control, featuring a high-level reasoning model and a low-level action model that are jointly optimized. For efficient training, we reformulate multi-step decision-making as a sequence of single-step subgoals and propose a foresight advantage function, which leverages execution feedback from the low-level model to guide high-level optimization. This design alleviates the path explosion issue encountered by Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art (SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark, significantly outperforming prior methods across three paradigms: prompt-based (AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot generalization on the ScreenSpot-v2 benchmark. On the more challenging Android-World benchmark, Hi-Agent also scales effectively with larger backbones, showing strong adaptability in high-complexity mobile control scenarios.
构建能够自主操作移动设备的智能体已引起越来越多的关注。虽然视觉语言模型（VLMs）展现出潜力，但现有方法大多依赖于直接的状态-动作映射，缺乏结构化的推理与规划，因此在面对新任务或未见过的UI布局时泛化性较差。我们推出了Hi-Agent，一种用于移动控制的可训练分层视觉语言智能体，其特点是包含一个高层推理模型和一个底层动作模型，并对二者进行联合优化。为了实现高效训练，我们将多步决策重新表述为一系列单步子目标，并提出了一种前瞻优势函数，利用底层模型的执行反馈来指导高层优化。这一设计缓解了群体相对策略优化（GRPO）在长程任务中遇到的路径爆炸问题，并实现了稳定、无需判别器（critic-free）的联合训练。Hi-Agent在Android-in-the-Wild (AitW)基准测试中达到了87.9%的任务成功率，创下新的SOTA，显著优于基于提示词（AppAgent: 17.7%）、监督学习（Filtered BC: 54.5%）和强化学习（DigiRL: 71.9%）的三类既有范式。它在ScreenSpot-v2基准测试中也展现了极具竞争力的零样本泛化能力。在更具挑战性的Android-World基准测试中，Hi-Agent还能随着骨干网络规模的扩大而有效扩展，在复杂移动控制场景中表现出强大的适应性。


## 1 INTRODUCTION
## 1 引言


Creating intelligent agents capable of assisting users with automated mobile device operations has received growing attention in recent years (Li et al. 2024). The rise of large-scale foundation models (Devlin et al. 2019) Radford et al. 2018 Raffel et al. 2020 Ouyang et al. 2022; Touvron et al. 2023), particularly vision-language models (VLMs) (Lu et al. 2019; Radford et al. 2021; Liu et al. 2023; Bai et al. 2023; Wang et al. 2024a), has opened new possibilities for instruction following, commonsense reasoning, and zero-shot generalization in this domain.
近年来，开发能够辅助用户自动操作移动设备的智能智能体受到了越来越多的关注（Li et al. 2024）。大规模基座模型（Devlin et al. 2019; Radford et al. 2018; Raffel et al. 2020; Ouyang et al. 2022; Touvron et al. 2023），特别是视觉语言模型（VLMs）（Lu et al. 2019; Radford et al. 2021; Liu et al. 2023; Bai et al. 2023; Wang et al. 2024a）的兴起，为该领域的指令遵循、常识推理和零样本泛化开辟了新的可能性。


Current methods for building mobile agents are broadly classified by their optimization strategy into two categories: prompt-based and post-trained agents. Prompt-based approaches leverage powerful, frozen large models through carefully designed prompts and tool-usage workflows (Zhang et al. 2023; Wang et al. Chen et al. 2024). While demonstrating strong initial capabilities, they are limited by high inference costs and an inability to adapt their parameters to downstream tasks. In contrast, post-trained agents fine-tune smaller, more efficient VLMs via supervised fine-tuning (SFT) or reinforcement learning (RL) for greater adaptability (Bai et al. 2024; Zhang & Zhang, 2024; Qin et al. 2025). Our work focuses on this RL-based post-training approach for mobile device control.
目前构建移动智能体的方法根据其优化策略大致可分为两类：基于提示词的智能体和经过后训练的智能体。基于提示词的方法通过精心设计的提示和工具使用流程，利用强大的冻结大模型（Zhang et al. 2023; Wang et al.; Chen et al. 2024）。虽然这些方法展示了强大的初始能力，但受到高推理成本和参数无法适应下游任务的限制。相比之下，后训练智能体通过监督微调（SFT）或强化学习（RL）来微调更小、更高效的VLM，以获得更强的适应性（Bai et al. 2024; Zhang &amp; Zhang, 2024; Qin et al. 2025）。我们的工作重点是针对移动设备控制的这种基于强化学习的后训练方法。


Within the post-trained paradigm, model architecture is a critical design choice. As illustrated in Figure 1, many agents adopt a flat architecture (Figure 1(a)). Some attempt to learn a direct state-to-action mapping, but this brittle mapping struggles to generalize to unseen tasks (Bai et al., 2024). Others employ a single model for both reasoning and decision-making, but this approach often
在后训练范式中，模型架构是一个关键的设计选择。如图1所示，许多智能体采用扁平化架构（图1(a)）。一些尝试学习直接的状态-动作映射，但这种脆弱的映射难以泛化到未见过的任务（Bai et al., 2024）。另一些则使用单一模型同时进行推理和决策，但这种方法通常


---



${}^{ \dagger  }$ Corresponding Author.
${}^{ \dagger  }$ 通讯作者。


---



<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_c2a6d5.jpg"/>



Figure 1: Different paradigms for mobile control agents. (a) Flat Agents use a single trainable model for state-to-(reason-)action mapping. (b) Hierarchical Agents use a planner to improve reasoning, but it is typically a frozen black-box. (c) Hi-Agent (Ours) enables joint optimization where the high-level reasoning and low-level action models are co-adapted and fully trainable.
图1：移动控制智能体的不同范式。(a) 扁平化智能体使用单一可训练模型进行状态到（推理-）动作的映射。(b) 分层智能体使用规划器来改进推理，但该规划器通常是冻结的黑盒。(c) Hi-Agent（本研究）实现了联合优化，其中高层推理和底层动作模型是协同适应且完全可训练的。


demands massive computational resources and extensive high-quality data for training (Gu et al. 2025). Recently, hierarchical architectures have emerged (Figure 1(b)) to decompose the problem by using a high-level model for reasoning and a low-level model for execution, thereby simplifying the optimization challenge (Agashe et al. 2025). However, the high-level model often remains frozen, precluding true end-to-end learning and co-adaptation between the two levels.
需要庞大的计算资源和海量高质量数据进行训练 (Gu et al. 2025)。最近出现了层级化架构 (图 1(b))，通过使用高层模型进行推理和底层模型进行执行来分解问题，从而简化优化挑战 (Agashe et al. 2025)。然而，高层模型通常保持冻结，阻碍了真正的端到端学习以及两层之间的协同适应。


To overcome these limitations, we propose a third architectural paradigm: a jointly optimized hierarchical agent (Figure 1(c)). We introduce Hi-Agent, a hierarchical agent where both the high-level reasoning model $\left( {\pi }_{h}\right)$ and the low-level action model $\left( {\pi }_{\ell }\right)$ are trainable and co-adapted during post-training. This approach marries the structural robustness of a hierarchy with the adaptability of end-to-end optimization, allowing the planner to learn what constitutes an effective subgoal based on direct feedback from the executor's performance.
为了克服这些限制，我们提出了第三种架构范式：联合优化的层级智能体 (图 1(c))。我们推出了 Hi-Agent，这是一种层级智能体，其高层推理模型 $\left( {\pi }_{h}\right)$ 和底层动作模型 $\left( {\pi }_{\ell }\right)$ 在后训练阶段均可训练且相互协同。该方法将层级结构的稳健性与端到端优化的适应性相结合，使规划器能够根据执行器表现的直接反馈，学习何为有效的子目标。


We introduce a novel training strategy based on Group Relative Policy Optimization (GRPO) (Shao et al. 2024; Guo et al. 2025). To make GRPO tractable for long-horizon tasks, we first reformulate them into a sequence of single-step subgoal predictions, reducing the optimization complexity from exponential $\left( {G}^{n}\right)$ to linear $\left( {n \cdot  G}\right)$ . Second,we introduce a foresight advantage function that propagates low-level execution feedback to guide the high-level optimization. This enables stable, critic-free, and sample-efficient joint training.
我们提出了一种基于群体相对策略优化 (GRPO) 的新型训练策略 (Shao et al. 2024; Guo et al. 2025)。为了使 GRPO 适用于长程任务，我们首先将其重新表述为一系列单步子目标预测，将优化复杂度从指数级 $\left( {G}^{n}\right)$ 降低到线性级 $\left( {n \cdot  G}\right)$。其次，我们引入了一种前瞻优势函数，将底层执行反馈传播以指导高层优化。这实现了稳定、无评论器且样本高效的联合训练。


Our main contributions are as follows:
我们的主要贡献如下：


- We propose Hi-Agent, a trainable hierarchical agent with a jointly optimized planner and executor that combines structured reasoning with end-to-end adaptation for mobile control.
- 我们提出了 Hi-Agent，这是一种具有联合优化规划器和执行器的可训练层级智能体，为移动端控制结合了结构化推理与端到端自适应。


- We develop a GRPO-based training framework with a foresight advantage function, which overcomes the path explosion and enables stable credit assignment for high-level planning.
- 我们开发了一个基于 GRPO 的训练框架，配合前瞻优势函数，克服了路径爆炸并为高层规划实现了稳定的信用分配。


- Hi-Agent achieves SOTA performance and strong generalization, demonstrating robustness, versatility, and scalability across benchmarks like AitW, ScreenSpot-v2, and AndroidWorld.
- Hi-Agent 实现了 SOTA 性能和极强的泛化能力，在 AitW、ScreenSpot-v2 和 AndroidWorld 等基准测试中展现了稳健性、通用性和可扩展性。


Experiments show that Hi-Agent achieves a new state-of-the-art 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark, significantly outperforming prior methods. It also demonstrates competitive zero-shot generalization on the ScreenSpot-v2 benchmark and scales effectively on the more complex AndroidWorld benchmark, highlighting its excellent adaptability.
实验表明，Hi-Agent 在 Android-in-the-Wild (AitW) 基准测试中达到了 87.9% 的任务成功率，刷新了现有纪录并显著超越先前方法。它还在 ScreenSpot-v2 基准测试中展现了极具竞争力的零样本泛化能力，并在更复杂的 AndroidWorld 基准测试中实现了有效扩展，彰显了其卓越的适应性。


## 2 RELATED WORK
## 2 相关工作


2.1 VISION-LANGUAGE AGENTS WITH TOOL-AUGMENTED MOBILE CONTROL
2.1 具备工具增强移动控制能力的视觉语言智能体


Large vision-language models, augmented by specialized tools, have demonstrated strong performance on various tasks (Yang et al. 2023; FAIR Qian et al. 2023). In mobile device control, approaches combine models, tools, and skills to enhance automation. For example, AppAgent (Zhang et al. 2023) builds on GPT-4V by leveraging Android XML files for on-screen localization and learns to use new applications via path exploration or human demonstrations. MobileAgent (Wang et al.) uses a visual module to locate screen elements without XML data, paired with incremental self-planning to traverse app interfaces. Mobile-Agent-v2 (Wang et al., 2025) introduces a multi-agent paradigm, combining a language model and a vision-language model to support task progression and content-focused navigation. While these methods leverage powerful base models and sophisticated tool coordination, they typically avoid updating the base model's parameters. As a result, performance gains are limited, and the large size of these models—often exceeding hundreds of billions of parameters—can hinder real-world deployment.
由专用工具增强的大型视觉语言模型在多项任务中展现了强劲性能 (Yang et al. 2023; FAIR Qian et al. 2023)。在移动设备控制领域，各种方法通过结合模型、工具和技能来提升自动化程度。例如，AppAgent (Zhang et al. 2023) 基于 GPT-4V，利用 Android XML 文件进行屏幕定位，并通过路径探索或人类演示学习使用新应用。MobileAgent (Wang et al.) 使用视觉模块在无需 XML 数据的情况下定位屏幕元素，并配合增量式自我规划来遍历应用界面。Mobile-Agent-v2 (Wang et al., 2025) 引入了多智能体范式，结合语言模型和视觉语言模型来支持任务推进和专注内容的导航。虽然这些方法利用了强大的基础模型和复杂的工具协调，但它们通常避免更新基础模型的参数。因此，性能提升有限，且这些模型庞大的体积（通常超过千亿参数）可能会阻碍实际部署。


### 2.2 PARAMETER-EFFICIENT LEARNING FOR MOBILE DEVICE CONTROL
### 2.2 移动设备控制的参数高效学习


To balance model size and efficacy, researchers have explored fine-tuning vision-language models on demonstration data for mobile device control. Auto-GUI (Zhang & Zhang, 2024) interacts directly with user interfaces-without relying on external tools or low-level system data-by applying gradient-based updates on expert demonstration datasets. DigiRL (Bai et al., 2024) adopts a two-stage reinforcement learning pipeline: it first pretrains a policy in an offline RL setting, then transitions to online RL to refine state-action mappings. DigiQ (Bai et al., 2025a) eliminates the need for online interaction by learning a VLM Q-value function solely from offline data, using temporal-difference (TD) learning on frozen intermediate layers instead of retraining the entire model-achieving performance comparable to DigiRL. However, because these methods directly map tasks to actions, small deviations from the training distribution (e.g., shifts in application locations or UI layout changes) can break the learned mapping and require retraining. Our work addresses this limitation by introducing a reasoning component that transforms direct mappings into a hierarchical "reason first, then act" framework, improving generalization and interpretability.
为了平衡模型规模与效能，研究人员探索了在移动设备控制的演示数据上微调视觉语言模型。Auto-GUI (Zhang &amp; Zhang, 2024) 通过在专家演示数据集上应用基于梯度的更新，直接与用户界面交互，而不依赖外部工具或底层系统数据。DigiRL (Bai et al., 2024) 采用两阶段强化学习流水线：首先在离线 RL 设置下预训练策略，然后转入在线 RL 以优化状态-动作映射。DigiQ (Bai et al., 2025a) 通过仅从离线数据中学习 VLM Q值函数，消除了在线交互的需求，它在冻结的中间层上使用时序差分 (TD) 学习而非重新训练整个模型，实现了与 DigiRL 相当的性能。然而，由于这些方法将任务直接映射到动作，训练分布的微小偏离（例如应用位置偏移或 UI 布局变化）可能会破坏已学习的映射并需要重新训练。我们的工作通过引入推理组件解决了这一局限性，将直接映射转化为“先推理，后行动”的分层框架，提高了泛化性和可解释性。


### 2.3 REINFORCEMENT LEARNING-BASED POST-TRAINING FOR VISION-LANGUAGE MODELS
### 2.3 基于强化学习的视觉语言模型后训练


Post-training typically refers to applying reinforcement learning (RL) directly to foundation large language models (LLMs) or VLMs without relying on supervised fine-tuning (SFT) as a prerequisite. OpenAI O1 (OpenAI, 2024b) has demonstrated that RL-driven post-training can effectively enhance the reasoning capabilities of LLMs in a scalable manner, requiring fewer computational resources than SFT. To further reduce RL training overhead, DeepSeekMath (Shao et al. 2024) employs GRPO, eliminating the need for a critic model comparable in size to the policy. Instead, it uses group-based rewards to estimate advantages, yielding significant improvements in mathematical, programming, and scientific reasoning tasks. Adapting GRPO to mobile multi-modal control presents two challenges: the exponential growth of reasoning paths and the lack of dense reward signals for high-level planning. We address both issues by designing a hierarchical optimization framework that reduces the reasoning complexity from ${G}^{n}$ to $n \cdot  G$ ,and by incorporating a foresight advantage function to guide high-level policy updates using low-level execution feedback.
后训练通常指直接对基础大语言模型 (LLM) 或 VLM 应用强化学习 (RL)，而不必以监督微调 (SFT) 为前提。OpenAI O1 (OpenAI, 2024b) 已证明，RL 驱动的后训练能以可扩展的方式有效增强 LLM 的推理能力，且比 SFT 所需的计算资源更少。为了进一步降低 RL 训练开销，DeepSeekMath (Shao et al. 2024) 采用 GRPO，消除了对规模与策略相当的评论员模型的需求。相反，它使用基于组的奖励来估计优势，在数学、编程和科学推理任务中取得了显著改进。将 GRPO 应用于移动多模态控制面临两个挑战：推理路径的指数级增长以及高层规划缺乏密集奖励信号。我们通过设计一个分层优化框架解决了这两个问题，将推理复杂度从 ${G}^{n}$ 降低到 $n \cdot  G$，并结合前瞻优势函数，利用底层执行反馈来引导高层策略更新。


## 3 PRELIMINARIES
## 3 预备知识


We model mobile device control as a multi-step decision-making process under a Markov Decision Process (MDP), defined as:
我们将移动设备控制建模为马尔可夫决策过程 (MDP) 下的多步决策过程，定义为：


$$
{M}_{\text{ Interact }} = \left( {\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma }\right) ,
$$



where $\mathcal{S} = {\mathcal{X}}_{\text{ img }} \times  \mathcal{L}$ is the state space of screen images and task instructions, $\mathcal{A}$ denotes atomic UI actions (e.g.,click,swipe), $\mathcal{P}$ captures environment transitions,and $\mathcal{R}$ provides task feedback.
其中 $\mathcal{S} = {\mathcal{X}}_{\text{ img }} \times  \mathcal{L}$ 是屏幕图像和任务指令的状态空间，$\mathcal{A}$ 表示原子 UI 动作（如点击、滑动），$\mathcal{P}$ 捕捉环境状态转移，而 $\mathcal{R}$ 提供任务反馈。


While the environment operates at the level of discrete UI actions, subgoal and action generation by language models unfolds token by token. To support RL training over such autoregressive outputs, we follow standard practice (Ouyang et al. 2022) and define a token-level MDP:
尽管环境在离散 UI 动作层面运行，但语言模型生成子目标和动作是逐个 Token 展开的。为了支持在此类自回归输出上进行 RL 训练，我们遵循标准做法 (Ouyang et al. 2022) 并定义了一个 Token 级 MDP：


$$
{M}_{\text{ Gen }} = \left( {{\mathcal{S}}_{\text{ tok }},{\mathcal{A}}_{\text{ tok }},{\mathcal{P}}_{\text{ tok }},{\mathcal{R}}_{\text{ tok }},\gamma }\right) ,
$$



where ${\mathcal{S}}_{\text{ tok }}$ is the space of sequences, ${\mathcal{A}}_{\text{ tok }}$ is the vocabulary,and ${\mathcal{P}}_{\text{ tok }}$ appends tokens deterministically. Rewards ${\mathcal{R}}_{\text{ tok }}$ are assigned post-generation,based on alignment with oracle actions.
其中 ${\mathcal{S}}_{\text{ tok }}$ 是序列空间，${\mathcal{A}}_{\text{ tok }}$ 是词表，${\mathcal{P}}_{\text{ tok }}$ 以确定性方式追加 Token。奖励 ${\mathcal{R}}_{\text{ tok }}$ 在生成后根据与专家动作的一致性进行分配。


This dual-MDP formulation enables structured learning: we optimize token-level generation via reinforcement learning while evaluating policies in the full multi-step environment.
这种双重 MDP 公式化实现了结构化学习：我们通过强化学习优化 Token 级生成，同时在完整的多步环境中评估策略。


## 4 METHOD
## 4 方法


To address the brittleness of direct state-to-action mappings, our key insight is to introduce dedicated reasoning and action components, transforming this flat mapping into a hierarchical decision process that follows the principle of first reason,then act. We define the overall policy as $\pi  = \left( {{\pi }_{h},{\pi }_{\ell }}\right)$ , where the high-level reasoning model ${\pi }_{h}$ predicts a semantic subgoal,and the low-level action model ${\pi }_{\ell }$ executes the atomic action based on the subgoal and the current screenshot.
为了解决直接状态-动作映射的脆弱性，我们的核心见解是引入专门的推理和动作组件，将这种扁平映射转化为遵循“先推理，后行动”原则的分层决策过程。我们将整体策略定义为 $\pi  = \left( {{\pi }_{h},{\pi }_{\ell }}\right)$，其中高层推理模型 ${\pi }_{h}$ 预测语义子目标，底层动作模型 ${\pi }_{\ell }$ 根据子目标和当前屏幕截图执行原子动作。


We organize this section into three parts. Section 4.1 formalizes our hierarchical structure using recursive value modeling. Section 4.2 introduces our hierarchical post-training method inspired by this decomposition. Section 4.3 presents the data generation pipeline and training implementation.
我们将本节分为三个部分。第 4.1 节使用递归价值建模对我们的分层结构进行公式化。第 4.2 节介绍了受此分解启发的分层后训练方法。第 4.3 节介绍了数据生成流水线和训练实现。


4.1 HIERARCHICAL TASK DECOMPOSITION FOR MOBILE CONTROL
4.1 移动控制的分层任务分解


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_fa33c8.jpg"/>



Figure 2: Illustration of recursive task decomposition under a hierarchical policy.
图 2：分层策略下的递归任务分解图示。


Mobile device control tasks often exhibit natural hierarchical structure. For example, consider the instruction "Send a message to Alice". As shown in Figure 2, this task can be broken down into subtasks such as "Open Messenger" and "Send message to Alice in Messenger", the latter of which may be further decomposed into "Search Alice", "Compose message", and "Press send". Each subgoal contributes to completing the overall task and fits into a recursive hierarchy.
移动设备控制任务通常表现出自然的层级结构。例如，考虑指令“给 Alice 发送消息”。如图 2 所示，该任务可以分解为诸如“打开 Messenger”和“在 Messenger 中给 Alice 发送消息”等子任务，后者可进一步分解为“搜索 Alice”、“撰写消息”和“点击发送”。每个子目标都有助于完成整体任务，并融入递归层级结构中。


In hierarchical RL (Pateria et al. 2021), the recursive structure is often formalized via value function decomposition. Following prior work (Dietterich, 2000, Ghavamzadeh & Mahadevan, 2007), we model the overall task as an MDP ${M}_{\text{ Interact }}$ ,which captures environment dynamics and UI-level feedback,and decompose it into subtasks $\left\{  {{M}_{0},{M}_{1},\ldots ,{M}_{n}}\right\}$ ,where ${M}_{0}$ is the root. The value function ${V}_{i}^{\pi }\left( s\right)$ for subtask ${M}_{i}$ under policy $\pi$ is defined as:
在层级强化学习（Pateria et al. 2021）中，递归结构通常通过价值函数分解来形式化。参考前人工作（Dietterich, 2000, Ghavamzadeh &amp; Mahadevan, 2007），我们将整体任务建模为一个 MDP ${M}_{\text{ Interact }}$，它捕获了环境动态和 UI 级反馈，并将其分解为子任务 $\left\{  {{M}_{0},{M}_{1},\ldots ,{M}_{n}}\right\}$，其中 ${M}_{0}$ 是根节点。在策略 $\pi$ 下，子任务 ${M}_{i}$ 的价值函数 ${V}_{i}^{\pi }\left( s\right)$ 定义为：


$$
{V}_{i}^{\pi }\left( s\right)  = \left\{  \begin{array}{ll} {Q}_{i}^{\pi }\left( {s,\pi \left( s\right) }\right)  = {V}_{g}^{\pi }\left( s\right)  + {C}_{i}^{\pi }\left( {s,g}\right) & \text{ if }i\text{ is composite, } \\  \mathop{\sum }\limits_{{s}^{\prime }}P\left( {{s}^{\prime } \mid  s,i}\right)  \cdot  R\left( {{s}^{\prime } \mid  s,i}\right) & \text{ if }i\text{ is primitive, } \end{array}\right. \tag{1}
$$



where $g = \pi \left( s\right)$ is the selected subtask and ${C}_{i}^{\pi }\left( {s,g}\right)$ denotes the expected return after $g$ completes:
其中 $g = \pi \left( s\right)$ 是选定的子任务，${C}_{i}^{\pi }\left( {s,g}\right)$ 表示 $g$ 完成后的期望回报：


$$
{C}_{i}^{\pi }\left( {s,g}\right)  = \mathop{\sum }\limits_{{{s}^{\prime },N}}{P}_{i}^{\pi }\left( {{s}^{\prime },N \mid  s,g}\right)  \cdot  {\gamma }^{N}{Q}_{i}^{\pi }\left( {{s}^{\prime },\pi \left( {s}^{\prime }\right) }\right) , \tag{2}
$$



where $\left( {{s}^{\prime },N}\right)$ denotes the resulting state and duration after completing $g$ . This recursive decomposition provides intuitive motivation that each subtask ${M}_{i}$ is optimized not only for its immediate executability (captured by ${V}_{g}^{\pi }\left( s\right)$ ),but also for its long-term impact on overall task success (modeled by $\left. {{C}_{i}^{\pi }\left( {s,g}\right) }\right)$ .
其中 $\left( {{s}^{\prime },N}\right)$ 表示完成 $g$ 后的结果状态和持续时间。这种递归分解提供了一种直观的动机，即每个子任务 ${M}_{i}$ 的优化不仅是为了其即时可执行性（由 ${V}_{g}^{\pi }\left( s\right)$ 捕获），也是为了其对整体任务成功的长期影响（由 $\left. {{C}_{i}^{\pi }\left( {s,g}\right) }\right)$ 建模）。


In practice, rather than maintaining a separate policy for every subtask, we implement a compact two-level architecture: a high-level reasoning policy ${\pi }_{h}$ that emits semantic subgoals ${g}_{t}$ ,and a low-level action policy ${\pi }_{\ell }$ that executes each subtask via atomic actions ${a}_{t}$ . This design enables cross-task skill reuse and efficient end-to-end training. We further analyze its optimality in Appendix A
在实践中，我们没有为每个子任务维护单独的策略，而是实现了一个紧凑的两层架构：一个发出语义子目标 ${g}_{t}$ 的高层推理策略 ${\pi }_{h}$，以及一个通过原子动作 ${a}_{t}$ 执行每个子任务的底层动作策略 ${\pi }_{\ell }$。这种设计实现了跨任务的技能复用和高效的端到端训练。我们进一步在附录 A 中分析了其最优性。


### 4.2 HIERARCHICAL POLICY POST-TRAINING
### 4.2 层级策略后训练


While recursive value decomposition offers useful intuition, explicitly modeling value functions $\left( {{V}_{a}^{\pi },{C}_{i}^{\pi }}\right)$ is computationally expensive and unstable,particularly for LLMs with sparse rewards. We thus adopt Group Relative Policy Optimization (GRPO) (Shao et al. 2024), a scalable, critic-free alternative that computes relative advantages over $G$ sampled outputs from the generation MDP ${M}_{\mathrm{{Gen}}}$ . The corresponding surrogate objective is:
虽然递归价值分解提供了有用的直觉，但显式建模价值函数 $\left( {{V}_{a}^{\pi },{C}_{i}^{\pi }}\right)$ 的计算成本高昂且不稳定，尤其是对于具有稀疏奖励的 LLM。因此，我们采用组相对策略优化（GRPO）（Shao et al. 2024），这是一种可扩展的、无评论器的替代方案，它计算从生成 MDP ${M}_{\mathrm{{Gen}}}$ 采样的 $G$ 个输出的相对优势。相应的代理目标为：


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_f53a93.jpg"/>



$$
\min \left( {\frac{{\pi }_{\theta }\left( {{o}_{i,t} \mid  q,{o}_{i, < t}}\right) }{{\pi }_{{\theta }_{\text{ old }}}\left( {{o}_{i,t} \mid  q,{o}_{i, < t}}\right) }{\widehat{A}}_{i,t},\operatorname{clip}\left( {\frac{{\pi }_{\theta }\left( {{o}_{i,t} \mid  q,{o}_{i, < t}}\right) }{{\pi }_{{\theta }_{\text{ old }}}\left( {{o}_{i,t} \mid  q,{o}_{i, < t}}\right) },1 - \epsilon ,1 + \epsilon }\right) {\widehat{A}}_{i,t})}\right\}
$$



(3)



Here, ${\pi }_{\theta }$ and ${\pi }_{{\theta }_{\text{ old }}}$ are the current and reference policies, $q$ is the task input, ${o}_{i,t}$ is the $t$ -th token in output ${o}_{i}$ sampled from ${\pi }_{{\theta }_{\text{ old }}},{\widehat{A}}_{i,t}$ is the estimated advantage,and $\epsilon$ is the clipping threshold.
这里，${\pi }_{\theta }$ 和 ${\pi }_{{\theta }_{\text{ old }}}$ 是当前策略和参考策略，$q$ 是任务输入，${o}_{i,t}$ 是从 ${\pi }_{{\theta }_{\text{ old }}},{\widehat{A}}_{i,t}$ 采样的输出 ${o}_{i}$ 中的第 $t$ 个标记，是估计的优势，$\epsilon$ 是裁剪阈值。


However, applying GRPO to long-horizon tasks presents two major challenges: (1) sampling complexity grows exponentially with trajectory length,requiring ${G}^{n}$ rollouts for $n$ -step; (2) high-level subgoals are abstract and not directly executable, making reward assignment difficult. To address these issues,we make three key modifications (Figure 3): (1) we decompose $n$ -step tasks into $n$ single-step subtasks,reducing sampling cost from ${G}^{n}$ to $n \cdot  G$ ; (2) we introduce a foresight reward for each subgoal ${g}_{t}$ from ${\pi }_{h}$ ,integrating execution feedback and subgoal quality; (3) we adopt an alternating optimization scheme for ${\pi }_{h}$ and ${\pi }_{\ell }$ to enable mutual adaptation during training.
然而，将 GRPO 应用于长程任务面临两大挑战：（1）采样复杂度随轨迹长度呈指数增长，$n$ 步需要 ${G}^{n}$ 次展开；（2）高层子目标是抽象的且无法直接执行，导致奖励分配困难。为了解决这些问题，我们进行了三项关键改进（图 3）：（1）我们将 $n$ 步任务分解为 $n$ 个单步子任务，将采样成本从 ${G}^{n}$ 降低到 $n \cdot  G$；（2）我们为来自 ${\pi }_{h}$ 的每个子目标 ${g}_{t}$ 引入前瞻奖励，整合执行反馈和子目标质量；（3）我们对 ${\pi }_{h}$ 和 ${\pi }_{\ell }$ 采用交替优化方案，以实现训练期间的相互适应。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_e079c6.jpg"/>



Figure 3: Hierarchical Policy Optimization. (a) Standard GRPO incurs exponential sample complexity $\left( {G}^{n}\right)$ and lacks intermediate reward signals in long-horizon tasks. (b) Hi-Agent reduces complexity to $n \cdot  G$ by decoupling subgoal generation from execution,and enables efficient joint training through foresight-guided subgoal evaluation.
图 3：分层策略优化。(a) 标准 GRPO 具有指数级样本复杂度 $\left( {G}^{n}\right)$，且在长跨度任务中缺乏中间奖励信号。(b) Hi-Agent 通过将子目标生成与执行解耦，将复杂度降低至 $n \cdot  G$，并通过前瞻引导的子目标评估实现高效联合训练。


High-Level Policy Optimization. At timestep $t,{\pi }_{h}$ generates a semantic subgoal ${g}_{t}$ . Inspired by the recursive decomposition (Section 4.1),we design a foresight reward function that encourages ${g}_{t}$ to be both immediately executable and conducive to long-term task progress.
高层策略优化。在时间步 $t,{\pi }_{h}$ 生成语义子目标 ${g}_{t}$。受递归分解（第 4.1 节）启发，我们设计了一个前瞻奖励函数，鼓励 ${g}_{t}$ 既能立即执行又有助于长期任务进展。


To capture both aspects, we combine three reward components: the format reward ${r}_{\mathrm{{fmt}}}\left( {g}_{t}\right)$ is a binary indicator that checks whether ${g}_{t}$ conforms to the required schema <reasoning>...</reasoning><instruction>Instruction>Instruction:...</instruction> the environment feedback reward ${r}_{\text{ env }}\left( {{s}_{t},{g}_{t},{a}_{t}}\right)$ evaluates whether the predicted atomic action ${a}_{t} = {\pi }_{\ell }\left( {{s}_{t},{g}_{t}}\right)$ matches the oracle action ${\widehat{a}}_{t}$ within a tolerance $\epsilon$ :
为了兼顾这两个方面，我们结合了三个奖励组件：格式奖励 ${r}_{\mathrm{{fmt}}}\left( {g}_{t}\right)$ 是一个二进制指标，检查 ${g}_{t}$ 是否符合要求的模式 <reasoning>...</reasoning><instruction>Instruction:...</instruction>；环境反馈奖励 ${r}_{\text{ env }}\left( {{s}_{t},{g}_{t},{a}_{t}}\right)$ 评估预测的原子动作 ${a}_{t} = {\pi }_{\ell }\left( {{s}_{t},{g}_{t}}\right)$ 是否在容差 $\epsilon$ 内匹配真值动作 ${\widehat{a}}_{t}$：


$$
{r}_{\text{ env }}\left( {{s}_{t},{g}_{t},{a}_{t}}\right)  = \mathbb{1}\left\{  {\operatorname{type}\left( {a}_{t}\right)  = \operatorname{type}\left( {\widehat{a}}_{t}\right)  \land  {\begin{Vmatrix}\operatorname{coord}\left( {a}_{t}\right)  - \operatorname{coord}\left( {\widehat{a}}_{t}\right) \end{Vmatrix}}_{2} < \epsilon }\right\}  ;
$$



and the feasibility reward ${\widehat{V}}_{\text{ judge }}\left( {{s}_{t},{g}_{t}}\right)$ is evaluated by a frozen vision-language model,instantiated as Qwen2.5-VL-72B-Instruct(Bai et al. 2025b). This model plays the role of an LLM-based judge (Zheng et al. 2023),estimating whether ${g}_{t}$ semantically meaningful and likely to contribute to long-term task success.
可行性奖励 ${\widehat{V}}_{\text{ judge }}\left( {{s}_{t},{g}_{t}}\right)$ 由冻结的视觉语言模型评估，具体为 Qwen2.5-VL-72B-Instruct (Bai et al. 2025b)。该模型扮演基于 LLM 的裁判角色 (Zheng et al. 2023)，评估 ${g}_{t}$ 在语义上是否有意义，以及是否可能有助于长期任务成功。


These components are combined into a weighted foresight reward:
这些组件被组合成一个加权前瞻奖励：


$$
{r}_{t}^{\left( h\right) } = {\lambda }_{1} \cdot  {r}_{\mathrm{{fmt}}}\left( {g}_{t}\right)  + {\lambda }_{2} \cdot  {r}_{\mathrm{{env}}}\left( {{s}_{t},{g}_{t},{\pi }_{\ell }}\right)  + {\lambda }_{3} \cdot  {\widehat{V}}_{\text{ judge }}\left( {{s}_{t},{g}_{t}}\right) ,\;{\widehat{A}}_{t}^{\left( h\right) } = \frac{{r}_{t}^{\left( h\right) } - {\mu }_{r}}{{\sigma }_{r}},
$$



where ${\mu }_{r}$ and ${\sigma }_{r}$ denote the mean and standard deviation of ${r}_{t}^{\left( h\right) }$ across the $G$ samples. Detailed designs and implementations for each reward component are provided in Appendix C
其中 ${\mu }_{r}$ 和 ${\sigma }_{r}$ 表示 ${r}_{t}^{\left( h\right) }$ 在 $G$ 个样本中的均值和标准差。每个奖励组件的详细设计和实现见附录 C。


Low-Level Policy Optimization. The low-level policy ${\pi }_{\ell }$ receives environment feedback based on whether it successfully completes the subgoal ${g}_{t}$ ,as defined by the environment reward ${r}_{\text{ env }}\left( {{s}_{t},{g}_{t},{\pi }_{\ell }}\right)$ introduced above. For training,we reuse this signal as the step-level reward:
底层策略优化。底层策略 ${\pi }_{\ell }$ 根据其是否成功完成子目标 ${g}_{t}$ 接收环境反馈，该反馈由上文引入的环境奖励 ${r}_{\text{ env }}\left( {{s}_{t},{g}_{t},{\pi }_{\ell }}\right)$ 定义。在训练中，我们复用该信号作为步级奖励：


$$
{r}_{t}^{\left( \ell \right) } = \left\{  {\begin{array}{ll} 1 & \text{ if }{\pi }_{\ell }\text{ completes }{g}_{t}, \\  0 & \text{ otherwise } \end{array},\;{\widehat{A}}_{t}^{\left( \ell \right) } = \frac{{r}_{t}^{\left( \ell \right) } - {\mu }_{r}^{\ell }}{{\sigma }_{r}^{\ell }},}\right. \tag{4}
$$



where ${\mu }_{r}^{\ell }$ and ${\sigma }_{r}^{\ell }$ denote the mean and standard deviation of ${r}_{t}^{\left( \ell \right) }$ across the current batch.
其中 ${\mu }_{r}^{\ell }$ 和 ${\sigma }_{r}^{\ell }$ 表示当前批次中 ${r}_{t}^{\left( \ell \right) }$ 的均值和标准差。


Alternating Joint Optimization. We alternate updates between ${\pi }_{h}$ and ${\pi }_{\ell }$ to facilitate coordination. At iteration $k$ ,we first fix ${\pi }_{h}^{\left( k - 1\right) }$ and optimize ${\pi }_{\ell }^{\left( k\right) }$ using environment rewards,then fix ${\pi }_{\ell }^{\left( k\right) }$ and update ${\pi }_{h}^{\left( k\right) }$ with the foresight advantage:
交替联合优化。我们在 ${\pi }_{h}$ 和 ${\pi }_{\ell }$ 之间交替更新以促进协同。在迭代 $k$ 时，我们首先固定 ${\pi }_{h}^{\left( k - 1\right) }$ 并使用环境奖励优化 ${\pi }_{\ell }^{\left( k\right) }$，然后固定 ${\pi }_{\ell }^{\left( k\right) }$ 并利用前瞻优势更新 ${\pi }_{h}^{\left( k\right) }$：


$$
{\theta }_{\ell }^{\left( k\right) } \leftarrow  \arg \mathop{\max }\limits_{{\theta }_{\ell }}{\mathcal{J}}_{\mathrm{{GRPO}}}\left( {{\pi }_{\ell }^{{\theta }_{\ell }} \mid  {\pi }_{h}^{{\theta }_{h}^{\left( k - 1\right) }}}\right) ,\;{\theta }_{h}^{\left( k\right) } \leftarrow  \arg \mathop{\max }\limits_{{\theta }_{h}}{\mathcal{J}}_{\mathrm{{GRPO}}}\left( {{\pi }_{h}^{{\theta }_{h}} \mid  {\pi }_{\ell }^{{\theta }_{\ell }^{\left( k\right) }}}\right) . \tag{5}
$$



### 4.3 DATA GENERATION AND TRAINING IMPLEMENTATION
### 4.3 数据生成与训练实现


Data Generation. To enable efficient training, we construct an automated pipeline that interacts with Android emulators to generate subgoal-action trajectories. A hierarchical oracle-built from Qwen2.5-VL-72B (reasoning ${\pi }_{h}^{ * }$ ) and Qwen2.5-VL-7B (action ${\pi }_{\ell }^{ * }$ )-produces demonstrations without manual annotation or rollbacks. To ensure a fair evaluation and mitigate data leakage, our process maintains a strict separation between training and test distributions. For AitW, we only reuse task instructions from the official splits to generate entirely new interaction trajectories, rather than using the original demonstration data. For the template-based AndroidWorld, we use different randomization seeds for the training and evaluation sets to prevent instance-level overlap. This process yielded over 1,200 high-quality, manually verified samples across all tasks. A comprehensive breakdown of our data construction protocol, dataset statistics, and a quantitative analysis of train-test overlap are provided in Appendix B.
数据生成。为了实现高效训练，我们构建了一个自动化流水线，通过与 Android 模拟器交互来生成子目标-动作轨迹。一个由 Qwen2.5-VL-72B（推理 ${\pi }_{h}^{ * }$）和 Qwen2.5-VL-7B（动作 ${\pi }_{\ell }^{ * }$）构建的分层专家系统在无需人工标注或回溯的情况下生成演示。为确保公平评估并缓解数据泄漏，我们的流程在训练和测试分布之间保持严格分离。对于 AitW，我们仅重复使用官方划分的任务指令来生成全新的交互轨迹，而非使用原始演示数据。对于基于模板的 AndroidWorld，我们在训练集和评估集中使用不同的随机种子，以防止实例级重叠。该过程在所有任务中产生了超过 1,200 个经过人工验证的高质量样本。附录 B 提供了关于数据构建协议、数据集统计信息以及训练-测试重叠定量分析的详细说明。


Each trajectory $\tau  = {\left\{  \left( {s}_{t},{u}_{t},{\widehat{g}}_{t},{\widehat{a}}_{t}\right) \right\}  }_{t = 1}^{T}$ consists of the UI screen state ${s}_{t}$ ,task instruction ${u}_{t}$ ,the generated semantic subgoal ${\widehat{g}}_{t}$ ,and the corresponding atomic UI action ${\widehat{a}}_{t}$ ,where:
每条轨迹 $\tau  = {\left\{  \left( {s}_{t},{u}_{t},{\widehat{g}}_{t},{\widehat{a}}_{t}\right) \right\}  }_{t = 1}^{T}$ 由 UI 屏幕状态 ${s}_{t}$、任务指令 ${u}_{t}$、生成的语义子目标 ${\widehat{g}}_{t}$ 以及相应的原子 UI 动作 ${\widehat{a}}_{t}$ 组成，其中：


$$
{\widehat{g}}_{t} \sim  {\pi }_{h}^{ * }\left( {\widehat{g} \mid  {s}_{t},{u}_{t}}\right) ,\;{\widehat{a}}_{t} = {\pi }_{\ell }^{ * }\left( {\widehat{a} \mid  {s}_{t},{\widehat{g}}_{t}}\right) .
$$



These trajectories serve as ground-truth references for computing the rewards described in Section 4.2, and are stored in structured JSON format:
这些轨迹作为计算第 4.2 节所述奖励的基准参考，并以结构化的 JSON 格式存储：


---



\{ "image_path": "android/save/images/xxx.png",
\{ "image_path": "android/save/images/xxx.png",


"problem": "Search for hotels in Washington DC",
"problem": "搜索华盛顿特区的酒店",


"instruction": "Click on the Chrome icon to open the browser.",
"instruction": "点击 Chrome 图标以打开浏览器。",


"solution": \{ "action_type": "DUAL_POINT",
"solution": \{ "action_type": "DUAL_POINT",


"touch_point": [0.7781, 0.6972] \}\}
"touch_point": [0.7781, 0.6972] \}\}


---



Training and Implementation. We jointly train the high-level policy ${\pi }_{h}$ and the low-level policy ${\pi }_{\ell }$ using our modified GRPO scheme,which incorporates foresight advantage estimation and alternating optimization. Both components are instantiated with Qwen2.5VL-3B-Instruct.
训练与实现。我们使用改进的 GRPO 方案共同训练高层策略 ${\pi }_{h}$ 和底层策略 ${\pi }_{\ell }$，该方案结合了前瞻优势估计和交替优化。两个组件均采用 Qwen2.5VL-3B-Instruct 实例化。


Our GRPO-based training pipeline is implemented using the Huggingface TRL library ${}^{1}$ and the GRPOTrainer module from VLM-R1 (Shen et al., 2025). All experiments are conducted on four NVIDIA A800 80GB GPUs, with each training run taking approximately 22 hours. Complete implementation details, including data collection pipeline, training procedure, and model configuration, are provided in Appendix C
我们基于 GRPO 的训练流水线使用 Huggingface TRL 库 ${}^{1}$ 和来自 VLM-R1 (Shen et al., 2025) 的 GRPOTrainer 模块实现。所有实验均在四块 NVIDIA A800 80GB GPU 上进行，每次训练运行耗时约 22 小时。完整的实现细节，包括数据收集流水线、训练程序和模型配置，均在附录 C 中提供。


---



https://github.com/huggingface/trl



https://github.com/om-ai-lab/VLM-R1



---



<table><tr><td colspan="2" rowspan="2"></td><td colspan="2">AitW General</td><td colspan="2">WebShopping</td></tr><tr><td>Train</td><td>Test</td><td>Train</td><td>Test</td></tr><tr><td rowspan="4">Prompt-based</td><td>SoM (GPT-4V)</td><td>5.2</td><td>13.5</td><td>3.1</td><td>8.3</td></tr><tr><td>SoM (Gemini 1.5 Pro)</td><td>32.3</td><td>16.7</td><td>6.3</td><td>11.5</td></tr><tr><td>AppAgent (GPT-4V)</td><td>13.5</td><td>17.7</td><td>12.5</td><td>8.3</td></tr><tr><td>AppAgent (Gemini 1.5 Pro)</td><td>14.6</td><td>16.7</td><td>5.2</td><td>8.3</td></tr><tr><td rowspan="3">Supervised Fine-tuned</td><td>CogAgent</td><td>25.0</td><td>25.0</td><td>31.3</td><td>38.5</td></tr><tr><td>AutoUI</td><td>27.7</td><td>22.9</td><td>20.7</td><td>25.0</td></tr><tr><td>Filtered BC</td><td>51.0±0.9</td><td>54.5±1.3</td><td>37.2 ± 4.7</td><td>43.8±1.7</td></tr><tr><td rowspan="3">Reinforcement Learning</td><td>Digi-RL</td><td>63.5±0.0</td><td>71.9±1.1</td><td>68.2 ± 6.8</td><td>67.2 ± 1.5</td></tr><tr><td>Digi-Q</td><td>61.5±2.3</td><td>71.2±2.</td><td>53.1±1.7</td><td>58.0 ± 2.1</td></tr><tr><td>Hi-Agent (Ours)</td><td>76.4±0.2</td><td>87.9±1.9</td><td>70.3±0.2</td><td>68.8 ± 0.3</td></tr></table>
<table><tbody><tr><td colspan="2" rowspan="2"></td><td colspan="2">AitW 通用</td><td colspan="2">网络购物</td></tr><tr><td>训练集</td><td>测试集</td><td>训练集</td><td>测试集</td></tr><tr><td rowspan="4">基于提示词</td><td>SoM (GPT-4V)</td><td>5.2</td><td>13.5</td><td>3.1</td><td>8.3</td></tr><tr><td>SoM (Gemini 1.5 Pro)</td><td>32.3</td><td>16.7</td><td>6.3</td><td>11.5</td></tr><tr><td>AppAgent (GPT-4V)</td><td>13.5</td><td>17.7</td><td>12.5</td><td>8.3</td></tr><tr><td>AppAgent (Gemini 1.5 Pro)</td><td>14.6</td><td>16.7</td><td>5.2</td><td>8.3</td></tr><tr><td rowspan="3">有监督微调</td><td>CogAgent</td><td>25.0</td><td>25.0</td><td>31.3</td><td>38.5</td></tr><tr><td>AutoUI</td><td>27.7</td><td>22.9</td><td>20.7</td><td>25.0</td></tr><tr><td>过滤式行为克隆</td><td>51.0±0.9</td><td>54.5±1.3</td><td>37.2 ± 4.7</td><td>43.8±1.7</td></tr><tr><td rowspan="3">强化学习</td><td>Digi-RL</td><td>63.5±0.0</td><td>71.9±1.1</td><td>68.2 ± 6.8</td><td>67.2 ± 1.5</td></tr><tr><td>Digi-Q</td><td>61.5±2.3</td><td>71.2±2.</td><td>53.1±1.7</td><td>58.0 ± 2.1</td></tr><tr><td>Hi-Agent (本研究)</td><td>76.4±0.2</td><td>87.9±1.9</td><td>70.3±0.2</td><td>68.8 ± 0.3</td></tr></tbody></table>


Table 1: Main comparisons on AitW benchmark. Success rates (%) on the General and WebShopping subsets. Each RL-based method is run three times; mean and std are reported. Following prior work(Bai et al. 2024, 2025a), evaluation uses the first 96 instructions.
表 1：AitW 基准测试主要对比。General 和 WebShopping 子集的成功率 (%)。每个基于强化学习的方法运行三次；报告均值和标准差。遵循前人工作（Bai et al. 2024, 2025a），评估使用前 96 条指令。


## 5 EXPERIMENTAL EVALUATION
## 5 实验评估


We conduct a comprehensive evaluation of Hi-Agent on mobile device control tasks, focusing on four aspects: (i) task performance against prior baselines on the AitW benchmark (Section 5.1); (ii) adaptabil-generalization to unseen UI layouts and unseen tasks in Screenspot-v2 (Section 5.1); (iii) adaptability to different backbone models and training algorithms (Section 5.2); and (iv) scalability to larger models and more complex tasks on the AndroidWorld benchmark (Section 5.3).
我们对 Hi-Agent 在移动设备控制任务上进行了全面评估，重点关注四个方面：(i) 在 AitW 基准测试上对比先前基线的任务性能（第 5.1 节）；(ii) 在 Screenspot-v2 中对未知 UI 布局和未知任务的泛化能力（第 5.1 节）；(iii) 对不同主干模型和训练算法的适应性（第 5.2 节）；以及 (iv) 在 AndroidWorld 基准测试上对更大模型和更复杂任务的可扩展性（第 5.3 节）。


Environments. Ait $W$ is a large-scale benchmark with five mobile control task categories(Rawles et al. 2023). Following prior work(Bai et al. 2024; 2025a), we evaluate on its two most challenging subsets-General and WebShopping-each consisting of the first 96 tasks. The former focuses on information access and app usage; the latter targets product search across e-commerce platforms.
环境。Ait $W$ 是一个包含五类移动控制任务的大规模基准测试（Rawles et al. 2023）。遵循前人工作（Bai et al. 2024; 2025a），我们在其最具挑战性的两个子集——General 和 WebShopping——上进行评估，每个子集包含前 96 个任务。前者侧重于信息获取和应用使用；后者针对电子商务平台的产品搜索。


Observation and Action Space. To ensure generalization, Hi-Agent operates under a unified observation and action space. Observations consist solely of RGB screenshots, without any structured UI annotations, bounding boxes, or Set-of-Marks (SoM) (Zheng et al., 2024). The action space includes normalized $\left( {x,y}\right)$ taps,long-presses,and swipes; variable-length text entry; functional button presses (e.g., HOME, BACK, ENTER); and task completion signals.
观测与动作空间。为确保泛化性，Hi-Agent 在统一的观测和动作空间下运行。观测仅由 RGB 屏幕截图组成，没有任何结构化 UI 注释、边界框或标记集 (SoM) (Zheng et al., 2024)。动作空间包括归一化的 $\left( {x,y}\right)$ 点击、长按和滑动；变长文本输入；功能按钮按下（例如 HOME、BACK、ENTER）；以及任务完成信号。


Baselines. We compare Hi-Agent against representative agents from three categories: (1) Prompt-based agents, which rely on large closed-source backbones (e.g., GPT-4V (OpenAI, 2024a), Gemini 1.5 Pro (Team et al. 2023)). We include SoM (Zheng et al. 2024) and AppAgent (Zhang et al. 2023) (2) Supervised fine-tuned agents, trained via imitation learning on labeled demonstrations with full parameter updates, including CogAgent (Hong et al. 2024), AutoUI (Zhang & Zhang 2024), and Filtered BC (Pan et al. 2024). (3) Post-trained RL agents, optimized via offline or offline-to-online reinforcement learning. These agents directly update parameters based on task rewards. We include DigiRL (Bai et al. 2024) and DigiQ (Bai et al. 2025a).
基线。我们将 Hi-Agent 与三类代表性代理进行对比：(1) 基于提示的代理，依赖大型闭源主干模型（例如 GPT-4V (OpenAI, 2024a)、Gemini 1.5 Pro (Team et al. 2023)）。我们纳入了 SoM (Zheng et al. 2024) 和 AppAgent (Zhang et al. 2023)；(2) 监督微调代理，通过在标注演示上进行全参数更新的模仿学习训练，包括 CogAgent (Hong et al. 2024)、AutoUI (Zhang & Zhang 2024) 和 Filtered BC (Pan et al. 2024)；(3) 经过后期训练的 RL 代理，通过离线或离线转在线强化学习进行优化。这些代理根据任务奖励直接更新参数。我们纳入了 DigiRL (Bai et al. 2024) 和 DigiQ (Bai et al. 2025a)。


### 5.1 MAIN PERFORMANCE AND GENERALIZATION ANALYSIS
### 5.1 主要性能与泛化分析


Task Performance on AitW. Hi-Agent achieves 87.9% and 68.8% success rates on the General and WebShopping test sets, respectively, establishing a new SOTA. It surpasses the strongest prompt-based agents (APP Agent: 17.7% on General; SoM: 11.5% on WebShopping) by +63.7%, the best supervised method (Filtered BC: 54.5% on General; 43.8% on WebShopping) by +29.2%, and the top RL baseline (DigiRL: 71.9% on General; 67.2% on WebShopping) by +8.8%. These results highlight the benefits of our hierarchical design and foresight-guided optimization. We also identify environment errors in the original WebShopping setup-correcting them further boosts Hi-Agent's success rate to over 90%, as detailed in Appendix E.3
AitW 上的任务性能。Hi-Agent 在 General 和 WebShopping 测试集上分别实现了 87.9% 和 68.8% 的成功率，创下了新的 SOTA。它超过了最强的基于提示的代理（APP Agent：General 17.7%；SoM：WebShopping 11.5%）+63.7%，超过了最佳监督方法（Filtered BC：General 54.5%；WebShopping 43.8%）+29.2%，并超过了顶尖 RL 基线（DigiRL：General 71.9%；WebShopping 67.2%）+8.8%。这些结果突显了我们的层次化设计和前瞻引导优化的优势。我们还识别出原始 WebShopping 设置中的环境错误——纠正这些错误后，Hi-Agent 的成功率进一步提升至 90% 以上，详见附录 E.3。


Analyzing Performance Gains. To explain Hi-Agent's substantial gains over prior RL methods (e.g., +8.8% vs. DigiRL), we examine their failure modes. As shown in Figure 4a, over 70% of both
性能提升分析。为了解释 Hi-Agent 相比先前 RL 方法（如对比 DigiRL 提升 +8.8%）的实质性提升，我们检查了它们的失败模式。如图 4a 所示，超过 70% 的


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_f97430.jpg"/>



Figure 4: Robustness analysis on AitW. (a) Over 70% of General tasks are search-based, causing prior RL methods to overfit. (b) Layout shift from home screen to all-apps view alters app positions. (c) Hi-Agent remains robust $\left( {{87.9}\%  \rightarrow  {83.2}\% }\right)$ ,while DigiRL drops sharply $\left( {{71.9}\%  \rightarrow  {27.6}\% }\right)$ .
图 4：AitW 上的鲁棒性分析。(a) 超过 70% 的 General 任务是基于搜索的，导致先前的 RL 方法过拟合。(b) 从主屏幕到所有应用视图的布局偏移改变了应用位置。(c) Hi-Agent 保持鲁棒 $\left( {{87.9}\%  \rightarrow  {83.2}\% }\right)$，而 DigiRL 大幅下降 $\left( {{71.9}\%  \rightarrow  {27.6}\% }\right)$。


training episodes and test tasks in the General subset are search-based (e.g., "search the weather in Paris"), reflecting a strong distributional skew. End-to-end RL agents tend to overfit to these dominant UI patterns-such as clicking fixed coordinates to launch Chrome and enter queries-while struggling to generalize to rare but structurally distinct tasks (e.g., "open Clock").
General子集中的训练回合和测试任务多基于搜索（如“搜索巴黎天气”），反映了强烈的分布偏移。端到端强化学习智能体倾向于过度拟合这些主导的UI模式（例如点击固定坐标启动Chrome并输入查询），而在泛化到罕见但结构不同的任务（如“打开闹钟”）时表现挣扎。


In contrast,Hi-Agent decouples reasoning and execution: the high-level model ${\pi }_{h}$ generates subgoals (e.g.,"open Chrome"),while the low-level model ${\pi }_{\ell }$ grounds them into UI actions. This abstraction promotes skill reuse and generalization. We visualize representative success and failure cases in Appendix D, and provide a detailed analysis of task-wise performance in Appendix E
相比之下，Hi-Agent将推理与执行解耦：高层模型 ${\pi }_{h}$ 生成子目标（如“打开Chrome”），而底层模型 ${\pi }_{\ell }$ 将其落地为UI操作。这种抽象促进了技能复用与泛化。我们在附录D中展示了代表性的成功与失败案例，并在附录E中提供了详细的任务表现分析。


Robustness and Generalization. We test Hi-Agent's generalization capabilities through two challenging scenarios. First, to assess robustness against UI layout perturbations, we change the agent's starting screen in AitW from the familiar home view to the all-apps view (Figure 4b). While Di-giRL's performance drops sharply from 71.9% to 27.6%, exposing its reliance on memorized coordinates, Hi-Agent remains highly effective, with its success rate only dropping slightly from 87.9% to 83.2% (Figure 4c). The generalization capabilities of our architecture extend to the component level; our low-level action model $\left( {\pi }_{\ell }\right)$ ,when trained on AitW,achieves competitive zero-shot performance on the ScreenSpot-v2 UI grounding benchmark (Wu et al. 2024). We provide detailed performance tables for the zero-shot evaluation in Appendix E.4 and qualitative visualizations of the layout perturbation experiment in Appendix E
稳健性与泛化性。我们通过两个具有挑战性的场景测试Hi-Agent的泛化能力。首先，为了评估对UI布局扰动的稳健性，我们将AitW中智能体的起始屏幕从熟悉的家视图更改为所有应用视图（图4b）。虽然DigiRL的性能从71.9%骤降至27.6%，暴露了其对记忆坐标的依赖，但Hi-Agent依然高度有效，成功率仅从87.9%略微下降至83.2%（图4c）。我们架构的泛化能力延伸到了组件层面；在AitW上训练的底层动作模型 $\left( {\pi }_{\ell }\right)$ 在ScreenSpot-v2 UI落地基准测试中（Wu et al. 2024）实现了具有竞争力的零样本性能。我们在附录E.4提供了零样本评估的详细性能表，并在附录E中提供了布局扰动实验的定性可视化。


### 5.2 COMPONENT ABLATION AND ADAPTATION STUDY
### 5.2 组件消融与适配研究


We conduct ablation and adaptation studies on the AitW benchmark to assess the effectiveness and flexibility of our hierarchical framework.
我们在AitW基准上进行消融和适配研究，以评估我们分层框架的有效性和灵活性。


Ablation on Hierarchical Structure and Post-training. We conduct an ablation study using Qwen2.5VL-3B as the backbone. We compare three configurations: (1) Hi-Agent w/o Hierarchy & Post-train (Qwen-3B (Raw)): the base model without hierarchy or training; (2) Hi-Agent w/o Post-train (Qwen-3B + Hierarchy): a two-level model with hierarchical structure but without post-training; (3) Hi-Agent: our full method with hierarchical decomposition and post-training.
分层结构与后期训练的消融。我们使用Qwen2.5VL-3B作为骨干模型进行消融研究。我们比较了三种配置：(1) Hi-Agent w/o Hierarchy &amp; Post-train (Qwen-3B (Raw))：无层次结构或训练的基础模型；(2) Hi-Agent w/o Post-train (Qwen-3B + Hierarchy)：具有分层结构但未经后期训练的双层模型；(3) Hi-Agent：具有层次分解和后期训练的完整方法。


As shown in Figure 5(a), incorporating hierarchy alone boosts performance from 1.6% to 60.0%, and full post-training further improves it to ${87.9}\%$ ,confirming the complementary benefits of task decomposition and RL-based post training. Appendix E. provides more details.
如图5(a)所示，仅引入分层结构就将性能从1.6%提升至60.0%，而完整的后期训练将其进一步提高到 ${87.9}\%$ ，证实了任务分解和基于强化学习的后期训练的互补效益。附录E提供了更多细节。


Adaptation to Backbone Models. To assess generalization to different base models, we replace Qwen2.5VL with GPT-40 and test it under two configurations: 1) GPT-40 (Raw): the base model used directly without hierarchy; (2) GPT-40 + Hierarchy: augmented with our two-level structure, but without post training. As shown in Figure 5a, even without training, adding hierarchy improves GPT-40's performance from 17.7% to 79.8%, demonstrating the general utility of our design.
骨干模型的适配。为了评估对不同基础模型的泛化能力，我们将Qwen2.5VL替换为GPT-4O，并在两种配置下进行测试：1) GPT-4O (Raw)：直接使用而不加分层结构的基础模型；(2) GPT-4O + Hierarchy：增加了我们的双层结构但未进行后期训练。如图5a所示，即使没有训练，添加分层结构也将GPT-4O的性能从17.7%提升至79.8%，证明了我们设计的通用效用。


Comparison with Supervised Fine-Tuning. We compare our RL-based approach against standard supervised fine-tuning (SFT) on the same hierarchical Qwen-3B architecture and training data, using
与监督微调的比较。我们将基于强化学习的方法与在相同分层Qwen-3B架构和训练数据上进行的标准监督微调（SFT）进行比较，使用


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_160b3b.jpg"/>



Figure 5: Effectiveness and efficiency of Hi-Agent. (a) Task success across model scales and training algorithms, showing consistent gains from hierarchical modeling and post-training. (b) Training curves under different group sizes $G$ ; larger $G$ improves stability and speeds up convergence.
图5：Hi-Agent的有效性和效率。(a) 不同模型规模和训练算法下的任务成功率，显示了分层建模和后期训练带来的持续收益。(b) 不同组大小 $G$ 下的训练曲线；较大的 $G$ 提高了稳定性并加速了收敛。


LLaMA-Factory for the SFT implementation. As shown in Figure 5(a), while SFT achieves a respectable 67.6% success rate, our GRPO-based Hi-Agent reaches a significantly higher 87.9%. This suggests our RL solution is more robust, promoting better generalization where SFT can overfit to demonstration patterns in dynamic GUI environments.
LLaMA-Factory进行SFT实现。如图5(a)所示，虽然SFT实现了不错的67.6%成功率，但我们基于GRPO的Hi-Agent达到了显著更高的87.9%。这表明我们的强化学习方案更具稳健性，在SFT可能过度拟合动态GUI环境演示模式的情况下，促进了更好的泛化。


Impact of Group Size in GRPO. We further investigate the effect of the group size $G$ in our improved GRPO. Figure 5(b) shows training curves under different $G$ values. Larger groups yield more stable learning signals and faster convergence by providing better estimates of relative advantage. This confirms the practical importance of $G$ in balancing efficiency and robustness.
GRPO中组大小的影响。我们进一步研究了改进版GRPO中组大小 $G$ 的影响。图5(b)展示了不同 $G$ 值下的训练曲线。较大的组通过提供更好的相对优势估计，产生更稳定的学习信号和更快的收敛。这证实了 $G$ 在平衡效率和稳健性方面的实际重要性。


### 5.3 SCALABILITY TO LARGER MODELS AND MORE COMPLEX TASKS
### 5.3 对更大模型和更复杂任务的可扩展性


To assess scalability, we evaluate Hi-Agent with larger models on the more challenging Android-World benchmark, which requires stronger reasoning, planning, and fine-grained control than AitW.
为了评估可扩展性，我们在更具挑战性的Android-World基准上使用更大的模型评估Hi-Agent，该基准比AitW需要更强的推理、规划和精细控制能力。


<table><tr><td>Model</td><td>Success Rate</td></tr><tr><td>Qwen2-VL-2B (fine-tuned)</td><td>9.0</td></tr><tr><td>GPT-4 Turbo (Rawles et al. 2024)</td><td>30.6</td></tr><tr><td>GPT-4o (Wang et al. 2024b)</td><td>34.5</td></tr><tr><td>GPT-40 + UGround (Gou et al. 2024)</td><td>44.0</td></tr><tr><td>GPT-40 + Aria-UI (Yang et al. 2024)</td><td>44.8</td></tr><tr><td>UI-TARS (Qin et al. 2025)</td><td>46.6</td></tr><tr><td>Agent S2 (Agashe et al. 2025)</td><td>54.3</td></tr><tr><td>Hi-Agent $\left( {{\mathbf{{3B}}}^{ * } + {\mathbf{{3B}}}^{ * }}\right)$</td><td>26.3</td></tr><tr><td>Hi-Agent $\left( {{\mathbf{{7B}}}^{ * } + {\mathbf{{7B}}}^{ * }}\right)$</td><td>31.9</td></tr><tr><td>Hi-Agent (32B+7B*)</td><td>43.9</td></tr><tr><td>Hi-Agent (72B+7B*)</td><td>56.5</td></tr></table>
<table><tbody><tr><td>模型</td><td>成功率</td></tr><tr><td>Qwen2-VL-2B (微调)</td><td>9.0</td></tr><tr><td>GPT-4 Turbo (Rawles et al. 2024)</td><td>30.6</td></tr><tr><td>GPT-4o (Wang et al. 2024b)</td><td>34.5</td></tr><tr><td>GPT-40 + UGround (Gou et al. 2024)</td><td>44.0</td></tr><tr><td>GPT-40 + Aria-UI (Yang et al. 2024)</td><td>44.8</td></tr><tr><td>UI-TARS (Qin et al. 2025)</td><td>46.6</td></tr><tr><td>Agent S2 (Agashe et al. 2025)</td><td>54.3</td></tr><tr><td>Hi-Agent $\left( {{\mathbf{{3B}}}^{ * } + {\mathbf{{3B}}}^{ * }}\right)$</td><td>26.3</td></tr><tr><td>Hi-Agent $\left( {{\mathbf{{7B}}}^{ * } + {\mathbf{{7B}}}^{ * }}\right)$</td><td>31.9</td></tr><tr><td>Hi-Agent (32B+7B*)</td><td>43.9</td></tr><tr><td>Hi-Agent (72B+7B*)</td><td>56.5</td></tr></tbody></table>


Table 2: AndroidWorld task success rates. *denotes post-trained models.
表 2：AndroidWorld 任务成功率。*表示经过后训练的模型。


We scale both the high-level model ${\pi }_{h}$ and low-level model ${\pi }_{\ell }$ in Hi-Agent. As shown in Table 2, our hierarchical framework scales effectively with model size and consistently improves performance under greater task complexity. In particular, the configuration using a 72B reasoning model and a 7B action model achieves a 56.5% success rate, outperforming the GPT-40 baseline by over 22 absolute points (56.5% vs. 34.5%). A detailed per-task success breakdown and visual illustrations on Android-World are provided in Appendix E.5
我们扩展了 Hi-Agent 中的高层模型 ${\pi }_{h}$ 和底层模型 ${\pi }_{\ell }$。如表 2 所示，我们的分层框架随模型规模有效扩展，并在更高任务复杂度下持续提升性能。特别地，使用 72B 推理模型和 7B 动作模型的配置达到了 56.5% 的成功率，比 GPT-4o 基线高出超过 22 个绝对百分点（56.5% vs. 34.5%）。附录 E.5 提供了 AndroidWorld 上详细的逐任务成功率分析和视觉说明。


These results highlight that our method scales to high-capacity models and complex tasks. By decoupling reasoning and execution, Hi-Agent enables large models to generalize better and solve long-horizon tasks efficiently.
这些结果强调了我们的方法可扩展至高容量模型和复杂任务。通过将推理与执行解耦，Hi-Agent 使大模型能够更好地泛化并高效解决长程任务。


## 6 CONCLUSION
## 6 结论


We propose Hi-Agent, a scalable hierarchical vision-language agent that decouples high-level subgoal reasoning and low-level action execution. By combining structured task decomposition with foresight-guided GRPO optimization, Hi-Agent significantly outperforms prompt-based, supervised, and RL-based baselines in both task success and generalization, while maintaining strong scalability with model size and task complexity.
我们提出了 Hi-Agent，这是一种可扩展的分层视觉-语言智能体，它将高层子目标推理与底层动作执行解耦。通过将结构化任务分解与前瞻引导的 GRPO 优化相结合，Hi-Agent 在任务成功率和泛化能力上显著优于基于提示、监督学习和强化学习的基线，同时保持了对模型规模和任务强度的强大可扩展性。


---



https://github.com/hiyouga/LLaMA-Factory



---



## REFERENCES
## 参考文献


Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: A compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906, 2025.
Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: 一种用于计算机使用智能体的组合式通用-专家框架。arXiv 预印本 arXiv:2504.00906, 2025。


Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37:12461-12495, 2024.
Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: 使用自主强化学习训练真实场景设备控制智能体。计算机神经信息处理系统进展 (NeurIPS), 37:12461-12495, 2024。


Hao Bai, Yifei Zhou, Li Erran Li, Sergey Levine, and Aviral Kumar. Digi-q: Learning q-value functions for training device-control agents. arXiv preprint arXiv:2502.15760, 2025a.
Hao Bai, Yifei Zhou, Li Erran Li, Sergey Levine, and Aviral Kumar. Digi-q: 学习用于训练设备控制智能体的 Q 值函数。arXiv 预印本 arXiv:2502.15760, 2025a。


Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: 一个用于理解、定位、文本读取等功能的多功能视觉-语言模型, 2023。


Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025b.
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL 技术报告。arXiv 预印本 arXiv:2502.13923, 2025b。


Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, et al. Spa-bench: A comprehensive benchmark for smartphone agent evaluation. In NeurIPS 2024 Workshop on Open-World Agents, 2024.
Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, et al. Spa-bench: 一个用于智能手机智能体评估的综合基准。2024 年 NeurIPS 开放世界智能体研讨会, 2024。


Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the Conference on the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4171-4186, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: 用于语言理解的深度双向 Transformer 预训练。北美计算语言学协会会议论文集：人类语言技术, pp. 4171-4186, 2019。


Thomas G Dietterich. Hierarchical reinforcement learning with the MAXQ value function decomposition. Journal of Artificial Intelligence Research, 13:227-303, 2000.
Thomas G Dietterich. 基于 MAXQ 值函数分解的分层强化学习。人工智能研究杂志, 13:227-303, 2000。


Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067-1074, 2022.
Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. 通过结合语言模型与战略推理在外交官游戏中达到人类水平。Science, 378(6624):1067-1074, 2022。


Mohammad Ghavamzadeh and Sridhar Mahadevan. Hierarchical average reward reinforcement learning. Journal of Machine Learning Research, 8(11), 2007.
Mohammad Ghavamzadeh and Sridhar Mahadevan. 分层平均奖励强化学习。机器学习研究杂志, 8(11), 2007。


Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for GUI agents. arXiv preprint arXiv:2410.05243, 2024.
Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. 像人类一样探索数字世界：GUI 智能体的通用视觉定位。arXiv 预印本 arXiv:2410.05243, 2024。


Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, et al. Ui-venus technical report: Building high-performance ui agents with rft. arXiv preprint arXiv:2508.10833, 2025.
Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, et al. Ui-venus 技术报告：利用拒绝采样微调（RFT）构建高性能 UI 智能体。arXiv 预印本 arXiv:2508.10833, 2025。


Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1：通过强化学习激发大语言模型的推理能力。arXiv 预印本 arXiv:2501.12948, 2025。


Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. CogAgent: A visual language model for GUI agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14281-14290, 2024.
Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. CogAgent：一种用于 GUI 智能体的视觉语言模型。收录于 IEEE/CVF 计算机视觉与模式识别会议论文集（CVPR），第 14281-14290 页，2024。


Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al. Personal LLM agents: Insights and survey about the capability, efficiency and security. arXiv preprint arXiv:2401.05459, 2024.
Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al. 个人大模型智能体：关于能力、效率与安全性的见解与综述。arXiv 预印本 arXiv:2401.05459, 2024。


Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36:34892-34916, 2023.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 视觉指令微调。神经信息处理系统大会（NeurIPS）进展，36:34892-34916, 2023。


Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. VilBert: Pretraining task-agnostic visiolin-guistic representations for vision-and-language tasks. Advances in Neural Information Processing Systems, 32, 2019.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT：为视觉语言任务预训练任务无关的视觉语言表示。神经信息处理系统大会（NeurIPS）进展，32, 2019。


OpenAI. Gpt-4v(ision) technical work and authors, 2024a. https://openai.com/ contributions/gpt-4v/
OpenAI. GPT-4V(ision) 技术工作与作者，2024a。https://openai.com/contributions/gpt-4v/


OpenAI. Learning to reason with LLMs, 2024b. https://openai.com/index/ learning-to-reason-with-llms/
OpenAI. 学习用大模型进行推理，2024b。https://openai.com/index/learning-to-reason-with-llms/


Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, pp. 27730-27744, 2022.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 训练语言模型以遵循人类反馈指令。神经信息处理系统大会（NeurIPS）进展，第 27730-27744 页，2022。


Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024.
Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. 数字智能体的自主评估与完善。arXiv 预印本 arXiv:2404.06474, 2024。


Shubham Pateria, Budhitama Subagdja, Ah-hwee Tan, and Chai Quek. Hierarchical reinforcement learning: A comprehensive survey. ACM Computing Surveys, 54(5):1-35, 2021.
Shubham Pateria, Budhitama Subagdja, Ah-hwee Tan, and Chai Quek. 分层强化学习：综合综述。ACM 计算综述（ACM Computing Surveys），54(5):1-35, 2021。


Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. ChatDev: Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.
Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. ChatDev：面向软件开发的交流型智能体。arXiv 预印本 arXiv:2307.07924, 2023。


Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025.
Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. UI-TARS：利用原生智能体开拓自动化 GUI 交互。arXiv 预印本 arXiv:2501.12326, 2025。


Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 通过生成式预训练提升语言理解能力。2018。


Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763, 2021.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, 等. 从自然语言监督中学习可迁移的视觉模型. 发表于国际机器学习大会 (ICML), 第 8748-8763 页, 2021.


Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 使用统一的文本到文本 Transformer 探索迁移学习的极限. 机器学习研究杂志 (JMLR), 21(140):1-67, 2020.


Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. An-droidInTheWild: A large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:59708-59728, 2023.
Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, 和 Timothy Lillicrap. AndroidInTheWild: 一个用于安卓设备控制的大规模数据集. 神经信息处理系统进展 (NeurIPS), 36:59708-59728, 2023.


Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. AndroidWorld: A dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024.
Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, 等. AndroidWorld: 一个用于自主智能体的动态基准测试环境. arXiv 预印本 arXiv:2405.14573, 2024.


Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. DeepseekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, 等. DeepseekMath: 在开源语言模型中挑战数学推理的极限. arXiv 预印本 arXiv:2402.03300, 2024.


Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: A stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025.
Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, 等. Vlm-r1: 一个稳定且可泛化的 r1 风格大型视觉语言模型. arXiv 预印本 arXiv:2504.07615, 2025.


Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, 等. Gemini: 一个高性能多模态模型家族. arXiv 预印本 arXiv:2312.11805, 2023.


Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, 等. Llama: 开源且高效的基础语言模型. arXiv 预印本 arXiv:2302.13971, 2023.


Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. In ICLR 2024 Workshop on Large Language Model (LLM) Agents.
Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, 和 Jitao Sang. Mobile-agent: 具有视觉感知能力的自主多模态移动设备智能体. 发表于 ICLR 2024 大语言模型 (LLM) 智能体研讨会.


Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-Agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. Advances in Neural Information Processing Systems, pp. 2686-2710, 2025.
Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, 和 Jitao Sang. Mobile-Agent-v2: 通过多智能体协作实现高效导航的移动设备操作系统助手. 神经信息处理系统进展 (NeurIPS), 第 2686-2710 页, 2025.


Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-VL: Enhancing vision-language model's perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a.
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, 等. Qwen2-VL: 提升视觉语言模型在任何分辨率下对世界的感知能力. arXiv 预印本 arXiv:2409.12191, 2024a.


Yiqin Wang, Haoji Zhang, Jingqi Tian, and Yansong Tang. Ponder & press: Advancing visual GUI agent towards general computer control. arXiv preprint arXiv:2412.01268, 2024b.
Yiqin Wang, Haoji Zhang, Jingqi Tian, 和 Yansong Tang. Ponder & press: 推动视觉 GUI 智能体迈向通用计算机控制. arXiv 预印本 arXiv:2412.01268, 2024b.


Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: A foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024.
Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, 等. Os-atlas: 一个用于通用 GUI 智能体的基础动作模型. arXiv 预印本 arXiv:2410.23218, 2024.


Hui Yang, Sifu Yue, and Yunzhong He. Auto-GPT for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224, 2023.
Hui Yang, Sifu Yue, and Yunzhong He. Auto-GPT for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224, 2023.


Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-UI: Visual grounding for GUI instructions. arXiv preprint arXiv:2412.16256, 2024.
Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-UI: Visual grounding for GUI instructions. arXiv preprint arXiv:2412.16256, 2024.


Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. AppAgent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023.
Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. AppAgent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023.


Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. In Findings of the Association for Computational Linguistics, pp. 3132-3149, 2024.
Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. In Findings of the Association for Computational Linguistics, pp. 3132-3149, 2024.


Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is a generalist web agent, if grounded. In International Conference on Machine Learning, pp. 61349-61385. PMLR, 2024.
Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is a generalist web agent, if grounded. In International Conference on Machine Learning, pp. 61349-61385. PMLR, 2024.


Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595-46623, 2023.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595-46623, 2023.


## APPENDIX
## 附录


Appendix Overview. The supplementary material provides additional theoretical analysis, implementation details, and extended experimental results to support the main paper.
附录概览。补充材料提供了额外的理论分析、实现细节以及扩展的实验结果，用以支持正文内容。


- Section A presents a formal analysis of global optimality under recursive decomposition in our hierarchical framework.
- A节正式分析了我们分层框架中递归分解下的全局最优性。


- Section B describes our data construction protocol and provides a detailed analysis of the train-test overlap to ensure fair evaluation.
- B节阐述了我们的数据构建方案，并详细分析了训练集与测试集的重叠情况，以确保评估的公正性。


- Section C details the training procedure, including data generation, model configurations, and hyperparameter settings.
- C节详细说明了训练流程，包括数据生成、模型配置以及超参数设置。


- Section D provides qualitative case studies, including both successful and failure examples to illustrate model behavior.
- D节提供了定性案例研究，包括成功与失败的示例，用以阐释模型行为。


- Section E includes extended experiments: evaluation under UI layout perturbation, a zero-shot generalization test on the ScreenSpot-v2 benchmark, analysis of the WebShopping subset, and additional statistics on the AndroidWorld benchmark.
- E节包含扩展实验：UI布局扰动下的评估、ScreenSpot-v2基准测试集的零样本泛化测试、WebShopping子集的分析，以及AndroidWorld基准测试集的额外统计数据。


## A GLOBAL OPTIMALITY VIA RECURSIVE CONSTRUCTION
## A 通过递归构建实现全局最优性


Here, we formally establish the conditions under which a recursively optimal hierarchical policy $\left( {{\pi }_{h},{\pi }_{\ell }}\right)$ achieves global optimality. Following the notation and recursive decomposition structure defined in Section 4.1. we consider an MDP $M$ hierarchically decomposed into subtasks $\left\{  {{M}_{0},{M}_{1},\ldots ,{M}_{m}}\right\}$ ,with ${M}_{0}$ representing the root task.
在此，我们正式确立递归最优分层策略 $\left( {{\pi }_{h},{\pi }_{\ell }}\right)$ 达成全局最优性的条件。遵循第4.1节定义的符号和递归分解结构，我们考虑一个分层分解为子任务 $\left\{  {{M}_{0},{M}_{1},\ldots ,{M}_{m}}\right\}$ 的MDP $M$ ，其中 ${M}_{0}$ 代表根任务。


Proposition 1 (Global Optimality Condition). Let ${\pi }^{ * }$ denote the optimal flat policy for MDP $M$ . Assume this optimal sequence can be partitioned into a sequence of valid subtasks under the hierarchical decomposition. Then,a recursively optimal hierarchical policy $\pi  = \left( {{\pi }_{h},{\pi }_{\ell }}\right)$ is also a globally optimal policy,i.e., ${V}^{\pi }\left( {s}_{0}\right)  = {V}^{{\pi }^{ * }}\left( {s}_{0}\right)$ .
命题1（全局最优性条件）。令 ${\pi }^{ * }$ 表示MDP $M$ 的最优平铺策略。假设该最优序列可以在分层分解下划分为一系列有效的子任务。那么，递归最优分层策略 $\pi  = \left( {{\pi }_{h},{\pi }_{\ell }}\right)$ 也是全局最优策略，即 ${V}^{\pi }\left( {s}_{0}\right)  = {V}^{{\pi }^{ * }}\left( {s}_{0}\right)$ 。


Proof. We prove by contradiction. Assume that the recursively optimal policy $\pi$ is not globally optimal. This implies there exists another hierarchical policy $\widetilde{\pi }$ such that for some starting state ${s}_{0}$ , its value is strictly greater: ${V}^{\widetilde{\pi }}\left( {s}_{0}\right)  > {V}^{\pi }\left( {s}_{0}\right)$ .
证明。我们采用反证法。假设递归最优策略 $\pi$ 不是全局最优的。这意味着存在另一个分层策略 $\widetilde{\pi }$ ，使得对于某些初始状态 ${s}_{0}$ ，其值严格更大： ${V}^{\widetilde{\pi }}\left( {s}_{0}\right)  > {V}^{\pi }\left( {s}_{0}\right)$ 。


Let us identify the first decision point $\left( {{s}_{k},{M}_{k}}\right)$ where the policies diverge. At this state, $\pi$ chooses subgoal ${g}_{k}$ while $\widetilde{\pi }$ chooses a different subgoal ${g}_{k}^{\prime }$ . Since this is the first point of divergence,the value obtained by following $\widetilde{\pi }$ from this state onward must be strictly greater than that from following $\pi$ .
让我们确定策略分歧的第一个决策点 $\left( {{s}_{k},{M}_{k}}\right)$ 。在该状态下， $\pi$ 选择子目标 ${g}_{k}$ ，而 $\widetilde{\pi }$ 选择不同的子目标 ${g}_{k}^{\prime }$ 。由于这是第一个分歧点，从此状态开始遵循 $\widetilde{\pi }$ 获得的值必须严格大于遵循 $\pi$ 获得的值。


However,a recursively optimal policy $\pi$ ,by definition,selects the subgoal that maximizes the expected future return. This return is captured by the hierarchical Q-value:
然而，根据定义，递归最优策略 $\pi$ 会选择使期望未来回报最大化的子目标。该回报由分层 Q 值捕获：


$$
{Q}_{k}^{\pi }\left( {{s}_{k},g}\right)  = {V}_{g}^{\pi }\left( {s}_{k}\right)  + {C}_{k}^{\pi }\left( {{s}_{k},g}\right) .
$$



The completion function ${C}_{k}^{\pi }\left( {{s}_{k},g}\right)$ correctly accounts for stochastic termination by averaging over the distribution of all possible exit states and durations, as defined in Eq. (2).
如等式 (2) 所定义，完成函数 ${C}_{k}^{\pi }\left( {{s}_{k},g}\right)$ 通过对所有可能的退出状态和持续时间的分布进行平均，正确地考虑了随机终止。


The choice made by the recursively optimal policy $\pi$ at state ${s}_{k}$ is therefore:
因此，递归最优策略 $\pi$ 在状态 ${s}_{k}$ 下做出的选择为：


$$
{g}_{k} = \arg \mathop{\max }\limits_{g}{Q}_{k}^{\pi }\left( {{s}_{k},g}\right) .
$$



A direct consequence of this maximization is that for any alternative subgoal ${g}_{k}^{\prime }$ ,the following inequality must hold:
这种最大化的直接结果是，对于任何替代子目标 ${g}_{k}^{\prime }$ ，以下不等式必须成立：


$$
{Q}_{k}^{\pi }\left( {{s}_{k},{g}_{k}}\right)  \geq  {Q}_{k}^{\pi }\left( {{s}_{k},{g}_{k}^{\prime }}\right) .
$$



This implies that switching the choice from ${g}_{k}$ to ${g}_{k}^{\prime }$ cannot increase the expected value from state ${s}_{k}$ onward. This contradicts our earlier deduction that the value of policy $\widetilde{\pi }$ (which chose ${g}_{k}^{\prime }$ ) must be strictly greater.
这意味着将选择从 ${g}_{k}$ 切换到 ${g}_{k}^{\prime }$ 不会增加从状态 ${s}_{k}$ 开始的期望值。这与我们之前的推论相矛盾，即策略 $\widetilde{\pi }$ （选择了 ${g}_{k}^{\prime }$ ）的值必须严格更大。


Therefore,our initial assumption that $\pi$ is not globally optimal must be false. Hence,a recursively optimal hierarchical policy is globally optimal.
因此，我们最初关于 $\pi$ 不是全局最优的假设必然是错误的。故而，递归最优分层策略是全局最优的。


Expressivity. Our two-level hierarchical framework consists of a high-level reasoning policy ${\pi }_{h}$ generating semantic subgoals ${g}_{t}$ ,and a low-level policy ${\pi }_{\ell }$ executing these subgoals via primitive actions ${a}_{t}$ . Given that ${\pi }_{h}$ can directly emit atomic actions as subgoals,and ${\pi }_{\ell }$ is capable of executing them,the joint policy space $\left( {{\pi }_{h},{\pi }_{\ell }}\right)$ fully encompasses the space of flat policies. Therefore,recursively optimal hierarchical policies retain the expressivity necessary for achieving global optimality.
表达能力。我们的两层分层框架由生成语义子目标 ${g}_{t}$ 的高层推理策略 ${\pi }_{h}$ ，以及通过原子动作 ${a}_{t}$ 执行这些子目标的底层策略 ${\pi }_{\ell }$ 组成。鉴于 ${\pi }_{h}$ 可以直接发出原子动作作为子目标，且 ${\pi }_{\ell }$ 能够执行它们，联合策略空间 $\left( {{\pi }_{h},{\pi }_{\ell }}\right)$ 完全涵盖了扁平策略空间。因此，递归最优分层策略保留了实现全局最优所需的表达能力。


Foresight Advantage. To further align local subgoal optimization with global task success, we introduce a foresight advantage:
预见优势。为了进一步使局部子目标优化与全局任务成功保持一致，我们引入了预见优势：


$$
{\widehat{A}}_{t}^{\left( h\right) } = \frac{{r}_{t}^{\left( h\right) } - {\mu }_{r}}{{\sigma }_{r}},\;\text{ where }\;{r}_{t}^{\left( h\right) } = {\lambda }_{1}{r}_{\mathrm{{fmt}}}\left( {g}_{t}\right)  + {\lambda }_{2}{r}_{\mathrm{{env}}}\left( {{s}_{t},{g}_{t},{\pi }_{\ell }}\right)  + {\lambda }_{3}{\widehat{V}}_{\text{ judge }}\left( {{s}_{t},{g}_{t}}\right) .
$$



Here, ${r}_{\text{ fmt }}$ reflects syntactic and semantic subgoal correctness, ${r}_{\text{ env }}$ evaluates execution feedback from the environment,and ${\widehat{V}}_{\text{ judge }}$ estimates long-term subgoal feasibility via a pretrained VLM oracle. This reward shaping mechanism mitigates the risk of locally greedy yet globally suboptimal subgoal selection,guiding ${\pi }_{h}$ to reason with foresight and converge toward globally optimal task strategies.
此处， ${r}_{\text{ fmt }}$ 反映了子目标的句法和语义正确性， ${r}_{\text{ env }}$ 评估来自环境的执行反馈， ${\widehat{V}}_{\text{ judge }}$ 通过预训练的 VLM 预言机估计长期子目标的可行性。这种奖励塑形机制降低了局部贪婪但全局次优的子目标选择风险，引导 ${\pi }_{h}$ 具备预见性地推理并收敛于全局最优的任务策略。


## B DATA CONSTRUCTION AND OVERLAP ANALYSIS
## B 数据构建与重叠分析


We carefully avoided data leakage between training and evaluation. Below, we clarify the data preparation and task partitioning across both AitW and AndroidWorld benchmarks.
我们仔细避免了训练与评估之间的数据泄漏。下面，我们阐明了 AitW 和 AndroidWorld 基准测试中的数据准备和任务划分。


AitW (General & WebShopping) We selected the first 96 instruction texts from the official splits-matching baseline evaluation setups-and generated new trajectories using our automatic data collection pipeline (Section 4.3). We did not use any raw trajectories from the original dataset; only instruction texts were reused. All collected trajectories were manually verified for correctness.
AitW (General &amp; WebShopping) 我们从官方划分中选择了前 96 条指令文本（与基线评估设置相匹配），并使用我们的自动数据采集流水线（第 4.3 节）生成了新的轨迹。我们没有使用原始数据集中的任何原始轨迹；仅复用了指令文本。所有采集的轨迹均经过人工核实以确保正确性。


AndroidWorld This benchmark uses parameterized task templates such as "Create a new contact for \{name\} with number \{number\}". Each task instance is dynamically generated with randomized parameters. We ensured that training and evaluation used different random seeds to avoid any template-level overlap.
AndroidWorld 该基准测试使用参数化任务模板，例如“为 \{name\} 创建号码为 \{number\} 的新联系人”。每个任务实例都是使用随机参数动态生成的。我们确保训练和评估使用不同的随机种子，以避免任何模板层面的重叠。


Task Overlap Quantification We provide a detailed quantification of task overlap between training and evaluation sets in Table 3 The minor overlap in AitW stems from a small number of tasks that appear in both the original train and test splits provided by the benchmark creators, for which we used the instruction texts. Our methodology ensures no trajectory-level overlap.
任务重叠量化 我们在表 3 中提供了训练集和评估集之间任务重叠的详细量化。AitW 中的少量重叠源于基准创建者提供的原始训练和测试划分中都出现了少量任务，我们使用了这些任务的指令文本。我们的方法论确保了不存在轨迹层面的重叠。


Table 3: Task overlap analysis between training data generation and evaluation sets.
表 3：训练数据生成与评估集之间的任务重叠分析。


<table><tr><td>Benchmark</td><td>#Tasks Used in Training</td><td>#Tasks in Test</td><td>Overlap Ratio</td></tr><tr><td>AitW-General</td><td>96</td><td>96</td><td>6.25%</td></tr><tr><td>AitW-WebShopping</td><td>96</td><td>96</td><td>5.21%</td></tr><tr><td>AndroidWorld</td><td>116</td><td>116</td><td>0%</td></tr></table>
<table><tbody><tr><td>基准测试</td><td>训练所用任务数</td><td>测试任务数</td><td>重合率</td></tr><tr><td>AitW-General</td><td>96</td><td>96</td><td>6.25%</td></tr><tr><td>AitW-WebShopping</td><td>96</td><td>96</td><td>5.21%</td></tr><tr><td>AndroidWorld</td><td>116</td><td>116</td><td>0%</td></tr></tbody></table>


Dataset Scale We provide detailed statistics of our training data across all benchmarks in Table 4 For AitW, we selected 96 task instructions from the training splits of both General and WebShopping, consistent with prior work. We re-executed each using our Oracle agent, collecting new trajectories. After manual verification and filtering, this resulted in 205 verified samples for AitW-General and 389 for AitW-WebShopping. For AndroidWorld, which defines 116 parameterized task templates, we instantiated one randomized goal per template and collected training samples via the Oracle policy. We retained 682 high-quality samples after manual validation.
数据集规模 我们在表 4 中提供了所有基准测试训练数据的详细统计。对于 AitW，我们从 General 和 WebShopping 的训练拆分中选择了 96 条任务指令，这与之前的工作一致。我们使用 Oracle 代理重新执行了每条指令并收集了新轨迹。经过人工验证和筛选，最终在 AitW-General 中获得了 205 个验证样本，在 AitW-WebShopping 中获得了 389 个。对于定义了 116 个参数化任务模板的 AndroidWorld，我们为每个模板实例化了一个随机目标，并通过 Oracle 策略收集了训练样本。经人工验证后，我们保留了 682 个高质量样本。


Table 4: Training data statistics across all benchmarks.
表 4：所有基准测试的训练数据统计。


<table><tr><td>Benchmark</td><td>Subset</td><td>#Task Instructions</td><td>#Verified Samples</td></tr><tr><td>AitW</td><td>General</td><td>96</td><td>205</td></tr><tr><td>AitW</td><td>WebShopping</td><td>96</td><td>389</td></tr><tr><td>AndroidWorld</td><td>Full</td><td>116</td><td>682</td></tr></table>
<table><tbody><tr><td>基准测试</td><td>子集</td><td>#任务指令</td><td>#已验证样本</td></tr><tr><td>AitW</td><td>通用</td><td>96</td><td>205</td></tr><tr><td>AitW</td><td>网络购物</td><td>96</td><td>389</td></tr><tr><td>AndroidWorld</td><td>全量</td><td>116</td><td>682</td></tr></tbody></table>


## C DETAILED TRAINING PIPELINE
## C 详细训练流程


Our hierarchical training framework decomposes long-horizon mobile tasks into single-step subgoals, enabling efficient optimization using GRPO. Here, we elaborate on the full training pipeline, emphasizing the mechanisms used to generate training signals and their integration within GRPO.
我们的分层训练框架将长程移动任务分解为单步子目标，从而利用 GRPO 实现高效优化。在此，我们详细阐述完整的训练流程，重点介绍生成训练信号的机制及其在 GRPO 中的集成方式。


### C.1 REWARD DATASET GENERATION
### C.1 奖励数据集生成


Due to limitations of the Android emulator regarding state rollback, obtaining rewards by sequentially interacting with the environment becomes computationally expensive. Therefore, we design an oracle model based on our hierarchical architecture, consisting of a Qwen2.5-VL-72B reasoning model paired with a Qwen2.5-VL-7B action model, to automatically generate accurate reward datasets without manual labeling. To ensure high-quality data generation, we carefully crafted prompts for the 72B reasoning model, guiding it to generate reliable subgoal instructions conditioned on task descriptions, previous actions, and current screen states.
由于 Android 模拟器在状态回滚方面的限制，通过与环境进行顺序交互来获取奖励的计算成本极高。因此，我们基于分层架构设计了一个 Oracle 模型，由 Qwen2.5-VL-72B 推理模型与 Qwen2.5-VL-7B 动作模型组成，旨在无需人工标注即可自动生成准确的奖励数据集。为确保生成高质量数据，我们为 72B 推理模型精心设计了提示词，引导其根据任务描述、历史动作和当前屏幕状态生成可靠的子目标指令。


For clarity, we present the exact prompt structure used by the 72B reasoning model below:
为清晰起见，我们在下方展示了 72B 推理模型所使用的准确提示词结构：


---



You are a mobile operation Agent that performs precise screen interactions. Analyze the
你是一个进行精确屏幕交互的移动端操作智能体。请分析


input and generate the next action instruction.
输入并生成下一步动作指令。


#Task Description
#任务描述


Execute multi-step mobile tasks through sequential single-step decisions.
通过连续的单步决策执行多步移动任务。


#Input Components
#输入组件


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\{ "image": "Screen image (analyze UI elements)",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\{ "image": "屏幕图像 (分析 UI 元素)",


&nbsp;&nbsp;&nbsp;&nbsp;"text/Previous Actions": ["action_type": "...", "touch_point": "[x,y]", ...],
&nbsp;&nbsp;&nbsp;&nbsp;"text/历史动作": ["action_type": "...", "touch_point": "[x,y]", ...],


&nbsp;&nbsp;&nbsp;&nbsp;"text/Goal": "Current task objective" \}
&nbsp;&nbsp;&nbsp;&nbsp;"text/目标": "当前任务目标" \}


#Action Output Components
#动作输出组件


You should only output concise and clear action instructions, including action types and
你应仅输出简明扼要的动作指令，包括动作类型与


action targets, without specific coordinates.
动作目标，不含具体坐标。


#Output Format (strictly follow):
#输出格式（严格遵守）：


&nbsp;&nbsp;&nbsp;&nbsp;<reasoning>



&nbsp;&nbsp;&nbsp;&nbsp;1. Analyze previous action sequence
&nbsp;&nbsp;&nbsp;&nbsp;1. 分析之前的动作序列


&nbsp;&nbsp;&nbsp;&nbsp;2. Identify important elements in current screen
&nbsp;&nbsp;&nbsp;&nbsp;2. 识别当前屏幕中的重要元素


3. Determine required next-step action instruction
3. 确定所需的下一步动作指令


&nbsp;&nbsp;&nbsp;&nbsp;</reasoning>



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<instruction>



&nbsp;&nbsp;&nbsp;&nbsp;Instruction: ...
&nbsp;&nbsp;&nbsp;&nbsp;指令：...


&nbsp;&nbsp;&nbsp;&nbsp;</instruction>



&nbsp;&nbsp;&nbsp;&nbsp;#Examples
&nbsp;&nbsp;&nbsp;&nbsp;#示例


&nbsp;&nbsp;&nbsp;&nbsp;example1:
&nbsp;&nbsp;&nbsp;&nbsp;示例1：


&nbsp;&nbsp;&nbsp;&nbsp;Input: "Previous Actions: xx
&nbsp;&nbsp;&nbsp;&nbsp;输入：“历史动作：xx


Output: <reasoning> xx </reasoning> <instruction> xx </instruction>
输出：<reasoning> xx </reasoning> <instruction> xx </instruction>


example x: xxx
示例 x: xxx


---



We empirically verify the oracle model's effectiveness on the first 96 tasks from the AitW benchmark. The hierarchical oracle, powered by the structured prompt and dual-model architecture, achieves a task success rate of 93.2%. This demonstrates that our oracle can reliably serve as an automated annotator for large-scale subgoal-action data collection, enabling scalable and accurate training.
我们通过 AitW 基准测试的前 96 个任务实证验证了 Oracle 模型的有效性。在结构化提示词和双模型架构的支持下，分层 Oracle 实现了 93.2% 的任务成功率。这表明我们的 Oracle 可以可靠地作为自动标注器，用于大规模“子目标-动作”数据的收集，从而实现可扩展且准确的训练。


An example of the collected reward dataset is presented below, structured clearly in JSON format for consistency and ease of use:
下方展示了所收集的奖励数据集示例，采用清晰的 JSON 格式以确保一致性并易于使用：


---



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\{



&nbsp;&nbsp;&nbsp;&nbsp;"image_path": "android/save/images/test3/1743001445.7178237_1.png",
&nbsp;&nbsp;&nbsp;&nbsp;"image_path": "android/save/images/test3/1743001445.7178237_1.png",


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"problem": "Search for hotels in Washington DC",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"problem": "搜索华盛顿特区的酒店",


"instruction": "Click on the Chrome icon to open the browser.",
"instruction": "点击 Chrome 图标以打开浏览器。",


"solution": "Action Decision: \{action_type: DUAL_POINT, touch_point: [0.7781, 0.6972],
"solution": "动作决策：\{action_type: DUAL_POINT, touch_point: [0.7781, 0.6972],


&nbsp;&nbsp;&nbsp;&nbsp;lift_point: [0.7781, 0.6972], typed_text: ""
&nbsp;&nbsp;&nbsp;&nbsp;lift_point: [0.7781, 0.6972], typed_text: ""


\}



---



During training, the "solution" field serves as a reference signal for both the low-level action model and the high-level reasoning model: the former receives direct execution supervision, while the latter is optimized via foresight rewards that incorporate oracle feedback on the executability and quality of predicted subgoals.
在训练期间，“solution”字段作为低层动作模型和高层推理模型的参考信号：前者接收直接的执行监督，而后者通过前瞻奖励进行优化，该奖励结合了关于预测子目标的可执行性和质量的专家反馈。


### C.2 GRPO TRAINING PROCEDURE
### C.2 GRPO 训练流程


We leverage the constructed reward dataset to post-train both components of our hierarchical policy using a modified GRPO framework. To train the high-level reasoning model ${\pi }_{h}$ ,we compute a foresight reward signal by aggregating three components: format reward, execution feedback reward, and subgoal feasibility reward. Each is described below.
我们利用构建的奖励数据集，使用改进的 GRPO 框架对分层策略的两个组件进行后期训练。为了训练高层推理模型 ${\pi }_{h}$，我们通过聚合三个部分来计算前瞻奖励信号：格式奖励、执行反馈奖励和子目标可行性奖励。各部分说明如下。


Format Reward. To ensure subgoals generated by the reasoning model conform to a syntactically valid and semantically interpretable structure,we define a binary format reward ${r}_{\mathrm{{fmt}}}\left( {g}_{t}\right)$ based on regular expression matching. Only subgoals matching the following format receive positive reward:
格式奖励。为了确保推理模型生成的子目标符合语法有效且语义可解释的结构，我们定义了基于正则表达式匹配的二值化格式奖励 ${r}_{\mathrm{{fmt}}}\left( {g}_{t}\right)$。只有符合以下格式的子目标才能获得正向奖励：


---



<reasoning> ... </reasoning>
<reasoning> ... </reasoning>


&nbsp;&nbsp;&nbsp;&nbsp;<instruction>Instruction: ...</instruction>
&nbsp;&nbsp;&nbsp;&nbsp;<instruction>Instruction: ...</instruction>


---



This pattern ensures that each subgoal contains both a reasoning trace and a structured instruction. Subgoals that omit either tag or violate the structural layout are penalized with zero format reward.
该模式确保每个子目标都包含推理轨迹和结构化指令。缺少任一标签或违反结构布局的子目标将被处以零格式奖励。


Execution Feedback Reward. To supervise the low-level action model ${\pi }_{\ell }$ ,we compare its predicted action ${a}_{t} = {\pi }_{\ell }\left( {{s}_{t},{g}_{t}}\right)$ against the oracle action ${\widehat{a}}_{t}$ in the dataset. The reward ${r}_{\text{ env }}\left( {{s}_{t},{g}_{t},{a}_{t}}\right)$ is defined as:
执行反馈奖励。为了监督低层动作模型 ${\pi }_{\ell }$，我们将预测动作 ${a}_{t} = {\pi }_{\ell }\left( {{s}_{t},{g}_{t}}\right)$ 与数据集中的专家动作 ${\widehat{a}}_{t}$ 进行比较。奖励 ${r}_{\text{ env }}\left( {{s}_{t},{g}_{t},{a}_{t}}\right)$ 定义为：


$$
{r}_{\text{ env }}\left( {{s}_{t},{g}_{t},{a}_{t}}\right)  = \mathbb{1}\left\{  {\operatorname{type}\left( {a}_{t}\right)  = \operatorname{type}\left( {\widehat{a}}_{t}\right)  \land  {\begin{Vmatrix}\operatorname{coord}\left( {a}_{t}\right)  - \operatorname{coord}\left( {\widehat{a}}_{t}\right) \end{Vmatrix}}_{2} < \epsilon }\right\}  ,
$$



where $\epsilon$ is a threshold for coordinate similarity (set to 0.002 in our experiments). In the formula,type $\left( {a}_{t}\right)$ and type $\left( {\widehat{a}}_{t}\right)$ respectively denote the action types of ${a}_{t}$ and ${\widehat{a}}_{t}$ ,while coord $\left( {a}_{t}\right)$ and $\operatorname{coord}\left( {\widehat{a}}_{t}\right)$ correspond to the coordinates of the actions ${a}_{t}$ and ${\widehat{a}}_{t}$ . This reward is also propagated to the high-level model to encourage generation of executable subgoals.
其中 $\epsilon$ 是坐标相似度阈值（在我们的实验中设置为 0.002）。公式中，type $\left( {a}_{t}\right)$ 和 type $\left( {\widehat{a}}_{t}\right)$ 分别表示 ${a}_{t}$ 和 ${\widehat{a}}_{t}$ 的动作类型，而 coord $\left( {a}_{t}\right)$ 和 $\operatorname{coord}\left( {\widehat{a}}_{t}\right)$ 对应于动作 ${a}_{t}$ 和 ${\widehat{a}}_{t}$ 的坐标。该奖励也会传播到高层模型，以鼓励生成可执行的子目标。


Subgoal Feasibility Reward. To measure whether the predicted subgoal ${g}_{t}$ is appropriate and feasible under the current screen context, we employ a frozen Qwen2.5-VL-72B model as a subgoal evaluator. The evaluation prompt is carefully designed to enforce atomicity, context validity, keyboard preconditions, and target presence. Only when all criteria are satisfied is a reward of 1 returned; otherwise, the reward is 0 . The exact prompt used is as follows:
子目标可行性奖励。为了衡量预测的子目标 ${g}_{t}$ 在当前屏幕上下文中是否合适且可行，我们使用冻结的 Qwen2.5-VL-72B 模型作为子目标评估器。评估提示词经过精心设计，以强制执行原子性、上下文有效性、键盘前提条件和目标存在性。只有当所有标准都满足时，才返回奖励 1；否则，奖励为 0。所使用的具体提示词如下：


---



You are a mobile operation instruction validator. Strictly evaluate if the generated
你是一名移动端操作指令验证员。请严格评估生成的指令对当前步骤是否有效。


instruction is valid for the current step.
指令对当前步骤是否有效。


#Evaluation Criteria (ALL must be met for reward=1):
#评估标准（必须全部满足才能获得奖励=1）：


1. Atomic Action: Must represent ONE actionable step (e.g., "click X" not "click X then
1. 原子动作：必须代表一个可执行步骤（例如，“点击 X”而不是“点击 X 然后


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;do Y")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;执行 Y”）


2. Context Match: Must logically follow from the current screen state
2. 上下文匹配：必须符合当前屏幕状态的逻辑


3. Keyboard State: For text input instructions, keyboard MUST be visible/activated
3. 键盘状态：对于文本输入指令，键盘必须可见/激活


4. Target Existence: Referenced UI element must be present in current screen
4. 目标存在：引用的 UI 元素必须存在于当前屏幕中


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Evaluation Rules:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#评估规则：


&nbsp;&nbsp;&nbsp;&nbsp;- Reward=1 ONLY when ALL criteria are satisfied
&nbsp;&nbsp;&nbsp;&nbsp;- 仅当满足所有标准时，奖励=1


&nbsp;&nbsp;&nbsp;&nbsp;- Reward $= 0$ for ANY violation
&nbsp;&nbsp;&nbsp;&nbsp;- 任何违规行为奖励 $= 0$


&nbsp;&nbsp;&nbsp;&nbsp;#Examples:
&nbsp;&nbsp;&nbsp;&nbsp;#示例：


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Valid Example 1]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[有效示例 1]


Task: Search for hotels in Washington DC
任务：在华盛顿特区搜索酒店


&nbsp;&nbsp;&nbsp;&nbsp;Screen: Home screen with Chrome icon
&nbsp;&nbsp;&nbsp;&nbsp;屏幕：带有 Chrome 图标的主屏幕


&nbsp;&nbsp;&nbsp;&nbsp;Instruction: "click Chrome"
&nbsp;&nbsp;&nbsp;&nbsp;指令：“点击 Chrome”


$\rightarrow$ Reward=1
$\rightarrow$ 奖励=1


&nbsp;&nbsp;&nbsp;&nbsp;[Invalid Example 1]
&nbsp;&nbsp;&nbsp;&nbsp;[无效示例 1]


&nbsp;&nbsp;&nbsp;&nbsp;Task: Search for hotels in Washington DC
&nbsp;&nbsp;&nbsp;&nbsp;任务：在华盛顿特区搜索酒店


Screen: Chrome search page (no keyboard)
屏幕：Chrome 搜索页面（无键盘）


Instruction: "type hotels"
指令：“输入 hotels”


$\rightarrow$ Violates Rule $3 \rightarrow$ Reward $= 0$
$\rightarrow$ 违反规则 $3 \rightarrow$ 奖励 $= 0$


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Valid Example 2]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[有效示例 2]


&nbsp;&nbsp;&nbsp;&nbsp;Task: Search for hotels in Washington DC
&nbsp;&nbsp;&nbsp;&nbsp;任务：搜索华盛顿特区的酒店


Screen: Chrome search page (keyboard visible)
屏幕：Chrome 搜索页面（键盘可见）


Instruction: "type hotels in Washington DC"
指令：“输入华盛顿特区的酒店”


&nbsp;&nbsp;&nbsp;&nbsp;$\rightarrow$ Reward=1
&nbsp;&nbsp;&nbsp;&nbsp;$\rightarrow$ 奖励=1


&nbsp;&nbsp;&nbsp;&nbsp;[Invalid Example 2]
&nbsp;&nbsp;&nbsp;&nbsp;[无效示例 2]


&nbsp;&nbsp;&nbsp;&nbsp;Task: Search for hotels in Washington DC
&nbsp;&nbsp;&nbsp;&nbsp;任务：搜索华盛顿特区的酒店


Screen: Chrome search page (no keyboard)
屏幕：Chrome 搜索页面（无键盘）


Instruction: "click search bar"
指令：“点击搜索栏”


$\rightarrow$ Valid action $\rightarrow$ Reward=1
$\rightarrow$ 有效操作 $\rightarrow$ 奖励=1


&nbsp;&nbsp;&nbsp;&nbsp;[Invalid Example 3]
&nbsp;&nbsp;&nbsp;&nbsp;[无效示例 3]


&nbsp;&nbsp;&nbsp;&nbsp;Task: Search for hotels in Washington DC
&nbsp;&nbsp;&nbsp;&nbsp;任务：搜索华盛顿特区的酒店


Screen: Home screen
屏幕：主屏幕


Instruction: "open Chrome and search"
指令：“打开 Chrome 并搜索”


$\rightarrow$ Violates Rule $1 \rightarrow$ Reward $= 0$
$\rightarrow$ 违反规则 $1 \rightarrow$ 奖励 $= 0$


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Edge Case]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[边缘情况]


&nbsp;&nbsp;&nbsp;&nbsp;Task: Search for hotels in Washington DC
&nbsp;&nbsp;&nbsp;&nbsp;任务：搜索华盛顿特区的酒店


&nbsp;&nbsp;&nbsp;&nbsp;Screen: Search results page
&nbsp;&nbsp;&nbsp;&nbsp;屏幕：搜索结果页


Instruction: "click back button"
指令：“点击返回按钮”


$\rightarrow$ Valid but unrelated to task $\rightarrow$ Still Reward=1
$\rightarrow$ 有效但与任务无关 $\rightarrow$ 奖励仍为 1


#Output Format:
# 输出格式：


\{"reward": 1\} or \{"reward": 0\}
\{"reward": 1\} 或 \{"reward": 0\}


NO explanations. Strict JSON format only.
无需解释。仅限严格的 JSON 格式。


---



This evaluator forms the third term in the foresight reward:
该评估器构成前瞻奖励的第三项：


$$
{r}_{t}^{\left( h\right) } = {\lambda }_{1} \cdot  {r}_{\mathrm{{fmt}}}\left( {g}_{t}\right)  + {\lambda }_{2} \cdot  {r}_{\mathrm{{env}}}\left( {{s}_{t},{g}_{t},{\pi }_{\ell }}\right)  + {\lambda }_{3} \cdot  {\widehat{V}}_{\text{ judge }}\left( {{s}_{t},{g}_{t}}\right)
$$



This integrated signal guides the reasoning model to produce subgoals that are both well-formed and pragmatically executable, closing the loop between semantic intent and environmental grounding.
这一综合信号引导推理模型生成既规范又具备实际可执行性的子目标，从而闭合语义意图与环境落地之间的环路。


### C.3 TRAINING AND DEPLOYMENT OF HIERARCHICAL MODELS
### C.3 分层模型的训练与部署


Following the construction of the reward dataset, we proceed to train a compact reasoning model using the oracle-generated subgoal-instruction pairs. To promote better generalization and minimize manual prompt engineering, we intentionally adopt an extremely simplified prompt for the 3B-scale reasoning model. This design choice ensures that the model can generalize beyond prompt-specific templates and reduces deployment complexity. The same prompt is used for both training and inference:
在构建奖励数据集之后，我们使用由 Oracle 生成的“子目标-指令”对来训练一个紧凑的推理模型。为了提升泛化能力并减少人工提示工程，我们特意为 3B 规模的推理模型采用了极简的提示词。这一设计选择确保了模型能够超越特定提示模板进行泛化，并降低了部署复杂度。训练和推理均使用相同的提示词：


---



You are a mobile operation Agent that performs precise screen interactions. Analyze the
你是一个执行精确屏幕交互的移动端操作智能体。请分析


input and generate the next action instruction.
输入并生成下一步操作指令。


STRICTLY follow this structure:
严格遵守以下结构：


<reasoning> reasoning process here </reasoning> <instruction>Instruction:
<reasoning> 此处为推理过程 </reasoning> <instruction>指令：


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...</instruction>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...</instruction>


---



Low-Level Execution via Function-Call Interface. The low-level action model ${\pi }_{\ell }$ interacts with the Android environment through structured API-based function calls. Each atomic action is expressed as a JSON-formatted tool invocation, providing clear semantics for device control. The exact prompt used is as follows:
通过函数调用接口执行低层操作。低层动作模型 ${\pi }_{\ell }$ 通过结构化的 API 函数调用与 Android 环境交互。每个原子动作均表示为 JSON 格式的工具调用，为设备控制提供清晰的语义。所使用的具体提示词如下：


---



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#Tools
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;##工具


You may call one or more functions to assist with the user query.
你可以调用一个或多个函数来协助处理用户查询。


The following function is available:
以下是可用函数：


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<tools>



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\{



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name": "mobile_use",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name": "mobile_use",


"description": "Use a touchscreen to interact with a mobile device, and take screenshots.
"description": "使用触摸屏与移动设备交互，并截取屏幕截图。


The screen's resolution is 1092x2408. Supported actions include clicking, typing,
屏幕分辨率为 1092x2408。支持的动作包括点击、输入、


&nbsp;&nbsp;&nbsp;&nbsp;swiping, system button presses, and more.",
&nbsp;&nbsp;&nbsp;&nbsp;滑动、按下系统按钮等。",


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"parameters": \{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"parameters": \{


"action": ["click", "type", "swipe", "key", "system_button", "terminate"],
"action": ["click", "type", "swipe", "key", "system_button", "terminate"],


"coordinate": $\left\lbrack  {x,y}\right\rbrack$ ,
"coordinate": $\left\lbrack  {x,y}\right\rbrack$ ,


"text": "Optional input text",
"text": "可选输入文本",


&nbsp;&nbsp;&nbsp;&nbsp;"button": ["Back", "Home", "Menu", "Enter"],
&nbsp;&nbsp;&nbsp;&nbsp;"button": ["Back", "Home", "Menu", "Enter"],


&nbsp;&nbsp;&nbsp;&nbsp;"status": ["success", "failure"]
&nbsp;&nbsp;&nbsp;&nbsp;"status": ["success", "failure"]


\}



&nbsp;&nbsp;&nbsp;&nbsp;</tools>



---



This interface allows the high-level reasoning model to focus exclusively on intent prediction, while the low-level action model translates these into executable atomic operations. It not only simplifies control flow but also improves modularity and debugging during large-scale mobile task execution.
该接口使高层推理模型能够专注于意图预测，而由低层动作模型将其转化为可执行的原子操作。这不仅简化了控制流，还提高了大规模移动任务执行过程中的模块化程度和调试效率。


### C.4 TRAINING HYPERPARAMETERS AND CONFIGURATION
### C.4 训练超参数与配置


We adopt the GRPOTrainer implementation from VLM-R1 (Shen et al. 2025) for training our high-level and low-level models. The complete training configuration is summarized in Table 5
我们采用来自 VLM-R1 (Shen et al. 2025) 的 GRPOTrainer 实现来训练我们的高层与底层模型。完整的训练配置详见表 5。


<table><tr><td>Hyperparameter</td><td>Value</td></tr><tr><td>Max Prompt Length</td><td>526</td></tr><tr><td>Number of Generations (G)</td><td>6</td></tr><tr><td>Batch Size per Device</td><td>3</td></tr><tr><td>Gradient Accumulation Steps</td><td>2</td></tr><tr><td>Number of Training Epochs</td><td>2</td></tr><tr><td>Max Completion Length</td><td>256</td></tr><tr><td>Optimizer Precision</td><td>bfloat16</td></tr><tr><td>Gradient Checkpointing</td><td>true</td></tr><tr><td>Attention Implementation</td><td>flash_attention_2</td></tr><tr><td>Temperature</td><td>0.9</td></tr><tr><td>Top- $p$</td><td>1.0</td></tr><tr><td>Top-k</td><td>50</td></tr><tr><td>Repetition Penalty</td><td>1.0</td></tr><tr><td>Learning Rate</td><td>$1 \times  {10}^{-6}$</td></tr><tr><td>KL Coefficient $\left( \beta \right)$</td><td>0.04</td></tr><tr><td>Foresight Reward Weight ${\lambda }_{1}$</td><td>0.4</td></tr><tr><td>Foresight Reward Weight ${\lambda }_{2}$</td><td>0.3</td></tr><tr><td>Foresight Reward Weight ${\lambda }_{3}$</td><td>0.3</td></tr><tr><td>Clipping Threshold $\left( \epsilon \right)$</td><td>0.2</td></tr></table>
<table><tbody><tr><td>超参数</td><td>数值</td></tr><tr><td>最大提示词长度</td><td>526</td></tr><tr><td>生成数量 (G)</td><td>6</td></tr><tr><td>单设备批大小</td><td>3</td></tr><tr><td>梯度累积步数</td><td>2</td></tr><tr><td>训练轮数</td><td>2</td></tr><tr><td>最大生成长度</td><td>256</td></tr><tr><td>优化器精度</td><td>bfloat16</td></tr><tr><td>梯度检查点</td><td>true</td></tr><tr><td>注意力机制实现</td><td>flash_attention_2</td></tr><tr><td>温度</td><td>0.9</td></tr><tr><td>Top- $p$</td><td>1.0</td></tr><tr><td>Top-k</td><td>50</td></tr><tr><td>重复惩罚</td><td>1.0</td></tr><tr><td>学习率</td><td>$1 \times  {10}^{-6}$</td></tr><tr><td>KL 系数 $\left( \beta \right)$</td><td>0.04</td></tr><tr><td>前瞻奖励权重 ${\lambda }_{1}$</td><td>0.4</td></tr><tr><td>前瞻奖励权重 ${\lambda }_{2}$</td><td>0.3</td></tr><tr><td>前瞻奖励权重 ${\lambda }_{3}$</td><td>0.3</td></tr><tr><td>裁剪阈值 $\left( \epsilon \right)$</td><td>0.2</td></tr></tbody></table>


Table 5: Training hyperparameters used for hierarchical model optimization.
表 5：用于分层模型优化的训练超参数。


Training is conducted on four NVIDIA A800 80GB GPUs, and each full run takes approximately 22 hours to complete. The key software stack includes flash_attn 2.7.4.post1, torch 2.6.0, transformers 4.49.0, and tr1 0.16.0.dev0. These configurations ensure stable training, efficient memory usage via FlashAttention, and compatibility with the GRPOTrainer pipeline.
训练在四块 NVIDIA A800 80GB GPU 上进行，每次完整运行大约需要 22 小时。核心软件栈包括 flash_attn 2.7.4.post1、torch 2.6.0、transformers 4.49.0 以及 trl 0.16.0.dev0。这些配置确保了稳定的训练、通过 FlashAttention 实现的高效内存利用，以及与 GRPOTrainer 流水的兼容性。


### C.5 BASELINE HYPERPARAMETERS AND CONFIGURATION
### C.5 基准模型超参数与配置


We employed a series of baseline models, setting their hyperparameters in strict accordance with the configurations reported in the original papers. The training configuration is summarized in Table 6
我们采用了一系列基准模型，并严格按照原论文报告的配置设置其超参数。训练配置总结在表 6 中。


Table 6 presents the primary training hyperparameters for DigiRL (Bai et al., 2024) and DigiQ (Bai et al. 2025a). For a more comprehensive list of settings, please refer to the original papers.
表 6 展示了 DigiRL (Bai et al., 2024) 和 DigiQ (Bai et al. 2025a) 的主要训练超参数。更详细的设置列表请参阅原论文。


## D CASE STUDY: QUALITATIVE ANALYSIS OF HI-AGENT
## D 案例研究：HI-AGENT 的定性分析


We present qualitative examples to visualize the hierarchical reasoning and action execution process of Hi-Agent. The goal is to provide insight into how the agent decomposes abstract task instructions into semantic subgoals, and grounds them into executable atomic actions on the mobile UI. As evidenced by the examples, the agent now exhibits a clear capacity both to interpret given instructions and to manipulate the smartphone interface.
我们通过定性示例来展示 Hi-Agent 的分层推理与动作执行过程。其目的是深入了解智能体如何将抽象的任务指令分解为语义子目标，并将其落实为移动端 UI 上可执行的原子动作。如示例所示，该智能体现已展现出解析指令和操控智能手机界面的明确能力。


---



https://github.com/om-ai-lab/VLM-R1



---



<table><tr><td>Method</td><td>Hyperparameter</td><td>Value</td></tr><tr><td rowspan="12">DigiRL</td><td>actor lr</td><td>3e-3</td></tr><tr><td>value function lr</td><td>3e-3</td></tr><tr><td>instruction value function lr</td><td>3e-3</td></tr><tr><td>batch size</td><td>128</td></tr><tr><td>rollout trajectories</td><td>16</td></tr><tr><td>replay buffer size</td><td>5000</td></tr><tr><td>rollout temperature</td><td>1.0</td></tr><tr><td>maximum gradient norm</td><td>0.01</td></tr><tr><td>GAE $\lambda$</td><td>0.5</td></tr><tr><td>actor updates per iteration</td><td>20</td></tr><tr><td>value function updates per iteration</td><td>5</td></tr><tr><td>instruction value function updates per iteration</td><td>5</td></tr><tr><td rowspan="6">Digi-Q</td><td>actor lr</td><td>1e-4</td></tr><tr><td>value function lr</td><td>1e-5 (general), 5e-6 (webshop)</td></tr><tr><td>batch size</td><td>128</td></tr><tr><td>maximum gradient norm</td><td>0.01</td></tr><tr><td>actor updates per iteration</td><td>30</td></tr><tr><td>value function updates per iteration</td><td>20</td></tr></table>
<table><tbody><tr><td>方法</td><td>超参数</td><td>数值</td></tr><tr><td rowspan="12">DigiRL</td><td>策略网络学习率</td><td>3e-3</td></tr><tr><td>价值函数学习率</td><td>3e-3</td></tr><tr><td>指令价值函数学习率</td><td>3e-3</td></tr><tr><td>批大小</td><td>128</td></tr><tr><td>采样轨迹数</td><td>16</td></tr><tr><td>重放池大小</td><td>5000</td></tr><tr><td>采样温度</td><td>1.0</td></tr><tr><td>最大梯度范数</td><td>0.01</td></tr><tr><td>GAE $\lambda$</td><td>0.5</td></tr><tr><td>每轮策略网络更新次数</td><td>20</td></tr><tr><td>每轮价值函数更新次数</td><td>5</td></tr><tr><td>每轮指令价值函数更新次数</td><td>5</td></tr><tr><td rowspan="6">Digi-Q</td><td>策略网络学习率</td><td>1e-4</td></tr><tr><td>价值函数学习率</td><td>1e-5 (通用), 5e-6 (网上商店)</td></tr><tr><td>批大小</td><td>128</td></tr><tr><td>最大梯度范数</td><td>0.01</td></tr><tr><td>每轮策略网络更新次数</td><td>30</td></tr><tr><td>每轮价值函数更新次数</td><td>20</td></tr></tbody></table>


Table 6: Baseline Methods Hyperparameters
表 6：基准方法超参数


### D.1 ILLUSTRATION OF TASK DECOMPOSITION
### D.1 任务分解图示


Figure 6 shows an example where Hi-Agent successfully completes the task "Send a message to Alice". The reasoning model decomposes the goal into subgoals such as opening Messenger, selecting the receiver box, and typing the contact name. These semantic subgoals are then executed through low-level UI actions (e.g., Click (x, y), Input "Alice"), bridging symbolic reasoning and visual grounding.
图 6 展示了 Hi-Agent 成功完成“给 Alice 发送消息”任务的示例。推理模型将目标分解为子目标，例如打开 Messenger、选择收件人框以及输入联系人姓名。随后，这些语义子目标通过低层级 UI 操作（例如 Click (x, y), Input "Alice"）执行，从而桥接了符号推理与视觉定位。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_1792aa.jpg"/>



Figure 6: Hi-Agent first decomposes the high-level task into interpretable subgoals, then executes them via grounded UI actions.
图 6：Hi-Agent 首先将高层级任务分解为可解释的子目标，然后通过定位后的 UI 操作执行这些目标。


### D.2 QUALITATIVE SUCCESS EXAMPLES
### D.2 定性成功案例


Figures 7 and 8 depict procedural tasks involving the Clock application, which require structured interaction across multiple UI states.
图 7 和图 8 描绘了涉及时钟应用程序的程序性任务，这些任务需要跨多个 UI 状态进行结构化交互。


In Figure 7, the agent completes the task "Open the clock" by identifying the correct application icon from the home screen or app drawer and issuing a click command. Although visually simple, this case tests the agent's ability to robustly locate app-specific UI elements under varying layouts. Figure 8 demonstrates a more complex interaction: "Set an alarm for 4PM". The reasoning model first identifies that this goal entails a sequence of subtasks—launching the Clock app, selecting the "Alarm" tab, configuring the time selector to "4" and "PM," and confirming the alarm setup. The action model grounds these semantic instructions into a precise sequence of atomic UI operations. This example showcases the hierarchical policy's capacity to parse abstract temporal goals and execute interface-specific configurations through multi-step navigation.
在图 7 中，智能体通过从主屏幕或应用抽屉中识别正确的应用程序图标并发布点击指令，完成了“打开时钟”的任务。虽然视觉上很简洁，但该案例测试了智能体在不同布局下稳健定位特定应用 UI 元素的能力。图 8 展示了一个更复杂的交互：“设置下午 4 点的闹钟”。推理模型首先识别出该目标包含一系列子任务——启动时钟应用、选择“闹钟”选项卡、将时间选择器配置为“4”和“PM”，并确认闹钟设置。动作模型将这些语义指令落实为一系列精确的原子级 UI 操作。该示例展示了层级策略解析抽象时间目标并通过多步导航执行特定界面配置的能力。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_551480.jpg"/>



Figure 7: Illustration of Hi-Agent completing the task "Open the clock".
图 7：Hi-Agent 完成“打开时钟”任务的图示。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_865a8a.jpg"/>



Figure 8: Illustration of Hi-Agent completing the task "Set an alarm for 4PM".
图 8：Hi-Agent 完成“设置下午 4 点闹钟”任务的图示。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_f73d94.jpg"/>



Figure 9: Illustration of Hi-Agent completing the task "Search for hotels in Washington DC".
图 9：Hi-Agent 完成“在华盛顿特区搜索酒店”任务的图示。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_24acc8.jpg"/>



Figure 10: Illustration of Hi-Agent completing the task "Search for vegetarians on Maps".
图 10：Hi-Agent 完成“在地图上搜索素食者”任务的图示。


Figures 9-11 present search-oriented scenarios, demonstrating Hi-Agent's capability to interpret user intents, select appropriate applications, and execute tasks via step-wise decomposition. In Figure 11 the directive "Play the new Drake video on YouTube" is fulfilled by launching YouTube, issuing a text query, parsing the result list, and selecting the most recent entry-illustrating content retrieval, ambiguity resolution, and UI grounding. Figure 9 depicts a Chrome-based web search for "hotels in Washington DC," where the agent opens Chrome, inputs the query, and awaits the search results, thus emulating standard browser workflows. Figure 10 shows the task "Search for vegetarians on Maps," in which the agent launches Maps, activates the search bar, and issues a location-based query, evidencing spatial reasoning and semantic grounding.
图 9-11 展示了以搜索为导向的场景，证明了 Hi-Agent 理解用户意图、选择合适应用并通过逐步分解执行任务的能力。在图 11 中，“在 YouTube 上播放最新的 Drake 视频”指令通过启动 YouTube、发布文本查询、解析结果列表并选择最新条目得以实现——这说明了内容检索、歧义消除和 UI 定位。图 9 描绘了在 Chrome 中进行“华盛顿特区酒店”的网络搜索，智能体打开 Chrome、输入查询并等待搜索结果，从而模拟了标准的浏览器工作流。图 10 显示了“在地图上搜索素食者”的任务，其中智能体启动地图、激活搜索栏并发布基于位置的查询，证明了空间推理和语义定位能力。


Across these examples, Hi-Agent demonstrates the ability to plan multi-step routines, recover from intermediate states, and generate semantically appropriate and executable instructions under diverse application contexts.
在这些示例中，Hi-Agent 展示了在不同应用语境下规划多步例程、从中间状态恢复以及生成语义恰当且可执行指令的能力。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_c82e51.jpg"/>



Figure 11: Illustration of Hi-Agent completing the task "Play the new Drake video on YouTube".
图 11：Hi-Agent 完成“在 YouTube 上播放最新的 Drake 视频”任务的图示。


### D.3 FAILURE TAXONOMY AND ANALYSIS
### D.3 失败分类与分析


We categorize common failure cases into five representative types observed across evaluation tasks:
我们将评估任务中观察到的常见失败案例归纳为五个代表性类别：


Complex UI or Missing Target: the required UI element is ambiguous or absent (e.g., an out-of-stock product or visually occluded item), leading to redundant actions (repeated swipes) and step-overflow.
复杂UI或目标缺失：所需UI元素歧义或缺失（例如商品缺货或视觉遮挡），导致冗余操作（重复滑动）和步数溢出。


External Dependency or Latency: slow page loads, missing content, or emulator instability cause premature termination, since our action space lacks a dedicated "wait" operation.
外部依赖或延迟：页面加载缓慢、内容缺失或模拟器不稳定导致过早终止，因为我们的动作空间缺乏专门的“等待”操作。


Incorrect Navigation Path: the agent selects an unintended app or menu (e.g., using a third-party app to access system settings), resulting in irrecoverable divergence from the optimal path.
错误导航路径：智能体选择了非预期的应用或菜单（例如使用第三方应用访问系统设置），导致与最优路径发生不可逆的偏离。


Premature Termination: the agent exits before achieving the final subgoal, often due to reward misalignment or incorrect completion assumption.
过早终止：智能体在达成最终子目标前退出，通常由于奖励不一致或错误的完成假设所致。


Goal Misunderstanding: partial misinterpretation of instructions (e.g., confusing search with shopping tasks or opting for web search instead of app interaction).
目标误解：对指令的部分误读（例如混淆搜索与购物任务，或选择网页搜索而非应用交互）。


Figures 12 and 13 illustrate two failure cases representative of the first two categories. Figure 12 illustrates a task failure that occurred when the website lacked the cargo required to complete the mission. Figure 13 depicts how network instability combined with constraints on the action space led to repeated access attempts and ultimately caused the step-count limit to be exceeded.
图12和13展示了前两个类别的典型失败案例。图12说明了因网站缺少完成任务所需的货物而导致的失败。图13描绘了网络不稳定结合动作空间的限制，如何导致重复尝试并最终造成步数超限。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_d7cb70.jpg"/>



Figure 12: Failure due to product unavailability and repeated swiping exceeding the step limit.
图12：由于产品不可用及重复滑动导致超过步数限制的失败。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_8d568e.jpg"/>



Figure 13: Failure due to long page load and the lack of a "wait" operation.
图13：由于页面加载时间长且缺乏“等待”操作导致的失败。


## E ADDITIONAL RESULTS
## E 补充结果


This section provides complementary analyses and visualizations for four extended analyses beyond the main evaluation: (1) distribution of task completion outcomes, including the proportion of each error category after classification, (2) generalization under UI layout shift, (3) error diagnosis and correction for WebShopping tasks in AitW, and (4) large-scale deployment of Hi-Agent in the AndroidWorld benchmark.
本节为主要评估之外的四个扩展分析提供补充分析和可视化：(1) 任务完成结果的分布，包括分类后各类错误的比例；(2) UI布局偏移下的泛化能力；(3) AitW中网络购物任务的错误诊断与修正；(4) Hi-Agent在AndroidWorld基准测试中的大规模部署。


### E.1 FAILURE TASKS DISTRIBUTION
### E.1 失败任务分布


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_eece50.jpg"/>



Figure 14: Distribution of task success and failure across all evaluation instances.
图14：所有评估实例中任务成功与失败的分布。


As shown in Figure 14, after GRPO training, the hierarchical model exhibits a remarkable enhancement in its comprehension of application pages and a noticeable improvement in task disassembly. Consequently, the task success rate of the hierarchical model following GRPO training has significantly increased. The rise in errors related to External Dependency or Latency is attributed to the previous model failing to access the correct website and encountering task failures before facing network issues. After the model's capability was elevated through GRPO training, these inherent environmental issues were laid bare.
如图14所示，经过GRPO训练后，分层模型在应用页面理解和任务拆解方面表现出显著增强。因此，GRPO训练后分层模型的任务成功率大幅提升。外部依赖或延迟错误的增加，归因于之前的模型因无法访问正确网站而在遇到网络问题前就已失败。随着模型能力通过GRPO训练提升，这些固有的环境问题便显露出来。


### E.2 ROBUSTNESS TO LAYOUT PERTURBATION
### E.2 对布局扰动的鲁棒性


We visualize how layout changes impact agent performance in Figure 15 The task is "What's a good restaurant in Las Vegas?". During training, agents are initialized from the home screen, but in this evaluation setting, the starting screen is changed to the all-apps view, causing a significant layout shift. Under this condition, DigiRL fails to locate Chrome and instead opens the Contacts app, repeating incorrect actions. In contrast, Hi-Agent successfully completes the task. Thanks to its hierarchical architecture, the high-level reasoning model remains unaffected by coordinate-level changes and generates consistent subgoals, while the low-level action model grounds those subgoals to new visual contexts.
我们在图15中可视化了布局变化对智能体性能的影响。任务是“拉斯维加斯有哪些好餐厅？”。训练期间智能体从主屏幕启动，但在该评估设置中，起始屏幕改为应用列表视图，导致显著的布局偏移。在此条件下，DigiRL无法定位Chrome，转而打开联系人应用并重复错误操作。相比之下，Hi-Agent成功完成了任务。得益于其分层架构，高层推理模型不受坐标级变化的影响并生成一致的子目标，而底层动作模型则将这些子目标锚定到新的视觉语境中。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_0e728a.jpg"/>



Figure 15: Layout shift visualization for the task "What's a good restaurant in Las Vegas?". When the layout is perturbed by switching from the home screen (training setting) to the all-apps view (test setting), DigiRL fails to locate Chrome and repeatedly interacts with the wrong app. In contrast, Hi-Agent completes the task successfully by leveraging its hierarchical decomposition, which enables robust subgoal generation and grounding under spatial variation.
图15：任务“拉斯维加斯有哪些好餐厅？”的布局偏移可视化。当布局因从主屏幕（训练设置）切换到应用列表视图（测试设置）而受到扰动时，DigiRL无法定位Chrome并反复与错误的应用交互。相比之下，Hi-Agent通过分层拆解成功完成任务，这种架构能够在空间变化下实现稳健的子目标生成与定位。


### E.3 ERROR ANALYSIS AND CORRECTION ON AITW WEBSHOPPING SUBSET
### E.3 AITW WEBSHOPPING 子集的错误分析与修正


We analyze the agent's performance on the AitW WebShopping task subset. The current success rates are 70.3% on training and 68.8% on testing tasks. After manual inspection, we find that two problematic domains-newegg.com and costco.com-consistently lead to failure: the former blocks agent access, while the latter prevents <ENTER> key inputs. This observation aligns with prior findings reported in DigiQ (Bai et al. 2025a). When we replace these domains with ebay.com and rerun the WebShopping subset, success rates improve significantly to 92.7% on train and 91.2% on test (see Table 7).
我们分析了智能体在 AitW WebShopping 任务子集上的表现。当前训练集和测试集的成功率分别为 70.3% 和 68.8%。经人工检查，我们发现两个问题域名——newegg.com 和 costco.com——导致了持续的失败：前者屏蔽了智能体的访问，而后者阻止了 <ENTER> 键输入。这一观察结果与 DigiQ (Bai et al. 2025a) 中报告的先前发现一致。当我们将这些域名替换为 ebay.com 并重新运行 WebShopping 子集时，训练集和测试集的成功率分别显著提升至 92.7% 和 91.2%（见表 7）。


Table 7: Success rate before and after WebShopping subset correction.
表 7：WebShopping 子集修正前后的成功率。


<table><tr><td></td><td>Original</td><td>After Domain Replacement</td></tr><tr><td>Train Subset</td><td>70.3%</td><td>92.7%</td></tr><tr><td>Test Subset</td><td>68.8%</td><td>91.2%</td></tr></table>
<table><tbody><tr><td></td><td>原文</td><td>领域替换后</td></tr><tr><td>训练子集</td><td>70.3%</td><td>92.7%</td></tr><tr><td>测试子集</td><td>68.8%</td><td>91.2%</td></tr></tbody></table>


### E.4 ZERO-SHOT GENERALIZATION ON GUI GROUNDING
### E.4 GUI 定位的零样本泛化


To further validate the generalization capability of our hierarchical architecture, we evaluate the low-level action model $\left( {\pi }_{\ell }\right)$ on the ScreenSpot-v2 benchmark (Wu et al. 2024) in a zero-shot setting. ScreenSpot-v2 is a comprehensive GUI grounding benchmark spanning mobile, web, and desktop platforms, designed to test an agent's fundamental ability to locate text and icon/widget elements. For this experiment, we take the low-level model trained on AitW data and directly apply it to ScreenSpot-v2 without any fine-tuning.
为了进一步验证分层架构的泛化能力，我们在零样本设置下，通过 ScreenSpot-v2 基准测试（Wu et al. 2024）评估了底层动作模型 $\left( {\pi }_{\ell }\right)$。ScreenSpot-v2 是一个涵盖移动端、网页端和桌面端的全面 GUI 定位基准，旨在测试智能体定位文本及图标/组件的基础能力。在此实验中，我们直接将基于 AitW 数据训练的底层模型应用于 ScreenSpot-v2，未进行任何微调。


As shown in Table 8 our 7B low-level action model achieves a highly competitive 91.5% average score in this zero-shot setting, outperforming several larger, specialized SFT models. This result demonstrates that our training framework encourages the action model to learn robust and generalizable visual representations, rather than merely overfitting to the training tasks. The model's strong grounding ability across diverse platforms is further illustrated by the qualitative examples in Figure 16
如表 8 所示，我们的 7B 底层动作模型在零样本设置下取得了 91.5% 的极具竞争力的平均分数，优于多个更大规模的专用 SFT 模型。该结果证明了我们的训练框架能够促使动作模型学习鲁棒且泛化性强的视觉表示，而非仅仅过拟合于训练任务。模型在不同平台上的强大定位能力在图 16 的定性示例中得到了进一步展示。


Table 8: Zero-shot performance on the ScreenSpot-v2 benchmark. Our low-level model $\left( {\pi }_{\ell }\right)$ is evaluated without any fine-tuning on this dataset. Baselines are from original papers.
表 8：ScreenSpot-v2 基准测试的零样本表现。我们的底层模型 $\left( {\pi }_{\ell }\right)$ 在未对该数据集进行任何微调的情况下接受了评估。基线数据源自原始论文。


<table><tr><td rowspan="2">Models</td><td colspan="2">Mobile</td><td colspan="2">Desktop</td><td colspan="2">Web</td><td rowspan="2">Avg</td></tr><tr><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td></tr><tr><td colspan="8">Closed-source Models</td></tr><tr><td>GPT-40</td><td>26.6</td><td>24.2</td><td>24.2</td><td>19.3</td><td>12.8</td><td>11.8</td><td>20.1</td></tr><tr><td>UI-TARS-1.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>94.2</td></tr><tr><td>Seed1.5-VL</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>95.2</td></tr><tr><td colspan="8">GUI-specific Models (SFT)</td></tr><tr><td>SeeClick-9.6B</td><td>78.4</td><td>50.7</td><td>70.1</td><td>29.3</td><td>55.2</td><td>32.5</td><td>55.1</td></tr><tr><td>UGround-7B</td><td>75.1</td><td>84.5</td><td>85.1</td><td>61.4</td><td>84.6</td><td>71.9</td><td>76.3</td></tr><tr><td>UI-TARS-7B</td><td>96.9</td><td>89.1</td><td>95.4</td><td>85.0</td><td>93.6</td><td>85.2</td><td>91.6</td></tr><tr><td>Jedi-7B</td><td>96.9</td><td>87.2</td><td>95.9</td><td>87.9</td><td>94.4</td><td>84.2</td><td>91.7</td></tr><tr><td>GUI-Actor-7B</td><td>97.6</td><td>88.2</td><td>96.9</td><td>85.7</td><td>93.2</td><td>86.7</td><td>92.1</td></tr><tr><td colspan="8">GUI-specific Models (RL)</td></tr><tr><td>UI-R1-E-3B</td><td>98.2</td><td>83.9</td><td>94.8</td><td>75.0</td><td>93.2</td><td>83.7</td><td>89.5</td></tr><tr><td>LPO</td><td>97.9</td><td>82.9</td><td>95.9</td><td>86.4</td><td>95.6</td><td>84.2</td><td>90.5</td></tr><tr><td>GTA1-7B</td><td>99.0</td><td>88.6</td><td>94.9</td><td>89.3</td><td>92.3</td><td>86.7</td><td>92.4</td></tr><tr><td>GTA1-72B</td><td>99.3</td><td>92.4</td><td>97.4</td><td>89.3</td><td>95.3</td><td>91.4</td><td>94.8</td></tr><tr><td colspan="8">Ours (Zero-Shot from AitW)</td></tr><tr><td>Hi-Agent $\left( {{\pi }_{\ell },7\mathrm{\;B}}\right)$</td><td>96.6</td><td>81.0</td><td>95.9</td><td>84.3</td><td>94.9</td><td>91.1</td><td>91.5</td></tr></table>
<table><tbody><tr><td rowspan="2">模型</td><td colspan="2">移动端</td><td colspan="2">桌面端</td><td colspan="2">Web端</td><td rowspan="2">均值</td></tr><tr><td>文本</td><td>图标</td><td>文本</td><td>图标</td><td>文本</td><td>图标</td></tr><tr><td colspan="8">闭源模型</td></tr><tr><td>GPT-4o</td><td>26.6</td><td>24.2</td><td>24.2</td><td>19.3</td><td>12.8</td><td>11.8</td><td>20.1</td></tr><tr><td>UI-TARS-1.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>94.2</td></tr><tr><td>Seed1.5-VL</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>95.2</td></tr><tr><td colspan="8">GUI专用模型 (SFT)</td></tr><tr><td>SeeClick-9.6B</td><td>78.4</td><td>50.7</td><td>70.1</td><td>29.3</td><td>55.2</td><td>32.5</td><td>55.1</td></tr><tr><td>UGround-7B</td><td>75.1</td><td>84.5</td><td>85.1</td><td>61.4</td><td>84.6</td><td>71.9</td><td>76.3</td></tr><tr><td>UI-TARS-7B</td><td>96.9</td><td>89.1</td><td>95.4</td><td>85.0</td><td>93.6</td><td>85.2</td><td>91.6</td></tr><tr><td>Jedi-7B</td><td>96.9</td><td>87.2</td><td>95.9</td><td>87.9</td><td>94.4</td><td>84.2</td><td>91.7</td></tr><tr><td>GUI-Actor-7B</td><td>97.6</td><td>88.2</td><td>96.9</td><td>85.7</td><td>93.2</td><td>86.7</td><td>92.1</td></tr><tr><td colspan="8">GUI专用模型 (RL)</td></tr><tr><td>UI-R1-E-3B</td><td>98.2</td><td>83.9</td><td>94.8</td><td>75.0</td><td>93.2</td><td>83.7</td><td>89.5</td></tr><tr><td>LPO</td><td>97.9</td><td>82.9</td><td>95.9</td><td>86.4</td><td>95.6</td><td>84.2</td><td>90.5</td></tr><tr><td>GTA1-7B</td><td>99.0</td><td>88.6</td><td>94.9</td><td>89.3</td><td>92.3</td><td>86.7</td><td>92.4</td></tr><tr><td>GTA1-72B</td><td>99.3</td><td>92.4</td><td>97.4</td><td>89.3</td><td>95.3</td><td>91.4</td><td>94.8</td></tr><tr><td colspan="8">本研究 (基于 AitW 的零样本)</td></tr><tr><td>Hi-Agent $\left( {{\pi }_{\ell },7\mathrm{\;B}}\right)$</td><td>96.6</td><td>81.0</td><td>95.9</td><td>84.3</td><td>94.9</td><td>91.1</td><td>91.5</td></tr></tbody></table>


### E.5 LARGE-SCALE DEPLOYMENT ON ANDROIDWORLD
### E.5 在 ANDROIDWORLD 上的大规模部署


In Section 5.3 we show that Hi-Agent scales to larger models and more complex mobile environments. Using a Qwen2.5-VL-72B reasoning model and a 7B action model, our hierarchical agent achieves a success rate of 56.5% on the AndroidWorld benchmark-demonstrating competitive performance among methods that rely solely on raw screenshots as input.
在第 5.3 节中，我们展示了 Hi-Agent 可扩展至更大规模的模型和更复杂的移动环境。通过使用 Qwen2.5-VL-72B 推理模型和 7B 动作模型，我们的分层代理在 AndroidWorld 基准测试中实现了 56.5% 的成功率——在仅依赖原始截图作为输入的方法中展现了极具竞争力的性能。


Figure 18 illustrates Hi-Agent solving a structured input task: "Add the following expenses into the pro expense: Movie Night-375.45-Entertainment-Urgent". The agent opens the expense app, fills each field accurately, and uses swiping gestures to select the correct category. Figure 19 shows a temporal query task: "What is on my schedule for October 28 at 2:45am in Simple Calendar Pro?". The agent distinguishes between multiple dates on the UI (e.g., 28 at the top vs. bottom), selects the correct one, and parses event information directly from the screen.
图 18 展示了 Hi-Agent 解决结构化输入任务的过程：“将以下支出添加到 Pro Expense 中：电影之夜-375.45-娱乐-紧急”。代理打开支出应用，准确填写每个字段，并使用滑动操作选择正确的类别。图 19 展示了一个时间查询任务：“Simple Calendar Pro 中 10 月 28 日凌晨 2:45 的日程是什么？”。代理能够区分界面上的多个日期（例如，顶部与底部的 28 日），选择正确日期，并直接从屏幕解析事件信息。


We also report per-app success statistics in Figure 17 The agent performs reliably on structured apps such as Clock, Settings, and Expense. In contrast, it struggles with apps like Markor and Retro, which require prior usage familiarity-without such user-specific guidance, even humans may find them hard to operate. Another common failure mode involves tasks lacking explicit termination signals (e.g., taking a photo, scrolling to the end of a page). Unlike humans, who adjust behavior dynamically through feedback, the agent only receives discrete visual frames, making it hard to infer when the task should be terminated.
我们还在图 17 中报告了各应用的成功率统计。代理在时钟、设置和支出等结构化应用上表现可靠。相比之下，它在 Markor 和 Retro 等应用上表现欠佳，这些应用需要先前的操作经验——若缺乏此类针对用户的指导，即使是人类也可能难以操作。另一种常见的失败模式涉及缺乏明确终止信号的任务（例如，拍照、滚动到页面底部）。人类能通过反馈动态调整行为，而代理仅接收离散的视觉帧，难以推断任务何时应当终止。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_da43fc.jpg"/>



Figure 16: Qualitative examples of Hi-Agent's zero-shot GUI grounding performance on diverse tasks from the ScreenSpot-v2 benchmark.
图 16：Hi-Agent 在 ScreenSpot-v2 基准测试多样化任务中零样本 GUI 定位性能的定性示例。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_60ed01.jpg"/>



Figure 17: Per-app success statistics in AndroidWorld.
图 17：AndroidWorld 中各应用的成功率统计。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_42b368.jpg"/>



Figure 18: Illustration of Hi-Agent completing the task "Add the following expenses into the pro expense: name|amount_dollars|category_name|note Movie Night|\$375.45|Entertainment|Urgent".
图 18：Hi-Agent 完成任务的说明：“将以下支出添加到 Pro Expense 中：名称|金额（美元）|类别名称|备注 电影之夜|\$375.45|娱乐|紧急”。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2026/02/2026_02_18__00_21_43_c28dd8.jpg"/>



Figure 19: Illustration of Hi-Agent completing the task "What is on my schedule for October 28 at 2:45am in Simple Calendar Pro? Answer with the titles only. If there are multiples titles, format your answer in a comma separated list. ".
图 19：Hi-Agent 完成任务的说明：“Simple Calendar Pro 中 10 月 28 日凌晨 2:45 的日程是什么？仅回答标题。若有多个标题，请以逗号分隔列表的形式格式化答案。”