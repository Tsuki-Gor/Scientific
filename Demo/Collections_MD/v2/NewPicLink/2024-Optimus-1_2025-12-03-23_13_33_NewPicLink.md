# Optimus-1 Empowered Agents Excel in Long-Horizon Tasks
# Optimus-1 赋能智能体在长时序任务中表现卓越


Zaijing Li ${}^{12}$ , Yuquan Xie ${}^{1}$ , Rui Shao ${}^{1}$ * Gongwei Chen ${}^{1}$ , Dongmei Jiang ${}^{2}$ , Liqiang Nie ${}^{1 * }$
李再景 ${}^{12}$ , 谢宇泉 ${}^{1}$ , 邵锐 ${}^{1}$ * 陈功伟 ${}^{1}$ , 蒋冬梅 ${}^{2}$ , 聂立强 ${}^{1 * }$


${}^{1}$ Harbin Institute of Technology, Shenzhen
${}^{1}$ 哈尔滨工业大学（深圳）


${}^{2}$ Peng Cheng Laboratory
${}^{2}$ 鹏城实验室


\{1zj14011,xieyuquan20016,rshaojimmy,nieliqiang\}@gmail.com
\{1zj14011,xieyuquan20016,rshaojimmy,nieliqiang\}@gmail.com


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_68072f.jpg"/>



Figure 1: An illustration of Optimus-1 performing long-horizon tasks in Minecraft. Given the task "Craft stone sword", Knowledge-Guided Planner incorporates knowledge from Hierarchical Directed Knowledge Graph into planning, then Action Controller executes these planning sequences step-by-step. During the execution of the task, the Experience-Driven Reflector is periodically activated and retrieve experience from Abstracted Multimodal Experience Pool to make reflection.
图1：Optimus-1 在 Minecraft 中执行长时序任务的示意图。针对任务“制作石剑”，知识引导规划器将分层有向知识图谱中的知识纳入规划，随后动作控制器逐步执行这些规划序列。在任务执行过程中，经验驱动反思器会定期被激活并从抽象多模态经验池检索经验以进行反思。


## Abstract
## 摘要


Building a general-purpose agent is a long-standing vision in the field of artificial intelligence. Existing agents have made remarkable progress in many domains, yet they still struggle to complete long-horizon tasks in an open world. We attribute this to the lack of necessary world knowledge and multimodal experience that can guide agents through a variety of long-horizon tasks. In this paper, we propose a Hybrid Multimodal Memory module to address the above challenges. It 1) transforms knowledge into Hierarchical Directed Knowledge Graph that allows agents to explicitly represent and learn world knowledge, and 2) summarises historical information into Abstracted Multimodal Experience Pool that provide agents
构建通用智能体是人工智能领域的长期愿景。现有智能体在许多领域取得了显著进展，但在开放世界中仍难以完成长时序任务。我们认为这是由于缺乏能在多种长时序任务中引导智能体的必要世界知识和多模态经验。为此，本文提出了一种混合多模态记忆模块以应对上述挑战。该模块 1) 将知识转化为分层有向知识图谱，使智能体能够显式表示和学习世界知识，2) 将历史信息摘要为抽象多模态经验池，为智能体提供


---



*Corresponding authors
* 通信作者


---



with rich references for in-context learning. On top of the Hybrid Multimodal Memory module, a multimodal agent, Optimus-1, is constructed with dedicated Knowledge-guided Planner and Experience-Driven Reflector, contributing to a better planning and reflection in the face of long-horizon tasks in Minecraft. Extensive experimental results show that Optimus-1 significantly outperforms all existing agents on challenging long-horizon task benchmarks, and exhibits near human-level performance on many tasks. In addition, we introduce various Multimodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show that Optimus-1 exhibits strong generalization with the help of the Hybrid Multimodal Memory module, outperforming the GPT-4V baseline on various tasks. Please see the project page at https://cybertronagent.github.io/Optimus- 1.github.io/.
丰富的上下文学习参考。基于混合多模态记忆模块，我们构建了多模态智能体 Optimus-1，配备专用的知识引导规划器和经验驱动反思器，从而在 Minecraft 的长时序任务中实现更好的规划与反思。大量实验结果表明，Optimus-1 在具有挑战性的长时序任务基准上显著优于现有所有智能体，并在许多任务上展现接近人类水平的表现。此外，我们引入了多种多模态大语言模型（MLLMs）作为 Optimus-1 的骨干。实验结果显示，在混合多模态记忆模块的帮助下，Optimus-1 表现出强泛化能力，在多项任务上超越 GPT-4V 基线。项目页面请见 https://cybertronagent.github.io/Optimus- 1.github.io/。


## 1 Introduction
## 1 引言


Optimus Prime faces complex tasks alongside humans in Transformers to protect the peace of the planet. Creating an agent [44, 13] like Optimus that can perceive, plan, reflect, and complete long-horizon tasks in an open world has been a longstanding aspiration in the field of artificial intelligence $\left\lbrack  {{22},{36},{37},{27},{58}}\right\rbrack$ . Early research developed simple policy through reinforcement learning [7] or imitation learning [1, 25]. A lot of work [47, 50] have utilized Large Language Models (LLMs) as action planners for agents, generating executable sub-goal sequences for low-level action controllers. Further, recent studies [52, 33] employed Multimodal Large Language Models (MLLMs) $\left\lbrack  {4,{39},{56}}\right\rbrack$ as planner and reflector. Leveraging the powerful instruction-following and logical reasoning capabilities of (Multimodal) LLMs [24], LLM-based agents have achieved remarkable success across multiple domains $\left\lbrack  {{14},9,{10},{55}}\right\rbrack$ . Nevertheless,the ability of these agents to complete long-horizon tasks still falls significantly short of human-level performance.
Optimus Prime 在《变形金刚》中与人类并肩面对复杂任务以保卫星球和平。创造像 Optimus 那样能在开放世界中感知、规划、反思并完成长时序任务的智能体一直是人工智能领域的愿望 $\left\lbrack  {{22},{36},{37},{27},{58}}\right\rbrack$ 。早期研究通过强化学习 [7] 或模仿学习 [1, 25] 开发了简单策略。大量工作 [47, 50] 利用大语言模型（LLMs）作为智能体的动作规划器，生成可执行的子目标序列供底层动作控制器使用。最近的研究 [52, 33] 更是采用多模态大语言模型（MLLMs）$\left\lbrack  {4,{39},{56}}\right\rbrack$ 作为规划器和反思器。借助（多模态）LLMs 在遵循指令和逻辑推理方面的强大能力 [24]，基于 LLM 的智能体已在多个领域取得显著成功 $\left\lbrack  {{14},9,{10},{55}}\right\rbrack$ 。尽管如此，这些智能体完成长时序任务的能力仍远低于人类水平。


According to relevant studies [28, 42, 46], the human ability to complete long-horizon tasks in an open world relies on long-term memory storage, which is divided into knowledge and experience. The storage and utilization of knowledge and experience play a crucial role in guiding human behavior and enabling humans to adapt flexibly to their environments in order to accomplish long-horizon tasks. Inspired by this theory, we summarize the challenges faced by current agents as follows:
相关研究 [28, 42, 46] 表明，人类在开放世界中完成长时序任务的能力依赖于长期记忆存储，且可分为知识与经验两类。知识与经验的存储和利用在引导人类行为、使人类灵活适应环境以完成长时序任务方面起着关键作用。受该理论启发，我们将当前智能体面临的挑战总结如下：


Insufficient Exploration of Structured Knowledge: Structured knowledge, encompassing open world rules, object relationships, and interaction methods with the environment, is essential for agents to complete complex tasks [34,44]. However,MLLMs such as GPT-4V ${}^{1}$ lack sufficient knowledge in Minecraft. Existing agents $\left\lbrack  {1,{25},7}\right\rbrack$ only learn dispersed knowledge from video data and are unable to efficiently represent and learn this structured knowledge, rendering them incapable of performing complex tasks.
结构化知识探索不足：结构化知识包含开放世界规则、对象关系及与环境的交互方式，对于智能体完成复杂任务至关重要[34,44]。然而，诸如 GPT-4V ${}^{1}$ 的多模态大模型在 Minecraft 中知识不足。现有智能体 $\left\lbrack  {1,{25},7}\right\rbrack$ 仅从视频数据学习零散知识，无法高效表示与学习此类结构化知识，因而无法执行复杂任务。


Lack of Multimodal Experience: Humans derive successful strategies and lessons from information on historical experience [8, 32], which assists them in tackling current complex tasks. In a similar manner, agents can benefit from in-context learning with experience demonstrations [43, 54]. However, existing agents [47, 51, 33] only consider unimodal information, which prevents them from learning from multimodal experience as humans do.
缺乏多模态经验：人类从历史经验信息中获得成功策略与教训[8,32]，以帮助应对当前复杂任务。类似地，智能体可通过带有经验示例的上下文学习受益[43,54]。然而，现有智能体[47,51,33] 仅考虑单模态信息，无法像人类那样从多模态经验中学习。


To address the aforementioned challenges, we propose Hybrid Multimodal Memory module that consists of Hierarchical Directed Knowledge Graph (HDKG) and Abstracted Multimodal Experience Pool (AMEP). For HDKG, we map the logical relationships between objects into a directed graph structure, thereby transforming knowledge into high-level semantic representations. HDKG efficiently provides the agent with the necessary knowledge for task execution, without requiring any parameter updates. For AMEP, we dynamically summarize and store the multimodal information (e.g., environment, agent state, task plan, video frames, etc.) from the agent's task execution process, ensuring that historical information contains both a global overview and local details. Different from the method of directly storing successful cases as experience [52], AMEP considers both successful and failed cases as references. This innovative approach of incorporating failure cases into in-context learning significantly enhances the performance of the agent.
为应对上述挑战，我们提出由分层有向知识图（HDKG）与抽象多模态经验池（AMEP）组成的混合多模态记忆模块。对于 HDKG，我们将对象间的逻辑关系映射为有向图结构，从而将知识转化为高层语义表示。HDKG 无需参数更新即可高效为智能体提供执行任务所需的知识。对于 AMEP，我们动态摘要并存储智能体执行任务过程中的多模态信息（如环境、智能体状态、任务计划、视频帧等），确保历史信息既包含全局概览又保留局部细节。不同于直接将成功案例存为经验的方法[52]，AMEP 同时将成功与失败案例作为参考。这一将失败案例纳入上下文学习的创新方法显著提升了智能体性能。


---



${}^{1}$ https://openai.com/index/gpt-4v-system-card/
${}^{1}$ https://openai.com/index/gpt-4v-system-card/


---



On top of the Hybrid Multimodal Memory module, we construct a multimodal composable agent, Optimus-1. As shown in Figure 1, Optimus-1 consists of Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. To enhance the ability of agents to cope with complex environments and long-horizon tasks, Knowledge-Guided Planner incorporates visual observation into the planning phase, leveraging HDKG to capture the knowledge needed. This allows the agent to efficiently transform tasks into executable sub-goals. Action Controller takes the sub-goal and the current observation as inputs and generates low-level actions, interacting with the game environment to update the agent's state. In open-world complex environments, agents are prone to be erroneous when performing long-horizon tasks. To address this, we propose Experience-Driven Reflector, which is periodically activated to retrieve relevant multimodal experiences from AMEP. This encourages the agent to reflect on its current actions and refine the plan.
基于混合多模态记忆模块，我们构建了可组合多模态智能体 Optimus-1。如图1 所示，Optimus-1 包含知识引导规划器、经验驱动反思器与动作控制器。为增强智能体应对复杂环境与长时序任务的能力，知识引导规划器将视觉观测纳入规划阶段，利用 HDKG 捕捉所需知识，使智能体能高效将任务转化为可执行子目标。动作控制器以子目标与当前观测为输入生成低层动作，与游戏环境交互以更新智能体状态。在开放世界复杂环境中，智能体在执行长时序任务时易出错。为此，我们提出周期性激活的经验驱动反思器，从 AMEP 检索相关多模态经验，促使智能体反思当前行为并优化计划。


We validate the performance of Optimus-1 in Minecraft, a popular open-world game environment. Experimental results show that Optimus-1 exhibits remarkable performance on long-horizon tasks, representing up to ${30}\%$ improvement over existing agents. Moreover,we introduce various Multimodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show that Optimus-1 has a 2 to 6 times performance improvement with the help of Hybrid Multimodal Memory, outperforming powerful GPT-4V baseline on lots of long-horizon tasks. Additionally, we verified that the plug-and-play Hybrid Multimodal Memory can drive Optimus-1 to incrementally improve its performance in a self-evolution manner. The extensive experimental results show that Optimus-1 makes a major step toward a general agent with a human-like level of performance. Main contributions of our paper:
我们在流行的开放世界游戏环境 Minecraft 中验证了 Optimus-1 的性能。实验结果表明 Optimus-1 在长时序任务上表现卓越，相较于现有智能体提升高达 ${30}\%$。此外，我们引入多种多模态大语言模型（MLLMs）作为 Optimus-1 的骨干网络。实验显示，在混合多模态记忆的帮助下，Optimus-1 的性能提升为 2 到 6 倍，且在大量长时序任务上超越强大的 GPT-4V 基线。我们还验证了即插即用的混合多模态记忆能够驱动 Optimus-1 以自我进化的方式逐步提升性能。大量实验结果表明，Optimus-1 在向具有人类水平表现的通用智能体迈出重要一步。本文主要贡献：


- We propose Hybrid Multimodal Memory module which is composed of HDKG and AMEP. HDKG helps the agent make the planning of long-horizon tasks efficiently. AMEP provides refined historical experience and guides the agent to reason about the current situation state effectively.
- 我们提出由 HDKG 与 AMEP 组成的混合多模态记忆模块。HDKG 帮助智能体高效规划长时序任务，AMEP 提供精炼的历史经验并引导智能体有效推理当前状态。


- On top of the Hybrid Multimodal Memory module, we construct Optimus-1, which consists of Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. Optimus-1 outperforms all baseline agents on long-horizon task benchmarks, and exhibits capabilities close to the level of human players.
- 基于混合多模态记忆模块，我们构建了包含知识引导规划器、经验驱动反思器与动作控制器的 Optimus-1。Optimus-1 在长时序任务基准上优于所有基线智能体，并展现出接近人类玩家的能力。


- Driven by Hybrid Multimodal Memory, various MLLM-based Optimus-1 have demonstrated 2 to 6 times performance improvement, demonstrating the generalization of Hybrid Multimodal Memory.
- 在混合多模态记忆的驱动下，多种基于 MLLM 的 Optimus-1 展示了 2 到 6 倍的性能提升，验证了混合多模态记忆的泛化性。


## 2 Optimus-1
## 2 Optimus-1


In this section, we first elaborate on how to implement the Hybrid Multimodal Memory in Sec 2.1. As a core innovation, it plays a crucial role in enabling Optimus-1 to execute long-horizon tasks. Next, we give an overview of Optimus-1 framework (Sec 2.2), which consists of Hybrid Multimodal Memory, Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. Finally, we introduce a non-parametric learning approach to expand the hybrid multimodal memory (Sec 2.3), thereby enhancing the success rate of task execution for Optimus-1.
在本节中，我们首先详细说明如何在 Sec 2.1 中实现混合多模态记忆。作为核心创新，它在使 Optimus-1 执行长时域任务方面起关键作用。接着，我们概述 Optimus-1 框架（Sec 2.2），其由混合多模态记忆、知识引导规划器、经验驱动反思器和动作控制器组成。最后，我们介绍一种非参数学习方法以扩展混合多模态记忆（Sec 2.3），从而提高 Optimus-1 的任务执行成功率。


### 2.1 Hybrid Multimodal Memory
### 2.1 混合多模态记忆


In order to endow agent with a long-term memory storage mechanism [28, 46], we propose the Hybrid Multimodal Memory module, which consists of Abstracted Multimodal Experience Pool (AMEP) and Hierarchical Directed Knowledge Graph (HDKG).
为使智能体具备长期记忆存储机制 [28, 46]，我们提出了混合多模态记忆模块，该模块由抽象化多模态经验池（AMEP）和分层有向知识图（HDKG）构成。


#### 2.1.1 Abstracted Multimodal Experience Pool
#### 2.1.1 抽象化多模态经验池


Relevant studies $\left\lbrack  {{23},{29},{17},{15}}\right\rbrack$ highlight the importance of historical information for agents completing long-horizon tasks. Minedojo [7] and Voyager [47] employed unimodal storage of historical information. Jarvis-1 [52] used a multimodal experience mechanism that stores task planning and visual information without summarization, posing challenges to storage capacity and retrieval speed. To address this issue, we propose AMEP, which aims to dynamically summarize all multimodal information during task execution. It preserves the integrity of long-horizon data while enhancing storage and retrieval efficiency.
相关研究 $\left\lbrack  {{23},{29},{17},{15}}\right\rbrack$ 强调了历史信息对于智能体完成长时域任务的重要性。Minedojo [7] 与 Voyager [47] 采用了单模态的历史信息存储。Jarvis-1 [52] 使用了一种不经摘要的多模态经验机制，存储任务规划和视觉信息，导致存储容量与检索速度方面的挑战。为解决此问题，我们提出 AMEP，旨在任务执行过程中动态摘要所有多模态信息。在保留长时域数据完整性的同时，提高了存储和检索效率。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_26eb73.jpg"/>



Figure 2: (a) Extraction process of multimodal experience. The frames are filtered through video buffer and image buffer, then MineCLIP [7] is employed to compute the visual and sub-goal similarities and finally they are stored in Abstracted Multimodal Experience Pool. (b) Overview of Hierarchical Directed Knowledge Graph. Knowledge is stored as a directed graph, where its nodes represent objects, and directed edges point to materials that can be crafted by this object.
图 2：（a）多模态经验的提取过程。帧通过视频缓冲区和图像缓冲区过滤，然后使用 MineCLIP [7] 计算视觉与子目标相似度，最终存入抽象化多模态经验池。（b）分层有向知识图概览。知识以有向图形式存储，节点表示物体，有向边指向该物体可合成的材料。


Specifically, as depicted in Figure 2, to conduct the static visual information abstraction, the video stream captured by Optimus-1 during task execution is first input to a video buffer, filtering the stream at a fixed frequency of 1 frame per second. Based on the filtered video frames, to further perform a dynamic visual information abstraction, these frames are then fed into an image buffer with a window size of 16 , where the image similarity is dynamically computed and final abstracted frames are adaptively updated. To align such abstracted visual information with the corresponding textual sub-goal, we then utilize MineCLIP [7], a pre-trained video-text alignment model, to calculate their multimodal correlation. When this correlation exceeds a threshold, the corresponding image buffer and textual sub-goal are saved as multimodal experience into a pool. Finally, we further incorporate environment information, agent initial state, and plan generated by Knowledge-Guided Planner, into such a pool, which forms the AMEP. In this way, we consider the multimodal information of each sub-goal, and summarise it to finally compose the multimodal experience of the given task.
具体而言，如图 2 所示，为进行静态视觉信息抽象，Optimus-1 在任务执行期间采集的视频流首先输入视频缓冲区，以每秒 1 帧的固定频率过滤流。基于过滤后的视频帧，为进一步执行动态视觉信息抽象，这些帧接着被送入窗口大小为 16 的图像缓冲区，在其中动态计算图像相似度并自适应更新最终摘要帧。为将此类摘要视觉信息与对应的文本子目标对齐，我们随后使用预训练的视频-文本对齐模型 MineCLIP [7] 来计算它们的多模态相关性。当该相关性超过阈值时，相应的图像缓冲区内容和文本子目标作为多模态经验保存到池中。最后，我们进一步将环境信息、智能体初始状态以及由知识引导规划器生成的计划合入该池，形成 AMEP。通过这种方式，我们考虑并摘要每个子目标的多模态信息，最终构成给定任务的多模态经验。


#### 2.1.2 Hierarchical Directed Knowledge Graph
#### 2.1.2 分层有向知识图


In Minecraft, mining and crafting represent a complex knowledge network crucial for effective task planning. For instance,crafting a diamond sword $\checkmark$ requires two diamonds $\odot$ and one wooden stick /, while mining diamonds requires an iron pickaxe 3 , which involving further materials and steps. Such knowledge is essential for an agent's ability to perform long-horizon complex tasks. Instead of implicit learning through fine-tuning [33, 60], we propose HDKG, which transforms knowledge into a graph representation. It enables the agent to perform explicit learning by retrieving information from the knowledge graph.
在 Minecraft 中，采矿与合成构成了对有效任务规划至关重要的复杂知识网络。例如，合成一把钻石剑 $\checkmark$ 需要两个钻石 $\odot$ 和一根木棍 /，而采矿钻石则需要一把铁镐 3 ，这又牵涉到进一步的材料和步骤。此类知识对于智能体执行长时域复杂任务至关重要。我们提出 HDKG，将知识转化为图表示，允许智能体通过从知识图检索信息来进行显式学习，而不是通过微调 [33, 60] 进行隐式学习。


As shown in the Figure 2,we transform knowledge into a graph $\mathcal{D}\left( {\mathcal{V},\mathcal{E}}\right)$ ,where nodes set $\mathcal{V}$ represent objects,and directed edges set $\mathcal{E}$ point to nodes that can be crafted by this object. An edge $e \in  \mathcal{E}$ in the $\mathcal{D}$ can be represented as $e = \left( {u,v}\right)$ ,where $u,v \in  \mathcal{V}$ . The directed graph efficiently stores and updates knowledge. For a given object $x$ ,retrieving the corresponding node allows extraction of a sub-graph ${\mathcal{D}}_{j}\left( {{\mathcal{V}}_{j},{\mathcal{E}}_{j}}\right)  \in  \mathcal{D}$ ,where nodes set ${\mathcal{V}}_{j}$ and edges set ${\mathcal{E}}_{j}$ can be formulated as:
如图2所示，我们将知识转化为图$\mathcal{D}\left( {\mathcal{V},\mathcal{E}}\right)$，其中节点集合$\mathcal{V}$表示物体，定向边集合$\mathcal{E}$指向可由该物体合成的节点。图$\mathcal{D}$中的一条边$e \in  \mathcal{E}$可表示为$e = \left( {u,v}\right)$，其中$u,v \in  \mathcal{V}$。有向图高效地存储并更新知识。对于给定物体$x$，检索对应节点可提取子图${\mathcal{D}}_{j}\left( {{\mathcal{V}}_{j},{\mathcal{E}}_{j}}\right)  \in  \mathcal{D}$，其节点集合${\mathcal{V}}_{j}$和边集合${\mathcal{E}}_{j}$可表示为：


$$
{\mathcal{V}}_{j} = \{ v \in  \mathcal{V} \mid  x\} ,\;{\mathcal{E}}_{j} = \left\{  {e = \left( {u,v}\right)  \in  \mathcal{V} \mid  u \in  {\mathcal{V}}_{j} \cup  v \in  {\mathcal{V}}_{j}}\right\}  , \tag{1}
$$



Then by topological sorting, we can get all the materials and their relationships needed to complete the task. This knowledge is provided to the Knowledge-Guided Planner as a way to generate a more reasonable sequence of sub-goals. With HDKG, we can significantly enhance the world knowledge of the agent in a train-free manner.
然后通过拓扑排序，我们可以得到完成任务所需的所有材料及其关系。这些知识作为生成更合理子目标序列的手段提供给知识引导规划器。借助HDKG，我们可以在无需训练的情况下显著增强代理的世界知识。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_dc8b95.jpg"/>



Figure 3: Overview framework of our Optimus-1. Optimus-1 consists of Knowledge-Guided Planner, Experience-Driven Reflector, Action Controller, and Hybrid Multimodal Memory architecture. Given the task "craft stone sword", Optimus-1 incorporates the knowledge from HDKG into Knowledge-Guided Planning, then Action Controller generates low-level actions. Experience-Driven Reflector is periodically activated to introduce multimodal experience from AMEP to determine if the current task can be executed successfully. If not, it will ask the Knowledge-Guided Planner to refine the plan.
图3：Optimus-1的总体框架。Optimus-1由知识引导规划器、经验驱动反思器、动作控制器和混合多模态记忆架构组成。以“合成石剑”为任务，Optimus-1将HDKG中的知识纳入知识引导规划，然后动作控制器生成低级动作。经验驱动反思器周期性激活，从AMEP引入多模态经验以判断当前任务能否成功执行，若不能，则要求知识引导规划器细化计划。


### 2.2 Optimus-1: Framework
### 2.2 Optimus-1：框架


Relevant studies indicate that the human brain is essential for planning and reflection, while the cerebellum controls low-level actions, both crucial for complex tasks [40,41]. Inspired by this, we divide the structure of Optimus-1 into Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. In a given game environment with a long-horizon task, the Knowledge-Guided Planner senses the environment, retrieves knowledge from HDKG, and decomposes the task into executable sub-goals. The action controller then sequentially executes these sub-goals. During execution, the Experience-Driven Reflector is activated periodically, leveraging historical experience from AMEP to assess whether Optimus-1 can complete the current sub-goal. If not, it instructs the Knowledge-Guided Planner to revise its plan. Through iterative interaction with the environment, Optimus-1 ultimately completes the task.
相关研究表明，大脑在规划与反思中至关重要，小脑负责低级动作控制，两者对复杂任务都很关键[40,41]。受此启发，我们将Optimus-1的结构划分为知识引导规划器、经验驱动反思器和动作控制器。在具有长时序任务的游戏环境中，知识引导规划器感知环境、从HDKG检索知识并将任务分解为可执行的子目标，动作控制器随后按序执行这些子目标。在执行过程中，经验驱动反思器会周期性激活，利用来自AMEP的历史经验评估Optimus-1是否能完成当前子目标，若不能则指示知识引导规划器修订计划。通过与环境的反复交互，Optimus-1最终完成任务。


Knowledge-Guided Planner. Open-world environments vary greatly, affecting task execution. Previous approaches [51] using LLMs for task planning failed to consider the environment, leading to the failure of tasks. For example, an agent in a cave aims to catch fish. It lacks visual information to plan conditions on the current situation, such as "leave the cave and find a river". Therefore, we integrate environmental information into the planning stage. Unlike Jarvis-1 [52] and MP5 [33], which convert observation to textual descriptions, Optimus-1 directly employs observation as visual conditions to generate environment-related plans, i.e., sub-goal sequences. This results in more comprehensive and reasonable planning. More importantly, Knowledge-Guided Planner retrieves the knowledge needed to complete the task from HDKG, allowing task planning to be done once, rather than generating the next step in each iteration. Given the task $t$ ,observation $o$ ,the sub-goals sequence ${g}_{1},{g}_{2},{g}_{3},\ldots ,{g}_{n}$ can be formulated as:
知识引导规划器。开放世界环境差异巨大，影响任务执行。以往使用LLM进行任务规划的方法[51]未考虑环境信息，导致任务失败。例如，洞穴中的代理目标是钓鱼，却缺乏视觉信息以据此制定当前情境下的计划条件，如“离开洞穴寻找河流”。因此我们将环境信息整合到规划阶段。不同于将观测转换为文本描述的Jarvis-1[52]和MP5[33]，Optimus-1直接将观测作为视觉条件以生成与环境相关的计划，即子目标序列，从而得到更全面合理的规划。更重要的是，知识引导规划器从HDKG检索完成任务所需的知识，使得任务规划可一次完成，而不是每次迭代生成下一步。给定任务$t$、观测$o$，子目标序列${g}_{1},{g}_{2},{g}_{3},\ldots ,{g}_{n}$可表示为：


$$
{g}_{1},{g}_{2},{g}_{3},\ldots ,{g}_{n} = {p}_{\theta }\left( {o,t,{p}_{\eta }\left( t\right) }\right) , \tag{2}
$$



where $n$ is the number of sub-goals, ${p}_{\eta }$ denotes sub-graph retrieved from HDKG, ${p}_{\theta }$ denotes MLLM. In this paper, we employ OpenAI's GPT-4V as Knowledge-Guided Planner and Experience-Driven Reflector. We also evaluate other alternatives of GPT-4V, such as open-source models like Deepseek-VL [26] and InternLM-XComposer2-VL [6] in Section 3.4.
其中$n$为子目标数量，${p}_{\eta }$表示从HDKG检索的子图，${p}_{\theta }$表示MLLM。本文中，我们采用OpenAI的GPT-4V作为知识引导规划器和经验驱动反思器。我们也在第3.4节评估了GPT-4V的其他替代方案，如开源模型Deepseek-VL[26]和InternLM-XComposer2-VL[6]。


Action Controller. It takes the sub-goal and the current observation as inputs and then generates low-level actions, which are control signals for the mouse and keyboard. Thus, it can interact with the game environment to update the agent's state and the observation. The formulation is as follows:
动作控制器。它以子目标和当前观测为输入，生成低级动作，即鼠标和键盘的控制信号，从而与游戏环境交互以更新智能体状态和观测。其形式化表示如下：


$$
{a}_{k} = {p}_{\pi }\left( {o,{g}_{i}}\right) , \tag{3}
$$



where ${a}_{k}$ denotes low-level action at time $k,{p}_{\pi }$ denotes action controller. Unlike generating code $\left\lbrack  {{47},{33},{50}}\right\rbrack$ ,generating control actions for the mouse and keyboard $\left\lbrack  {1,{25},{52},3}\right\rbrack$ more closely resembles human behavior. In this paper, we employ STEVE-1 [25] as our Action Controller.
其中 ${a}_{k}$ 表示时刻 $k,{p}_{\pi }$ 的低级动作，表示动作控制器。与生成代码 $\left\lbrack  {{47},{33},{50}}\right\rbrack$ 不同，为鼠标和键盘生成控制动作 $\left\lbrack  {1,{25},{52},3}\right\rbrack$ 更接近人类行为。在本文中，我们采用 STEVE-1 [25] 作为动作控制器。


Experience-Driven Reflector. The sub-goals generated by Knowledge-Guided Planner are interdependent. The failure of any sub-goal halts the execution of subsequent ones, leading to overall task failure. Therefore, a reflection module is essential to identify and rectify errors promptly. During task execution, the Experience-Driven Reflector activates at regular intervals, retrieving historical experience from AMEP, and then analyzing the current state of Optimus-1. The reflection results of Optimus-1 are categorized as COMPLETE, CONTINUE, or REPLAN. COMPLETE indicates successful execution, prompting the action controller to proceed to the next sub-goal. CONTINUE signifies ongoing execution without additional feedback. REPLAN denotes failure, requiring the Knowledge-Guided Planner to revise the plan. The reflection $r$ generated by Experience-Driven Reflector can be formulated as:
经验驱动反思器。由知识引导规划器生成的子目标相互依赖，任何子目标的失败都会阻止后续子目标的执行，导致整体任务失败。因此，需要反思模块以及时识别并纠正错误。在任务执行过程中，经验驱动反思器按固定间隔启动，从 AMEP 检索历史经验，然后分析 Optimus-1 的当前状态。Optimus-1 的反思结果被分类为 COMPLETE、CONTINUE 或 REPLAN。COMPLETE 表示执行成功，促使动作控制器进入下一个子目标；CONTINUE 表示正在执行且无需额外反馈；REPLAN 表示失败，需要知识引导规划器修订计划。经验驱动反思器生成的反思 $r$ 可形式化表示为：


$$
r = {p}_{\theta }\left( {o,{g}_{i},{p}_{\epsilon }\left( t\right) }\right) , \tag{4}
$$



where ${p}_{\epsilon }$ denotes multimodal experience retrieved from AMEP. Experimental results in Section 3.3 demonstrate that the Experience-Driven Reflector significantly enhances the success rate of long-horizon tasks.
其中 ${p}_{\epsilon }$ 表示从 AMEP 检索的多模态经验。第 3.3 节的实验结果表明，经验驱动反思器显著提升了长时序任务的成功率。


During task execution, even in cases where task failure necessitates REPLAN, multimodal experiences are stored in AMEP. Thus, during the reflection phase, Optimus-1 can retrieve the most relevant cases from each of the three scenarios COMPLETE, CONTINUE, and REPLAN from AMEP as references. Experimental Results in Section 3.3 demonstrate the effectiveness of this innovative method of incorporating failure cases into in-context learning.
在任务执行过程中，即便任务失败需 REPLAN，多模态经验仍会被存入 AMEP。因此在反思阶段，Optimus-1 可从 AMEP 中检索来自 COMPLETE、CONTINUE 和 REPLAN 三类情景中最相关的案例作为参考。第 3.3 节的实验结果表明，将失败案例纳入情境学习的这一创新方法是有效的。


### 2.3 Non-parametric Learning of Hybrid Multimodal Memory
### 2.3 混合多模态记忆的非参数学习


To implement the Hybrid Multimodal Memory and enhance Optimus-1's capacity, we propose a nonparametric learning method named "free exploration-teacher guidance". In the free exploration phase, Optimus-1's equipment and tasks are randomly initialized, and it explores random environments, acquiring world knowledge through environmental feedback. For example, it learns that "a stone sword $\checkmark$ can be crafted with a wooden stick $\angle$ and two cobblestones $\text{ ♥ }$ ”,storing this in the HDKG. Additionally, successful and failed cases are stored in the AMEP, providing reference experience for the reflection phase. We initialize multiple Optimus-1, and they share the same HDKG and AMEP. Thus the memory is filled up efficiently. After free exploration, Optimus-1 has basic world knowledge and multimodal experience. In the teacher guidance phase, Optimus-1 needs to learn a small number of long-horizon tasks based on extra knowledge. For example,it learns "a diamond sword $\swarrow$ is obtained by a stick $\angle$ and two diamonds $\odot$ " from the teacher,then perform the task "craft diamond sword". During the teacher guidance phase, Optimus-1's memory is further expanded and it gains the experience of executing complete long-horizon tasks.
为实现混合多模态记忆并增强 Optimus-1 的能力，我们提出一种名为“自由探索—教师引导”的非参数学习方法。在自由探索阶段，随机初始化 Optimus-1 的装备和任务，它在随机环境中探索，通过环境反馈获取世界知识。例如，它学习到“石剑 $\checkmark$ 可由木棍 $\angle$ 和两块圆石 $\text{ ♥ }$ 制作”，并将其存入 HDKG。此外，成功与失败的案例被存入 AMEP，为反思阶段提供参照经验。我们初始化多个 Optimus-1，它们共享相同的 HDKG 与 AMEP，从而高效填充记忆。在自由探索后，Optimus-1 具备基本的世界知识与多模态经验。在教师引导阶段，Optimus-1 在额外知识基础上学习少量长时序任务。例如，它从教师处学到“钻石剑 $\swarrow$ 由木棍 $\angle$ 和两颗钻石 $\odot$ 获得”，然后执行“制作钻石剑”任务。在教师引导阶段，Optimus-1 的记忆进一步扩展，并获得执行完整长时序任务的经验。


Unlike fine-tuning, this method enhances Optimus-1 incrementally without updating parameters, in a self-evolution manner. Starting with an empty Hybrid Multimodal Memory, Optimus-1 iterates between "free exploration-teacher guidance" learning and unseen task inference. With each iteration, its memory capacity grows, enabling mastery of tasks from easy to hard.
与微调不同，该方法以自我进化的方式增量增强 Optimus-1，而不更新参数。从空的混合多模态记忆开始，Optimus-1 在“自由探索—教师引导”学习与未见任务推理之间迭代。随着每次迭代，其记忆容量增长，使其能够从易到难掌握任务。


Table 1: Main Result of Optimus-1 on long-horizon tasks benchmark. We report the average success rate (SR), average number of steps (AS), and average time (AT) on each task group, the results of each task can be found in the Appendix F. Lower AS and AT metrics mean that the agent is more efficient at completing the task,while $+ \infty$ indicates that the agent is unable to complete the task. Overall represents the average result on the five groups of Iron, Gold, Diamond, Redstone, and Armor.
表 1：Optimus-1 在长时序任务基准上的主要结果。我们报告每组任务的平均成功率 (SR)、平均步数 (AS) 和平均时间 (AT)，每个任务的结果见附录 F。较低的 AS 和 AT 指标表示智能体完成任务更高效，而 $+ \infty$ 表示智能体无法完成该任务。Overall 表示 Iron、Gold、Diamond、Redstone 和 Armor 五组的平均结果。


<table><tr><td>Group</td><td>Metric</td><td>GPT-3.5</td><td>GPT-4V</td><td>DEPS</td><td>Jarvis-1</td><td>Optimus-1</td><td>Human-level</td></tr><tr><td rowspan="3">Wood</td><td>SR↑</td><td>40.16</td><td>41.42</td><td>77.01</td><td>93.76</td><td>98.60</td><td>100.00</td></tr><tr><td>AT $\downarrow$</td><td>56.39</td><td>55.15</td><td>85.53</td><td>67.76</td><td>47.09</td><td>31.08</td></tr><tr><td>AS $\downarrow$</td><td>1127.78</td><td>1103.04</td><td>1710.61</td><td>1355.25</td><td>841.94</td><td>621.59</td></tr><tr><td rowspan="3">Stone</td><td>SR $\uparrow$</td><td>20.40</td><td>20.89</td><td>48.52</td><td>89.20</td><td>92.35</td><td>100.00</td></tr><tr><td>AT $\downarrow$</td><td>135.71</td><td>132.77</td><td>138.71</td><td>141.50</td><td>129.94</td><td>80.85</td></tr><tr><td>AS $\downarrow$</td><td>2714.21</td><td>2655.47</td><td>2574.30</td><td>2830.05</td><td>2518.88</td><td>1617.00</td></tr><tr><td rowspan="3">Iron</td><td>SR↑</td><td>0.00</td><td>0.00</td><td>16.37</td><td>36.15</td><td>46.69</td><td>86.00</td></tr><tr><td>AT $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>944.61</td><td>722.78</td><td>651.33</td><td>434.38</td></tr><tr><td>AS $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>8892.24</td><td>8455.51</td><td>6017.85</td><td>5687.60</td></tr><tr><td rowspan="3"><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_99a097.jpg"/></td><td>SR $\uparrow$</td><td>0.00</td><td>0.00</td><td>0.00</td><td>7.20</td><td>8.51</td><td>17.31</td></tr><tr><td>AT $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>787.37</td><td>726.35</td><td>557.08</td></tr><tr><td>AS $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>15747.13</td><td>15527.07</td><td>13141.60</td></tr><tr><td rowspan="3">Diamond</td><td>SR↑</td><td>0.00</td><td>0.00</td><td>0.60</td><td>8.98</td><td>11.61</td><td>16.98</td></tr><tr><td>AT $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>1296.96</td><td>1255.06</td><td>1150.98</td><td>744.82</td></tr><tr><td>AS $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>23939.30</td><td>25101.25</td><td>23019.64</td><td>16237.54</td></tr><tr><td rowspan="3"><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_fc98a8.jpg"/> Redstone</td><td>SR $\uparrow$</td><td>0.00</td><td>0.00</td><td>0.00</td><td>16.31</td><td>25.02</td><td>33.27</td></tr><tr><td>AT $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>1070.42</td><td>932.50</td><td>617.89</td></tr><tr><td>AS $\downarrow$</td><td>$+ \infty$</td><td>+0</td><td>$+ \infty$</td><td>17408.40</td><td>12709.99</td><td>12357.00</td></tr><tr><td rowspan="3">\$ Armor</td><td>SR↑</td><td>0.00</td><td>0.00</td><td>9.98</td><td>15.82</td><td>19.47</td><td>28.48</td></tr><tr><td>AT $\downarrow$</td><td>$+ \infty$</td><td>+∞</td><td>997.59</td><td>924.60</td><td>824.53</td><td>551.30</td></tr><tr><td>AS $\downarrow$</td><td>$+ \infty$</td><td>+∞</td><td>17951.95</td><td>16492.96</td><td>16350.56</td><td>11026.00</td></tr><tr><td>Overall</td><td>SR $\uparrow$</td><td>0.00</td><td>0.00</td><td>5.39</td><td>16.89</td><td>22.26</td><td>36.41</td></tr></table>
<table><tbody><tr><td>组</td><td>度量</td><td>GPT-3.5</td><td>GPT-4V</td><td>DEPS</td><td>Jarvis-1</td><td>Optimus-1</td><td>人类水平</td></tr><tr><td rowspan="3">木制</td><td>SR↑</td><td>40.16</td><td>41.42</td><td>77.01</td><td>93.76</td><td>98.60</td><td>100.00</td></tr><tr><td>AT $\downarrow$</td><td>56.39</td><td>55.15</td><td>85.53</td><td>67.76</td><td>47.09</td><td>31.08</td></tr><tr><td>AS $\downarrow$</td><td>1127.78</td><td>1103.04</td><td>1710.61</td><td>1355.25</td><td>841.94</td><td>621.59</td></tr><tr><td rowspan="3">石头</td><td>SR $\uparrow$</td><td>20.40</td><td>20.89</td><td>48.52</td><td>89.20</td><td>92.35</td><td>100.00</td></tr><tr><td>AT $\downarrow$</td><td>135.71</td><td>132.77</td><td>138.71</td><td>141.50</td><td>129.94</td><td>80.85</td></tr><tr><td>AS $\downarrow$</td><td>2714.21</td><td>2655.47</td><td>2574.30</td><td>2830.05</td><td>2518.88</td><td>1617.00</td></tr><tr><td rowspan="3">铁制</td><td>SR↑</td><td>0.00</td><td>0.00</td><td>16.37</td><td>36.15</td><td>46.69</td><td>86.00</td></tr><tr><td>AT $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>944.61</td><td>722.78</td><td>651.33</td><td>434.38</td></tr><tr><td>AS $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>8892.24</td><td>8455.51</td><td>6017.85</td><td>5687.60</td></tr><tr><td rowspan="3"><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_99a097.jpg"/></td><td>SR $\uparrow$</td><td>0.00</td><td>0.00</td><td>0.00</td><td>7.20</td><td>8.51</td><td>17.31</td></tr><tr><td>AT $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>787.37</td><td>726.35</td><td>557.08</td></tr><tr><td>AS $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>15747.13</td><td>15527.07</td><td>13141.60</td></tr><tr><td rowspan="3">钻石</td><td>SR↑</td><td>0.00</td><td>0.00</td><td>0.60</td><td>8.98</td><td>11.61</td><td>16.98</td></tr><tr><td>AT $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>1296.96</td><td>1255.06</td><td>1150.98</td><td>744.82</td></tr><tr><td>AS $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>23939.30</td><td>25101.25</td><td>23019.64</td><td>16237.54</td></tr><tr><td rowspan="3"><img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_fc98a8.jpg"/> 红石</td><td>SR $\uparrow$</td><td>0.00</td><td>0.00</td><td>0.00</td><td>16.31</td><td>25.02</td><td>33.27</td></tr><tr><td>AT $\downarrow$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>$+ \infty$</td><td>1070.42</td><td>932.50</td><td>617.89</td></tr><tr><td>AS $\downarrow$</td><td>$+ \infty$</td><td>+0</td><td>$+ \infty$</td><td>17408.40</td><td>12709.99</td><td>12357.00</td></tr><tr><td rowspan="3">\$ 护甲</td><td>SR↑</td><td>0.00</td><td>0.00</td><td>9.98</td><td>15.82</td><td>19.47</td><td>28.48</td></tr><tr><td>AT $\downarrow$</td><td>$+ \infty$</td><td>+∞</td><td>997.59</td><td>924.60</td><td>824.53</td><td>551.30</td></tr><tr><td>AS $\downarrow$</td><td>$+ \infty$</td><td>+∞</td><td>17951.95</td><td>16492.96</td><td>16350.56</td><td>11026.00</td></tr><tr><td>总体</td><td>SR $\uparrow$</td><td>0.00</td><td>0.00</td><td>5.39</td><td>16.89</td><td>22.26</td><td>36.41</td></tr></tbody></table>


## 3 Experiments
## 3 实验


### 3.1 Experiments Setting
### 3.1 实验设置


Environment. To ensure realistic gameplay like human players, we employ MineRL [11] with Minecraft 1.16.5 as our simulation environment. The agent operates at a fixed speed of 20 frames per second and only interacts with the environment via low-level action control signals of the mouse and keyboard. For more information about the detailed descriptions of the observation and action spaces, please refer to the Appendix B.
环境。为了确保像人类玩家一样的真实游戏体验，我们使用基于 Minecraft 1.16.5 的 MineRL [11] 作为仿真环境。智能体以固定速度每秒 20 帧运行，仅通过鼠标和键盘的低级动作控制信号与环境交互。有关观测与动作空间的详细描述，请参阅附录 B。


Benchmark. We constructed a benchmark of 67 tasks to evaluate the Optimus-1's ability to complete long-horizon tasks. As illustrated in Table 5, we divide the 67 Minecraft tasks into 7 groups according to recommended categories in Minecraft. Please refer to Appendix D for more details.
基准。我们构建了由 67 个任务组成的基准，用于评估 Optimus-1 完成长时序任务的能力。如表 5 所示，我们按照 Minecraft 推荐的类别将这 67 个任务分为 7 组。更多细节请参阅附录 D。


Baseline. We compare Optimus-1 with various agents,including GPT-3.5 ${}^{2}$ ,GPT-4V,DEPS [51], and Jarvis-1 [52] on the challenging long-horizon tasks benchmark. In addition, we employed 10 volunteers to perform the same task on the benchmark, and their average performance served as a human-level baseline. Please refer to Appendix D. 2 for more details about human-level baseline. For a more comprehensive comparison, we also report Optimus-1's performances on the benchmark used by Voyager [47], MP5 [33], and DEPS [51] in the Appendix F.2. Note that we initialize Optimus-1 with an empty inventory, while DEPS [51] and Jarvis-1 [52] have tools in their initial state. This makes it more challenging for Optimus-1 to perform the same tasks.
基线。我们将 Optimus-1 与多种智能体比较，包括 GPT-3.5 ${}^{2}$、GPT-4V、DEPS [51] 和 Jarvis-1 [52] 在这一具有挑战性的长时序任务基准上的表现。此外，我们还邀请了 10 名志愿者在该基准上执行相同任务，他们的平均表现作为人类基线。有关人类基线的更多细节，请参阅附录 D.2。为更全面的比较，我们还在附录 F.2 报告了 Optimus-1 在 Voyager [47]、MP5 [33] 和 DEPS [51] 使用的基准上的表现。注意我们将 Optimus-1 初始化为空物品栏，而 DEPS [51] 和 Jarvis-1 [52] 在初始状态中具有工具，这使得 Optimus-1 更难完成相同任务。


---



${}^{2}$ https://openai.com/research/gpt-3.5
${}^{2}$ https://openai.com/research/gpt-3.5


---



Table 2: Ablation study results. We report average Table 3: Ablation study on AMEP. We report success rate (SR) on each task group. P., R., K., the average success rate (SR) on each task group. E. represent Planning, Reflection, Knowledge, and Zero, Suc., and Fail. represent retrieving from Experience, respectively. AMEP without getting the case, getting the success
表 2：消融实验结果。我们报告每个任务组的平均成功率（SR）。表 3：关于 AMEP 的消融研究。我们报告每个任务组的成功率（SR）。P., R., K., E. 分别代表规划、反思、知识和从经验检索（Zero、Suc. 和 Fail. 分别表示未命中、成功与失败），AMEP 若未获取案例、获取成功的


<table><tr><td colspan="4">Ablation Setting</td><td colspan="5">Task Group</td></tr><tr><td>P.</td><td>R.</td><td>K.</td><td>E.</td><td>Wood</td><td>Stone</td><td>Iron</td><td>Gold</td><td>Diamond</td></tr><tr><td></td><td></td><td></td><td></td><td>14.29</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>✓</td><td></td><td></td><td></td><td>42.95</td><td>25.67</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>✓</td><td>✓</td><td></td><td></td><td>55.00</td><td>47.37</td><td>18.11</td><td>2.08</td><td>1.11</td></tr><tr><td>✓</td><td>✓</td><td></td><td>✓</td><td>73.53</td><td>64.20</td><td>24.19</td><td>3.08</td><td>1.86</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td></td><td>92.37</td><td>69.63</td><td>38.33</td><td>3.49</td><td>2.42</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>97.49</td><td>94.26</td><td>53.33</td><td>11.54</td><td>9.59</td></tr></table>
<table><tbody><tr><td colspan="4">消融设置</td><td colspan="5">任务组</td></tr><tr><td>P.</td><td>R.</td><td>K.</td><td>E.</td><td>木材</td><td>石头</td><td>铁</td><td>金</td><td>钻石</td></tr><tr><td></td><td></td><td></td><td></td><td>14.29</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>✓</td><td></td><td></td><td></td><td>42.95</td><td>25.67</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td>✓</td><td>✓</td><td></td><td></td><td>55.00</td><td>47.37</td><td>18.11</td><td>2.08</td><td>1.11</td></tr><tr><td>✓</td><td>✓</td><td></td><td>✓</td><td>73.53</td><td>64.20</td><td>24.19</td><td>3.08</td><td>1.86</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td></td><td>92.37</td><td>69.63</td><td>38.33</td><td>3.49</td><td>2.42</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>97.49</td><td>94.26</td><td>53.33</td><td>11.54</td><td>9.59</td></tr></tbody></table>


case, and getting the failure case, respectively.
分别用于正常情况和获取故障情况。


<table><tr><td colspan="3">Ablation Setting</td><td colspan="5">Task Group</td></tr><tr><td>Zero</td><td>Suc.</td><td>Fai.</td><td>Wood</td><td>Stone</td><td>Iron</td><td>Gold</td><td>Diamond</td></tr><tr><td>✓</td><td></td><td></td><td>92.00</td><td>79.26</td><td>36.32</td><td>4.25</td><td>3.25</td></tr><tr><td></td><td>✓</td><td></td><td>95.00</td><td>84.29</td><td>46.98</td><td>9.36</td><td>7.89</td></tr><tr><td></td><td></td><td>✓</td><td>95.00</td><td>81.10</td><td>45.47</td><td>7.50</td><td>6.39</td></tr><tr><td></td><td>✓</td><td>✓</td><td>97.49</td><td>94.26</td><td>53.33</td><td>11.54</td><td>9.59</td></tr></table>
<table><tbody><tr><td colspan="3">消融设置</td><td colspan="5">任务组</td></tr><tr><td>零</td><td>成功</td><td>失败</td><td>木</td><td>石</td><td>铁</td><td>金</td><td>钻石</td></tr><tr><td>✓</td><td></td><td></td><td>92.00</td><td>79.26</td><td>36.32</td><td>4.25</td><td>3.25</td></tr><tr><td></td><td>✓</td><td></td><td>95.00</td><td>84.29</td><td>46.98</td><td>9.36</td><td>7.89</td></tr><tr><td></td><td></td><td>✓</td><td>95.00</td><td>81.10</td><td>45.47</td><td>7.50</td><td>6.39</td></tr><tr><td></td><td>✓</td><td>✓</td><td>97.49</td><td>94.26</td><td>53.33</td><td>11.54</td><td>9.59</td></tr></tbody></table>


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_5baadd.jpg"/>



Figure 4: Illustration of the role of reflection mechanism. Without the help of reflective mechanisms, STEVE-1 [25] often gets into trouble and fails to complete the task. While Optimus-1, with the help of the Experience-Driven Reflector, leverages the AMEP to retrieve relevant experience, reflect current situation and correct errors. This improves Optimus-1's success rate on long-horizon tasks.
图4：反思机制作用示意图。缺少反思机制时，STEVE-1 [25] 经常出错并无法完成任务；而 Optimus-1 在 Experience-Driven Reflector 的帮助下，借助 AMEP 检索相关经验，反思当前情形并纠正错误，从而提高了在长时序任务中的成功率。


Evaluation Metrics. The agent always starts in survival mode, with an empty inventory. We conducted at least 30 times for each task using different world seeds and reported the average success rate to ensure fair and thorough evaluation. Additionally, we add the average steps and average time of completing the task as evaluation metrics.
评估指标。代理始终从生存模式开始，背包为空。我们对每个任务至少进行了 30 次实验，使用不同世界种子并报告平均成功率，以确保评估公平且充分。此外，我们还将完成任务的平均步数和平均时间作为评估指标。


### 3.2 Experimental Results
### 3.2 实验结果


The overall experimental results on benchmark are shown in Table 1, see the accuracy for each task in Appendix F. Optimus-1 has a success rate near ${100}\%$ on the Wood Group - Compared with Jarvis-1, Optimus-1 has 29.28% and 53.40% improvement on the Diamond Group Q and Redstone Group respectively. Optimus-1 achieves the best performance and the shortest elapsed time among all task groups. It reveals the effectiveness and efficiency of our proposed Optimus-1 framework. Moreover, compared with all baselines,Optimus-1 performance was closer (average 5.37% improvement) to human levels on long-horizon task groups.
基准上的整体实验结果见表1，单项任务的准确率见附录 F。Optimus-1 在 Wood 组的成功率接近 ${100}\%$——与 Jarvis-1 相比，Optimus-1 在 Diamond 组 Q 和 Redstone 组分别提升了 29.28% 和 53.40%。Optimus-1 在所有任务组中表现最佳且耗时最短，证明了所提框架的有效性与效率。此外，与所有基线相比，Optimus-1 在长时序任务组上更接近人类水平（平均提升 5.37%）。


### 3.3 Ablation Study
### 3.3 消融研究


We conduct extensive ablation experiments on 18 tasks, experiment setting can be found in Table 6. As shown in Table 2, we first remove Knowledge-Driven Planner and Experience-Driven Reflector, the performance of Optimus-1 on all task groups drops dramatically. It demonstrates the necessity of Knowledge-Guided Planner and Experience-Driven Reflector modules for performing long-horizon tasks. As for Hybrid Multimodal Memory, we remove HDKG from Optimus-1. Without the help of world knowledge,the performance of Optimus-1 decreased by an average of ${20}\%$ across all task groups. We then removed AMEP, this resulted in the performance of Optimus-1 decreased by an average of 12%. Finally, we performed ablation experiments on the way of retrieving cases from AMEP. As shown in Table 3, without retrieving cases from AMEP, the success rate shows an average of 10% decrease across all groups. It reveals that this reflection mechanism, which considers both success and failure cases, has a significant impact on the performance of Optimus-1. To illustrate the role of the reflection mechanism, we have shown some cases in Figure 4.
我们在 18 个任务上进行了大规模消融实验，实验设置见表6。如表2 所示，首次移除 Knowledge-Driven Planner 与 Experience-Driven Reflector 后，Optimus-1 在所有任务组的性能显著下降，说明这两模块对于执行长时序任务是必要的。关于 Hybrid Multimodal Memory，我们从 Optimus-1 中去掉了 HDKG，失去世界知识的帮助后，Optimus-1 在各任务组的平均性能下降了 ${20}\%$。随后移除 AMEP，Optimus-1 的性能平均下降了 12%。最后我们对从 AMEP 检索案例的方式做了消融，如表3 所示，不从 AMEP 检索案例时，所有组的成功率平均下降约 10%。这表明同时考虑成功与失败案例的反思机制对 Optimus-1 的性能有显著影响。为说明反思机制的作用，我们在图4 中展示了若干案例。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_b2952a.jpg"/>



Figure 5: (a) With the help of Hybrid Multimodal Memory, various MLLM-based Optimus-1 have demonstrated 2 to 6 times performance improvement. (b) Illustration of the change in Optimus-1 success rate on the unseen task over 4 epochs.
图5：（a）在 Hybrid Multimodal Memory 的帮助下，各种基于 MLLM 的 Optimus-1 展现出 2 至 6 倍的性能提升。（b）未见任务上 Optimus-1 成功率随 4 个 epoch 的变化示意。


### 3.4 Generalization Ability
### 3.4 泛化能力


In this section, we explore an interesting issue: whether generic MLLMs can effectively perform various long-horizon complex tasks in Minecraft using Hybrid Multimodal Memory. As shown in Figure 5, We employ Deepseek-VL [26] and InternLM-XComposer2-VL [6] as Knowledge-Guided Planner and Experience-Driven Reflector. The experimental results show that the original MLLM has low performance on long-horizon tasks due to the lack of knowledge and experience of Minecraft. With the assistance of Hybrid Multimodal Memory, the performance of MLLMs has improved by 2 to 6 times across various task groups, outperforming the GPT-4V baseline. This encouraging result demonstrates the generalization of the proposed Hybrid Multimodal Memory.
本节探讨一个有趣问题：通用 MLLM 能否在 Hybrid Multimodal Memory 的辅助下，有效完成 Minecraft 中的各类长时序复杂任务。如图5 所示，我们选用 Deepseek-VL [26] 与 InternLM-XComposer2-VL [6] 作为 Knowledge-Guided Planner 与 Experience-Driven Reflector。实验结果表明，原始 MLLM 因缺乏 Minecraft 的知识与经验，在长时序任务上表现低下；在 Hybrid Multimodal Memory 辅助下，MLLM 在各任务组上的性能提升了 2 至 6 倍，优于 GPT-4V 基线。该结果鼓舞人心，证明了所提 Hybrid Multimodal Memory 的泛化性。


### 3.5 Self-Evolution via Hybrid Multimodal Memory
### 3.5 通过 Hybrid Multimodal Memory 的自我进化


As shown in Section 2.3, we randomly initialize the Hybrid Multimodal Memory of Optimus-1, then update it multiple times by using the "free exploration-teacher guidance" learning method. We set the epoch to 4, and the number of learning tasks to 160. At each period, Optimus-1 performs free exploration on 150 tasks and teacher guidance learning on the remaining 10 tasks, we then evaluate Optimus-1's learning ability on the task groups same as ablation study. Experimental results are shown in Figure 5. It reveals that Optimus-1 keeps getting stronger through the continuous expansion of memory during the learning process of multiple periods. Moreover, it demonstrates that MLLM with Hybrid Multimodal Memory can incarnate an expert agent in a self-evolution manner [45].
如第 2.3 节所示，我们随机初始化 Optimus-1 的 Hybrid Multimodal Memory，然后通过“自由探索—教师引导”的学习方式多次更新。我们将 epoch 设为 4，学习任务数为 160。每个周期内，Optimus-1 在 150 个任务上进行自由探索，在剩余 10 个任务上进行教师引导学习，随后按与消融研究相同的任务组评估其学习能力。实验结果见图5，表明 Optimus-1 在多周期学习过程中通过持续扩展记忆不断变强。此外，这也证明了带有 Hybrid Multimodal Memory 的 MLLM 能以自我进化的方式成为专家代理 [45]。


## 4 Related Work
## 4 相关工作


### 4.1 Agents in Minecraft
### 4.1 Minecraft 中的代理


We summarise the differences of existing Minecraft agents in the Appendix D.3. Earlier work $\left\lbrack  {{30},{57},2,3}\right\rbrack$ introduced policy models for agents to perform simple tasks in Minecraft. MineCLIP [7] used text-video data to train a contrastive video-language model as a reward model for policy, while VPT [1] pre-trained on unlabelled videos but lacked instruction as input. Building on VPT and MineCLIP, STEVE-1 [25] added text input to generate low-level action sequences from human instructions and images. However, these agents struggle with complex tasks due to limitations in instruction comprehension and planning. Recent work $\left\lbrack  {{50},{47},{61}}\right\rbrack$ incorporated LLMs as planning and reflection modules, but lacked visual information integration for adaptive planning. MP5
我们在附录 D.3 中总结了现有 Minecraft 代理的差异。早期工作 $\left\lbrack  {{30},{57},2,3}\right\rbrack$ 引入了策略模型以使代理在 Minecraft 中执行简单任务。MineCLIP [7] 使用文本-视频数据训练对比视频-语言模型作为策略的奖励模型，而 VPT [1] 在未标注视频上进行预训练但缺乏作为输入的指令。在 VPT 与 MineCLIP 的基础上，STEVE-1 [25] 增加了文本输入以根据人类指令和图像生成低级动作序列。然而，这些代理由于在指令理解和规划方面的局限，难以完成复杂任务。近期工作 $\left\lbrack  {{50},{47},{61}}\right\rbrack$ 将大模型用作规划与反思模块，但缺乏视觉信息的整合以进行自适应规划。MP5


[33], MineDreamer [60], and Jarvis-1 [52] enhanced situation-aware planning by obtaining textual descriptions of visual information, yet lacked detailed visual data. Optimus-1 addresses these issues by directly using observation as situation-aware conditions in the planning phase, enabling more rational, visually informed planning. Additionally, unlike other agents requiring multiple queries for task refinement, Optimus-1 generates a complete and effective plan in one step with the help of HDKG. This makes Optimus-1 planning more efficient.
[33]、MineDreamer [60] 和 Jarvis-1 [52] 通过获取视觉信息的文本描述来增强情境感知规划，但缺乏详细的视觉数据。Optimus-1 通过在规划阶段直接以观测作为情境条件来解决这些问题，从而实现更合理、以视觉为依据的规划。此外，不同于需要多次查询以细化任务的其他代理，Optimus-1 借助 HDKG 能在一步内生成完整且有效的计划，使得其规划更高效。


### 4.2 Memory in Agents
### 4.2 代理中的记忆


In the agent-environment interaction process, memory is key to achieving experience accumulation [21], environment exploration [16], and knowledge abstraction [59]. There are two forms to represent memory content in LLM-based agents: textual form $\left\lbrack  {{17},{15},{31}}\right\rbrack$ and parametric form $\left\lbrack  {5,{29},{48},{20}}\right\rbrack$ . In textual form, the information is explicitly retained and recalled by natural languages. In parametric form, the memory information [38] is encoded into parameters and implicitly influences the agent's actions. Recent work $\left\lbrack  {{49},{53},{12}}\right\rbrack$ has explored the long-term visual information storage $\left\lbrack  {{18},{19}}\right\rbrack$ and summarisation in MLLM. Our proposed hybrid multimodal memory module is plug-and-play and can provide world knowledge and multimodal experience for Optimus-1 efficiently.
在代理—环境交互过程中，记忆是实现经验累积 [21]、环境探索 [16] 和知识抽象 [59] 的关键。基于大模型的代理中有两种形式来表示记忆内容：文本形式 $\left\lbrack  {{17},{15},{31}}\right\rbrack$ 和参数化形式 $\left\lbrack  {5,{29},{48},{20}}\right\rbrack$ 。在文本形式中，信息以自然语言显式保留与召回。在参数化形式中，记忆信息 [38] 被编码到参数中并隐式影响代理的行为。近期工作 $\left\lbrack  {{49},{53},{12}}\right\rbrack$ 探索了长期视觉信息的存储 $\left\lbrack  {{18},{19}}\right\rbrack$ 与多模态大模型中的摘要。我们提出的混合多模态记忆模块为即插即用，可高效为 Optimus-1 提供世界知识与多模态经验。


## 5 Conclusion
## 5 结论


In this paper, we propose Hybrid Multimodal Memory module, which consists of two parts: HDKG and AMEP. HDKG provides the necessary world knowledge for the planning phase of the agent, and AMEP provides the refined historical experience for the reflection phase of the agent. On top of the Hybrid Multimodal Memory, we construct the multimodal composable agent, Optimus-1, in Minecraft. Extensive experimental results show that Optimus-1 outperforms all existing agents on long-horizon tasks. Moreover, we validate that general-purpose MLLMs, based on Hybrid Multimodal Memory and without additional parameter updates, can exceed the powerful GPT-4V baseline. The extensive experimental results show that Optimus-1 makes a major step toward a general agent with a human-like level of performance.
本文提出了混合多模态记忆模块，该模块由两部分组成：HDKG 与 AMEP。HDKG 为代理的规划阶段提供必要的世界知识，AMEP 为代理的反思阶段提供精炼的历史经验。基于混合多模态记忆，我们在 Minecraft 上构建了可组合的多模态代理 Optimus-1。大量实验证明，Optimus-1 在长时程任务上优于所有现有代理。此外，我们验证了基于混合多模态记忆且无需额外参数更新的通用多模态大模型可超越强大的 GPT-4V 基线。大量实验结果表明，Optimus-1 向具有人类水平表现的通用代理迈出了重要一步。


## 6 Limitation and Future Work
## 6 局限与未来工作


In the framework of Optimus-1, we are dedicated to leverage proposed Hierarchical Directed Knowledge Graph and Abstracted Multimodal Experience Pool can be used to enhance the agent's ability to plan and reflect. For Action Controller, we directly introduce STEVE-1 [25] as a generator of low-level actions. However, limited by STEVE-1's ability to follow instructions and execute complex actions, Optimus-1 is weak in completing challenging tasks such as "beat ender dragon" and "build a house". Therefore, a potential future research direction is to enhance the instruction following and action generation capabilities of action controller.
在 Optimus-1 框架中，我们致力于利用所提的分层有向知识图与抽象多模态经验池来增强代理的规划与反思能力。对于动作控制器，我们直接引入 STEVE-1 [25] 作为低级动作生成器。然而，受限于 STEVE-1 在遵循指令与执行复杂动作方面的能力，Optimus-1 在完成如“击败末影龙”和“建造房屋”等挑战性任务时表现较弱。因此，未来潜在的研究方向是增强动作控制器的指令遵循与动作生成能力。


In addition, most of the work, including Optimus-1, utilize a multimodal large language model for planning and reflection, which then drives an action controller to perform the task. Building an end-to-end vision-language-action agent will be future work.
此外，包括 Optimus-1 在内的大多数工作都使用多模态大语言模型进行规划与反思，然后驱动动作控制器执行任务。构建端到端的视觉-语言-动作代理将是未来工作。


## 7 Acknowledgement
## 7 致谢


This study is supported by National Natural Science Foundation of China (Grant No. 62236003 and 62306090), Shenzhen College Stability Support Plan (Grant No. GXWD20220817144428005), Natural Science Foundation of Guangdong Province of China (Grant No. 2024A1515010147), and Major Key Project of Peng Cheng Laboratory (Grant No. PCL2023A08).
本研究得到国家自然科学基金（项目编号 62236003 与 62306090）、深圳高校稳定支持计划（项目编号 GXWD20220817144428005）、广东省自然科学基金（项目编号 2024A1515010147）和鹏城实验室重大重点项目（项目编号 PCL2023A08）的资助。


## References
## 参考文献


[1] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:24639-24654, 2022.
[1] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): 通过观看未标注的在线视频学习行动。Advances in Neural Information Processing Systems, 35:24639-24654, 2022.


[2] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13734-13744, 2023.
[2] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. 通过目标感知表征学习与自适应视野预测实现开放世界多任务控制。In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13734-13744, 2023.


[3] Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Groot: Learning to follow instructions by watching gameplay videos. In The Twelfth International Conference on Learning Representations, 2023.
[3] Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Groot: 通过观看游戏视频学习遵循指令。In The Twelfth International Conference on Learning Representations, 2023.


[4] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. Lion: Empowering multimodal large language model with dual-level visual knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.
[4] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. Lion: 以双层视觉知识增强多模态大语言模型。In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.


[5] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491-6506, 2021.
[5] Nicola De Cao, Wilker Aziz, and Ivan Titov. 在语言模型中编辑事实知识。In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491-6506, 2021.


[6] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024.
[6] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: 掌握视觉-语言大模型中自由形式文本-图像构图与理解。arXiv preprint arXiv:2401.16420, 2024.


[7] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:18343-18362, 2022.
[7] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: 利用互联网级知识构建开放式具身代理。Advances in Neural Information Processing Systems, 35:18343-18362, 2022.


[8] Mariel K Goddu and Alison Gopnik. The development of human causal learning and reasoning. Nature Reviews Psychology, pages 1-21, 2024.
[8] Mariel K Goddu and Alison Gopnik. 人类因果学习与推理的发展。Nature Reviews Psychology, pages 1-21, 2024.


[9] Maitrey Gramopadhye and Daniel Szafir. Generating executable action plans with environmentally-aware language models. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3568-3575. IEEE, 2023.
[9] Maitrey Gramopadhye and Daniel Szafir. 使用具环境感知的语言模型生成可执行行动计划。In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3568-3575. IEEE, 2023.


[10] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023.
[10] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 一个具备规划、长上下文理解与程序综合能力的真实世界网络代理。arXiv preprint arXiv:2307.12856, 2023.


[11] William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440, 2019.
[11] William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: 一个大规模的Minecraft演示数据集。arXiv preprint arXiv:1907.13440, 2019.


[12] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-Imm: Memory-augmented large multimodal model for long-term video understanding. arXiv preprint arXiv:2404.05726, 2024.
[12] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-Imm: 用于长期视频理解的记忆增强型多模态大模型。arXiv preprint arXiv:2404.05726, 2024.


[13] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871, 2023.
[13] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. 在三维世界中的具身通用代理。arXiv preprint arXiv:2311.12871, 2023.


[14] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118-9147. PMLR, 2022.
[14] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 将语言模型作为零样本规划器：为具身代理提取可执行知识。In International Conference on Machine Learning, pages 9118-9147. PMLR, 2022.


[15] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen MacNeil. Memory sandbox: Transparent and interactive memory management for conversational agents. In Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-3, 2023.
[15] 黄子恒, Sebastian Gutierrez, Hemanth Kamana, 和 Stephen MacNeil. Memory sandbox：为会话代理提供透明且可交互的内存管理. 收录于第36届年度ACM用户界面软件与技术研讨会附件论文集, 页码 1-3, 2023.


[16] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.
[16] 蒋允帆, Agrim Gupta, 张子辰, 王冠志, 窦永强, 陈延军, 李飞飞, Anima Anandkumar, 朱昱科, 和 范林曦. Vima：基于多模态提示的通用机器人操作. 收录于NeurIPS 2022“面向决策的基础模型”研讨会, 2022.


[17] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.
[17] 李大成, 召如琳, 谢安泽, 盛颖, 郑连民, Joseph Gonzalez, Ion Stoica, 马学哲, 和 张浩. 开源大模型的上下文长度究竟能保证多久？收录于NeurIPS 2023“指令微调与指令遵循”研讨会, 2023.


[18] Xiaojie Li, Shaowei He, Jianlong Wu, Yue Yu, Liqiang Nie, and Min Zhang. Mask again: Masked knowledge distillation for masked video modeling. In Proceedings of the ACM International Conference on Multimedia, page 2221-2232. ACM, 2023.
[18] 李晓洁, 何少伟, 吴建龙, 余越, 聂立强, 和 张敏. Mask again：用于掩码视频建模的掩码知识蒸馏. 收录于ACM国际多媒体会议论文集, 页码 2221-2232. ACM, 2023.


[19] Xiaojie Li, Jianlong Wu, Shaowei He, Shuo Kang, Yue Yu, Liqiang Nie, and Min Zhang. Fine-grained key-value memory enhanced predictor for video representation learning. In Proceedings of the ACM International Conference on Multimedia, page 2264-2274. ACM, 2023.
[19] 李晓洁, 吴建龙, 何少伟, 康硕, 余越, 聂立强, 和 张敏. 细粒度键值记忆增强预测器用于视频表示学习. 收录于ACM国际多媒体会议论文集, 页码 2264-2274. ACM, 2023.


[20] Xiaojie Li, Yibo Yang, Xiangtai Li, Jianlong Wu, Yue Yu, Bernard Ghanem, and Min Zhang. Genview: Enhancing view quality with pretrained generative model for self-supervised learning. In Proceedings of the European Conference on Computer Vision. Springer, 2024.
[20] 李晓洁, 杨逸博, 李相泰, 吴建龙, 余越, Bernard Ghanem, 和 张敏. Genview：利用预训练生成模型提升视图质量以用于自监督学习. 收录于欧洲计算机视觉会议论文集. Springer, 2024.


[21] Xiaojie Li, Yibo Yang, Jianlong Wu, Bernard Ghanem, Liqiang Nie, and Min Zhang. Mamba-fscil: Dynamic adaptation with selective state space model for few-shot class-incremental learning. arXiv preprint arXiv:2407.06136, 2024.
[21] 李晓洁, 杨逸博, 吴建龙, Bernard Ghanem, 聂立强, 和 张敏. Mamba-fscil：用于少样本类增量学习的选择性状态空间模型的动态自适应. arXiv 预印本 arXiv:2407.06136, 2024.


[22] Zaijing Li, Fengxiao Tang, Ming Zhao, and Yusen Zhu. Emocaps: Emotion capsule based model for conversational emotion recognition. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1610-1618, 2022.
[22] 李在京, 唐丰潇, 赵铭, 和 朱雨森. Emocaps：基于情感胶囊的会话情感识别模型. 收录于ACL 2022发现论文, 页码1610-1618, 2022.


[23] Zaijing Li, Ting-En Lin, Yuchuan Wu, Meng Liu, Fengxiao Tang, Ming Zhao, and Yongbin Li. Unisa: Unified generative framework for sentiment analysis. In Proceedings of the 31st ACM International Conference on Multimedia, pages 6132-6142, 2023.
[23] 李在京, 林庭恩, 吴宇川, 刘猛, 唐丰潇, 赵铭, 和 李永斌. Unisa：用于情感分析的统一生成框架. 收录于第31届ACM国际多媒体会议论文集, 页码6132-6142, 2023.


[24] Zaijing Li, Gongwei Chen, Rui Shao, Dongmei Jiang, and Liqiang Nie. Enhancing the emotional generation capability of large language models via emotional chain-of-thought. arXiv preprint arXiv:2401.06836, 2024.
[24] 李在京, 陈公伟, 邵睿, 江冬梅, 和 聂立强. 通过情感思维链增强大语言模型的情感生成能力. arXiv 预印本 arXiv:2401.06836, 2024.


[25] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: A generative model for text-to-behavior in minecraft. Advances in Neural Information Processing Systems, 2023.
[25] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, 和 Sheila McIlraith. Steve-1：用于Minecraft的文本到行为生成模型. Advances in Neural Information Processing Systems, 2023.


[26] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024.
[26] 陆浩宇, 刘雯, 张博, 王炳轩, 董凯, 刘波, 孙景祥, 任同政, 李卓舒, 孙垚峰, 等. Deepseek-vl：迈向真实世界的视觉-语言理解. arXiv 预印本 arXiv:2403.05525, 2024.


[27] Qi Lv, Xiang Deng, Gongwei Chen, Michael Y Wang, and Liqiang Nie. Decision mamba: A multi-grained state space model with self-evolution regularization for offline rl. In NeurIPS, 2024.
[27] 吕琦, 邓翔, 陈公伟, Michael Y Wang, 和 聂立强. Decision mamba：具有自演化正则化的多粒度状态空间模型用于离线强化学习. 收录于NeurIPS, 2024.


[28] Simon Makin. The amyloid hypothesis on trial. Nature, 559(7715):S4-S4, 2018.
[28] Simon Makin. 淀粉样蛋白假说受审. Nature, 559(7715):S4-S4, 2018.


[29] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model editing at scale. In International Conference on Learning Representations, 2021.
[29] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, 和 Christopher D Manning. 大规模快速模型编辑. 收录于国际学习表征大会(ICLR), 2021.


[30] Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning, pages 2661-2670. PMLR, 2017.
[30] Junhyuk Oh, Satinder Singh, Honglak Lee, 和 Pushmeet Kohli. 通过多任务深度强化学习实现零样本任务泛化. 收录于第34届国际机器学习大会论文集, 页码2661-2670. PMLR, 2017.


[31] Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Sid-dartha Naidu. Giraffe: Adventures in expanding context lengths in llms. arXiv preprint arXiv:2308.10882, 2023.
[31] Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. Giraffe: 在扩展 llms 上下文长度的冒险. arXiv 预印本 arXiv:2308.10882, 2023.


[32] Eileen Parkes. Scientific progress is built on failure. Nature, 10, 2019.
[32] Eileen Parkes. 科学进步建立在失败之上. Nature, 10, 2019.


[33] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing Shao. Mp5: A multi-modal open-ended embodied system in minecraft via active perception. arXiv preprint arXiv:2312.07472, 2023.
[33] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing Shao. Mp5: 通过主动感知在 Minecraft 中的多模态开放式具身系统. arXiv 预印本 arXiv:2312.07472, 2023.


[34] Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, et al. Scaling instructable agents across many simulated worlds. arXiv preprint arXiv:2404.10179, 2024.
[34] Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, et al. 在众多模拟世界中扩展可指令代理. arXiv 预印本 arXiv:2404.10179, 2024.


[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.
[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 从自然语言监督中学习可迁移的视觉模型. 见 International conference on machine learning, 页 8748-8763. PMLR, 2021.


[36] Rui Shao, Xiangyuan Lan, Jiawei Li, and Pong C Yuen. Multi-adversarial discriminative deep domain generalization for face presentation attack detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10023-10031, 2019.
[36] Rui Shao, Xiangyuan Lan, Jiawei Li, and Pong C Yuen. 面向面部呈现攻击检测的多对抗判别深度领域泛化. 见 Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 页 10023-10031, 2019.


[37] Rui Shao, Tianxing Wu, and Ziwei Liu. Detecting and grounding multi-modal media manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6904-6913, 2023.
[37] Rui Shao, Tianxing Wu, and Ziwei Liu. 检测与定位多模态媒体篡改. 见 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 页 6904-6913, 2023.


[38] Rui Shao, Tianxing Wu, Jianlong Wu, Liqiang Nie, and Ziwei Liu. Detecting and grounding multi-modal media manipulation and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.
[38] Rui Shao, Tianxing Wu, Jianlong Wu, Liqiang Nie, and Ziwei Liu. 检测与定位多模态媒体篡改及其延伸. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.


[39] Leyang Shen, Gongwei Chen, Rui Shao, Weili Guan, and Liqiang Nie. Mome: Mixture of multimodal experts for generalist multimodal large language models. In NeurIPS, 2024.
[39] Leyang Shen, Gongwei Chen, Rui Shao, Weili Guan, and Liqiang Nie. MOME: 面向通用多模态大语言模型的多模态专家混合. 见 NeurIPS, 2024.


[40] Shan H Siddiqi, Konrad P Kording, Josef Parvizi, and Michael D Fox. Causal mapping of human brain function. Nature reviews neuroscience, pages 361-375, 2022.
[40] Shan H Siddiqi, Konrad P Kording, Josef Parvizi, and Michael D Fox. 人类大脑功能的因果映射. Nature reviews neuroscience, 页 361-375, 2022.


[41] JF Stein. Role of the cerebellum in the visual guidance of movement. Nature, pages 217-221, 1986.
[41] JF Stein. 小脑在视觉引导运动中的作用. Nature, 页 217-221, 1986.


[42] Tim Stuart, Andrew Butler, Paul Hoffman, Christoph Hafemeister, Efthymia Papalexi, William M Mauck, Yuhan Hao, Marlon Stoeckius, Peter Smibert, and Rahul Satija. Comprehensive integration of single-cell data. cell, 177(7):1888-1902, 2019.
[42] Tim Stuart, Andrew Butler, Paul Hoffman, Christoph Hafemeister, Efthymia Papalexi, William M Mauck, Yuhan Hao, Marlon Stoeckius, Peter Smibert, and Rahul Satija. 单细胞数据的全面整合. cell, 177(7):1888-1902, 2019.


[43] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286, 2023.
[43] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. 生成性多模态模型是上下文内学习者. arXiv 预印本 arXiv:2312.13286, 2023.


[44] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards general computer control: A multimodal agent for red dead redemption ii as a case study. arXiv preprint arXiv:2403.03186, 2024.
[44] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. 面向通用计算控制：以《荒野大镖客 II》为案例的多模态代理. arXiv 预印本 arXiv:2403.03186, 2024.


[45] Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. A survey on self-evolution of large language models. arXiv preprint arXiv:2404.14387, 2024.
[45] Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. 关于大语言模型自我进化的综述. arXiv 预印本 arXiv:2404.14387, 2024.


[46] Deniz Vatansever, Jonathan Smallwood, and Elizabeth Jefferies. Varying demands for cognitive control reveals shared neural processes supporting semantic and episodic memory retrieval. Nature communications, 12(1):2134, 2021.
[46] Deniz Vatansever, Jonathan Smallwood, and Elizabeth Jefferies. 认知控制需求的变化揭示了支持语义与情节记忆检索的共享神经过程. Nature communications, 12(1):2134, 2021.


[47] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.
[47] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: 一种结合大语言模型的开放式具身代理. arXiv preprint arXiv:2305.16291, 2023.


[48] Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, and Huajun Chen. Detoxifying large language models via knowledge editing. arXiv preprint arXiv:2403.14472, 2024.
[48] Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, and Huajun Chen. 通过知识编辑对大型语言模型进行去毒化. arXiv preprint arXiv:2403.14472, 2024.


[49] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and Jose M Alvarez. Omnidrive: A holistic llm-agent framework for autonomous driving with 3d perception, reasoning and planning. arXiv preprint arXiv:2405.01533, 2024.
[49] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and Jose M Alvarez. Omnidrive: 一个用于自动驾驶的整体型 llm-代理框架，具备 3D 感知、推理与规划. arXiv preprint arXiv:2405.01533, 2024.


[50] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.
[50] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. 描述、解释、规划与选择：利用大型语言模型的交互式规划使开放世界多任务代理成为可能. arXiv preprint arXiv:2302.01560, 2023.


[51] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.
[51] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. 描述、解释、规划与选择：利用大型语言模型的交互式规划使开放世界多任务代理成为可能. arXiv preprint arXiv:2302.01560, 2023.


[52] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. arXiv preprint arXiv:2311.05997, 2023.
[52] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: 具备记忆增强多模态语言模型的开放世界多任务代理. arXiv preprint arXiv:2311.05997, 2023.


[53] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. arXiv preprint arXiv:2404.03384, 2024.
[53] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: 通过大型语言模型实现高效长视频理解. arXiv preprint arXiv:2404.03384, 2024.


[54] Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. Exploring diverse in-context configurations for image captioning. Advances in Neural Information Processing Systems, 36, 2024.
[54] Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. 探索用于图像字幕的多样化示例内配置. Advances in Neural Information Processing Systems, 36, 2024.


[55] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023.
[55] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: 将多模态代理作为智能手机用户. arXiv preprint arXiv:2312.13771, 2023.


[56] Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and Xiaochun Cao. Cat: Enhancing multimodal large language model to answer questions in dynamic audio-visual scenarios. In ECCV, 2024.
[56] Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and Xiaochun Cao. Cat: 在动态视听场景中增强多模态大语言模型以回答问题. In ECCV, 2024.


[57] Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing Lu. Skill reinforcement learning and planning for open-world long-horizon tasks. arXiv preprint arXiv:2303.16563, 2023.
[57] Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing Lu. 技能强化学习与规划用于开放世界长时程任务. arXiv preprint arXiv:2303.16563, 2023.


[58] Haoyu Zhang, Meng Liu, Zixin Liu, Xuemeng Song, Yaowei Wang, and Liqiang Nie. Multifactor adaptive vision selection for egocentric video question answering. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 59310-59328. PMLR, 2024.
[58] Haoyu Zhang, Meng Liu, Zixin Liu, Xuemeng Song, Yaowei Wang, and Liqiang Nie. 用于第一人称视频问答的多因子自适应视觉选择. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 59310-59328. PMLR, 2024.


[59] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism of large language model based agents. arXiv preprint arXiv:2404.13501, 2024.
[59] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 基于大型语言模型代理的记忆机制综述. arXiv preprint arXiv:2404.13501, 2024.


[60] Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and Jing Shao. Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-world control. arXiv preprint arXiv:2403.12037, 2024.
[60] Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and Jing Shao. Minedreamer: 通过想象链学习遵循指令以用于模拟世界控制. arXiv preprint arXiv:2403.12037, 2024.


[61] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.
[61] 朱熙州、陈云涛、田浩、陶晨昕、苏伟杰、杨辰宇、黄高、李斌、卢乐为、王晓刚 等人。Ghost in the minecraft：通过具有基于文本知识与记忆的大型语言模型构建面向开放世界环境的一般能力代理。arXiv 预印本 arXiv:2305.17144, 2023。


## A Broader Impact
## 更广泛的影响


With the increasing capability level of Multimodal Large Language Models (MLLM) comes many potential benefits and also risks. On the positive side, we anticipate that the techniques that used to create Optimus-1 could be applied to the creation of helpful agents in robotics, video games, and the web. This plug-and-play architecture that we have created can be quickly adapted to different MLLMs, and the proposed methods also provide a viable solution for other application areas in the agent domain. However, on the negative side, it is imperative to acknowledge the inherent stochastic nature of MLLMs in text generation. If not addressed carefully, this could lead to devastating consequences for society. Prior to deploying MLLMs in conjunction with the Hybrid Multimodal Memory methodology, a comprehensive assessment of their potential risks must be undertaken. We hope that while the stakes are low, works such as ours can improve access to safety research on instruction-following models in multimodal agents domains.
随着多模态大型语言模型（MLLM）能力的提升，其带来许多潜在益处与风险。从积极方面看，我们预期用于构建 Optimus-1 的技术可被应用于机器人、电子游戏和网络中的有用代理。我们构建的即插即用架构可以快速适配不同 MLLM，所提方法也为代理领域的其他应用提供了可行方案。但从消极方面，必须承认 MLLM 在文本生成上固有的随机性。若不谨慎处理，可能对社会造成严重后果。在将 MLLM 与混合多模态记忆方法结合部署前，必须进行全面的风险评估。我们希望在风险较低的情况下，像我们这样的工作能促进多模态代理中遵从指令模型的安全研究可及性。


## B Minecraft
## B Minecraft


Minecraft is an extremely popular sandbox video game developed by Mojang Studios ${}^{3}$ . It allows players to explore a blockly, procedurally generated 3D world with infinite terrain, discover and extract raw materials, craft tools and items, and build structures or earthworks (shown in Figure 6). Minecraft is a valuable and representative environment for evaluating long-horizon tasks, offering greater diversity and complexity compared to other environments. Unlike web/app navigation [55] and embodied manipulation [16], Minecraft is an open world with a complex and dynamic environment (79 biomes, including ocean, plains, forest, desert, etc.). To complete long-horizon tasks, agents must achieve multiple sub-goals (e.g., 15 sub-goals to craft a diamond sword), making the construction of a Minecraft agent quite challenging. Many studies [47, 33, 52] have chosen Minecraft as the environment for validating performance on long-horizon tasks. Extensive experimental results in the paper show that Optimus-1 outperforms all baselines. Therefore, we chose Minecraft as open-world environment to evaluate the ability of agents to perform long-horizon tasks.
Minecraft 是 Mojang Studios 开发的一款极受欢迎的沙盒电子游戏 ${}^{3}$ 。它允许玩家在带方块的程序生成三维世界中探索无限地形、发现并开采原材料、制作工具与物品，以及建造结构或土木工程（见图6）。Minecraft 是评估长时序任务的有价值且具有代表性的环境，相比其他环境提供更高的多样性与复杂性。与网页/应用导航 [55] 和具身操控 [16] 不同，Minecraft 是一个开放世界，环境复杂且动态（79 种生物群系，包括海洋、平原、森林、沙漠等）。要完成长时序任务，代理必须达成多个子目标（例如制作一把钻石剑需 15 个子目标），这使得构建 Minecraft 代理非常具有挑战性。许多研究 [47, 33, 52] 选择 Minecraft 作为验证长时序任务性能的环境。论文中的大量实验结果表明 Optimus-1 优于所有基线。因此，我们选择 Minecraft 作为开放世界环境来评估代理执行长时序任务的能力。


### B.1 Basic Rules
### B.1 基本规则


Biomes. The Minecraft world is divided into different areas called "biomes". Different biomes contain different blocks and plants and change how the land is shaped. There are 79 biomes in Minecraft 1.16.5, including ocean, plains, forest, desert, etc. Diverse environments have high requirements for the generalization of agents.
生物群系。Minecraft 世界被划分为称为“生物群系”的不同区域。不同生物群系包含不同的方块与植物，并改变地形的形成。Minecraft 1.16.5 中有 79 种生物群系，包括海洋、平原、森林、沙漠等。多样的环境对代理的泛化能力提出了很高的要求。


Time. Time passes within this world, and a game day lasts for 20 real-world minutes. Nighttime is much more dangerous than daytime: the game starts at dawn, and agents have 10 minutes of game time before nightfall. Hostile or neutral mobs spawn when night falls, and most of these mobs are dangerous, trying to attack agents. How to survive in such a dangerous world is an open problem for Minecraft agents research.
时间。该世界中会流逝时间，一天游戏时长为 20 分钟现实时间。夜晚比白天危险得多：游戏从黎明开始，代理在夜幕降临前有 10 分钟游戏时间。夜晚降临时会生成敌对或中立生物群，大多数这些生物具有攻击性，会试图袭击代理。如何在如此危险的世界中生存是 Minecraft 代理研究的一项开放问题。


Item. In Minecraft 1.16.5, there are 975 items can be obtained, such as wooden pickaxe 7, iron sword &. Item can be obtained by crafting or destroying blocks or attacking entities. For example, agent can attack cows $\nabla$ to obtain leather $\nabla$ and beef $\nabla$ . Agent also can use 1 stick $\angle$ and 2 diamonds 0 to craft diamond sword $\mathcal{L}$ .
物品。在 Minecraft 1.16.5 中，可获得 975 种物品，例如木镐、铁剑等。物品可通过制作、摧毁方块或攻击实体获得。例如，代理可以攻击牛 $\nabla$ 以获得皮革 $\nabla$ 和牛肉 $\nabla$ 。代理也可以用 1 根木棍 $\angle$ 和 2 颗钻石 $\nabla$ 制作钻石剑 $\mathcal{L}$ 。


Gameplay progress. Progression primarily involves discovering and utilizing various materials and resources, each of which unlocks new capabilities and options. For instance, crafting a wooden pickaxe $\nearrow$ enables the player to mine stone and a furnace $\nabla$ ; these,in turn,allow for the mining and smelting of iron or $\nabla$ . Subsequently, an iron pickaxe $\nearrow$ permits the extraction of diamonds $\odot$ ,and a diamond pickaxe $\nearrow$ can mine virtually any block in the game. Similarly, cultivating different crops allows for the breeding of various animals, each providing distinct resources beyond mere sustenance. Enemy drops also have specific applications, with some being more beneficial than others. By integrating resources from mining, farming, and breeding, players can enchant their equipment. The collection and crafting of materials also facilitate construction, enabling players to build diverse structures. Beyond practical considerations such as secure bases and farms, the creative aspect of building personalized structures constitutes a significant part of the Minecraft experience.
游戏进程。进展主要通过发现和利用各种材料与资源来实现，每种资源都能解锁新的能力与选项。例如，制作木镐 $\nearrow$ 使玩家能够采集石头和熔炉 $\nabla$；这些反过来又允许开采与熔炼铁或 $\nabla$。随后，铁镐 $\nearrow$ 可以开采钻石 $\odot$，而钻石镐 $\nearrow$ 几乎能采掘游戏中任何方块。同样，种植不同作物可用于繁育多种动物，每种动物提供除食物外的独特资源。敌人掉落物也有特定用途，某些掉落物比其他的更有价值。通过整合采矿、耕作和繁育的资源，玩家可以附魔装备。材料的收集与制作也便于建造，使玩家能够建造各式结构。除了诸如安全基地与农场等实用要素外，建造具有个人风格的创造性结构也是 Minecraft 体验的重要组成部分。


---



${}^{3}$ https://www.minecraft.net/en-us/article/meet-mojang-studios
${}^{3}$ https://www.minecraft.net/en-us/article/meet-mojang-studios


---



<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_507c27.jpg"/>



Figure 6: The screenshots in Minecraft. (a). The world has different complex terrains, including plains, river, forest and mine. (b). The agent can use crafting table to craft tools and items with recipes. (c). The agent can use the furnace to smelt ore to obtain precious ingot. (d). The agent can grow wheat near the river.
图6：Minecraft 的截图。（a）世界包含多种复杂地形，包括平原、河流、森林和矿井。（b）代理可使用工作台按配方合成工具和物品。（c）代理可使用熔炉熔炼矿石以获得贵重锭子。（d）代理可在河边种植小麦。


Freedom. In Minecraft, player can do anything they can imagine. Player can craft tools, smelt ore, brew potions, trade with villagers and wandering traders, attack mobs, grow crops, raise animals in captivity, etc. Player even can use redstone $\nabla$ to build a computer. This is a world of freedom and infinite possibilities.
自由。在Minecraft中，玩家可以做任何他们能想象的事。玩家可以合成工具、熔炼矿石、酿制药水、与村民和流浪商人交易、攻击怪物、种植农作物、圈养养殖等。玩家甚至可以使用红石 $\nabla$ 搭建计算机。这是一个自由与无限可能的世界。


More Challenge than Diamond (C). Progression beyond the Overworld is fairly limited: Eventually, you can build a nether portal to reach the Nether, where you can get materials for more complex crafting, the resources to brew potions, and the top tier of tools and armor. The Nether materials also let you reach the End dimension, where you must defeat the Ender Dragon to unlock the outer End Islands, where you can get an elytra that lets you fly, and shulker boxes for more storage.
比钻石更具挑战（C）。超越主世界的进展相当有限：最终你可以建造下界传送门前往下界，在那里可获得用于更复杂合成的材料、酿制药水的资源以及顶级工具与护甲。下界材料也能让你到达末地，在那里你必须击败末影龙以解锁外部末地群岛，在那里可以获取能让你飞行的鞘翅和用于更多存储的苦力怕箱。


### B.2 Observation and Action Spaces
### B.2 观测与动作空间


Observation. Our observation space is completely consistent with human players. The agent only receives an RGB image with dimensions of ${640} \times  {360}$ during the gameplay process,including the hotbar, health indicators, food saturation, and animations of the player's hands. It is worth helping the agent see more clearly in extremely dark environments, we have added a night vision effect for the agent, which increases the brightness of the environment during the night.
观测。我们的观测空间与人类玩家完全一致。代理在游戏过程中仅接收一张尺寸为 ${640} \times  {360}$ 的 RGB 图像，包括快捷栏、生命指示、饱食度以及玩家双手的动画。为了帮助代理在极暗环境中看得更清晰，我们为代理增加了夜视效果，在夜间提升环境亮度。


Action Spaces. Our action space is almost similar to human players, except for craft and smelt actions. It consists of two parts: the mouse and the keyboard. The keypresses are responsible for controlling the movement of agents, such as jumping, forward, back, etc. The mouse movements are responsible for controlling the perspective of agents and the cursor movements when the GUI is opened. The left and right buttons of the mouse are responsible for attacking and using or placing items. In Minecraft, precise mouse movements are important when completing complex tasks that need open inventory or crafting table. In order to achieve both the same action space with MineDojo
动作空间。我们的动作空间与人类玩家几乎相同，除合成与熔炼动作外。它由两部分组成：鼠标与键盘。按键负责控制代理的移动，如跳跃、前进、后退等。鼠标移动负责控制代理视角及打开界面时的光标移动。鼠标左右键负责攻击和使用或放置物品。在Minecraft中，完成需要打开物品栏或工作台的复杂任务时，精确的鼠标移动非常重要。为实现与 MineDojo 相同的动作空间


Table 4: Our action space.
表4：我们的动作空间。


<table><tr><td>Index</td><td>Action</td><td>Human Action</td><td>Description</td></tr><tr><td>1</td><td>Forward</td><td>key W</td><td>Move forward.</td></tr><tr><td>2</td><td>Back</td><td>key S</td><td>Move back.</td></tr><tr><td>3</td><td>Left</td><td>key A</td><td>Strafe left.</td></tr><tr><td>4</td><td>Right</td><td>key D</td><td>Strafe right.</td></tr><tr><td>5</td><td>Jump</td><td>key Space</td><td>Jump. When swimming, keeps the player afloat.</td></tr><tr><td>6</td><td>Sneak</td><td>key left Shift</td><td>Slowly move in the current direction of movement.</td></tr><tr><td>7</td><td>Sprint</td><td>key left Ctrl</td><td>Move quickly in the direction of current movement.</td></tr><tr><td>8</td><td>Attack</td><td>left Button</td><td>Destroy blocks (hold down); Attack entity (click once).</td></tr><tr><td>9</td><td>Use</td><td>right Button</td><td>Place blocks, entity, open items or other interact actions defined by game.</td></tr><tr><td>10</td><td>hotbar [1-9]</td><td>keys 1-9</td><td>Selects the appropriate hotbar item.</td></tr><tr><td>11</td><td>Open/Close Inventory</td><td>key E</td><td>Opens the Inventory. Close any open GUI.</td></tr><tr><td>12</td><td>Yaw</td><td>move Mouse X</td><td>Turning; aiming; camera movement.Ranging from -180 to +180.</td></tr><tr><td>13</td><td>Pitch</td><td>move Mouse Y</td><td>Turning; aiming; camera movement.Ranging from -180 to +180.</td></tr><tr><td>14</td><td>Craft</td><td>-</td><td>Execute a crafting recipe to obtain new item</td></tr><tr><td>15</td><td>Smelt</td><td>-</td><td>Execute a smelting recipe to obtain new item.</td></tr></table>
<table><tbody><tr><td>索引</td><td>操作</td><td>玩家操作</td><td>描述</td></tr><tr><td>1</td><td>前进</td><td>按键 W</td><td>前进。</td></tr><tr><td>2</td><td>后退</td><td>按键 S</td><td>后退。</td></tr><tr><td>3</td><td>左移</td><td>按键 A</td><td>向左平移。</td></tr><tr><td>4</td><td>右移</td><td>按键 D</td><td>向右平移。</td></tr><tr><td>5</td><td>跳跃</td><td>按键 空格</td><td>跳跃。游泳时保持漂浮。</td></tr><tr><td>6</td><td>潜行</td><td>按键 左 Shift</td><td>以当前移动方向缓慢移动。</td></tr><tr><td>7</td><td>冲刺</td><td>按键 左 Ctrl</td><td>以当前移动方向快速移动。</td></tr><tr><td>8</td><td>攻击</td><td>左键</td><td>破坏方块（按住）；攻击实体（点击一次）。</td></tr><tr><td>9</td><td>使用</td><td>右键</td><td>放置方块、与实体互动、打开物品或游戏定义的其他互动动作。</td></tr><tr><td>10</td><td>快捷栏 [1-9]</td><td>按键 1-9</td><td>选择对应的快捷栏物品。</td></tr><tr><td>11</td><td>打开/关闭物品栏</td><td>按键 E</td><td>打开物品栏。关闭任何打开的界面。</td></tr><tr><td>12</td><td>偏航</td><td>移动 鼠标 X</td><td>转向；瞄准；摄像机移动。范围从 -180 到 +180。</td></tr><tr><td>13</td><td>俯仰</td><td>移动 鼠标 Y</td><td>转向；瞄准；摄像机移动。范围从 -180 到 +180。</td></tr><tr><td>14</td><td>合成</td><td>-</td><td>执行合成配方以获得新物品</td></tr><tr><td>15</td><td>熔炼</td><td>-</td><td>执行熔炼配方以获得新物品。</td></tr></tbody></table>


[7], we abstract the craft and the smelt action into action space. The detailed action space is described in Table 4.
[7], 我们将制作和熔炼动作抽象为动作空间。详细的动作空间见表4。


### B.3 Long-horizon Tasks
### B.3 长期任务


Long-horizon Tasks are complex tasks that require world knowledge to solve and consist of multiple indispensable subtask sequences. In Minecraft, technology has six levels, including wood $\nabla$ ,stone -,iron $\varphi$ ,golden $\varphi$ ,diamond $\Theta$ ,and netherite $\varphi$ . Wooden tools can mine stone-level blocks,but can't mine iron-level and upper-level blocks. Stone tools can mine iron-level blocks, but can't mine diamond-level and upper-level blocks. Iron-level tools can mine diamond-level blocks, but can't mine netherite-level blocks. Diamond-level tools can mine any level blocks.
长期任务是需要世界知识才能解决并由多个不可或缺的子任务序列组成的复杂任务。在 Minecraft 中，工具有六个等级，包括木制 $\nabla$、石制 -、铁制 $\varphi$、金制 $\varphi$、钻石 $\Theta$ 和下界合金 $\varphi$。木制工具可以采掘石级方块，但不能采掘铁级及以上方块。石质工具可以采掘铁级方块，但不能采掘钻石级及以上方块。铁级工具可以采掘钻石级方块，但不能采掘下界合金级方块。钻石级工具可以采掘任意等级的方块。


For example, the agent now wants to complete the task "Craft iron sword &". The agent needs to craft wood-level tools to mine stone2,and craft stone-level tools to mine iron ore2. In order to craft tools, the agent needs a crafting table (e). To smelt iron ore - into iron ingot -, the agent needs a furnace summary, the agent needs to obtain many raw materials, wood-level and stone-level tools, 1 crafting table, 1 furnace, and most importantly, 2 iron ingots. The process of this task is shown in Figure 7.
例如，代理现在想完成任务 “Craft iron sword &”。代理需要制作木级工具来采掘石2，并制作石级工具来采掘铁矿石2。为了制作工具，代理需要一个工作台 (e)。要把铁矿石 - 熔炼成铁锭 -，代理需要一个熔炉摘要，代理需要获取大量原材料、木级和石级工具、1 个工作台、1 个熔炉，最重要的是 2 个铁锭。该任务的流程如图7所示。


## C Theory
## C 理论


In this section, we briefly introduce the relevant theory of cognitive science. For more details, please refer to the original articles.
在本节中，我们简要介绍认知科学相关理论。更多细节请参阅原文。


Our ability to understand and predict the world around us depends on our long-term memory stores, which have historically been divided into two distinct systems [28, 42, 46]. The semantic memory system provides a conceptual framework for describing the similar meanings of words and objects as they are encountered in different contexts (e.g., a bee is a flying insect with yellow and black stripes that produces honey), whereas the episodic memory system records our personal experiences characterized by the co-occurrence of words and objects at different times and places (e.g., being stung by a bee while eating honey at a picnic last weekend). These information stores and the interactions between them play a crucial role in guiding our behaviour and giving us the flexibility to adapt to the various demands of our environment.
我们理解和预测周围世界的能力依赖于长期记忆存储，历来被划分为两个不同系统 [28, 42, 46]。语义记忆系统为在不同情境中遇到的词语和物体提供描述其相似含义的概念框架（例如：蜜蜂是一种有黄黑条纹并产蜜的会飞昆虫），而情景记忆系统记录以词语和物体在不同时间地点共同出现为特征的个人经历（例如：上周末在野餐时一边吃蜂蜜一边被蜜蜂蜇）。这些信息存储及其相互作用在指导我们的行为并使我们能灵活适应环境需求方面起着关键作用。


In this paper, inspired by the above theory, we divide the agent memory module into two parts: knowledge and experience. Based on this, we propose Hierarchical Directed Knowledge Graph and Abstracted Multimodal Experience Pool to enable the agent to acquire, store, and utilize knowledge and experience during the execution of tasks. Extensive experimental results demonstrate the effectiveness of the proposed methodology.
在本文中，受上述理论启发，我们将代理记忆模块划分为知识与经验两部分。在此基础上，我们提出了分层定向知识图与抽象多模态经验池，使代理在执行任务时能够获取、存储并利用知识与经验。大量实验结果验证了所提方法的有效性。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_dde9b6.jpg"/>



Figure 7: Processing of task "Craft 1 iron sword". Optimus-1 needs thousands of steps to complete this task. To craft and smelt precisely, the mouse movements action can't have any error.
图7：任务 “Craft 1 iron sword” 的处理过程。Optimus-1 需要成千上万步才能完成该任务。为了精确制作和熔炼，鼠标移动动作不能有任何误差。


## D Benchmark Suite
## D 基准套件


### D.1 Benchmark
### D.1 基准


We constructed a benchmark of 67 tasks to evaluate Optimus-1's ability to complete long-horizon tasks in Minecraft. According to recommended categories in Minecraft, we have classified these tasks into 7 groups: Wood $\square$ ,Stone $\square$ ,Iron $\square$ ,Gold $\square$ ,Diamond $\square$ ,Redstone $\square$ ,Armor $\square$ . The statistics for benchmark are shown in Table 5. Due to the varying complexity of these tasks, we adopt different maximum gameplay steps (Max. Steps) for each task. The maximum steps are determined by the average steps that human players need to complete the task. Due to the randomness of Minecraft, the world and initial spawn point of the agent could vary a lot. In our benchmark setting, We initialize the agent with an empty inventory, which makes it necessary for the agent to complete a series of sub-goals (mining materials, crafting tools) in order to perform any tasks. This makes every task challenging, even for human players.
我们构建了包含67个任务的基准来评估 Optimus-1 在 Minecraft 中完成长期任务的能力。根据 Minecraft 的推荐类别，我们将这些任务分为7组：木制 $\square$、石制 $\square$、铁制 $\square$、金制 $\square$、钻石 $\square$、红石 $\square$、护甲 $\square$。基准统计见表5。由于这些任务复杂度各不相同，我们为每个任务采用不同的最大游戏步数（Max. Steps）。最大步数由人类玩家完成任务所需的平均步数决定。由于 Minecraft 的随机性，世界和代理的初始生成点可能差异很大。在我们的基准设置中，我们将代理初始化为空背包，这使得代理必须完成一系列子目标（采掘材料、制作工具）才能执行任何任务。这使得每个任务即便对人类玩家也具有挑战性。


Note that Diamonds are a very rare item that only spawns in levels 2 to 16 and have a 0.0846% chance of spawning in Minecraft 1.16.5. Diamonds are usually found near level 9, or in man-made or natural mines no higher than level 16. To mitigate the significant impact of diamond generation probability on the agent's likelihood of successfully completing the task, we have adjusted the
注意钻石是非常稀有的物品，只在2到16层生成，在 Minecraft 1.16.5 中的生成概率为0.0846%。钻石通常在9层附近或不高于16层的人工或天然矿井中发现。为减轻钻石生成概率对代理成功完成任务可能产生的重大影响，我们已对


Table 5: Setting of 7 groups encompassing 67 Minecraft long-horizon tasks.
表5：涵盖67个 Minecraft 长期任务的7组设置。


<table><tr><td>Group</td><td>Task Num.</td><td>Example Task</td><td>Max. Steps</td><td>Initial Inventory</td><td>Avg. Sub-goal Num.</td></tr><tr><td>Wooden</td><td>10</td><td>Craft a wooden axe</td><td>3600</td><td>Empty</td><td>5</td></tr><tr><td>Stone</td><td>9</td><td>Craft one stone pickaxe</td><td>7200</td><td>Empty</td><td>9</td></tr><tr><td>GIRON</td><td>16</td><td>Craft a iron pickaxe</td><td>12000</td><td>Empty</td><td>13</td></tr><tr><td>Golden</td><td>6</td><td>Mine gold and smelt into golden ingot</td><td>36000</td><td>Empty</td><td>16</td></tr><tr><td>Redstone</td><td>6</td><td>Craft a piston</td><td>36000</td><td>Empty</td><td>17</td></tr><tr><td>○ Diamond</td><td>7</td><td>Dig down and mine a diamond</td><td>36000</td><td>Empty</td><td>15</td></tr><tr><td>£ Armor</td><td>13</td><td>Craft one iron helmet</td><td>36000</td><td>Empty</td><td>16</td></tr></table>
<table><tbody><tr><td>组别</td><td>任务编号</td><td>示例任务</td><td>最多步数</td><td>初始物品栏</td><td>平均子目标数</td></tr><tr><td>木制</td><td>10</td><td>制作一把木斧</td><td>3600</td><td>空</td><td>5</td></tr><tr><td>石制</td><td>9</td><td>制作一把石镐</td><td>7200</td><td>空</td><td>9</td></tr><tr><td>铁制</td><td>16</td><td>制作一把铁镐</td><td>12000</td><td>空</td><td>13</td></tr><tr><td>金质</td><td>6</td><td>采集金并熔炼成金锭</td><td>36000</td><td>空</td><td>16</td></tr><tr><td>红石</td><td>6</td><td>制作活塞</td><td>36000</td><td>空</td><td>17</td></tr><tr><td>○ 钻石</td><td>7</td><td>向下挖掘并开采一颗钻石</td><td>36000</td><td>空</td><td>15</td></tr><tr><td>£ 盔甲</td><td>13</td><td>制作一顶铁头盔</td><td>36000</td><td>空</td><td>16</td></tr></tbody></table>


Table 6: We evaluate Optimus-1 on these tasks in ablation study which are the subset of our benchmark.
表6：我们在消融研究中评估 Optimus-1 在这些任务上的表现，这些任务是我们基准的子集。


<table><tr><td>Group</td><td>Task</td><td>Sub-Goal Num.</td><td>Max. Step</td><td>Initial Inventory</td></tr><tr><td rowspan="2">Wooden</td><td>Craft a wooden axe</td><td>5</td><td>3600</td><td>Empty</td></tr><tr><td>Craft a crafting table</td><td>3</td><td>3600</td><td>Empty</td></tr><tr><td rowspan="3">Stone</td><td>Craft a stone pickaxe</td><td>10</td><td>7200</td><td>Empty</td></tr><tr><td>Craft a stone axe</td><td>10</td><td>7200</td><td>Empty</td></tr><tr><td>Craft a furnace</td><td>9</td><td>7200</td><td>Empty</td></tr><tr><td rowspan="5">Tron</td><td>Craft a iron pickaxe</td><td>13</td><td>12000</td><td>Empty</td></tr><tr><td>Craft a bucket</td><td>13</td><td>12000</td><td>Empty</td></tr><tr><td>Craft a rail</td><td>13</td><td>12000</td><td>Empty</td></tr><tr><td>Craft a iron sword</td><td>12</td><td>12000</td><td>Empty</td></tr><tr><td>Craft a shears</td><td>12</td><td>12000</td><td>Empty</td></tr><tr><td rowspan="3">Golden</td><td>Craft a golden pickaxe</td><td>16</td><td>36000</td><td>Empty</td></tr><tr><td>Craft a golden axe</td><td>16</td><td>36000</td><td>Empty</td></tr><tr><td>Smelt a golden ingot</td><td>15</td><td>36000</td><td>Empty</td></tr><tr><td rowspan="5">Diamond</td><td>Craft a diamond pickaxe</td><td>15</td><td>36000</td><td>Empty</td></tr><tr><td>Craft a diamond axe</td><td>16</td><td>36000</td><td>Empty</td></tr><tr><td>Craft a diamond hoe</td><td>15</td><td>36000</td><td>Empty</td></tr><tr><td>Craft a diamond sword</td><td>15</td><td>36000</td><td>Empty</td></tr><tr><td>Dig down and mine a diamond</td><td>15</td><td>36000</td><td>Empty</td></tr></table>
<table><tbody><tr><td>组别</td><td>任务</td><td>子目标数量</td><td>最大步数</td><td>初始物品栏</td></tr><tr><td rowspan="2">木制</td><td>制作一把木斧</td><td>5</td><td>3600</td><td>空</td></tr><tr><td>制作工作台</td><td>3</td><td>3600</td><td>空</td></tr><tr><td rowspan="3">石制</td><td>制作一把石镐</td><td>10</td><td>7200</td><td>空</td></tr><tr><td>制作一把石斧</td><td>10</td><td>7200</td><td>空</td></tr><tr><td>制作熔炉</td><td>9</td><td>7200</td><td>空</td></tr><tr><td rowspan="5">铁器</td><td>制作一把铁镐</td><td>13</td><td>12000</td><td>空</td></tr><tr><td>制作一只水桶</td><td>13</td><td>12000</td><td>空</td></tr><tr><td>制作一段铁轨</td><td>13</td><td>12000</td><td>空</td></tr><tr><td>制作一把铁剑</td><td>12</td><td>12000</td><td>空</td></tr><tr><td>制作一把剪刀</td><td>12</td><td>12000</td><td>空</td></tr><tr><td rowspan="3">金制</td><td>制作一把金镐</td><td>16</td><td>36000</td><td>空</td></tr><tr><td>制作一把金斧</td><td>16</td><td>36000</td><td>空</td></tr><tr><td>熔炼一块金锭</td><td>15</td><td>36000</td><td>空</td></tr><tr><td rowspan="5">钻石</td><td>制作一把钻石镐</td><td>15</td><td>36000</td><td>空</td></tr><tr><td>制作一把钻石斧</td><td>16</td><td>36000</td><td>空</td></tr><tr><td>制作一把钻石锄</td><td>15</td><td>36000</td><td>空</td></tr><tr><td>制作一把钻石剑</td><td>15</td><td>36000</td><td>空</td></tr><tr><td>向下挖并开采一颗钻石</td><td>15</td><td>36000</td><td>空</td></tr></tbody></table>


diamond generation probability to 20%, spawns in levels 2 to 16. This setting applies to human players as well.
将钻石生成概率改为20%，在2到16层生成。此设置同样适用于人类玩家。


In the ablation study, we select the subset of our benchmark as the evaluation set (shown in Table 6). The environment setting is the same as the benchmark.
在消融实验中，我们选择基准测试的一个子集作为评估集（见表6）。环境设置与基准测试相同。


### D.2 Baselines
### D.2 基线


Existing Baseline. On the one hand, we employ GPT-3.5 and GPT-4V as baseline, which are evaluated without integrating hybrid multimodal memory modules. During the planning phase, they generate a plan for the action controller based on task prompt (and observation). During the reflection phase, they generate reflection results in a zero-shot manner. On the other hand, we compare existing SOTA Agents [51, 52] in Minecraft.
现有基线。一方面，我们采用 GPT-3.5 和 GPT-4V 作为基线，在评估时未集成混合多模态记忆模块。在规划阶段，它们根据任务提示（和观测）为动作控制器生成计划；在反思阶段，它们以零样本方式生成反思结果。另一方面，我们比较了 Minecraft 中现有的 SOTA Agents [51, 52]。


Human-level Baseline. To better demonstrate agent's performance level in Minecraft, we hired 10 volunteers to play the game as a human-level baseline. The volunteers played the game with the same environment and settings, and every volunteer asked to perform the each task on the benchmark 10 times. Ultimately, we used the average scores of 10 volunteers as the human-level baseline. The results of the human-level baseline are shown in Table 1. To ensure the validity of the experiment, we ensured that each volunteer had at least 20 hours of Minecraft gameplay before conducting the experiment. For each volunteer, we pay \$25 as reward.
人类水平基线。为更好地展示智能体在 Minecraft 中的表现，我们聘请了10名志愿者作为人类水平基线。志愿者在相同环境和设置下游玩，每位志愿者需在基准测试上各执行每项任务10次。最终，我们使用10名志愿者的平均分作为人类水平基线。人类基线结果见表1。为确保实验有效性，我们确保每位志愿者在实验前至少有20小时的 Minecraft 游戏经验。每位志愿者的报酬为25美元。


Table 7: Statistics for various Minecraft agents.
表7：各类 Minecraft 智能体的统计数据。


<table><tr><td>Agent</td><td>Pub.</td><td>Env.</td><td>Input</td><td>Output</td><td>Planning</td><td>Reflection</td><td>Knowledge</td><td>Experience</td></tr><tr><td>VPT [1]</td><td>NeurIPS' 22</td><td>MineRL</td><td>V</td><td>low-level action</td><td></td><td></td><td></td><td></td></tr><tr><td>MineDOJO [7]</td><td>NeurIPS' 22</td><td>MineDOJO</td><td>T+V</td><td>low-level action</td><td></td><td></td><td></td><td></td></tr><tr><td>STEVE-1 [25]</td><td>NeurIPS' 23</td><td>MineRL</td><td>T+V</td><td>low-level action</td><td></td><td></td><td></td><td></td></tr><tr><td>Voyager [47]</td><td>NeurIPS' 23</td><td>Mineflayer</td><td>T+V</td><td>code</td><td>✓</td><td>✓</td><td></td><td>✓</td></tr><tr><td>DEPS [51]</td><td>NeurIPS' 23</td><td>MineDOJO</td><td>T+V</td><td>code</td><td>✓</td><td>✓</td><td></td><td></td></tr><tr><td>GROOT [3]</td><td>ICLR' 24</td><td>MineRL</td><td>T+V</td><td>low-level action</td><td></td><td></td><td></td><td></td></tr><tr><td>MP5 [33]</td><td>CVPR' 24</td><td>MineDOJO</td><td>T+V</td><td>code</td><td>✓</td><td>✓</td><td></td><td></td></tr><tr><td>Jarvis-1 [52]</td><td>-</td><td>MineRL</td><td>T+V</td><td>low-level action</td><td>✓</td><td>✓</td><td></td><td>✓</td></tr><tr><td>Optimus-1</td><td>-</td><td>MineRL</td><td>T+V</td><td>low-level action</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr></table>
<table><tbody><tr><td>代理</td><td>出版</td><td>环境</td><td>输入</td><td>输出</td><td>规划</td><td>反思</td><td>知识</td><td>经验</td></tr><tr><td>VPT [1]</td><td>NeurIPS' 22</td><td>MineRL</td><td>V</td><td>低级动作</td><td></td><td></td><td></td><td></td></tr><tr><td>MineDOJO [7]</td><td>NeurIPS' 22</td><td>MineDOJO</td><td>文本+视觉</td><td>低级动作</td><td></td><td></td><td></td><td></td></tr><tr><td>STEVE-1 [25]</td><td>NeurIPS' 23</td><td>MineRL</td><td>文本+视觉</td><td>低级动作</td><td></td><td></td><td></td><td></td></tr><tr><td>Voyager [47]</td><td>NeurIPS' 23</td><td>Mineflayer</td><td>文本+视觉</td><td>代码</td><td>✓</td><td>✓</td><td></td><td>✓</td></tr><tr><td>DEPS [51]</td><td>NeurIPS' 23</td><td>MineDOJO</td><td>文本+视觉</td><td>代码</td><td>✓</td><td>✓</td><td></td><td></td></tr><tr><td>GROOT [3]</td><td>ICLR' 24</td><td>MineRL</td><td>文本+视觉</td><td>低级动作</td><td></td><td></td><td></td><td></td></tr><tr><td>MP5 [33]</td><td>CVPR' 24</td><td>MineDOJO</td><td>文本+视觉</td><td>代码</td><td>✓</td><td>✓</td><td></td><td></td></tr><tr><td>Jarvis-1 [52]</td><td>-</td><td>MineRL</td><td>文本+视觉</td><td>低级动作</td><td>✓</td><td>✓</td><td></td><td>✓</td></tr><tr><td>Optimus-1</td><td>-</td><td>MineRL</td><td>文本+视觉</td><td>低级动作</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr></tbody></table>


### D.3 Minecraft Agents
### D.3 Minecraft 代理


In this section, we summarise the differences between existing Minecraft agents. As shown in the Table 7,earlier work $\left\lbrack  {1,7,{25},3}\right\rbrack$ constructed Transformer-based policy network as agent. Recent work [47, 51, 52, 33] introduces the Multimodal Large Language Model, which empowers the agent to complete long-horizon tasks by exploiting the powerful language comprehension and planning capabilities of LLM.
在本节中，我们总结了现有 Minecraft 代理之间的差异。如表 7 所示，早期工作 $\left\lbrack  {1,7,{25},3}\right\rbrack$ 构建了基于 Transformer 的策略网络作为代理。近期工作 [47, 51, 52, 33] 引入了多模态大语言模型，这使代理能够利用 LLM 强大的语言理解与规划能力来完成长时序任务。


In the Mineflayer and Minedojo environments, agents [7, 47, 51, 33] can accomplish sub-goals by calling APIs (in the form of codes), which is a different behavioral pattern from humans. In MineRL [11], agents [1, 25, 3, 52] must generate low-level actions to perform tasks, which is more challenging to accomplish long-horizon tasks.
在 Mineflayer 和 Minedojo 环境中，代理 [7, 47, 51, 33] 可以通过调用 API（以代码形式）来完成子目标，这与人类的行为模式不同。在 MineRL [11] 中，代理 [1, 25, 3, 52] 必须生成低级动作来执行任务，这使得完成长时序任务更具挑战性。


Moreover, existing agents lack knowledge and experience, and their performance in Minecraft is still vastly gapped from the human level. In this paper, we introduce Hybrid Multimodal Memory, which empowers Optimus-1 with hierarchical knowledge and multimodal experience. This makes Optimus-1 significantly outperform all existing agents on challenging long-horizon tasks benchmark, and exhibits near human-level performance on many tasks.
此外，现有代理缺乏知识与经验，在 Minecraft 中的表现仍与人类水准存在巨大差距。本文提出了混合多模态记忆（Hybrid Multimodal Memory），为 Optimus-1 提供层级化知识与多模态经验，使其在具有挑战性的长时序任务基准上显著优于所有现有代理，并在许多任务上表现出接近人类水平的性能。


## E Implementation Details
## E 实现细节


### E.1 Hybrid Multimodal Memory
### E.1 混合多模态记忆


#### E.1.1 Abstracted Multimodal Experience Pool
#### E.1.1 抽象化多模态经验池


Relevant studies $\left\lbrack  {5,{29},{17},{15}}\right\rbrack$ have demonstrated the importance of memory for agents to complete long-horizon tasks. To implement the memory mechanism, Minedojo [7] and Voyager [47] only considered unimodal storage of historical information. Jarvis-1 [52] considered a multimodal memory mechanism to store task planning and visual information as experience, but it stores all historical information without summarisation. This approach stores all visual images, which poses a huge challenge in storage size and retrieval efficiency. To solve the problem, we propose the Abstracted Multimodal Experience Pool structure, which summarizes all historical information during the agent's execution of the task, which maintains the integrity of long sequential information and greatly improves the storage and retrieval efficiency of the experience.
相关研究 $\left\lbrack  {5,{29},{17},{15}}\right\rbrack$ 已证明记忆对代理完成长时序任务的重要性。为实现记忆机制，Minedojo [7] 和 Voyager [47] 仅考虑了历史信息的单模态存储。Jarvis-1 [52] 考虑了将任务规划和视觉信息作为经验存储的多模态记忆机制，但其存储了所有历史信息且不进行摘要。该方法保存所有视觉图像，带来巨大的存储量与检索效率挑战。为解决该问题，我们提出了抽象化多模态经验池结构，在代理执行任务过程中对所有历史信息进行摘要，既保持长序列信息的完整性，又大幅提升经验的存储与检索效率。


As shown in Figure 2, we first input the visual image stream to the video buffer, which filters the image stream at a fixed frequency. It makes the length of the image stream substantially shorter. Empirically, we set the frequency of filtering to 1 second/frame, meaning that the video buffer takes one frame per second from the original image stream to compose the filtered image stream. We found that above this frequency makes the visual information redundant (too much similarity between images), and below this frequency does not preserve enough complete visual information.
如图 2 所示，我们首先将视觉图像流输入视频缓冲区，对图像流以固定频率进行过滤，从而大幅缩短图像序列长度。经验上，我们将过滤频率设为 1 秒/帧，即视频缓冲区从原始图像流中每秒取一帧组成过滤后的图像流。我们发现高于该频率会使视觉信息冗余（图像间过于相似），低于该频率又无法保留足够完整的视觉信息。


Then, we feed the filtered frames into an image buffer with a window size of 16 . We dynamically compute the similarity between images in the image buffer, when a new image comes in, we compute the similarity between the new image and the most recent image, and then we remove the image with the highest similarity in order to keep the image buffer's window size to 16.
随后，我们将过滤后的帧送入窗口大小为 16 的图像缓冲区。我们动态计算缓冲区内图像间的相似度，当新图像到来时，计算新图像与最近图像的相似度，然后移除相似度最高的图像，以保持图像缓冲区窗口大小为 16。


Subsequently, we introduce MineCLIP [7], a pre-trained model of video-text alignment with a structure similar to CLIP [35], as our visual summariser. For a given sub-goal, it calculates the correlation between the visual content within the current memory bank and the sub-goal, and when this correlation exceeds a pre-set threshold, the frames within the memory bank are saved as the visual memories corresponding to that sub-goal. Finally, we store the visual memories with the sub goal's textual description into the Abstracted Multimodal Experience Pool. In addition, we incorporate the environment information, agent initial state, plan from Knowledge-Guided Planner, etc. into the experience memory of the given task. In this way, we consider the history information of each sub-goal and summaries and summarise it to finally compose the multimodal experience of the given task.
接着，我们引入 MineCLIP [7]，一种与 CLIP [35] 结构相似的预训练视频-文本对齐模型，作为我们的视觉总结器。对于给定子目标，它计算当前记忆库内视觉内容与该子目标之间的相关性，当相关性超过预设阈值时，记忆库内的帧被保存为对应该子目标的视觉记忆。最后，我们将这些视觉记忆与子目标的文本描述一并存入抽象化多模态经验池。此外，我们将环境信息、代理初始状态、知识引导规划器的计划等纳入该任务的经验记忆。通过这种方式，我们考虑并对每个子目标的历史信息进行摘要，最终构成给定任务的多模态经验。


Note that we also store these visual memories as failure cases when the feedback from the reflection phase is REPLAN. Therefore, when Optimus-1 executes a long-horizon task, it can retrieve past successes and failures as references and update memory after the task is finished. In the reflection phase, Optimus-1 retrieve the most relevant cases from Abstracted Multimodal Experience Pool, which contains the three scenarios COMPLETE, CONTINUE, and REPLAN, to help the agent better assess which state the current situation belongs to. This approach of considering both successful and failed cases for in-context learning is inspired by related research [8, 32], and its effectiveness is validated in Section 3.3.
注意当反思阶段的反馈为 REPLAN 时，我们也将这些视觉记忆作为失败案例存储。因此，当 Optimus-1 执行长时序任务时，它可以检索过去的成功与失败作为参考，并在任务结束后更新记忆。在反思阶段，Optimus-1 从抽象化多模态经验池检索最相关的案例，该池包含 COMPLETE、CONTINUE 和 REPLAN 三种情形，以帮助代理更好地判断当前情形所属状态。该同时考虑成功与失败案例用于上下文学习的方法受相关研究 [8, 32] 启发，其有效性在第 3.3 节中得到验证。


#### E.1.2 Hierarchical Directed Knowledge Graph
#### E.1.2 层级化有向知识图


As shown in the Figure 2,crafting a diamond sword $\checkmark$ requires two diamonds $\odot$ and a wooden stick /, while mining diamonds requires an iron pickaxe 7, which in turn requires additional raw materials and crafting steps. We transform this mine and craft knowledge into a graph structure, where the nodes of the graph are objects, and the nodes point to objects that can be crafted or completed by that object. With directed graph, we show that connections between objects are established, and that this knowledge can be stored and updated efficiently. For a given object, we only need to retrieve the corresponding node to extract the corresponding subgraph from the knowledge graph. Then by topological sorting, we can get the antecedents and required materials for the object, and this information is provided to the Knowledge-Guided Planner as a way to generate a more reasonable sequence of sub-goals. With Hierarchical Directed Knowledge Graph, we can significantly enhance the world knowledge of the agent in a train-free manner, as shown in the experimental results in Section 3.3.
如图2所示，制作一把钻石剑 $\checkmark$ 需要两颗钻石 $\odot$ 和一根木棍 /，而挖掘钻石需要一把铁镐 7，这又需要额外的原料和制作步骤。我们将此挖掘与制作知识转化为图结构，图的节点为物品，节点指向可由该物品制作或生成的物品。通过有向图，我们展示了物品间连接的建立，并且该知识可以被高效存储和更新。对于某一给定物品，只需检索对应节点即可从知识图中提取相应子图。然后通过拓扑排序，我们可以得到该物品的前置项和所需材料，并将这些信息提供给知识引导规划器，以生成更合理的子目标序列。借助分层有向知识图，我们可以在无训练的情况下显著增强智能体的世界知识，如第3.3节的实验结果所示。


Our HDKG can be efficiently updated and expanded. When adding new nodes, the HDKG can be updated by simply merging the nodes and relationships into the graph. This method involves local linear modifications to the graph rather than altering the entire graph, making the process efficient and time-saving. For example, when M new nodes and N edges are added, the HDKG can be updated with M+N times of operations. Moreover, an HDKG containing 851 objects (nodes) requires less than 1 MB of memory. Thus, the HDKG can be efficiently updated and maintained.
我们的 HDKG 可以高效更新和扩展。添加新节点时，只需将节点与关系合并到图中即可更新 HDKG。此方法对图进行局部线性修改，而非改动整个图，使得过程高效且省时。例如，当新增 M 个节点和 N 条边时，更新 HDKG 需要进行 M+N 次操作。此外，包含 851 个对象（节点）的 HDKG 占用内存小于 1 MB。因此，HDKG 可被高效更新和维护。


### E.2 Hybrid Multimodal Memory Driven Optimus-1
E.2 混合多模态记忆驱动的 Optimus-1


In order to implement the proposed Hybrid Multimodal Memory and to progressively increase the capacity of Optimus-1 in a self-evolution manner, we propose a non-parametric learning method named "free exploration-teacher guidance".
为实现所提的混合多模态记忆并以自我进化的方式逐步提升 Optimus-1 的容量，我们提出了一种名为“自由探索-教师引导”的非参数学习方法。


In the free exploration phase, we randomly initialize the environment, materials, and tasks. For the task "craft a wooden pickaxe", we provide initial materials (three planks, two sticks), and then Optimus-1 (only the action controller activated) attempts to complete the task. If the environment feedback indicates the task is successful, the knowledge \{3 planks, 2 sticks → wooden pickaxe\} is added to the HDKG. Note that we randomly initialize materials and their quantities, which means that the task may not always succeed. As a result, each free exploration may not acquire the corresponding knowledge, but it can record the relevant experience (whether successful or fail). In the free exploration phase, Optimus-1 learns simple atomic operations, such as crafting sticks in the Wooden Group and mining diamonds in the Diamond Group.
在自由探索阶段，我们随机初始化环境、材料和任务。对于任务“制作一把木镐”，我们提供初始材料（三块木板、两根木棍），然后 Optimus-1（仅激活行动控制器）尝试完成任务。如果环境反馈表明任务成功，知识 \{3 planks, 2 sticks → wooden pickaxe\} 会被加入 HDKG。注意我们随机初始化材料及其数量，这意味着任务可能并不总是成功。因此，每次自由探索可能无法获得对应知识，但能记录相关经验（成功或失败）。在自由探索阶段，Optimus-1 学习简单的原子操作，例如在木材组制作木棍和在钻石组挖掘钻石。


In the teacher guidance phase, Optimus-1 need to learn a small number of long-horizon tasks based on extra knowledge. For example, during the free exploration phase, Optimus-1 mastered crafting stick $\angle$ and mining diamond $\odot$ ,but did not know that "a diamond sword $\measuredangle$ is obtained by a stick $\angle$ and two diamonds ${\Theta }^{\prime \prime }$ . So we provide some task plans,which will serve as extra knowledge to guide
在教师引导阶段，Optimus-1 需要基于额外知识学习少量的长时任务。例如，在自由探索阶段，Optimus-1 掌握了制作木棍 $\angle$ 和挖掘钻石 $\odot$，但不知道“一把钻石剑 $\measuredangle$ 是由一根木棍 $\angle$ 和两颗钻石 ${\Theta }^{\prime \prime }$ 获得”。因此我们提供一些任务计划，作为额外知识来引导


Optimus-1 to complete the task of "craft diamond sword". We built the following automated process to get the task plan needed for "free exploration":
Optimus-1 完成“制作钻石剑”任务。我们构建了如下自动化流程以获取“自由探索”所需的任务计划：


- We randomly select 5 tasks for each Group (7 groups in total) that are not included in the benchmark.
- 我们为每个组随机选择 5 项任务（共 7 组），这些任务不包含在基准中。


- For each selected task, we use a script to automatically obtain the crafting relationships from the Minecraft Wiki ${}^{4}$ . Taking the task "craft a wooden sword" as an example,we use the script to automatically obtain the crafting relationships: 1 wooden stick, 2 planks, 1 crafting table $\rightarrow  1$ wooden sword,1 log $\rightarrow  4$ planks,2 planks $\rightarrow  4$ sticks,4 planks $\rightarrow  1$ crafting table.
- 对于每个选定任务，我们使用脚本从 Minecraft Wiki ${}^{4}$ 自动获取制作关系。以任务“制作一把木剑”为例，我们使用脚本自动获取制作关系：1 根木棍、2 块木板、1 张工作台 $\rightarrow  1$ → 木剑；1 根原木 $\rightarrow  4$ → 木板；2 块木板 $\rightarrow  4$ → 木棍；4 块木板 $\rightarrow  1$ → 工作台。


- These relationships are converted into a directed acyclic graph through an automated script. By performing a topological sort, the graph can be converted into tuples of materials and their quantities: (wooden sword, 1), (crafting table, 1), (wooden stick, 1) (planks, 8), (log, 2).
- 这些关系通过自动脚本被转换为有向无环图。通过拓扑排序，图可被转换为材料及其数量的元组：(wooden sword, 1), (crafting table, 1), (wooden stick, 1), (planks, 8), (log, 2)。


- We prompt GPT-4 to construct a plan in order from basic materials to advanced materials.
- 我们提示 GPT-4 按从基础材料到高级材料的顺序构建计划。


- Finally, we get the plan: 1. Get two logs 2. Craft eight planks 3. Craft a crafting table 4. Craft a wooden stick 5. Craft a wooden sword
- 最终，我们得到计划：1. 获取两根原木 2. 制作八块木板 3. 制作一张工作台 4. 制作一根木棍 5. 制作一把木剑


During the teacher guidance phase, Optimus-1's memory is further expanded and it gains the experience of executing complete long-horizon tasks. Teacher guidance phase allows Optimus-1 to acquire advanced knowledge and learn multimodal experiences through complete long-horizon tasks.
在教师指导阶段，Optimus-1 的记忆得到进一步扩展，并获得执行完整长时任务的经验。教师指导阶段使 Optimus-1 能通过完整的长时任务获取高级知识并学习多模态经验。


### E.3 Backbone of Optimus-1
### E.3 Optimus-1 的骨干


Optimus-1 consists of Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. In this paper, we employ OpenAI's GPT-4V ${}^{5}$ as Knowledge-Guided Planner and Experience-Driven Reflector, and STEVE-1 [25] as Action Controller. We also employ open-source models like Deepseek-VL [26] and InternLM-XComposer2-VL [6] as Knowledge-Guided Planner and Experience-Driven Reflector.
Optimus-1 由知识引导规划器、经验驱动反思器和动作控制器组成。本文中，我们采用 OpenAI 的 GPT-4V ${}^{5}$ 作为知识引导规划器和经验驱动反思器，采用 STEVE-1 [25] 作为动作控制器。我们也使用开源模型如 Deepseek-VL [26] 和 InternLM-XComposer2-VL [6] 作为知识引导规划器与经验驱动反思器。


All experiments were implemented on 4x NVIDIA A100 GPUs. We employ multiple Optimus-1 to perform different tasks at the same time, and this parallelized inference greatly improves our experimental efficiency. In the free exploration and teacher guidance phases, there is no need to access OpenAI's API, and the learning process takes approximately 16 hours on 4x A100 80G GPUs. During the inference phase, it takes about 20 hours on 4x A100 80G GPUs.
所有实验在 4 块 NVIDIA A100 GPU 上实现。我们并行使用多个 Optimus-1 同时执行不同任务，这种并行推理大幅提高了实验效率。在自由探索和教师指导阶段无需访问 OpenAI 的 API，学习过程在 4×A100 80G GPUs 上约需 16 小时。推理阶段在 4×A100 80G GPUs 上约需 20 小时。


Throughout the experiment, we spent about \$5,000 to access the GPT-4V API. However, we also offer more cost-effective solutions. As shown in Figure 5, if we employ Deepseek-VL [26] or InternLM-XComposer2-VL [6] as Optimus-1's backbone, we can get comparable performance with low-cost!
在整个实验过程中，我们为访问 GPT-4V API 花费了约 \$5,000。然而，我们也提供更具成本效益的方案。如图 5 所示，若采用 Deepseek-VL [26] 或 InternLM-XComposer2-VL [6] 作为 Optimus-1 的骨干，便可以低成本获得相当性能！


### E.4 Prompt for Optimus-1
### E.4 Optimus-1 的提示语


We show the prompt templates for Experience-Driven Reflector and Action Controller as follows.
我们展示了经验驱动反思器和动作控制器的提示模板如下。


---



System: You are a MineCraft game expert and you can guide agents to complete complex tasks.
System: 你是 Minecraft 游戏专家，可以指导智能体完成复杂任务。


User: For a given game screen and task, you need to complete <goal inference> and <visual
User: 针对给定的游戏画面和任务，你需要完成 <goal inference> 与 <visual


	inference>.
	inference>。


<goal inference>: According to the task, you need to infer the weapons, equipment, or
<goal inference>: 根据任务，你需要推断完成任务所需的武器、装备或


	materials required to complete the task.
	材料。


<visual inference>: According to the game screen, you need to infer the following aspects:
<visual inference>: 根据游戏画面，你需要推断以下方面：


	health bar, food bar, hotbar, environment.
	血条、饥饿条、快捷栏、环境。


I will give you an example as follow:
我将给你一个示例，示例如下：


[Example]
[Example]


<task>: craft a stone sword.
: 制作一把石剑。


<goal inference>: stone sword
<goal inference>: 石剑


<visual inference>



health bar: full
生命值：满


food bar: full
饥饿值：满


---



---



${}^{4}$ https://minecraft.wiki/
${}^{4}$ https://minecraft.wiki/


${}^{5}$ https://openai.com/index/gpt-4v-system-card/
${}^{5}$ https://openai.com/index/gpt-4v-system-card/


---



---



hotbar: empty
快捷栏：空


environment: forest
环境：森林


Here is a game screen and task, you MUST output in example format.
这是一个游戏画面和任务，你必须以示例格式输出。


<task>: \{task\}.
<task>: \{task\}.


<game screen>: \{image\}
<game screen>: \{image\}


Assistant:
助理：


User: Now you need to make a plan with the help of <visual info> and <craft graph>.
用户：现在你需要在<visual info>和<craft graph>的帮助下制定一个计划。


<visual info>: Consists of the following aspects: health bar, food bar, hotbar, environment.
<visual info>: 由以下方面组成：生命值、饥饿值、快捷栏、环境。


	Based on the current visual information, you need to consider whether prequel steps
	基于当前的视觉信息，你需要考虑是否需要前置步骤


	needed to ensure that agent can complete the task.
	以确保代理能够完成该任务。


<craft graph>: a top-down list of all the tools and materials needed to complete the task.
<craft graph>: 完成任务所需的所有工具和材料的自上而下清单。


I will give you an example of planning under specific visual conditions as follow:
我将给你一个在特定视觉条件下规划的示例，内容如下：


[Example]
[示例]


\{example\}
\{示例\}


Here is a game screen and task, you MUST output in example format. Remember <task planning>
这是一个游戏画面与任务，你必须以示例格式输出。记住 <task planning>


	MUST output in example format.
	必须按示例格式输出。


<task>: \{task\}
: \{task\}


<game screen>: \{image\}
<game screen>: \{image\}


<craft graph>: \{graph\}
<craft graph>: \{graph\}


Assistant:
助手：


---



Listing 1: Prompt for Knowledge-Guided Planner.
图 1：知识引导规划器的提示。


---



System: You are a MineCraft game expert and you can guide agents to complete complex tasks.
系统：你是 Minecraft 专家，能够引导玩家完成复杂任务。


	Agent is executing the task: \{task\}.
	Agent 正在执行任务： \{task\}.


Given two images about agent's state before executing the task and its current state, you
给定两张图，一张是代理在执行任务前的状态，另一张是其当前状态，你


	should first detection the environment (forest, cave, ocean, etc.,) in which the agent
	应该首先检测智能体所处的环境（森林、洞穴、海洋等）


	is located, then determine whether the agent's current situation is done, continue, or
	位于何处，然后判断代理当前情形是已完成、继续，还是


	replan.
	重新计划。


<done>: Comparing the image before the task was performed, the current image reveals that the
: 与执行任务前的图像比较，当前图像显示


	task is complete.
	任务已完成。


<continue>: Current image reveals that the task is NOT complete, but agent is in good state (   )
<continue>: 当前图像显示任务尚未完成，但代理状态良好 (   )


	good health, not hungry) with high likelihood to complete task.
	健康良好、不挨饿，极有可能完成任务。


<replan>: Current image reveals that the task is NOT complete, and agent is in bad state (bad
<replan>: 当前图像显示任务未完成，且代理处于不良状态（bad


	health, or hungry) or situation (in danger, or in trouble), need for replanning. For
	健康状态（生病或饥饿）或处境（有危险或遇到麻烦）需要重新规划。为


	replan, you need to further determine whether the agent's predicament is "drop_down" or
	重新规划，你需要进一步判断该代理的困境是“drop_down”还是


	"in_water". "drop_down" means that the agent has fallen into a cave or is trapped in a
	"in_water". "drop_down" 意味着代理已掉入洞穴或被困在一个


	mountain or river, while "in_water" means that the agent is in the ocean and needs to
	山或河流，而 "in_water" 意味着代理在海洋中并需要


	return to land immediately.
	立即上岸。


User: I'll give you some examples to illustrate the different situations. Each example
User: 我将给你一些例子来说明不同的情形。每个例子


	consists of two images, where the first image is the state of the agent before
	由两幅图像组成，第一幅为代理执行前的状态


	performing the task and the second image is the current state of the agent.
	执行该任务的图像为第一张，第二张图像显示的是代理的当前状态。


[Examples]
[示例]


<done>: \{image1\},\{image2\}
: \{image1\},\{image2\}


<continue>: \{image1\},\{image2\}
<continue>: \{image1\},\{image2\}


<replan>: \{image1\},\{image2\}
<replan>: \{image1\},\{image2\}


Now given two images about agent's state before executing the task and its current state, you
现在给出两张图片，分别表示代理在执行任务前的状态和当前状态，你


	MUST and ONLY output in following format:
	必须且仅以以下格式输出：


Enviroment: <environment>
环境： <environment>


Situation: <situation>
情形： <situation>


(if situation is replan) Predicament: <predicament>
(如果情形为重新规划) 困境： <predicament>


---



Listing 2: Prompt for Experience-Driven Reflector.
列表 2：基于经验驱动的反思器提示。


## F Additional Experimental Results
## F 额外实验结果


### F.1 Full Results on Our Benchmark
### F.1 我们基准的完整结果


We list the results of each task on the benchmark below, with details including task name, sub-goal numbers, success rate (SR), average number of steps (AS), average time (AT), and eval times. All tasks are evaluated in Minecraft 1.16.5 Survival Mode. Note that each time Optimus-1 performs a task, we initial it with an empty initial inventory and a random start point. This makes it challenging for Optimus-1 to perform each task.
我们在下方列出基准中每个任务的结果，包含任务名称、子目标数、成功率（SR）、平均步数（AS）、平均时间（AT）及评估次数。所有任务均在 Minecraft 1.16.5 生存模式下评估。注意每次 Optimus-1 执行任务时，我们以空初始物品栏和随机起点初始化它，这使得 Optimus-1 完成每项任务更具挑战性。


Table 8: The results of Optimus-1 on various tasks in the Wood group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively.
表 8：Optimus-1 在木材组各项任务上的结果。SR、AS、AT 分别表示成功率、平均步数和平均时间（秒）。


<table><tr><td>Task</td><td>Sub-Goal Num.</td><td>SR</td><td>AS</td><td>AT(s)</td><td>Eval Times</td></tr><tr><td>Craft a wooden shovel</td><td>6</td><td>95.00</td><td>995.58</td><td>49.78</td><td>40</td></tr><tr><td>Craft a wooden pickaxe</td><td>5</td><td>100.00</td><td>1153.91</td><td>57.70</td><td>30</td></tr><tr><td>Craft a wooden axe</td><td>5</td><td>96.67</td><td>1010.28</td><td>50.51</td><td>30</td></tr><tr><td>Craft a wooden hoe</td><td>5</td><td>100.00</td><td>1042.80</td><td>52.14</td><td>30</td></tr><tr><td>Craft a stick</td><td>4</td><td>97.14</td><td>372.97</td><td>18.65</td><td>70</td></tr><tr><td>Craft a crafting table</td><td>3</td><td>98.55</td><td>448.63</td><td>22.43</td><td>69</td></tr><tr><td>Craft a wooden sword</td><td>5</td><td>100.00</td><td>1214.90</td><td>60.74</td><td>30</td></tr><tr><td>Craft a chest</td><td>4</td><td>100.00</td><td>573.80</td><td>28.69</td><td>30</td></tr><tr><td>Craft a bowl</td><td>4</td><td>100.00</td><td>744.30</td><td>37.21</td><td>30</td></tr><tr><td>Craft a ladder</td><td>4</td><td>100.00</td><td>820.30</td><td>41.02</td><td>30</td></tr></table>
<table><tbody><tr><td>任务</td><td>子目标编号</td><td>成功率</td><td>自动评分</td><td>可获得奖励(秒)</td><td>评估次数</td></tr><tr><td>制作一把木锹</td><td>6</td><td>95.00</td><td>995.58</td><td>49.78</td><td>40</td></tr><tr><td>制作一把木镐</td><td>5</td><td>100.00</td><td>1153.91</td><td>57.70</td><td>30</td></tr><tr><td>制作一把木斧</td><td>5</td><td>96.67</td><td>1010.28</td><td>50.51</td><td>30</td></tr><tr><td>制作一把木锄</td><td>5</td><td>100.00</td><td>1042.80</td><td>52.14</td><td>30</td></tr><tr><td>制作一根木棍</td><td>4</td><td>97.14</td><td>372.97</td><td>18.65</td><td>70</td></tr><tr><td>制作一张工作台</td><td>3</td><td>98.55</td><td>448.63</td><td>22.43</td><td>69</td></tr><tr><td>制作一把木剑</td><td>5</td><td>100.00</td><td>1214.90</td><td>60.74</td><td>30</td></tr><tr><td>制作一个箱子</td><td>4</td><td>100.00</td><td>573.80</td><td>28.69</td><td>30</td></tr><tr><td>制作一个碗</td><td>4</td><td>100.00</td><td>744.30</td><td>37.21</td><td>30</td></tr><tr><td>制作一把梯子</td><td>4</td><td>100.00</td><td>820.30</td><td>41.02</td><td>30</td></tr></tbody></table>


Table 9: The results of Optimus-1 on various tasks in the Stone group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively.
表 9：Optimus-1 在 Stone 组各项任务上的结果。SR、AS、AT 分别表示成功率、平均步数和平均时间（秒）。


<table><tr><td>Task</td><td>Sub-Goal Num.</td><td>SR</td><td>AS</td><td>AT(s)</td><td>Eval Times</td></tr><tr><td>Craft a stone shovel</td><td>8</td><td>90.32</td><td>2221.00</td><td>111.05</td><td>31</td></tr><tr><td>Craft a stone pickaxe</td><td>10</td><td>96.77</td><td>2310.09</td><td>115.50</td><td>31</td></tr><tr><td>Craft a stone axe</td><td>10</td><td>96.88</td><td>2112.59</td><td>105.63</td><td>32</td></tr><tr><td>Craft a stone hoe</td><td>8</td><td>94.64</td><td>2684.60</td><td>134.23</td><td>56</td></tr><tr><td>Craft a charcoal</td><td>9</td><td>88.57</td><td>3083.35</td><td>154.17</td><td>35</td></tr><tr><td>Craft a smoker</td><td>9</td><td>90.24</td><td>3118.89</td><td>155.94</td><td>41</td></tr><tr><td>Craft a stone sword</td><td>8</td><td>94.29</td><td>2067.92</td><td>103.40</td><td>35</td></tr><tr><td>Craft a furnace</td><td>9</td><td>93.75</td><td>2842.71</td><td>142.14</td><td>32</td></tr><tr><td>Craft a torch</td><td>8</td><td>85.71</td><td>2109.00</td><td>105.45</td><td>95</td></tr></table>
<table><tbody><tr><td>任务</td><td>子目标 数量</td><td>SR</td><td>AS</td><td>AT(秒)</td><td>评估次数</td></tr><tr><td>制作石铲</td><td>8</td><td>90.32</td><td>2221.00</td><td>111.05</td><td>31</td></tr><tr><td>制作石镐</td><td>10</td><td>96.77</td><td>2310.09</td><td>115.50</td><td>31</td></tr><tr><td>制作石斧</td><td>10</td><td>96.88</td><td>2112.59</td><td>105.63</td><td>32</td></tr><tr><td>制作石锄</td><td>8</td><td>94.64</td><td>2684.60</td><td>134.23</td><td>56</td></tr><tr><td>制作木炭</td><td>9</td><td>88.57</td><td>3083.35</td><td>154.17</td><td>35</td></tr><tr><td>制作熏炉</td><td>9</td><td>90.24</td><td>3118.89</td><td>155.94</td><td>41</td></tr><tr><td>制作石剑</td><td>8</td><td>94.29</td><td>2067.92</td><td>103.40</td><td>35</td></tr><tr><td>制作熔炉</td><td>9</td><td>93.75</td><td>2842.71</td><td>142.14</td><td>32</td></tr><tr><td>制作火把</td><td>8</td><td>85.71</td><td>2109.00</td><td>105.45</td><td>95</td></tr></tbody></table>


Moreover, in MineRL [11] environment, 'steps' refers to the number of interactions between the agent and the environment, occurring at a frequency of 20 times per second. For example, if an agent takes 2 seconds to complete the task "chop a tree", it interacts with the environment 40 times, resulting in a recorded steps number of 40. Experimental results show that Optimus-1's average task completion step (AS) is significantly lower than other baselines.
此外，在 MineRL [11] 环境中，“steps” 指的是智能体与环境的交互次数，交互频率为每秒 20 次。例如，如果智能体完成“砍树”任务需耗时 2 秒，则与环境交互 40 次，记录的 steps 数为 40。实验结果表明，Optimus-1 的平均任务完成步数 (AS) 显著低于其他基线。


### F.2 Results on Other Benchmark
### F.2 其他基准的结果


For a more comprehensive comparison with current Minecraft Agents, we also report Optimus-1's performances on the benchmark used by Voyager [47], MP5 [33], and DEPS [51] below. Due to the different environments and settings, agents perform tasks with varying degrees of difficulty. For example, Optimus-1 requires low-level action to perform any task in MineRL [11], and we initialize its inventory to be empty. While Voyager [47] performs tasks in Mineflayer ${}^{6}$ environment only through encapsulated code, MP5 [33] performs tasks in MineDOJO [7] environment only needs a specific control signal to craft tools, no low-level actions (mouse movement and click) are needed.
为与当前的 Minecraft Agents 进行更全面的比较，我们还在下文报告了 Optimus-1 在 Voyager [47]、MP5 [33] 和 DEPS [51] 使用的基准上的表现。由于环境和设置不同，代理执行任务的难度各异。例如，Optimus-1 在 MineRL [11] 中执行任何任务都需要低级动作，我们将其背包初始化为空；而 Voyager [47] 在 Mineflayer ${}^{6}$ 环境中仅通过封装的代码执行任务，MP5 [33] 在 MineDOJO [7] 环境中执行任务只需特定的控制信号来制作工具，无需低级动作（鼠标移动和点击）。


Optimus-1's success rate in completing tasks with these baselines is shown in the Table 15 and Table 16, and Optimus-1's efficiency in unlocking the tech tree in Minecraft is shown in the Figure 8. These results reveal that Optimus-1 outperforms a variety of powerful baseline agents, even in challenging environmental settings!
Optimus-1 与这些基线在完成任务上的成功率见表 15 和表 16，Optimus-1 在解锁 Minecraft 技术树方面的效率见图 8。这些结果表明，即便在具有挑战性的环境设置下，Optimus-1 仍优于多种强大的基线代理！


---



${}^{6}$ https://github.com/PrismarineJS/mineflayer
${}^{6}$ https://github.com/PrismarineJS/mineflayer


---



Table 10: The results of Optimus-1 on various tasks in the Iron group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively.
表 10：Optimus-1 在铁组各种任务上的结果。SR、AS、AT 分别表示成功率、平均步数和平均时间（秒）。


<table><tr><td>Task</td><td>Sub Goal Num.</td><td>SR</td><td>AS</td><td>AT(s)</td><td>Eval Times</td></tr><tr><td>Craft an iron shovel</td><td>13</td><td>54.79</td><td>5677.35</td><td>637.81</td><td>73</td></tr><tr><td>Craft an iron pickaxe</td><td>13</td><td>59.42</td><td>6157.39</td><td>591.81</td><td>69</td></tr><tr><td>Craft an iron axe</td><td>13</td><td>54.29</td><td>6026.26</td><td>676.97</td><td>70</td></tr><tr><td>Craft an iron hoe</td><td>13</td><td>52.70</td><td>6650.97</td><td>743.82</td><td>74</td></tr><tr><td>Craft a bucket</td><td>13</td><td>54.29</td><td>6124.61</td><td>591.35</td><td>70</td></tr><tr><td>Craft a hopper</td><td>14</td><td>46.67</td><td>7242.14</td><td>710.17</td><td>60</td></tr><tr><td>Craft a rail</td><td>13</td><td>42.19</td><td>6713.07</td><td>754.48</td><td>64</td></tr><tr><td>Craft an iron sword</td><td>12</td><td>57.14</td><td>5625.49</td><td>633.91</td><td>70</td></tr><tr><td>Craft a shears</td><td>12</td><td>53.62</td><td>5058.00</td><td>570.35</td><td>69</td></tr><tr><td>Craft a smithing table</td><td>12</td><td>44.93</td><td>5317.39</td><td>594.81</td><td>69</td></tr><tr><td>Craft a tripwire hook</td><td>13</td><td>48.57</td><td>4968.74</td><td>562.66</td><td>70</td></tr><tr><td>Craft a chain</td><td>13</td><td>44.93</td><td>5764.42</td><td>645.33</td><td>69</td></tr><tr><td>Craft an iron bars</td><td>12</td><td>42.00</td><td>6508.43</td><td>723.13</td><td>50</td></tr><tr><td>Craft an iron nugget</td><td>12</td><td>30.99</td><td>4697.23</td><td>525.29</td><td>71</td></tr><tr><td>Craft a blast furnace</td><td>14</td><td>25.71</td><td>7760.67</td><td>711.05</td><td>35</td></tr><tr><td>Craft a stonecutter</td><td>13</td><td>34.78</td><td>5993.38</td><td>675.52</td><td>46</td></tr></table>
<table><tbody><tr><td>任务</td><td>子目标数</td><td>SR</td><td>AS</td><td>AT(s)</td><td>评估次数</td></tr><tr><td>制作一个铁锹</td><td>13</td><td>54.79</td><td>5677.35</td><td>637.81</td><td>73</td></tr><tr><td>制作一个铁镐</td><td>13</td><td>59.42</td><td>6157.39</td><td>591.81</td><td>69</td></tr><tr><td>制作一个铁斧</td><td>13</td><td>54.29</td><td>6026.26</td><td>676.97</td><td>70</td></tr><tr><td>制作一个铁锄</td><td>13</td><td>52.70</td><td>6650.97</td><td>743.82</td><td>74</td></tr><tr><td>制作一个桶</td><td>13</td><td>54.29</td><td>6124.61</td><td>591.35</td><td>70</td></tr><tr><td>制作一个漏斗</td><td>14</td><td>46.67</td><td>7242.14</td><td>710.17</td><td>60</td></tr><tr><td>制作一段铁轨</td><td>13</td><td>42.19</td><td>6713.07</td><td>754.48</td><td>64</td></tr><tr><td>制作一把铁剑</td><td>12</td><td>57.14</td><td>5625.49</td><td>633.91</td><td>70</td></tr><tr><td>制作一把剪刀</td><td>12</td><td>53.62</td><td>5058.00</td><td>570.35</td><td>69</td></tr><tr><td>制作一张铁匠工作台</td><td>12</td><td>44.93</td><td>5317.39</td><td>594.81</td><td>69</td></tr><tr><td>制作一个绊线钩</td><td>13</td><td>48.57</td><td>4968.74</td><td>562.66</td><td>70</td></tr><tr><td>制作一段链子</td><td>13</td><td>44.93</td><td>5764.42</td><td>645.33</td><td>69</td></tr><tr><td>制作一些铁栏杆</td><td>12</td><td>42.00</td><td>6508.43</td><td>723.13</td><td>50</td></tr><tr><td>制作一个铁粒</td><td>12</td><td>30.99</td><td>4697.23</td><td>525.29</td><td>71</td></tr><tr><td>制作一个高炉</td><td>14</td><td>25.71</td><td>7760.67</td><td>711.05</td><td>35</td></tr><tr><td>制作一个切石机</td><td>13</td><td>34.78</td><td>5993.38</td><td>675.52</td><td>46</td></tr></tbody></table>


Table 11: The results of Optimus-1 on various tasks in the Gold group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively.
表11：Optimus-1 在 Gold 组各任务上的结果。SR、AS、AT 分别表示成功率、平均步数和平均时间（秒）。


<table><tr><td>Task</td><td>Sub Goal Num.</td><td>SR</td><td>AS</td><td>AT(s)</td><td>Eval Times</td></tr><tr><td>Craft a golden shovel</td><td>16</td><td>9.80</td><td>13734.75</td><td>686.74</td><td>51</td></tr><tr><td>Craft a golden pickaxe</td><td>16</td><td>13.75</td><td>9672.00</td><td>783.60</td><td>80</td></tr><tr><td>Craft a golden axe</td><td>16</td><td>4.44</td><td>10158.75</td><td>707.94</td><td>45</td></tr><tr><td>Craft a golden hoe</td><td>16</td><td>3.33</td><td>13120.50</td><td>756.03</td><td>27</td></tr><tr><td>Craft a golden sword</td><td>16</td><td>3.33</td><td>9792.00</td><td>789.60</td><td>26</td></tr><tr><td>Smelt and craft a golden ingot</td><td>15</td><td>16.42</td><td>9630.27</td><td>681.51</td><td>67</td></tr></table>
<table><tbody><tr><td>任务</td><td>子目标编号</td><td>SR</td><td>AS</td><td>AT(秒)</td><td>评估次数</td></tr><tr><td>合成一把金锹</td><td>16</td><td>9.80</td><td>13734.75</td><td>686.74</td><td>51</td></tr><tr><td>合成一把金镐</td><td>16</td><td>13.75</td><td>9672.00</td><td>783.60</td><td>80</td></tr><tr><td>合成一把金斧</td><td>16</td><td>4.44</td><td>10158.75</td><td>707.94</td><td>45</td></tr><tr><td>合成一把金铲</td><td>16</td><td>3.33</td><td>13120.50</td><td>756.03</td><td>27</td></tr><tr><td>合成一把金剑</td><td>16</td><td>3.33</td><td>9792.00</td><td>789.60</td><td>26</td></tr><tr><td>熔炼并合成一个金锭</td><td>15</td><td>16.42</td><td>9630.27</td><td>681.51</td><td>67</td></tr></tbody></table>


Table 12: The results of Optimus-1 on various tasks in the Diamond group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively.
表12：Optimus-1 在 Diamond 组各任务上的结果。SR、AS、AT 分别表示成功率、平均步数和平均时间（秒）。


<table><tr><td>Task</td><td>Sub Goal Num.</td><td>SR</td><td>AS</td><td>AT(s)</td><td>Eval Times</td></tr><tr><td>Craft a diamond shovel</td><td>15</td><td>18.75</td><td>23696.75</td><td>1184.84</td><td>64</td></tr><tr><td>Craft a diamond pickaxe</td><td>15</td><td>15.71</td><td>32189.50</td><td>1609.46</td><td>70</td></tr><tr><td>Craft a diamond axe</td><td>16</td><td>4.00</td><td>21920.50</td><td>1096.03</td><td>75</td></tr><tr><td>Craft a diamond hoe</td><td>15</td><td>4.61</td><td>24031.00</td><td>1201.55</td><td>65</td></tr><tr><td>Craft a diamond sword</td><td>15</td><td>14.52</td><td>27555.50</td><td>1377.78</td><td>62</td></tr><tr><td>Dig down and mine a diamond</td><td>15</td><td>9.09</td><td>20782.13</td><td>1039.11</td><td>64</td></tr><tr><td>Craft a jukebox</td><td>15</td><td>14.58</td><td>25056.00</td><td>1252.80</td><td>48</td></tr></table>
<table><tbody><tr><td>任务</td><td>子目标编号</td><td>成功率</td><td>辅助支持</td><td>平均时间(秒)</td><td>评估次数</td></tr><tr><td>合成钻石铲</td><td>15</td><td>18.75</td><td>23696.75</td><td>1184.84</td><td>64</td></tr><tr><td>合成钻石镐</td><td>15</td><td>15.71</td><td>32189.50</td><td>1609.46</td><td>70</td></tr><tr><td>合成钻石斧</td><td>16</td><td>4.00</td><td>21920.50</td><td>1096.03</td><td>75</td></tr><tr><td>合成钻石锄</td><td>15</td><td>4.61</td><td>24031.00</td><td>1201.55</td><td>65</td></tr><tr><td>合成钻石剑</td><td>15</td><td>14.52</td><td>27555.50</td><td>1377.78</td><td>62</td></tr><tr><td>向下挖掘并采到钻石</td><td>15</td><td>9.09</td><td>20782.13</td><td>1039.11</td><td>64</td></tr><tr><td>合成唱片机</td><td>15</td><td>14.58</td><td>25056.00</td><td>1252.80</td><td>48</td></tr></tbody></table>


Table 13: The results of Optimus-1 on various tasks in the Redstone group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively.
表13：Optimus-1在Redstone组各任务上的结果。SR、AS、AT分别表示成功率、平均步数和平均时间（秒）。


<table><tr><td>Language Instruction</td><td>Sub-Goal Num.</td><td>SR</td><td>AS</td><td>AT(s)</td><td>Eval Times</td></tr><tr><td>Craft a piston</td><td>16</td><td>28.57</td><td>6457.10</td><td>822.85</td><td>35</td></tr><tr><td>Craft a redstone torch</td><td>16</td><td>29.63</td><td>6787.87</td><td>939.39</td><td>27</td></tr><tr><td>Craft an activator rail</td><td>18</td><td>15.68</td><td>8685.62</td><td>934.28</td><td>51</td></tr><tr><td>Craft a compass</td><td>23</td><td>15.00</td><td>14908.67</td><td>845.43</td><td>40</td></tr><tr><td>Craft a dropper</td><td>16</td><td>37.50</td><td>7272.80</td><td>1063.64</td><td>40</td></tr><tr><td>Craft a note block</td><td>16</td><td>24.32</td><td>6727.89</td><td>936.39</td><td>37</td></tr></table>
<table><tbody><tr><td>语言说明</td><td>子目标编号</td><td>SR</td><td>AS</td><td>AT(秒)</td><td>评估次数</td></tr><tr><td>制作一个活塞</td><td>16</td><td>28.57</td><td>6457.10</td><td>822.85</td><td>35</td></tr><tr><td>制作一根红石火把</td><td>16</td><td>29.63</td><td>6787.87</td><td>939.39</td><td>27</td></tr><tr><td>制作一个触发铁轨</td><td>18</td><td>15.68</td><td>8685.62</td><td>934.28</td><td>51</td></tr><tr><td>制作一个指南针</td><td>23</td><td>15.00</td><td>14908.67</td><td>845.43</td><td>40</td></tr><tr><td>制作一个投掷器</td><td>16</td><td>37.50</td><td>7272.80</td><td>1063.64</td><td>40</td></tr><tr><td>制作一个音符盒</td><td>16</td><td>24.32</td><td>6727.89</td><td>936.39</td><td>37</td></tr></tbody></table>


Table 14: The results of Optimus-1 on various tasks in the Armor group. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively.
表14：Optimus-1 在 Armor 组各任务上的结果。SR、AS、AT 分别表示成功率、平均步数和平均时间（秒）。


<table><tr><td>Task</td><td>Sub Goal Num.</td><td>SR</td><td>AS</td><td>AT(s)</td><td>Eval Times</td></tr><tr><td>Craft shield</td><td>14</td><td>43.33</td><td>7229.00</td><td>861.45</td><td>30</td></tr><tr><td>Craft iron chestplate</td><td>14</td><td>47.22</td><td>7230.24</td><td>851.51</td><td>36</td></tr><tr><td>Craft iron boots</td><td>14</td><td>23.81</td><td>6597.33</td><td>729.87</td><td>42</td></tr><tr><td>Craft iron leggings</td><td>14</td><td>6.67</td><td>9279.00</td><td>763.95</td><td>30</td></tr><tr><td>Craft iron helmet</td><td>14</td><td>58.14</td><td>6287.11</td><td>814.36</td><td>43</td></tr><tr><td>Craft diamond helmet</td><td>17</td><td>2.08</td><td>7342.00</td><td>867.10</td><td>48</td></tr><tr><td>Craft diamond chestplate</td><td>17</td><td>2.70</td><td>7552.00</td><td>777.60</td><td>37</td></tr><tr><td>Craft diamond leggings</td><td>17</td><td>9.68</td><td>7664.67</td><td>883.23</td><td>31</td></tr><tr><td>Craft diamond boots</td><td>17</td><td>16.67</td><td>10065.60</td><td>803.28</td><td>30</td></tr><tr><td>Craft golden helmet</td><td>17</td><td>12.50</td><td>11563.25</td><td>778.16</td><td>32</td></tr><tr><td>Craft golden leggings</td><td>17</td><td>14.60</td><td>10107.33</td><td>805.37</td><td>41</td></tr><tr><td>Craft golden boots</td><td>17</td><td>6.06</td><td>10311.00</td><td>915.55</td><td>33</td></tr><tr><td>Craft golden chestplate</td><td>17</td><td>9.67</td><td>10407.58</td><td>820.38</td><td>31</td></tr></table>
<table><tbody><tr><td>任务</td><td>子目标数</td><td>成功率</td><td>助攻次数</td><td>攻击时间(s)</td><td>评估次数</td></tr><tr><td>制作盾牌</td><td>14</td><td>43.33</td><td>7229.00</td><td>861.45</td><td>30</td></tr><tr><td>制作铁胸甲</td><td>14</td><td>47.22</td><td>7230.24</td><td>851.51</td><td>36</td></tr><tr><td>制作铁靴</td><td>14</td><td>23.81</td><td>6597.33</td><td>729.87</td><td>42</td></tr><tr><td>制作铁护腿</td><td>14</td><td>6.67</td><td>9279.00</td><td>763.95</td><td>30</td></tr><tr><td>制作铁头盔</td><td>14</td><td>58.14</td><td>6287.11</td><td>814.36</td><td>43</td></tr><tr><td>制作钻石头盔</td><td>17</td><td>2.08</td><td>7342.00</td><td>867.10</td><td>48</td></tr><tr><td>制作钻石胸甲</td><td>17</td><td>2.70</td><td>7552.00</td><td>777.60</td><td>37</td></tr><tr><td>制作钻石护腿</td><td>17</td><td>9.68</td><td>7664.67</td><td>883.23</td><td>31</td></tr><tr><td>制作钻石靴</td><td>17</td><td>16.67</td><td>10065.60</td><td>803.28</td><td>30</td></tr><tr><td>制作金头盔</td><td>17</td><td>12.50</td><td>11563.25</td><td>778.16</td><td>32</td></tr><tr><td>制作金护腿</td><td>17</td><td>14.60</td><td>10107.33</td><td>805.37</td><td>41</td></tr><tr><td>制作金靴</td><td>17</td><td>6.06</td><td>10311.00</td><td>915.55</td><td>33</td></tr><tr><td>制作金胸甲</td><td>17</td><td>9.67</td><td>10407.58</td><td>820.38</td><td>31</td></tr></tbody></table>


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_ca6565.jpg"/>



Figure 8: An illustration of Optimus-1 unlocking the tech tree in Minecraft.
图8：展示 Optimus-1 在 Minecraft 中解锁科技树的示意图。


Table 15: Result on Process-Dependent Tasks compared with MP5 [33]. SR, AS, AT denote success rate, average number of steps, and average time (seconds), respectively.
表15：在过程依赖任务上的结果，与 MP5 [33] 对比。SR、AS、AT 分别表示成功率、平均步数和平均时间（秒）。


<table><tr><td colspan="2" rowspan="2">Task Level</td><td>MP5 [33]</td><td colspan="3">Optimus-1</td></tr><tr><td>SR</td><td>SR</td><td>AS</td><td>AT(s)</td></tr><tr><td rowspan="6">Basic Level</td><td>log</td><td>96.67</td><td>100.00</td><td>586.58</td><td>29.33</td></tr><tr><td>sand</td><td>96.67</td><td>94.32</td><td>1540.33</td><td>77.02</td></tr><tr><td>planks</td><td>96.67</td><td>100.00</td><td>571.06</td><td>28.55</td></tr><tr><td>stick</td><td>96.67</td><td>97.14</td><td>372.97</td><td>18.65</td></tr><tr><td>crafting table</td><td>93.33</td><td>98.55</td><td>448.63</td><td>22.43</td></tr><tr><td>Average</td><td>96.00</td><td>98.00</td><td>703.91</td><td>35.20</td></tr><tr><td rowspan="6">Wooden Level</td><td>bowl</td><td>93.33</td><td>100.00</td><td>744.30</td><td>37.21</td></tr><tr><td>boat</td><td>93.33</td><td>92.86</td><td>1170.00</td><td>58.50</td></tr><tr><td>chest</td><td>90.00</td><td>100.00</td><td>573.80</td><td>28.69</td></tr><tr><td>wooden sword</td><td>86.67</td><td>100.00</td><td>1214.90</td><td>60.74</td></tr><tr><td>wooden pickaxe</td><td>80.00</td><td>100.00</td><td>1153.91</td><td>57.70</td></tr><tr><td>Average</td><td>88.67</td><td>98.57</td><td>971.38</td><td>48.56</td></tr><tr><td rowspan="6">Stone Level</td><td>cobblestone</td><td>80.00</td><td>95.29</td><td>1492.00</td><td>74.60</td></tr><tr><td>furnace</td><td>80.00</td><td>93.75</td><td>2842.71</td><td>142.14</td></tr><tr><td>stone pickaxe</td><td>80.00</td><td>96.77</td><td>2310.09</td><td>115.50</td></tr><tr><td>iron ore</td><td>60.00</td><td>50.00</td><td>3017.00</td><td>150.85</td></tr><tr><td>glass</td><td>80.00</td><td>81.11</td><td>3870.75</td><td>193.54</td></tr><tr><td>Average</td><td>76.00</td><td>83.38</td><td>2706.51</td><td>135.32</td></tr><tr><td rowspan="6">Iron Level</td><td>iron ingot</td><td>56.67</td><td>59.42</td><td>4697.23</td><td>634.86</td></tr><tr><td>shield *</td><td>56.67</td><td>43.33</td><td>7229.00</td><td>661.45</td></tr><tr><td>bucket</td><td>53.33</td><td>54.29</td><td>6124.61</td><td>606.23</td></tr><tr><td>iron pickaxe</td><td>50.00</td><td>59.42</td><td>6157.39</td><td>607.87</td></tr><tr><td>iron door</td><td>43.33</td><td>48.28</td><td>5528.00</td><td>676.40</td></tr><tr><td>Average</td><td>52.00</td><td>52.94</td><td>5947.25</td><td>637.36</td></tr><tr><td rowspan="6">Diamond Level</td><td>diamond ore *</td><td>30.00</td><td>9.09</td><td>20782.13</td><td>1039.10</td></tr><tr><td>mind redstone</td><td>20.00</td><td>25.12</td><td>6787.87</td><td>739.39</td></tr><tr><td>compass</td><td>16.67</td><td>15.00</td><td>14908.67</td><td>745.43</td></tr><tr><td>diamond pickaxe</td><td>23.33</td><td>15.71</td><td>32189.50</td><td>1609.48</td></tr><tr><td>piston</td><td>20.00</td><td>28.57</td><td>6457.10</td><td>622.85</td></tr><tr><td>Average</td><td>22.00</td><td>18.70</td><td>14963.83</td><td>948.19</td></tr></table>
<table><tbody><tr><td colspan="2" rowspan="2">任务等级</td><td>MP5 [33]</td><td colspan="3">Optimus-1</td></tr><tr><td>SR</td><td>SR</td><td>AS</td><td>AT(s)</td></tr><tr><td rowspan="6">基础等级</td><td>原木</td><td>96.67</td><td>100.00</td><td>586.58</td><td>29.33</td></tr><tr><td>沙子</td><td>96.67</td><td>94.32</td><td>1540.33</td><td>77.02</td></tr><tr><td>木板</td><td>96.67</td><td>100.00</td><td>571.06</td><td>28.55</td></tr><tr><td>木棍</td><td>96.67</td><td>97.14</td><td>372.97</td><td>18.65</td></tr><tr><td>工作台</td><td>93.33</td><td>98.55</td><td>448.63</td><td>22.43</td></tr><tr><td>平均</td><td>96.00</td><td>98.00</td><td>703.91</td><td>35.20</td></tr><tr><td rowspan="6">木制等级</td><td>碗</td><td>93.33</td><td>100.00</td><td>744.30</td><td>37.21</td></tr><tr><td>船</td><td>93.33</td><td>92.86</td><td>1170.00</td><td>58.50</td></tr><tr><td>箱子</td><td>90.00</td><td>100.00</td><td>573.80</td><td>28.69</td></tr><tr><td>木剑</td><td>86.67</td><td>100.00</td><td>1214.90</td><td>60.74</td></tr><tr><td>木镐</td><td>80.00</td><td>100.00</td><td>1153.91</td><td>57.70</td></tr><tr><td>平均</td><td>88.67</td><td>98.57</td><td>971.38</td><td>48.56</td></tr><tr><td rowspan="6">石制等级</td><td>圆石</td><td>80.00</td><td>95.29</td><td>1492.00</td><td>74.60</td></tr><tr><td>熔炉</td><td>80.00</td><td>93.75</td><td>2842.71</td><td>142.14</td></tr><tr><td>石镐</td><td>80.00</td><td>96.77</td><td>2310.09</td><td>115.50</td></tr><tr><td>铁矿</td><td>60.00</td><td>50.00</td><td>3017.00</td><td>150.85</td></tr><tr><td>玻璃</td><td>80.00</td><td>81.11</td><td>3870.75</td><td>193.54</td></tr><tr><td>平均</td><td>76.00</td><td>83.38</td><td>2706.51</td><td>135.32</td></tr><tr><td rowspan="6">铁制等级</td><td>铁锭</td><td>56.67</td><td>59.42</td><td>4697.23</td><td>634.86</td></tr><tr><td>盾牌 *</td><td>56.67</td><td>43.33</td><td>7229.00</td><td>661.45</td></tr><tr><td>桶</td><td>53.33</td><td>54.29</td><td>6124.61</td><td>606.23</td></tr><tr><td>铁镐</td><td>50.00</td><td>59.42</td><td>6157.39</td><td>607.87</td></tr><tr><td>铁门</td><td>43.33</td><td>48.28</td><td>5528.00</td><td>676.40</td></tr><tr><td>平均</td><td>52.00</td><td>52.94</td><td>5947.25</td><td>637.36</td></tr><tr><td rowspan="6">钻石等级</td><td>钻石矿石 *</td><td>30.00</td><td>9.09</td><td>20782.13</td><td>1039.10</td></tr><tr><td>采集红石</td><td>20.00</td><td>25.12</td><td>6787.87</td><td>739.39</td></tr><tr><td>指南针</td><td>16.67</td><td>15.00</td><td>14908.67</td><td>745.43</td></tr><tr><td>钻石镐</td><td>23.33</td><td>15.71</td><td>32189.50</td><td>1609.48</td></tr><tr><td>活塞</td><td>20.00</td><td>28.57</td><td>6457.10</td><td>622.85</td></tr><tr><td>平均</td><td>22.00</td><td>18.70</td><td>14963.83</td><td>948.19</td></tr></tbody></table>


Table 16: Results (success rate) on 8 META TASK groups compared with DEPS [50].
表16：与 DEPS [50] 比较的 8 个 META TASK 组上的结果（成功率）。


<table><tr><td>Meta-Task</td><td>Task Object</td><td>InnerMonologue</td><td>Code-as-Policy</td><td>DEPS [50]</td><td>Ours</td></tr><tr><td rowspan="7">Basic MT1</td><td>planks</td><td>83.3</td><td>83.3</td><td>83.3</td><td>100.0</td></tr><tr><td>stick</td><td>83.3</td><td>83.3</td><td>86.7</td><td>97.1</td></tr><tr><td>chest</td><td>0.0</td><td>50.0</td><td>76.7</td><td>100.0</td></tr><tr><td>sign</td><td>0.0</td><td>43.3</td><td>86.7</td><td>94.3</td></tr><tr><td>boat</td><td>26.7</td><td>56.7</td><td>73.3</td><td>92.9</td></tr><tr><td>trapdoor</td><td>56.7</td><td>56.7</td><td>76.7</td><td>96.2</td></tr><tr><td>bowl</td><td>23.3</td><td>46.7</td><td>80.0</td><td>100.0</td></tr><tr><td rowspan="10">Tool(Simple) MT2</td><td>crafting_table</td><td>70.0</td><td>70.0</td><td>90.0</td><td>98.5</td></tr><tr><td>wooden_pickaxe</td><td>80.0</td><td>80.0</td><td>80.0</td><td>100.0</td></tr><tr><td>wooden_sword</td><td>83.3</td><td>83.3</td><td>86.7</td><td>100.0</td></tr><tr><td>wooden_shovel</td><td>76.7</td><td>76.7</td><td>90.0</td><td>95.0</td></tr><tr><td>furnace</td><td>40.0</td><td>40.0</td><td>66.7</td><td>93.7</td></tr><tr><td>stone_pickaxe</td><td>36.7</td><td>53.3</td><td>73.3</td><td>96.7</td></tr><tr><td>stone_axe</td><td>30.0</td><td>30.0</td><td>70.0</td><td>96.8</td></tr><tr><td>stone_hoe</td><td>36.7</td><td>56.7</td><td>66.7</td><td>94.6</td></tr><tr><td>stone_shovel</td><td>36.7</td><td>36.7</td><td>66.7</td><td>90.3</td></tr><tr><td>stone_sword</td><td>53.3</td><td>36.7</td><td>80.0</td><td>94.2</td></tr><tr><td rowspan="6">Hunt and Food MT3</td><td>bed</td><td>6.7</td><td>6.7</td><td>43.3</td><td>90.0</td></tr><tr><td>painting</td><td>16.7</td><td>16.7</td><td>86.7</td><td>92.2</td></tr><tr><td>carpet</td><td>0.0</td><td>13.3</td><td>43.3</td><td>91.3</td></tr><tr><td>cooked_porkchop</td><td>0.0</td><td>0.0</td><td>50.0</td><td>90.0</td></tr><tr><td>cooked_beef</td><td>0.0</td><td>0.0</td><td>63.3</td><td>90.0</td></tr><tr><td>cooked_mutton</td><td>0.0</td><td>0.0</td><td>66.7</td><td>90.0</td></tr><tr><td rowspan="5">Dig-down MT4</td><td>stone_stairs</td><td>36.7</td><td>16.7</td><td>66.7</td><td>90.3</td></tr><tr><td>stone_slab</td><td>16.7</td><td>33.3</td><td>73.3</td><td>91.2</td></tr><tr><td>lever</td><td>46.7</td><td>46.7</td><td>83.3</td><td>91.0</td></tr><tr><td>coal</td><td>6.7</td><td>0.0</td><td>20.0</td><td>86.5</td></tr><tr><td>torch</td><td>6.7</td><td>0.0</td><td>13.3</td><td>85.7</td></tr><tr><td rowspan="9">Equipment MT5</td><td>leather_boots</td><td>13.3</td><td>13.3</td><td>60.0</td><td>68.2</td></tr><tr><td>leather_chestplate</td><td>0.0</td><td>6.7</td><td>36.7</td><td>64.2</td></tr><tr><td>leather_helmet</td><td>6.7</td><td>0.0</td><td>70.0</td><td>65.9</td></tr><tr><td>leather_leggings</td><td>20.0</td><td>0.0</td><td>56.7</td><td>65.5</td></tr><tr><td>iron_chestplate</td><td>0.0</td><td>0.0</td><td>0.0</td><td>47.2</td></tr><tr><td>iron_leggings</td><td>0.0</td><td>0.0</td><td>3.3</td><td>6.6</td></tr><tr><td>iron_helmet</td><td>0.0</td><td>0.0</td><td>3.3</td><td>58.1</td></tr><tr><td>iron_boots</td><td>0.0</td><td>0.0</td><td>20.0</td><td>23.8</td></tr><tr><td>shield</td><td>0.0</td><td>6.7</td><td>13.3</td><td>43.3</td></tr><tr><td rowspan="7">Tool Complex MT6</td><td>bucket</td><td>0.0</td><td>3.3</td><td>6.7</td><td>54.3</td></tr><tr><td>shears</td><td>0.0</td><td>0.0</td><td>30.0</td><td>53.6</td></tr><tr><td>iron_pickaxe</td><td>6.7</td><td>0.0</td><td>10.0</td><td>59.4</td></tr><tr><td>iron_axe</td><td>0.0</td><td>0.0</td><td>16.7</td><td>54.3</td></tr><tr><td>iron_hoe</td><td>0.0</td><td>0.0</td><td>13.3</td><td>52.7</td></tr><tr><td>iron_shovel</td><td>0.0</td><td>0.0</td><td>13.3</td><td>57.8</td></tr><tr><td>iron_sword</td><td>0.0</td><td>3.3</td><td>6.7</td><td>54.7</td></tr><tr><td rowspan="5">Iron-Stage MT7</td><td>iron_bars</td><td>0.0</td><td>0.0</td><td>6.7</td><td>42.0</td></tr><tr><td>hopper</td><td>0.0</td><td>0.0</td><td>6.7</td><td>46.7</td></tr><tr><td>iron_door</td><td>0.0</td><td>0.0</td><td>3.3</td><td>48.3</td></tr><tr><td>tripwire_hook</td><td>6.7</td><td>0.0</td><td>30.0</td><td>48.6</td></tr><tr><td>rail</td><td>0.0</td><td>0.0</td><td>6.7</td><td>42.2</td></tr><tr><td>Challenge MT8</td><td>diamond</td><td>0.0</td><td>0.0</td><td>0.6</td><td>9.1</td></tr></table>
<table><tbody><tr><td>元任务</td><td>任务对象</td><td>内心独白</td><td>代码即策略</td><td>DEPS [50]</td><td>我们的</td></tr><tr><td rowspan="7">基础 MT1</td><td>木板</td><td>83.3</td><td>83.3</td><td>83.3</td><td>100.0</td></tr><tr><td>木棍</td><td>83.3</td><td>83.3</td><td>86.7</td><td>97.1</td></tr><tr><td>箱子</td><td>0.0</td><td>50.0</td><td>76.7</td><td>100.0</td></tr><tr><td>告示牌</td><td>0.0</td><td>43.3</td><td>86.7</td><td>94.3</td></tr><tr><td>船</td><td>26.7</td><td>56.7</td><td>73.3</td><td>92.9</td></tr><tr><td>活板门</td><td>56.7</td><td>56.7</td><td>76.7</td><td>96.2</td></tr><tr><td>碗</td><td>23.3</td><td>46.7</td><td>80.0</td><td>100.0</td></tr><tr><td rowspan="10">工具（简单） MT2</td><td>工作台</td><td>70.0</td><td>70.0</td><td>90.0</td><td>98.5</td></tr><tr><td>木镐</td><td>80.0</td><td>80.0</td><td>80.0</td><td>100.0</td></tr><tr><td>木剑</td><td>83.3</td><td>83.3</td><td>86.7</td><td>100.0</td></tr><tr><td>木铲</td><td>76.7</td><td>76.7</td><td>90.0</td><td>95.0</td></tr><tr><td>熔炉</td><td>40.0</td><td>40.0</td><td>66.7</td><td>93.7</td></tr><tr><td>石镐</td><td>36.7</td><td>53.3</td><td>73.3</td><td>96.7</td></tr><tr><td>石斧</td><td>30.0</td><td>30.0</td><td>70.0</td><td>96.8</td></tr><tr><td>石锄</td><td>36.7</td><td>56.7</td><td>66.7</td><td>94.6</td></tr><tr><td>石铲</td><td>36.7</td><td>36.7</td><td>66.7</td><td>90.3</td></tr><tr><td>石剑</td><td>53.3</td><td>36.7</td><td>80.0</td><td>94.2</td></tr><tr><td rowspan="6">狩猎与食物 MT3</td><td>床</td><td>6.7</td><td>6.7</td><td>43.3</td><td>90.0</td></tr><tr><td>画</td><td>16.7</td><td>16.7</td><td>86.7</td><td>92.2</td></tr><tr><td>地毯</td><td>0.0</td><td>13.3</td><td>43.3</td><td>91.3</td></tr><tr><td>熟猪排</td><td>0.0</td><td>0.0</td><td>50.0</td><td>90.0</td></tr><tr><td>熟牛排</td><td>0.0</td><td>0.0</td><td>63.3</td><td>90.0</td></tr><tr><td>熟羊肉</td><td>0.0</td><td>0.0</td><td>66.7</td><td>90.0</td></tr><tr><td rowspan="5">向下挖 MT4</td><td>石阶</td><td>36.7</td><td>16.7</td><td>66.7</td><td>90.3</td></tr><tr><td>石台阶</td><td>16.7</td><td>33.3</td><td>73.3</td><td>91.2</td></tr><tr><td>拉杆</td><td>46.7</td><td>46.7</td><td>83.3</td><td>91.0</td></tr><tr><td>煤炭</td><td>6.7</td><td>0.0</td><td>20.0</td><td>86.5</td></tr><tr><td>火把</td><td>6.7</td><td>0.0</td><td>13.3</td><td>85.7</td></tr><tr><td rowspan="9">装备 MT5</td><td>皮靴</td><td>13.3</td><td>13.3</td><td>60.0</td><td>68.2</td></tr><tr><td>皮胸甲</td><td>0.0</td><td>6.7</td><td>36.7</td><td>64.2</td></tr><tr><td>皮头盔</td><td>6.7</td><td>0.0</td><td>70.0</td><td>65.9</td></tr><tr><td>皮护腿</td><td>20.0</td><td>0.0</td><td>56.7</td><td>65.5</td></tr><tr><td>铁胸甲</td><td>0.0</td><td>0.0</td><td>0.0</td><td>47.2</td></tr><tr><td>铁护腿</td><td>0.0</td><td>0.0</td><td>3.3</td><td>6.6</td></tr><tr><td>铁头盔</td><td>0.0</td><td>0.0</td><td>3.3</td><td>58.1</td></tr><tr><td>铁靴</td><td>0.0</td><td>0.0</td><td>20.0</td><td>23.8</td></tr><tr><td>盾牌</td><td>0.0</td><td>6.7</td><td>13.3</td><td>43.3</td></tr><tr><td rowspan="7">工具（复杂） MT6</td><td>桶</td><td>0.0</td><td>3.3</td><td>6.7</td><td>54.3</td></tr><tr><td>剪刀</td><td>0.0</td><td>0.0</td><td>30.0</td><td>53.6</td></tr><tr><td>铁镐</td><td>6.7</td><td>0.0</td><td>10.0</td><td>59.4</td></tr><tr><td>铁斧</td><td>0.0</td><td>0.0</td><td>16.7</td><td>54.3</td></tr><tr><td>铁锄</td><td>0.0</td><td>0.0</td><td>13.3</td><td>52.7</td></tr><tr><td>铁铲</td><td>0.0</td><td>0.0</td><td>13.3</td><td>57.8</td></tr><tr><td>铁剑</td><td>0.0</td><td>3.3</td><td>6.7</td><td>54.7</td></tr><tr><td rowspan="5">铁阶段 MT7</td><td>铁栏杆</td><td>0.0</td><td>0.0</td><td>6.7</td><td>42.0</td></tr><tr><td>漏斗</td><td>0.0</td><td>0.0</td><td>6.7</td><td>46.7</td></tr><tr><td>铁门</td><td>0.0</td><td>0.0</td><td>3.3</td><td>48.3</td></tr><tr><td>绊线钩</td><td>6.7</td><td>0.0</td><td>30.0</td><td>48.6</td></tr><tr><td>铁轨</td><td>0.0</td><td>0.0</td><td>6.7</td><td>42.2</td></tr><tr><td>挑战 MT8</td><td>钻石</td><td>0.0</td><td>0.0</td><td>0.6</td><td>9.1</td></tr></tbody></table>


## G Case Study
## G 案例研究


This section introduces several cases to comprehensively demonstrate Optimus-1's capabilities.
本节介绍若干案例以全面展示 Optimus-1 的能力。


Figures 9, 10, and 11 demonstrate the superiority of our reflection mechanism, which dynamically adjusts the plan based on the current game progress.
图 9、10 和 11 展示了我们反思机制的优越性，该机制会根据当前游戏进度动态调整计划。


- Figure 9 illustrates Optimus-1's replanning ability. When Optimus-1 realizes it cannot complete a task (such as a craft failure shown in the figure), it will replan the current task and continue execution.
- 图 9 展示了 Optimus-1 的重规划能力。当 Optimus-1 发现无法完成某项任务（如图中所示的制作失败），它会对当前任务进行重规划并继续执行。


- Figures 10 and 11 showcase Optimus-1's ability to make judgments based on visual signals. When Optimus-1 determines that it has completed a task (such as "kill a cow", in Figure 10), it will finish the current task and move on to the next one. If Optimus-1 discovers that it has not yet completed the task and the task has not failed(as shown in Figure 11), it will continue executing the task.
- 图 10 和 11 展示了 Optimus-1 根据视觉信号做出判断的能力。当 Optimus-1 判定已完成任务（如图 10 中的“找到并击杀一头牛”），它会结束当前任务并转入下一个任务。如果 Optimus-1 发现任务尚未完成且任务未失败（如图 11 所示），则会继续执行该任务。


Figures 12 and 13 illustrate the advantages of planning with knowledge. With the Hierarchical Directed Knowledge Graph, we can generate a high-quality plan in one step and dynamically adjust the plan based on current visual signals.
图 12 和 13 说明了基于知识规划的优势。借助层次定向知识图谱，我们可以一步生成高质量计划，并根据当前视觉信号动态调整该计划。


- Figure 12 demonstrates the importance of knowledge. For a long-horizon task such as "Mine 1 diamond Q," Optimus-1 first generates a plan based on the Hierarchical Directed Knowledge Graph. However, this plan needs to be adjusted based on the current visual signals. For example, in this figure, Optimus-1 appears in a cave, so the primary task is not to "chop a tree" but to "leave the cave" first. Only after exiting the cave can Optimus-1 proceed with the initial plan.
- 图 12 说明了知识的重要性。对于像“挖到 1 颗钻石 Q”这样的长时序任务，Optimus-1 首先基于层次定向知识图谱生成计划，但该计划需要根据当前视觉信号进行调整。例如在本图中，Optimus-1 出现在一个洞穴中，因此首要任务不是“砍树”，而是先“离开洞穴”。只有退出洞穴后，Optimus-1 才能继续执行初始计划。


- Figure 13 demonstrates the high efficiency of our method. Agents like MP5 [33] and Voyager [47] use an iterative planning approach, which is very time-consuming, generating the final plan step by step. During this process, agent does not take any action. As shown in Figure 13, a zombie is gradually approaching the agent, but the agent is still iterating on its plan. Optimus-1, however, generates the plan in one step based on the Hierarchical Directed Knowledge Graph and makes reasonable plans based on the current visual signals.
- 图 13 展示了我们方法的高效性。像 MP5 [33] 和 Voyager [47] 这样的智能体采用迭代规划方式，极其耗时，逐步生成最终计划。在此过程中，智能体不会采取任何行动。如图 13 所示，一只僵尸逐步接近智能体，但该智能体仍在迭代其计划。而 Optimus-1 则基于层次定向知识图谱一步生成计划，并结合当前视觉信号制定合理方案。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_137493.jpg"/>



Figure 9: The process of completing the task "Craft 1 wooden pickaxe". Optimus-1 gives wrong planning. When Optimus-1 realizes it cannot complete the task, it will replan the current task.
图 9：完成任务“制作 1 把木镐”的过程。Optimus-1 给出了错误的计划。当 Optimus-1 意识到无法完成任务时，会对当前任务进行重规划。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_23344f.jpg"/>



Figure 10: The process of completing the task "Find a cow and kill it". Hierarchical Directed Knowledge Graph indicates that having a wooden sword will make the task easier to complete. Therefore, Optimus-1 first crafts a wooden sword and then proceeds to find and kill a cow.
图 10：完成任务“找到一头牛并击杀它”的过程。层次定向知识图谱表明拥有一把木剑会使任务更容易完成。因此，Optimus-1 先制作一把木剑，然后去寻找并击杀牛。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_f19aff.jpg"/>



Figure 11: The process of completing the task "Chop tree to obtain 10 logs". Hierarchical Directed Knowledge graph indicates that no tools are needed to complete this goal. After finding a tree, Optimus-1 starts chopping it down. The task requires a substantial amount of wood, so midway through, Optimus-1 performs a reflection. The task is not yet complete but is progressing smoothly, and the result of the reflection is to continue.
图 11：完成任务“砍树以获得 10 根原木”的过程。层次定向知识图谱表明完成该目标无需工具。找到树后，Optimus-1 开始砍伐。由于任务需要大量木材，途中 Optimus-1 进行了反思。任务尚未完成但进展顺利，反思结果是继续执行。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_3bdd58.jpg"/>



Figure 12: The process of completing the task "Mine 1 diamond $\odot$ ". Mining diamonds $\odot$ is a highly complex task. Diamonds can only be mined with an iron pickaxe $\nearrow$ ,so an iron pickaxe $\nearrow$ must be crafted first. Crafting an iron pickaxe $\nearrow$ requires iron ingots $\rightarrow$ ,which are smelted from iron or $\rightarrow$ . Mining iron or requires a stone pickaxe $\nearrow$ . Crafting a stone pickaxe $\nearrow$ requires stone in turn must be mined with a wooden pickaxe $\nearrow$ . Crafting a wooden pickaxe $\nearrow$ requires wooden planks $\nabla$ and sticks $\angle$ . All these crafting processes require a crafting table $\nabla$ ,and smelting requires a furnace . In this case, the agent spawns at a cave, so Optimus-1 must leave the cave to chop logs.
图 12：完成任务“挖到 1 颗钻石 $\odot$ ”的过程。挖钻石 $\odot$ 是一项高度复杂的任务。钻石只能用铁镐 $\nearrow$ 挖掘，因此必须先制作铁镐 $\nearrow$ 。制作铁镐 $\nearrow$ 需要铁锭 $\rightarrow$，铁锭由铁矿或 $\rightarrow$ 冶炼而成。开采铁矿或需要一把石镐 $\nearrow$。制作石镐 $\nearrow$ 又需要采矿得到的石头，而采矿需要木镐 $\nearrow$。制作木镐 $\nearrow$ 需要木板 $\nabla$ 和木棍 $\angle$。所有这些合成过程都需要工作台 $\nabla$，冶炼则需要熔炉。在本例中，智能体出生在洞穴内，因此 Optimus-1 必须先离开洞穴去砍伐原木。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_17_10_cfe28b.jpg"/>



Figure 13: In this example, a zombie is slowly approaching the agent. Agents like MP5 [33] and Voyager [47] uses an iterative planning strategy to generate the plan, which consumes a great deal of time and puts the agent in danger. While Optimus-1 directly generates a plan in one step based on the knowledge graph. Using the current visual information, it makes a plan to "run to a sunny place," allowing the agent to avoid danger then begin to achieve sub-goals.
图 13：在此示例中，一只僵尸正慢慢接近智能体。像 MP5 [33] 和 Voyager [47] 这样的智能体使用迭代规划策略生成计划，消耗大量时间并使智能体陷入危险。而 Optimus-1 则基于知识图谱一步直接生成计划，利用当前视觉信息制定“跑到有阳光的地方”的计划，使智能体避险后开始实现子目标。


## NeurIPS Paper Checklist
## NeurIPS 论文检查表


## 1. Claims
## 1. 声明


Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
问题：摘要和引言中的主要声明是否准确反映了论文的贡献和范围？


## Answer: [Yes]
## 答案：[Yes]


Justification: We clearly state the claims and contributions of this paper in the abstract and introduction.
理由：我们在摘要和引言中清晰地陈述了本文的主张和贡献。


## 2. Limitations
## 2. 限制


Question: Does the paper discuss the limitations of the work performed by the authors?
问题：论文是否讨论了作者所做工作的局限性？


Answer: [Yes]
答案：[Yes]


Justification: We clearly state the limitations of this paper in the Section 6.
理由：我们在第6节中清晰地陈述了本文的局限性。


## 3. Theory Assumptions and Proofs
## 3. 理论假设与证明


Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
问题：对于每个理论结果，论文是否提供了完整的假设集合和完整（且正确）的证明？


Answer:[NA]
答案:[NA]


Justification: In this paper, we do not propose new theories or principles.
理由：在本文中，我们未提出新的理论或原理。


## 4. Experimental Result Reproducibility
## 4. 实验结果可复现性


Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
问题：论文是否充分披露了复现论文主要实验结果所需的所有信息，以致影响论文的主要论点和/或结论（无论是否提供代码和数据）？


## Answer: [Yes]
## 答案：[Yes]


Justification: We describe the proposed methods and experimental setup in detail in the Section 2 and Section 3, and the implementation details in Appendix F.
理由：我们在第2节和第3节详细描述了所提方法和实验设置，并在附录F中给出实现细节。


## 5. Open access to data and code
## 5. 数据和代码的开放获取


Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
问题：论文是否提供了数据和代码的开放获取，并在补充材料中提供了足够的说明以忠实复现主要实验结果？


## Answer: [Yes]
## 答复：[是]


Justification: We release our code and project in: https://cybertronagent.github.io/Optimus- 1.github.io/.
理由：我们在此发布我们的代码和项目：https://cybertronagent.github.io/Optimus- 1.github.io/。


## 6. Experimental Setting/Details
## 6. 实验设置/细节


Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
问题：论文是否说明了理解结果所需的所有训练和测试细节（例如数据划分、超参数、如何选择、优化器类型等）？


## Answer: [Yes]
## 答复：[是]


Justification: Our method requires no parameter training. But we detail the benchmark and environment settings in Section 3 and Appendix E.
理由：我们的方法不需要参数训练。但我们在第3节和附录E中详细说明了基准和环境设置。


## 7. Experiment Statistical Significance
## 7. 实验统计显著性


Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
问题：论文是否适当地报告了误差条并正确定义，或提供了关于实验统计显著性的其他适当信息？


## Answer: [Yes]
## 答复：[是]


Justification: For each evaluation task, we ran it at least 30 times and calculated the average accuracy to minimise random errors and uncertainty factors.
理由：对于每个评估任务，我们至少运行了30次并计算平均准确率，以尽量减少随机误差和不确定因素。


## 8. Experiments Compute Resources
## 8. 实验计算资源


Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
问题：对于每个实验，论文是否提供了足够的计算资源信息（计算节点类型、内存、执行时间）以重现实验？


Answer: [Yes]
答复：[是]


Justification: We report the computer resource in Appendix F.3.
理由：我们在附录F.3中报告了计算资源。


## 9. Code Of Ethics
## 9. 道德守则


Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
问题：论文中的研究是否在各方面均符合 NeurIPS 道德守则 https://neurips.cc/public/EthicsGuidelines？


## Answer: [Yes]
## 回答：[是]


Justification: The research conducted in the paper complies with the NeurIPS Code of Ethics in all respects.
理由：论文中开展的研究在各方面均符合 NeurIPS 伦理准则。


## 10. Broader Impacts
## 10. 广泛影响


Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
问题：论文是否讨论了该工作可能带来的正面社会影响和负面社会影响？


## Answer: [Yes]
## 回答：[是]


Justification: We state the Broader Impacts of the paper in the Appendix A.
理由：我们在附录 A 中陈述了论文的广泛影响。


## 11. Safeguards
## 11. 保障措施


Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
问题：论文是否描述了为负责任地发布具有高误用风险的数据或模型（如预训练语言模型、图像生成器或抓取的数据集）而采取的保障措施？


## Answer: [Yes]
## 回答：[是]


Justification: We describe the safeguards in place to responsibly release models with a high risk in Appendix A.
理由：我们在附录 A 中描述了负责任发布高风险模型所采取的保障措施。


## 12. Licenses for existing assets
## 12. 现有资源的许可证


Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
问题：论文中使用的资源（例如代码、数据、模型）的创建者或原始所有者是否得到适当致谢，且其许可证和使用条款是否被明确提及并得到遵守？


## Answer: [Yes]
## 回答：[是]


Justification: We state the models used in Section 2 and comply with all licences.
理由：我们在第2节中说明了所用模型并遵守所有许可证。


### 13.New Assets
### 13. 新资源


Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
问题：论文中引入的新资源是否有充分文档化，并且文档是否随资源一起提供？


Answer: [NA]
Answer: [NA]


Justification: The paper does not release new assets.
Justification: 论文未发布新的资源。


## 14. Crowdsourcing and Research with Human Subjects
## 14. Crowdsourcing and Research with Human Subjects


Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?
Question: 对于众包实验和涉及人的研究，论文是否包含提供给参与者的完整说明文字和截图（如适用），以及关于补偿（如有）的详细信息？


Answer: [NA]
Answer: [NA]


Justification: We hired volunteers to play Minecraft and presented the details in the Appendix E.2.
Justification: 我们招募了志愿者玩 Minecraft，详情见附录 E.2。


## 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects
## 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects


Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
Question: 论文是否描述了研究参与者可能遭受的风险、是否已向受试者披露这些风险，以及是否已获得机构审查委员会（IRB）批准或基于你所在国家或机构要求的等效审批/审查？


Answer: [NA]
Answer: [NA]


Justification: We hired volunteers to play Minecraft as human-level baseline. We only record the games results, and volunteers are not exposed to any risks.
Justification: 我们招募了志愿者玩 Minecraft 作为人类基线。我们仅记录游戏结果，志愿者未暴露于任何风险。