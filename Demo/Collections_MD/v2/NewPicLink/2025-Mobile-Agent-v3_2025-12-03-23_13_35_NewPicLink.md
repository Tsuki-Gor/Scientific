# Mobile-Agent-v3: Fundamental Agents for GUI Automa- TION
# Mobile-Agent-v3：用于 GUI 自动化的基础代理


Jiabo Ye* Xi Zhang* Haiyang Xu*i Haowei Liu Junyang Wang Zhaoqing Zhu
叶家博* 张曦* 许海阳*i 刘皓炜 王俊阳 朱昭庆


Ziwei Zheng Feiyu Gao Junjie Cao Zhengxi Lu
郑梓玮 高飞宇 曹俊杰 卢正熙


Jitong Liao Qi Zheng Fei Huang Jingren Zhou Ming Yan'
廖济通 郑琦 黄飞 周敬人 颜明'


Tongyi Lab , Alibaba Group
通义实验室，阿里巴巴集团


\{shuofeng.xhy, ym119608\}@alibaba-inc.com
\{shuofeng.xhy, ym119608\}@alibaba-inc.com


https://github.com/X-PLUG/MobileAgent



## Abstract
## 摘要


This paper introduces GUI-Owl, a foundational GUI agent model that achieves new state-of-the-art performance among open-source end-to-end models across ten GUI benchmarks spanning both desktop and mobile environments, covering grounding, question answering, planning, decision-making, and general procedural knowledge in GUI automation scenarios. Notably, GUI-Owl-7B achieves a score of 66.4 on the AndroidWorld benchmark and 29.4 on the OSWorld-Verified benchmark. Building on this model, we propose a general-purpose GUI agent framework, Mobile-Agent-v3, which further enhances GUI-Owl's performance (73.3 on An-droidWorld and 37.7 on OSWorld-Verified), achieving a new state-of-the-art among GUI agent frameworks based on open-source models. GUI-Owl incorporates several key innovations: 1) Large-scale Environment Infrastructure: We introduce a cloud-based virtual environment infrastructure spanning different operating systems (including Android, Ubuntu, macOS, and Windows). This underpins our Self-Evolving GUI Trajectory Production framework, which generates high-quality interaction data through sophisticated query generation and correctness judgment. The framework leverages GUI-Owl's capabilities to continuously refine trajectories, creating a self-reinforcing improvement cycle. It supports multiple downstream data pipelines, enabling robust data collection while reducing manual annotation needs. 2) Diverse Foundational Agents Capability Construction: by incorporating foundational UI data-such as grounding, planning, and action semantic recognition-alongside diverse reasoning and reflecting patterns, GUI-Owl not only supports end-to-end decision making but can also serve as a specialized module integrated into multi-agent frameworks; 3) Scalable Environment RL: we also develop a scalable reinforcement learning framework that enables fully asynchronous training and better aligns the model's decision with real-world usage. In addition, we introduce Trajectory-aware Relative Policy Optimization (TRPO) for online environment RL, which achieves 34.9 on the OSWorld-Verified benchmark. GUI-Owl and Mobile-Agent-v3 are open-sourced at: https://github.com/X-PLUG/MobileAgent.
本文介绍了 GUI-Owl，一种基础 GUI 代理模型，在涵盖桌面与移动环境、包含落地、问答、规划、决策与通用程序化知识的十个 GUI 基准上，在开源端到端模型中取得了新的最先进表现。值得注意的是，GUI-Owl-7B 在 AndroidWorld 基准上得分 66.4，在 OSWorld-Verified 基准上得分 29.4。在此模型基础上，我们提出了通用 GUI 代理框架 Mobile-Agent-v3，进一步提升了 GUI-Owl 的性能（AndroidWorld 得 73.3，OSWorld-Verified 得 37.7），在基于开源模型的 GUI 代理框架中达成新的最先进水平。GUI-Owl 包含若干关键创新：1）大规模环境基础设施：我们引入了跨不同操作系统（包括 Android、Ubuntu、macOS 和 Windows）的基于云的虚拟环境基础设施。该基础设施支撑我们的自我进化 GUI 轨迹生成框架，该框架通过复杂的查询生成与正确性判定产生高质量的交互数据。该框架利用 GUI-Owl 的能力持续优化轨迹，形成自我强化的改进循环，支持多条下游数据流水线，实现稳健的数据采集并减少人工标注需求。2）多样化的基础代理能力构建：通过引入基础 UI 数据——如落地、规划与动作语义识别——以及多样的推理与反思模式，GUI-Owl 不仅支持端到端决策，还可以作为专用模块集成进多智能体框架；3）可扩展的环境强化学习：我们还开发了一个可扩展的强化学习框架，使得训练完全异步化并更好地将模型决策与真实世界使用对齐。此外，我们提出了用于在线环境 RL 的轨迹感知相对策略优化（TRPO），在 OSWorld-Verified 基准上取得 34.9 的成绩。GUI-Owl 与 Mobile-Agent-v3 已开源于： https://github.com/X-PLUG/MobileAgent。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_18_24_a35ffd.jpg"/>



Figure 1: Performance overview on mainstream GUI-automation benchmarks.
图 1：主流 GUI 自动化基准的性能概览。


---



*Equal contribution
*同等贡献


${}^{ \dagger  }$ Corresponding author and project leader
${}^{ \dagger  }$ 通信作者与项目负责人


---



<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_18_24_47c5bf.jpg"/>



Figure 2: Overview of our Mobile-Agent-v3. We illustrate our multi-platform environment supporting, our core capability, and some GUI automation examples generated by Mobile-Agent-v3.
图 2：我们的 Mobile-Agent-v3 概览。我们展示了多平台环境支持、核心能力及 Mobile-Agent-v3 生成的一些 GUI 自动化示例。


## 1 INTRODUCTION
## 1 引言


Graphical User Interfaces (GUIs) Agents (Hu et al., 2024; Zhang et al., 2024a; Nguyen et al., 2024; Wang et al., 2024d; Gao et al., 2024; Wang et al., 2024a) is designed to automate daily and professional tasks based on human instructions across various device environments, thereby enhancing production efficiency. With the rapid advancement of multimodal large models and reasoning technologies, vision-based GUI agents have demonstrated strong task execution capabilities across various device environments, including PCs, mobile devices, and web platforms.
图形用户界面（GUI）代理（Hu 等，2024；Zhang 等，2024a；Nguyen 等，2024；Wang 等，2024d；Gao 等，2024；Wang 等，2024a）旨在根据人类指令在各种设备环境中自动化日常和专业任务，从而提升生产效率。随着多模态大模型与推理技术的快速发展，基于视觉的 GUI 代理已在包括 PC、移动设备与网页平台的多种设备环境中展现出强大的任务执行能力。


The existing methods can be broadly divided into two categories. The first category builds agent frameworks based on closed-source models (Yang et al., 2025; Xie et al., 2025; Agashe et al., 2025; Song et al., 2025; Wang et al., 2024b), however, these approaches struggle to handle unfamiliar tasks and adapt to dynamic environments. The second category focuses mainly on end-to-end model performance (Qin et al., 2025; Wang et al., 2025a), but such methods often fail to follow instructions faithfully and lack compatibility with diverse agent frameworks, significantly limiting their practical utility. The GUI agents require this foundational model to have the following capabilities: 1) The strong UI perception capabilities (such as for Mobile, PC, and Web); 2) The planning, reflection, and reasoning in various dynamic environments; 3) The flexibility to integrate with various multi-agent frameworks.
现有方法大致可分为两类。第一类基于闭源模型构建代理框架（Yang 等，2025；Xie 等，2025；Agashe 等，2025；Song 等，2025；Wang 等，2024b），但这些方法在处理不熟悉任务和适应动态环境方面存在困难。第二类主要关注端到端模型性能（Qin 等，2025；Wang 等，2025a），但此类方法常常无法忠实地遵循指令，且与多样的代理框架兼容性差，显著限制了其实用性。GUI 代理需要该基础模型具备以下能力：1）强大的 UI 感知能力（如适用于移动、PC 与网页）；2）在各种动态环境中的规划、反思与推理能力；3）与各类多智能体框架集成的灵活性。


In this paper, we propose GUI-Owl, a native end-to-end multimodal agent designed as a foundational model for GUI automation. Built upon Qwen2.5-VL and extensively post-trained on large-scale, diverse GUI interaction data, GUI-Owl unifies perception, grounding, reasoning, planning, and action execution within a single policy network. The model achieves robust cross-platform interaction, handling multi-turn decision making with explicit intermediate reasoning, and supports both autonomous operation and role-specific deployment in multi-agent systems. To further enhance its adaptability, we develop specialized datasets for core foundational tasks—including UI grounding, task planning, and action semantics-and employ a scalable reinforcement learning framework to align GUI-Owl's decision policy with real-world task success. Beyond single-agent deployment, GUI-Owl can be instantiated as different specialized agents within a multi-agent framework Mobile-Agent-v3 where multiple role agents coordinate and share partial observations and reasoning traces to tackle complex, long-horizon automation workflows.
在本文中，我们提出了 GUI-Owl，这是一个原生端到端多模态智能体，被设计为用于 GUI 自动化的基础模型。GUI-Owl 基于 Qwen2.5-VL 构建，并在大规模、多样化的 GUI 交互数据上进行了广泛的后训练，它将感知、定位、推理、规划和动作执行统一在一个策略网络中。该模型实现了强大的跨平台交互，通过显式的中间推理处理多轮决策，并且支持在多智能体系统中自主运行和特定角色部署。为了进一步增强其适应性，我们为核心基础任务开发了专门的数据集，包括 UI 定位、任务规划和动作语义，并采用可扩展的强化学习框架使 GUI-Owl 的决策策略与现实世界的任务成功率相匹配。除了单智能体部署外，GUI-Owl 还可以在多智能体框架 Mobile-Agent-v3 中实例化为不同的专业智能体，其中多个角色智能体进行协调并共享部分观察结果和推理轨迹，以处理复杂、长期的自动化工作流程。


Large-scale Environment Infrastructure. To train our GUI agent, we developed a comprehensive large-scale environment infrastructure for GUI interaction data collection. This infrastructure leverages cloud-based technologies, including cloud phones and cloud computers on Alibaba Cloud (Cloud, 2018), spanning mobile, PC, and web platforms. It creates dynamic virtual environments that enable diverse and realistic interaction scenarios across various operating systems and devices. Central to this infrastructure is our Self-Evolving GUI Trajectory Production pipeline. This innovative system collects high-quality trajectory data through a sophisticated process involving: high-quality query generation that mimics real-world user interactions, model roll-outs where GUI-Owl and Mobile-Agent-v3 interacts with virtual environments, rigorous correctness judgment to ensure data quality, and query-specific guidance generation for challenging scenarios. This self-evolving pipeline creates a continuous improvement cycle, enhancing both our dataset quality and the GUI-Owl model's capabilities over time. By combining cloud technology with multi-platform environments, our infrastructure enables efficient, scalable model development while reducing manual annotation needs.
大规模环境基础设施。为了训练我们的 GUI 智能体，我们开发了一个全面的大规模环境基础设施，用于收集 GUI 交互数据。该基础设施利用基于云的技术，包括阿里云上的云手机和云电脑（Cloud, 2018），涵盖移动、PC 和 Web 平台。它创建了动态虚拟环境，能够在各种操作系统和设备上实现多样化和真实的交互场景。这个基础设施的核心是我们的自我进化 GUI 轨迹生成管道。这个创新系统通过一个复杂的过程收集高质量的轨迹数据，包括：模拟现实世界用户交互的高质量查询生成、GUI-Owl 和 Mobile-Agent-v3 与虚拟环境进行交互的模型推演、确保数据质量的严格正确性判断，以及为具有挑战性的场景生成特定查询的指导。这个自我进化的管道形成了一个持续改进的循环，随着时间的推移提高我们的数据集质量和 GUI-Owl 模型的能力。通过将云技术与多平台环境相结合，我们的基础设施能够实现高效、可扩展的模型开发，同时减少手动标注的需求。


Diverse Foundational Agents Capability Construction. Based on the generated trajectories, we introduce multiple downstream data construction pipelines to enhance the agent's fundamental UI capabilities, including: (i) a grounding pipeline that covers both UI element localization-based on functional, appearance, and layout instructions- and fine-grained word/character grounding; (ii) a task planning pipeline that distills procedural knowledge from historical successful trajectories and large-scale pretrained LLMs to handle long-horizon, multi-application tasks; and (iii) an action semantics pipeline that captures the relationship between actions and resulting state transitions through before/after UI observations. Furthermore, we synthesize reasoning and reflecting data using offline hint-guided rejection sampling, distillation from a multi-agent framework, and iterative online rejection sampling. This supervision enables the agent to perform independent reasoning and to engage in complex, long-horizon collaborative reasoning within the Mobile-Agent-v3 framework, adapting its reasoning style to the specific role it assumes.
多样化基础智能体能力构建。基于生成的轨迹，我们引入了多个下游数据构建管道，以增强智能体的基本 UI 能力，包括：（i）一个定位管道，涵盖基于功能、外观和布局指令的 UI 元素定位以及细粒度的单词/字符定位；（ii）一个任务规划管道，从历史成功轨迹和大规模预训练大语言模型中提取程序知识，以处理长期、多应用任务；（iii）一个动作语义管道，通过 UI 前后观察捕捉动作与状态转换之间的关系。此外，我们使用离线提示引导拒绝采样、从多智能体框架中蒸馏和迭代在线拒绝采样来合成推理和反思数据。这种监督使智能体能够进行独立推理，并在 Mobile-Agent-v3 框架内进行复杂、长期的协作推理，根据其承担的特定角色调整推理风格。


Scalable Environment RL. We also develop a scalable training framework grounded in a unified interface for multi-task training that standardizes interactions for both single-turn reasoning and multi-turn agentic tasks, and we decouple experience generation from policy updates to provide fine-grained control over policy adherence. This design supports fully asynchronous training and better aligns the model's decision-making with real-world usage. We further introduce Trajectory-aware Relative Policy Optimization (TRPO) to address training with long, variable-length action sequences in online-environment RL. TRPO uses trajectory-level rewards to compute step-level advantages and employs a replay buffer to improve the stability of reinforcement learning.
可扩展环境强化学习。我们还开发了一个基于统一接口的可扩展训练框架，用于多任务训练，该接口对单轮推理和多轮智能体任务的交互进行了标准化，并且我们将经验生成与策略更新解耦，以对策略遵循进行细粒度控制。这种设计支持完全异步训练，并使模型的决策与现实世界的使用更好地对齐。我们进一步引入了轨迹感知相对策略优化（TRPO），以解决在线环境强化学习中长的、可变长度动作序列的训练问题。TRPO 使用轨迹级奖励来计算步骤级优势，并使用回放缓冲区来提高强化学习的稳定性。


We evaluate GUI-Owl across a wide range of benchmarks that comprehensively measure native agent capability on GUI automation including grounding, single-step decision, general question answering and evaluate on online environment. GUI-Owl-7B outperforms all state-of-the-art models of comparable size. In particular, GUI-Owl-7B achieves scores of 34.9 on OSWorld-Verified and 66.4 on AndroidWorld. Moreover, GUI-Owl-32B demonstrates outstanding performance, surpassing even proprietary models. On MMBench-GUI and AndroidControl, GUI-Owl- 32B outperforms all models, including GPT-40 and Claude 3.7. In grounding capability evaluations, GUI-Owl-32B surpasses all models of the same size and achieves competitive performance compared with proprietary models. When combined with Mobile-Agent-v3, it achieves scores of 37.7 on OSWorld and 73.3 on AndroidWorld, which clearly demonstrates its capability as a fundamental agent for GUI automation.
我们在广泛的基准测试中评估了 GUI-Owl，这些基准测试全面衡量了原生智能体在 GUI 自动化方面的能力，包括定位、单步决策、通用问答，并在在线环境中进行评估。GUI-Owl-7B 优于所有同等规模的现有最先进模型。特别是，GUI-Owl-7B 在 OSWorld-Verified 上的得分达到 34.9，在 AndroidWorld 上达到 66.4。此外，GUI-Owl-32B 表现出色，甚至超过了专有模型。在 MMBench-GUI 和 AndroidControl 上，GUI-Owl-32B 优于所有模型，包括 GPT-40 和 Claude 3.7。在定位能力评估中，GUI-Owl-32B 超过了所有同等规模的模型，并且与专有模型相比也取得了有竞争力的表现。当与 Mobile-Agent-v3 结合使用时，它在 OSWorld 上的得分达到 37.7，在 AndroidWorld 上达到 73.3，这清楚地证明了它作为 GUI 自动化基础智能体的能力。


## 2 GUI-OwL
## 2 GUI-OwL


GUI-Owl is an end-to-end multimodal model that unifies capabilities such as perception, planning, decision-making, and grounding within GUI scenarios. By leveraging extensive and diverse datasets for post-training based on Qwen2.5-VL, GUI-Owl is able to interact with graphical user interfaces on Mobile, PC, and Web platforms. We further apply reinforcement learning to GUI-Owl to align its capabilities with diverse downstream requirements. This alignment enables the model not only to autonomously perform multi-turn GUI interaction tasks, but also to generalize to specific applications such as question answering, captioning, planning, and grounding. Moreover, GUI-Owl can assume various roles within a multi-agent framework, in which individual agents fulfill their respective responsibilities, coordinate their actions, and collaboratively accomplish more complex tasks.
GUI-Owl 是一个端到端多模态模型，在 GUI 场景中统一了感知、规划、决策与落地等能力。通过在 Qwen2.5-VL 基础上利用大规模多样化数据进行后训练，GUI-Owl 能在移动端、PC 和 Web 平台与图形界面进行交互。我们进一步对 GUI-Owl 应用强化学习，使其能力与多样的下游需求对齐。该对齐使模型不仅能自主完成多回合 GUI 交互任务，还能推广到问答、图注、规划与落地等具体应用。此外，GUI-Owl 可在多智能体框架中担任多种角色，各智能体履行各自职责、协调行动，协同完成更复杂任务。


### 2.1 END-TO-END GUI INTERACTIONS
### 2.1 端到端 GUI 交互


We model the interaction process between GUI-Owl and the device, as well as the completion of the specified task,as a multi-turn decision-making process. Given the available action space $A = \left\{  {{a}^{1},{a}^{2},\ldots ,{a}^{\left| A\right| }}\right\}$ of the environment,the current environment observation ${S}_{t} \in  \mathcal{S}$ ,which can be a screenshot in common,and the history
我们将 GUI-Owl 与设备之间的交互过程及指定任务的完成建模为多回合决策过程。给定环境的可用动作空间 $A = \left\{  {{a}^{1},{a}^{2},\ldots ,{a}^{\left| A\right| }}\right\}$、当前环境观测 ${S}_{t} \in  \mathcal{S}$（常见为截图）以及历史


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_18_24_303830.jpg"/>



Figure 3: Illustration of the interaction flow of GUI-Owl. The system message defines the available action space, the user message contains the task instruction, compressed histories, and current observation, while the response message includes the agent's reasoning, action summaries, and the final action output.
图 3：GUI-Owl 交互流程示意。系统消息定义可用动作空间，用户消息包含任务指令、压缩历史与当前观测，响应消息包括代理的推理、动作摘要与最终动作输出。


of past operations ${H}_{t} = \left\{  {\left( {{S}_{1},{a}_{1}}\right) ,\left( {{S}_{2},{a}_{2}}\right) ,\ldots ,\left( {{S}_{t - 1},{a}_{t - 1}}\right) }\right\}$ ,the model selects an action from the action space $A$ and executes it in the environment to obtain the next time-step’s observation ${S}_{t + 1}$ .
的过去操作 ${H}_{t} = \left\{  {\left( {{S}_{1},{a}_{1}}\right) ,\left( {{S}_{2},{a}_{2}}\right) ,\ldots ,\left( {{S}_{t - 1},{a}_{t - 1}}\right) }\right\}$，模型从动作空间 $A$ 中选择一个动作并在环境中执行，以获得下一时间步的观测 ${S}_{t + 1}$。


Formally,at each time step $t,{a}_{t} \sim  \pi \left( {\cdot  \mid  {S}_{t},{H}_{t}}\right)$ ,here, $\pi$ denotes policy model (GUI-Owl),which maps the current observation and historical operations to a probability distribution over the action space $A$ .
形式化地，在每一时间步 $t,{a}_{t} \sim  \pi \left( {\cdot  \mid  {S}_{t},{H}_{t}}\right)$，此处 $\pi$ 表示策略模型（GUI-Owl），它将当前观测与历史操作映射为动作空间 $A$ 上的概率分布。


We present the interaction flow of GUI-Owl in Figure 3. In practice, we support flexible prompts to organize the action space into system messages. By default, we adopt the Qwen function calling format. Detail action space definition is presented in Table 9 and Table 10. For user messages, we sequentially provide the original task, historical information, and observations. To save GPU memory and improve inference speed, we typically retain only the most recent 1 to 3 images. Notably, requiring a robust reasoning process before the actual output of an action decision can enhance the model's ability to adapt to complex tasks and situations. Therefore, we require the model to first "reasoning" before making a decision, and then execute the action based on this reasoning content. However, since lengthy thoughts over multiple turn interactions may cause the conversation history to become excessively long, we additionally require the model to output a "conclusion" summarizing the key information of the current step. Finally, only the conclusion is stored in the historical context.
我们在图 3 中展示了 GUI-Owl 的交互流程。实践中，我们支持灵活的提示将动作空间组织为系统消息。默认采用 Qwen 函数调用格式。动作空间的具体定义见表 9 和表 10。对于用户消息，我们按顺序提供原始任务、历史信息与观测。为节省 GPU 内存并提升推理速度，通常仅保留最近 1 到 3 张图像。值得注意的是，在实际输出动作决策前要求稳健的推理过程，可增强模型适应复杂任务与情境的能力。因此，我们要求模型先进行“推理”再决策，并基于该推理内容执行动作。然而，由于多回合交互中的冗长思考可能导致对话历史过长，我们额外要求模型输出一个总结当前步骤关键信息的“结论”。最终，仅将结论存入历史上下文。


The actions output by the model are translated into actual device operation commands (for example, we use ADB commands for Android devices, and pyautogui code for desktop operations). Meanwhile, the latest screenshot of the device's display is further captured and used as the observation of the environment.
模型输出的动作被转换为实际设备操作命令（例如，对于 Android 设备我们使用 ADB 命令，桌面操作使用 pyautogui 代码）。同时，进一步截取设备显示的最新屏幕截图并作为环境的观测。


### 2.2 FOUNDATIONAL AGENTS CAPABILITY
### 2.2 基础代理能力


GUI-Owl can function not only as a native agent capable of independently interacting with GUIs, but also provides a variety of foundational capabilities to support downstream standalone calls or integration into a multi-agent framework. To this end, we collect and construct datasets for various capabilities such as grounding, caption and planning. These datasets are mixed with general instruction data during training, and we found that the model also possesses zero-shot GUI question-answering capability as well as general instruction-following abilities for unseen tasks.
GUI-Owl 不仅能作为能够独立与 GUI 交互的本地代理运行，还提供多种基础能力以支持下游独立调用或集成到多智能体框架。为此，我们收集并构建了用于落地、图注与规划等多种能力的数据集。这些数据集在训练时与通用指令数据混合，我们发现模型也具备零样本的 GUI 问答能力以及对未见任务的一般指令遵循能力。


#### 2.2.1 Self-Evolving Trajectory Production Framework
#### 2.2.1 自我进化轨迹生成框架


To scale up the trajectory data, we propose a Self-Evolving GUI Trajectory Production pipeline, which contrasts with traditional methods that strongly rely on manual annotation (Wang et al., 2025a; Qin et al., 2025). This framework leverages the capabilities of GUI-Owl itself, continuously generating new trajectories through roll-out and assessing their correctness to obtain large-scale high-quality interaction data. Subsequently, these data are utilized to enhance the model's capabilities, creating a reinforcing cycle of improvement. As shown in Figure 4, the process begins with constructing dynamic virtual environments across mobile, PC, and web platforms, paired with high-quality query generation. Given these queries, the GUI-Owl model and Mobile-Agent-v3 framework performs step-by-step actions in the environments to produce roll-out trajectories. A Trajectory Correctness Judgment Module then evaluates these trajectories at both the step and trajectory levels to identify and filter out errors. For difficult queries, a Query-specific Guidance Generation module provides human- or model-generated ground-truth trajectories to guide the agent. The cleaned and enriched data is then used for reinforcement fine-tuning, enabling the model to iteratively improve its ability to generate successful GUI trajectories, thereby reducing human annotation needs and achieving continuous self-improvement. More details can be found in Section 5.4.
为扩大轨迹数据规模，我们提出了一个自进化的 GUI 轨迹生成流水线，这与严重依赖人工标注的传统方法形成对比（Wang 等人，2025a；Qin 等人，2025）。该框架利用 GUI - Owl 自身的能力，通过滚动生成不断产生新的轨迹，并评估其正确性以获得大规模高质量的交互数据。随后，利用这些数据增强模型的能力，形成一个不断改进的强化循环。如图 4 所示，该过程始于在移动、PC 和网络平台上构建动态虚拟环境，并进行高质量的查询生成。给定这些查询，GUI - Owl 模型和 Mobile - Agent - v3 框架在环境中逐步执行操作以生成滚动轨迹。然后，轨迹正确性判断模块在步骤和轨迹层面评估这些轨迹，以识别并过滤错误。对于困难的查询，特定查询指导生成模块提供人工或模型生成的真实轨迹来引导代理。经过清理和丰富的数据随后用于强化微调，使模型能够迭代提高其生成成功 GUI 轨迹的能力，从而减少人工标注需求并实现持续的自我改进。更多细节可在 5.4 节中找到。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_18_24_7f86f4.jpg"/>



Figure 4: Illustration of the our self-evolving trajectory data production pipeline.
图 4：我们的自进化轨迹数据生成流水线示意图。


- High-Quality Query Generation. For mobile apps, we developed a screenshot-action framework utilizing a human-annotated Directed Acyclic Graph (DAG) (Patil et al., 2025) that models realistic navigation flows and captures multi-constraint user queries. The process involves path sampling, metadata extraction, instruction synthesis using LLMs, refinement through few-shot LLM prompting, and interface validation via web crawlers. This framework minimizes LLM hallucinations while ensuring realistic and controllable query generation. For computer applications, we addressed the challenges of atomic operational skills and software operational pathways. We combined manual annotation and LLM-assisted generation to create queries for atomic operations (e.g., clicking, scrolling, dragging) and complex software interactions. We utilized accessibility trees and deep-search chains to acquire operational pathways, and employed MLLMs to generate executable commands based on screenshots and exemplar inputs. This comprehensive approach ensures diverse, realistic, and accurate query generation across different GUI environments.
- 高质量查询生成。对于移动应用程序，我们开发了一个截图 - 动作框架，利用人工标注的有向无环图（DAG）（Patil 等人，2025）来建模现实的导航流程并捕获多约束用户查询。该过程包括路径采样、元数据提取、使用大语言模型进行指令合成、通过少样本大语言模型提示进行细化以及通过网络爬虫进行界面验证。该框架在确保生成现实且可控查询的同时，最大限度地减少大语言模型的幻觉。对于计算机应用程序，我们解决了原子操作技能和软件操作路径的挑战。我们结合人工标注和大语言模型辅助生成，为原子操作（如点击、滚动、拖动）和复杂软件交互创建查询。我们利用可访问性树和深度搜索链来获取操作路径，并使用多模态大语言模型根据截图和示例输入生成可执行命令。这种全面的方法确保了在不同 GUI 环境中生成多样化、现实且准确的查询。


- Trajectory Correctness Judgment Module. Our Trajectory Correctness Judgment Module employs a two-tiered system to evaluate the quality of generated GUI trajectories. It consists of a Step-Level Critic and a Trajectory-Level Critic. The Step-Level Critic analyzes individual actions within a trajectory by examining pre-action and post-action states, producing an analysis, summary, and categorical annotation (GOOD, NEUTRAL, HARMFUL) for each step. The Trajectory-Level Critic assesses the overall trajectory using a dual-channel approach: a Textual Reasoning Channel leveraging large language models, and a Multi-Modal Reasoning Channel incorporating both visual and textual data. The final correctness judgment is determined through a consensus mechanism. This comprehensive approach ensures robust evaluation of GUI trajectories, combining granular step-level insights with holistic trajectory-level assessment to maintain high-quality training data for our GUI-Owl model.
- 轨迹正确性判断模块。我们的轨迹正确性判断模块采用两层系统来评估生成的 GUI 轨迹的质量。它由步骤级评判器和轨迹级评判器组成。步骤级评判器通过检查动作前后的状态来分析轨迹中的单个动作，为每个步骤生成分析、总结和分类标注（良好、中性、有害）。轨迹级评判器使用双通道方法评估整个轨迹：一个是利用大语言模型的文本推理通道，另一个是结合视觉和文本数据的多模态推理通道。最终的正确性判断通过共识机制确定。这种全面的方法确保了对 GUI 轨迹的稳健评估，将细致的步骤级洞察与整体的轨迹级评估相结合，为我们的 GUI - Owl 模型维护高质量的训练数据。


- Query-specific Guidance Generation. This module utilizes successful trajectories to create guidance for improved model performance. The process involves: (1) Action Description: A VLM generates descriptions for each action's outcome in reference trajectories. Inputs include pre- and post-action screenshots and action decisions. For coordinate-based actions, we highlight interaction points to aid VLM analysis. (2) Quality Control: For model-generated trajectories, the VLM cross-references the model's decision rationale to validate step effectiveness, filtering out suboptimal actions. (3) Guidance Synthesis: Action descriptions are concatenated and fed into a Large Language Model (LLM), which summarizes the essential steps required to complete the query, producing query-specific guidance. This approach enables the generation of targeted guidance, potentially improving the model's ability to handle complex queries and reducing the need for extensive rollouts or manual annotations.
- 特定查询指导生成。该模块利用成功的轨迹为提高模型性能创建指导。该过程包括：（1）动作描述：视觉语言模型为参考轨迹中每个动作的结果生成描述。输入包括动作前后的截图和动作决策。对于基于坐标的动作，我们突出交互点以帮助视觉语言模型分析。（2）质量控制：对于模型生成的轨迹，视觉语言模型交叉引用模型的决策理由以验证步骤的有效性，过滤掉次优动作。（3）指导合成：将动作描述连接起来并输入到大语言模型（LLM）中，大语言模型总结完成查询所需的基本步骤，生成特定查询的指导。这种方法能够生成有针对性的指导，有可能提高模型处理复杂查询的能力并减少对大量滚动生成或人工标注的需求。


#### 2.2.2 Diverse GUI Data Synthesis
#### 2.2.2 多样化 GUI 数据合成


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_18_24_e8d92a.jpg"/>



Figure 5: Overview of our grounding data construction pipeline.
图 5：我们的基础数据构建流水线概述。


Grounding. Accurate localization and semantic understanding of graphical user interface elements are essential for the development of robust and reliable visual interface agents, with these capabilities primarily embodied in grounding. As illustrated in Figure 5, to improve grounding capabilities, we construct two types of grounding task datasets from multiple data sources.
基础定位。准确的图形用户界面元素定位和语义理解对于开发强大可靠的视觉界面代理至关重要，这些能力主要体现在基础定位中。如图 5 所示，为了提高基础定位能力，我们从多个数据源构建了两种类型的基础定位任务数据集。


- For UI element grounding (with function-based or appearance- & layout-based instruction), we collect data from three sources: 1) Open-source datasets: Publicly released GUI datasets are utilized from UI-Vision (Nayak et al., 2025), and GUI-R1 (Luo et al., 2025). 2) Grounding data synthesis using A11y tree: Extracting bounding boxes and functional descriptive information of UI elements through the accessibility (ally) tree in both mobile and computer environments. And the appearance and layout descriptions are additionally generated using MLLMs such as Qwen2.5VL (Bai et al., 2025). 3) Dense grounding generation on crawled PC images: To tackle the scarcity of PC grounding data, diverse screenshots are crawled from Google Images using popular app names as keywords. Given the high density and visual complexity of UI components in PC screenshots, we employ SAM (Kirillov et al., 2023) to segment images into subregions, enabling more precise grounding. MLLMs then perform dense grounding within each segmented region.
- 对于 UI 元素定位（使用基于功能或基于外观与布局的指令），我们从三个来源收集数据：1) 开源数据集：利用了 UI-Vision（Nayak 等人，2025 年）和 GUI-R1（Luo 等人，2025 年）公开发布的 GUI 数据集。2) 使用 A11y 树合成定位数据：通过移动和计算机环境中的无障碍（ally）树提取 UI 元素的边界框和功能描述信息。此外，还使用 Qwen2.5VL（Bai 等人，2025 年）等多模态大语言模型生成外观和布局描述。3) 在爬取的 PC 图像上进行密集定位生成：为解决 PC 定位数据稀缺的问题，使用流行应用名称作为关键词从谷歌图片爬取各种截图。鉴于 PC 截图中 UI 组件的高密度和视觉复杂性，我们使用 SAM（Kirillov 等人，2023 年）将图像分割成子区域，以实现更精确的定位。然后，多模态大语言模型在每个分割区域内进行密集定位。


To reduce noise and further improve the quality, we clean the collected grounding annotations by comparing them with UI detection results from Omniparser V2 (Yu et al.,2025) (bounding boxes with ${IoU} < {\tau }_{q}$ are removed,where ${\tau }_{g} = {0.5}$ ). Additionally,we rephrase the original instructions into more natural, task-oriented descriptions using LLMs (e.g., Qwen2.5-Max (Team, 2024)).
为减少噪声并进一步提高质量，我们将收集到的定位注释与 Omniparser V2（Yu 等人，2025 年）的 UI 检测结果进行比较，以清理这些注释（移除带有 ${IoU} < {\tau }_{q}$ 的边界框，其中 ${\tau }_{g} = {0.5}$）。此外，我们使用大语言模型（如 Qwen2.5-Max（团队，2024 年））将原始指令重新表述为更自然、面向任务的描述。


- For fine-grained words and characters grounding, we collect a set of document images and employ OCR tools to extract textual content and their corresponding spatial positions. Based on the annotated data, we build fine-grained text localization data to enable precise grounding of specific words and characters.
- 对于细粒度的单词和字符定位，我们收集了一组文档图像，并使用 OCR 工具提取文本内容及其对应的空间位置。基于标注数据，我们构建细粒度的文本定位数据，以实现对特定单词和字符的精确定位。


Task Planning. As foundational agents are often used to accomplish long-horizon, complex tasks, the model needs to possess background knowledge of complex task planning. We construct such data from two perspectives:
任务规划。由于基础智能体通常用于完成长期、复杂的任务，模型需要具备复杂任务规划的背景知识。我们从两个角度构建此类数据：


- 1) Distilling from Historical Trajectories. Given historical successful trajectory data, we first construct fine-grained descriptions of each page transition. This information is then combined with the model's historical actions and organized into a task execution manual through an LLM. By feeding this manual into GUI-Owl, we evaluate its quality based on changes in the task completion rate.
- 1) 从历史轨迹中提炼。给定历史成功轨迹数据，我们首先构建每个页面转换的细粒度描述。然后，将此信息与模型的历史动作相结合，并通过大语言模型整理成任务执行手册。将该手册输入 GUI-Owl，我们根据任务完成率的变化评估其质量。


- 2) Distilling from Large-scale Pretrained LLM. To further enhance the model's generalization on various tasks, we further distill knowledge from large-scale pretrained LLMs. First, we collect a list which covering mainstream apps, and then use either human annotators or models to synthesize plausible tasks. These tasks are designed to be as complex as possible and to span multiple features and even multiple applications; task specifications with obvious errors are filtered out. We then feed these tasks to a language model (e.g., Qwen3-235B) and further consolidate and clean the resulting plans, yielding task-specific planning data.
- 2) 从大规模预训练大语言模型中提炼。为进一步提高模型在各种任务上的泛化能力，我们从大规模预训练大语言模型中进一步提炼知识。首先，我们收集一份涵盖主流应用的列表，然后使用人工标注员或模型合成合理的任务。这些任务设计得尽可能复杂，涵盖多个功能甚至多个应用；过滤掉有明显错误的任务规范。然后，我们将这些任务输入语言模型（如 Qwen3 - 235B），并进一步整合和清理生成的计划，得到特定任务的规划数据。


Action Semantics. We notice that a model's ability to perceive how actions affect page-state changes is crucial. Based on collected trajectories, we extensively collect a large corpus of pre- and post-action screenshots and construct a two-tier dataset. At the first tier, we require the model to directly predict the intervening action-including its type and parameters-based on the before-and-after images; such data can be obtained directly from offline-collected trajectories.
动作语义。我们注意到，模型感知动作如何影响页面状态变化的能力至关重要。基于收集的轨迹，我们广泛收集大量动作前后的截图语料库，并构建一个两层数据集。在第一层，我们要求模型根据前后图像直接预测中间动作（包括其类型和参数）；此类数据可直接从离线收集的轨迹中获取。


Subsequently, we ask the model to produce a natural-language description that covers both the executed action and its effects. To construct annotations for this data, we design a workflow that first generates an action description from the pre-action screenshot and the given action parameters (for coordinate-awared actions, the target location is drawn on the image to cue the model), using a multimodal model (e.g., Qwen-VL-Max). We then use the same multimodal model with the before-and-after images to analyze page changes and assess whether the changes are semantically consistent with the action. Through multiple rounds of voting, we retain the higher-scoring action descriptions.
随后，我们要求模型生成一个涵盖已执行动作及其效果的自然语言描述。为构建此数据的注释，我们设计了一个工作流程，首先使用多模态模型（如 Qwen - VL - Max）根据动作前的截图和给定的动作参数（对于需要坐标的动作，在图像上绘制目标位置以提示模型）生成动作描述。然后，我们使用相同的多模态模型结合前后图像分析页面变化，并评估这些变化是否与动作在语义上一致。通过多轮投票，我们保留得分较高的动作描述。


#### 2.2.3 Enhanced Robust Reasoning
#### 2.2.3 增强鲁棒推理


Reasoning ability is essential for a fundamental agent, as it enables the model to move beyond merely imitating action sequences and instead capture the underlying logic that governs them. To this end, we first propose a set of diverse data synthesis strategies to enrich the variety of reasoning patterns. We then integrate reinforcement learning to further align these reasoning patterns with the dynamics of real-world environments.
推理能力对于基础智能体至关重要，因为它使模型能够超越单纯模仿动作序列，而是掌握支配这些序列的底层逻辑。为此，我们首先提出一套多样化的数据合成策略，以丰富推理模式的多样性。然后，我们集成强化学习，使这些推理模式更好地适应现实世界环境的动态变化。


Offline Hint-Guided Rejection Sampling. We synthesize reasoning data via rejection sampling. Specifically, given a collected trajectory
离线提示引导的拒绝采样。我们通过拒绝采样合成推理数据。具体而言，给定一个收集到的轨迹


$$
T = \left\{  {\left( {{a}_{0},{S}_{0}}\right) ,\left( {{a}_{1},{S}_{1}}\right) ,\ldots ,\left( {{a}_{t},{S}_{t}}\right) }\right\}  ,
$$



we prompt VLMs to generate reasoning content for each step based on its preceding history. The generated reasoning is then separated from the original context and used independently for action prediction. We evaluate the validity of each reasoning sequence by checking whether the predicted action matches the ground-truth action.
我们提示视觉语言模型根据每个步骤的先前历史生成推理内容。然后，将生成的推理与原始上下文分离，并独立用于动作预测。我们通过检查预测动作是否与真实动作匹配来评估每个推理序列的有效性。


To encourage diversity in reasoning patterns, we adopt hints of different styles, for instance, requiring the model to follow a fixed chain-of-thought template or encouraging it to produce the simplest possible reasoning process. During this procedure, we observe that, for certain steps, the VLMs struggle to obtain actions consistent with the ground truth. For such cases, we first manually verify the correctness of the ground-truth action. If the action is deemed reasonable, we then feed it back to the VLMs to guide the generation of reasoning conclusions consistent with that action type.
为了鼓励推理模式的多样性，我们采用不同风格的提示，例如要求模型遵循固定的思路链模板或鼓励其给出尽可能简洁的推理过程。在此过程中，我们发现对某些步骤，VLMs 很难得到与事实一致的动作。对于此类情况，我们首先手动验证该事实动作的正确性。如果该动作被认为合理，我们便将其反馈给 VLMs，以引导生成与该动作类型一致的推理结论。


Distillation from Multi-Agent Framework. We note that even when style prompts are provided to encourage end-to-end reasoning generation, the model can still be influenced by certain inherent biases, which in turn limit reasoning diversity. In contrast, a multi-agent framework decomposes a single-step decision into the collaboration of multiple specialized roles, each approaching the current step from a different perspective. Since each agent focuses exclusively on its own subtask, it can more effectively avoid such biases. Motivated by this observation, we collect the outputs of individual agents from the Mobile-Agent-v3, and employ a large language model to integrate their diverse reasoning contents into a unified end-to-end reasoning output. The resulting reasoning content is paired with the action sequences obtained from Mobile-Agent-v3, forming the training dataset for reasoning.
来自多智能体框架的蒸馏。我们注意到即便提供风格提示以鼓励端到端的推理生成，模型仍可能被某些固有偏差影响，进而限制推理多样性。相比之下，多智能体框架将单步决策分解为多个专门角色的协作，每个角色从不同视角处理当前步骤。由于每个 agent 只专注于自身子任务，它能更有效地避免此类偏差。基于此观察，我们收集 Mobile-Agent-v3 中各个 agent 的输出，并使用大语言模型将它们多样的推理内容整合为统一的端到端推理输出。得到的推理内容与从 Mobile-Agent-v3 获得的动作序列配对，构成推理的训练数据集。


Iterative Online Rejection Sampling. We observe that improving the base model's reasoning capability also enhances its ability to accomplish a wider range of tasks. Moreover, newly generated trajectory data can be further exploited for model training. Therefore, we adopt an iterative online rejection sampling framework, in which our model rolls out trajectories on query data under two modes:
迭代在线拒绝采样。我们观察到提升基础模型的推理能力也增强了其完成更广泛任务的能力。此外，新生成的轨迹数据可进一步用于模型训练。因此，我们采用迭代在线拒绝采样框架，其中模型在查询数据上以两种模式展开轨迹：


1. End-to-end generation: the model directly generates reasoning and action predictions in an end-to-end fashion, which is used to improve its holistic reasoning capability.
1. 端到端生成：模型直接以端到端方式生成推理和动作预测，用于提升其整体推理能力。


2. Integration with Mobile-Agent-v3: GUI-Owl is incorporated into the Mobile-Agent-v3 framework. The inputs and outputs are collected to train the corresponding agent role models.
2. 与 Mobile-Agent-v3 集成：将 GUI-Owl 并入 Mobile-Agent-v3 框架，收集输入与输出以训练相应的角色模型。


Formally,given query data $\mathcal{Q}$ and a model ${M}^{\left( k\right) }$ at iteration $k$ ,trajectories are generated as:
形式化地，给定查询数据 $\mathcal{Q}$ 和第 $k$ 次迭代的模型 ${M}^{\left( k\right) }$，按如下方式生成轨迹：


$$
{\mathcal{T}}^{\left( k\right) } = \operatorname{Rollout}\left( {{M}^{\left( k\right) },\mathcal{Q}}\right) , \tag{1}
$$



where ${\mathcal{T}}^{\left( k\right) }$ contains both end-to-end outputs ${\mathcal{T}}_{\mathrm{{E2E}}}^{\left( k\right) }$ and role-specific outputs ${\mathcal{T}}_{\text{ Role }}^{\left( k\right) }$ . The newly collected trajectories are then used to update the model:
其中 ${\mathcal{T}}^{\left( k\right) }$ 同时包含端到端输出 ${\mathcal{T}}_{\mathrm{{E2E}}}^{\left( k\right) }$ 与角色特定输出 ${\mathcal{T}}_{\text{ Role }}^{\left( k\right) }$ 。新收集的轨迹随后用于更新模型：


$$
{M}^{\left( k + 1\right) } = \operatorname{Train}\left( {{M}^{\left( k\right) },{\mathcal{T}}_{\text{ filtered }}^{\left( k\right) }}\right) . \tag{2}
$$



Directly training on all collected steps often yields a suboptimal model. To address this, we apply the following filtering and balancing strategies:
直接在所有收集到的步骤上训练通常会产生次优模型。为此，我们采用以下筛选和平衡策略：


1. Critic-based filtering: A Critic pipeline scores each step ${s}_{t} \in  {\mathcal{T}}^{\left( k\right) }$ ,and those with scores below a threshold ${\tau }_{c}$ are removed:
1. 基于评论者的过滤：一个 Critic 流程对每个步骤 ${s}_{t} \in  {\mathcal{T}}^{\left( k\right) }$ 打分，得分低于阈值 ${\tau }_{c}$ 的被移除：


$$
{\mathcal{T}}_{\text{ filtered }} = \left\{  {{s}_{t} \mid  \text{ CriticScore }\left( {s}_{t}\right)  \geq  {\tau }_{c}}\right\}  .
$$



2. Thought-action consistency check: We verify the logical consistency between the reasoning content (thought) and the executed action. Steps that fail this check are discarded.
2. 思路—动作一致性检查：我们核验推理内容（思路）与执行动作之间的逻辑一致性。不通过该检查的步骤被丢弃。


3. Task re-weighting: Let ${p}_{\text{ succ }}\left( \text{ task }\right)$ denote the success rate of a given task. For tasks with high ${p}_{\text{ succ }}$ , we downsample their training occurrence,while for tasks with low ${p}_{\text{ succ }}$ ,we upsample their instances to ensure balanced learning.
3. 任务重采样权重：令 ${p}_{\text{ succ }}\left( \text{ task }\right)$ 表示某任务的成功率。对于高 ${p}_{\text{ succ }}$ 的任务，我们下采样其训练出现频率，而对于低 ${p}_{\text{ succ }}$ 的任务，我们上采样其实例以确保学习平衡。


4. Reflector balancing: We observe that the Reflector Agent predominantly produces positive outputs, leading to class imbalance. We recalibrate its data as follows:
4. 反思者平衡：我们观察到 Reflector Agent 主要产生正向输出，导致类别不平衡。我们按如下方式重新校准其数据：


- If the Reflector marks step $i$ as negative and this feedback causes step $i + 1$ to be judged positive,the feedback for step $i$ is retained.
- 若 Reflector 将步骤 $i$ 标记为负向且该反馈导致步骤 $i + 1$ 被判断为正向，则保留对步骤 $i$ 的反馈。


- Otherwise, we retain Reflector inputs and responses only from trajectories where all steps are judged positive by the reflactor.
- 否则，我们仅保留那些所有步骤均被 reflector 判定为正向的轨迹中的 Reflector 输入与响应。


Finally, we re-balance the dataset so that positive and negative samples have equal size.
最后，我们对数据集进行再平衡，使正负样本数量相等。


## 3 Training Paradigm
## 3 训练范式


GUI-Owl is initialized from Qwen2.5-VL and trained through a three-stage process designed to progressively enhance its capabilities in GUI understanding, reasoning, and robust execution.
GUI-Owl 从 Qwen2.5-VL 初始化，经过三阶段训练以逐步增强其在 GUI 理解、推理和稳健执行方面的能力。


- Pre-training Phase: We collect a large-scale pre-training corpus covering fundamental UI understanding, interaction trajectory data, and general reasoning data. This data is used to continually pre-train Qwen2.5-VL, strengthening its grounding in basic GUI element recognition, action prediction, and general reasoning, thereby establishing a strong foundation for subsequent interaction-oriented training.
- 预训练阶段：我们收集大规模预训练语料，涵盖基础 UI 理解、交互轨迹数据和通用推理数据。该数据用于持续预训练 Qwen2.5-VL，强化其在基础 GUI 元素识别、动作预测和通用推理方面的能力，从而为后续面向交互的训练奠定坚实基础。


- Iterative Tuning Phase: We deploy the model in real-world environments such as desktops and mobile devices to perform large-scale task execution. The resulting trajectories are cleaned, scored, and further transformed into diverse reasoning datasets, which are then used for offline training. This iterative Tuning process enables GUI-Owl to accumulate effective reasoning patterns applicable across varied scenarios, improving its adaptability and decision-making in complex UI tasks.
- 迭代调优阶段：我们将模型部署在桌面与移动等真实环境中进行大规模任务执行。所得轨迹经过清洗、评分并进一步转化为多样化的推理数据集，随后用于离线训练。本迭代调优过程使 GUI-Owl 积累适用于多场景的有效推理模式，提高其在复杂 UI 任务中的适应性与决策能力。


- Reinforcement Learning Phase: We develop an asynchronous RL framework that allows the model to efficiently learn from direct interaction with real environments. This phase focuses on reinforcing successful behaviors and increasing execution consistency, thereby improving both the success rate and stability of GUI-Owl in practical deployments.
- 强化学习阶段：我们构建了一个异步 RL 框架，使模型能高效地从与真实环境的直接交互中学习。本阶段侧重于强化成功行为并提升执行一致性，从而提高 GUI-Owl 在实际部署中的成功率与稳定性。


### 3.1 Scalable Reinforcement Learning
### 3.1 可扩展强化学习


#### 3.1.1 INFRASTRUCTURE
#### 3.1.1 基础设施


Where enriched trajectory and reasoning data expand the model's knowledge base and reasoning capabilities, the model is expected to exhibit lower uncertainty and higher stability in real-world usage. Therefore, we further introduce reinforcement learning to better align GUI-Owl with practical application.
随着丰富的轨迹与推理数据扩展模型的知识库和推理能力，模型在真实使用中应表现出更低的不确定性和更高的稳定性。因此，我们进一步引入强化学习以更好地将 GUI-Owl 与实际应用对齐。


To facilitate an efficient and flexible training framework for training with environment multi-turn interactions, we develop a general infrastructure with the following key features:
为支持基于环境多回合交互的高效灵活训练框架，我们开发了具有以下关键特性的通用基础设施：


- Unified Interface for multi-task training: Our framework is built on a unified task plug-in interface that standardizes interactions for both single-turn reasoning and complex, multi-turn agentic tasks. This modular design allows diverse new environments and tasks to be seamlessly integrated without altering the core infrastructure.
- 多任务训练的统一接口：我们的框架基于统一任务插件接口构建，规范了单回合推理与复杂多回合主体任务的交互。该模块化设计允许在不更改核心基础设施的情况下无缝接入多样的新环境与任务。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_18_24_bcd7ad.jpg"/>



Figure 6: Overview of our scalable RL infrastructure, which unifies single-turn reasoning and multi-turn agentic training in a fully decoupled rollout-update framework. All components can run in parallel for high throughput, with diverse task-specific interactions plugged into the scalable experience maker with a unified interface. A rollout manager assigns task IDs, collects trajectories and rewards, and coordinates data flow via a shared data center.
图 6：我们可扩展 RL 基础设施的概览，在一个完全解耦的 rollout-update 框架中统一单回合推理与多回合主体训练。所有组件可并行运行以实现高吞吐，任务特定的多样交互通过统一接口插入到可扩展的经验生成器中。rollout 管理器分配任务 ID、收集轨迹与奖励，并通过共享数据中心协调数据流。


- Decoupled, Controllable Rollout: We decouple the experience generation (rollout) phase from policy updates, giving operators precise control over the entire data supply chain. This control is multi-faceted: the manager can dictate the degree of policy-adherence, from a strictly synchronous on-policy mode to an asynchronous, slightly off-policy mode for speed. It also has full control over resource allocation, deploying rollouts on hardware optimized for inference to maximize throughput. This granular control enables us to fine-tune the data generation process to achieve an optimal balance among optimization guarantees, speed, and cost.
- 解耦且可控的 rollout：我们将经验生成（rollout）阶段与策略更新解耦，赋予运维对整个数据供应链的精确控制。该控制是多方面的：管理者可决定策略遵从程度，从严格同步的 on-policy 模式到为提速而略微 off-policy 的异步模式；同时可完全控制资源分配，将 rollout 部署在针对推理优化的硬件上以最大化吞吐。此细粒度控制使我们能微调数据生成流程，在优化保证、速度与成本之间取得最佳平衡。


#### 3.1.2 Task Mixture
#### 3.1.2 任务混合


We apply GRPO (Guo et al., 2025) to train GUI-Owl on static tasks, and apply the trajectory-aware Relative Policy Optimization (TRPO) strategy for training in online environments. In this section, we present the data preparation methods for different downstream reinforcement learning tasks and introduce trajectory-aware relative policy optimization.
我们对静态任务使用 GRPO (Guo et al., 2025) 训练 GUI-Owl，并在在线环境中采用考虑轨迹的相对策略优化（TRPO）策略进行训练。本节介绍不同下游强化学习任务的数据准备方法，并引入轨迹感知的相对策略优化。


For grounding task, a subset of data from GUI-R1 (Luo et al., 2025), UI-Vision (Nayak et al., 2025) serves as the foundational dataset. To further enhance RL performance in challenging fine-grained grounding, a curated collection of high-difficulty fine-grained grounding samples is incorporated (i.e., where the target UI regions occupy less than ${0.1}\%$ of the entire screenshot area). Subsequently,the all datasets are performed ${n}_{g}$ sampling iterations (where ${n}_{q} = 8$ in our implementation) utilizing the policy model GUI-Owl before RL,and sample instances that exhibit partial failure cases as training corpus for RL optimization.
对于落地任务，选取自 GUI-R1 (Luo et al., 2025)、UI-Vision (Nayak et al., 2025) 的部分数据作为基础数据集。为进一步提升在高难度细粒度落地任务中的 RL 性能，加入一批精心挑选的高难度细粒度落地样本（即目标 UI 区域占整个截图面积不足 ${0.1}\%$ 的情况）。随后，所有数据集在 RL 之前使用策略模型 GUI-Owl 进行 ${n}_{g}$ 次采样迭代（在我们的实现中为 ${n}_{q} = 8$），并将表现出部分失败案例的样本作为 RL 优化的训练语料。


To enhance the capabilities of low-level (i.e., step-level) actions, we introduce single-turn reinforcement learning. The training data is derived directly from individual steps within high-quality offline interaction trajectories.
为增强低级（即步级）动作能力，我们引入单回合强化学习。训练数据直接来源于高质量离线交互轨迹中的单步。


While the preceding RL phases build foundational skills, applying them to complex, multi-step tasks in an online environment introduces significant challenges. Therefore,, we also conduct online reinforcement learning for GUI-Owl on virtual environments. These tasks are selected from the training task pool and use either rule-based or critic-based rewards as feedback signals for determining task completion.
尽管前述 RL 阶段构建了基础技能，但将其应用于在线环境中的复杂多步任务仍面临重大挑战。因此，我们还在虚拟环境中对 GUI-Owl 进行在线强化学习。这些任务从训练任务池中选取，并使用基于规则或基于评判器的奖励作为确定任务完成的反馈信号。


Trajectory-aware Relative Policy Optimization for Online Environment RL. Real-world user tasks are often characterized by long and variable-length action sequences. In such scenarios, rewards are typically sparse and only available as a delayed success signal upon task completion. To overcome these obstacles, we employ a trajectory-aware relative policy optimization strategy (TRPO) extended to GRPO (Guo et al., 2025). This approach circumvents the challenge of assigning per-step rewards, a task that is nearly impossible to perform accurately in complex GUI interactions. Instead, we evaluate the entire trajectory $\tau$ after its completion. Our experiments are conducted on the OSWorld-Verified benchmark (Xie et al., 2024), where the outcome of each task is programmatically verifiable,allowing us to obtain a reliable,single,holistic reward scalar $R\left( \tau \right)$ . Specifically, this reward is the sum of an accuracy reward (1 for a successful trajectory, 0 otherwise) and a format reward, which penalizes malformed actions with a value of -0.5 .
面向轨迹的相对策略优化用于在线环境强化学习。真实用户任务通常由长度长且可变的动作序列构成。在此类场景中，奖励通常稀疏，且仅在任务完成时以延迟的成功信号形式提供。为克服这些障碍，我们采用了扩展到 GRPO (Guo et al., 2025) 的面向轨迹的相对策略优化策略 (TRPO)。该方法避免了逐步分配奖励的难题——在复杂 GUI 交互中几乎不可能准确完成。相反，我们在整个轨迹完成后评估该轨迹 $\tau$ 。我们的实验在 OSWorld-Verified 基准上进行 (Xie et al., 2024)，每个任务的结果可程序化地验证，从而使我们能获得可靠的、单一的整体奖励标量 $R\left( \tau \right)$ 。具体而言，该奖励是准确度奖励（成功轨迹为 1， 否则为 0）与格式奖励之和，格式奖励对格式错误的动作惩罚为 -0.5。


This trajectory-level reward is then used to compute a normalized advantage estimate, which provides a stable learning signal across all steps of the trajectory. The advantage for a given trajectory $\tau$ is calculated as: ${\widehat{A}}_{\tau } = \; \frac{R\left( \tau \right)  - \bar{R}}{{\sigma }_{R} + \epsilon }$ ,where $\bar{R}$ and ${\sigma }_{R}$ are the running mean and standard deviation of rewards observed across multiple trajectories. This normalized advantage ${\widehat{A}}_{\tau }$ is then uniformly distributed to every action taken within that trajectory, ensuring that all steps contributing to a successful outcome receive a consistent positive signal, thereby mitigating the credit assignment problem.
该轨迹级奖励随后用于计算归一化优势估计，为轨迹的所有步骤提供稳定的学习信号。给定轨迹的优势为 $\tau$ 计算如下：${\widehat{A}}_{\tau } = \; \frac{R\left( \tau \right)  - \bar{R}}{{\sigma }_{R} + \epsilon }$ ，其中 $\bar{R}$ 与 ${\sigma }_{R}$ 分别是跨多条轨迹观测到的奖励的运行均值与标准差。此归一化优势 ${\widehat{A}}_{\tau }$ 然后平均分配到该轨迹中采取的每个动作，确保所有促成成功结果的步骤都收到一致的正信号，从而缓解归因分配问题。


Given the inherent sparsity of successful trajectories in GUI tasks, we incorporate a replay buffer to stabilize training. This buffer stores historically successful trajectories, indexed by their task_id. During the sampling process, if a generated group of trajectories for a task results entirely in failures, one failing trajectory is randomly replaced with a successful one from the buffer corresponding to the same task. This injection of positive examples ensures the effectiveness of the training signal in every batch.
鉴于 GUI 任务中成功轨迹的固有稀疏性，我们引入了回放缓冲区以稳定训练。该缓冲区按 task_id 存储历史成功轨迹。在采样过程中，如果为某任务生成的一组轨迹全部失败，则随机将其中一条失败轨迹替换为来自该任务缓冲区的一条成功轨迹。此正例注入确保每个批次中训练信号的有效性。


Our final policy optimization objective for a batch of $G$ trajectories is defined by the following loss function:
我们针对一批 $G$ 条轨迹的最终策略优化目标由以下损失函数定义：


$$
{\mathcal{L}}_{\text{ TRPO }} =  - \frac{1}{N}\mathop{\sum }\limits_{{i = 1}}^{G}\mathop{\sum }\limits_{{s = 1}}^{{S}_{i}}\mathop{\sum }\limits_{{t = 1}}^{\left| {\mathbf{o}}_{i,s}\right| }\left\{  {\min \left\lbrack  {{r}_{t}\left( \theta \right) {\widehat{A}}_{{\tau }_{i}},\operatorname{clip}\left( {{r}_{t}\left( \theta \right) ,1 - \epsilon ,1 + \epsilon }\right) {\widehat{A}}_{{\tau }_{i}}}\right\rbrack  }\right\} \tag{3}
$$



where $N$ is the total number of tokens in the batch, ${\widehat{A}}_{{\tau }_{i}}$ is the trajectory-level advantage for trajectory $i$ ,and ${r}_{t}\left( \theta \right)  = \frac{{\pi }_{\theta }\left( {{o}_{s,t} \mid  \ldots }\right) }{{\pi }_{{\theta }_{\text{ old }}}\left( {{o}_{s,t} \mid  \ldots }\right) }$ is the probability ratio of a token under the current and old policies. This clipped objective function stabilizes training while effectively leveraging the holistic trajectory-level reward signal for long-horizon GUI automation tasks.
其中 $N$ 是批次中的令牌总数，${\widehat{A}}_{{\tau }_{i}}$ 是轨迹 $i$ 的轨迹级优势，${r}_{t}\left( \theta \right)  = \frac{{\pi }_{\theta }\left( {{o}_{s,t} \mid  \ldots }\right) }{{\pi }_{{\theta }_{\text{ old }}}\left( {{o}_{s,t} \mid  \ldots }\right) }$ 是当前策略与旧策略下某令牌的概率比。该裁剪目标函数在稳定训练的同时，有效利用轨迹级整体奖励信号来处理长时域 GUI 自动化任务。


In practice, the high resolution of GUI screenshots means that a complete interaction trajectory can quickly exceed the model's context length (e.g., 32k for Qwen2.5-VL). To manage this, we segment each full multi-turn trajectory into several single-step data instances for the policy update. The loss computed for each step-wise instance is then scaled by the total number of steps in its original, complete trajectory. This approach addresses the issue of unbalanced optimization for trajectories of different lengths.
在实践中，GUI 截图的高分辨率意味着完整交互轨迹很快就可能超过模型的上下文长度（例如 Qwen2.5-VL 为 32k）。为应对这一点，我们将每条完整的多轮轨迹分割为若干用于策略更新的单步数据实例。然后将为每个逐步实例计算的损失按其原始完整轨迹的总步数进行缩放。该方法解决了不同长度轨迹的优化不均衡问题。


## 4 Mobile-AGENT-v3
## 4 Mobile-AGENT-v3


The agent-based framework method modularizes complex GUI tasks into multiple relatively simple tasks, and can achieve higher performance with the cooperation of agents with different roles. (Zhang et al., 2025a; Li et al., 2024; Wang et al., 2024b; 2025b; Agashe et al., 2025; 2024; Zhang et al., 2025b; Li et al., 2025; Liu et al., 2024; Nong et al., 2024; Wu et al., 2024a;b; Sun et al., 2024; Zhang et al., 2024b; Zheng et al., 2024; Patel et al., 2024; Niu et al., 2024; Tan et al., 2024). As noted earlier, GUI-Owl possesses multi-agent collaboration capabilities. Building upon this foundation, we further propose Mobile-Agent-v3, a multi-agent framework endowed with capabilities for knowledge evolution, task planning, sub-task execution, and reflective reasoning. In this section, we discuss the architecture of Mobile-Agent-v3.
基于代理的框架方法将复杂 GUI 任务模块化为多个相对简单的任务，并可通过不同角色代理的协作实现更高性能。(Zhang et al., 2025a; Li et al., 2024; Wang et al., 2024b; 2025b; Agashe et al., 2025; 2024; Zhang et al., 2025b; Li et al., 2025; Liu et al., 2024; Nong et al., 2024; Wu et al., 2024a;b; Sun et al., 2024; Zhang et al., 2024b; Zheng et al., 2024; Patel et al., 2024; Niu et al., 2024; Tan et al., 2024)。如前所述，GUI-Owl 具备多代理协作能力。在此基础上，我们进一步提出了 Mobile-Agent-v3，一种具备知识演化、任务规划、子任务执行与反思推理能力的多代理框架。本节讨论 Mobile-Agent-v3 的架构。


As presented in Figure 7, the Mobile-Agent-v3 framework coordinates four specialized agents to achieve robust, adaptive, and long-horizon GUI task automation:
如图7所示，Mobile-Agent-v3 框架协调四个专职代理以实现稳健、适应性强且长期的 GUI 任务自动化：


- Manager Agent $\left( \mathcal{M}\right)$ : Serves as the strategic planner. At initialization,it decomposes a high-level instruction $I$ into an ordered subgoal list $S{S}_{0}$ using external knowledge ${K}_{\mathrm{{RAG}}}$ . During execution, it updates the plan based on results and feedback, re-prioritizing, modifying, or inserting corrective subgoals.
- Manager Agent $\left( \mathcal{M}\right)$：作为战略规划者。初始化时，它利用外部知识 ${K}_{\mathrm{{RAG}}}$ 将高层指令 $I$ 分解为有序的子目标列表 $S{S}_{0}$。执行过程中，基于结果与反馈更新计划，重新排序、修改或插入纠正性子目标。


- Worker Agent $\left( \mathcal{W}\right)$ : Acts as the tactical executor. It selects and performs the most relevant actionable subgoal from $S{S}_{t}$ given the current GUI state ${S}_{t}$ ,prior feedback ${F}_{t - 1}$ ,and accumulated notes ${\mathcal{N}}_{t}$ , producing an action tuple ${A}_{t}$ that records reasoning,action,and intent.
- Worker Agent $\left( \mathcal{W}\right)$：作为战术执行者。它在给定当前 GUI 状态 ${S}_{t}$、先前反馈 ${F}_{t - 1}$ 和积累笔记 ${\mathcal{N}}_{t}$ 的情况下，从 $S{S}_{t}$ 中选择并执行最相关的可操作子目标，产出记录推理、动作和意图的动作元组 ${A}_{t}$。


- Reflector Agent $\left( \mathcal{R}\right)$ : Functions as the self-correction mechanism. It compares the intended outcome from the Worker with the actual state transition $\left( {{S}_{t} \rightarrow  {S}_{t + 1}}\right)$ ,classifying the result as SUCCESS or FAILURE and generating detailed causal feedback ${\phi }_{t}$ for the Manager.
- Reflector Agent $\left( \mathcal{R}\right)$：作为自我纠错机制。它将 Worker 的预期结果与实际状态转变 $\left( {{S}_{t} \rightarrow  {S}_{t + 1}}\right)$ 比较，将结果归类为 SUCCESS 或 FAILURE，并为 Manager 生成详细的因果反馈 ${\phi }_{t}$。


- Notetaker Agent (C): Maintains persistent contextual memory. Triggered only on SUCCESS, it extracts and stores critical screen elements (e.g.,codes,credentials) as notes ${N}_{t}$ . The cumulative memory ${\mathcal{N}}_{t + 1}$ supports both planning and execution in future steps.
- 笔记记录器代理（C）：维护持久的上下文记忆。仅在成功时触发，它会提取并存储关键屏幕元素（例如，代码、凭证）作为笔记 ${N}_{t}$。累积记忆 ${\mathcal{N}}_{t + 1}$ 支持未来步骤中的规划和执行。


The Manager decomposes and dynamically updates the plan, the Worker executes selected subgoals, the Reflector evaluates outcomes and provides diagnostic feedback, and the Notetaker preserves valuable transient information. This loop continues until all subgoals are completed or the instruction $I$ is fulfilled. More details can be found in Section 7.
管理器对计划进行分解并动态更新，执行者执行选定的子目标，反思者评估结果并提供诊断反馈，笔记记录器保存有价值的临时信息。这个循环会一直持续，直到所有子目标完成或指令 $I$ 得到执行。更多细节可在第 7 节中找到。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_18_24_b0f4f0.jpg"/>



Figure 7: Mobile-Agent-v3 architecture. The system consists of six modules: (1) a RAG module for retrieving external world knowledge, (2) a Manager Agent for subgoal planning and guidance, (3) a Worker Agent for GUI operation, (4) a Reflector Agent for evaluation and feedback (5) a Notetaker Agent for recording important note, and (6) A GUI device interface supporting phone and PC environments.
图 7：移动代理 v3 架构。该系统由六个模块组成：（1）用于检索外部世界知识的 RAG 模块；（2）用于子目标规划和指导的管理器代理；（3）用于图形用户界面操作的执行者代理；（4）用于评估和反馈的反思者代理；（5）用于记录重要笔记的笔记记录器代理；（6）支持手机和 PC 环境的图形用户界面设备接口。


## 5 EXPERIMENTS
## 5 实验


### 5.1 Model Evaluation
### 5.1 模型评估


In this section, we evaluate GUI-Owl on a wide range of benchmarks to thoroughly assess its performance as a fundamental agent in GUI-based scenarios. We train GUI-Owl-7B and GUI-Owl-32B, which are initialized from the Qwen2.5-VL models of the corresponding sizes. We conduct extensive experiments to evaluate GUI-Owl in four key dimensions: grounding capability, comprehensive GUI understanding, end-to-end agent capability, and multi-agent capability.
在本节中，我们在广泛的基准测试中对 GUI - Owl 进行评估，以全面评估其在基于图形用户界面场景中作为基础代理的性能。我们训练了 GUI - Owl - 7B 和 GUI - Owl - 32B，它们分别从相应大小的 Qwen2.5 - VL 模型初始化。我们进行了广泛的实验，从四个关键维度评估 GUI - Owl：定位能力、全面的图形用户界面理解能力、端到端代理能力和多代理能力。


#### 5.1.1 GROUNDING CAPABILITY
#### 5.1.1 定位能力


The grounding capability evaluates a model's ability to locate the corresponding UI element given a natural-language query. We use ScreenSpot V2, ScreenSpot Pro, OSWorld-G, and MMBench-GUI L2 as benchmarks. ScreenSpot v2 covers mobile, desktop, and web scenarios, while ScreenSpot-Pro primarily evaluates a model's localization ability at ultra-high resolutions. OSWorld-G contains finely annotated queries. MMBench-GUI L2 has the broadest coverage and more faithfully reflects a model's grounding performance in real-world settings. The performance comparisons are shown in Table 1, Table 2, Table 3 and Table 4.
定位能力评估模型在给定自然语言查询时定位相应用户界面元素的能力。我们使用 ScreenSpot V2、ScreenSpot Pro、OSWorld - G 和 MMBench - GUI L2 作为基准测试。ScreenSpot v2 涵盖了移动、桌面和网页场景，而 ScreenSpot - Pro 主要评估模型在超高分辨率下的定位能力。OSWorld - G 包含精细标注的查询。MMBench - GUI L2 的覆盖范围最广，能更真实地反映模型在现实环境中的定位性能。性能比较结果显示在表 1、表 2、表 3 和表 4 中。


GUI-Owl-7B achieves state-of-the-art performance among all 7B models. On screenspot-pro, which focuses on high-resolution images, we achieve a score of 54.9, significantly exceeding the performance of UI-TARS-72B and Qwen2.5-VL 72B. GUI-Owl-7B also achieves competitive performance on OSWorld-G compared to UI-TARS-72B. GUI-Owl-32B surpasses all models of the same size. MMBench-GUI-L2 evaluates a very broad and challenging set of queries, where our model scores 80.49, substantially outperforming all existing models. GUI-Owl-32B further achieves a performance level of 82.97 and demonstrates leading grounding capabilities across various domains.
GUI - Owl - 7B 在所有 7B 模型中达到了最先进的性能。在专注于高分辨率图像的 screenspot - pro 上，我们取得了 54.9 的分数，显著超过了 UI - TARS - 72B 和 Qwen2.5 - VL 72B 的性能。与 UI - TARS - 72B 相比，GUI - Owl - 7B 在 OSWorld - G 上也取得了有竞争力的性能。GUI - Owl - 32B 超越了所有相同大小的模型。MMBench - GUI - L2 评估了一组非常广泛且具有挑战性的查询，我们的模型在此测试中得分 80.49，大幅优于所有现有模型。GUI - Owl - 32B 进一步达到了 82.97 的性能水平，并在各个领域展示了领先的定位能力。


#### 5.1.2 COMPREHENSIVE GUI UNDERSTANDING
#### 5.1.2 全面的图形用户界面理解


Comprehensive GUI Understanding examines whether a GUI model can accurately interpret interface states and produce appropriate responses. We adopt two benchmarks for this evaluation. MMBench-GUI-L1 assesses the model's UI understanding and single-step decision-making capability through a question-answering format. Android Control evaluates the model's ability to perform single-step decisions within pre-annotated trajectory contexts.
全面的图形用户界面理解考察图形用户界面模型是否能准确解释界面状态并做出适当响应。我们采用两个基准测试进行此评估。MMBench - GUI - L1 通过问答形式评估模型的用户界面理解和单步决策能力。Android Control 评估模型在预先标注的轨迹上下文中执行单步决策的能力。


<table><tr><td rowspan="2">Agent Model</td><td colspan="2">Mobile</td><td colspan="2">Desktop</td><td colspan="2">Web</td><td rowspan="2">Overall</td></tr><tr><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td></tr><tr><td colspan="8">Proprietary Models</td></tr><tr><td>Operator (OpenAI, 2025a)</td><td>47.3</td><td>41.5</td><td>90.2</td><td>80.3</td><td>92.8</td><td>84.3</td><td>70.5</td></tr><tr><td>Claude 3.7 Sonnet (Anthropic, 2025a)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>87.6</td></tr><tr><td>UI-TARS-1.5 (Qin et al., 2025)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>94.2</td></tr><tr><td>Seed-1.5-VL (Team, 2025)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>95.2</td></tr><tr><td colspan="8">Open-Source Models</td></tr><tr><td>SeeClick (Cheng et al., 2024)</td><td>78.4</td><td>50.7</td><td>70.1</td><td>29.3</td><td>55.2</td><td>32.5</td><td>55.1</td></tr><tr><td>OmniParser-v2 (Yu et al., 2025)</td><td>95.5</td><td>74.6</td><td>92.3</td><td>60.9</td><td>88.0</td><td>59.6</td><td>80.7</td></tr><tr><td>Qwen2.5-VL-3B (Bai et al., 2025)</td><td>93.4</td><td>73.5</td><td>88.1</td><td>58.6</td><td>88.0</td><td>71.4</td><td>80.9</td></tr><tr><td>UI-TARS-2B (Qin et al., 2025)</td><td>95.2</td><td>79.1</td><td>90.7</td><td>68.6</td><td>87.2</td><td>78.3</td><td>84.7</td></tr><tr><td>OS-Atlas-Base-4B (Wu et al., 2024b)</td><td>95.2</td><td>75.8</td><td>90.7</td><td>63.6</td><td>90.6</td><td>77.3</td><td>85.1</td></tr><tr><td>OS-Atlas-Base-7B (Wu et al., 2024b)</td><td>96.2</td><td>83.4</td><td>89.7</td><td>69.3</td><td>94.0</td><td>79.8</td><td>87.1</td></tr><tr><td>JEDI-3B (Xie et al., 2025)</td><td>96.6</td><td>81.5</td><td>96.9</td><td>78.6</td><td>88.5</td><td>83.7</td><td>88.6</td></tr><tr><td>Qwen2.5-VL-7B (Bai et al., 2025)</td><td>97.6</td><td>87.2</td><td>90.2</td><td>74.2</td><td>93.2</td><td>81.3</td><td>88.8</td></tr><tr><td>UI-TARS-72B (Qin et al., 2025)</td><td>94.8</td><td>86.3</td><td>91.2</td><td>87.9</td><td>91.5</td><td>87.7</td><td>90.3</td></tr><tr><td>UI-TARS-7B (Qin et al., 2025)</td><td>96.9</td><td>89.1</td><td>95.4</td><td>85.0</td><td>93.6</td><td>85.2</td><td>91.6</td></tr><tr><td>JEDI-7B (Xie et al., 2025)</td><td>96.9</td><td>87.2</td><td>95.9</td><td>87.9</td><td>94.4</td><td>84.2</td><td>91.7</td></tr><tr><td>GUI-Owl-7B</td><td>99.0</td><td>92.4</td><td>96.9</td><td>85.0</td><td>93.6</td><td>85.2</td><td>92.8</td></tr><tr><td>GUI-Owl-32B</td><td>98.6</td><td>90.0</td><td>97.9</td><td>87.8</td><td>94.4</td><td>86.7</td><td>93.2</td></tr></table>
<table><tbody><tr><td rowspan="2">代理模型</td><td colspan="2">移动设备</td><td colspan="2">桌面端</td><td colspan="2">网页端</td><td rowspan="2">总体</td></tr><tr><td>文本</td><td>图标</td><td>文本</td><td>图标</td><td>文本</td><td>图标</td></tr><tr><td colspan="8">专有模型</td></tr><tr><td>运营商（OpenAI，2025a）</td><td>47.3</td><td>41.5</td><td>90.2</td><td>80.3</td><td>92.8</td><td>84.3</td><td>70.5</td></tr><tr><td>Claude 3.7 Sonnet（Anthropic，2025a）</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>87.6</td></tr><tr><td>UI-TARS-1.5（Qin 等人，2025）</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>94.2</td></tr><tr><td>Seed-1.5-VL（团队，2025）</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>95.2</td></tr><tr><td colspan="8">开源模型</td></tr><tr><td>SeeClick（Cheng 等人，2024）</td><td>78.4</td><td>50.7</td><td>70.1</td><td>29.3</td><td>55.2</td><td>32.5</td><td>55.1</td></tr><tr><td>OmniParser-v2（Yu 等人，2025）</td><td>95.5</td><td>74.6</td><td>92.3</td><td>60.9</td><td>88.0</td><td>59.6</td><td>80.7</td></tr><tr><td>Qwen2.5-VL-3B（Bai 等人，2025）</td><td>93.4</td><td>73.5</td><td>88.1</td><td>58.6</td><td>88.0</td><td>71.4</td><td>80.9</td></tr><tr><td>UI-TARS-2B（Qin 等人，2025）</td><td>95.2</td><td>79.1</td><td>90.7</td><td>68.6</td><td>87.2</td><td>78.3</td><td>84.7</td></tr><tr><td>OS-Atlas-Base-4B（Wu 等人，2024b）</td><td>95.2</td><td>75.8</td><td>90.7</td><td>63.6</td><td>90.6</td><td>77.3</td><td>85.1</td></tr><tr><td>OS-Atlas-Base-7B（Wu 等人，2024b）</td><td>96.2</td><td>83.4</td><td>89.7</td><td>69.3</td><td>94.0</td><td>79.8</td><td>87.1</td></tr><tr><td>JEDI-3B（Xie 等人，2025）</td><td>96.6</td><td>81.5</td><td>96.9</td><td>78.6</td><td>88.5</td><td>83.7</td><td>88.6</td></tr><tr><td>Qwen2.5-VL-7B（Bai 等人，2025）</td><td>97.6</td><td>87.2</td><td>90.2</td><td>74.2</td><td>93.2</td><td>81.3</td><td>88.8</td></tr><tr><td>UI-TARS-72B（Qin 等人，2025）</td><td>94.8</td><td>86.3</td><td>91.2</td><td>87.9</td><td>91.5</td><td>87.7</td><td>90.3</td></tr><tr><td>UI-TARS-7B（Qin 等人，2025）</td><td>96.9</td><td>89.1</td><td>95.4</td><td>85.0</td><td>93.6</td><td>85.2</td><td>91.6</td></tr><tr><td>JEDI-7B（Xie 等人，2025）</td><td>96.9</td><td>87.2</td><td>95.9</td><td>87.9</td><td>94.4</td><td>84.2</td><td>91.7</td></tr><tr><td>GUI-Owl-7B</td><td>99.0</td><td>92.4</td><td>96.9</td><td>85.0</td><td>93.6</td><td>85.2</td><td>92.8</td></tr><tr><td>GUI-Owl-32B</td><td>98.6</td><td>90.0</td><td>97.9</td><td>87.8</td><td>94.4</td><td>86.7</td><td>93.2</td></tr></tbody></table>


Table 1: Comparison with state-of-the-art methods on the ScreenSpot-V2 dataset. Underlined denotes the second-best open-source performance.
表 1：在 ScreenSpot-V2 数据集上与最先进方法的比较。下划线表示第二佳开源性能。


<table><tr><td rowspan="2">Agent Model</td><td colspan="2">Development</td><td colspan="2">Creative</td><td colspan="2">CAD</td><td colspan="2">Scientific</td><td colspan="2">Office</td><td colspan="2">OS</td><td>Avg</td></tr><tr><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td></td></tr><tr><td colspan="14">Proprietary Models</td></tr><tr><td>GPT-40 (Hurst et al., 2024)</td><td>1.3</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>2.1</td><td>0.0</td><td>1.1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.8</td></tr><tr><td>Claude 3.7 Sonnet (Anthropic, 2025a)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>27.7</td></tr><tr><td>Operator (OpenAI, 2025a)</td><td>50.0</td><td>19.3</td><td>51.5</td><td>23.1</td><td>16.8</td><td>14.1</td><td>58.3</td><td>24.5</td><td>60.5</td><td>28.3</td><td>34.6</td><td>30.3</td><td>36.6</td></tr><tr><td>Seed-1.5-VL (Team, 2025)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>60.9</td></tr><tr><td>UI-TARS-1.5 (Qin et al., 2025)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>61.6</td></tr><tr><td colspan="14">Open-Source Models</td></tr><tr><td>UI-TARS-2B (Qin et al., 2025)</td><td>47.4</td><td>4.1</td><td>42.9</td><td>6.3</td><td>17.8</td><td>4.7</td><td>56.9</td><td>17.3</td><td>50.3</td><td>17.0</td><td>21.5</td><td>5.6</td><td>27.7</td></tr><tr><td>Qwen2.5-VL-3B (Bai et al., 2025)</td><td>38.3</td><td>3.4</td><td>40.9</td><td>4.9</td><td>22.3</td><td>6.3</td><td>44.4</td><td>10.0</td><td>48.0</td><td>17.0</td><td>33.6</td><td>4.5</td><td>25.9</td></tr><tr><td>Qwen2.5-VL-7B (Bai et al., 2025)</td><td>51.9</td><td>4.8</td><td>36.9</td><td>8.4</td><td>17.8</td><td>1.6</td><td>48.6</td><td>8.2</td><td>53.7</td><td>18.9</td><td>34.6</td><td>7.9</td><td>27.6</td></tr><tr><td>UI-R1-E-3B (Lu et al., 2025)</td><td>46.1</td><td>6.9</td><td>41.9</td><td>4.2</td><td>37.1</td><td>12.5</td><td>56.9</td><td>21.8</td><td>65.0</td><td>26.4</td><td>32.7</td><td>10.1</td><td>33.5</td></tr><tr><td>UI-TARS-7B (Qin et al., 2025)</td><td>58.4</td><td>12.4</td><td>50.0</td><td>9.1</td><td>20.8</td><td>9.4</td><td>63.9</td><td>31.8</td><td>63.3</td><td>20.8</td><td>30.8</td><td>16.9</td><td>35.7</td></tr><tr><td>InfiGUI-R1-3B (Liu et al., 2025)</td><td>51.3</td><td>12.4</td><td>44.9</td><td>7.0</td><td>33.0</td><td>14.1</td><td>58.3</td><td>20.0</td><td>65.5</td><td>28.3</td><td>43.9</td><td>12.4</td><td>35.7</td></tr><tr><td>JEDI-3B (Xie et al., 2025)</td><td>61.0</td><td>13.8</td><td>53.5</td><td>8.4</td><td>27.4</td><td>9.4</td><td>54.2</td><td>18.2</td><td>64.4</td><td>32.1</td><td>38.3</td><td>9.0</td><td>36.1</td></tr><tr><td>GUI-G1-3B (Zhou et al., 2025)</td><td>50.7</td><td>10.3</td><td>36.6</td><td>11.9</td><td>39.6</td><td>9.4</td><td>61.8</td><td>30.0</td><td>67.2</td><td>32.1</td><td>23.5</td><td>10.6</td><td>37.1</td></tr><tr><td>UI-TARS-72B (Qin et al., 2025)</td><td>63.0</td><td>17.3</td><td>57.1</td><td>15.4</td><td>18.8</td><td>12.5</td><td>64.6</td><td>20.9</td><td>63.3</td><td>26.4</td><td>42.1</td><td>15.7</td><td>38.1</td></tr><tr><td>JEDI-7B (Xie et al., 2025)</td><td>42.9</td><td>11.0</td><td>50.0</td><td>11.9</td><td>38.0</td><td>14.1</td><td>72.9</td><td>25.5</td><td>75.1</td><td>47.2</td><td>33.6</td><td>16.9</td><td>39.5</td></tr><tr><td>Qwen2.5-VL-32B (Bai et al., 2025)</td><td>74.0</td><td>21.4</td><td>61.1</td><td>13.3</td><td>38.1</td><td>15.6</td><td>78.5</td><td>29.1</td><td>76.3</td><td>37.7</td><td>55.1</td><td>27.0</td><td>47.6</td></tr><tr><td>SE-GUI-7B (Yuan et al., 2025)</td><td>68.2</td><td>19.3</td><td>57.6</td><td>9.1</td><td>51.3</td><td>42.2</td><td>75.0</td><td>28.2</td><td>78.5</td><td>43.4</td><td>49.5</td><td>25.8</td><td>47.3</td></tr><tr><td>GUI-G2-7B (Tang et al., 2025)</td><td>68.8</td><td>17.2</td><td>57.1</td><td>15.4</td><td>55.8</td><td>12.5</td><td>77.1</td><td>24.5</td><td>74.0</td><td>32.7</td><td>57.9</td><td>21.3</td><td>47.5</td></tr><tr><td>Qwen2.5-VL-72B (Bai et al., 2025)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>53.3</td></tr><tr><td>GUI-Owl-7B</td><td>76.6</td><td>31.0</td><td>59.6</td><td>27.3</td><td>64.5</td><td>21.9</td><td>79.1</td><td>37.3</td><td>77.4</td><td>39.6</td><td>59.8</td><td>33.7</td><td>54.9</td></tr><tr><td>GUI-Owl-32B</td><td>84.4</td><td>39.3</td><td>65.2</td><td>18.2</td><td>62.4</td><td>28.1</td><td>82.6</td><td>39.1</td><td>81.4</td><td>39.6</td><td>70.1</td><td>36.0</td><td>58.0</td></tr></table>
<table><tbody><tr><td rowspan="2">代理模型</td><td colspan="2">开发</td><td colspan="2">创意</td><td colspan="2">计算机辅助设计</td><td colspan="2">科学</td><td colspan="2">办公</td><td colspan="2">操作系统</td><td>平均</td></tr><tr><td>文本</td><td>图标</td><td>文本</td><td>图标</td><td>文本</td><td>图标</td><td>文本</td><td>图标</td><td>文本</td><td>图标</td><td>文本</td><td>图标</td><td></td></tr><tr><td colspan="14">专有模型</td></tr><tr><td>GPT - 40（赫斯特等人，2024年）</td><td>1.3</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>2.1</td><td>0.0</td><td>1.1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.8</td></tr><tr><td>Claude 3.7 Sonnet（Anthropic公司，2025a）</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>27.7</td></tr><tr><td>Operator（OpenAI公司，2025a）</td><td>50.0</td><td>19.3</td><td>51.5</td><td>23.1</td><td>16.8</td><td>14.1</td><td>58.3</td><td>24.5</td><td>60.5</td><td>28.3</td><td>34.6</td><td>30.3</td><td>36.6</td></tr><tr><td>Seed - 1.5 - VL（团队，2025年）</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>60.9</td></tr><tr><td>UI - TARS - 1.5（秦等人，2025年）</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>61.6</td></tr><tr><td colspan="14">开源模型</td></tr><tr><td>UI - TARS - 2B（秦等人，2025年）</td><td>47.4</td><td>4.1</td><td>42.9</td><td>6.3</td><td>17.8</td><td>4.7</td><td>56.9</td><td>17.3</td><td>50.3</td><td>17.0</td><td>21.5</td><td>5.6</td><td>27.7</td></tr><tr><td>Qwen2.5 - VL - 3B（白等人，2025年）</td><td>38.3</td><td>3.4</td><td>40.9</td><td>4.9</td><td>22.3</td><td>6.3</td><td>44.4</td><td>10.0</td><td>48.0</td><td>17.0</td><td>33.6</td><td>4.5</td><td>25.9</td></tr><tr><td>Qwen2.5 - VL - 7B（白等人，2025年）</td><td>51.9</td><td>4.8</td><td>36.9</td><td>8.4</td><td>17.8</td><td>1.6</td><td>48.6</td><td>8.2</td><td>53.7</td><td>18.9</td><td>34.6</td><td>7.9</td><td>27.6</td></tr><tr><td>UI - R1 - E - 3B（陆等人，2025年）</td><td>46.1</td><td>6.9</td><td>41.9</td><td>4.2</td><td>37.1</td><td>12.5</td><td>56.9</td><td>21.8</td><td>65.0</td><td>26.4</td><td>32.7</td><td>10.1</td><td>33.5</td></tr><tr><td>UI - TARS - 7B（秦等人，2025年）</td><td>58.4</td><td>12.4</td><td>50.0</td><td>9.1</td><td>20.8</td><td>9.4</td><td>63.9</td><td>31.8</td><td>63.3</td><td>20.8</td><td>30.8</td><td>16.9</td><td>35.7</td></tr><tr><td>InfiGUI - R1 - 3B（刘等人，2025年）</td><td>51.3</td><td>12.4</td><td>44.9</td><td>7.0</td><td>33.0</td><td>14.1</td><td>58.3</td><td>20.0</td><td>65.5</td><td>28.3</td><td>43.9</td><td>12.4</td><td>35.7</td></tr><tr><td>JEDI - 3B（谢等人，2025年）</td><td>61.0</td><td>13.8</td><td>53.5</td><td>8.4</td><td>27.4</td><td>9.4</td><td>54.2</td><td>18.2</td><td>64.4</td><td>32.1</td><td>38.3</td><td>9.0</td><td>36.1</td></tr><tr><td>GUI - G1 - 3B（周等人，2025年）</td><td>50.7</td><td>10.3</td><td>36.6</td><td>11.9</td><td>39.6</td><td>9.4</td><td>61.8</td><td>30.0</td><td>67.2</td><td>32.1</td><td>23.5</td><td>10.6</td><td>37.1</td></tr><tr><td>UI - TARS - 72B（秦等人，2025年）</td><td>63.0</td><td>17.3</td><td>57.1</td><td>15.4</td><td>18.8</td><td>12.5</td><td>64.6</td><td>20.9</td><td>63.3</td><td>26.4</td><td>42.1</td><td>15.7</td><td>38.1</td></tr><tr><td>JEDI - 7B（谢等人，2025年）</td><td>42.9</td><td>11.0</td><td>50.0</td><td>11.9</td><td>38.0</td><td>14.1</td><td>72.9</td><td>25.5</td><td>75.1</td><td>47.2</td><td>33.6</td><td>16.9</td><td>39.5</td></tr><tr><td>Qwen2.5 - VL - 32B（白等人，2025年）</td><td>74.0</td><td>21.4</td><td>61.1</td><td>13.3</td><td>38.1</td><td>15.6</td><td>78.5</td><td>29.1</td><td>76.3</td><td>37.7</td><td>55.1</td><td>27.0</td><td>47.6</td></tr><tr><td>SE - GUI - 7B（袁等人，2025年）</td><td>68.2</td><td>19.3</td><td>57.6</td><td>9.1</td><td>51.3</td><td>42.2</td><td>75.0</td><td>28.2</td><td>78.5</td><td>43.4</td><td>49.5</td><td>25.8</td><td>47.3</td></tr><tr><td>GUI - G2 - 7B（唐等人，2025年）</td><td>68.8</td><td>17.2</td><td>57.1</td><td>15.4</td><td>55.8</td><td>12.5</td><td>77.1</td><td>24.5</td><td>74.0</td><td>32.7</td><td>57.9</td><td>21.3</td><td>47.5</td></tr><tr><td>Qwen2.5 - VL - 72B（白等人，2025年）</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>53.3</td></tr><tr><td>GUI - Owl - 7B</td><td>76.6</td><td>31.0</td><td>59.6</td><td>27.3</td><td>64.5</td><td>21.9</td><td>79.1</td><td>37.3</td><td>77.4</td><td>39.6</td><td>59.8</td><td>33.7</td><td>54.9</td></tr><tr><td>GUI - Owl - 32B</td><td>84.4</td><td>39.3</td><td>65.2</td><td>18.2</td><td>62.4</td><td>28.1</td><td>82.6</td><td>39.1</td><td>81.4</td><td>39.6</td><td>70.1</td><td>36.0</td><td>58.0</td></tr></tbody></table>


Table 2: Comparison with state-of-the-art methods on the ScreenSpot-Pro dataset. Underlined denotes the second-best open-source performance.
表 2：在 ScreenSpot-Pro 数据集上与最先进方法的比较。带下划线表示第二佳的开源性能。


On the MMBench-GUI-L1 benchmark, GUI-Owl scores 84.5, 86.9, and 90.9 on the easy, medium, and hard levels, respectively, substantially outperforming all existing models. On Android Control, it achieves a score of 72.8, establishing the highest performance among all 7B models. We find that GUI-Owl-32B achieves a score of 76.6, surpassing the current state-of-the-art UI-TARS-72B. GUI-Owl-32B significantly outperforms GUI-Owl-7B across different difficulty levels and domains, reflecting its more comprehensive and sufficient reserve of GUI knowledge.
在 MMBench-GUI-L1 基准上，GUI-Owl 在简单、中等和困难三个等级上分别得分 84.5、86.9 和 90.9，远超现有所有模型。在 Android Control 上，其得分为 72.8，创下所有 7B 模型中的最高性能。我们发现 GUI-Owl-32B 达到 76.6 的分数，超过了当前的最先进模型 UI-TARS-72B。GUI-Owl-32B 在不同难度等级和领域上显著优于 GUI-Owl-7B，反映出其更全面且充足的 GUI 知识储备。


<table><tr><td>Agent Model</td><td>Text Matching</td><td>Element Recog.</td><td>Layout Underst.</td><td>Fine-grained Manip.</td><td>Overall</td></tr><tr><td colspan="6">Proprietary Models</td></tr><tr><td>Gemini-2.5-Pro (Deepmind, 2025b)</td><td>59.8</td><td>45.5</td><td>49.0</td><td>33.6</td><td>45.2</td></tr><tr><td>Operator (OpenAI, 2025a)</td><td>51.3</td><td>42.4</td><td>46.6</td><td>31.5</td><td>40.6</td></tr><tr><td>Seed1.5-VL (Team, 2025)</td><td>73.9</td><td>66.7</td><td>69.6</td><td>47.0</td><td>62.9</td></tr><tr><td colspan="6">Open-Source Models</td></tr><tr><td>OS-Atlas-7B (Wu et al., 2024b)</td><td>44.1</td><td>29.4</td><td>35.2</td><td>16.8</td><td>27.7</td></tr><tr><td>UGround-V1-7B (Gou et al., 2024)</td><td>51.3</td><td>40.3</td><td>43.5</td><td>24.8</td><td>36.4</td></tr><tr><td>Aguvis-7B (Xu et al., 2024)</td><td>55.9</td><td>41.2</td><td>43.9</td><td>28.2</td><td>38.7</td></tr><tr><td>UI-TARS-7B (Qin et al., 2025)</td><td>60.2</td><td>51.8</td><td>54.9</td><td>35.6</td><td>47.5</td></tr><tr><td>UI-TARS-72B (Qin et al., 2025)</td><td>69.4</td><td>60.6</td><td>62.9</td><td>45.6</td><td>57.1</td></tr><tr><td>Qwen2.5-VL-3B (Bai et al., 2025)</td><td>41.4</td><td>28.8</td><td>34.8</td><td>13.4</td><td>27.3</td></tr><tr><td>Qwen2.5-VL-7B (Bai et al., 2025)</td><td>45.6</td><td>32.7</td><td>41.9</td><td>18.1</td><td>31.4</td></tr><tr><td>Qwen2.5-VL-32B (Bai et al., 2025)</td><td>63.2</td><td>47.3</td><td>49.0</td><td>36.9</td><td>46.5</td></tr><tr><td>JEDI-3B (Xie et al., 2025)</td><td>67.4</td><td>53.0</td><td>53.8</td><td>44.3</td><td>50.9</td></tr><tr><td>JEDI-7B (Xie et al., 2025)</td><td>65.9</td><td>55.5</td><td>57.7</td><td>46.9</td><td>54.1</td></tr><tr><td>GUI-Owl-7B</td><td>64.8</td><td>63.6</td><td>61.3</td><td>41.0</td><td>55.9</td></tr><tr><td>GUI-Owl-32B</td><td>67.0</td><td>64.5</td><td>67.2</td><td>45.6</td><td>58.0</td></tr></table>
<table><tbody><tr><td>代理模型</td><td>文本匹配</td><td>元素识别</td><td>布局理解</td><td>细粒度操作</td><td>总体</td></tr><tr><td colspan="6">专有模型</td></tr><tr><td>Gemini - 2.5 - Pro（Deepmind，2025b）</td><td>59.8</td><td>45.5</td><td>49.0</td><td>33.6</td><td>45.2</td></tr><tr><td>Operator（OpenAI，2025a）</td><td>51.3</td><td>42.4</td><td>46.6</td><td>31.5</td><td>40.6</td></tr><tr><td>Seed1.5 - VL（团队，2025）</td><td>73.9</td><td>66.7</td><td>69.6</td><td>47.0</td><td>62.9</td></tr><tr><td colspan="6">开源模型</td></tr><tr><td>OS - Atlas - 7B（Wu 等人，2024b）</td><td>44.1</td><td>29.4</td><td>35.2</td><td>16.8</td><td>27.7</td></tr><tr><td>UGround - V1 - 7B（Gou 等人，2024）</td><td>51.3</td><td>40.3</td><td>43.5</td><td>24.8</td><td>36.4</td></tr><tr><td>Aguvis - 7B（Xu 等人，2024）</td><td>55.9</td><td>41.2</td><td>43.9</td><td>28.2</td><td>38.7</td></tr><tr><td>UI - TARS - 7B（Qin 等人，2025）</td><td>60.2</td><td>51.8</td><td>54.9</td><td>35.6</td><td>47.5</td></tr><tr><td>UI - TARS - 72B（Qin 等人，2025）</td><td>69.4</td><td>60.6</td><td>62.9</td><td>45.6</td><td>57.1</td></tr><tr><td>Qwen2.5 - VL - 3B（Bai 等人，2025）</td><td>41.4</td><td>28.8</td><td>34.8</td><td>13.4</td><td>27.3</td></tr><tr><td>Qwen2.5 - VL - 7B（Bai 等人，2025）</td><td>45.6</td><td>32.7</td><td>41.9</td><td>18.1</td><td>31.4</td></tr><tr><td>Qwen2.5 - VL - 32B（Bai 等人，2025）</td><td>63.2</td><td>47.3</td><td>49.0</td><td>36.9</td><td>46.5</td></tr><tr><td>JEDI - 3B（Xie 等人，2025）</td><td>67.4</td><td>53.0</td><td>53.8</td><td>44.3</td><td>50.9</td></tr><tr><td>JEDI - 7B（Xie 等人，2025）</td><td>65.9</td><td>55.5</td><td>57.7</td><td>46.9</td><td>54.1</td></tr><tr><td>GUI - Owl - 7B</td><td>64.8</td><td>63.6</td><td>61.3</td><td>41.0</td><td>55.9</td></tr><tr><td>GUI - Owl - 32B</td><td>67.0</td><td>64.5</td><td>67.2</td><td>45.6</td><td>58.0</td></tr></tbody></table>


Table 3: Comparison with state-of-the-art methods on the OSWorld-G dataset. Underlined denotes the second-best open-source performance.
表 3：与 OSWorld - G 数据集上的最先进方法的比较。带下划线的表示次优的开源性能。


<table><tr><td rowspan="2">Model</td><td colspan="2">Windows</td><td colspan="2">MacOS</td><td colspan="2">Linux</td><td colspan="2">iOS</td><td colspan="2">Android</td><td colspan="2">Web</td><td rowspan="2">Overall</td></tr><tr><td>Basic</td><td>Adv.</td><td>Basic</td><td>Adv.</td><td>Basic</td><td>Adv.</td><td>Basic</td><td>Adv.</td><td>Basic</td><td>Adv.</td><td>Basic</td><td>Adv.</td></tr><tr><td>GPT-40 (Hurst et al., 2024)</td><td>1.48</td><td>1.10</td><td>8.69</td><td>4.34</td><td>1.05</td><td>1.02</td><td>5.10</td><td>3.33</td><td>2.53</td><td>1.41</td><td>3.23</td><td>2.92</td><td>2.87</td></tr><tr><td>Claude-3.7 (Anthropic, 2025a)</td><td>1.48</td><td>0.74</td><td>12.46</td><td>7.51</td><td>1.05</td><td>0.00</td><td>13.69</td><td>10.61</td><td>1.40</td><td>1.40</td><td>3.23</td><td>2.27</td><td>4.66</td></tr><tr><td>Qwen-Max-VL (Bai et al., 2025)</td><td>43.91</td><td>36.76</td><td>58.84</td><td>56.07</td><td>53.93</td><td>30.10</td><td>77.39</td><td>59.09</td><td>79.49</td><td>70.14</td><td>74.84</td><td>58.77</td><td>58.03</td></tr><tr><td>Aguvis-7B-720P (Xu et al., 2024)</td><td>37.27</td><td>21.69</td><td>48.12</td><td>33.27</td><td>33.51</td><td>25.00</td><td>67.52</td><td>65.15</td><td>60.96</td><td>50.99</td><td>61.61</td><td>45.45</td><td>45.66</td></tr><tr><td>ShowUI-2B (Lin et al., 2025)</td><td>9.23</td><td>4.41</td><td>24.06</td><td>10.40</td><td>25.13</td><td>11.73</td><td>28.98</td><td>19.70</td><td>17.42</td><td>8.73</td><td>22.90</td><td>12.66</td><td>15.96</td></tr><tr><td>OS-Atlas-Base-7B (Wu et al., 2024b)</td><td>36.90</td><td>18.75</td><td>44.35</td><td>21.68</td><td>31.41</td><td>13.27</td><td>74.84</td><td>48.79</td><td>69.60</td><td>46.76</td><td>61.29</td><td>35.39</td><td>41.42</td></tr><tr><td>UGround-V1-7B (Gou et al., 2024)</td><td>66.79</td><td>38.97</td><td>71.30</td><td>48.55</td><td>56.54</td><td>31.12</td><td>92.68</td><td>70.91</td><td>93.54</td><td>70.99</td><td>88.71</td><td>64.61</td><td>65.68</td></tr><tr><td>InternVL3-72B (Zhu et al., 2025)</td><td>70.11</td><td>42.64</td><td>75.65</td><td>52.31</td><td>59.16</td><td>41.33</td><td>93.63</td><td>80.61</td><td>92.70</td><td>78.59</td><td>90.65</td><td>65.91</td><td>72.20</td></tr><tr><td>Qwen2.5-VL-72B (Bai et al., 2025)</td><td>55.72</td><td>33.82</td><td>49.86</td><td>30.06</td><td>40.31</td><td>20.92</td><td>56.05</td><td>28.18</td><td>55.62</td><td>25.35</td><td>68.39</td><td>45.78</td><td>41.83</td></tr><tr><td>Qwen2.5-VL-7B (Bai et al., 2025)</td><td>31.37</td><td>16.54</td><td>31.30</td><td>21.97</td><td>21.47</td><td>12.24</td><td>66.56</td><td>55.15</td><td>35.11</td><td>35.21</td><td>40.32</td><td>32.47</td><td>33.85</td></tr><tr><td>UI-TARS-1.5-7B (Qin et al., 2025)</td><td>68.27</td><td>38.97</td><td>68.99</td><td>44.51</td><td>64.40</td><td>37.76</td><td>88.54</td><td>69.39</td><td>90.45</td><td>69.29</td><td>80.97</td><td>56.49</td><td>64.32</td></tr><tr><td>UI-TARS-72B-DPO (Qin et al., 2025)</td><td>78.60</td><td>51.84</td><td>80.29</td><td>62.72</td><td>68.59</td><td>51.53</td><td>90.76</td><td>81.21</td><td>92.98</td><td>80.00</td><td>88.06</td><td>68.51</td><td>74.25</td></tr><tr><td>GUI-Owl-7B</td><td>86.35</td><td>61.76</td><td>81.74</td><td>64.45</td><td>74.35</td><td>61.73</td><td>94.90</td><td>83.03</td><td>95.78</td><td>83.66</td><td>93.22</td><td>72.72</td><td>80.49</td></tr><tr><td>GUI-Owl-32B</td><td>85.61</td><td>65.07</td><td>84.93</td><td>67.05</td><td>76.96</td><td>63.27</td><td>95.22</td><td>85.45</td><td>96.07</td><td>87.04</td><td>95.48</td><td>80.84</td><td>82.97</td></tr></table>
<table><tbody><tr><td rowspan="2">模型</td><td colspan="2">Windows</td><td colspan="2">macOS</td><td colspan="2">Linux</td><td colspan="2">iOS</td><td colspan="2">安卓</td><td colspan="2">网页端</td><td rowspan="2">总体</td></tr><tr><td>基础版</td><td>高级版</td><td>基础版</td><td>高级版</td><td>基础版</td><td>高级版</td><td>基础版</td><td>高级版</td><td>基础版</td><td>高级版</td><td>基础版</td><td>高级版</td></tr><tr><td>GPT - 40（赫斯特等人，2024年）</td><td>1.48</td><td>1.10</td><td>8.69</td><td>4.34</td><td>1.05</td><td>1.02</td><td>5.10</td><td>3.33</td><td>2.53</td><td>1.41</td><td>3.23</td><td>2.92</td><td>2.87</td></tr><tr><td>Claude - 3.7（Anthropic公司，2025a）</td><td>1.48</td><td>0.74</td><td>12.46</td><td>7.51</td><td>1.05</td><td>0.00</td><td>13.69</td><td>10.61</td><td>1.40</td><td>1.40</td><td>3.23</td><td>2.27</td><td>4.66</td></tr><tr><td>通义千问 - Max - VL（白等人，2025年）</td><td>43.91</td><td>36.76</td><td>58.84</td><td>56.07</td><td>53.93</td><td>30.10</td><td>77.39</td><td>59.09</td><td>79.49</td><td>70.14</td><td>74.84</td><td>58.77</td><td>58.03</td></tr><tr><td>阿古维斯 - 7B - 720P（徐等人，2024年）</td><td>37.27</td><td>21.69</td><td>48.12</td><td>33.27</td><td>33.51</td><td>25.00</td><td>67.52</td><td>65.15</td><td>60.96</td><td>50.99</td><td>61.61</td><td>45.45</td><td>45.66</td></tr><tr><td>ShowUI - 2B（林等人，2025年）</td><td>9.23</td><td>4.41</td><td>24.06</td><td>10.40</td><td>25.13</td><td>11.73</td><td>28.98</td><td>19.70</td><td>17.42</td><td>8.73</td><td>22.90</td><td>12.66</td><td>15.96</td></tr><tr><td>OS - Atlas - Base - 7B（吴等人，2024b）</td><td>36.90</td><td>18.75</td><td>44.35</td><td>21.68</td><td>31.41</td><td>13.27</td><td>74.84</td><td>48.79</td><td>69.60</td><td>46.76</td><td>61.29</td><td>35.39</td><td>41.42</td></tr><tr><td>UGround - V1 - 7B（苟等人，2024年）</td><td>66.79</td><td>38.97</td><td>71.30</td><td>48.55</td><td>56.54</td><td>31.12</td><td>92.68</td><td>70.91</td><td>93.54</td><td>70.99</td><td>88.71</td><td>64.61</td><td>65.68</td></tr><tr><td>InternVL3 - 72B（朱等人，2025年）</td><td>70.11</td><td>42.64</td><td>75.65</td><td>52.31</td><td>59.16</td><td>41.33</td><td>93.63</td><td>80.61</td><td>92.70</td><td>78.59</td><td>90.65</td><td>65.91</td><td>72.20</td></tr><tr><td>通义千问2.5 - VL - 72B（白等人，2025年）</td><td>55.72</td><td>33.82</td><td>49.86</td><td>30.06</td><td>40.31</td><td>20.92</td><td>56.05</td><td>28.18</td><td>55.62</td><td>25.35</td><td>68.39</td><td>45.78</td><td>41.83</td></tr><tr><td>通义千问2.5 - VL - 7B（白等人，2025年）</td><td>31.37</td><td>16.54</td><td>31.30</td><td>21.97</td><td>21.47</td><td>12.24</td><td>66.56</td><td>55.15</td><td>35.11</td><td>35.21</td><td>40.32</td><td>32.47</td><td>33.85</td></tr><tr><td>UI - TARS - 1.5 - 7B（秦等人，2025年）</td><td>68.27</td><td>38.97</td><td>68.99</td><td>44.51</td><td>64.40</td><td>37.76</td><td>88.54</td><td>69.39</td><td>90.45</td><td>69.29</td><td>80.97</td><td>56.49</td><td>64.32</td></tr><tr><td>UI - TARS - 72B - DPO（秦等人，2025年）</td><td>78.60</td><td>51.84</td><td>80.29</td><td>62.72</td><td>68.59</td><td>51.53</td><td>90.76</td><td>81.21</td><td>92.98</td><td>80.00</td><td>88.06</td><td>68.51</td><td>74.25</td></tr><tr><td>GUI - 猫头鹰 - 7B</td><td>86.35</td><td>61.76</td><td>81.74</td><td>64.45</td><td>74.35</td><td>61.73</td><td>94.90</td><td>83.03</td><td>95.78</td><td>83.66</td><td>93.22</td><td>72.72</td><td>80.49</td></tr><tr><td>GUI - 猫头鹰 - 32B</td><td>85.61</td><td>65.07</td><td>84.93</td><td>67.05</td><td>76.96</td><td>63.27</td><td>95.22</td><td>85.45</td><td>96.07</td><td>87.04</td><td>95.48</td><td>80.84</td><td>82.97</td></tr></tbody></table>


Table 4: Comparison with state-of-the-art methods on the MMBench-GUI-L2 dataset. Underlined denotes the second-best open-source performance.
表 4：在 MMBench - GUI - L2 数据集上与最先进方法的比较。下划线表示次优的开源性能。


#### 5.1.3 End2end and Multi-Agent capability on Online environment
#### 5.1.3 在线环境下的端到端和多智能体能力


While the aforementioned evaluations measure a model's performance in single-step decision-making, they suffer from two main limitations: (1) Errors in individual steps do not accumulate, making it impossible to assess the ability to accomplish complete tasks; (2) Although there may be multiple valid ways to complete a task, the ground-truth step sequences may reflect specific preferences, which can result in an unfair evaluation across models. To more comprehensively evaluate both the end-to-end agent capability and the multi-agent capability, we adopt realistic interactive environments - AndroidWorld and OSWorld-Verified.
虽然上述评估衡量了模型在单步决策中的性能，但它们存在两个主要局限性：（1）单个步骤中的错误不会累积，因此无法评估完成完整任务的能力；（2）尽管完成一项任务可能有多种有效方法，但真实步骤序列可能反映了特定偏好，这可能导致对各模型的评估不公平。为了更全面地评估端到端智能体能力和多智能体能力，我们采用了现实的交互环境——AndroidWorld 和 OSWorld - Verified。


GUI-Owl-7B outperforms UITARS 1.5 on AndroidWorld, and Mobile-Agent-v3 achieves an even greater lead, significantly surpassing all existing models. On OSWorld-Verified, GUI-Owl also outperforms the open-source OpenCUA-7B. We further adopt GUI-Owl-32B into Mobile-Agent-v3, it achieves 37.7 on OSWorld-Verified and 73.3 on AndroidWorld. This suggests that GUI-Owl is not only capable of independently solving tasks, but is also well-suited for integration into a multi-agent framework.
GUI - Owl - 7B 在 AndroidWorld 上的表现优于 UITARS 1.5，而 Mobile - Agent - v3 的领先优势更大，显著超越了所有现有模型。在 OSWorld - Verified 上，GUI - Owl 也优于开源的 OpenCUA - 7B。我们进一步将 GUI - Owl - 32B 集成到 Mobile - Agent - v3 中，它在 OSWorld - Verified 上的得分达到 37.7，在 AndroidWorld 上达到 73.3。这表明 GUI - Owl 不仅能够独立解决任务，还非常适合集成到多智能体框架中。


### 5.2 TRAJECTORY-LEVEL ONLINE REINFORCEMENT LEARNING
### 5.2 轨迹级在线强化学习


To validate the efficacy of our trajectory-level online reinforcement learning strategy, we conducted a series of experiments on the OSWorld-Verified (Xie et al., 2024) benchmark, with all tasks limited to a maximum of 15 steps. The results, illustrated in Figure 8, demonstrate the clear advantages of our proposed approach. Starting from an initial checkpoint with a 27.1 success rate, our method shows consistent, stable improvement throughout training, ultimately achieving a peak success rate of over 34.9. This steady learning curve underscores the effectiveness of our trajectory-aware relative policy optimization. By calculating a single,normalized advantage estimate ${\widehat{A}}_{\tau }$ for an entire trajectory, our method successfully mitigates the severe credit assignment problem inherent in long-horizon GUI tasks and provides a coherent learning signal.
为了验证我们的轨迹级在线强化学习策略的有效性，我们在 OSWorld - Verified（Xie 等人，2024）基准测试上进行了一系列实验，所有任务最多限制为 15 步。如图 8 所示，实验结果显示了我们提出的方法的明显优势。从初始检查点的 27.1%成功率开始，我们的方法在整个训练过程中表现出持续、稳定的改进，最终达到了超过 34.9%的最高成功率。这种稳定的学习曲线凸显了我们的轨迹感知相对策略优化的有效性。通过为整个轨迹计算单个归一化优势估计 ${\widehat{A}}_{\tau }$，我们的方法成功缓解了长视野 GUI 任务中固有的严重信用分配问题，并提供了一致的学习信号。


<table><tr><td>Model</td><td>Windows</td><td>MacOS</td><td>Linux</td><td>iOS</td><td>Android</td><td>Web</td><td>Overall</td></tr><tr><td colspan="8">Easy Level</td></tr><tr><td>GPT-40 (Hurst et al., 2024)</td><td>62.47</td><td>67.89</td><td>62.38</td><td>58.52</td><td>56.41</td><td>58.51</td><td>60.16</td></tr><tr><td>Claude-3.5 (Anthropic, 2024)</td><td>41.34</td><td>50.04</td><td>41.61</td><td>42.03</td><td>38.96</td><td>41.79</td><td>41.54</td></tr><tr><td>Qwen2.5-VL-72B (Bai et al., 2025)</td><td>65.86</td><td>75.23</td><td>73.02</td><td>67.24</td><td>58.09</td><td>72.08</td><td>66.98</td></tr><tr><td>UI-TARS-72B-DPO (Qin et al., 2025)</td><td>41.59</td><td>28.52</td><td>35.16</td><td>31.08</td><td>52.25</td><td>35.33</td><td>40.18</td></tr><tr><td>InternVL3-72B (Zhu et al., 2025)</td><td>74.67</td><td>78.72</td><td>79.16</td><td>83.57</td><td>80.10</td><td>81.18</td><td>79.15</td></tr><tr><td>GUI-Owl-7B</td><td>82.96</td><td>84.52</td><td>85.57</td><td>82.61</td><td>83.28</td><td>88.13</td><td>84.50</td></tr><tr><td>GUI-Owl-32B</td><td>93.70</td><td>89.29</td><td>93.30</td><td>95.65</td><td>90.49</td><td>94.06</td><td>92.75</td></tr><tr><td colspan="8">Medium Level</td></tr><tr><td>GPT-40 (Hurst et al., 2024)</td><td>56.33</td><td>63.13</td><td>59.70</td><td>54.06</td><td>57.69</td><td>54.98</td><td>57.24</td></tr><tr><td>Claude-3.5 (Anthropic, 2025a)</td><td>39.28</td><td>47.63</td><td>45.97</td><td>44.57</td><td>42.03</td><td>34.33</td><td>41.26</td></tr><tr><td>Owen2.5-VL-72B (Bai et al., 2025)</td><td>66.29</td><td>72.73</td><td>72.63</td><td>59.27</td><td>66.24</td><td>68.24</td><td>67.45</td></tr><tr><td>UI-TARS-72B-DPO (Qin et al., 2025)</td><td>38.83</td><td>41.60</td><td>37.14</td><td>41.72</td><td>54.74</td><td>31.55</td><td>41.77</td></tr><tr><td>InternVL3-72B (Zhu et al., 2025)</td><td>71.46</td><td>78.58</td><td>79.88</td><td>78.43</td><td>81.36</td><td>78.67</td><td>77.89</td></tr><tr><td>GUI-Owl-7B</td><td>88.89</td><td>88.10</td><td>91.24</td><td>84.35</td><td>85.25</td><td>83.56</td><td>86.86</td></tr><tr><td>GUI-Owl-32B</td><td>94.07</td><td>84.52</td><td>95.88</td><td>87.83</td><td>92.79</td><td>88.58</td><td>91.74</td></tr><tr><td colspan="8">Hard Level</td></tr><tr><td>GPT-40 (Hurst et al., 2024)</td><td>60.69</td><td>60.38</td><td>52.42</td><td>45.27</td><td>50.93</td><td>50.83</td><td>53.49</td></tr><tr><td>Claude-3.5 (Anthropic, 2025a)</td><td>37.40</td><td>42.70</td><td>34.07</td><td>40.86</td><td>36.96</td><td>38.11</td><td>37.55</td></tr><tr><td>Owen2.5-VL-72B (Bai et al., 2025)</td><td>70.68</td><td>68.91</td><td>70.98</td><td>57.59</td><td>53.94</td><td>68.10</td><td>64.56</td></tr><tr><td>UI-TARS-72B-DPO (Oin et al., 2025)</td><td>31.48</td><td>35.87</td><td>24.19</td><td>36.33</td><td>58.13</td><td>19.94</td><td>35.78</td></tr><tr><td>InternVL3-72B (Zhu et al., 2025)</td><td>75.08</td><td>77.44</td><td>76.19</td><td>70.37</td><td>75.73</td><td>78.11</td><td>75.70</td></tr><tr><td>GUI-Owl-7B</td><td>87.78</td><td>96.43</td><td>94.33</td><td>87.83</td><td>88.85</td><td>94.06</td><td>90.90</td></tr><tr><td>GUI-Owl-32B</td><td>93.33</td><td>95.24</td><td>95.88</td><td>92.17</td><td>95.41</td><td>92.69</td><td>94.19</td></tr></table>
<table><tbody><tr><td>模型</td><td>Windows</td><td>macOS</td><td>Linux</td><td>iOS</td><td>安卓</td><td>网页端</td><td>总体</td></tr><tr><td colspan="8">简单难度</td></tr><tr><td>GPT - 40（赫斯特等人，2024年）</td><td>62.47</td><td>67.89</td><td>62.38</td><td>58.52</td><td>56.41</td><td>58.51</td><td>60.16</td></tr><tr><td>Claude - 3.5（安睿深，2024年）</td><td>41.34</td><td>50.04</td><td>41.61</td><td>42.03</td><td>38.96</td><td>41.79</td><td>41.54</td></tr><tr><td>通义千问2.5 - VL - 72B（白等人，2025年）</td><td>65.86</td><td>75.23</td><td>73.02</td><td>67.24</td><td>58.09</td><td>72.08</td><td>66.98</td></tr><tr><td>UI - TARS - 72B - DPO（秦等人，2025年）</td><td>41.59</td><td>28.52</td><td>35.16</td><td>31.08</td><td>52.25</td><td>35.33</td><td>40.18</td></tr><tr><td>书生·浦语InternVL3 - 72B（朱等人，2025年）</td><td>74.67</td><td>78.72</td><td>79.16</td><td>83.57</td><td>80.10</td><td>81.18</td><td>79.15</td></tr><tr><td>GUI - 猫头鹰 - 7B</td><td>82.96</td><td>84.52</td><td>85.57</td><td>82.61</td><td>83.28</td><td>88.13</td><td>84.50</td></tr><tr><td>GUI - 猫头鹰 - 32B</td><td>93.70</td><td>89.29</td><td>93.30</td><td>95.65</td><td>90.49</td><td>94.06</td><td>92.75</td></tr><tr><td colspan="8">中等难度</td></tr><tr><td>GPT - 40（赫斯特等人，2024年）</td><td>56.33</td><td>63.13</td><td>59.70</td><td>54.06</td><td>57.69</td><td>54.98</td><td>57.24</td></tr><tr><td>Claude - 3.5（安睿深，2025a）</td><td>39.28</td><td>47.63</td><td>45.97</td><td>44.57</td><td>42.03</td><td>34.33</td><td>41.26</td></tr><tr><td>通义千问2.5 - VL - 72B（白等人，2025年）</td><td>66.29</td><td>72.73</td><td>72.63</td><td>59.27</td><td>66.24</td><td>68.24</td><td>67.45</td></tr><tr><td>UI - TARS - 72B - DPO（秦等人，2025年）</td><td>38.83</td><td>41.60</td><td>37.14</td><td>41.72</td><td>54.74</td><td>31.55</td><td>41.77</td></tr><tr><td>书生·浦语InternVL3 - 72B（朱等人，2025年）</td><td>71.46</td><td>78.58</td><td>79.88</td><td>78.43</td><td>81.36</td><td>78.67</td><td>77.89</td></tr><tr><td>GUI - 猫头鹰 - 7B</td><td>88.89</td><td>88.10</td><td>91.24</td><td>84.35</td><td>85.25</td><td>83.56</td><td>86.86</td></tr><tr><td>GUI - 猫头鹰 - 32B</td><td>94.07</td><td>84.52</td><td>95.88</td><td>87.83</td><td>92.79</td><td>88.58</td><td>91.74</td></tr><tr><td colspan="8">困难难度</td></tr><tr><td>GPT - 40（赫斯特等人，2024年）</td><td>60.69</td><td>60.38</td><td>52.42</td><td>45.27</td><td>50.93</td><td>50.83</td><td>53.49</td></tr><tr><td>Claude - 3.5（安睿深，2025a）</td><td>37.40</td><td>42.70</td><td>34.07</td><td>40.86</td><td>36.96</td><td>38.11</td><td>37.55</td></tr><tr><td>通义千问2.5 - VL - 72B（白等人，2025年）</td><td>70.68</td><td>68.91</td><td>70.98</td><td>57.59</td><td>53.94</td><td>68.10</td><td>64.56</td></tr><tr><td>UI - TARS - 72B - DPO（奥因等人，2025年）</td><td>31.48</td><td>35.87</td><td>24.19</td><td>36.33</td><td>58.13</td><td>19.94</td><td>35.78</td></tr><tr><td>书生·浦语InternVL3 - 72B（朱等人，2025年）</td><td>75.08</td><td>77.44</td><td>76.19</td><td>70.37</td><td>75.73</td><td>78.11</td><td>75.70</td></tr><tr><td>GUI - 猫头鹰 - 7B</td><td>87.78</td><td>96.43</td><td>94.33</td><td>87.83</td><td>88.85</td><td>94.06</td><td>90.90</td></tr><tr><td>GUI - 猫头鹰 - 32B</td><td>93.33</td><td>95.24</td><td>95.88</td><td>92.17</td><td>95.41</td><td>92.69</td><td>94.19</td></tr></tbody></table>


Table 5: Comparison with state-of-the-art methods on the MMBench-GUI-L1 dataset. Underlined denotes the second-best open-source performance.
表5：在 MMBench-GUI-L1 数据集上与最先进方法的比较。下划线表示开源方法中的第二名表现。


<table><tr><td>Model</td><td>Score</td></tr><tr><td>Claude-3.5 (Anthropic, 2024)</td><td>12.5</td></tr><tr><td>GPT-40 (Hurst et al., 2024)</td><td>20.8</td></tr><tr><td>Gemini 2.0 (Deepmind, 2025a)</td><td>28.5</td></tr><tr><td>Qwen2-VL-72B (Wang et al., 2024c)</td><td>59.1</td></tr><tr><td>Aguvis-72B (Xu et al., 2024)</td><td>66.4</td></tr><tr><td>Qwen2.5-VL-72B (Bai et al., 2025)</td><td>67.4</td></tr><tr><td>UI-TARS-7B (Qin et al., 2025)</td><td>72.5</td></tr><tr><td>UI-TARS-72B (Qin et al., 2025)</td><td>74.7</td></tr><tr><td>GUI-Owl-7B</td><td>72.8</td></tr><tr><td>GUI-Owl-32B</td><td>76.6</td></tr></table>
<table><tbody><tr><td>模型</td><td>得分</td></tr><tr><td>Claude - 3.5（Anthropic，2024年）</td><td>12.5</td></tr><tr><td>GPT - 40（Hurst等人，2024年）</td><td>20.8</td></tr><tr><td>Gemini 2.0（Deepmind，2025a）</td><td>28.5</td></tr><tr><td>Qwen2 - VL - 72B（Wang等人，2024c）</td><td>59.1</td></tr><tr><td>Aguvis - 72B（Xu等人，2024年）</td><td>66.4</td></tr><tr><td>Qwen2.5 - VL - 72B（Bai等人，2025年）</td><td>67.4</td></tr><tr><td>UI - TARS - 7B（Qin等人，2025年）</td><td>72.5</td></tr><tr><td>UI - TARS - 72B（Qin等人，2025年）</td><td>74.7</td></tr><tr><td>GUI - Owl - 7B</td><td>72.8</td></tr><tr><td>GUI - Owl - 32B</td><td>76.6</td></tr></tbody></table>


Table 6: Model performance on the Android Control benchmark. Extract match scores with high-level instruction are reported. Underlined denotes the second-best open-source performance.
表 6：模型在 Android Control 基准上的表现。报告了带高级指令的提取匹配得分。下划线表示第二佳开源表现。


<table><tr><td rowspan="2">Agent Model</td><td colspan="2">Online</td></tr><tr><td>OSWorld-Verified</td><td>AndroidWorld</td></tr><tr><td colspan="3">Proprietary Models</td></tr><tr><td>SeedVL-1.5 (Team, 2025)</td><td>34.1</td><td>62.1</td></tr><tr><td>Claude-4-sonnet (Anthropic, 2025b)</td><td>43.9</td><td>-</td></tr><tr><td>OpenAI CUA o3 (OpenAI, 2025b)</td><td>23.0</td><td>-</td></tr><tr><td>UI-TARS-1.5 (Qin et al., 2025)</td><td>-</td><td>64.2</td></tr><tr><td colspan="3">Open-Source Models</td></tr><tr><td>UI-TARS-72B-DPO (Qin et al., 2025)</td><td>24.0</td><td>46.6</td></tr><tr><td>OpenCUA-7B (Wang et al., 2025a)</td><td>28.2</td><td>-</td></tr><tr><td>OpenCUA-32B (Wang et al., 2025a)</td><td>34.8</td><td>-</td></tr><tr><td>UI-TARS1.5-7B (Qin et al., 2025)</td><td>27.4</td><td>-</td></tr><tr><td>GUI-Owl-7B</td><td>34.9*</td><td>66.4</td></tr><tr><td>Mobile-Agent-v3</td><td>37.7</td><td>73.3</td></tr></table>
<table><tbody><tr><td rowspan="2">代理模型</td><td colspan="2">在线</td></tr><tr><td>OSWorld 验证</td><td>安卓世界</td></tr><tr><td colspan="3">专有模型</td></tr><tr><td>SeedVL - 1.5（团队，2025 年）</td><td>34.1</td><td>62.1</td></tr><tr><td>Claude - 4 - sonnet（Anthropic，2025b）</td><td>43.9</td><td>-</td></tr><tr><td>OpenAI CUA o3（OpenAI，2025b）</td><td>23.0</td><td>-</td></tr><tr><td>UI - TARS - 1.5（Qin 等人，2025 年）</td><td>-</td><td>64.2</td></tr><tr><td colspan="3">开源模型</td></tr><tr><td>UI - TARS - 72B - DPO（Qin 等人，2025 年）</td><td>24.0</td><td>46.6</td></tr><tr><td>OpenCUA - 7B（Wang 等人，2025a）</td><td>28.2</td><td>-</td></tr><tr><td>OpenCUA - 32B（Wang 等人，2025a）</td><td>34.8</td><td>-</td></tr><tr><td>UI - TARS1.5 - 7B（Qin 等人，2025 年）</td><td>27.4</td><td>-</td></tr><tr><td>GUI - Owl - 7B</td><td>34.9*</td><td>66.4</td></tr><tr><td>移动代理 v3</td><td>37.7</td><td>73.3</td></tr></tbody></table>


Table 7: Online evaluation results on OSWorld-Verified and AndroidWorld benchmarks. Underlined denotes the second-best open-source performance.
表 7：在 OSWorld-Verified 和 AndroidWorld 基准测试上的在线评估结果。下划线表示次优的开源性能。


*A variant of GUI-Owl specifically RL-tuned for a desktop environment (Section 5.2). The general version of GUI-Owl achieves a score of 29.4.
*GUI-Owl 的一个变体，专门针对桌面环境进行了强化学习调优（第 5.2 节）。GUI-Owl 的通用版本得分为 29.4。


The most critical insight comes from the ablation study, Online Filtering (DAPO). This variant, which disables our successful-trajectory replay buffer and the mechanism for carrying over unused rollouts, confirms the value of our specific design choices. While this model still shows a positive learning trend, its performance is notably more volatile and ultimately inferior, peaking at around 31.5% before declining. This instability highlights the challenge of sparse positive feedback; without the replay buffer injecting successful examples, the agent struggles to learn from the vast space of failing trajectories. The final performance gap between our full model and this ablation underscores the importance of data efficiency. By retaining and reusing all generated rollouts, our full method maximizes the utility of costly interactions, providing a richer training signal that leads to more stable and superior final performance.
最关键的见解来自消融实验，即在线过滤（DAPO）。此变体禁用了我们的成功轨迹重放缓冲区和未使用滚动的结转机制，证实了我们特定设计选择的价值。虽然该模型仍显示出正向学习趋势，但其性能明显更不稳定，最终表现较差，在达到约 31.5%的峰值后下降。这种不稳定性凸显了稀疏正向反馈的挑战；如果没有重放缓冲区注入成功示例，智能体很难从大量失败轨迹中学习。我们的完整模型与该消融实验之间的最终性能差距强调了数据效率的重要性。通过保留和重用所有生成的滚动，我们的完整方法最大限度地提高了高成本交互的效用，提供了更丰富的训练信号，从而实现更稳定和更优的最终性能。


Our comparison with the Offline Filtering (GRPO) baseline further justifies our online data selection methodology. Offline filtering is a very common technique for preparing RL data by removing tasks that are statically identified as all-successful or all-failing across multiple inference runs. However, the results show this approach is not suitable for GUI automation tasks that require long-range, multi-step planning. After an initial small gain, its performance stagnates around a 29.1 success rate before degrading significantly. The failure arises because the final reward depends on a long sequence of actions, making outcomes highly sensitive to minor policy changes during training. Such sensitivity causes abrupt, non-linear shifts in success rates, in contrast to the smoother improvement observed in single-step reasoning tasks. Relying solely on offline filtering further aggravates this issue, leading to severe overfitting. As our results confirm, a more effective solution is a dynamic online filtering strategy, which continuously adapts the training distribution to the agent's evolving policy.
我们与离线过滤（GRPO）基线的比较进一步证明了我们的在线数据选择方法的合理性。离线过滤是一种非常常见的强化学习数据预处理技术，通过去除在多次推理运行中被静态识别为全成功或全失败的任务来准备数据。然而，结果表明这种方法不适用于需要长程、多步规划的 GUI 自动化任务。在最初的小幅提升后，其性能在约 29.1%的成功率附近停滞，然后显著下降。失败的原因是最终奖励取决于一长串动作，使得结果对训练期间的微小策略变化高度敏感。这种敏感性导致成功率出现突然的非线性变化，与单步推理任务中观察到的更平滑的改进形成对比。仅依赖离线过滤会进一步加剧这个问题，导致严重的过拟合。正如我们的结果所证实的，更有效的解决方案是动态在线过滤策略，它可以根据智能体不断演变的策略持续调整训练分布。


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_18_24_f09e9c.jpg"/>



Figure 8: Training dynamics of GUI-Owl-7B on OSWorld-Verified. We limit the maximum interaction steps to 15 by default. Offline Filtering removes tasks with all-success or all-failure outcomes before applying vanilla GRPO, serving as common preprocessing. Online Filtering moves all tasks to online training and applies DAPO for selective filtering. Experience Managing activates both the replay buffer and the use of leftover rollouts after batch filling, as described in Section 3.1.2.
图 8：GUI-Owl-7B 在 OSWorld-Verified 上的训练动态。默认情况下，我们将最大交互步数限制为 15。离线过滤在应用普通 GRPO 之前去除全成功或全失败结果的任务，作为常见的预处理。在线过滤将所有任务转移到在线训练并应用 DAPO 进行选择性过滤。经验管理激活重放缓冲区和批量填充后剩余滚动的使用，如第 3.1.2 节所述。


In summary, the results validate that while trajectory-level optimization provides a solid foundation, it is our novel experience management, which combines a success-replay mechanism with maximum data utilization, that is crucial for achieving stable and efficient performance. This methodology allows GUI-Owl-7B to achieve state-of-the-art results among open-source models of the same model size. Notably, under identical experimental settings, our model also surpasses the performance of powerful proprietary models like Claude-4-Sonnet. This demonstrates that our specialized online RL fine-tuning strategy can effectively elevate strong base models, enabling them to excel in complex, long-horizon interactive tasks and rival the capabilities of significantly larger systems.
总之，结果验证了虽然轨迹级优化提供了坚实的基础，但我们新颖的经验管理方法，即结合成功重放机制和最大数据利用率，对于实现稳定和高效的性能至关重要。这种方法使 GUI-Owl-7B 在相同模型大小的开源模型中取得了最先进的结果。值得注意的是，在相同的实验设置下，我们的模型还超越了像 Claude - 4 - Sonnet 这样强大的专有模型的性能。这表明我们专门的在线强化学习微调策略可以有效地提升强大的基础模型，使其在复杂的长程交互任务中表现出色，并与更大规模的系统相媲美。


5.2.1 SCALING OF INTERACTION STEPS AND HISTORICAL IMAGES
5.2.1 交互步数和历史图像的扩展


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_18_24_eb4711.jpg"/>



Figure 9: Performance of GUI-Owl-7B on OSWorld-Verified with varying numbers of historical images and interaction-step budgets.
图 9：GUI-Owl-7B 在 OSWorld-Verified 上使用不同数量的历史图像和交互步预算的性能。


We further analyze, on OSWorld, how GUI-Owl's performance varies with the number of historical screenshots and the interaction-step budget. As shown in Figure 9, performance increases steadily as more historical images are provided. This is because the model's understanding of UI changes relies on contrasts between consecutive frames, and additional images also help the model promptly reflect on and correct persistent erroneous behaviors. We also observe that increasing the interaction-step budget improves performance, indicating that our model has a significant advantage on long-horizon tasks.
我们进一步在 OSWorld 上分析了 GUI-Owl 的性能如何随历史截图数量和交互步预算的变化而变化。如图 9 所示，随着提供更多历史图像，性能稳步提升。这是因为模型对 UI 变化的理解依赖于连续帧之间的对比，额外的图像也有助于模型及时反思和纠正持续的错误行为。我们还观察到增加交互步预算可以提高性能，这表明我们的模型在长程任务上具有显著优势。


### 5.3 Effect of Reasoning Data Synthesis
### 5.3 推理数据合成的效果


<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_18_24_a62be8.jpg"/>



Figure 10: Effect of reasoning data synthesis on Android World.
图 10：推理数据合成在 Android World 上的效果。


Our offline reasoning data synthesis primarily comes from two methods: Offline Hint-Guided Rejection Sampling and Distillation from Multi-agent Framework. We also mix in general-purpose reasoning SFT data to maintain the model's generalization. Beyond the offline data, we use online iterative sampling, continually leveraging updated models to synthesize trajectories with reasoning. We analyze these components separately in Figure 10.
我们的离线推理数据合成主要来自两种方法：离线提示引导拒绝采样和多智能体框架蒸馏。我们还混入了通用推理 SFT 数据以保持模型的泛化能力。除了离线数据，我们使用在线迭代采样，不断利用更新后的模型合成带有推理的轨迹。我们在图 10 中分别分析了这些组件。


We begin validation from an early checkpoint and use performance on AndroidWorld to assess the impact of reasoning synthesis. First, we observe that as we incrementally add data from Offline Hint-Guided Rejection Sampling, distillation from a multi-agent framework, and general-purpose reasoning SFT data, the model's performance steadily improves. Moreover, adding general reasoning data yields a modest performance gain, indicating that maintaining general reasoning capability is also important for GUI interaction reasoning.
我们从早期检查点开始验证，并使用 AndroidWorld 上的性能来评估推理合成的影响。首先，我们观察到随着我们逐步添加来自离线提示引导拒绝采样、多智能体框架蒸馏和通用推理 SFT 数据，模型的性能稳步提升。此外，添加通用推理数据带来了适度的性能提升，这表明保持通用推理能力对于 GUI 交互推理也很重要。


We also examine the gains from iterative training. Starting from the same checkpoint and iteratively training with newly updated trajectory data, we observe sustained performance improvements. It is because, as the model's reasoning ability improves, an increasing share of tasks in the training query set can be completed, thereby enriching the diversity of the training data and enabling the model to learn more robust reasoning capability.
我们还考察了迭代训练带来的收益。从相同检查点出发并用新更新的轨迹数据进行迭代训练时，我们观察到性能持续提升。这是因为随着模型推理能力的改善，训练查询集中可完成的任务比例增加，从而丰富训练数据的多样性，使模型能够学习到更鲁棒的推理能力。


### 5.4 Evaluation on Agentic Frameworks
### 5.4 在智能体框架上的评估


To evaluate the adaptability of GUI-Owl in real-world scenarios, we benchmarked its performance as the core vision model within established agentic frameworks. We integrated various VLMs into two distinct setups: the Mobile-Agent-E (Wang et al., 2025b) framework on the dynamic AndroidWorld environment, and the Agent-S2 (Agashe et al., 2025) framework on the OS World desktop environment. This tests the models' ability to generalize across both mobile and PC platforms. The models evaluated include UI-TARS-1.5, UI-TARS-72B, Qwen2.5-VL, Seed-1.5-VL, alongside our GUI-Owl-7B and GUI-Owl-32B.
为了评估 GUI-Owl 在真实场景中的适应性，我们将其作为核心视觉模型置于已有的智能体框架中进行基准测试。我们在两种不同设置中集成了多种 VLM：在动态的 AndroidWorld 环境中使用 Mobile-Agent-E (Wang et al., 2025b) 框架，在桌面 OS World 环境中使用 Agent-S2 (Agashe et al., 2025) 框架，测试模型在移动和 PC 平台上的泛化能力。评估的模型包括 UI-TARS-1.5、UI-TARS-72B、Qwen2.5-VL、Seed-1.5-VL，以及我们的 GUI-Owl-7B 和 GUI-Owl-32B。


The experimental results, presented in Table Section 5.4, show that GUI-Owl models achieve substantially higher success rates than all baselines on both mobile and desktop platforms. GUI-Owl-32B, in particular, sets the highest result with a score of 62.1 on AndroidWorld and 48.4 on OSWorld. We attribute this superior agentic adaptability primarily to GUI-Owl's enhanced instruction-following capability. Unlike baseline models that may struggle to interpret the specific directives from an agent's planner, GUI-Owl excels at grounding these commands to the correct visual elements on the screen. This leads to more precise action generation (e.g., clicks and text inputs) and critically reduces the accumulation of errors in multi-step tasks. By more reliably executing each step in a sequence, GUI-Owl ensures higher overall task success, making it a more robust and effective "brain" for GUI agents.
实验结果（见第 5.4 节表格）表明，GUI-Owl 模型在移动和桌面平台上均比所有基线模型取得显著更高的成功率。尤其是 GUI-Owl-32B，在 AndroidWorld 上取得 62.1 的最高成绩，在 OSWorld 上取得 48.4。我们将这种出色的智能体适应性主要归因于 GUI-Owl 更强的指令执行能力。与可能难以解读来自智能体规划器具体指令的基线模型不同，GUI-Owl 更善于将这些指令定位到屏幕上的正确视觉元素，从而生成更精确的操作（如点击和文本输入），并显著减少多步任务中错误的累积。通过更可靠地执行序列中的每一步，GUI-Owl 提高了整体任务成功率，使其成为 GUI 智能体更稳健、更高效的“智囊”。


<table><tr><td rowspan="2">Model</td><td colspan="2">Success Rate (%)</td></tr><tr><td>Mobile-Agent-E (Wang et al., 2025b) on AndroidWorld</td><td>Agent-S2 (Agashe et al., 2025) on a subset of OSWorld-Verified</td></tr><tr><td colspan="3">Baseline Models</td></tr><tr><td>UI-TARS-1.5 (Qin et al., 2025)</td><td>14.1</td><td>14.7</td></tr><tr><td>UI-TARS-72B (Qin et al., 2025)</td><td>14.8</td><td>19.0</td></tr><tr><td>Qwen2.5-VL-72B (Bai et al., 2025)</td><td>52.6</td><td>38.6</td></tr><tr><td>Seed-1.5-VL (Team, 2025)</td><td>56.0</td><td>39.7</td></tr><tr><td colspan="3">Our Models</td></tr><tr><td>GUI-Owl-7B</td><td>59.5</td><td>40.8</td></tr><tr><td>GUI-Owl-32B</td><td>62.1</td><td>48.4</td></tr></table>
<table><tbody><tr><td rowspan="2">模型</td><td colspan="2">成功率（%）</td></tr><tr><td>Mobile-Agent-E（Wang et al., 2025b）在 AndroidWorld 上</td><td>Agent-S2（Agashe et al., 2025）在 OSWorld-Verified 的子集上</td></tr><tr><td colspan="3">基线模型</td></tr><tr><td>UI-TARS-1.5（Qin et al., 2025）</td><td>14.1</td><td>14.7</td></tr><tr><td>UI-TARS-72B（Qin et al., 2025）</td><td>14.8</td><td>19.0</td></tr><tr><td>Qwen2.5-VL-72B（Bai et al., 2025）</td><td>52.6</td><td>38.6</td></tr><tr><td>Seed-1.5-VL（Team, 2025）</td><td>56.0</td><td>39.7</td></tr><tr><td colspan="3">我们的模型</td></tr><tr><td>GUI-Owl-7B</td><td>59.5</td><td>40.8</td></tr><tr><td>GUI-Owl-32B</td><td>62.1</td><td>48.4</td></tr></tbody></table>


Table 8: Performance comparison on agentic frameworks. We report the Success Rate (%) on both mobile (AndroidWorld) and desktop (OSWorld-Verified) environments. A representative subset of OSWorld-Verified are selected to capture core challenges while reducing computational costs. Underlined denotes the second-best performance.
表 8：代理框架性能对比。我们报告在移动端（AndroidWorld）和桌面端（OSWorld-Verified）环境下的成功率（%）。选取了 OSWorld-Verified 的代表性子集以覆盖核心挑战并降低计算成本。下划线表示第二佳表现。


<table><tr><td>Action</td><td>Definition</td></tr><tr><td>key</td><td>Perform a key event on the mobile device using adb's keyevent syntax.</td></tr><tr><td>click</td><td>Click the point on the screen with specified (x, y) coordinates.</td></tr><tr><td>long_press</td><td>Press the point on the screen with specified (x, y) coordinates for a specified number of seconds.</td></tr><tr><td>swipe</td><td>Swipe from starting point with specified $\left( {x,y}\right)$ coordinates to endpoint with specified $({x2}$ , y2) coordinates.</td></tr><tr><td>type</td><td>Input the specified text into the activated input box.</td></tr><tr><td>answer</td><td>Output the specified answer.</td></tr><tr><td>system_button</td><td>Press the specified system button: Back, Home, Menu, or Enter.</td></tr><tr><td>open</td><td>Open an application on the device specified by text.</td></tr><tr><td>wait</td><td>Wait for a specified number of seconds for changes to occur.</td></tr><tr><td>terminate</td><td>Terminate the current task and report its completion status: success or failure.</td></tr></table>
<table><tbody><tr><td>操作</td><td>定义</td></tr><tr><td>按键</td><td>使用 adb 的按键事件语法在移动设备上执行按键事件。</td></tr><tr><td>点击</td><td>点击屏幕上指定 (x, y) 坐标的点。</td></tr><tr><td>长按</td><td>在屏幕上指定 (x, y) 坐标的点长按指定的秒数。</td></tr><tr><td>滑动</td><td>从指定 $\left( {x,y}\right)$ 坐标的起点滑动到指定 ($({x2}$, y2) 坐标的终点。</td></tr><tr><td>输入</td><td>将指定的文本输入到激活的输入框中。</td></tr><tr><td>回答</td><td>输出指定的答案。</td></tr><tr><td>系统按钮</td><td>按下指定的系统按钮：返回、主页、菜单或回车键。</td></tr><tr><td>打开</td><td>打开设备上由文本指定的应用程序。</td></tr><tr><td>等待</td><td>等待指定的秒数以等待变化发生。</td></tr><tr><td>终止</td><td>终止当前任务并报告其完成状态：成功或失败。</td></tr></tbody></table>


Table 9: Action Space of GUI-Owl on Mobile.
表 9：移动端 GUI-Owl 的动作空间。


## 6 Details of Self-Evolving Trajectory Data Production
## 6 自进化轨迹数据生成详情


In this section, we present the details of our self-evolving trajectory data production pipeline.
在本节中，我们将详细介绍我们的自进化轨迹数据生成流程。


### 6.1 Overview
### 6.1 概述


GUI automation tasks operate in online interactive environments, which renders manual annotation of trajectory data exceedingly tedious and costly, posing significant challenges for GUI trajectory data collection. To address these challenges, we develop a self-evolving GUI trajectory data production pipeline. This approach leverages the capabilities of GUI-Owl itself, continuously generating new trajectories through rollout and assessing their correctness to obtain high-quality training data. Subsequently, these data are utilized to enhance the model's capabilities, creating a reinforcing cycle of improvement.
GUI 自动化任务在在线交互环境中运行，这使得手动标注轨迹数据极为繁琐且成本高昂，给 GUI 轨迹数据收集带来了巨大挑战。为应对这些挑战，我们开发了一个自进化的 GUI 轨迹数据生成流程。该方法利用 GUI-Owl 自身的能力，通过滚动生成不断产生新的轨迹，并评估其正确性以获取高质量的训练数据。随后，利用这些数据提升模型的能力，形成一个不断改进的强化循环。


Our pipeline, illustrated in Figure 4, operates through the following stages: (1) The process initiates with the construction of online virtual environments encompassing mobile, PC, and web platforms, alongside the generation of diverse queries covering a wide range of potential GUI scenarios; (2) Given these queries, the GUI-Owl model predicts actions step-by-step, which are then executed within the online virtual environments, yielding rollout trajectories; (3) A Trajectory Correctness Judgement module, incorporating a multimodal critic framework, evaluates the correctness of all roll-out trajectories. Successful trajectories are collected to create a rich dataset of interaction sequences that capture temporal dependencies and diverse GUI states; (4) For challenging queries where the GUI-Owl model struggles to produce successful trajectories despite numerous attempts, we introduce a Query-specific Guidance Generation module. This module synthesizes step-level guidance based on ground-truth trajectories produced by human annotation or other models, facilitating GUI-Owl's handling of difficult tasks and enhancing the efficiency of the entire data generation pipeline; (5) Finally, all processed data is compiled for reinforcement fine-tuning of GUI-Owl. The model undergoes continuous updates, creating a feedback loop where its ability to generate effective roll-out trajectories improves over time, progressively reducing reliance on manual data collection and achieving self-evolution.
如图 4 所示，我们的流程通过以下阶段运行：(1) 该过程首先构建包含移动、PC 和 Web 平台的在线虚拟环境，并生成涵盖各种潜在 GUI 场景的多样化查询；(2) 基于这些查询，GUI-Owl 模型逐步预测动作，然后在在线虚拟环境中执行这些动作，产生滚动轨迹；(3) 一个包含多模态评判框架的轨迹正确性判断模块评估所有滚动轨迹的正确性。收集成功的轨迹，以创建一个丰富的交互序列数据集，该数据集捕捉时间依赖关系和多样化的 GUI 状态；(4) 对于 GUI-Owl 模型尽管多次尝试仍难以产生成功轨迹的具有挑战性的查询，我们引入了一个特定查询指导生成模块。该模块根据人工标注或其他模型产生的真实轨迹合成步骤级指导，帮助 GUI-Owl 处理困难任务，提高整个数据生成流程的效率；(5) 最后，将所有处理后的数据进行汇总，用于对 GUI-Owl 进行强化微调。模型不断更新，形成一个反馈循环，随着时间的推移，其生成有效滚动轨迹的能力不断提高，逐步减少对手动数据收集的依赖，实现自进化。


<table><tr><td>Action</td><td>Definition</td></tr><tr><td>key</td><td>Performs key down presses on the arguments passed in order, then performs key releases in reverse order.</td></tr><tr><td>type</td><td>Input a string of text. Use the clear parameter to decide whether to overwrite the existing text, and use the enter parameter to decide whether the enter key should be pressed after typing the text.</td></tr><tr><td>mouse_move</td><td>Move the cursor to a specified $\left( {x,y}\right)$ pixel coordinate on the screen.</td></tr><tr><td>click</td><td>Click the left mouse button at a specified $\left( {x,y}\right)$ pixel coordinate on the screen.</td></tr><tr><td>drag</td><td>Click at a specified $\left( {x,y}\right)$ pixel coordinate on the screen,and drag the cursor to another specified (x2, y2) pixel coordinate on the screen.</td></tr><tr><td>right_click</td><td>Click the right mouse button at a specified $\left( {x,y}\right)$ pixel coordinate on the screen.</td></tr><tr><td>middle_click</td><td>Click the middle mouse button at a specified $\left( {x,y}\right)$ pixel coordinate on the screen.</td></tr><tr><td>double_click</td><td>Double-click the left mouse button at a specified $\left( {x,y}\right)$ pixel coordinate on the screen.</td></tr><tr><td>scroll</td><td>Performs a scroll of the mouse scroll wheel.</td></tr><tr><td>wait</td><td>Wait for a specified number of seconds for changes to occur.</td></tr><tr><td>terminate</td><td>Terminate the current task and report its completion status: success or failure.</td></tr></table>
<table><tbody><tr><td>操作</td><td>定义</td></tr><tr><td>按键</td><td>按顺序对传入的参数执行按键按下，然后按相反顺序释放按键。</td></tr><tr><td>输入</td><td>输入一串文本。使用 clear 参数决定是否覆盖现有文本，使用 enter 参数决定输入后是否按回车键。</td></tr><tr><td>鼠标移动</td><td>将光标移动到屏幕上指定的 $\left( {x,y}\right)$ 像素坐标。</td></tr><tr><td>点击</td><td>在屏幕上指定的 $\left( {x,y}\right)$ 像素坐标处左键单击。</td></tr><tr><td>拖拽</td><td>在屏幕上指定的 $\left( {x,y}\right)$ 像素坐标处按下并将光标拖动到另一指定的 (x2, y2) 像素坐标。</td></tr><tr><td>右键点击</td><td>在屏幕上指定的 $\left( {x,y}\right)$ 像素坐标处右键单击。</td></tr><tr><td>中键点击</td><td>在屏幕上指定的 $\left( {x,y}\right)$ 像素坐标处按下鼠标中键。</td></tr><tr><td>双击</td><td>在屏幕上指定的 $\left( {x,y}\right)$ 像素坐标处左键双击。</td></tr><tr><td>滚动</td><td>执行鼠标滚轮滚动操作。</td></tr><tr><td>等待</td><td>等待指定秒数以便发生变化。</td></tr><tr><td>终止</td><td>终止当前任务并报告其完成状态：成功或失败。</td></tr></tbody></table>


Table 10: Action Space of GUI-Owl on Desktop.
表 10：GUI-Owl 在桌面端的动作空间。


This self-evolving data production pipeline effectively addresses the unique challenges of GUI automation tasks, enabling the creation of robust and versatile GUI intelligent agents capable of handling the complexities of modern graphical user interfaces while continuously improving the efficiency and quality of the data production process itself.
该自我进化的数据生产流水线有效解决了 GUI 自动化任务的独特挑战，使得能够创建强健且多用途的 GUI 智能代理，能够处理现代图形用户界面的复杂性，同时持续提升数据生产流程自身的效率与质量。


### 6.2 HIGH-QUALITY QUERY GENERATION
### 6.2 高质量查询生成


As highlighted in our overview, generating high-quality queries is a critical component of our self-evolving GUI trajectory data production pipeline. These queries need to cover a wide range of possible user intentions and tasks, reflecting the multifaceted nature of GUI interactions. In this section, we present our innovative approach to query generation for mobile and computer applications, which ensures diversity, realism, and accuracy in the produced queries.
如我们概述中所述，生成高质量查询是自我进化 GUI 轨迹数据生产流水线的核心环节。这些查询需覆盖广泛的用户意图与任务，反映 GUI 交互的多面性。本节介绍我们针对移动与电脑应用的创新查询生成方法，确保所生成查询的多样性、真实性与准确性。


Mobile. For mobile applications, we develop a screenshot-action framework that captures the essence of user interactions while maintaining controllability and extensibility. At the core of our query generation process is a human-annotated directed acyclic graph (DAG) $\mathcal{G} = \langle P,A\rangle$ for each task. Here, $\bar{P} = \left\{  {{p}_{1},\ldots ,{p}_{n}}\right\}$ represents screenshots (e.g.,home,ordering,payment),and $A \subseteq  P \times  P$ defines valid transitions between them. Each screenshot ${p}_{i}$ includes a description ${d}_{i}$ of the screenshot’s content and purpose and a set of available slot-value pairs that represent possible user choices or inputs on that screenshot. This structure allows us to model realistic navigation flows within apps and capture the multi-constraint nature of user queries.
移动端。对于移动应用，我们设计了一个截图-动作框架，该框架在保持可控性与可扩展性的同时捕捉用户交互的本质。在查询生成流程的核心是为每个任务人工标注的有向无环图 (DAG) $\mathcal{G} = \langle P,A\rangle$。其中，$\bar{P} = \left\{  {{p}_{1},\ldots ,{p}_{n}}\right\}$ 表示截图（如主页、点餐、支付），$A \subseteq  P \times  P$ 定义它们之间的有效转移。每个截图 ${p}_{i}$ 包含对截图内容与用途的描述 ${d}_{i}$ 以及一组可用的槽-值对，表示该截图上可能的用户选择或输入。此结构使我们能够对应用内的真实导航流程建模，并捕捉用户查询的多约束特性。


Specifically, our query generation process involves the following steps: (1) Path Sampling: We sample a path ${P}^{\prime } = \left\{  {{p}_{{\sigma }_{1}},\ldots ,{p}_{{\sigma }_{k}}}\right\}$ on the DAG $\mathcal{G}$ . This path represents a realistic sequence of screenshot transitions within the app. (2) Metadata Extraction: From the sampled path,we obtain screenshot descriptions ${D}^{\prime } = \left\{  {{d}_{{\sigma }_{1}},\ldots ,{d}_{{\sigma }_{k}}}\right\}$ and the corresponding slot-value pairs ${K}^{\prime },{V}^{\prime }$ . (3) Instruction Synthesis: The extracted metadata is fed to a Large Language Model (LLM) to synthesize constrained instructions. This approach ensures that the generated queries are both realistic and aligned with the app's structure. (4) Refinement: To enhance naturalness, we refine the raw DAG paths using few-shot LLM prompting. This step transforms explicit navigation instructions into more natural user queries. For example, "Open the takeout app, click on the food entry" becomes "Order me takeout". (5) Interface Validation: To maintain accuracy, we employ web crawlers to collect real-time interface data from target applications. This ensures that aligned with current app functionality. In conclusion, in our screenshot-action framework, the use of manually defined slot-value pairs minimizes LLM hallucinations, while the DAG structure ensures realistic and controllable navigation flows.
具体而言，我们的查询生成流程包括以下步骤：(1) 路径采样：在 DAG $\mathcal{G}$ 上采样一条路径 ${P}^{\prime } = \left\{  {{p}_{{\sigma }_{1}},\ldots ,{p}_{{\sigma }_{k}}}\right\}$，该路径代表应用内真实的截图转移序列。(2) 元数据提取：从采样路径中获取截图描述 ${D}^{\prime } = \left\{  {{d}_{{\sigma }_{1}},\ldots ,{d}_{{\sigma }_{k}}}\right\}$ 及相应的槽-值对 ${K}^{\prime },{V}^{\prime }$。(3) 指令合成：将提取的元数据输入大型语言模型 (LLM) 以合成受约束的指令。该方法确保生成的查询既真实又与应用结构一致。(4) 精化：为增强自然性，我们使用少量示例的 LLM 提示对原始 DAG 路径进行精化，将明确的导航指令转化为更自然的用户查询。例如，“打开外卖应用，点击食品项”变为“帮我点外卖”。(5) 界面验证：为保持准确性，我们使用网络爬虫从目标应用收集实时界面数据，以确保与当前应用功能保持一致。综上，在我们的截图-动作框架中，手工定义的槽-值对最小化了 LLM 的幻觉，而 DAG 结构保证了真实且可控的导航流程。


Computer. To acquire operational trajectories for the training of intelligent agents, the initial and most crucial step is the batch acquisition of command data. Unlike mobile phones, the computer usage domain typically involves productivity applications, such as web browsers, document editors, file explorers, and email clients. When it comes to intelligent agents, the utilization of these software tools via keyboard and mouse manipulations presents two primary challenges.
电脑端。为获取训练智能代理所需的操作轨迹，首要且最关键的步骤是批量获取命令数据。与手机不同，电脑使用领域通常涉及办公类应用，例如网页浏览器、文档编辑器、文件管理器和邮件客户端。在智能代理的应用中，通过键盘与鼠标操作这些软件工具面临两大主要挑战。


Firstly, there is the fundamental issue of atomic operational skills. Humans, after learning, can proficiently use a mouse for clicking and scrolling and a keyboard for input and shortcut execution. However, intelligent agents driven by vision-language models often lack basic knowledge of atomic operations, such as scrolling through content on web pages or selecting editing targets via dragging in office software.
其一，是原子操作技能的基本问题。人类在学习后能够熟练使用鼠标进行点击与滚动，使用键盘进行输入与快捷键操作。然而，由视觉-语言模型驱动的智能代理常常缺乏对原子操作的基本认知，例如在网页上滚动浏览内容或在办公软件中通过拖拽选择编辑目标。


Secondly, software operational pathways must be navigated, such as accessing privacy settings in Chrome or adjusting page margins in Microsoft Word. Accomplishing these objectives necessitates a series of actions, including clicks, scrolls, and inputs, to reach the requisite configuration options.
其二，是软件操作路径的导航问题，例如在 Chrome 中进入隐私设置或在 Microsoft Word 中调整页面页边距。完成这些目标需要一系列动作，包括点击、滚动与输入，以到达所需的配置选项。


Therefore, to bestow intelligent agents with computer usage capabilities, we have synthesized user instructions, targeting both atomic operational skills and software operational pathways, through a combination of manual annotation and automated generation facilitated by Large Language Models (LLMs).
因此，为赋予智能代理电脑使用能力，我们通过人工标注与大型语言模型 (LLM) 辅助的自动生成相结合，合成了针对原子操作技能与软件操作路径的用户指令。


1) Atomic Operations: For common atomic operations with the mouse and keyboard, we initially acquired operational objects within a PC environment via manual annotation. Examples include: a) Double-clicking: This involves creating software icons, folders, etc., to train the model in double-click operations. b) Input: This involves creating files in formats such as Word, Excel, and PowerPoint to train the model's capability to accurately input text at specified locations. c) Dragging: Similarly, files in Word, Excel, and PowerPoint formats are created to train the model to select specific text or move objects through dragging.
1) 原子操作：对于常见的鼠标与键盘原子操作，我们起初在 PC 环境中通过人工标注获取操作对象。示例包括：a) 双击：为训练模型的双击操作，创建软件图标、文件夹等。b) 输入：创建 Word、Excel、PowerPoint 等格式的文件，以训练模型在指定位置准确输入文本的能力。c) 拖拽：同样创建 Word、Excel、PowerPoint 格式的文件，以训练模型通过拖拽选择特定文本或移动对象的能力。


Once operational objects are obtained, we input screenshots of these objects and exemplar commands into the Vision-Language Model (VLM), leveraging its in-context learning capabilities to generate additional executable commands within the current page.
在获取操作对象后，我们将这些对象的截图与示例命令输入视觉-语言模型（VLM），利用其上下文学习能力在当前页面生成额外的可执行命令。


2) Software Operational Pathways: For common software operational pathways, we devised a set of automated deep-search chains. Utilizing an accessibility (a11y) tree, we acquire positional and functional information of actionable elements within software interfaces, and by integrating operational pathway memory and replay, we achieve a tree-structured search of actionable elements (e.g., multi-level menus) to garner corresponding operational pathways.
2) 软件操作路径：对于常见的软件操作路径，我们设计了一套自动化深度搜索链。利用无障碍（a11y）树获取软件界面中可操作元素的位置与功能信息，并结合操作路径记忆与重放，实现对可操作元素（如多级菜单）的树状搜索以获取相应的操作路径。


The endpoint settings of each operational pathway pertain to disparate objects. For example, some configurations alter global file attributes (such as image scaling), whereas others necessitate pre-selecting operational objects (such as altering the font size of a text segment).
每条操作路径的终点设置对应不同对象。例如，有些配置会改变全局文件属性（如图片缩放），而另一些则需要事先选定操作对象（如更改一段文本的字体大小）。


Therefore, to derive legally executable commands based on operational paths, we employ an LLM to ascertain whether an operational pathway requires pre-selection of an operational object. For pathways necessitating selected objects, we input manually annotated file screenshots and operational pathways into the VLM, thereby generating commands executable within the current page.
因此，为了基于操作路径生成在法律上可执行的命令，我们使用大模型判断某条操作路径是否需要预先选择操作对象。对于需要选定对象的路径，我们将人工标注的文件截图和操作路径输入 VLM，从而生成在当前页面可执行的命令。


### 6.3 TRAJECTORY CORRECTNESS JUDGMENT MODULE
### 6.3 轨迹正确性判定模块


The Trajectory Correctness Judgment Module plays a crucial role in our self-evolving GUI trajectory data production pipeline. Its primary purposes are twofold: to assess the correctness of roll-out trajectories generated by the GUI-Owl model, and to cleanse erroneous steps within otherwise correct trajectories, thereby enhancing the overall quality of our training data. This module is essential for maintaining high standards in our data collection process, ensuring that only accurate and complete trajectories are used for model training.
轨迹正确性判定模块在我们的自我演化 GUI 轨迹数据生成流水线中发挥关键作用。其主要目的有二：评估 GUI-Owl 模型生成的展开轨迹的正确性，以及在整体正确的轨迹中剔除错误步骤，从而提升训练数据的整体质量。该模块对于维持数据采集过程的高标准至关重要，确保仅使用准确且完整的轨迹进行模型训练。


Our approach to trajectory correctness judgment is comprehensive, operating at both the step level and the trajectory level. This two-tiered system allows for a nuanced evaluation of each action within a trajectory, as well as an overall assessment of the entire interaction sequence.
我们对轨迹正确性判定的做法是全面的，在步骤层面和轨迹层面同时运作。该两层系统允许对轨迹中每个动作进行细致评估，并对整个交互序列作总体判断。


Problem Definition of Trajectory Correctness Judgment. GUI automation tasks can be formalized as a Markov Decision Process: $\mathcal{M} = \left( {\mathcal{E},\mathcal{A},\mathcal{P}}\right)$ ,where $E$ represents the environment state (including user instructions, interaction history,and screenshots), $A$ is the action space,and $P$ is the transition probability.
轨迹正确性判定的问题定义。GUI 自动化任务可被形式化为马尔可夫决策过程：$\mathcal{M} = \left( {\mathcal{E},\mathcal{A},\mathcal{P}}\right)$，其中 $E$ 表示环境状态（包括用户指令、交互历史与截图），$A$ 为动作空间，$P$ 为转移概率。


The Trajectory Correctness Judgement Module consists of two interconnected components: (1) Step-Level Critic: it evaluates individual actions within a trajectory. It analyzes the pre-action state, the executed action, and the post-action state to determine the appropriateness of each step. (2) Trajectory-Level Critic: This component assesses the overall correctness of the entire trajectory. It utilizes the outputs from the Step-Level Critic along with the original user instruction to make a final judgment on the trajectory's success in accomplishing the user's goal.
轨迹正确性判定模块由两部分互联组件组成：（1）步骤级批判器：评估轨迹中的各个动作。它分析动作前状态、执行的动作与动作后状态，以判定每一步的适当性。（2）轨迹级批判器：评估整个轨迹的总体正确性。它利用步骤级批判器的输出以及原始用户指令，对轨迹是否成功实现用户目标作出最终判断。


The relationship between these two levels is hierarchical and complementary. The Step-Level Critic provides granular insights into each action, which are then synthesized by the Trajectory-Level Reflection to form a holistic evaluation of the entire interaction sequence.
这两层之间的关系是分级且互补的。步骤级批判器提供对每个动作的细粒度见解，随后由轨迹级反思综合这些见解以形成对整段交互序列的整体评估。


Step-Level Critic. Achieving a reliable Step-Level Critic presents a sophisticated challenge that demands nuanced environmental perception and comprehensive understanding. The methodology necessitates a meticulous analysis of pre- and post-action screenshots, coupled with a detailed examination of the executed operation, to accurately assess its contribution towards fulfilling the user's designated objective.
步骤级批判器。构建可靠的步骤级批判器是一个复杂的挑战，要求细致的环境感知与全面理解。该方法需要对动作前后截图进行精密分析，并对执行的操作进行详尽审查，以准确评估其对完成用户指定目标的贡献。


Initially, we annotate the critical interaction regions on the pre-action screenshot, enabling the model to focus on pivotal areas of intervention, including precise operational details—such as clicking, long-pressing, or scrolling—to facilitate a comprehensive evaluation of the action's alignment with the user's goal.
起初，我们在动作前截图上标注关键交互区域，使模型能聚焦于干预的要点，包括精确的操作细节——如点击、长按或滚动——以便全面评估该操作与用户目标的一致性。


Formally,Step-Level Critic can be conceptualized as a function ${\pi }_{\text{ critic }}^{\text{ step }}\left( {\epsilon ,a,{\epsilon }^{\prime }}\right)$ ,where $\epsilon$ represents the pre-action environmental state (encompassing user instructions, interaction history, and the initial screenshot), a denotes the executed operation,and ${\epsilon }^{\prime }$ encapsulates the post-action environmental state. The function generates three critical outputs:
形式上，步骤级批判器可被表征为函数 ${\pi }_{\text{ critic }}^{\text{ step }}\left( {\epsilon ,a,{\epsilon }^{\prime }}\right)$，其中 $\epsilon$ 代表动作前的环境状态（包含用户指令、交互历史与初始截图），a 表示已执行的操作，${\epsilon }^{\prime }$ 封装动作后的环境状态。该函数生成三项关键输出：


- An analysis $a \in  \mathcal{A}$ that provides a detailed interpretation of the action’s context and consequences
- 一份分析 $a \in  \mathcal{A}$，提供对该动作背景与后果的详细解读


- A summary $s \in  \mathcal{S}$ that concisely captures the key insights of the action (typically within 30 words)
- 一个简明的摘要 $s \in  \mathcal{S}$，简洁捕捉该操作的关键洞见（通常不超过30字）


- An annotation $l \in  \{ {GOOD},{NEUTRAL},{HARMFUL}\}$ that categorizes the action’s effectiveness towards the user's objective
- 一个注释 $l \in  \{ {GOOD},{NEUTRAL},{HARMFUL}\}$，对该操作朝向用户目标的有效性进行分类


This detailed evaluation at the step level is crucial for identifying and potentially correcting erroneous actions within trajectories, thus improving the overall quality of our training data.
该步骤级别的详细评估对于识别并可能纠正轨迹中的错误操作至关重要，从而提升我们训练数据的整体质量。


Trajectory-Level Critic. The Trajectory-Level Critic, ${\pi }_{\text{ critic }}^{\text{ traj }}\left( {I,T,{\pi }_{\text{ critic }}^{\text{ step }}}\right)$ ,where $I$ represents the user instruction and $T$ represents the action trajectory,provides a comprehensive evaluation of the entire trajectory. It employs a two-channel approach: (1) Textual Reasoning Channel $\left( {\pi }_{\text{ text }}\right)$ : Utilizes large language models to assess trajectory correctness based on screenshot caption, textual summaries of each step. (2) Multi-Modal Reasoning Channel ( ${\pi }_{\text{ multimodal }}$ ): Incorporates both visual screenshots and textual summaries for a more comprehensive evaluation. The textual channel provides concise semantic reasoning, while the multi-modal channel enriches the analysis with visual context, the combination of them helps to mitigate potential biases and limitations inherent in single-modal evaluation.
轨迹级评审。轨迹级评审 ${\pi }_{\text{ critic }}^{\text{ traj }}\left( {I,T,{\pi }_{\text{ critic }}^{\text{ step }}}\right)$，其中 $I$ 表示用户指令，$T$ 表示操作轨迹，对整个轨迹提供全面评估。它采用双通道方法：（1）文本推理通道 $\left( {\pi }_{\text{ text }}\right)$：利用大语言模型基于截图说明和每步的文本摘要评估轨迹正确性。（2）多模态推理通道（${\pi }_{\text{ multimodal }}$）：结合视觉截图与文本摘要进行更全面的评估。文本通道提供简洁的语义推理，而多模态通道通过视觉上下文丰富分析，二者结合有助于缓解单模态评估固有的偏差与局限。


The final GUI trajectory correctness is determined by a consensus mechanism:
最终 GUI 轨迹的正确性由共识机制决定：


$$
\text{ Trajectory Correctness } = \left\{  \begin{array}{ll} \text{ Correct, } & \text{ if }{\pi }_{\text{ text }}\left( {T,I}\right)  = \text{ Correct } \land  {\pi }_{\text{ multimodal }}\left( {T,I}\right)  = \text{ Correct } \\  \text{ Incorrect, } & \text{ otherwise } \end{array}\right. \tag{4}
$$



In conclusion, this multi-channel approach enhances robustness, processes complementary information, and ensures rigorous validation of trajectories.
总之，这种多通道方法增强了鲁棒性，处理互补信息，并确保对轨迹的严格验证。


### 6.4 Query-specific Guidance Generation
### 6.4 针对查询的指导生成


In our constructed query set, some queries pose significant challenges for the model, potentially requiring numerous rollouts to obtain a successful trajectory. Some other queries are even more insurmountable and necessitate manual annotation for reference operational trajectories. To acquire more diverse training data, we devise a Query-specific Guidance Generation module, which leverages existing successful trajectories to generate guidance that assists the model in producing more successful trajectories.
在我们构建的查询集中，部分查询对模型构成重大挑战，可能需要大量 rollouts 才能获得成功轨迹；另一些查询则更为难以攻克，需要人工标注参考操作轨迹。为获取更多多样化训练数据，我们设计了针对查询的指导生成模块，利用现有成功轨迹生成指导，帮助模型产出更多成功轨迹。


Initially, for the obtained reference trajectories, we employ a VLM to generate descriptions of the outcomes of each action. Specifically, the input consists of screenshots of the screen before and after the action execution, coupled with the model's or human's action decisions. The VLM is prompted to observe and describe the result of the current action execution, such as "clicked and activated the search box" or "entered the number 100". When actions involve coordinates, we annotate the interaction locations with circles on the pre-action screenshots to help the VLM focus on detailed screen changes.
最初，对于已获得的参考轨迹，我们使用 VLM 为每个操作的结果生成描述。具体输入包括操作前后屏幕截图，以及模型或人工的操作决策。提示 VLM 观察并描述当前操作执行的结果，例如“点击并激活了搜索框”或“输入了数字100”。当操作涉及坐标时，我们在操作前截图上用圆圈标注交互位置，帮助 VLM 聚焦于屏幕的细微变化。


Regarding the reference trajectories obtained from model rollouts, given the considerable difficulty of the queries, errors or ineffective operations are inevitable. Thus, the VLM also refers to the model's decision rationale, determining whether the outcomes of each step align with the model's expectations. Operations that do not meet expectations or fail to elicit effective responses are subsequently filtered out during the guidance synthesis process.
对于从模型 rollouts 获得的参考轨迹，鉴于查询难度较大，错误或无效操作难以避免。因此，VLM 还参考模型的决策理由，判断每步结果是否符合模型预期。不符合预期或未产生有效反应的操作将在指导合成过程中被过滤掉。


After acquiring descriptions for each step of the action execution results, we concatenate the descriptions for all steps within the trajectory. Utilizing a LLM, we summarize the essential steps required to complete the query, thereby yielding query-specific guidance.
在获取每步操作结果的描述后，我们将轨迹中所有步骤的描述串联起来。利用 LLM，总结完成该查询所需的关键步骤，从而生成针对该查询的具体指导。


### 6.5 EXAMPLES OF TRAINING DATA
### 6.5 训练数据示例


We show the format of end-to-end training data on a desktop platform in Figure 11.
我们在图11中展示了桌面平台端到端训练数据的格式。


## 7 Details of Mobile-Agent-v3
## 7 Mobile-Agent-v3 细节


### 7.1 Core Components and Formalism
### 7.1 核心组件与形式化


The operational dynamics of the Mobile-Agent-v3 framework are defined by a set of state variables and the specialized functions of its constituent agents. We formalize these components as follows.
Mobile-Agent-v3 框架的运行动力学由一组状态变量及其组成代理的专用函数决定。我们将这些组件形式化如下。


#### 7.1.1 State Variables and Definitions
#### 7.1.1 状态变量与定义


Let the entire process be a sequence of operations indexed by timestep $t \in  \{ 0,1,\ldots ,T\}$ .
令整个过程为按时间步 $t \in  \{ 0,1,\ldots ,T\}$ 索引的操作序列。


- Device State $\left( {S}_{t}\right)$ : The state of the GUI device at timestep $t$ ,represented as a high-dimensional tensor ${S}_{t} \in  \mathcal{S} \subseteq  {\mathbb{R}}^{H \times  W \times  C}$ ,where $H,W,C$ are the height,width,and channel dimensions of the screen capture, respectively. ${S}_{0}$ denotes the initial state.
- 设备状态 $\left( {S}_{t}\right)$ ：在时间步 $t$ 的 GUI 设备状态，表示为高维张量 ${S}_{t} \in  \mathcal{S} \subseteq  {\mathbb{R}}^{H \times  W \times  C}$，其中 $H,W,C$ 分别为屏幕截图的高度、宽度和通道维度。${S}_{0}$ 表示初始状态。


- Subsequent Subgoals $\left( {S{S}_{t}}\right)$ : An ordered list of pending subgoals formulated by the Manager Agent. It is defined as $C{S}_{t} = \left( {{g}_{1},{g}_{2},\ldots ,{g}_{k}}\right)$ ,where each ${g}_{i}$ is a natural language string describing a discrete step towards the main goal.
- 后续子目标 $\left( {S{S}_{t}}\right)$ ：由管理代理制定的有序待办子目标列表。定义为 $C{S}_{t} = \left( {{g}_{1},{g}_{2},\ldots ,{g}_{k}}\right)$，其中每个 ${g}_{i}$ 为描述朝主目标迈进的离散步骤的自然语言字符串。


- Completed Subgoals $\left( {C{S}_{t}}\right)$ : A set containing subgoals that have been successfully executed and verified. It is defined as $S{S}_{t} = \left\{  {{\bar{g}}_{1},{\bar{g}}_{2},\ldots ,{\bar{g}}_{m}}\right\}$ . This prevents redundant operations and tracks progress.
- 已完成子目标 $\left( {C{S}_{t}}\right)$ ：包含已成功执行并验证的子目标的集合。定义为 $S{S}_{t} = \left\{  {{\bar{g}}_{1},{\bar{g}}_{2},\ldots ,{\bar{g}}_{m}}\right\}$。用于避免重复操作并跟踪进度。


- Action $\left( {A}_{t}\right)$ : The operation executed by the Worker Agent at timestep $t$ . An action is a structured tuple ${A}_{t} = \left( {{\tau }_{t},{\alpha }_{t},{\sigma }_{t}}\right)  \in  \mathcal{A}$ ,where:
- 动作 $\left( {A}_{t}\right)$ ：工作代理在时间步 $t$ 执行的操作。动作为结构化元组 ${A}_{t} = \left( {{\tau }_{t},{\alpha }_{t},{\sigma }_{t}}\right)  \in  \mathcal{A}$，其中：


- ${\tau }_{t}$ : The thought process,a textual rationale for selecting the action.
- ${\tau }_{t}$ ：思考过程，选择该动作的文本理由。


- ${\alpha }_{t}$ : The concrete,low-level action command (e.g.,click(x,y),type("text")).
- ${\alpha }_{t}$ ：具体的低级动作命令（例如，click(x,y)、type("text")）。


- ${\sigma }_{t}$ : A concise summary of the action’s intended effect.
- ${\sigma }_{t}$ ：对该动作意图效果的简明概述。


- Reflection Feedback $\left( {F}_{t}\right)$ : The output generated by the Reflector Agent after observing the consequences of action ${A}_{t}$ . It is a tuple ${F}_{t} = \left( {{j}_{t},{\phi }_{t}}\right)  \in  \{$ SUCCESS,FAILURE $\}  \times  \Phi$ ,where:
- 反思反馈 $\left( {F}_{t}\right)$ ：反思者在观察动作 ${A}_{t}$ 后产生的输出。为元组 ${F}_{t} = \left( {{j}_{t},{\phi }_{t}}\right)  \in  \{$ SUCCESS,FAILURE $\}  \times  \Phi$，其中：


- ${j}_{t}$ : A binary judgment on the outcome of ${A}_{t}$ .
- ${j}_{t}$ ：对 ${A}_{t}$ 结果的二元判断。


- ${\phi }_{t}$ : A detailed textual feedback,particularly a diagnostic analysis in case of "FAILURE". $\Phi$ represents the space of all possible feedback texts.
- ${\phi }_{t}$ ：详细的文本反馈，特别是在“FAILURE”情况下的诊断分析。$\Phi$ 表示所有可能反馈文本的空间。


- Notes $\left( {N}_{t}\right)$ : A collection of critical,potentially transient information captured by the Notetaker Agent. The cumulative knowledge base at step $t$ is ${\mathcal{N}}_{t} = \mathop{\bigcup }\limits_{{i = 0}}^{{t - 1}}{N}_{i}$ .
- 记录 $\left( {N}_{t}\right)$ ：笔记者捕获的重要且可能是短暂的信息集合。步骤 $t$ 的累积知识库为 ${\mathcal{N}}_{t} = \mathop{\bigcup }\limits_{{i = 0}}^{{t - 1}}{N}_{i}$。


7.2 Agent Architecture in Detail
7.2 代理架构详述


#### 7.2.1 External Knowledge Retrieval with RAG
#### 7.2.1 使用 RAG 的外部知识检索


To enable the agent to complete tasks requiring real-time information or domain-specific knowledge (e.g., checking today's weather, finding recent sports scores, or looking up app-specific tutorials), we incorporate a RAG module. This module is invoked at the beginning of a task to retrieve relevant information from external sources, such as the internet, and provide it as context to the agent system.
为使代理完成需要实时信息或领域特定知识的任务（例如，查询今日天气、查找最新体育比分或检索应用相关教程），我们加入了 RAG 模块。该模块在任务开始时被调用，从外部来源（如互联网）检索相关信息，并将其作为上下文提供给代理系统。


The process can be formalized as follows. Given an initial user instruction $I$ ,the RAG module first processes it into one or more search engine-friendly queries $Q$ .
该过程可形式化如下。给定一个初始用户指令 $I$，RAG 模块首先将其处理为一个或多个适合搜索引擎的查询 $Q$。


$$
Q = \text{ GenerateQueries(I) }
$$



Subsequently,the system uses these queries $Q$ to retrieve a set of relevant documents or text snippets $D = \; \left\{  {{d}_{1},{d}_{2},\ldots ,{d}_{n}}\right\}$ from an external knowledge source (e.g.,a web search engine).
随后，系统使用这些查询 $Q$ 从外部知识源（例如网页搜索引擎）检索一组相关文档或文本片段 $D = \; \left\{  {{d}_{1},{d}_{2},\ldots ,{d}_{n}}\right\}$。


$$
D = \text{ SearchEngine }\left( Q\right)
$$



Finally, the retrieved content is processed and summarized to form a concise, information-rich body of knowledge, ${K}_{\mathrm{{RAG}}}$ .
最后，将检索到的内容处理并总结，形成一个简洁且信息丰富的知识体，${K}_{\mathrm{{RAG}}}$。


$$
{K}_{\mathrm{{RAG}}} = \text{ Process }\left( D\right)
$$



This retrieved knowledge ${K}_{\mathrm{{RAG}}}$ is passed to the Manager agent during its initialization phase (as shown in Algorithm Algorithm 1,lines 3-4). This allows the Manager to generate its initial plan $\left( {S{S}_{0},C{S}_{0}}\right)$ based on more comprehensive and accurate information, thereby significantly improving the quality of the plan and the likelihood of task success. For example, for an instruction like "Should I take an umbrella to the park today?", ${K}_{\mathrm{{RAG}}}$ would contain the weather forecast, enabling the Manager to create a plan that includes steps like "open weather app" and "check for rain probability".
这些检索到的知识 ${K}_{\mathrm{{RAG}}}$ 在管理者代理初始化阶段被传递（如算法 1，第 3-4 行所示）。这使管理者能够基于更全面准确的信息生成其初始计划 $\left( {S{S}_{0},C{S}_{0}}\right)$，从而显著提升计划质量和任务成功的可能性。例如，对于“今天去公园我应不应该带伞？”这样的指令，${K}_{\mathrm{{RAG}}}$ 会包含天气预报，使管理者能制定包含“打开天气应用”“查看降雨概率”等步骤的计划。


#### 7.2.2 The Manager Agent: Dynamic Task Planning and Coordination
#### 7.2.2 管理者代理：动态任务规划与协调


The Manager Agent serves as the strategic core of the framework. Its function $\mathcal{M}$ is responsible for decomposing the high-level user instruction $I$ into a coherent sequence of subgoals and dynamically adapting this plan throughout the execution process.
管理者代理是框架的战略核心。其功能 $\mathcal{M}$ 负责将高级用户指令 $I$ 分解为连贯的子目标序列，并在执行过程中动态调整该计划。


Initially,at $t = 0$ ,the Manager performs a decomposition:
最初，在 $t = 0$，管理者执行分解：


$$
\left( {S{S}_{0},C{S}_{0}}\right)  = {\mathcal{M}}_{\text{ init }}\left( {I,{S}_{0},{K}_{\mathrm{{RAG}}}}\right) \tag{5}
$$



where ${K}_{\mathrm{{RAG}}}$ is external knowledge retrieved by the RAG module to inform the decomposition of potentially domain-specific or complex instructions. $C{S}_{0}$ is initialized as an empty set $\varnothing$ .
其中 ${K}_{\mathrm{{RAG}}}$ 是由 RAG 模块检索的外部知识，用以指导对可能具有领域特定或复杂性的指令的分解。$C{S}_{0}$ 被初始化为空集 $\varnothing$。


In subsequent steps $t > 0$ ,the Manager updates the plan based on the latest execution results:
在随后的步骤中 $t > 0$，管理者根据最新的执行结果更新计划：


$$
\left( {S{S}_{t},C{S}_{t}}\right)  = {\mathcal{M}}_{\text{ update }}\left( {I,{S}_{t - 1},S{S}_{t - 1},C{S}_{t - 1},{A}_{t - 1},{F}_{t - 1},{\mathcal{N}}_{t}}\right) \tag{6}
$$



If the previous action was successful $\left( {{j}_{t - 1} = }\right.$ SUCCESS),the Manager identifies the completed subgoal in $S{S}_{t - 1}$ , moves it to $C{S}_{t}$ ,and re-prioritizes the remaining tasks in $S{S}_{t}$ . If the action failed $\left( {{j}_{t - 1} = }\right.$ FAILURE),the Manager leverages the diagnostic feedback ${\phi }_{t - 1}$ to revise the plan. This may involve re-ordering subgoals,modifying an existing subgoal, inserting a new corrective subgoal, or even reverting to a previous strategy.
如果先前动作成功 $\left( {{j}_{t - 1} = }\right.$ SUCCESS)，管理者在 $S{S}_{t - 1}$ 中识别已完成的子目标，将其移至 $C{S}_{t}$，并重新为剩余任务在 $S{S}_{t}$ 中排序。如果动作失败 $\left( {{j}_{t - 1} = }\right.$ FAILURE)，管理者利用诊断反馈 ${\phi }_{t - 1}$ 修订计划。此过程可能包括重新排序子目标、修改现有子目标、插入新的纠正子目标，甚至回退到先前策略。


#### 7.2.3 The Worker Agent: Grounded Action Execution
#### 7.2.3 执行者代理：有根据的动作执行


The Worker Agent is the tactical executor, translating the strategic plan from the Manager into concrete interactions with the GUI. Its function $\mathcal{W}$ aims to execute the highest-priority,currently feasible subgoal from the guidance list $C{S}_{t}$ .
执行者代理是战术执行者，将管理者的战略计划转化为与 GUI 的具体交互。其功能 $\mathcal{W}$ 旨在执行指导列表 $C{S}_{t}$ 中优先级最高且当前可行的子目标。


$$
{A}_{t} = \mathcal{W}\left( {I,{S}_{t},S{S}_{t},{F}_{t - 1},{\mathcal{N}}_{t}}\right) \tag{7}
$$



Upon receiving the subgoal list $S{S}_{t}$ ,the Worker inspects a small subset from the top of the list (e.g.,the top $N$ subgoals). It analyzes the current screen ${S}_{t}$ to determine which of these subgoals is most relevant and actionable. The decision-making process is informed by feedback from the previous step, ${F}_{t - 1}$ ,to avoid repeating errors,and the accumulated notes, ${\mathcal{N}}_{t}$ ,to utilize previously stored information (e.g.,using a password saved in notes). The output, ${A}_{t} = \left( {{\tau }_{t},{\alpha }_{t},{\sigma }_{t}}\right)$ ,provides a transparent record of its reasoning,action,and intent,which is crucial for reflection.
在收到子目标列表 $S{S}_{t}$ 后，执行者检查列表顶部的一小部分（例如，前 $N$ 个子目标）。它分析当前屏幕 ${S}_{t}$ 来确定这些子目标中哪个最相关且可执行。决策过程受前一步反馈 ${F}_{t - 1}$ 的影响以避免重复错误，并利用累积笔记 ${\mathcal{N}}_{t}$ 使用先前存储的信息（例如使用保存在笔记中的密码）。输出 ${A}_{t} = \left( {{\tau }_{t},{\alpha }_{t},{\sigma }_{t}}\right)$ 提供其推理、动作和意图的透明记录，这对反思至关重要。


#### 7.2.4 The Reflector Agent: Self-Correction through Reflection
#### 7.2.4 反思者代理：通过反思实现自我修正


The Reflector Agent is a critical component for ensuring robustness and learning from mistakes. It embodies the framework’s capacity for self-assessment. Its function $\mathcal{R}$ evaluates the efficacy of an action by comparing the state transition with the Worker's intent.
反思者代理是确保稳健性并从错误中学习的关键组件。它体现了框架的自我评估能力。其功能 $\mathcal{R}$ 通过将状态转移与执行者的意图进行比较来评估动作的有效性。


$$
{F}_{t} = \mathcal{R}\left( {I,{S}_{t},{S}_{t + 1},{A}_{t}}\right) \tag{8}
$$



The Reflector analyzes the pre-action state ${S}_{t}$ ,the post-action state ${S}_{t + 1}$ ,and the action tuple ${A}_{t}$ . A judgment ${j}_{t} =$ SUCCESS is rendered if the state change ${S}_{t} \rightarrow  {S}_{t + 1}$ aligns with the progress articulated in the Worker’s thought ${\tau }_{t}$ and summary ${\sigma }_{t}$ . Conversely, ${j}_{t} =$ FAILURE is returned if the GUI presents an error,remains unchanged unexpectedly,or transitions to an irrelevant state. In case of failure,the feedback ${\phi }_{t}$ provides a causal analysis, such as action click(123, 456) on button "Submit" did not proceed to the next page; an error message "Invalid credentials" is now visible. This detailed feedback is vital for the Manager's replanning phase.
反思者分析操作前状态 ${S}_{t}$、操作后状态 ${S}_{t + 1}$ 与操作元组 ${A}_{t}$。如果状态变化 ${S}_{t} \rightarrow  {S}_{t + 1}$ 与执行者的思考 ${\tau }_{t}$ 与摘要 ${\sigma }_{t}$ 中表述的进展一致，则判断为 ${j}_{t} =$ SUCCESS。相反，如果界面显示错误、意外未发生变化或转到无关状态，则返回 ${j}_{t} =$ FAILURE。若失败，反馈 ${\phi }_{t}$ 提供因果分析，例如在“提交”按钮上点击(click(123, 456)) 未能进入下一页；现在可见错误信息“Invalid credentials”。此详细反馈对于管理者的重新规划阶段至关重要。


#### 7.2.5 The Notetaker Agent: Persistent Contextual Memory
#### 7.2.5 记录者代理：持久的上下文记忆


The Notetaker Agent addresses the challenge of state volatility in GUI interactions, where crucial information may appear on one screen and be required on a subsequent,different screen. The Notetaker's function $\mathcal{C}$ is to identify and persist such information.
记录者代理解决 GUI 交互中状态易变的问题，在某一屏幕出现的重要信息可能在后续不同屏幕被需要。记录者的功能 $\mathcal{C}$ 是识别并持久保存此类信息。


$$
{N}_{t} = \mathcal{C}\left( {S}_{t}\right) \tag{9}
$$



This agent is triggered only upon a successful action ( ${j}_{t} =$ SUCCESS). It scans the state transition for pieces of information designated as vital for the ongoing task (e.g., reservation codes, order numbers, user-generated content, entered credentials). This information is structured into the note set ${N}_{t}$ . The cumulative notes ${\mathcal{N}}_{t + 1} = {\mathcal{N}}_{t} \cup  {N}_{t}$ are then made available to both the Manager and the Worker in future steps, creating a persistent memory that informs long-horizon planning and execution.
该代理仅在操作成功（${j}_{t} =$ SUCCESS）时触发。它扫描状态转变中被指定为对当前任务至关重要的信息片段（例如预订码、订单号、用户生成内容、输入的凭证）。这些信息被结构化为笔记集 ${N}_{t}$。累积笔记 ${\mathcal{N}}_{t + 1} = {\mathcal{N}}_{t} \cup  {N}_{t}$ 随后在后续步骤中提供给管理者和执行者，形成一种持久记忆，支持长期的规划与执行。


Algorithm 1 Mobile-Agent-v3 Execution Loop
算法 1 Mobile-Agent-v3 执行循环


---



1: Input: User instruction $I$ ,initial device state ${S}_{0}$ ,max timesteps ${T}_{\max }$
1: 输入：用户指令 $I$、初始设备状态 ${S}_{0}$、最大时间步数 ${T}_{\max }$


	: Initialize: Manager $\mathcal{M}$ ,Worker $\mathcal{W}$ ,Reflector $\mathcal{R}$ ,Notetaker $\mathcal{C}$
	: 初始化：管理者 $\mathcal{M}$、执行者 $\mathcal{W}$、反思者 $\mathcal{R}$、记录者 $\mathcal{C}$


	$\vartriangleright$ Init Manager Phase: Initialize Plan
	$\vartriangleright$ 初始化 管理者阶段：初始化计划


	Retrieve external knowledge ${K}_{\mathrm{{RAG}}} \leftarrow  \operatorname{RAG}\left( I\right)$
	检索外部知识 ${K}_{\mathrm{{RAG}}} \leftarrow  \operatorname{RAG}\left( I\right)$


	$\left( {S{S}_{0},C{S}_{0}}\right)  \leftarrow  {\mathcal{M}}_{\text{ init }}\left( {I,{S}_{0},{K}_{\mathrm{{RAG}}}}\right)$



	${\mathcal{N}}_{0} \leftarrow  \varnothing ,{F}_{-1} \leftarrow$ null, $t \leftarrow  0$
	${\mathcal{N}}_{0} \leftarrow  \varnothing ,{F}_{-1} \leftarrow$ null，$t \leftarrow  0$


	while $t < {T}_{\max }$ and $S{S}_{t} \neq  \varnothing$ do
	当 $t < {T}_{\max }$ 与 $S{S}_{t} \neq  \varnothing$ 为真时循环


		$\vartriangleright$ Worker Phase: Execute Action
		$\vartriangleright$ 执行者阶段：执行操作


		${A}_{t} \leftarrow  \mathcal{W}\left( {I,{S}_{t},S{S}_{t},{F}_{t - 1},{\mathcal{N}}_{t}}\right)$



		if ${A}_{t} =$ TERMINATE then
		如果 ${A}_{t} =$ TERMINATE 则


			Break
			中断


		end if
		结束如果


		${S}_{t + 1} \leftarrow$ ExecuteOnDevice $\left( {A}_{t}\right)$
		${S}_{t + 1} \leftarrow$ 在设备上执行 $\left( {A}_{t}\right)$


		$\vartriangleright$ Reflector Phase: Evaluate Outcome
		$\vartriangleright$ 反思者阶段：评估结果


		${F}_{t} \leftarrow  \mathcal{R}\left( {I,{S}_{t},{S}_{t + 1},{A}_{t}}\right)$



		$\vartriangleright$ Notetaker Phase: Persist Information
		$\vartriangleright$ 记录者阶段：保存信息


		if ${F}_{t}$ .status $=$ SUCCESS then
		如果 ${F}_{t}$ .status $=$ SUCCESS 那么


			${N}_{t} \leftarrow  \mathcal{C}\left( {S}_{t}\right)$



			${\mathcal{N}}_{t + 1} \leftarrow  {\mathcal{N}}_{t} \cup  \left\{  {N}_{t}\right\}$



		else
		否则


			${\mathcal{N}}_{t + 1} \leftarrow  {\mathcal{N}}_{t}$



		end if
		结束如果


		$\vartriangleright$ Manager Phase: Update Plan
		$\vartriangleright$ 管理者阶段：更新计划


		$\left( {S{S}_{t + 1},C{S}_{t + 1}}\right)  \leftarrow  {\mathcal{M}}_{\text{ update }}\left( {I,{S}_{t},S{S}_{t},C{S}_{t},{A}_{t},{F}_{t},{\mathcal{N}}_{t + 1}}\right)$



		$t \leftarrow  t + 1$



	end while
		结束循环


	if $S{S}_{t} = \varnothing$ then
		如果 $S{S}_{t} = \varnothing$ 那么


		return Task Succeeded
		返回 任务 成功


	else
		否则


		return Task Failed (Timeout or Stalemate)
		返回 任务 失败（超时或僵局）


	end if
		结束 如果


---



### 7.3 INTEGRATED WORKFLOW AND ALGORITHM
### 7.3 集成工作流与算法


The Mobile-Agent-v3 framework operates as a cyclical, state-driven process. The workflow begins with a user instruction and terminates when the task is complete or deemed unachievable. The entire process is formalized in Algorithm 1.
Mobile-Agent-v3 框架作为一个循环的、基于状态的过程运行。工作流以用户指令开始，以任务完成或被判定为无法实现时结束。整个过程在算法 1 中形式化描述。


The process is initialized with a high-level user instruction $I$ . The Manager Agent,aided by the RAG module, creates an initial subgoal plan $S{S}_{0}$ . The system then enters an iterative loop. In each iteration $t$ ,the Worker Agent selects and executes a subgoal,resulting in action ${A}_{t}$ . The environment transitions to a new state ${S}_{t + 1}$ . The Reflector Agent evaluates this transition,producing feedback ${F}_{t}$ . If the action was successful,the Notetaker Agent may record pertinent information as ${N}_{t}$ . Finally,the Manager Agent updates the task plan to $\left( {S{S}_{t + 1},C{S}_{t + 1}}\right)$ based on the feedback.
过程以一个高级用户指令 $I$ 初始化。经理代理在 RAG 模块的辅助下，创建初始子目标计划 $S{S}_{0}$ 。系统随后进入迭代循环。在每次迭代中 $t$ ，工作者代理选择并执行一个子目标，产生动作 ${A}_{t}$ 。环境转变为新状态 ${S}_{t + 1}$ 。反思代理评估此转变，产生反馈 ${F}_{t}$ 。若动作成功，记事代理可能将相关信息记录为 ${N}_{t}$ 。最后，经理代理根据反馈将任务计划更新为 $\left( {S{S}_{t + 1},C{S}_{t + 1}}\right)$ 。


Termination occurs under two conditions:
终止发生在两个条件下：


1. Task Completion: The Manager determines the task is complete, resulting in an empty pending subgoal list $\left( {S{S}_{t} = \varnothing }\right)$ .
1. 任务完成：经理确定任务完成，导致待处理子目标列表为空 $\left( {S{S}_{t} = \varnothing }\right)$ 。


2. Execution Stalemate: The Worker Agent determines that no pending subgoals in $S{S}_{t}$ can be executed on the current state ${S}_{t}$ ,even after several retries or plan revisions.
2. 执行僵局：工作者代理确定在当前状态 ${S}_{t}$ 下无法执行 $S{S}_{t}$ 中的任何待处理子目标，即使经过多次重试或计划修正。


This structured, reflective, and adaptive loop enables the framework to navigate complex sequences of interactions, handle unexpected events, and robustly pursue the completion of the user's goal.
这一结构化、反思性且自适应的循环使框架能够导航复杂的交互序列、处理意外事件，并稳健地推进用户目标的完成。


### 7.4 CASE STUDY
### 7.4 案例研究


Figure 12 shows a complete Mobile-Agent-v3 operation flow, including the outputs of the manager, worker, and reflector. The manager's output shows that subgoals are constantly updated as the task progresses. The worker consistently outputs actions guided by the subgoals output by the manager. Notably, the red text in Figure 12 illustrates a successful reflection. After the worker's click operation in the previous step failed, the reflector successfully discovered the problem and provided feedback to the manager and worker in the next step. Finally, the worker repeated the click operation to correct the problem.
图 12 展示了完整的 Mobile-Agent-v3 操作流程，包括经理、工作者和反思者的输出。经理的输出显示随着任务进展子目标不断更新。工作者始终根据经理输出的子目标产生动作。值得注意的是，图 12 中的红色文本示例说明了一次成功的反思。在前一步工作者的点击操作失败后，反思者成功发现问题并在下一步向经理和工作者提供了反馈。最终，工作者重复点击操作以纠正问题。


## 8 Conclusion
## 8 结论


In this paper, we present GUI-Owl, a native end-to-end multimodal agent model that unifies perception, grounding, reasoning, planning, and action execution within a single scalable framework for GUI automation. Building upon Qwen2.5-VL and extensively post-trained on large-scale, diverse GUI interaction data, GUI-Owl achieves state-of-the-art performance across a broad range of challenging benchmarks, surpassing both open-source and proprietary systems, including GPT-40 and Claude 3.7. Through synthesized reasoning data and a scalable reinforcement learning framework, GUI-Owl is capable of versatile decision-making from autonomous single-agent execution to collaborative multi-agent role coordination within our Mobile-Agent-v3 framework.
本文提出了 GUI-Owl，一种原生端到端的多模态代理模型，将感知、定位、推理、规划与动作执行在单一可扩展框架下统一用于 GUI 自动化。基于 Qwen2.5-VL 并在大规模、多样化的 GUI 交互数据上广泛后训练，GUI-Owl 在广泛的挑战性基准上实现了最先进的性能，超越了开源与专有系统，包括 GPT-40 与 Claude 3.7。通过合成推理数据和可扩展的强化学习框架，GUI-Owl 能够在我们的 Mobile-Agent-v3 框架中实现从自主单代理执行到协作多代理角色协调的多样化决策能力。


---



										Example of end-to-end training data
										端到端训练数据示例


I



	\{



			"role": "system",
			"role": "system",


		"content": [\{"type": "text", "text": "You are a helpful assistant. \\n\\nYou may call one or
		"content": [\{"type": "text", "text": "你是一个乐于助人的助手。 \\n\\n你可以调用一个或


more functions to assist with the user query.\\n\\nYou are provided with function signatures
更多功能以协助用户查询。\\n\\n为您提供了函数签名


within <tools></tools> XML tags:\\n<tools>\\n\{\\"type\\": \\"function\\", \\"function\\": \{\\"name\\":
在 <tools></tools> XML 标签内:\\n<tools>\\n\{\\"type\\": \\"function\\", \\"function\\": \{\\"name\\":


\\"computer_use\\", \\"description\\": \\"Use a mouse and keyboard to interact with a computer, and
"computer_use", "description": "使用鼠标和键盘与计算机交互，且


take screenshots.\\\\n* This is an interface to a desktop GUI. You do not have access to a
截屏。\\\\n* 这是桌面图形界面的接口。你无法访问到一台


terminal or applications menu. You must click on desktop icons to start applications.\\\\n* Some
终端或应用菜单。必须点击桌面图标来启动应用。\\\\n* 一些


applications may take time to start or process actions, so you may need to wait and take
应用程序可能需要时间启动或处理操作，所以您可能需要等待并采取


successive screenshots to see the results of your actions. E.g. if you click on Firefox and a
连续截图以查看操作结果。例如，如果你单击 Firefox 并且一个


window doesn't open, try wait and taking another screenshot.\\N* The screen's resolution is
窗口未打开，尝试等待并再次截图。\\N* 屏幕分辨率为


1932x1092.\\\\n* Whenever you intend to move the cursor to click on an element like an icon, you
1932x1092.\\\\n* 每当你打算移动光标去点击某个元素（例如图标）时，你


should consult a screenshot to determine the coordinates of the element before moving the
在移动之前应查看截图以确定元素的坐标


cursor.\\\\n* If you tried clicking on a program or link but it failed to load, even after waiting,
光标。\\\\n* 如果你尝试点击一个程序或链接但它未能加载，即使等待后仍如此，


try adjusting your cursor position so that the tip of the cursor visually falls on the element
尝试调整光标位置，使光标尖端视觉上正好落在该元素上


that you want to click.\\\\n* Make sure to click any buttons, links, icons, etc with the cursor
要点击的项目。\\\\n* 请确保用光标点击所有按钮、链接、图标等


tip in the center of the element. Don't click boxes on their edges unless asked.\\",
将指针放在元素中心。除非另有要求，否则不要在边缘点击方框。\\",


\\"parameters\\": \{\\"properties\\": \{\\"action\\": \{\\"description\\": \\"The action to perform. The
\"parameters\": {\"properties\": {\"action\": {\"description\": \"要执行的操作。可用操作有：


available actions are:\\\\n* `key`: Performs key down presses on the arguments passed in order,
* `key`：按顺序对传入参数执行按键按下，然后按相反顺序释放按键。\",


then performs key releases in reverse order.\\\\n* `type`: Type a string of text on the
* `type`：在键盘上输入一段文本。\",


keyboard.\\\\n* `mouse_move`: Move the cursor to a specified (x, y) pixel coordinate on the
* `mouse_move`：将光标移动到屏幕上的指定 (x, y) 像素坐标。\",


screen.\\\\n* `\\\\left_click`: Click the left mouse button.\\\\n* `left_click_drag`: Click and drag the
* `left_click`：点击鼠标左键。\",


cursor to a specified (x, y) pixel coordinate on the screen.\\\\n* \\right_click`: Click the right
* `left_click_drag`：按住并拖动光标到屏幕上的指定 (x, y) 像素坐标。\",


mouse button.\\\\n* middle_click: Click the middle mouse button.\\\\n* `double_click`: Double-
* `right_click`：点击鼠标右键。\",


click the left mouse button.\\\\n* `scroll`: Performs a scroll of the mouse scroll wheel.\\\\n*
* `middle_click`：点击鼠标中键。\",


`wait` : Wait specified seconds for the change to happen.\\\\n* `terminate`: Terminate the current
* `double_click`：双击鼠标左键。\",


task and report its completion status.\\", \\"enum\\": [\\"key\\", \\"type\\", \\"mouse_move\\",
* `scroll`：滚动鼠标滚轮。\",


\\"left_click\\", \\"left_click_drag\\", \\"right_click\\", \\"middle_click\\", \\"double_click\\",
* `wait`：等待指定秒数以便变更发生。\",


\\"scroll\\", \\"wait\\", \\"terminate\\"], \\"type\\": \\"string\\"\}, \\"keys\\": \{\\"description\\":
* `terminate`：终止当前任务并报告其完成状态。\", \"enum\": [\"key\", \"type\", \"mouse_move\",


\\"Required only by `action=key`.\\", \\"type\\": \\"array\\"\}, \\"text\\": \\"description\\": \\"Required
\"left_click\", \"left_click_drag\", \"right_click\", \"middle_click\", \"double_click\",


only by action=type`.\\", \\"type\\": \\"string\\"\}, \\"coordinate\\": \{\\"description\\": \\"(x, y): The
\"scroll\", \"wait\", \"terminate\"], \"type\": \"string\"}, \"keys\": {\"description\":


x (pixels from the left edge) and y (pixels from the top edge) coordinates to move the mouse to.
\"仅在 action=key 时需要。\", \"type\": \"array\"}, \"text\": {\"description\": \"仅在 action=type 时需要。\", \"type\": \"string\"}, \"coordinate\": {\"description\": \"(x, y)：将鼠标移动到的 x（距左边缘的像素）和 y（距上边缘的像素）坐标。\",


Required only by `action=mouse_move` and `action=left_click_drag`.\\", \\"type\\": \\"array\\"\},
\"仅在 `action=mouse_move` 和 `action=left_click_drag` 时需要。\", \"type\": \"array\"},


\\"pixels\\": \{\\"description\\": \\"The amount of scrolling to perform. Positive values scroll up,
\"pixels\": {\"description\": \"要执行的滚动量。正值向上滚动，


negative values scroll down. Required only by `action=scroll`.\\", \\"type\\": \\"number\\"\},
负值向下滚动。仅在 `action=scroll` 时需要。\\", \\"type\\": \\"number\\"\\},


\\"time\\": \{\\"description\\": \\"The seconds to wait. Required only by `action=wait`.", \\"type\\":
\"time\": {\"description\": \"等待的秒数。仅在 `action=wait` 时必需。\", \"type\":


\\"number\\"\}, \\"status\\": \{\\"description\\": \\"The status of the task. Required only by
\\"number\\"\}, \\"status\\": \{\\"description\\": \\"任务的状态。仅当由


`action=terminate`.\\", \\"type\\": \\"string\\", \\"enum\\": [\\"success\\", \\"failure\\"]\}\},
`action=terminate`.\\", \\"type\\": \\"string\\", \\"enum\\": [\\"success\\", \\"failure\\"]\}\},


\\"required\\": [\\"action\\"], \\"type\\": \\"object\\"\}\}\}\\n</tools>\\n\\nFor each function call, return
\\"required\\": [\\"action\\"], \\"type\\": \\"object\\"\}\}\}\\n</tools>\\n\\n对于每次函数调用，返回


a json object with function name and arguments within <tool_call></tool_call> XML
一个包含函数名和参数的 JSON 对象，位于 <tool_call></tool_call> XML 内


tags:\\n<tool_call>\\n\{\\"name\\": <function-name>, \\"arguments\\": <args-json-
标签:\\n<tool_call>\\n\{\\"name\\": <function-name>, \\"arguments\\": <args-json-


object>>\\n</tool_call>" \}]
object>>\\n</tool_call>" \}]


	\},



	\{



			"role": "user",
			"role": "user",


			"content": [
			"content": [


				\{"type": "text", "text": "The user query: I want to share "Oscar and the wolf -
				\{"type": "text", "text": "用户查询：我想分享 \"Oscar and the wolf -


somebody wants'' music to my friend karin.iversen@example.com via gmail\\nBefore answering,
有人想要给我的朋友 karin.iversen@example.com via gmail\\n 的音乐。在回复前，


explain your reasoning step-by-step in <thinking></thinking> <ags, and insert them before the
逐步解释你的推理过程，在 <thinking></thinking> <ags，并将它们插入到之前的


<tool_call></tool_call> XML tags.\\nTask progress (You have done the following operation on the
XML 标签。\\n任务进度（您已对以下内容执行过操作：


current device): "\},
当前设备): "\},


				\{



					"type": "image",
					"type": "image",


					"image": <image>
					"image": <image>


				\}



		]



	\},



			"role": "assistant",
			"role": "assistant",


		"content": [\{"type": "text", "text": <thinking>\\nThe global task objective is to share the
		"content": [\{"type": "text", "text": <thinking>\\n全局任务目标是将


music \\"05car and the wolf - somebody wants\\" to the email karin.iversen@example.com via Gmail.
音乐 \\"05car and the wolf - somebody wants\\" 通过 Gmail 分享给邮箱 karin.iversen@example.com。


Currently, the screen shows a video upload interface, which is not relevant to the task at hand.
当前屏幕显示视频上传界面，这与当前任务无关。


To proceed towards the goal, I need to navigate back to a more relevant screen where I can find
为推进目标，我需要返回到更相关的界面，在那里我可以找到


the music or access the sharing options.\\n\\nAction: Go back to the previous
该音乐或访问分享选项。\\n\\n操作：返回到上一个


screen.\\n</thinking>\\n<tool_call>\\n\{\\"name\\": \\"mobile_use\\", \\"arguments\\": \{\\"action\\":
界面。\\n</thinking>\\n<tool_call>\\n\{\\"name\\": \\"mobile_use\\", \\"arguments\\": \{\\"action\\":


\\"system_button\\", \\"button\\": \\"Back\\"\}\}\\n</tool_call>\\n "\}]
\\"system_button\\", \\"button\\": \\"Back\\"\}\}\\n</tool_call>\\n "\}]


	\}



]



											Figure 11: Format of end-to-end training data.
											Figure 11: Format of end-to-end training data.


---



<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/12/2025_12_03__23_18_24_d57bda.jpg"/>



Figure 12: A case of a complete Mobile-Agent-3 operation process on a desktop platform. The red text represents successful reflection content.
Figure 12: A case of a complete Mobile-Agent-3 operation process on a desktop platform. The red text represents successful reflection content.


## REFERENCES
## REFERENCES


Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like a human. arXiv preprint arXiv:2410.08164, 2024. 10
Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like a human. arXiv preprint arXiv:2410.08164, 2024. 10


Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: A compositional generalist-specialist framework for computer use agents, 2025. URL https://arxiv.org/abs/2504.00906.2, 10, 16, 17
Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: A compositional generalist-specialist framework for computer use agents, 2025. URL https://arxiv.org/abs/2504.00906.2, 10, 16, 17


Anthropic. Claude-3-5-sonnet. Technical report, Anthropic, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. 14
Anthropic. Claude-3-5-sonnet. Technical report, Anthropic, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. 14


Anthropic. Claude 3.7 sonnet and claude code. Technical report, Anthropic, 2025a. URL https://www.anthropic.com/news/claude-3-7-sonnet.System Card. 12, 13, 14
Anthropic。Claude 3.7 sonnet 与 claude code。技术报告，Anthropic，2025a。URL https://www.anthropic.com/news/claude-3-7-sonnet.System Card。12, 13, 14


Anthropic. Claude-4-sonnet. Technical report, Anthropic, 2025b. URL https://www.anthropic.com/news/ claude-4. 14
Anthropic。Claude-4-sonnet。技术报告，Anthropic，2025b。URL https://www.anthropic.com/news/ claude-4。14


Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6, 12, 13, 14, 17
Shuai Bai、Keqin Chen、Xuejing Liu、Jialin Wang、Wenbin Ge、Sibo Song、Kai Dang、Peng Wang、Shijie Wang、Jun Tang、Humen Zhong、Yuanzhi Zhu、Mingkun Yang、Zhaohai Li、Jianqiang Wan、Pengfei Wang、Wei Ding、Zheren Fu、Yiheng Xu、Jiabo Ye、Xi Zhang、Tianbao Xie、Zesen Cheng、Hang Zhang、Zhibo Yang、Haiyang Xu 和 Junyang Lin。Qwen2.5-vl 技术报告。arXiv 预印本 arXiv:2502.13923，2025。6, 12, 13, 14, 17


Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9313-9332, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.505.12
Kanzhi Cheng、Qiushi Sun、Yougang Chu、Fangzhi Xu、Li YanTao、Jianbing Zhang 和 Zhiyong Wu。SeeClick：利用 GUI 定位构建高级视觉 GUI 代理。发表于第 62 届计算语言学协会年会论文集（第 1 卷：长篇论文），第 9313-9332 页，泰国曼谷，2024 年 8 月。计算语言学协会。URL https://aclanthology.org/2024.acl-long.505。12


Alibaba Cloud. Introducing alibaba cloud, 2018. 3
阿里云。Introducing alibaba cloud，2018。3


Deepmind. Introducing gemini 2.0: our new ai model for the agentic era. Technical report, Deepmind, 2025a. URL https://blog.google/technology/google-deepmind/ google-gemini-ai-update-december-2024/#project-astra. 14
Deepmind。Introducing gemini 2.0：我们面向代理时代的新 AI 模型。技术报告，Deepmind，2025a。URL https://blog.google/technology/google-deepmind/ google-gemini-ai-update-december-2024/#project-astra。14


Deepmind. Gemini 2.5: Our most intelligent ai model. Technical report, Deepmind, 2025b. URL https:// blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/. 13
Deepmind。Gemini 2.5：我们最智能的 AI 模型。技术报告，Deepmind，2025b。URL https:// blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/。13


Minghe Gao, Wendong Bu, Bingchen Miao, Yang Wu, Yunfei Li, Juncheng Li, Siliang Tang, Qi Wu, Yueting Zhuang, and Meng Wang. Generalist virtual agents: A survey on autonomous agents across digital platforms. ArXiv preprint, abs/2411.10943, 2024. URL https://arxiv.org/abs/2411.10943.2
Minghe Gao、Wendong Bu、Bingchen Miao、Yang Wu、Yunfei Li、Juncheng Li、Siliang Tang、Qi Wu、Yueting Zhuang 和 Meng Wang。通用虚拟代理：关于跨数字平台自治代理的综述。ArXiv 预印本，abs/2411.10943，2024。URL https://arxiv.org/abs/2411.10943。2


Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. 13
Boyu Gou、Ruohan Wang、Boyuan Zheng、Yanan Xie、Cheng Chang、Yiheng Shu、Huan Sun 和 Yu Su。像人类一样导航数字世界：面向 GUI 代理的通用视觉定位。arXiv 预印本 arXiv:2410.05243，2024。13


Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 9
Daya Guo、Dejian Yang、Haowei Zhang、Junxiao Song、Ruoyu Zhang、Runxin Xu、Qihao Zhu、Shirong Ma、Peiyi Wang、Xiao Bi 等。Deepseek-r1：通过强化学习激励大型语言模型的推理能力。arXiv 预印本 arXiv:2501.12948，2025。9


Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et al. Os agents: A survey on mllm-based agents for general computing devices use. 2024. 2
Xueyu Hu、Tao Xiong、Biao Yi、Zishu Wei、Ruixuan Xiao、Yurun Chen、Jiasheng Ye、Meiling Tao、Xiangxin Zhou、Ziyu Zhao 等。OS Agents：关于基于 MLLM 的通用计算设备代理的综述。2024。2


Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-40 system card. arXiv preprint arXiv:2410.21276, 2024. 12, 13, 14
Aaron Hurst、Adam Lerer、Adam P Goucher、Adam Perelman、Aditya Ramesh、Aidan Clark、AJ Ostrow、Akila Welihinda、Alan Hayes、Alec Radford 等。Gpt-40 系统卡。arXiv 预印本 arXiv:2410.21276，2024。12, 13, 14


Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. 6
Alexander Kirillov、Eric Mintun、Nikhila Ravi、Hanzi Mao、Chloe Rolland、Laura Gustafson、Tete Xiao、Spencer Whitehead、Alexander C. Berg、Wan-Yen Lo、Piotr Dollár 和 Ross Girshick。Segment anything。arXiv:2304.02643，2023。6


Ning Li, Xiangmou Qu, Jiamu Zhou, Jun Wang, Muning Wen, Kounianhua Du, Xingyu Lou, Qiuying Peng, and Weinan Zhang. Mobileuse: A gui agent with hierarchical reflection for autonomous mobile operation. arXiv preprint arXiv:2507.16853, 2025. 10
宁力, 曲向谋, 周嘉木, 王峻, 温穆宁, 杜口年华, 楼星宇, 彭秋英, 和张伟南. Mobileuse: 一种具有分层反思的 GUI 代理用于自主移动操作. arXiv 预印本 arXiv:2507.16853, 2025. 10


Yanda Li, Chi Zhang, Wanqi Yang, Bin Fu, Pei Cheng, Xin Chen, Ling Chen, and Yunchao Wei. Appagent v2: Advanced agent for flexible mobile interactions. arXiv preprint arXiv:2408.11824, 2024. 10
李燕达, 张驰, 杨晚期, 付斌, 程裴, 陈鑫, 陈玲, 和魏云超. Appagent v2: 灵活移动交互的高级代理. arXiv 预印本 arXiv:2408.11824, 2024. 10


Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Stan Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 19498-19508, 2025. 13
林庆宏, 李林杰, 高迪飞, 杨正远, 吴世伟, 白泽晨, 雷卫贤, 王丽娟, 和寿正明. Showui: 用于 GUI 可视代理的单一视觉-语言-动作模型. 收录于计算机视觉与模式识别会议论文集, 页码 19498-19508, 2025. 13


Xiao Liu, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao, Iat Long Iong, Jiadai Sun, Jiaqi Wang, et al. Autoglm: Autonomous foundation agents for guis. arXiv preprint arXiv:2411.00820, 2024. 10
肖柳、秦博、梁东竹、董广、赖寒宇、张涵辰、赵涵林、Iat Long Iong、孙家岱、王家奇等。Autoglm：面向 GUI 的自主基础代理。arXiv 预印本 arXiv:2411.00820, 2024. 10


Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025. 12
刘雨航、李鹏翔、谢聪凯、胡泽维、韩晓天、张胜宇、杨宏霞、吴飞。Infigui-r1：将多模态 GUI 代理从反应型执行者推进为深思熟虑的推理者。arXiv 预印本 arXiv:2504.14239, 2025. 12


Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. 12
陆政曦、柴雨翔、郭亚轩、尹熙、刘亮、王浩、肖翰、任帅、熊冠京、李鸿胜。Ui-r1：通过强化学习提升 GUI 代理的动作预测。arXiv 预印本 arXiv:2503.21620, 2025. 12


Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: A generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. 6, 9
罗润、王璐、何万维、夏小博。Gui-r1：一种面向 GUI 代理的通用 r1 风格视觉-语言动作模型。arXiv 预印本 arXiv:2504.10458, 2025. 6, 9


Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan A Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, M Tamer Özsu, Aishwarya Agrawal, David Vazquez, et al. Ui-vision: A desktop-centric gui benchmark for visual perception and interaction. arXiv preprint arXiv:2503.15661, 2025. 6, 9
Shravan Nayak、Jiang Xiangru、林清宏、Juan A Rodriguez、Montek Kalsi、Rabiul Awal、Nicolas Chapados、M Tamer Özsu、Aishwarya Agrawal、David Vazquez 等。Ui-vision：一个以桌面为中心的 GUI 视觉感知与交互基准。arXiv 预印本 arXiv:2503.15661, 2025. 6, 9


Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. Gui agents: A survey. ArXiv preprint, abs/2412.13501, 2024. URL https://arxiv.org/abs/2412.13501.2
Nguyen Dang、陈健、王瑜、吴刚、朴南勇、胡政勉、吕汉嘉、吴俊达、Ryan Aponte、夏宇 等。Gui agents：综述。ArXiv 预印本，abs/2412.13501, 2024。URL https://arxiv.org/abs/2412.13501.2


Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan Leng, He Kong, Yi Chang, and Qi Wang. Screenagent: A vision language model-driven computer control agent. arXiv preprint arXiv:2402.07945, 2024. 10
牛润良、李金东、王世奇、付雅丽、胡熙宇、冷雪源、孔赫、常毅、王启。Screenagent：由视觉-语言模型驱动的计算机控制代理。arXiv 预印本 arXiv:2402.07945, 2024. 10


Songqin Nong, Jiali Zhu, Rui Wu, Jiongchao Jin, Shuo Shan, Xiutian Huang, and Wenhao Xu. Mobileflow: A multimodal llm for mobile gui agent. arXiv preprint arXiv:2407.04346, 2024. 10
农松勤、朱佳莉、吴瑞、金炯超、单硕、黄秀天、徐文豪。Mobileflow：面向移动 GUI 代理的多模态 LLM。arXiv 预印本 arXiv:2407.04346, 2024. 10


OpenAI. Computer-using agent: Introducing a universal interface for ai to interact with the digital world. 2025a. URL https://openai.com/index/computer-using-agent.12, 13
OpenAI。Computer-using agent：为 AI 与数字世界交互引入通用接口。2025a。URL https://openai.com/index/computer-using-agent.12, 13


OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, 2025b. URL https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf.System Card. 14
OpenAI。OpenAI o3 与 o4-mini 系统卡。技术报告，OpenAI，2025b。URL https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf。System Card. 14


Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, and Sepp Hochreiter. Large language models can self-improve at web agent tasks. arXiv preprint arXiv:2405.20309, 2024. 10
Ajay Patel、Markus Hofmarcher、Claudiu Leoveanu-Condrei、Marius-Constantin Dinu、Chris Callison-Burch、Sepp Hochreiter。大语言模型可在网络代理任务上自我提升。arXiv 预印本 arXiv:2405.20309, 2024. 10


Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. 5
Shishir G. Patil、毛焕志、季呈杰、严泛嘉、Vishnu Suresh、Ion Stoica、Joseph E. Gonzalez。伯克利函数调用排行榜（BFCL）：从工具使用到对大语言模型的代理性评估。收录于第 42 届国际机器学习大会，2025. 5


Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. 2, 4, 12, 13, 14, 17
秦宇佳、叶一宁、方俊杰、王浩明、梁世豪、田志佐、张俊达、李家豪、李云欣、黄诗珏 等。Ui-tars：以原生代理开创自动化 GUI 交互。arXiv 预印本 arXiv:2501.12326, 2025. 2, 4, 12, 13, 14, 17


Linxin Song, Yutong Dai, Viraj Prabhu, Jieyu Zhang, Taiwei Shi, Li Li, Junnan Li, Silvio Savarese, Zeyuan Chen, Jieyu Zhao, et al. Coact-1: Computer-using agents with coding as actions. arXiv preprint arXiv:2508.03923, 2025. 2
Linxin Song, Yutong Dai, Viraj Prabhu, Jieyu Zhang, Taiwei Shi, Li Li, Junnan Li, Silvio Savarese, Zeyuan Chen, Jieyu Zhao, et al. Coact-1: 以编码为动作的计算机使用代理。arXiv 预印本 arXiv:2508.03923, 2025. 2


Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyou Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2024. 10
Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyou Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. Os-genesis: 通过逆任务合成自动构建 GUI 代理轨迹。arXiv 预印本 arXiv:2412.19723, 2024. 10


Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, et al. Cradle: Empowering foundation agents towards general computer control. arXiv preprint arXiv:2403.03186, 2024. 10
Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, et al. Cradle: 赋能基础代理以实现通用计算机控制。arXiv 预印本 arXiv:2403.03186, 2024. 10


Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, et al. Gui-g2: Gaussian reward modeling for gui grounding. arXiv preprint arXiv:2507.15846, 2025. 12
Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, et al. Gui-g2: 用于 GUI 定位的高斯奖励建模。arXiv 预印本 arXiv:2507.15846, 2025. 12


ByteDance Seed Team. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. 12, 13, 14, 17
ByteDance Seed Team. Seed1.5-vl 技术报告。arXiv 预印本 arXiv:2505.07062, 2025. 12, 13, 14, 17


Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 6
Qwen Team. Qwen2.5 技术报告。arXiv 预印本 arXiv:2412.15115, 2024. 6


Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. arXiv preprint arXiv:2406.01014, 2024a. 2
Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: 通过多代理协作实现高效导航的移动设备操作助手。arXiv 预印本 arXiv:2406.01014, 2024a. 2


Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. Advances in Neural Information Processing Systems, 37:2686-2710, 2024b. 2, 10
Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: 通过多代理协作实现高效导航的移动设备操作助手。Advances in Neural Information Processing Systems, 37:2686-2710, 2024b. 2, 10


Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024c. 14
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: 提升视觉-语言模型对任意分辨率世界感知的能力。arXiv 预印本 arXiv:2409.12191, 2024c. 14


Shuai Wang, Weiwen Liu, Jingxuan Chen, Weinan Gan, Xingshan Zeng, Shuai Yu, Xinlong Hao, Kun Shao, Yasheng Wang, and Ruiming Tang. Gui agents with foundation models: A comprehensive survey. ArXiv preprint, abs/2411.04890, 2024d. URL https://arxiv.org/abs/2411.04890.2
Shuai Wang, Weiwen Liu, Jingxuan Chen, Weinan Gan, Xingshan Zeng, Shuai Yu, Xinlong Hao, Kun Shao, Yasheng Wang, and Ruiming Tang. 基础模型驱动的 GUI 代理：一项综合综述。ArXiv 预印本, abs/2411.04890, 2024d. URL https://arxiv.org/abs/2411.04890.2


Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open foundations for computer-use agents, 2025a. URL https://arxiv.org/abs/2508.09123.2, 4, 14
Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: 面向计算机使用代理的开放基础, 2025a. URL https://arxiv.org/abs/2508.09123.2, 4, 14


Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. Mobile-agent-e: Self-evolving mobile assistant for complex tasks. arXiv preprint arXiv:2501.11733, 2025b. 10, 16, 17
王振海龙, 徐海洋, 王俊阳, 张曦, 闫明, 张霁, 黄飞, 及 纪恒. Mobile-agent-e: 面向复杂任务的自我进化移动助手. arXiv 预印本 arXiv:2501.11733, 2025b. 10, 16, 17


Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024a. 10
吴智勇, 韩成成, 丁子辰, 翁振民, 刘周勉泽, 姚顺宇, 于涛, 及 孔令鹏. Os-copilot: 面向具自我改进的一般性计算机代理. arXiv 预印本 arXiv:2402.07456, 2024a. 10


Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: A foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024b. 10, 12, 13
吴智勇, 吴振宇, 徐方志, 王奕安, 孙秋实, 贾承佑, 程坎志, 丁子辰, 陈力衡, Paul Pu Liang, 等. Os-atlas: 通用图形界面代理的基础动作模型. arXiv 预印本 arXiv:2410.23218, 2024b. 10, 12, 13


Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:52040-52094, 2024. 9, 13
谢天宝, 张丹阳, 陈季轩, 李晓川, 赵思衡, 曹瑞胜, Toh J Hua, 程周俊, 申东灿, 雷方宇, 等. Osworld: 在真实计算机环境中为开放式任务评测多模态代理. Advances in Neural Information Processing Systems, 37:52040-52094, 2024. 9, 13


Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. Scaling computer-use grounding via user interface decomposition and synthesis, 2025. URL https://arxiv.org/abs/2505.13227.2, 12, 13
谢天宝, 邓嘉琦, 李晓川, 杨君林, 吴浩远, 陈季轩, 胡文静, 王新源, 徐宇辉, 王泽坤, 许义衡, 王俊利, Doyen Sahoo, 于涛, 及 熊才明. 通过用户界面分解与合成扩展计算机使用归因, 2025. URL https://arxiv.org/abs/2505.13227.2, 12, 13


Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. 13, 14
许义衡, 王泽坤, 王俊利, 陆敦杰, 谢天宝, Amrita Saha, Doyen Sahoo, 于涛, 及 熊才明. Aguvis: 用于自主 GUI 交互的统一纯视觉代理. arXiv 预印本 arXiv:2412.04454, 2024. 13, 14


Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, et al. Gta1: Gui test-time scaling agent. arXiv preprint arXiv:2507.05791, 2025. 2
杨岩, 李东旭, 戴雨彤, 杨宇浩, 罗子阳, 赵子睿, 胡志远, 黄俊哲, Amrita Saha, 陈泽元, 等. Gta1: GUI 测试时缩放代理. arXiv 预印本 arXiv:2507.05791, 2025. 2


Wenwen Yu, Zhibo Yang, Jianqiang Wan, Sibo Song, Jun Tang, Wenqing Cheng, Yuliang Liu, and Xiang Bai. Omniparser v2: Structured-points-of-thought for unified visual text parsing and its generality to multimodal large language models. arXiv preprint arXiv:2502.16161, 2025. 6, 12
余文文, 杨志博, 万建强, 宋思博, 唐俊, 程文清, 刘昱良, 及 白翔. Omniparser v2: 用于统一视觉文本解析的结构化思维点及其对多模态大语言模型的通用性. arXiv 预印本 arXiv:2502.16161, 2025. 6, 12


Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, et al. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning. arXiv preprint arXiv:2505.12370, 2025. 12
袁新斌, 张健, 李凯鑫, 蔡卓轩, 姚路建, 陈杰, 王恩光, 侯齐斌, 陈晋炜, 蒋鹏涛, 等. 通过自我进化强化学习增强 GUI 代理的视觉定位. arXiv 预印本 arXiv:2505.12370, 2025. 12


Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, et al. Large language model-brained gui agents: A survey. ArXiv preprint, abs/2411.18279, 2024a. URL https://arxiv.org/abs/2411.18279.2
张朝云, 贺世林, 钱家煦, 李博文, 李立群, 秦思, 康宇, 马明华, 林庆伟, Saravan Rajmohan, 等. 大语言模型驱动的 GUI 代理综述. ArXiv 预印本, abs/2411.18279, 2024a. URL https://arxiv.org/abs/2411.18279.2


Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: A ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024b. 10
张朝云, 李立群, 贺世林, 张徐, 乔博, 秦思, 马明华, 康宇, 林庆伟, Saravan Rajmohan, 等. Ufo: 面向 Windows 操作系统交互的 UI 聚焦代理. arXiv 预印本 arXiv:2402.07939, 2024b. 10


Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 1-20, 2025a. 10
张驰, 赵阳, 刘家轩, 李彦达, 韩昱成, 陈昕, 黄泽彪, 傅斌, 余刚. Appagent：作为智能手机用户的多模态代理。在 2025 年 CHI 人机交互大会论文集中，页码 1-20，2025a. 10


Zhong Zhang, Yaxi Lu, Yikun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong, Haotian Chen, Yankai Lin, et al. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning. arXiv preprint arXiv:2506.01391, 2025b. 10
张中, 陆亚曦, 傅艺坤, 霍玉鹏, 杨申志, 吴业赛, 司涵, 丛鑫, 陈浩天, 林彦凯, 等. Agentcpm-gui：用强化微调构建移动使用代理。arXiv 预印本 arXiv:2506.01391，2025b. 10


Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is a generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. 10
郑博远, 苟博宇, 吉智炯, 孙欢, 苏禹. Gpt-4v (ision) 是一个通用网页代理，前提是有落地。arXiv 预印本 arXiv:2401.01614，2024. 10


Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinqlin Jia, et al. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents. arXiv preprint arXiv:2505.15810, 2025. 12
周宇琪, 戴孙浩, 王帅, 周凯文, 贾沁芊, 等. Gui-g1：理解类 R1-zero 训练在 GUI 代理视觉定位中的应用。arXiv 预印本 arXiv:2505.15810，2025. 12


Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 13, 14
朱金国, 王伟云, 陈哲, 刘朝阳, 叶胜龙, 顾力新, 田浩, 段育晨, 苏维杰, 邵杰, 等. Internvl3：探索开源多模态模型的先进训练与测试时策略。arXiv 预印本 arXiv:2504.10479，2025. 13, 14