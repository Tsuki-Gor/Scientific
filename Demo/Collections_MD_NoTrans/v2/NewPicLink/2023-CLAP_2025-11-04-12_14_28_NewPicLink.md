

<!-- Meanless: するのは、コータので、すめますココロンドを-->

# CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts

Yichao Cai , Yuhang Liu , Zhen Zhang , and Javen Qinfeng Shi

Australian Institute for Machine Learning, University of Adelaide, SA 5000, Australia \{yichao.cai,yuhang.liu01,zhen.zhang02,javen.shi\}@adelaide.edu.au

Abstract. Contrastive vision-language models, such as CLIP, have garnered considerable attention for various dowmsteam tasks, mainly due to the remarkable ability of the learned features for generalization. However, the features they learned often blend content and style information, which somewhat limits their generalization capabilities under distribution shifts. To address this limitation, we adopt a causal generative perspective for multimodal data and propose contrastive learning with data augmentation to disentangle content features from the original representations. To achieve this, we begin with exploring image augmentation techniques and develop a method to seamlessly integrate them into pre-trained CLIP-like models to extract pure content features. Taking a step further, recognizing the inherent semantic richness and logical structure of text data, we explore the use of text augmentation to isolate latent content from style features. This enables CLIP-like model's encoders to concentrate on latent content information, refining the learned representations by pre-trained CLIP-like models. Our extensive experiments across diverse datasets demonstrate significant improvements in zero-shot and few-shot classification tasks, alongside enhanced robustness to various perturbations. These results underscore the effectiveness of our proposed methods in refining vision-language representations and advancing the state-of-the-art in multimodal learning. ${}^{1}$

Keywords: Data Augmentation - Latent Variables - Disentanglement

## 1 Introduction

Vision-language models, exemplified by CLIP [36], have garnered substantial attention due to their exceptional generalization capabilities, achieved through the learned features, obtained by utilizing a cross-modal contrastive loss [20, 25, 36]. However, despite being pre-trained on extensive datasets, CLIP-like models fall short in disentangling latent content information and latent style information. Consequently, they are not immune to spurious correlations, i.e., style-related information is erroneously utilized to predict task-related labels. These limitations become evident in the presence of distribution shifts or adversarial attacks where spurious correlations often change across different environments. For examples, (1) a notable dependence on specific input text prompts has been reported for zero-shot capabilities [21, 47, 48]; (2) performance decline in few-shot scenarios has been observed in few-shot learning scenarios [13, 36]; and (3) susceptibility to adversarial attacks has been explored [33, 43, 45].

---

<!-- Footnote -->

${}^{1}$ Our code is available at https://github.com/YichaoCail/CLAP

<!-- Footnote -->

---




<!-- Meanless: 2 Y. Cai et al.-->

<!-- Media -->

<!-- figureText: style change "a photo of a car" "a photo of a chair" "a photo of a dog" (c) CAM visualization examples "a photo of a bird" original CLIP (a) Image augmentation style change augment text augment (b) Text augmentation -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_30_24_67edea.jpg"/>

Fig. 1: Causal generative models of vision-language data. Image and text data are generated through distinct underlying deterministic processes, ${\mathbf{g}}_{\mathbf{x}}$ for images and ${\mathbf{g}}_{\mathbf{t}}$ for texts,derived from a unified latent space with latent content variables $\mathbf{c}$ and latent style variables $\mathbf{s}$ . Latent content $\mathbf{c}$ exclusively determines the sample label $\mathbf{y}$ . (a) Soft interventions on latent style variables $\mathbf{s}$ result in $\widetilde{\mathbf{s}}$ ,subsequently generating augmented images $\widetilde{\mathbf{x}}$ . (b) Due to the same latent space,soft interventions on latent style variables $\mathbf{s}$ can also result in $\widetilde{\mathbf{s}}$ ,producing augmented text $\widetilde{\mathbf{t}}$ . (c) A qualitative comparison of image features for zero-shot classification using "a photo of a [class]" prompts, visualized using class activation map (CAM) 32, demonstrates that while image augmentation can enhance CLIP features, the features obtained through text augmentation methods predominantly focus on the content.

<!-- Media -->

Taking a causal perspective, this work begin with a simple yet effective method, image augmentation, to disentangle content and style information within the learned representations of CLIP-like models. This approach is inspired by recent advancements in theoretical development in causal representation learning [41], which demonstrate that augmented image can be interpreted as a result of soft interventions on latent style variables, as depicted in Fig. 1a. Such augmentation results in a natural data pair where content information remains unchanged while style information changes. Consequently, using contrastive learning, it becomes feasible to isolate the invariant content information from the variant style information. Motivated by this theoretical advancement, we propose a practical method to incorporate image augmentation into CLIP-like models to extract content information from the original learned features. Specifically, a disentangled network is designed to fine-tune the pre-trained CLIP model by using a contrastive loss with image augmentation.

Despite the advancements made in disentangling content and style information from the original features learned by CLIP-like models through image augmentation, we recognize an inherent limitation: it is generally challenging to design adequate image augmentations to ensure all style factors change in an image. Theoretically, disentangling content and style information necessitates changes in all style factors [41]. However, inducing sufficient changes in latent style through image augmentation poses challenges due to the high dimensionality and complexity of style information in image data. Achieving significant style variation via artificially designed image augmentation techniques, such as transforming a photograph of a dog into a sketch while preserving content but dramatically altering style, is notably difficult.


<!-- Meanless: CLAP: Contrastive Learning with Augmented Prompts 3-->

Taking a further step, rather than relying on image augmentation, we explore the use of text augmentation to disentangle latent content and style factors. This shift is motivated by two key observations: 1) Vision and language data share the same latent space. Therefore, text augmentation can also be utilized to induce changes in latent style factors instead of image augmentation. 2) Text data inherently possesses high semanticity and logical structure, making it more amenable to property-wise manipulation compared to image data. Consequently, implementing sufficient style changes through text augmentation is more feasible than image augmentation, contributing to isolating content from style information, see Fig. 1c for visual comparison. For instance, transforming text from "a photo of a dog" to "a sketch of a dog" is straightforward in the language modality, whereas achieving a similar transformation in image data is challenging. Inspired by these observations, we posit that introducing style variations through text augmentation, as illustrated Fig. 1b, provides a more effective approach for learning vision-language content features than relying on image augmentation.

In summary, our contributions include: (1) Aimed at disentangling latent content and style factors to refine vision-language features of pre-trained CLIP-like models, we propose constrastive learning with data augmentation to fine tune the original features of pre-trained CLIP-like models from a causal perspective. (2) We present a novel method customized for pre-trained CLIP-like models. This method leverages a disentangled network, which is trained using contrastive learning with image augmentation, to extract latent content features from the learned features provided by image encoder of CLIP-like models. (3) We propose Contrastive Learning with Augmented Prompts (CLAP), to extract latent content features from representations of CLIP-like models. It begins by training a disentangled network using the pre-trained text encoder of CLIP-like models and text augmentation. Subsequently, the trained disentangled network is transferred to the image encoder of CLIP-like models. (4) Experiments conducted on a large real dataset demonstrate the effectiveness of the proposed image augmentation and text augmentation in terms of zero-shot and few-shot performance, as well as robustness against perturbations.

## 2 Related Work

Contrastive Vision-Language Models Using a cross-modal contrastive loss, CLIP [36] revolutionarily introduced a scalable contrastive vision-language model by leveraging a large corpus of internet-sourced image-text pairs, demonstrating unprecedented zero-shot learning capabilities and exceptional generalization ability across datasets and supporting numerous downstream tasks [38]. ALIGN 20 expanded the scale of contrastive vision-language modeling, training on up to one billion image-text pairs while integrating the vision transformer's self-attention mechanism [11], which further enhanced performance. Despite their successes, CLIP-like models exhibit sensitivity to input text prompts [21, 48, leading to variable performance across different prompts. Efforts to mitigate this prompt sensitivity through prompt learning and engineering 9.14, 21.47 48 focus on customizing prompts for specific tasks but do not fundamentally enhance CLIP's representations. Furthermore, CLIP-like models are vulnerable to adversarial attacks [4, 12], with current strategies [33, 45] involving adversarial-natural image pairs to improve resilience. Our work diverges from task-specific approaches by aiming to enhance CLIP's representations from a disentanglement perspective, addressing the aforementioned issues inherent in CLIP-like models.


<!-- Meanless: 4 Y. Cai et al.-->

Disentangled Representation Learning Aimed at segregating intrinsic latent factors in data into distinct, controllable representations, disentangled representation learning benefits various applications [24, 40, 44]. Specifically, in classification tasks, it's shown that enhancing the model's performance and robustness against data distribution perturbations can be achieved by more effectively disentangling invariant content variables, without needing to identify all intrinsic latent variables completely [22, 26-28]. Within single modalities, studies such as [49] have illustrated that contrastive learning 7.16, 18 can potentially reverse the data generative process, aiding in the separation of representations. Furthermore, 41 suggest that image augmentation can help isolate content variables from the latent space through significant stylistic changes. [19] employs mixture techniques for data augmentation, enabling more abundant cross-modal matches. Diverging from these methods, our approach focuses on employing text augmentation to disentangle latent content variables, introducing a unique approach to learn refined vision-language representations.

## 3 A Causal Generative Model for Multi-Modal Data

To understand pretrained CLIP-like models, we investigate the underlying causal generative process for vision-language data. We consider the following causal generative model as depicted in Fig. 1. In the proposed model, the shared latent space ruling vision and language data is divided into two distinct sub-spaces: one corresponding to the latent content variables $\mathbf{c}$ and the other to the latent style variables $\mathbf{s}$ . The latent content variables are posited to determine the object label $\mathbf{y}$ ,a relationship corroborated by prior studies [22,29,31]. Furthermore,to elucidate the correlation between the latent style variable $\mathbf{s}$ and the object variable $\mathbf{y}$ ,our model incorporates the premise that the latent content variable $\mathbf{c}$ causally influences the latent style variable $\mathbf{s}$ ,in concordance with the principles of causal representation learning highlighted in recent literature [10, 29, 41]. Additionally, considering the diversity between image data and text data, where information in image data is typically much more details while information in text data tends to be more logically structured nature, we posit distinct causal mechanisms for the generation processes. Our causal generative model is formulated as following structural causal models [2]:


<!-- Meanless: CLAP: Contrastive Learning with Augmented Prompts 5-->

$$
\mathbf{s} \mathrel{\text{:=}} {\mathbf{g}}_{\mathbf{s}}\left( \mathbf{c}\right) ,\mathbf{x} \mathrel{\text{:=}} {\mathbf{g}}_{\mathbf{x}}\left( {\mathbf{c},\mathbf{s}}\right) ,\mathbf{t} \mathrel{\text{:=}} {\mathbf{g}}_{\mathbf{t}}\left( {\mathbf{c},\mathbf{s}}\right) ,\mathbf{y} \mathrel{\text{:=}} {\mathbf{g}}_{\mathbf{y}}\left( \mathbf{c}\right) . \tag{1}
$$

In Eq. (1),the style variable $\mathbf{s}$ is causally influenced by the content via ${\mathbf{g}}_{\mathbf{s}};\mathbf{x}$ and $\mathbf{t}$ denote visual and textual data,respectively. Both visual and textual data are causally produced by the shared latent variables $\mathbf{c}$ and $\mathbf{s}$ through distinct, reversible generative processes: ${\mathbf{g}}_{\mathbf{x}}$ for images and ${\mathbf{g}}_{\mathbf{t}}$ for text data,respectively. The label $\mathbf{y}$ of a sample is exclusively determined by the content variable $\mathbf{c}$ via ${\mathbf{g}}_{\mathbf{y}}$ . For simplicity,exogenous noises are implicitly assumed but not explicitly represented in the causal generative model's formulation, aligning with the common understanding that each latent variable is influenced by exogenous noise.

Recent seminal work in [41] has demonstrated that the latent content variable $\mathbf{c}$ can be identified up to block identifiability (i.e., $\mathbf{c}$ can be isolated from style variable $\mathbf{s}$ ),by requiring all latent style variables to change (e.g.,soft interventions on all latent style variables). This change can be achieved through image augmentation,i.e.,the augmented image $\widetilde{\mathbf{x}}$ can be interpreted as a generative result of $\widetilde{\mathbf{s}}$ ,which is produced through soft interventions on original latent style variables $\mathbf{s}$ . Despite such theoretical advancement,the practical implementation of this theoretical result within CLIP-like models remains unclear. In this study, we propose a practical method to disentangle content and style information within CLIP-like models by employing image augmentation, as detailed in Sec. 4.1. Moreover, we recognize that implementing sufficient changes on all latent style variables $\mathbf{s}$ through text augmentation is more feasible than image augmentation, due to high semanticity and logical structure in text data, we delve into the use of text augmentation to separate content information from style information, as discussed in Sec. 4.2.

## 4 Isolating Content from Style with Data Augmentation

In this section, we propose the employment of data augmentation to extract content information from the learned features in pre-trained CLIP-like models. Essentially, data augmentation facilitates the alteration of style factors while preserving content factors. Consequently, leveraging contrastive learning enables the segregation of content information from style information. We delve into two distinct forms of data augmentation, namely image augmentation (Sec. 4.1) and text augmentation (Sec. 4.2).

### 4.1 Isolating Content from Style with Augmented Images

While recent studies (von et al., 2021) have offered assurance regarding the disentanglement of content and style through contrastive learning with data augmentation, it remains unclear how these theoretical findings can be applied to the realm of vision-language models. We convert the theoretical findings into CLIP-like models in the following. The theoretical findings suggest using In-foNCE loss [34] to extract content information, as outlined below:


<!-- Meanless: 6 Y. Cai et al.-->

$$
\mathcal{L}\left( {\mathbf{f};{\left\{  {\mathbf{x}}_{i},{\widetilde{\mathbf{x}}}_{i}\right\}  }_{i = 1}^{b},\tau }\right)  =  - \frac{1}{b}\mathop{\sum }\limits_{{i = 1}}^{b}\log \frac{\exp \left\lbrack  {\left\langle  {\mathbf{f}\left( {\mathbf{x}}_{i}\right) ,\mathbf{f}\left( {\widetilde{\mathbf{x}}}_{i}\right) }\right\rangle  /\tau }\right\rbrack  }{\mathop{\sum }\limits_{{j = 1}}^{b}\exp \left\lbrack  {\left\langle  {\mathbf{f}\left( {\mathbf{x}}_{i}\right) ,\mathbf{f}\left( {\widetilde{\mathbf{x}}}_{j}\right) }\right\rangle  /\tau }\right\rbrack  }, \tag{2}
$$

<!-- Media -->

<!-- figureText: ${\mathrm{f}}_{\mathrm{x}}^{ * }$ InfoNCE InfoNCE ${\mathrm{f}}_{\mathrm{t}}^{ * }$ max similarity ${\mathrm{f}}_{\mathrm{c}}^{ * }$ ${\mathrm{f}}_{\mathrm{x}}^{ * }$ #stop gradient (b) Text augmentation (c) Zero-shot inference augment - stop gradient (a) Image augmentation -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_30_24_7f8fca.jpg"/>

Fig. 2: Refining CLIP through data augmentation. (a) Training involves a disentangled network ${\mathbf{f}}_{\mathbf{c}}$ ,utilizing contrastive loss on original and augmented image pairs $\mathbf{x}$ and $\widetilde{\mathbf{x}}$ , with CLIP’s image encoder ${\mathbf{f}}_{\mathbf{x}}^{ * }$ holding frozen gradients. (b) More efficient content feature learning is achieved through contrastive learning with augmented text prompts $\mathbf{t}$ and $\widetilde{\mathbf{t}}$ ,using the fixed text encoder ${\mathbf{f}}_{\mathbf{t}}^{ * }$ of CLIP. (c) Inference stage: The trained disentangled network ${\mathbf{f}}_{\mathbf{c}}^{ * }$ integrates with CLIP’s text and image encoders, ${\mathbf{f}}_{\mathbf{t}}^{ * }$ and ${\mathbf{f}}_{\mathbf{x}}^{ * }$ ,to enable zero-shot inference for an input image $\mathbf{x}$ and class names ${\mathbf{t}}_{1}$ to ${\mathbf{t}}_{n}$ .

<!-- Media -->

where ${\left\{  {\mathbf{x}}_{i}\right\}  }_{i = 1}^{b}$ represents a batch of $b$ samples from the training dataset, $\mathbf{f}\left( {\mathbf{x}}_{i}\right)$ denotes sample ${\mathbf{x}}_{i}$ ’s features through model $\mathbf{f},{\widetilde{\mathbf{x}}}_{i}$ is the augmented counterpart of ${\mathbf{x}}_{i}$ ,and $\left\langle  {{\mathbf{z}}_{1},{\mathbf{z}}_{2}}\right\rangle$ represents the cosine similarity between two feature vectors, ${\mathbf{z}}_{1}$ and ${\mathbf{z}}_{2}$ ,and $\tau$ represents the temperature parameter influencing the loss.

We extend it to refine pre-trained vision-language models, utilizing contrastive learning with augmented images (hereinafter referred to as "Im.Aug"). As illustrated in Fig. 2a, we train a disentangled network on top of CLIP's pre-trained image encoder. To enhance training efficiency and the usability of the proposed method, we freeze the pre-trained image encoder. Based on an InfoNCE loss, the learning objective of Im. Aug is formulated as follows:

$$
{\mathbf{f}}_{\mathbf{c}}^{ * } = \mathop{\operatorname{argmin}}\limits_{{\mathbf{f}}_{\mathbf{c}}}\underset{{\left\{  {\mathbf{x}}_{i}\right\}  }_{i = 1}^{b} \in  {\mathcal{D}}_{\mathbf{x}}}{\mathbb{E}}\mathcal{L}\left( {{\mathbf{f}}_{\mathbf{c}} \circ  {\mathbf{f}}_{\mathbf{x}}^{ * };{\left\{  {\mathbf{x}}_{i},{\widetilde{\mathbf{x}}}_{i}\right\}  }_{i = 1}^{b},\tau }\right) , \tag{3}
$$

where ${\mathcal{D}}_{\mathbf{x}}$ denotes the training image dataset and $b$ represents the batch size, ${\mathbf{f}}_{\mathbf{c}}$ is the disentangled network undergoing training. The pre-trained CLIP image encoder is represented by ${\mathbf{f}}_{\mathbf{x}}^{ * }$ ,with the asterisk "*" signifying that the model weights remain fixed. The variable ${\mathbf{x}}_{i}$ refers to an image sampled from ${\mathcal{D}}_{\mathbf{x}}$ ,and ${\widetilde{\mathbf{x}}}_{i}$ is its augmented view.

The composition of the training dataset ${\mathcal{D}}_{\mathbf{x}}$ ,the image augmentation techniques used,the structure of the disentangled network ${\mathbf{f}}_{\mathbf{c}}$ ,and the utilization of ${\mathbf{f}}_{\mathbf{c}}^{ * }$ post-training are detailed in the following subsections.

Data Synthesis and Image Augmentation To generate training image data, we combine class names with various image and object attributes to create text prompts for each class. Using a stable diffusion model [39], we produce synthetic images that comprise our training dataset $\mathcal{D}\mathbf{x}$ . The creation of template prompts for stable diffusion is based on attributes such as object size, color, image type, and art style. As detailed in Tab. 1, the attributes include 10 colors and 3 sizes for objects, and 8 types and 2 art styles for images. By assembling these attributes into prompts like "a [art style] [image type] of a [object size] [object color] [class]", we generate 480 unique texts for each class, from which one image per prompt is synthesized. Further details on image synthesis and examples are available in Appendix B.1. For the image augmentation procedures, we adopt techniques commonly used in contrastive learning practice [7, 8, 41], specifically random cropping and color distortion.


<!-- Meanless: CLAP: Contrastive Learning with Augmented Prompts 7-->

<!-- Media -->

Table 1: Template-based prompts. Attributes used to generate text prompts follow the structured format "a [art style] [image type] of a [object size] [object color] [class]", where "[class]" represents the class names.

<table><tr><td>Object Color</td><td>Object Size</td><td>$\mathbf{{ImageType}}$</td><td>Art Style</td></tr><tr><td>yellow, green, black, blue, multicolored, orange. red, white, brown, purple</td><td>large, small, normal sized</td><td>painting, cartoon, infograph, sketch, photograph, clipart, mosaic art, sculpture</td><td>realistic, impressionistic</td></tr></table>

<!-- Media -->

Disentangled Network Structure Since the training process is based on CLIP's pre-trained lower-dimensional features, our disentangled network adopts a multi-layer perceptron (MLP) architecture. To fully benefit from the pre-trained CLIP text encoder, we construct a residual MLP featuring a zero-initialized projection, acting as the disentangled network, as depicted in Fig. 3. This design enables learning directly from the pre-trained representation space, avoiding a random starting point, inspired by ControlNet's zero-conv operation [46], which we adapt to a zero-linear operation within our residual MLP.

Within this architecture, the main branch includes a zero-initialized, bias-free linear layer positioned subsequent to the combination of a SiLU activation and a normally initialized linear layer. Conventionally, the dimensions of features before the initial linear layer, situated between the first and second linear layers,and following the second linear layer,are named as the input ${d}_{in}$ ,latent ${d}_{mid}$ ,and output ${d}_{out}$ dimensions,respectively. To rectify any mismatches between the input and output dimensions, the network employs nearest-neighbor downsampling within the shortcut path, thereby ensuring both alignment and the preservation of sharpness for the input features. During the inference stage, a weighting parameter $\alpha  > 0$ is introduced to modulate the portion of features emanating from the main branch before their integration with the input features, whereas this parameter remains constant at 1 throughout the training phase.

Inference After training,the disentangled network ${\mathbf{f}}_{\mathbf{c}}^{ * }$ is utilized following CLIP's image encoder to extract visual content features. Moreover, given that vision-language data generation is rooted in a unified latent space, as depicted in Sec. 3, ${\mathbf{f}}_{\mathbf{c}}^{ * }$ can be seamlessly integrated with CLIP’s image and text encoders to enhance zero-shot capabilities. As shown in Fig. 2c,for an image $\mathbf{x}$ ,the operation is formulated as the composition function ${\mathbf{f}}_{\mathbf{c}}^{ * } \circ  {\mathbf{f}}_{\mathbf{x}}^{ * }\left( \mathbf{x}\right)$ ,and similarly,for a text $\mathbf{t}$ ,as ${\mathbf{f}}_{\mathbf{c}}^{ * } \circ  {\mathbf{f}}_{\mathbf{t}}^{ * }\left( \mathbf{t}\right)$ . This integration preserves CLIP’s zero-shot functionality while achieving refined features through the improved disentanglement of content.


<!-- Meanless: 8 Y. Cai et al.-->

<!-- Media -->

<!-- figureText: Downsampling Zero Linear Output Features Random-initialized Zero-intialized Input Linear Features Parameter-free -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_30_24_aa5853.jpg"/>

Fig. 3: Structure of the disentangled network. The architecture encompass a residual block featuring a zero-initialized, bias-free linear layer to commence optimization from the input feature space. When the input and output dimension differ, a downsampling operation is utilized to achieve alignment. During inference,a scalar parameter $\alpha$ balance the main branch and input features before combination.

<!-- Media -->

### 4.2 Isolating Content from Style with Augmented Prompts

Despite progress in disentangling content and style via image augmentation, adequately altering all style factors in an image remains challenging due to the high dimensionality and complexity of style information in images. Achieving substantial style changes through augmentation, essential for complete disentanglement [41], is difficult with existing image augmentation techniques. On the contrary, text data inherently possesses high semanticity and logical structure, making it more amenable to property-wise manipulation compared to image data. To further exploring the disentanglement of content, we propose Contrastive Learning with Augmented Prompts (CLAP).

As depicted in Fig. 2b, CLAP employs an InfoNCE loss to train a disentangled network atop CLIP's pre-trained text encoder, keeping the encoder's gradients fixed, similar to Im.Aug. Leveraging the simpler structure of text, the template-based prompts previously utilized for synthesizing images now serve as the training text dataset,denoted by ${\mathcal{D}}_{\mathbf{t}}$ . Utilizing the same disentangled network as in Im.Aug, the learning objective of CLAP is outlined as follows:

$$
{\mathbf{f}}_{\mathbf{c}}^{ * } = \mathop{\operatorname{argmin}}\limits_{{\mathbf{f}}_{\mathbf{c}}}\mathbb{E}{\mathbb{E}}_{{\left\{  {\mathbf{t}}_{i}\right\}  }_{i = 1}^{b} \in  {\mathcal{D}}_{\mathbf{t}}}\mathcal{L}\left( {{\mathbf{f}}_{\mathbf{c}} \circ  {\mathbf{f}}_{\mathbf{t}}^{ * };{\left\{  {\mathbf{t}}_{i},{\widetilde{\mathbf{t}}}_{i}\right\}  }_{i = 1}^{b},\tau }\right)  + \lambda \mathcal{L}\left( {{\mathbf{f}}_{\mathbf{c}} \circ  {\mathbf{f}}_{\mathbf{t}}^{ * };{\left\{  {\mathbf{t}}_{i}^{c},{\widetilde{\mathbf{t}}}_{i}\right\}  }_{i = 1}^{b},1}\right) , \tag{4}
$$

where ${\mathbf{f}}_{\mathbf{t}}^{ * }$ denotes the pre-trained CLIP text encoder. The term ${\mathbf{t}}_{i}$ references a text prompt from $\mathcal{D}\mathbf{t}$ ,and ${\widetilde{\mathbf{t}}}_{i}$ represents its augmented view,produced via prompt augmentation techniques. On the equation’s right side, ${\mathbf{t}}_{i}^{c}$ specifies the class name associated with the text prompt ${\mathbf{t}}_{i}$ . This strategy aims to enhance variations between prompt pairs,especially in cases where the text dataset ${\mathcal{D}}_{\mathbf{t}}$ has a very limited number of samples. Here, $\lambda$ serves for adjusting the second term's importance in the total loss function. All other symbols in Eq. 4 maintain their definitions as described earlier.


<!-- Meanless: CLAP: Contrastive Learning with Augmented Prompts 9-->

<!-- Media -->

Table 2: Prompt augmentation techniques. Various augmented views are generated from an original text prompt using specific augmentation techniques: OSD (Object Size Deletion), OCD (Object Color Deletion), ITD (Image Type Deletion), ASD (Art Style Deletion), and SPO (Swapping Prompt Order).

<table><tr><td>Original</td><td>OSD</td><td>OCD</td><td>ITD</td><td>ASD</td><td>SPO</td></tr><tr><td>a realistic painting of a large red car</td><td>a realistic painting of a red car</td><td>a realistic painting of a large car</td><td>a realistic of a large red car</td><td>a painting of a large red car</td><td>a large red car in a realistic painting</td></tr></table>

<!-- Media -->

After training, the learned disentangled network is seamlessly integrated with both of CLIP's encoders to extract content representations, as depicted in Fig. 2c.

Prompt Augmentation To ensure text prompts undergo stylistic changes without compromising their content, we have developed specific augmentation techniques for synthetic text prompts. Drawing inspiration from Easy Data Augmentation (EDA) techniques [42], we adapted the Random Deletion (RD) and Random Swap (RS) techniques from EDA, customizing them to suit our prompt structure. To avoid inadvertently altering the content by introducing new object names or changing the core idea of a text prompt, our augmentation methods do not include random word insertions or replacements. Our primary augmentation techniques are Object Size Deletion (OSD), Object Color Deletion (OCD), Image Type Deletion (ITD), Art Style Deletion (ASD), and Swapping Prompt Order (SPO), each applied with a certain probability, as detailed in Tab. 2. Additionally, for down-stream datasets with few categories, to rich the population of training samples, we use an additional augmentation, named IGN (Inserting Gaussian Noise). Following the initializing protocol of prompt learning methods 47, 48, we insert a zero-mean Gaussian noise with 0.02 standard deviation with a noise length equals to 4 , to the tokenized prompts.

Intuitively, these prompt augmentation methods parallel random masking techniques used in image augmentation [6, 17]. However, prompt augmentations are more effective and precise than their image counterparts. This effectiveness arises because prompt augmentations can specifically target and eliminate a particular style element without impacting the content, whereas image masking, operating at the pixel or patch level, might inadvertently damage content information or lead to insufficient style changes.

## 5 Experiments

We conduct three primary experiments to assess our method: (1) zero-shot evaluation with diverse prompts to gauge zero-shot performance and its robustness to prompt perturbations; (2) linear probe tests on few-shot samples to evaluate the efficacy of the learned representations in few-shot settings; and (3) adversarial attack assessments on zero-shot and one-shot classifiers to determine their resistance to adversarial threats. We further conduct an ablative study on hyper-parameters, explore the impact of different prompt augmentation combinations and various sources of training prompts on CLAP's performance, and replicate experiments across different CLIP model sizes.


<!-- Meanless: 10 Y. Cai et al.-->

### 5.1 Experimental Setup

Implementation. Im. Aug and CLAP are implemented using the ViT-B/16 CLIP model and executed on an NVIDIA RTX 3090 GPU. To ensure reproducibility, the random seed for all stochastic processes is fixed at 2024. More information on implementation details is provided in Appendix A.1

Datasets. CLAP is assessed across four multi-domain datasets to examine its performance in varied environments: PACS [23] ( 4 domains, 7 categories), VLCS 1 (4 domains, 5 categories), OfficeHome [37] (4 domains, 65 categories), and DomainNet [35] ( 6 domains, 345 categories). For conciseness, we present average results across the domains for each dataset. Detailed experimental outcomes for each domain within these datasets are provided in Appendix A.4.

Compute efficiency. CLAP demonstrates faster convergence and shorter training times compared to Im.Aug. For CLAP, training on the PACS and VLCS datasets is completed in roughly 11 minutes, OfficeHome in approximately 14 minutes, and DomainNet in about 47 minutes. In contrast, Im.Aug requires around 16 minutes for PACS and VLCS, 50 minutes for OfficeHome, and 3.3 hours for DomainNet. Both Im.Aug and CLAP maintain CLIP's inference efficiency due to the disentangled network's efficient two-layer MLP structure.

### 5.2 Main Results

Zero-Shot Performance To assess zero-shot capabilities, CLAP undergoes evaluation using three specific fixed prompts: $\operatorname{ZS}\left( \mathrm{C}\right)$ ,utilizing only the class name within "[class]"; ZS(PC), with the format "a photo of a [class]"; and ZS(CP), structured as "a [class] in a photo". To thoroughly examine zero-shot proficiency, a dynamic prompt, $\mathrm{{ZS}}\left( \mathrm{{NC}}\right)$ ,formatted as "[noise][class]",is also used,where "[noise]" signifies the introduction of Gaussian noise characterized by a mean of 0 and a standard deviation of 0.02 .

As presented in Tab. 3, CLAP surpasses both CLIP and Im.Aug across all evaluated prompts for every dataset. Unlike the uniform enhancement in zero-shot performance CLAP achieves over CLIP, Im.Aug displays inconsistent results. A closer examination reveals CLAP's superiority over CLIP is especially significant for the dynamic ZS(NC) prompt. This demonstrates CLAP’s effectiveness in significantly improving zero-shot performance compared to the original CLIP representations.

In assessing the model's robustness to prompt perturbations, we examine the variances in zero-shot performance across different prompts by analyzing the range(R)and standard deviation $\left( \delta \right)$ of results derived from $\mathrm{{ZS}}\left( \mathrm{C}\right) ,\mathrm{{ZS}}\left( \mathrm{{CP}}\right)$ , and $\mathrm{{ZS}}\left( \mathrm{{PC}}\right)$ . Additionally,we investigate the decline $\left( {\Delta }_{\left( NC\right) }\right)$ in performance from $\mathrm{{ZS}}\left( \mathrm{C}\right)$ to $\mathrm{{ZS}}\left( \mathrm{{NC}}\right)$ as a broad indicator of resilience to noised prompts.


<!-- Meanless: CLAP: Contrastive Learning with Augmented Prompts 11-->

<!-- Media -->

Table 3: Zero-shot results across three distinct prompts: "C" for "[class]", "CP" for "a [class] in a photo", "PC" for "a photo of a [class]", and a dynamic prompt "NC" for "[noise][class]" showcase that CLAP consistently outperforms CLIP's zero-shot performance across all datasets, whereas image augmentation exhibits mixed outcomes.

<table><tr><td rowspan="2">Prompt</td><td rowspan="2"/><td colspan="5">Zero-shot performance, avg. 1top-1 acc.(%) (↑)</td></tr><tr><td>PACS</td><td>VLCS</td><td>Off.Home</td><td>Dom.Net</td><td>Overall</td></tr><tr><td rowspan="3">ZS(C)</td><td>CLIP</td><td>95.7</td><td>76.4</td><td>79.8</td><td>57.8</td><td>77.4</td></tr><tr><td>Im.Aug</td><td>96.5</td><td>79.5</td><td>77.0</td><td>51.5</td><td>76.1</td></tr><tr><td>CLAP</td><td>97.2</td><td>82.6</td><td>81.0</td><td>58.7</td><td>79.9</td></tr><tr><td rowspan="3">ZS(CP)</td><td>CLIP</td><td>95.2</td><td>82.0</td><td>79.5</td><td>57.0</td><td>78.4</td></tr><tr><td>Im.Aug</td><td>96.3</td><td>82.9</td><td>75.8</td><td>50.7</td><td>76.4</td></tr><tr><td>CLAP</td><td>97.3</td><td>83.4</td><td>80.5</td><td>58.0</td><td>79.8</td></tr><tr><td rowspan="3">ZS(PC)</td><td>CLIP</td><td>96.1</td><td>82.4</td><td>82.5</td><td>57.7</td><td>79.7</td></tr><tr><td>Im.Aug</td><td>96.5</td><td>83.0</td><td>78.6</td><td>51.6</td><td>77.4</td></tr><tr><td>CLAP</td><td>97.2</td><td>83.4</td><td>83.0</td><td>59.0</td><td>80.6</td></tr><tr><td rowspan="3">ZS(NC)</td><td>CLIP</td><td>90.8</td><td>68.3</td><td>71.5</td><td>51.0</td><td>70.4</td></tr><tr><td>Im.Aug</td><td>94.8</td><td>73.1</td><td>67.5</td><td>44.0</td><td>69.9</td></tr><tr><td>CLAP</td><td>97.2</td><td>81.0</td><td>73.5</td><td>52.6</td><td>76.1</td></tr></table>

<!-- Media -->

As presented in Tab. 4, CLAP significantly reduces the variance in zero-shot performance across various testing prompts, evidenced by markedly lower values of $\delta$ and $R$ ,and a less pronounced decrease in performance with a noised prompt, in contrast to Im.Aug and the baseline representations of CLIP. Although Im. Aug aids in reducing performance variance to some extent, its efficacy is notably inferior to that of CLAP. These findings highlight CLAP's enhanced robustness in maintaining consistent zero-shot performance across a diverse array of prompts.

Few-Shot Performance We conduct evaluations of 1-shot, 4-shot, 8-shot, and 16-shot linear probes across each domain within the four datasets. As illustrated in Fig. 4, CLAP significantly outperforms both CLIP and Im.Aug in few-shot learning scenarios. Notably, in the 1-shot setting CLAP achieves performance improvements over the linear-probe CLIP model by margins of $+ {10}\% , + {3.5}\%$ , $+ {2.5}\%$ ,and $+ {1.5}\%$ on the PACS,VLCS,OfficeHome,and DomainNet datasets, respectively. These improvements are especially significant in comparison to the gains observed with Im.Aug counterparts, underpinning CLAP's efficacy in few-shot scenarios. For detailed quantitative results, please refer to Appendix A.4,

Adversarial Performance To assess adversarial robustness, zero-shot (ZS(C)) and one-shot classifiers are evaluated against prominent adversarial attack methods, such as FGSM [15, PGD 30, and CW 5, by generating adversarial samples for testing. For FGSM, 1 adversarial iteration is employed, whereas for PGD and CW, 20 iterations are used, all with an epsilon of 0.031 . As indicated in Tab. 5, classifiers utilizing CLAP representations demonstrate superior resilience to these adversarial attacks compared to those based on CLIP representations. Across the four datasets, CLAP's zero-shot and 1-shot classifiers surpass CLIP by margins of $+ {7.6}\%$ and $+ {8.5}\%$ against FGSM, $+ {1.0}\%$ and $+ {11.7}\%$ against PGD-20,and +1.1% and +2.3% against CW-20, respectively. These figures notably exceed the performance improvements of $+ {4.4}\%$ and $+ {4.6}\%$ against FGSM, +0.3% and +6.2% against PGD-20, and 0% and +1.3% against CW-20 achieved by Im.Aug. The result suggests that CLAP efficiently enhances robustness against adversarial attacks in both zero-shot and one-shot scenarios.


<!-- Meanless: 12 Y. Cai et al.-->

<!-- Media -->

Table 4: CLAP more effectively reduces zero-shot performance variance across prompts than image augmentation,with $R$ and $\delta$ indicating the range and standard deviation for $\mathrm{{ZS}}\left( \mathrm{C}\right) ,\mathrm{{ZS}}\left( \mathrm{{CP}}\right)$ ,and $\mathrm{{ZS}}\left( \mathrm{{PC}}\right)$ . The decrease ${\Delta }_{\left( NC\right) }$ from $\mathrm{{ZS}}\left( \mathrm{C}\right)$ to $\mathrm{{ZS}}\left( \mathrm{{NC}}\right)$ highlights CLAP's enhanced robustness against prompt perturbations.

<table><tr><td rowspan="2"/><td rowspan="2">Metric Method</td><td colspan="5">Performance variance,avg. top-1 acc. (%) (↓)</td></tr><tr><td>PACS</td><td>VLCS</td><td>Off.Home</td><td>Dom.Net</td><td>Overall</td></tr><tr><td rowspan="3">$R$</td><td>CLIP</td><td>0.9</td><td>6.1</td><td>3.1</td><td>0.8</td><td>2.7</td></tr><tr><td>Im.Aug</td><td>0.1</td><td>3.6</td><td>2.8</td><td>0.9</td><td>1.9</td></tr><tr><td>CLAP</td><td>0.1</td><td>0.8</td><td>2.5</td><td>1.0</td><td>1.1</td></tr><tr><td rowspan="3">$\delta$</td><td>CLIP</td><td>0.4</td><td>2.8</td><td>1.4</td><td>0.4</td><td>1.2</td></tr><tr><td>Im.Aug</td><td>0.1</td><td>1.7</td><td>1.2</td><td>0.4</td><td>0.8</td></tr><tr><td>CLAP</td><td>0.0</td><td>0.4</td><td>1.1</td><td>0.4</td><td>0.5</td></tr><tr><td rowspan="3">${\Delta }_{\left( NC\right) }$</td><td>CLIP</td><td>4.9</td><td>8.1</td><td>8.3</td><td>6.8</td><td>7.0</td></tr><tr><td>Im.Aug</td><td>1.6</td><td>6.4</td><td>9.5</td><td>7.5</td><td>6.3</td></tr><tr><td>CLAP</td><td>0.0</td><td>1.6</td><td>7.5</td><td>6.1</td><td>3.8</td></tr></table>

<!-- figureText: CLAP-ZS Im.Aus 16 8 #of shots per class in OfficeHome #of shots per class in DomainNet JCLIP-ZS 16 #of shots per class in PACS #of shots per class in VLCS -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_30_24_5f7b0c.jpg"/>

Fig. 4: Few-shot linear probe comparisons of image-encoder features show that CLAP enhances CLIP's few-shot performance more effectively than Im.Aug. In the accompanying figure, "ZS" indicates the zero-shot performance using a "[class]" prompt.

<!-- Media -->

### 5.3 More Analysis

t-SNE Visualization In our t-SNE visualizations, we examine the representations of CLIP, Im.Aug, and CLAP for all images within the Art Painting domain of the PACS dataset. Fig. 5 shows that CLAP's image representations display a marked inter-class separation and tighter intra-class clustering than those of


<!-- Meanless: CLAP: Contrastive Learning with Augmented Prompts 13-->

<!-- Media -->

Table 5: Image augmentation and CLAP both enhance CLIP's zero-shot with the "[class]" prompt and 1-shot robustness against adversarial attacks, with CLAP showing greater improvements.

<table><tr><td colspan="2" rowspan="3">Setting Method</td><td colspan="13">Avg. top-1 acc. (%) under adversarial attacks(↑)</td></tr><tr><td colspan="4">FGSM</td><td colspan="4">PGD-20</td><td colspan="4">CW-20</td><td rowspan="2">Avg.</td></tr><tr><td>PACS</td><td>VLCS</td><td>O.H.</td><td>D.N.</td><td>PACS</td><td>VLCS</td><td>O.H.</td><td>D.N.</td><td>PACS</td><td>VLCS</td><td>O.H.</td><td>D.N.</td></tr><tr><td rowspan="3">ZS(C)</td><td>CLIP</td><td>86.8</td><td>65.6</td><td>57.9</td><td>22.5</td><td>29.1</td><td>2.0</td><td>10.1</td><td>10.7</td><td>27.4</td><td>1.5</td><td>7.4</td><td>7.6</td><td>29.2</td></tr><tr><td>Im.Aug</td><td>88.0</td><td>69.6</td><td>55.1</td><td>37.9</td><td>31.3</td><td>2.1</td><td>10.4</td><td>9.0</td><td>29.4</td><td>1.7</td><td>7.0</td><td>5.8</td><td>31.1</td></tr><tr><td>CLAP</td><td>88.7</td><td>71.9</td><td>58.5</td><td>44.2</td><td>30.8</td><td>3.2</td><td>10.6</td><td>11.2</td><td>29.8</td><td>2.3</td><td>8.1</td><td>8.0</td><td>32.7</td></tr><tr><td rowspan="3">1-shot</td><td>CLIP</td><td>66.7</td><td>45.2</td><td>34.3</td><td>22.5</td><td>34.8</td><td>16.0</td><td>5.6</td><td>11.3</td><td>18.9</td><td>0.7</td><td>4.5</td><td>3.2</td><td>23.7</td></tr><tr><td>Im. Aug</td><td>79.4</td><td>47.1</td><td>37.1</td><td>23.5</td><td>55.2</td><td>16.1</td><td>8.5</td><td>12.5</td><td>23.2</td><td>0.9</td><td>5.1</td><td>3.4</td><td>28.0</td></tr><tr><td>CLAP</td><td>89.6</td><td>52.2</td><td>37.1</td><td>23.9</td><td>73.4</td><td>21.2</td><td>7.4</td><td>12.5</td><td>27.0</td><td>1.1</td><td>5.0</td><td>3.5</td><td>31.9</td></tr></table>

<!-- figureText: (a) CLIP (b) Im. Aug (c) CLAP -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_30_24_43ba93.jpg"/>

Fig. 5: t-SNE visualizations of all images in the Art Painting of PACS dataset show CLAP outperforms the original CLIP and Im.Aug, with clearer inter-class distinctions and tighter intra-class clusters.

<!-- Media -->

CLIP and Im.Aug. This observation suggests that CLAP's representations are more closely tied to content information and less influenced by style information, in contrast to the other two.

Ablations In Fig. 6, we assess the zero-shot capabilities of our model using two distinct prompts, $\overline{\mathrm{{ZS}}}\left( \mathrm{C}\right)$ and $\overline{\mathrm{{ZS}}}\left( \mathrm{{PC}}\right)$ ,on the VLCS dataset. This analysis forms part of an ablative study aimed at understanding the influence of various hyper-parameters on model performance. Specifically, we examine: the dimensions of the latent layer within the MLP of the disentangled network, as illustrated in Fig. 6a; the temperature parameter $\left( \tau \right)$ in the loss function,as depicted in Fig. 6b and the weight coefficient $\left( \alpha \right)$ during the inference stage,as shown in Fig. 6c. Our findings indicate that CLAP consistently enhances zero-shot performance across all tested configurations for both prompts, while also significantly reducing the gap between the performances elicited by each prompt. These results underscore the efficacy of CLAP in accommodating a wide range of hyper-parameters.

Prompt Augmentation Combinations We explore diverse combinations of our tailored prompt augmentation methods and examine Easy Data Augmentation (EDA) techniques [42] on the VLCS dataset. Each tested technique showcases CLAP's enhancements over CLIP, with details available in Appendix A.2.


<!-- Meanless: 14 Y. Cai et al.-->

<!-- Media -->

<!-- figureText: Aug. top-1 acc. (%) Avg. top 1 acc. (%) CUP-ZS(C) CUP-ZS(C) CLAP-ZS(PC - CLAP-ZS(PC 0.5 0.7 0.9 -0 : 0.0 0.5 1.0 15 (b) Temperature $\tau$ in loss (c) Inference weight ${\log }_{10}\left( \alpha \right)$ 370 384 448 512 (a) Latent dimensions -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_30_24_224129.jpg"/>

Fig. 6: We conduct ablative study on hyper-parameter choices on the VLCS dataset, including latent dimensions, $\tau$ values,and $\alpha$ values during the inference stage. CLAP continuously enhance CLIP's performance throughout the tested values.

<!-- Media -->

Prompt Sources We assess the impact of different training prompt formats, originating from various synthetic sources, on the performance of the VLCS dataset, incorporating EDA techniques. Our evaluation includes our template-based prompts, LLM-generated prompts by ChatGPT-3.5 [3] (with the generation process detailed in Appendix B.2), prompts structured as "a [random] style of [class]," where "[random]" is filled with terms from a random word generator ${}^{2}$ , and prompts produced using the PromptStyler method [9]. The findings indicate that the training prompts with simpler forms tend to yield better performance, with detailed quantitative results presented in Appendix A.3.

Experiments on Different Model Scales In our repeated experiments assessing zero-shot performance on the ViT-L/14 and ResNet50x16 pre-trained with CLIP, we consistently find that CLAP improves zero-shot performance while also reducing performance variances. This consistent observation underscores CLAP's effectiveness in enhancing the quality of CLIP representations. For quantitative details supporting these findings, please see the Appendix C

## 6 Conclusion

To enhance pre-trained CLIP-like models, this study delves into disentangling latent content variables. Through a causal analysis of the underlying generative processes of vision-language data, we discover that training a disentangled network in one modality can effectively disentangle content across both modalities. Given the high semantic nature of text data, we identify that disentanglement is more achievable within the language modality through text augmentation interventions. Building on these insights, we introduce CLAP (Contrastive Learning with Augmented Prompts) to acquire disentangled vision-language content features. Comprehensive experiments validate CLAP's effectiveness, demonstrating significant improvements in zero-shot and few-shot performance, and enhancing robustness against perturbations. We anticipate that our work will inspire further exploration into disentangling latent variables within vision-language models. References

---

<!-- Footnote -->

2 https://github.com/vaibhavsingh97/random-word

<!-- Footnote -->

---




<!-- Meanless: CLAP: Contrastive Learning with Augmented Prompts 15-->

1. Albuquerque, I., Monteiro, J., Darvishi, M., Falk, T.H., Mitliagkas, I.: Generalizing to unseen domains via distribution matching. arXiv preprint arXiv:1911.00804 (2019). https://doi.org/10.48550/arXiv.1911.00804

2. Bollen, K.A.: Structural equations with latent variables, vol. 210. John Wiley & Sons (1989). https://doi.org/10.1002/9781118619179

3. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-lakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems 33, 1877-1901 (2020), https://dl.acm.org/doi/abs/10.5555/3495724.3495883

4. Carlini, N., Terzis, A.: Poisoning and backdooring contrastive learning. In: International Conference on Learning Representations (2021), https://openreview.net/forum?id=iC4UHbQ01Mp

5. Carlini, N., Wagner, D.: Towards evaluating the robustness of neural networks. In: 2017 IEEE Symposium on Security and Privacy (SP). pp. 39-57. IEEE Computer Society (2017). https://doi.org/10.1109/SP.2017.49

6. Chen, P., Liu, S., Zhao, H., Jia, J.: Gridmask data augmentation. arXiv preprint arXiv:2001.04086 (2020). https://doi.org/10.48550/arXiv.2001.04086

7. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: International conference on machine learning. pp. 1597-1607. PMLR (2020), https://dl.acm.org/doi/abs/10.5555/ 3524938.3525087

8. Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 15750-15758 (2021). https://doi.org/10.1109/CVPR46437.2021.01549

9. Cho, J., Nam, G., Kim, S., Yang, H., Kwak, S.: Promptstyler: Prompt-driven style generation for source-free domain generalization. In: Proceedings of the IEEE international conference on computer vision. pp. 15702-15712 (2023). https: //doi.org/10.1109/ICCV51070.2023.01439

10. Daunhawer, I., Bizeul, A., Palumbo, E., Marx, A., Vogt, J.E.: Identifiability results for multimodal contrastive learning. ICLR (2023), https://openreview.net/ forum?id=U_2kuqoTcB

11. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2020), https://openreview.net/forum?id= YicbFdNTTy

12. Fort, S.: Adversarial vulnerability of powerful near out-of-distribution detection. arXiv preprint arXiv:2201.07012 (2022). https://doi.org/10.48550/arXiv.2201.07012

13. Gao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y., Li, H., Qiao, Y.: Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision $\mathbf{{132}}\left( 2\right) ,{581} - {595}\left( {2024}\right)$ . https://doi.org/10.1007/ s11263-023-01891-x

14. Ge, C., Huang, R., Xie, M., Lai, Z., Song, S., Li, S., Huang, G.: Domain adaptation via prompt learning. arXiv preprint arXiv:2202.06687 (2022). https://doi.org/ 10.48550/arXiv.2202.06687

15. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial examples. In: International Conference on Learning Representations (2015). https: //doi.org/10.48550/arXiv.1412.6572


<!-- Meanless: 16 Y. Cai et al.-->

16. Grill, J.B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., Do-ersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al.: Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems 33, 21271-21284 (2020), https://dl.acm.org/doi/abs/10.5555/3495724.3497510

17. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 16000-16009 (2022). https://doi.org/10.1109/CVPR52688.2022.01553

18. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised visual representation learning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9729-9738 (2020). https://doi.org/10.1109/CVPR42600.2020.00975

19. Hong, T., Guo, X., Ma, J.: Itmix: Image-text mix augmentation for transferring clip to image classification. In: 2022 16th IEEE International Conference on Signal Processing (ICSP). vol. 1, pp. 129-133. IEEE (2022). https://doi.org/10.1109/ ICSP56322.2022.9965292

20. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H., Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning with noisy text supervision. In: International conference on machine learning. pp. 4904-4916. PMLR (2021), https://proceedings.mlr.press/v139/jia21b.html

21. Khattak, M.U., Rasheed, H., Maaz, M., Khan, S., Khan, F.S.: Maple: Multi-modal prompt learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19113-19122 (2023). https://doi.org/10.1109/CVPR52729.2023.01832

22. Kong, L., Xie, S., Yao, W., Zheng, Y., Chen, G., Stojanov, P., Akinwande, V., Zhang, K.: Partial disentanglement for domain adaptation. In: International Conference on Machine Learning. pp. 11455-11472. PMLR (2022), https:// proceedings.mlr.press/v162/kong22a.html

23. Li, D., Yang, Y., Song, Y.Z., Hospedales, T.M.: Deeper, broader and artier domain generalization. In: Proceedings of the IEEE international conference on computer vision. pp. 5542-5550 (2017). https://doi.org/10.1109/ICCV.2017.591

24. Li, H., Wang, X., Zhang, Z., Yuan, Z., Li, H., Zhu, W.: Disentangled contrastive learning on graphs. Advances in Neural Information Processing Systems 34, 21872- 21884 (2021), https://dl.acm.org/doi/10.5555/3540261.3541935

25. Li, Y., Liang, F., Zhao, L., Cui, Y., Ouyang, W., Shao, J., Yu, F., Yan, J.: Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. In: International Conference on Learning Representations (2021)

26. Liu, Y., Zhang, Z., Gong, D., Gong, M., Huang, B., van den Hengel, A., Zhang, K., Shi, J.Q.: Identifiable latent polynomial causal models through the lens of change. In: The Twelfth International Conference on Learning Representations (2024), https://openreview.net/forum?id=ia9fKO1Vjq

27. Liu, Y., Zhang, Z., Gong, D., Gong, M., Huang, B., Hengel, A.v.d., Zhang, K., Shi, J.Q.: Identifying weight-variant latent causal models. arXiv preprint arXiv:2208.14153 (2022). https://doi.org/10.48550/arXiv.2208.14153

28. Liu, Y., Zhang, Z., Gong, D., Gong, M., Huang, B., Hengel, A.v.d., Zhang, K., Shi, J.Q.: Identifiable latent neural causal models. arXiv preprint arXiv:2403.15711 (2024). https://doi.org/10.48550/arXiv.2403.15711

29. Liu, Y., Zhang, Z., Gong, D., Gong, M., Huang, B., Zhang, K., Shi, J.Q.: Identifying latent causal content for multi-source domain adaptation. arXiv preprint arXiv:2208.14161 (2022). https://doi.org/10.48550/arXiv.2208.14161


<!-- Meanless: CLAP: Contrastive Learning with Augmented Prompts 17-->

30. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning models resistant to adversarial attacks. In: International Conference on Learning Representations (2018), https://openreview.net/forum?id=rJzIBfZAb

31. Mahajan, D., Tople, S., Sharma, A.: Domain generalization using causal matching. In: International Conference on Machine Learning. pp. 7313-7324. PMLR (2021), https://proceedings.mlr.press/v139/mahajan21b.html

32. Mamooler, S.: Clip explainability. https://github.com/sMamooler/CLIP_ Explainability, accessed: 2024-03-06

33. Mao, C., Geng, S., Yang, J., Wang, X., Vondrick, C.: Understanding zero-shot adversarial robustness for large-scale models. In: The Eleventh International Conference on Learning Representations (2022), https://openreview.net/forum?id= P4bXCawRi5J

34. Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). https://doi.org/10.48550/ arXiv.1807.03748

35. Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., Wang, B.: Moment matching for multi-source domain adaptation. In: Proceedings of the IEEE international conference on computer vision. pp. 1406-1415 (2019). https://doi.org/10.1109/ ICCV.2019.00149

36. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021), https://proceedings.mlr.press/v139/radford21a.html

37. Rahman, M.M., Fookes, C., Baktashmotlagh, M., Sridharan, S.: Multi-component image translation for deep domain generalization. In: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 579-588. IEEE (2019). https://doi.org/10.1109/WACV.2019.00067

38. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: Zero-shot text-to-image generation. In: International Conference on Machine Learning. pp. 8821-8831. PMLR (2021), https://proceedings.mlr.press/v139/ramesh21a.html

39. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022). https://doi.org/10.1109/CVPR52729.2023.01389

40. Sanchez, E.H., Serrurier, M., Ortner, M.: Learning disentangled representations via mutual information estimation. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXII 16. pp. 205-221. Springer (2020). https://doi.org/10.1007/978-3-030-58542-6_13

41. Von Kügelgen, J., Sharma, Y., Gresele, L., Brendel, W., Schölkopf, B., Besserve, M., Locatello, F.: Self-supervised learning with data augmentations provably isolates content from style. Advances in neural information processing systems34, 16451-16467 (2021), https://dl.acm.org/doi/10.5555/3540261.3541519

42. Wei, J., Zou, K.: Eda: Easy data augmentation techniques for boosting performance on text classification tasks. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 6382-6388 (2019). https://doi.org/10.18653/v1/D19-1670


<!-- Meanless: 18 Y. Cai et al.-->

43. Wortsman, M., Ilharco, G., Kim, J.W., Li, M., Kornblith, S., Roelofs, R., Lopes, R.G., Hajishirzi, H., Farhadi, A., Namkoong, H., et al.: Robust fine-tuning of zero-shot models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7959-7971 (2022). https://doi.org/10.1109/ CVPR52688.2022.00780

44. Yang, M., Liu, F., Chen, Z., Shen, X., Hao, J., Wang, J.: Causalvae: Disentangled representation learning via neural structural causal models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9593-9602 (2021). https://doi.org/10.1109/CVPR46437.2021.00947

45. Yang, W., Mirzasoleiman, B.: Robust contrastive language-image pretraining against adversarial attacks. arXiv preprint arXiv:2303.06854 (2023). https://doi.org/10.48550/arXiv.2303.06854

46. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: IEEE International Conference on Computer Vision (ICCV) (2023). https://doi.org/10.1109/ICCV51070.2023.00355

47. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional prompt learning for vision-language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16816-16825 (2022). https://doi.org/10.1109/CVPR52688.2022.01631

48. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language models. International Journal of Computer Vision $\mathbf{{130}}\left( 9\right) ,{2337} - {2348}\left( {2022}\right)$ . https://doi.org/10.1007/s11263-022-01653-1

49. Zimmermann, R.S., Sharma, Y., Schneider, S., Bethge, M., Brendel, W.: Contrastive learning inverts the data generating process. In: International Conference on Machine Learning. pp. 12979-12990. PMLR (2021), https://proceedings.mlr.press/v139/zimmermann21a.html


# CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts APPENDIX

Yichao Cai , Yuhang Liu , Zhen Zhang , and Javen Qinfeng Shi

Australian Institute for Machine Learning, University of Adelaide, SA 5000, Australia \{yichao.cai,yuhang.liu01,zhen.zhang02,javen.shi\}@adelaide.edu.au

## Overview of the Appendix:

- More details on experiments using the CLIP pre-trained ViT-B/16 model are provided in Appendix A, including implementation details in Appendix A.1, investigations into prompt augmentation combinations in Appendix A.2, analysis of different training prompt sources in Appendix A.3, and detailed experiment results for each dataset in Appendix A.4.

- The processes of data synthesis with large models used in our approach are outlined in Appendix B. The image synthesis procedure for Im. Aug is detailed in Appendix B.1, and the approach for generating "LLM" prompts, used in analyzing prompt sources, is described in Appendix B.2.

- In Appendix C, we detail our repeated zero-shot experiments conducted with the CLIP pre-trained ViT-L/14 (Appendix C.1) and ResNet50x16 (Appendix C.2 models.

- In section Appendix D, we present discussions covering the underlying rationale for basing CLAP on the CLIP pre-trained models in Appendix D.1, and the impact of image augmentation and text augmentation in Appendix D.2

## A More on Experiments with ViT-B/16

### A.1 Implementation Details

In this section, we detail the implementation of our experiments utilizing the CLIP pre-trained ViT-B/16 model:

Network. The network's output dimension is aligned with the 512-dimensional CLIP features, thereby obviating the need for input feature downsampling. The latent dimensions are tailored to each dataset: 256 for PACS, 448 for OfficeHome, and 512 for VLCS and DomainNet, to accommodate the variety of categories and complexity of datasets. The weight parameter $\alpha$ is adjusted to 0.208 for PACS, 0.056 for VLCS, 0.14 for OfficeHome, and 0.2 for DomainNet, while it is consistently maintained at 1 throughout the training phase.


<!-- Meanless: 20 Y. Cai et al.-->

Training CLAP. Training parameters are consistent across datasets, employing the Adam optimizer with a learning rate of 0.0001 , limiting training to 8,000 steps with checking the average loss every 480 steps, and instituting early stopping after five checkpoints without a loss decrease of at least 0.01 . Batch sizes are adjusted to 8 for PACS and VLCS, 96 for OfficeHome, and 384 for Domain-Net,with the temperature parameter $\tau$ set at 0.5 for PACS and VLCS,and 0.3 for OfficeHome and DomainNet. The loss coefficient $\lambda$ is set to 1 for PACS and VLCS, and 0.0001 for OfficeHome and DomainNet, due to the first two datasets have less classes. Prompt augmentations, OSD+OCD+SPO, are applied across datasets all with a 0.5 probability. For the PACS and VLCS datasets, Gaussian noise with a zero mean and a standard deviation of 0.02 is randomly inserted at the beginning, middle, or end of the augmented-view prompts to enrich the training samples. In the linear probe evaluations for few-shot analysis, L2 normalization and cross-entropy loss are utilized for training over 1,000 epochs with a batch size of 32 , incorporating early stopping with a patience threshold of 10 epochs and a loss decrease criterion of 0.001 .

Training Im. Aug. We train a disentangled network using image augmentation, applying the InfoNCE loss with a temperature parameter $\tau$ set to 0.5 . This include image augmentation techniques,image cropping (scale $\in  \left\lbrack  {{0.64},{1.0}}\right\rbrack$ ) and color distortion (brightness $= {0.5}$ ,hue $= {0.3}$ ),each with a probability of 0.5 . Other training and inference configurations for Im. Aug are consistent with those used for CLAP across all datasets.

### A.2 Prompt Augmentation Combinations

In Tab. 1, we explore different combinations of our tailored prompt augmentation techniques and EDA (Easy Data Augmentation) [42] techniques on the VLCS dataset. Each combination demonstrates CLAP's effectiveness in enhancing CLIP's performance and reducing performance disparities. The combination of OSD+OCS+SPO+IGN achieves the highest average accuracy and the least variance, outperforming the EDA techniques. Notably, even without incorporating random noise in the augmentations, CLAP significantly surpasses CLIP in handling perturbations on prompts,as evidenced by the largely reduced ${\Delta }_{\left( NC\right) }$ .

### A.3 Prompt Sources

In Tab. 2, we examine the effects of various training prompt formats, sourced from different synthetic origins, on the VLCS dataset performance, utilizing

EDA techniques. The prompt formats are defined as follows: "Template" refers to the template-based prompts fundamental to our primary approach; "LLM" designates prompts created by ChatGPT-3.5 [3], with the generation process elaborated in Appendix B.2: "Random" describes prompts formatted as "a [random] style of [class]," with "[random]" being replaced by terms from a random word generator; and "Prm.Stl." indicates vectorized prompts generated through PromptStyler [9].


<!-- Meanless: APPENDIX 21-->

<!-- Media -->

Table 1: We evaluate prompt augmentation combinations on the VLCS dataset: OSD (①), OCD (②), ITD (③), ASD (④), SPO (⑤), and IGN (⑥). ZS(Avg.) shows average zero-shot accuracy acoss four distinct inference prompts. CLAP boosts CLIP's accuracy and reduces variances, with ①②⑤⑥ as the optimal combination.

<table><tr><td>Metrics</td><td>CLIP (base)</td><td>EDA</td><td>①②③ ④⑤⑥</td><td>①②③ ④⑤</td><td>Avg. top-1 acc. (%) of different augmentations ①②③ ④⑥</td><td>①②③ ④</td><td>③④⑤ ⑥</td><td>①②⑤ ⑥</td></tr><tr><td>ZS(Avg.) (↑)</td><td>77.3</td><td>81.6</td><td>82.0</td><td>80.1</td><td>82.0</td><td>79.6</td><td>82.1</td><td>82.6</td></tr><tr><td>$R\left(  \downarrow  \right)$</td><td>6.1</td><td>1.9</td><td>1.2</td><td>2.5</td><td>0.9</td><td>3.2</td><td>1.6</td><td>0.8</td></tr><tr><td>$\delta \left(  \downarrow  \right)$</td><td>2.8</td><td>0.9</td><td>0.6</td><td>1.2</td><td>0.4</td><td>1.5</td><td>0.7</td><td>0.4</td></tr><tr><td>${\Delta }_{\left( NC\right) }\left(  \downarrow  \right)$</td><td>8.1</td><td>2.3</td><td>1.7</td><td>3.0</td><td>1.8</td><td>3.4</td><td>2.0</td><td>1.6</td></tr></table>

Table 2: We employ EDA augmentation to train CLAP with diverse prompt sources on the VLCS dataset. Each prompt source contributes to improvements in CLIP’s zero-shot performance, with "Random" and "Template" prompts, in their simpler forms, yielding better outcomes.

<table><tr><td rowspan="2">Metrics</td><td rowspan="2">CLIP (base)</td><td colspan="4"/></tr><tr><td>LLM</td><td>Random</td><td>Prm.Stl.</td><td>Template</td></tr><tr><td>ZS(Avg.) (↑)</td><td>77.3</td><td>78.2</td><td>81.6</td><td>81.2</td><td>81.6</td></tr><tr><td>$R\left(  \downarrow  \right)$</td><td>6.1</td><td>3.2</td><td>0.7</td><td>2.7</td><td>1.9</td></tr><tr><td>$\delta \left(  \downarrow  \right)$</td><td>2.8</td><td>1.5</td><td>0.3</td><td>1.2</td><td>0.9</td></tr><tr><td>${\Delta }_{\left( NC\right) }\left(  \downarrow  \right)$</td><td>8.1</td><td>3.3</td><td>2.3</td><td>3.0</td><td>2.3</td></tr></table>

<!-- Media -->

Our experimental results indicate that CLAP, when trained across these varied prompt formats, enhances the performance of CLIP. Notably, despite the complex generation mechanisms of "LLM" and "Prm.Stl." prompts, the simpler, random-styled and template-based prompts demonstrate superior efficacy. However, it is important to highlight that the improvements attributed to these diverse prompt formats, trained with EDA, do not surpass the best performance of the prompt augmentations tailored for template-based prompts.

### A.4 Detailed Results on ViT-B/16

Details on Zero-Shot Evaluations We present the domain-level zero-shot performance with various prompts across each dataset in Tab. 3. CLAP consistently enhances CLIP's zero-shot performance across these different prompts. Given that CLAP exclusively utilizes text data for training, it does not compromise CLIP's inherent ability to generalize across domains, which is acquired from its extensive training dataset. Rather, by achieving a more effective disentanglement of content, it unequivocally enhances CLIP's zero-shot performance across all dataset domains.


<!-- Meanless: 22 Y. Cai et al.-->

<!-- Media -->

Table 3: Domain-level zero-shot results of the ViT-B/16 model on the test datasets.

<table><tr><td colspan="2" rowspan="3">Dataset Domains</td><td colspan="12">Domain-level avg. top-1 acc. (%) of zero-shot performance usig ViT-B/16 (↑)</td></tr><tr><td colspan="3">ZS(C)</td><td colspan="3">ZS(CP)</td><td colspan="3">ZS(PC)</td><td colspan="3">ZS(NC)</td></tr><tr><td>CLIP</td><td>Im. Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>96.4</td><td>96.9</td><td>97.5</td><td>93.4</td><td>97.0</td><td>97.6</td><td>97.4</td><td>97.6</td><td>97.6</td><td>87.8</td><td>93.5</td><td>97.1</td></tr><tr><td>C.</td><td>98.9</td><td>99.0</td><td>98.9</td><td>99.0</td><td>99.2</td><td>99.0</td><td>99.1</td><td>99.0</td><td>98.9</td><td>95.4</td><td>97.6</td><td>98.8</td></tr><tr><td>P.</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.3</td><td>99.6</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>93.1</td><td>99.0</td><td>99.9</td></tr><tr><td>S.</td><td>87.7</td><td>90.1</td><td>92.5</td><td>89.2</td><td>89.6</td><td>92.5</td><td>88.1</td><td>89.4</td><td>92.3</td><td>87.1</td><td>89.3</td><td>93.1</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>99.7</td><td>99.8</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>87.0</td><td>96.0</td><td>99.9</td></tr><tr><td>L.</td><td>61.8</td><td>66.2</td><td>67.7</td><td>69.9</td><td>70.4</td><td>70.4</td><td>70.2</td><td>70.2</td><td>70.7</td><td>55.9</td><td>59.9</td><td>65.9</td></tr><tr><td>S.</td><td>70.1</td><td>74.8</td><td>78.0</td><td>73.3</td><td>76.0</td><td>77.2</td><td>73.6</td><td>76.4</td><td>76.9</td><td>61.4</td><td>66.2</td><td>75.3</td></tr><tr><td>V.</td><td>73.9</td><td>77.1</td><td>84.9</td><td>84.8</td><td>85.4</td><td>86.0</td><td>86.1</td><td>85.6</td><td>86.2</td><td>68.9</td><td>70.3</td><td>82.9</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>80.5</td><td>79.0</td><td>81.8</td><td>80.1</td><td>76.0</td><td>81.6</td><td>83.2</td><td>78.7</td><td>83.2</td><td>73.0</td><td>69.2</td><td>73.6</td></tr><tr><td>C.</td><td>64.6</td><td>59.6</td><td>66.4</td><td>63.7</td><td>58.9</td><td>65.4</td><td>68.1</td><td>61.9</td><td>69.0</td><td>57.0</td><td>52.0</td><td>60.4</td></tr><tr><td>P.</td><td>86.3</td><td>83.6</td><td>87.5</td><td>86.6</td><td>83.4</td><td>87.2</td><td>89.1</td><td>86.6</td><td>89.7</td><td>77.2</td><td>72.3</td><td>78.9</td></tr><tr><td>R.</td><td>88.0</td><td>85.9</td><td>88.5</td><td>87.6</td><td>84.8</td><td>87.7</td><td>89.8</td><td>87.2</td><td>90.0</td><td>79.0</td><td>76.5</td><td>81.1</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>71.0</td><td>64.3</td><td>71.9</td><td>70.5</td><td>62.1</td><td>72.0</td><td>71.3</td><td>63.4</td><td>72.8</td><td>63.2</td><td>53.9</td><td>64.6</td></tr><tr><td>I.</td><td>48.6</td><td>40.5</td><td>50.6</td><td>47.7</td><td>40.7</td><td>49.5</td><td>47.8</td><td>40.0</td><td>50.5</td><td>42.9</td><td>35.0</td><td>45.1</td></tr><tr><td>P.</td><td>66.6</td><td>59.1</td><td>67.7</td><td>66.0</td><td>59.0</td><td>67.3</td><td>66.5</td><td>59.8</td><td>68.4</td><td>57.2</td><td>50.4</td><td>59.4</td></tr><tr><td>Q.</td><td>14.9</td><td>12.4</td><td>15.2</td><td>13.3</td><td>11.5</td><td>13.8</td><td>14.1</td><td>11.8</td><td>14.3</td><td>12.0</td><td>9.2</td><td>13.1</td></tr><tr><td>R.</td><td>82.6</td><td>76.6</td><td>83.1</td><td>82.2</td><td>75.8</td><td>82.2</td><td>83.4</td><td>78.2</td><td>83.7</td><td>75.2</td><td>67.9</td><td>75.6</td></tr><tr><td>S.</td><td>63.1</td><td>56.1</td><td>63.7</td><td>62.2</td><td>55.0</td><td>63.1</td><td>63.4</td><td>56.4</td><td>64.4</td><td>55.7</td><td>47.5</td><td>57.6</td></tr></table>

<!-- Media -->

Details on Few-Shot Evaluations We display the quantitative results of few-shot performance in Tab. 4. CLAP consistently enhances the few-shot capabilities, showcasing improvements across test datasets at a closer domain level.

Details on Adversarial Evaluations In Tab. 5, we detail our adversarial performance evaluations for PACS, VLCS, OfficeHome, and DomainNet, respectively. CLAP enhances both zero-shot and one-shot performance across all domains of the tested datasets. While Im.Aug boosts one-shot robustness against adversarial tasks, its impact on zero-shot adversarial robustness is inconsistent.

Details on Ablative Analysis In Tab. 6, we provide detailed results from our analysis on zero-shot performance using various combinations of prompt augmentations. Additionally, in Tab. 7, we present the outcomes of our ablative studies focusing on the hyperparameters $\tau$ ,latent dimension,and $\alpha$ ,respectively, each evaluated domain-wise. The results indicate that CLAP is effective across a wide range of hyperparameters.

## B Data Synthesis

### B.1 Synthetic Image Generation

We employ the stable diffusion [39] v2.1 model for generating synthetic images used in our comparing experiments, specifically utilizing the Stable Diffusion


<!-- Meanless: APPENDIX 23-->

<!-- Media -->

Table 4: Domain-level few-shot results of the ViT-B/16 model using the test datasets.

<table><tr><td rowspan="3" colspan="2">Dataset Domains</td><td colspan="15">Domain-level avg. top-1 acc. (%) of few-shot performance of ViT-B/16 (†)</td></tr><tr><td colspan="3">1-shot</td><td colspan="3">4-shot</td><td colspan="3">8-shot</td><td colspan="3">16-shot</td><td colspan="3">32-shot</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>79.5</td><td>84.1</td><td>94.5</td><td>92.4</td><td>96.4</td><td>97.2</td><td>95.1</td><td>97.2</td><td>98.4</td><td>97.9</td><td>98.1</td><td>98.4</td><td>98.8</td><td>99.1</td><td>98.9</td></tr><tr><td>C.</td><td>86.7</td><td>96.1</td><td>98.3</td><td>96.8</td><td>98.6</td><td>99.2</td><td>98.8</td><td>98.9</td><td>99.3</td><td>99.5</td><td>99.2</td><td>99.5</td><td>99.6</td><td>99.6</td><td>99.6</td></tr><tr><td>P.</td><td>97.4</td><td>99.8</td><td>99.9</td><td>99.6</td><td>99.8</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td></tr><tr><td>S.</td><td>75.1</td><td>80.0</td><td>87.3</td><td>91.1</td><td>92.3</td><td>92.5</td><td>92.3</td><td>92.3</td><td>92.9</td><td>92.4</td><td>92.6</td><td>93.1</td><td>93.9</td><td>94.2</td><td>94.1</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>99.2</td><td>99.7</td><td>99.8</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.8</td><td>99.7</td><td>99.9</td><td>99.7</td><td>99.9</td><td>99.9</td><td>99.9</td><td>100.0</td><td>99.9</td></tr><tr><td>L.</td><td>41.3</td><td>41.3</td><td>41.1</td><td>56.7</td><td>57.0</td><td>59.8</td><td>46.2</td><td>36.8</td><td>48.3</td><td>59.4</td><td>60.4</td><td>62.6</td><td>60.4</td><td>60.7</td><td>61.9</td></tr><tr><td>S.</td><td>45.3</td><td>46.1</td><td>50.8</td><td>61.9</td><td>63.7</td><td>69.0</td><td>67.4</td><td>67.7</td><td>71.3</td><td>75.9</td><td>76.8</td><td>80.9</td><td>77.4</td><td>78.6</td><td>81.0</td></tr><tr><td>V.</td><td>50.9</td><td>53.4</td><td>59.0</td><td>64.5</td><td>66.7</td><td>76.1</td><td>75.4</td><td>74.1</td><td>78.7</td><td>72.6</td><td>73.9</td><td>77.7</td><td>85.7</td><td>86.1</td><td>87.9</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>42.6</td><td>45.1</td><td>43.9</td><td>76.8</td><td>77.6</td><td>77.7</td><td>84.8</td><td>86.0</td><td>85.5</td><td>91.8</td><td>92.1</td><td>92.1</td><td>97.4</td><td>97.5</td><td>97.5</td></tr><tr><td>C.</td><td>40.1</td><td>45.0</td><td>43.8</td><td>69.9</td><td>70.2</td><td>70.5</td><td>75.8</td><td>75.9</td><td>76.6</td><td>81.6</td><td>81.6</td><td>81.6</td><td>89.0</td><td>89.0</td><td>89.2</td></tr><tr><td>P.</td><td>70.2</td><td>73.3</td><td>73.4</td><td>89.7</td><td>90.3</td><td>90.2</td><td>93.8</td><td>93.7</td><td>93.9</td><td>95.7</td><td>95.7</td><td>95.8</td><td>97.7</td><td>97.6</td><td>97.6</td></tr><tr><td>R.</td><td>58.4</td><td>59.3</td><td>59.4</td><td>81.7</td><td>83.1</td><td>82.9</td><td>89.7</td><td>89.5</td><td>89.9</td><td>92.9</td><td>92.7</td><td>93.2</td><td>95.8</td><td>95.8</td><td>95.8</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>42.1</td><td>43.6</td><td>43.8</td><td>66.8</td><td>67.5</td><td>67.8</td><td>74.2</td><td>74.3</td><td>74.6</td><td>78.5</td><td>78.6</td><td>78.8</td><td>82.8</td><td>82.8</td><td>82.7</td></tr><tr><td>L</td><td>19.5</td><td>20.8</td><td>21.0</td><td>38.5</td><td>39.3</td><td>39.7</td><td>46.7</td><td>47.0</td><td>47.3</td><td>53.2</td><td>53.2</td><td>53.6</td><td>60.0</td><td>59.9</td><td>60.1</td></tr><tr><td>P.</td><td>32.1</td><td>33.5</td><td>34.2</td><td>60.5</td><td>60.9</td><td>61.5</td><td>68.0</td><td>68.0</td><td>68.7</td><td>72.5</td><td>72.6</td><td>73.0</td><td>76.7</td><td>76.6</td><td>76.8</td></tr><tr><td>Q.</td><td>15.2</td><td>15.3</td><td>15.3</td><td>30.0</td><td>29.6</td><td>29.9</td><td>37.1</td><td>36.4</td><td>36.8</td><td>43.8</td><td>43.4</td><td>43.5</td><td>49.4</td><td>49.1</td><td>49.0</td></tr><tr><td>R.</td><td>50.8</td><td>52.1</td><td>52.7</td><td>76.7</td><td>77.0</td><td>77.6</td><td>81.7</td><td>81.9</td><td>82.2</td><td>84.0</td><td>83.9</td><td>84.3</td><td>85.9</td><td>85.9</td><td>86.0</td></tr><tr><td>S.</td><td>33.1</td><td>33.9</td><td>34.8</td><td>56.2</td><td>56.6</td><td>57.2</td><td>62.9</td><td>62.9</td><td>63.7</td><td>67.8</td><td>67.7</td><td>68.1</td><td>72.5</td><td>72.3</td><td>72.6</td></tr></table>

<!-- figureText: a realistic painting of an impressionistic a realistic photograph a realistic sketch of a an impressionistic large black mug painting of a normal sized green train a large blue aircraft mosaic art of a small of a large black car carrier black backpack -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_30_24_a512c7.jpg"/>

Fig. 1: Examples of synthetic images created with SDv2.1 and associated prompts.

<!-- Media -->

v2-1 Model Card available on Hugging Face ${}^{1}$ . For each class across the four datasets, we produce 480 images using our synthetic template prompts as input for the stable diffusion model. All generated images are of ${512} \times  {512}$ resolution. Examples of these synthetic images alongside their corresponding text prompts are displayed in Fig. 1.

### B.2 LLM Prompts Generation

We utilize ChatGPT-3.5 [3] to create the LLM prompts employed in our comparative analysis of different prompt sources. Fig. 2 illustrates the process of prompting ChatGPT-3.5 to generate text prompts for specific class names. For each class, we produce 120 samples, and below are a few examples from the generated prompts:

- Bird:

---

<!-- Footnote -->

https://huggingface.co/stabilityai/stable-diffusion-2-1

<!-- Footnote -->

---




<!-- Meanless: 24 Y. Cai et al.-->

<!-- Media -->

Table 5: Domain-level results under adversarial attacks of ViT-B/16 on the datasets.

<table><tr><td colspan="2" rowspan="4">Dataset Domains</td><td colspan="18">Domain-level avg. top-1 acc. (%) under adversarial attackings using ViT-B/16 (†)</td></tr><tr><td colspan="6">FGSM</td><td colspan="6">PGD-20</td><td colspan="6">CW-20</td></tr><tr><td colspan="3" rowspan="2">ZS-C CLIPIm.AugCLAP</td><td colspan="3">1-shot</td><td colspan="3">ZS-C</td><td colspan="3">1-shot</td><td colspan="3">ZS-C</td><td colspan="3">1-shot</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>76.3</td><td>79.3</td><td>79.3</td><td>61.2</td><td>78.0</td><td>87.3</td><td>1.7</td><td>2.2</td><td>1.8</td><td>16.0</td><td>42.1</td><td>63.1</td><td>1.5</td><td>2.0</td><td>2.3</td><td>0.5</td><td>1.1</td><td>1.7</td></tr><tr><td>C.</td><td>94.9</td><td>95.0</td><td>94.0</td><td>66.5</td><td>84.2</td><td>95.1</td><td>33.3</td><td>37.7</td><td>35.6</td><td>33.3</td><td>57.2</td><td>86.1</td><td>28.8</td><td>34.0</td><td>33.2</td><td>11.9</td><td>23.6</td><td>31.8</td></tr><tr><td>P.</td><td>91.6</td><td>90.3</td><td>91.7</td><td>67.4</td><td>80.8</td><td>92.1</td><td>5.7</td><td>7.0</td><td>6.7</td><td>27.1</td><td>55.0</td><td>69.8</td><td>4.7</td><td>4.9</td><td>5.8</td><td>0.7</td><td>2.7</td><td>4.1</td></tr><tr><td>S.</td><td>84.5</td><td>87.5</td><td>89.8</td><td>71.6</td><td>74.6</td><td>83.8</td><td>75.8</td><td>78.4</td><td>79.2</td><td>63.0</td><td>66.3</td><td>74.6</td><td>74.5</td><td>76.8</td><td>77.9</td><td>62.7</td><td>65.4</td><td>70.3</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>55.3</td><td>53.8</td><td>55.5</td><td>25.8</td><td>28.8</td><td>25.3</td><td>4.4</td><td>5.1</td><td>4.7</td><td>2.0</td><td>5.2</td><td>2.5</td><td>2.9</td><td>3.1</td><td>3.5</td><td>0.7</td><td>1.2</td><td>1.0</td></tr><tr><td>L.</td><td>49.4</td><td>45.5</td><td>50.6</td><td>27.0</td><td>32.6</td><td>30.4</td><td>15.2</td><td>14.9</td><td>16.0</td><td>6.4</td><td>8.9</td><td>8.0</td><td>12.4</td><td>11.2</td><td>13.0</td><td>6.1</td><td>8.3</td><td>7.7</td></tr><tr><td>S.</td><td>61.7</td><td>58.1</td><td>62.5</td><td>48.0</td><td>46.9</td><td>51.6</td><td>13.2</td><td>13.9</td><td>14.0</td><td>8.6</td><td>10.7</td><td>10.0</td><td>9.2</td><td>8.8</td><td>10.2</td><td>8.3</td><td>7.9</td><td>8.4</td></tr><tr><td>V.</td><td>65.3</td><td>63.2</td><td>65.6</td><td>36.5</td><td>40.1</td><td>41.0</td><td>7.5</td><td>7.9</td><td>7.9</td><td>5.3</td><td>9.4</td><td>8.9</td><td>5.2</td><td>4.8</td><td>5.6</td><td>2.9</td><td>2.8</td><td>2.9</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>55.3</td><td>53.8</td><td>55.5</td><td>25.8</td><td>28.8</td><td>25.3</td><td>4.4</td><td>5.1</td><td>4.7</td><td>2.0</td><td>5.2</td><td>2.5</td><td>2.9</td><td>3.1</td><td>3.5</td><td>0.7</td><td>1.2</td><td>1.0</td></tr><tr><td>C.</td><td>49.4</td><td>45.5</td><td>50.6</td><td>27.0</td><td>32.6</td><td>30.4</td><td>15.2</td><td>14.9</td><td>16.0</td><td>6.4</td><td>8.9</td><td>8.0</td><td>12.4</td><td>11.2</td><td>13.0</td><td>6.1</td><td>8.3</td><td>7.7</td></tr><tr><td>P.</td><td>61.7</td><td>58.1</td><td>62.5</td><td>48.0</td><td>46.9</td><td>51.6</td><td>13.2</td><td>13.9</td><td>14.0</td><td>8.6</td><td>10.7</td><td>10.0</td><td>9.2</td><td>8.8</td><td>10.2</td><td>8.3</td><td>7.9</td><td>8.4</td></tr><tr><td>R.</td><td>65.3</td><td>63.2</td><td>65.6</td><td>36.5</td><td>40.1</td><td>41.0</td><td>7.5</td><td>7.9</td><td>7.9</td><td>5.3</td><td>9.4</td><td>8.9</td><td>5.2</td><td>4.8</td><td>5.6</td><td>2.9</td><td>2.8</td><td>2.9</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>57.8</td><td>50.9</td><td>58.8</td><td>33.3</td><td>34.3</td><td>35.0</td><td>21.6</td><td>18.7</td><td>22.8</td><td>18.4</td><td>19.6</td><td>20.0</td><td>15.8</td><td>12.5</td><td>16.6</td><td>7.0</td><td>7.5</td><td>7.8</td></tr><tr><td>I.</td><td>35.8</td><td>28.0</td><td>37.0</td><td>12.2</td><td>13.3</td><td>13.2</td><td>6.1</td><td>3.7</td><td>6.7</td><td>4.6</td><td>5.3</td><td>5.1</td><td>3.3</td><td>1.9</td><td>3.7</td><td>0.9</td><td>0.9</td><td>0.9</td></tr><tr><td>P.</td><td>43.9</td><td>39.0</td><td>44.3</td><td>18.4</td><td>20.6</td><td>20.3</td><td>3.1</td><td>2.8</td><td>3.3</td><td>8.6</td><td>10.4</td><td>9.9</td><td>1.8</td><td>1.3</td><td>1.9</td><td>0.3</td><td>0.3</td><td>0.3</td></tr><tr><td>Q.</td><td>12.9</td><td>10.3</td><td>13.2</td><td>10.9</td><td>10.8</td><td>11.1</td><td>8.4</td><td>6.8</td><td>8.6</td><td>5.4</td><td>5.4</td><td>5.6</td><td>7.1</td><td>5.4</td><td>7.4</td><td>4.9</td><td>4.8</td><td>5.1</td></tr><tr><td>R.</td><td>62.1</td><td>55.9</td><td>62.4</td><td>34.5</td><td>35.9</td><td>36.5</td><td>7.1</td><td>6.5</td><td>7.5</td><td>17.6</td><td>19.7</td><td>19.6</td><td>4.5</td><td>3.4</td><td>4.7</td><td>1.2</td><td>1.4</td><td>1.4</td></tr><tr><td>S.</td><td>49.1</td><td>43.3</td><td>49.7</td><td>25.7</td><td>26.0</td><td>27.5</td><td>17.8</td><td>15.5</td><td>18.6</td><td>13.6</td><td>14.4</td><td>15.1</td><td>13.4</td><td>10.2</td><td>13.9</td><td>5.0</td><td>5.2</td><td>5.6</td></tr></table>

<!-- Media -->

- A pair of vibrant macaws converse in a lush, tropical rainforest, depicted in a lively, exotic wildlife painting.

- A solitary eagle watches over a vast, rugged canyon at sunrise, portrayed in a majestic, wilderness landscape photograph.

- Dog:

- A sleek Whippet races in a competitive dog track, illustrated in a fast-paced, dynamic sports style.

- A sturdy and reliable English Bulldog watching over a small shop, its solid presence reassuring to the owner.

- Car:

- A quirky art car parades through the streets in a colorful festival, captured in a fun, expressive style illustration.

- A high-tech, autonomous car maneuvers through a smart city environment, portrayed in a futuristic, sci-fi digital art piece.

- Chair:

- A folding chair at an outdoor wedding, elegantly decorated and part of a beautiful ceremony.

- A high-end executive chair in a law firm, projecting authority and professionalism.

- Person:

- An energetic coach motivates a team on a sports field, illustrated in an inspiring, leadership-focused painting.

- A graceful figure skater glides across an ice rink, captured in a delicate, winter-themed pastel drawing.


<!-- Meanless: APPENDIX 25-->

<!-- Media -->

Table 6: Zero-Shot Performance on VLCS Dataset Across Varied Augmentation Combinations and Prompt Sources: ① Random Object Size Deletion, ② Random Object Color Deletion, ③ Random Image Type Deletion, ④ Random Art Style Deletion, ⑤ Random Swapping Order, ⑥ Addition of Gaussian Noise.

<table><tr><td rowspan="2">Method Domains</td><td rowspan="2"/><td colspan="11">Avg. top-1 acc. (%) (↑) of different augmentations and prompts on VLCS</td></tr><tr><td>CLIP (base)</td><td>①②③ ④⑤⑥</td><td>①②③ ④⑤</td><td>①②③ ④⑥</td><td>①②③ ④</td><td>③④⑤ ⑥</td><td>①②⑤ ⑥</td><td colspan="4">EDA LLMRand.Pr.St.Temp.</td></tr><tr><td rowspan="4">ZS(C)</td><td>C.</td><td>99.7</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.9</td><td>97.9</td><td>99.7</td><td>99.9</td><td>99.9</td></tr><tr><td>L.</td><td>61.8</td><td>66.6</td><td>62.3</td><td>67.0</td><td>62.2</td><td>66.2</td><td>67.7</td><td>66.2</td><td>69.0</td><td>67.3</td><td>66.5</td></tr><tr><td>S.</td><td>70.1</td><td>78.1</td><td>75.5</td><td>78.0</td><td>74.3</td><td>78.5</td><td>78.0</td><td>73.2</td><td>76.9</td><td>73.5</td><td>76.9</td></tr><tr><td>V.</td><td>73.9</td><td>82.8</td><td>80.6</td><td>83.2</td><td>79.3</td><td>82.7</td><td>84.9</td><td>72.6</td><td>81.8</td><td>81.8</td><td>81.9</td></tr><tr><td rowspan="4">ZS(CP)</td><td>C.</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.8</td><td>99.9</td><td>99.9</td><td>99.9</td></tr><tr><td>L.</td><td>69.9</td><td>69.3</td><td>67.9</td><td>69.6</td><td>68.4</td><td>70.0</td><td>70.4</td><td>69.3</td><td>70.4</td><td>71.2</td><td>69.7</td></tr><tr><td>S.</td><td>73.3</td><td>77.6</td><td>76.4</td><td>76.7</td><td>75.9</td><td>78.8</td><td>77.2</td><td>76.2</td><td>75.2</td><td>75.1</td><td>78.0</td></tr><tr><td>V.</td><td>84.8</td><td>85.3</td><td>84.0</td><td>85.3</td><td>84.2</td><td>85.1</td><td>86.0</td><td>77.0</td><td>84.2</td><td>86.0</td><td>84.6</td></tr><tr><td rowspan="4">ZS(PC)</td><td>C.</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td></tr><tr><td>L.</td><td>70.2</td><td>70.0</td><td>68.0</td><td>70.1</td><td>68.5</td><td>70.0</td><td>70.7</td><td>67.5</td><td>70.6</td><td>71.8</td><td>70.0</td></tr><tr><td>S.</td><td>73.6</td><td>76.6</td><td>75.6</td><td>76.0</td><td>74.8</td><td>77.8</td><td>76.9</td><td>76.9</td><td>75.1</td><td>74.9</td><td>78.2</td></tr><tr><td>V.</td><td>86.1</td><td>85.7</td><td>84.7</td><td>85.7</td><td>84.5</td><td>85.5</td><td>86.2</td><td>78.2</td><td>84.6</td><td>86.8</td><td>84.8</td></tr><tr><td rowspan="4">ZS(NC)</td><td>C.</td><td>87.0</td><td>99.8</td><td>99.6</td><td>99.8</td><td>99.4</td><td>99.7</td><td>99.9</td><td>95.3</td><td>98.6</td><td>99.6</td><td>99.8</td></tr><tr><td>L.</td><td>55.9</td><td>65.2</td><td>61.3</td><td>65.6</td><td>60.5</td><td>65.4</td><td>65.9</td><td>63.0</td><td>66.7</td><td>64.0</td><td>64.7</td></tr><tr><td>S.</td><td>61.4</td><td>75.6</td><td>70.3</td><td>75.2</td><td>68.3</td><td>74.9</td><td>75.3</td><td>68.9</td><td>73.3</td><td>69.8</td><td>73.0</td></tr><tr><td>V.</td><td>68.9</td><td>80.1</td><td>75.2</td><td>80.4</td><td>73.8</td><td>79.4</td><td>82.9</td><td>69.3</td><td>79.6</td><td>77.2</td><td>78.6</td></tr></table>

<!-- Media -->

## C Experiments on Other CLIP Model Scales

### C.1 Experiments on ViT-L/14

We refined the output dimension to align with the input dimension of 768 . The chosen latent dimensions were 448 and 640 for PACS and VLCS, respectively, and 768 for both OfficeHome and DomainNet. The inference weighting $\alpha$ was set to 0.1 for PACS, 0.03 for VLCS, 0.14 for OfficeHome, and 0.2 for Domain-Net. All other training configurations remained consistent with the ViT-B/16 experiments across each dataset. The training configuration for Im. Aug was set the same as CLAP for each dataset,with the inference weighting $\alpha$ being 0.1 for PACS and 0.03 for the other three datasets.

Table 8 showcases the zero-shot results for the ViT-L/14 model using four distinct prompts, following the protocol established for the ViT-B/16 experiments. These results demonstrate that CLAP is more efficient than Im. Aug in enhancing zero-shot performance. Moreover, Tab. 9 illustrates that CLAP significantly reduces variations in zero-shot performance across different prompts, thereby confirming CLAP's performance improvements over CLIP across a range of model sizes. Detailed domain-level results are presented in Tab. 10, offering an in-depth analysis.

### C.2 Experiments on ResNet50x16

To validate our approach on different model structures, we repeated zero-shot experiments on the ResNet50x16 model pre-trained with CLIP. Since the output dimension of CLIP is the same as ViT-B/16, we used the same training configuration as ViT-B/16 for training Im. Aug and CLAP. For inference, we refined the weighting coefficient $\alpha$ to0.1,1,0.03,and 0.1 for Im.Aug,and0.03,0.2,0.06, and 0.1 for CLAP, for PACS, VLCS, OfficeHome, and DomainNet respectively.


<!-- Meanless: 26 Y. Cai et al.-->

<!-- Media -->

Table 7: Ablative study of hyperparameters on VLCS dataset using ViT-B/16 model.

<table><tr><td rowspan="3">Hyper- parameters</td><td rowspan="3">Value</td><td colspan="12">Avg. top-1 acc. (%) (↑) using ViT-B/16 on VLCS dataset</td></tr><tr><td colspan="4">ZS (C)</td><td colspan="4">ZS (CP)</td><td colspan="4">ZS (PC)</td></tr><tr><td>C.</td><td>L.</td><td>S.</td><td>V.</td><td>C.</td><td>L.</td><td>S.</td><td>V.</td><td>C.</td><td>L.</td><td>S.</td><td>V.</td></tr><tr><td rowspan="5">$\tau$</td><td>0.1</td><td>99.9</td><td>67.6</td><td>77.5</td><td>84.2</td><td>99.9</td><td>70.9</td><td>74.9</td><td>85.9</td><td>99.9</td><td>71.2</td><td>74.6</td><td>86.3</td></tr><tr><td>0.3</td><td>99.9</td><td>66.3</td><td>77.2</td><td>82.4</td><td>99.9</td><td>69.9</td><td>76.7</td><td>85.2</td><td>99.9</td><td>69.9</td><td>76.4</td><td>85.4</td></tr><tr><td>0.5</td><td>99.9</td><td>67.7</td><td>78.0</td><td>84.9</td><td>99.9</td><td>70.4</td><td>77.2</td><td>86.0</td><td>99.9</td><td>70.7</td><td>76.9</td><td>86.2</td></tr><tr><td>0.7</td><td>99.9</td><td>65.9</td><td>77.7</td><td>83.1</td><td>99.9</td><td>68.9</td><td>77.9</td><td>84.9</td><td>99.9</td><td>69.6</td><td>77.7</td><td>85.0</td></tr><tr><td>0.9</td><td>99.9</td><td>66.0</td><td>77.6</td><td>83.3</td><td>99.9</td><td>69.0</td><td>77.9</td><td>85.0</td><td>99.9</td><td>69.7</td><td>77.5</td><td>85.0</td></tr><tr><td rowspan="7">Lantent dim.</td><td>128.0</td><td>99.9</td><td>66.0</td><td>77.6</td><td>82.6</td><td>99.9</td><td>70.0</td><td>77.4</td><td>85.4</td><td>99.9</td><td>70.1</td><td>77.1</td><td>85.7</td></tr><tr><td>192.0</td><td>99.9</td><td>64.9</td><td>77.9</td><td>83.0</td><td>99.9</td><td>68.9</td><td>78.0</td><td>85.6</td><td>99.9</td><td>69.0</td><td>77.8</td><td>86.0</td></tr><tr><td>256.0</td><td>99.9</td><td>63.8</td><td>77.6</td><td>82.7</td><td>99.9</td><td>67.6</td><td>78.7</td><td>84.8</td><td>99.9</td><td>67.8</td><td>78.6</td><td>85.2</td></tr><tr><td>320.0</td><td>99.9</td><td>66.0</td><td>77.8</td><td>82.9</td><td>99.9</td><td>69.2</td><td>78.1</td><td>85.3</td><td>99.9</td><td>69.7</td><td>77.7</td><td>85.5</td></tr><tr><td>384.0</td><td>99.9</td><td>65.8</td><td>76.9</td><td>82.8</td><td>99.9</td><td>69.4</td><td>77.5</td><td>85.3</td><td>99.9</td><td>69.6</td><td>77.0</td><td>85.5</td></tr><tr><td>448.0</td><td>99.9</td><td>65.8</td><td>77.4</td><td>82.1</td><td>99.9</td><td>69.7</td><td>77.6</td><td>84.9</td><td>99.9</td><td>69.9</td><td>77.1</td><td>85.6</td></tr><tr><td>512.0</td><td>99.9</td><td>67.7</td><td>78.0</td><td>84.9</td><td>99.9</td><td>70.4</td><td>77.2</td><td>86.0</td><td>99.9</td><td>70.7</td><td>76.9</td><td>86.2</td></tr><tr><td rowspan="7">$\alpha$</td><td>${10}^{-{1.5}}$</td><td>99.9</td><td>66.5</td><td>77.9</td><td>83.1</td><td>99.9</td><td>70.4</td><td>77.1</td><td>86.0</td><td>99.9</td><td>70.3</td><td>76.6</td><td>86.1</td></tr><tr><td>${10}^{-1}$</td><td>99.9</td><td>69.5</td><td>77.5</td><td>85.7</td><td>99.9</td><td>70.4</td><td>77.1</td><td>86.2</td><td>99.9</td><td>70.9</td><td>76.5</td><td>86.1</td></tr><tr><td>${10}^{-{0.5}}$</td><td>99.9</td><td>70.6</td><td>75.2</td><td>85.5</td><td>99.9</td><td>70.7</td><td>75.7</td><td>85.9</td><td>99.9</td><td>71.0</td><td>75.1</td><td>85.7</td></tr><tr><td>${10}^{0}$</td><td>99.8</td><td>71.5</td><td>73.5</td><td>83.5</td><td>99.9</td><td>71.7</td><td>74.4</td><td>85.8</td><td>99.8</td><td>72.3</td><td>73.5</td><td>85.5</td></tr><tr><td>${10}^{0.5}$</td><td>99.8</td><td>72.0</td><td>73.1</td><td>85.5</td><td>99.8</td><td>72.2</td><td>73.7</td><td>85.7</td><td>99.8</td><td>72.5</td><td>72.9</td><td>85.6</td></tr><tr><td>${10}^{1}$</td><td>99.8</td><td>72.1</td><td>72.8</td><td>85.4</td><td>99.8</td><td>72.3</td><td>73.4</td><td>85.7</td><td>99.8</td><td>72.5</td><td>72.9</td><td>85.5</td></tr><tr><td>${10}^{1.5}$</td><td>99.8</td><td>72.1</td><td>72.8</td><td>85.4</td><td>99.8</td><td>72.2</td><td>73.3</td><td>85.7</td><td>99.8</td><td>72.6</td><td>72.7</td><td>85.5</td></tr></table>

<!-- Media -->

Table 11 showcases the zero-shot results for ResNet50x16 model across different prompts, substantiating that CLAP is more effective than Im.Aug in refining CLIP features. Moreover, Tab. 12 illustrates that both Im.Aug and CLAP reduce variations in zero-shot performance across different prompts, with the improvement of CLAP being more significant. The results validate our approach across different model scales, including both ViT-based and CNN-based structures. Domain-level results are detailed in Tab. 13.

## D Discussion

### D.1 Rationale behind CLAP's Foundation on CLIP

The primary challenge in cross-modal transferability lies in the significant domain gap between text and image data, which typically hinders the direct application of models trained in one modality to another. For a causal explaination, despite the consistency of the content variable that dictates the object label across modalities, the generative processes from latent variables to observations inherent to each modality differ markedly. The CLIP model, trained on a comprehensive dataset of image-text pairs with a symmetric InfoNCE loss, significantly ameliorates this issue. By aligning the features of text and images into similar patterns, it facilitates leveraging a network trained atop the CLIP encoder of one modality as a viable proxy for the other. Consequently, this allows for the direct application of the disentangled network trained in the text modality atop CLIP’s image encoder to refine representations.


<!-- Meanless: APPENDIX 27-->

<!-- Media -->

---

You
		I have some class names, such as "dog", "car", "person", "chair". I want you to generate various
style of these objects in different art-style pictures. Providing me with a list of different sentences
that decribe a scene contaning the object, with different object attributes (such as object size,
condition or age( if applicable), colors), different backgrounds the object may apear in; different
image types (such as photo, painting, etc).
ChatGPT
		Absolutely, "Il provide you with a list of descriptive sentences for each of the class names "dog",
"car", "person", "chair," incorporating various object attributes, backgrounds, and image types.
Each sentence will describe a unique scene:

---

Fig. 2: The prompting method we use for generating text prompts with ChatGPT-3.5.

Table 8: Zero-shot performance across four prompts ("C", "PC", "CP") and 1 noised prompts ("NC") with CLIP pre-trained ViT-L/14 model. CLAP demonstrates consistent gains in zero-shot performance across all datasets, validating its effectiveness.

<table><tr><td rowspan="2"/><td rowspan="2">Prompt Method.</td><td colspan="5">Zero-shot performance,avg. top-1 acc. (%) (↑)</td></tr><tr><td>PACS</td><td>VLCS</td><td>OfficeHome</td><td>DomainNet</td><td>Overall</td></tr><tr><td rowspan="3">ZS(C)</td><td>CLIP</td><td>97.6</td><td>77.1</td><td>85.9</td><td>63.2</td><td>80.9</td></tr><tr><td>Im.Aug</td><td>98.3</td><td>78.5</td><td>86.0</td><td>63.4</td><td>81.6</td></tr><tr><td>CLAP</td><td>98.5</td><td>80.7</td><td>87.5</td><td>64.2</td><td>82.7</td></tr><tr><td rowspan="3">ZS(CP)</td><td>CLIP</td><td>97.3</td><td>80.6</td><td>86.0</td><td>62.0</td><td>81.5</td></tr><tr><td>Im.Aug</td><td>98.3</td><td>81.1</td><td>86.1</td><td>62.4</td><td>82.0</td></tr><tr><td>CLAP</td><td>98.5</td><td>81.4</td><td>87.9</td><td>63.7</td><td>82.9</td></tr><tr><td rowspan="3">ZS(PC)</td><td>CLIP</td><td>98.4</td><td>81.7</td><td>86.5</td><td>63.5</td><td>82.5</td></tr><tr><td>Im.Aug</td><td>98.6</td><td>81.9</td><td>86.6</td><td>63.7</td><td>82.7</td></tr><tr><td>CLAP</td><td>98.6</td><td>82.2</td><td>88.0</td><td>64.5</td><td>83.3</td></tr><tr><td rowspan="3">ZS(NC)</td><td>CLIP</td><td>91.0</td><td>65.5</td><td>77.1</td><td>55.4</td><td>72.3</td></tr><tr><td>Im.Aug</td><td>95.6</td><td>69.3</td><td>77.1</td><td>55.7</td><td>74.4</td></tr><tr><td>CLAP</td><td>98.5</td><td>73.1</td><td>81.3</td><td>58.3</td><td>77.8</td></tr></table>

<!-- Media -->

### D.2 Impact of Image and Text Augmentations

Identifying pure content factors poses a significant challenge. This difficulty primarily arises from the need for finding effective augmentations of observational data to alter style factors significantly while preserving content integrity.

Through the cross-modal alignment provided by CLIP, we discovered that disentangling in one modality can seamlessly improve representations in both modalities. The impact of image augmentations has been well-explored and found effective at preserving content, but traditional methods do not impose sufficient changes to remove all style information. Our exploration of text augmentations reveals that the logical structure of text and the relative ease of implementing style changes can have a significant impact on achieving disentanglement. However, more efficient methods are worthy of exploration.


<!-- Meanless: 28 Y. Cai et al.-->

<!-- Media -->

Table 9: CLAP reduces the variance in zero-shot performance across different prompts with CLIP pre-trained ViT-L/14 model.

<table><tr><td colspan="2" rowspan="2">Metric Method</td><td colspan="5">Zero-shot variance,avg. top-1 acc. (%) (↓)</td></tr><tr><td>PACS</td><td>VLCS</td><td/><td>DomainNet</td><td>Overall</td></tr><tr><td rowspan="3">$R$</td><td>CLIP</td><td>1.0</td><td>4.6</td><td>0.6</td><td>1.5</td><td>1.9</td></tr><tr><td>Im.Aug</td><td>0.3</td><td>3.4</td><td>0.6</td><td>1.3</td><td>1.4</td></tr><tr><td>CLAP</td><td>0.1</td><td>1.5</td><td>0.4</td><td>0.7</td><td>0.7</td></tr><tr><td rowspan="3">$\delta$</td><td>CLIP</td><td>0.4</td><td>2.0</td><td>0.3</td><td>0.6</td><td>0.8</td></tr><tr><td>Im.Aug</td><td>0.1</td><td>1.5</td><td>0.3</td><td>0.5</td><td>0.6</td></tr><tr><td>CLAP</td><td>0.0</td><td>0.6</td><td>0.2</td><td>0.3</td><td>0.3</td></tr><tr><td rowspan="3">${\Delta }_{\left( NC\right) }$</td><td>CLIP</td><td>6.6</td><td>11.5</td><td>8.8</td><td>7.8</td><td>8.7</td></tr><tr><td>Im.Aug</td><td>2.7</td><td>9.2</td><td>8.9</td><td>7.7</td><td>7.1</td></tr><tr><td>CLAP</td><td>0.1</td><td>7.7</td><td>6.3</td><td>5.9</td><td>5.0</td></tr></table>

Table 10: Domain-level zero-shot results of the ViT-L/14 model on the test datasets.

<table><tr><td rowspan="3">Datasets</td><td rowspan="3">Domains</td><td colspan="12">Domain-level avg. top-1 acc. (%) of zero-shot performance using ViT-L/14 (↑)</td></tr><tr><td colspan="3">ZS(C)</td><td colspan="3">ZS(CP)</td><td colspan="3">ZS(PC)</td><td colspan="3">ZS(NC)</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>97.2</td><td>98.0</td><td>98.8</td><td>96.8</td><td>98.0</td><td>98.5</td><td>98.7</td><td>98.8</td><td>98.9</td><td>85.6</td><td>91.6</td><td>98.6</td></tr><tr><td>C.</td><td>99.5</td><td>99.6</td><td>99.8</td><td>98.3</td><td>99.6</td><td>99.7</td><td>99.5</td><td>99.6</td><td>99.7</td><td>95.9</td><td>98.1</td><td>99.6</td></tr><tr><td>P.</td><td>99.9</td><td>100.0</td><td>100.0</td><td>99.4</td><td>99.5</td><td>100.0</td><td>99.9</td><td>100.0</td><td>99.9</td><td>91.1</td><td>97.5</td><td>99.9</td></tr><tr><td>S.</td><td>93.8</td><td>95.7</td><td>95.5</td><td>94.8</td><td>96.0</td><td>95.7</td><td>95.4</td><td>95.9</td><td>95.8</td><td>91.5</td><td>95.2</td><td>95.8</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>99.9</td><td>87.5</td><td>87.9</td><td>94.4</td></tr><tr><td>L.</td><td>57.4</td><td>60.1</td><td>64.3</td><td>71.3</td><td>71.6</td><td>72.6</td><td>71.7</td><td>72.0</td><td>72.6</td><td>53.8</td><td>59.7</td><td>60.7</td></tr><tr><td>S.</td><td>71.0</td><td>72.4</td><td>74.4</td><td>66.2</td><td>67.4</td><td>66.8</td><td>69.9</td><td>70.4</td><td>69.9</td><td>55.9</td><td>60.5</td><td>62.9</td></tr><tr><td>V.</td><td>80.0</td><td>81.6</td><td>84.3</td><td>85.2</td><td>85.7</td><td>86.2</td><td>85.1</td><td>85.3</td><td>86.4</td><td>65.0</td><td>69.3</td><td>74.3</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>86.2</td><td>86.3</td><td>87.7</td><td>85.7</td><td>86.2</td><td>88.1</td><td>87.0</td><td>87.0</td><td>87.8</td><td>78.1</td><td>77.1</td><td>80.7</td></tr><tr><td>C.</td><td>73.3</td><td>73.4</td><td>75.7</td><td>73.8</td><td>73.4</td><td>76.0</td><td>73.1</td><td>73.5</td><td>76.0</td><td>65.9</td><td>66.3</td><td>70.6</td></tr><tr><td>P.</td><td>92.0</td><td>91.8</td><td>93.6</td><td>92.3</td><td>92.4</td><td>94.3</td><td>92.9</td><td>92.8</td><td>94.1</td><td>80.7</td><td>81.0</td><td>86.8</td></tr><tr><td>R.</td><td>92.2</td><td>92.7</td><td>93.0</td><td>92.2</td><td>92.4</td><td>93.4</td><td>93.1</td><td>93.3</td><td>93.9</td><td>83.8</td><td>84.0</td><td>86.9</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>78.4</td><td>78.5</td><td>79.1</td><td>77.5</td><td>77.7</td><td>78.8</td><td>79.4</td><td>79.4</td><td>79.7</td><td>70.0</td><td>70.4</td><td>72.8</td></tr><tr><td>I.</td><td>52.9</td><td>53.0</td><td>54.6</td><td>50.4</td><td>50.7</td><td>53.6</td><td>51.7</td><td>52.0</td><td>53.9</td><td>45.3</td><td>45.2</td><td>48.8</td></tr><tr><td>P.</td><td>70.4</td><td>70.8</td><td>72.4</td><td>68.9</td><td>69.9</td><td>72.1</td><td>69.9</td><td>70.6</td><td>72.7</td><td>59.9</td><td>60.3</td><td>64.8</td></tr><tr><td>Q.</td><td>21.5</td><td>21.6</td><td>22.5</td><td>20.6</td><td>20.9</td><td>21.7</td><td>22.6</td><td>22.8</td><td>22.9</td><td>17.9</td><td>18.4</td><td>20.2</td></tr><tr><td>R.</td><td>85.8</td><td>85.9</td><td>85.9</td><td>85.3</td><td>85.5</td><td>85.7</td><td>86.3</td><td>86.4</td><td>86.2</td><td>77.5</td><td>77.5</td><td>78.7</td></tr><tr><td>S.</td><td>70.2</td><td>70.4</td><td>70.7</td><td>69.4</td><td>69.8</td><td>70.6</td><td>71.0</td><td>71.3</td><td>71.5</td><td>62.0</td><td>62.2</td><td>64.6</td></tr></table>

<!-- Media -->

A promising direction for future research is to explore efficient combinations of both modalities to enhance disentangled semantics. As each modality has its unique advantages-Text data recapitulates properties well since it is preprocessed by human intelligence, while image data is more precise in depicting the exact same objects or events due to its more detailed nature - the impact of combining augmentations of both modalities could be substantial.


<!-- Meanless: APPENDIX 29-->

<!-- Media -->

Table 11: Zero-shot performance with CLIP pre-trained ResNet50x16 model. CLAP demonstrates consistent enhancement across all datasets, validating its effectiveness.

<table><tr><td rowspan="2"/><td rowspan="2"/><td colspan="5">Zero-shot performance,avg. top-1 acc. (%) (↑)</td></tr><tr><td>PACS</td><td>VLCS</td><td colspan="3">OfficeHome DomainNet Overall</td></tr><tr><td rowspan="3">ZS(C)</td><td>CLIP</td><td>96.1</td><td>70.4</td><td>80.4</td><td>57.1</td><td>76.0</td></tr><tr><td>Im.Aug</td><td>96.4</td><td>74.7</td><td>80.4</td><td>57.1</td><td>77.2</td></tr><tr><td>CLAP</td><td>97.0</td><td>79.9</td><td>81.6</td><td>58.0</td><td>79.1</td></tr><tr><td rowspan="3">ZS(CP)</td><td>CLIP</td><td>95.0</td><td>73.5</td><td>79.0</td><td>56.1</td><td>75.9</td></tr><tr><td>Im.Aug</td><td>95.7</td><td>75.8</td><td>79.3</td><td>56.5</td><td>76.8</td></tr><tr><td>CLAP</td><td>96.7</td><td>80.3</td><td>79.9</td><td>57.4</td><td>78.6</td></tr><tr><td rowspan="3">ZS(PC)</td><td>CLIP</td><td>96.5</td><td>78.4</td><td>81.7</td><td>57.1</td><td>78.4</td></tr><tr><td>Im.Aug</td><td>97.0</td><td>79.8</td><td>81.8</td><td>57.4</td><td>79.0</td></tr><tr><td>CLAP</td><td>96.8</td><td>80.1</td><td>82.5</td><td>58.2</td><td>79.4</td></tr><tr><td rowspan="3">ZS(NC) Im.Aug</td><td>CLIP</td><td>86.4</td><td>61.2</td><td>69.3</td><td>48.2</td><td>66.3</td></tr><tr><td/><td>88.3</td><td>71.3</td><td>69.5</td><td>48.7</td><td>69.4</td></tr><tr><td>CLAP</td><td>94.9</td><td>80.1</td><td>71.9</td><td>50.6</td><td>74.4</td></tr></table>

Table 12: CLAP consistently reduces variances in zero-shot performance across different prompts with CLIP pre-trained ResNet50x16 model, validating its effectiveness.

<table><tr><td colspan="2" rowspan="2">Metric Method</td><td colspan="5">Zero-shot variance,avg. top-1 acc. (%) (↓)</td></tr><tr><td>PACS</td><td>VLCS</td><td/><td/><td>Overall</td></tr><tr><td rowspan="3">$R$</td><td>CLIP</td><td>1.5</td><td>8.0</td><td>2.7</td><td>1.1</td><td>3.3</td></tr><tr><td>Im. Aug</td><td>1.3</td><td>5.1</td><td>2.5</td><td>0.9</td><td>2.4</td></tr><tr><td>CLAP</td><td>0.3</td><td>0.4</td><td>2.6</td><td>0.8</td><td>1.0</td></tr><tr><td rowspan="3">$\delta$</td><td>CLIP</td><td>0.6</td><td>3.3</td><td>1.1</td><td>0.5</td><td>1.4</td></tr><tr><td>Im.Aug</td><td>0.5</td><td>2.2</td><td>1.0</td><td>0.4</td><td>1.0</td></tr><tr><td>CLAP</td><td>0.1</td><td>0.2</td><td>1.1</td><td>0.3</td><td>0.4</td></tr><tr><td rowspan="3">${\Delta }_{\left( NC\right) }$</td><td>CLIP</td><td>9.7</td><td>9.3</td><td>11.1</td><td>8.9</td><td>9.7</td></tr><tr><td>Im.Aug</td><td>8.1</td><td>3.5</td><td>10.9</td><td>8.5</td><td>7.7</td></tr><tr><td>CLAP</td><td>2.1</td><td>-0.1</td><td>9.7</td><td>7.5</td><td>4.8</td></tr></table>

<!-- Media -->

Table 13: Domain-level zero-shot results using RestNet50x16 on the test datasets.

<!-- Media -->

<table><tr><td rowspan="3">Datasets</td><td rowspan="3">Domains</td><td colspan="12">Domain-level avg. top-1 acc. (%) of zero-shot performance using RN50x16 (†)</td></tr><tr><td colspan="3">ZS(C)</td><td colspan="3">ZS(CP)</td><td colspan="3">ZS(PC)</td><td colspan="3">ZS(NC)</td></tr><tr><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td><td>CLIP</td><td>Im.Aug</td><td>CLAP</td></tr><tr><td rowspan="4">PACS</td><td>A.</td><td>95.7</td><td>95.8</td><td>97.2</td><td>93.7</td><td>95.5</td><td>96.5</td><td>95.7</td><td>96.7</td><td>96.8</td><td>81.2</td><td>84.0</td><td>94.5</td></tr><tr><td>C.</td><td>98.3</td><td>98.2</td><td>99.0</td><td>98.1</td><td>98.8</td><td>99.0</td><td>98.6</td><td>98.7</td><td>98.9</td><td>92.3</td><td>93.2</td><td>98.0</td></tr><tr><td>P.</td><td>98.9</td><td>98.6</td><td>99.9</td><td>98.4</td><td>97.8</td><td>99.8</td><td>99.8</td><td>99.9</td><td>99.9</td><td>85.3</td><td>87.3</td><td>95.2</td></tr><tr><td>S.</td><td>91.5</td><td>93.1</td><td>91.9</td><td>89.7</td><td>90.8</td><td>91.5</td><td>91.8</td><td>92.9</td><td>91.6</td><td>86.9</td><td>88.6</td><td>92.1</td></tr><tr><td rowspan="4">VLCS</td><td>C.</td><td>96.8</td><td>97.1</td><td>99.3</td><td>99.7</td><td>99.4</td><td>99.3</td><td>99.7</td><td>99.6</td><td>99.4</td><td>75.6</td><td>89.3</td><td>99.4</td></tr><tr><td>L.</td><td>53.4</td><td>60.8</td><td>65.9</td><td>51.6</td><td>58.9</td><td>67.3</td><td>59.5</td><td>68.1</td><td>66.8</td><td>54.1</td><td>60.6</td><td>67.0</td></tr><tr><td>S.</td><td>63.2</td><td>70.9</td><td>69.5</td><td>68.0</td><td>72.9</td><td>69.5</td><td>72.9</td><td>73.7</td><td>69.0</td><td>52.0</td><td>66.7</td><td>69.5</td></tr><tr><td>V.</td><td>68.4</td><td>70.1</td><td>85.2</td><td>74.5</td><td>72.1</td><td>85.3</td><td>81.7</td><td>78.0</td><td>85.2</td><td>63.1</td><td>68.5</td><td>84.5</td></tr><tr><td rowspan="4">OfficeHome</td><td>A.</td><td>82.2</td><td>82.5</td><td>83.5</td><td>79.7</td><td>79.9</td><td>80.6</td><td>82.0</td><td>82.4</td><td>83.4</td><td>67.7</td><td>68.9</td><td>72.1</td></tr><tr><td>C.</td><td>63.0</td><td>62.9</td><td>64.7</td><td>61.7</td><td>62.2</td><td>62.8</td><td>65.4</td><td>65.3</td><td>66.1</td><td>54.6</td><td>55.0</td><td>56.8</td></tr><tr><td>P.</td><td>88.2</td><td>87.9</td><td>89.0</td><td>87.4</td><td>87.5</td><td>88.5</td><td>90.0</td><td>89.9</td><td>90.6</td><td>75.4</td><td>75.3</td><td>78.2</td></tr><tr><td>R.</td><td>88.1</td><td>88.2</td><td>89.1</td><td>87.3</td><td>87.5</td><td>87.6</td><td>89.2</td><td>89.5</td><td>89.7</td><td>79.5</td><td>78.9</td><td>80.3</td></tr><tr><td rowspan="6">DomainNet</td><td>C.</td><td>69.0</td><td>68.9</td><td>69.6</td><td>68.6</td><td>68.6</td><td>69.4</td><td>69.9</td><td>70.0</td><td>70.4</td><td>59.5</td><td>60.1</td><td>61.4</td></tr><tr><td>I.</td><td>51.0</td><td>51.1</td><td>52.7</td><td>48.2</td><td>49.0</td><td>50.6</td><td>48.2</td><td>48.9</td><td>50.7</td><td>41.2</td><td>41.6</td><td>44.3</td></tr><tr><td>P.</td><td>65.2</td><td>65.6</td><td>66.5</td><td>63.7</td><td>64.4</td><td>65.6</td><td>65.4</td><td>65.9</td><td>67.0</td><td>53.5</td><td>54.3</td><td>56.8</td></tr><tr><td>Q.</td><td>11.8</td><td>11.9</td><td>12.7</td><td>12.3</td><td>12.6</td><td>13.1</td><td>11.8</td><td>12.2</td><td>12.7</td><td>9.3</td><td>9.7</td><td>11.0</td></tr><tr><td>R.</td><td>82.1</td><td>82.2</td><td>83.1</td><td>81.6</td><td>81.8</td><td>82.6</td><td>83.3</td><td>83.4</td><td>83.8</td><td>72.9</td><td>73.0</td><td>74.7</td></tr><tr><td>S.</td><td>63.2</td><td>63.1</td><td>63.6</td><td>62.0</td><td>62.4</td><td>63.4</td><td>63.9</td><td>63.8</td><td>64.6</td><td>53.1</td><td>53.4</td><td>55.3</td></tr></table>

<!-- Media -->