# LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments

Hongyu Ding ${}^{1, * }$ ,Ziming ${\mathrm{{Xu}}}^{1, * }$ ,Yudong Fang ${}^{2}$ ,You ${\mathrm{{Wu}}}^{1}$ ,Zixuan Chen ${}^{1}$ ,

Jieqi ${\mathrm{{Shi}}}^{2, \dagger  }$ ,Jing ${\mathrm{{Huo}}}^{1, \dagger  }$ ,Yifan ${\mathrm{{Zhang}}}^{3}$ ,Yang ${\mathrm{{Gao}}}^{2}$

Abstract- Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires an agent to navigate unseen environments based on natural language instructions without any prior training. Current methods face a critical trade-off: either rely on environment-specific waypoint predictors that limit scene generalization, or underutilize the reasoning capabilities of large models during navigation. We introduce LaViRA, a simple yet effective zero-shot framework that addresses this dilemma by decomposing action into a coarse-to-fine hierarchy: Language Action for high-level planning, Vision Action for middle-level perceptual grounding, and Robot Action for low-level control. This modular decomposition allows us to leverage the distinct strengths of different scales of Multimodal Large Language Models (MLLMs) at each stage, creating a system that is powerful in its reasoning, grounding and practical control. LaViRA significantly outperforms existing state-of-the-art methods on the VLN-CE benchmark, demonstrating superior generalization capabilities in unseen environments, while maintaining transparency and efficiency for real-world deployment. Project page: https: //robo-lavira.github.io/lavira-zs-vln/

## I. INTRODUCTION

Vision-and-Language Navigation (VLN) presents the challenge of grounding natural language instructions within visual observations to enable an embodied agent to navigate through previously unseen environments [1]. Early VLN research was primarily conducted in discrete, graph-based settings where navigation is simplified to selecting paths between predefined nodes. To bridge the gap to the real world, Vision-and-Language Navigation in Continuous Environments (VLN-CE) [2] was introduced, removing the reliance on connectivity graphs and forcing agents to contend with realistic challenges like continuous visual perception and fine-grained motor control.

The recent success of large language models (LLMs) and multimodal large language models (MLLMs) has inspired a new frontier: zero-shot VLN-CE [3]-[6], where an agent navigates without any environment-specific training. Two main paradigms have emerged from this research. Waypoint

<!-- Media -->

<!-- figureText: Waypoint Prediction Instruction: Walk behind the couch and turn into the next room on your right Go Pixel Go Front SUND Ours: Language-Vision-Robot Action Translation CH Go Pose Prior works -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_40_09_de593e.jpg"/>

Fig. 1: Prior methods rely on pre-trained waypoint prediction or value mapping with limited online planning. Our LaViRA framework instead decomposes navigation into language-level planning ("Go Front"), vision-level grounding ("Go Pixel"), and robot-level control ("Go Pose"), fully leveraging MLLMs reasoning for efficient coarse-to-fine decision-making.

<!-- Media -->

prediction with large model reasoning. These methods combine a pre-trained waypoint predictor with an LLM or MLLM [3], [4]. The predictor proposes navigable waypoints in the agent's surroundings, and the large model reasons over these discrete options to select the best next step. This design leverages the large model's high-level planning abilities but comes at a cost: the waypoint predictor requires separate pre-training and struggles to generalize to novel, unseen environments. Value mapping with vision language models. An alternative approach discards the waypoint predictor and instead uses a vision-language model (e.g., BLIP [7]) to generate a heatmap of semantic relevance over the visual scene. The agent navigates towards the highest-scoring region. While this avoids the need for predictor training, it typically relegates the powerful large models to an offline instruction-parsing role, leaving the vision-language model to handle online navigation alone. This underutilizes the large model's sophisticated reasoning and decision-making capabilities.

These two paradigms reveal a fundamental trade-off. Waypoint-based methods excel at high-level reasoning but are constrained by a separate, inflexible waypoint prediction module. Value-mapping methods are more perceptually grounded but lack dynamic, high-level reasoning during navigation. This leads us to a simple but critical question:

---

<!-- Footnote -->

${}^{ * }$ Equal Contribution, ${}^{ \dagger  }$ Corresponding Author

${}^{1}$ Hongyu Ding,Ziming Xu,You Wu,Zixuan Chen and Jing Huo are with the School of Computer Science, Nanjing University, China. Emails: \{hongyuding, zimingxu, you\}@smail.nju.edu.cn, \{chenzx, huojing\}@nju.edu.cn

${}^{2}$ Yudong Fang,Jieqi Shi and Yang Gao are with the School of Intelligence Science and Technology, Nanjing University, China. Emails: 231880023@smail.nju.edu.cn, isjieqi@nju.edu.cn, gaoy@nju.edu.cn

${}^{3}$ Yifan Zhang is with the Institute of Automation,Chinese Academy of Sciences, China. Emails: yfzhang@nlpr.ia.ac.cn

<!-- Footnote -->

---

<!-- Meanless: NSRON ONSL EWGATIONSKH-->


Can we design a purely zero-shot VLN-CE framework that (1) removes the dependency on a pre-trained waypoint predictor and (2) fully harnesses the reasoning abilities of MLLMs for navigation decision making?

We answer this question with LaViRA: Language-Vision-Robot Action Translation. Our key insight is to decompose navigation into a coarse-to-fine, multi-stage action space, progressively refined from language to vision to robot control. Rather than requiring a single model to produce low-level controls directly, we allocate each stage to a model scale that best matches its reasoning or perceptual demands, allowing the system to exploit the complementary strengths of different models.

1) Language Action: A powerful MLLM acts as a high-level planner, analyzing the instruction, history, and current observation to produce a coarse strategic decision, such as which general direction to head, whether to backtrack, or when to stop.

2) Vision Action: A smaller, efficient MLLM takes this high-level decision and grounds it in the visual scene, identifying a specific object or region to move towards.

3) Robot Action: A simple, rule-based controller executes the low-level movement to the identified visual target.

This hierarchical decomposition offers three main advantages. First, it is purely zero-shot, removing the dependency on pre-trained waypoint predictors. Second, it fully engages MLLM reasoning at multiple granularities, from high-level planning to middle-level perceptual grounding and then to low-level control. Third, its modular design ensures transparency and practicality, enabling straightforward adaptation to both simulated and real-world robots.

Our contributions are as follows:

- We propose a general action decomposition strategy for zero-shot VLN-CE, separating navigation into language-level planning, vision-level grounding, and robot-level control, enabling flexible integration of reasoning and perception modules.

- We implement this strategy in LaViRA, a practical Language-Vision-Robot Action framework that leverages different scales of MLLMs in a fully zero-shot manner.

- We achieve state-of-the-art zero-shot performance on the VLN-CE benchmark while preserving effectiveness and efficiency for real-world deployment.

## II. RELATED WORK

## A. Vision-and-Language Navigation

Early research in Vision-and-Language Navigation (VLN) [1], where an agent follows instructions to navigate unseen environments, predominantly focused on discrete, graph-based settings [8]-[10]. Such environments enable high-level decision-making but fail to capture the continuous control demands of real-world scenarios. To address this, VLN in Continuous Environments (VLN-CE) [2] removes reliance on connectivity graphs and requires agents to perform fine-grained control actions like moving forward, rotating, and avoiding obstacles. This transition introduces new challenges in scene analysis, generalization, and low-level control.

Numerous VLN methods in both discrete and continuous environments have improved performance through enhanced cross-modal alignment [11], [12], reinforcement learning [13], data augmentation [14]-[16], and map-based representations [17]-[19]. However, these learning-based approaches require substantial environment-specific training, which limits their applicability for zero-shot deployment-motivating recent interest in training-free VLN-CE solutions.

## B. Zero-Shot VLN with Foundation Models

The rise of powerful foundation models like Large Language Models (LLMs) [20], [21] and Multimodal Large Language Models (MLLMs) [7], [22], [23] has spurred a new wave of zero-shot VLN-CE research, which can be categorized by how foundation models are integrated with robot control. One dominant paradigm is waypoint-based navigation [3], [4]. These methods use a LLM/MLLM to select from waypoints proposed by a pre-trained predictor [24]. However, this creates a critical dependency on the predictor, which may fail to generalize to new scenes and limits backtracking flexibility [4]. Another approach is heuristic value-mapping [5], [6], where a Vision-Language Model (VLM) generates a semantic heatmap to guide the agent. In these frameworks, a powerful LLM is only used for offline instruction parsing, underutilizing its dynamic reasoning capabilities during navigation. Furthermore, the reliance on hard constraints for progress estimation can introduce rigidity and limit adaptability in complex scenarios [5].

Our LaViRA framework bridges the gap between these approaches, introducing a Language-Vision-Robot Action Translation that progressively refines action from language-level planning to vision-level grounding to robot-level execution. This design allows MLLMs reasoning to directly influence decisions at multiple granularities, enabling fully zero-shot, interpretable, and adaptable navigation in continuous environments.

## C. Hierarchical Structure in VLN

Hierarchical architectures have emerged as a powerful paradigm for addressing complex, long-horizon navigation tasks. This strategy effectively decomposes the problem into high-level planning and low-level execution, allowing different modules to specialize. Current hierarchical approaches often combine high-level semantic planning with low-level motion control. One common strategy involves dividing the environment into distinct topological regions and having a high-level planner select a sequence of zones to traverse, while a low-level controller navigates within each zone [25], [26]. Other methods learn policies for both the manager and worker through techniques like imitation learning [27] or reinforcement learning [28]. A recent example, $\operatorname{Nav}{A}^{3}$ [29], uses a global policy to identify target regions and a pre-trained model for fine-grained navigation.


<!-- Media -->

<!-- figureText: "Turn right and walk Vision Action Robot Action Middle-level Control Grounding Target Object/Region doorway leading out of the kitchen and into the next room ... ’ Language Action instruction 1. Navigate to <direction> 2. Backtrack to<waypoint> observation Planning " The first part of the instruction has been completed. The next step is to turn right and ... " -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_40_09_bc0919.jpg"/>

Fig. 2: The LaViRA Pipeline. Our framework decomposes navigation into three sequential stages. (1) Language Action: A large MLLM processes the instruction, history, and current observation to generate a high-level plan, deciding whether to move forward, turn, backtrack, or stop. (2) Vision Action: A smaller, more efficient MLLM takes the high-level plan and progress estimation to identify a specific visual target in the chosen direction, outputting its bounding box and description. (3) Robot Action: The target's pixel coordinates are projected onto a global map, and a rule-based controller navigates the robot to the destination. This hierarchical process enables generalized and robust zero-shot vision-language navigation.

<!-- Media -->

However, a common limitation of these methods is their reliance on extensive, environment-specific training, which restricts generalization and hinders zero-shot deployment. In contrast, LaViRA's Language-Vision-Robot action decomposition is fully zero-shot. By leveraging MLLM reasoning and a rule-based controller, it eliminates the need for any training, which simplifies deployment and enhances modularity.

## III. Proposed Method

Our approach, LaViRA, introduces a novel framework for zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE). The core idea is simple yet effective: we decompose the complex navigation task into a coarse-to-fine hierarchy of actions: Language Action, Vision Action, and Robot Action. This modular decomposition allows us to leverage the distinct strengths of different scales of Multimodal Large Language Models (MLLMs) at each stage, creating a system that is powerful in its reasoning, grounding and practical for real-world deployment, all without requiring any environment-specific training.

The overall pipeline is illustrated in Figure 2. In the VLN-CE task, an agent must follow a natural language instruction $\mathcal{I}$ through an unseen environment. At each timestep $t$ ,it uses an egocentric observation ${I}_{t}$ to choose its next action ${\mathcal{A}}_{t}$ from a continuous space. To address this,our method decomposes the navigation process into a sequence of three hierarchical actions: a high-level directional plan (Language Action), the grounding of this plan into a specific visual target (Vision Action), and finally, the low-level movement to reach it (Robot Action). We detail each stage below.

## A. Language Action: High-Level Planning

The first stage of our framework addresses the question: Where should I generally go next? To answer this, we employ a powerful, large-scale MLLM (e.g., GPT-4o) that functions as a high-level planner. This model is responsible for interpreting the full context of the navigation task and producing a coarse, directional command.

Specifically, the model receives three types of input:

- Language Instruction $\mathcal{I}$ : The given natural language instruction provided at the start of the task.

- Current Observation ${\mathcal{O}}_{t}$ : A set of four images corresponding to the agent's front, left, right, and back views $\left\{  {{I}_{\text{front }},{I}_{\text{left }},{I}_{\text{right }},{I}_{\text{back }}}\right\}$ ,providing an informative understanding of the current location.

- Navigation History ${\mathcal{H}}_{t}$ : A structured summary of past observations and actions, formatted as a sequence like ${\mathcal{H}}_{t} = \left\{  {\left( {{\mathcal{O}}_{0},{\mathcal{A}}_{0}}\right) ,\left( {{\mathcal{O}}_{1},{\mathcal{A}}_{1}}\right) ,\ldots ,\left( {{\mathcal{O}}_{t - 1},{\mathcal{A}}_{t - 1}}\right) }\right\}$ . This provides crucial context on what has already been accomplished.

Given this rich multimodal context, the MLLM performs two tasks simultaneously. First, it generates a Progress Estimation ${\mathcal{P}}_{t}$ ,a natural language assessment of how much of the instruction has been completed. This explicit reasoning step forces the model to track its progress against the overall instruction. Second, based on its analysis, the model selects a Language Action ${\mathcal{A}}_{t}^{\text{lang }}$ from a discrete set:

- navigate to <direction>: Move forward, left, right, or behind.

- backtrack to <waypoint>: Return to a previously visited waypoint location.

- stop: Terminate the navigation.

This process can be formulated as:

$$
\left( {{\mathcal{A}}_{t}^{\text{lang }},{\mathcal{P}}_{t}}\right)  = {\operatorname{MLLM}}_{\text{large }}\left( {\mathcal{I},{\mathcal{H}}_{t},{\mathcal{O}}_{t}}\right)  \tag{1}
$$

This stage effectively abstracts the continuous environment into a few high-level choices, allowing the large MLLM to focus its powerful reasoning capabilities on strategic, long-term planning.

## B. Vision Action: Perceptual Grounding

Once a high-level Language Action is determined, the next stage must answer: What specific thing should I move towards in that direction? This is the role of the Vision Action stage, where we ground the abstract plan into a concrete perceptual target.


<!-- Media -->

<!-- figureText: Language Action Prompt Vision Action Prompt You are an expert specialized in vision-language navigation task. Please review the contexts below and help the robot navigate Navigation Instruction "Turn right and walk to the cabinet in front of you. Turn right and ..." Current Observation \{ "chosen direction image" Navigation History - Previously visited targets: cabinet in the kitchen, doorway leading out of the kitchen, .. - Progress Analysis: "The first part of the instruction has been completed. The next step is ..." Your Task 1. Analyze at what stage the current instruction has been completed and what should be done new 2. Identify the most relevant target object/region for what you should do next. 3. Specify ONLY ONE target object/region. And it should not be too close to you Response format (JSON) "reasoning": "<brief explanation of decision>", "bbox_2d": [x1, y1, x2, y2] "target": "<description of target object/region>" Guidelines - Target description should be specific and clear - Consider the instruction completion progress based on visited targets - Use the progress analysis to inform your decision Now, decide the Vision Action based on the above contexts. You are an expert specialized in vision-language navigation task. Please review the contexts below and help the robot navigate. Navigation Instruction "Turn right and walk to the cabinet in front of you. Turn right and ..." Current Observation \{ "front" Navigation History Action Options 1. navigate to [direction_id] - choose a direction and go forwar 2. backtrack to [waypoint_id] - return to a previous waypoint 3. stop - end of navigation trajectory Response format (JSON) "progress_analysis": "<assessment of current progress toward instruction completion>", "reasoning": "<explanation of chosen action>", Guidelines - Consider instruction completion progress - Backtrack only if current path seems unproductive or dead-end - Choose direction that best advances toward goal Now, decide the Language Action based on the above contexts. -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_40_09_9313b1.jpg"/>

Fig. 3: Prompts for Language and Vision Actions. (Left) The prompt for the Language Action model, which takes in full context to decide on a high-level direction. (Right) The prompt for the Vision Action model, which uses the output from the first stage to ground the decision in a specific visual target.

<!-- Media -->

For this task, we use a smaller, more efficient MLLM (e.g., Qwen2.5-VL-32B). This choice is deliberate: grounding is a more focused perception task that does not require the same extensive world knowledge as high-level planning, making a smaller model more suitable and computationally efficient. As our ablation studies in Section IV-C confirm, pairing a powerful planner with a specialized, efficient grounding model yields optimal performance. The model is prompted with:

- Language Instruction $\mathcal{I}$ : The original instruction.

- Progress Estimation ${\mathcal{P}}_{t}$ : The text generated by the Language Action model.

- Chosen Direction Image ${I}_{dir}$ : The single image corresponding to the direction chosen in the previous stage ${\mathcal{A}}_{t}^{\text{lang }}$ .

The model's task is to identify the most relevant object or region in the image that aligns with the next step of the instruction. It outputs a Vision Action ${\mathcal{A}}_{t}^{vis}$ in a structured format containing a bounding box and its description. This can be expressed as:

$$
{\mathcal{A}}_{t}^{\text{vis }} = {\operatorname{MLLM}}_{\text{small }}\left( {\mathcal{I},{\mathcal{P}}_{t},{I}_{\text{dir }}}\right)  \tag{2}
$$

The output ${\mathcal{A}}_{t}^{\text{vis }}$ is a dictionary containing:

- Bounding Box ${bbo}{x}_{2d}$ : The 2D coordinates $\lbrack  \times  1,\mathrm{y}1$ , $\mathrm{x}2,\mathrm{y}2\rbrack$ localizing the target.

- Target Description: A textual description of the identified object or region.

As shown in Figure 3, the prompt instructs the model to select a target that is not too close, encouraging meaningful progress. This stage effectively translates the high-level plan into a tangible, visually verifiable goal.

## C. Robot Action: Low-Level Control

The final Robot Action stage answers: How do I physically get there? It translates the identified visual target into low-level motor commands using a robust, rule-based controller. Pixel-to-World Projection. First, we select the bottom-center pixel of the target's bounding box and project it into a 3D point in the world frame. This involves unprojecting the 2D pixel to the camera's 3D coordinate system using the intrinsic matrix $\mathbf{K}$ :

$$
{\left\lbrack  {X}_{\text{cam }},{Y}_{\text{cam }},{Z}_{\text{cam }}\right\rbrack  }^{T} = d \cdot  {\mathbf{K}}^{-1} \cdot  {\left\lbrack  {u}_{\text{target }},{v}_{\text{target }},1\right\rbrack  }^{T} \tag{3}
$$

where $d$ is the depth. This $3\mathrm{D}$ point is then transformed from the camera frame to the world frame using the agent's current pose ${\mathbf{T}}_{\text{agent }} = \left( {{x}_{\text{agent }},{z}_{\text{agent }},{\theta }_{\text{agent }}}\right)$ to yield a target position ${\mathbf{p}}_{\text{world }} = \left( {{x}_{\text{world }},{z}_{\text{world }}}\right)$ on the 2D map:

$$
\left\lbrack  \begin{array}{l} {x}_{\text{world }} \\  {z}_{\text{world }} \end{array}\right\rbrack   = \left\lbrack  \begin{array}{l} {x}_{\text{agent }} \\  {z}_{\text{agent }} \end{array}\right\rbrack   + \left\lbrack  \begin{matrix} \cos \theta &  - \sin \theta \\  \sin \theta & \cos \theta  \end{matrix}\right\rbrack  \left\lbrack  \begin{matrix} {Z}_{\text{cam }} \\   - {X}_{\text{cam }} \end{matrix}\right\rbrack   \tag{4}
$$


Path Planning and Control. Given the target position ${\mathbf{p}}_{\text{world }}$ ,the agent computes a short-term path using the Fast Marching Method (FMM). A low-level controller then executes this path with local obstacle avoidance. This deterministic final step grounds the reasoning chain in physical action, ensuring interpretability and making the system adaptable to different robot platforms by simply swapping the controller.

## IV. Simulation Experiments

We validate LaViRA in simulation to answer three key questions: (1) How does it compare against state-of-the-art methods on the standard VLN-CE benchmark? (2) Does performance support our hypothesis of using different MLLM scales for different decision granularities? (3) What is the contribution of each component in our framework?

## A. Experimental Setup

Environment and Dataset. We use the Habitat simulator [30] with the VLN-CE dataset [2], which extends the R2R benchmark from Matterport3D (MP3D) [8] for continuous navigation. Following recent zero-shot works [3], [4], we report results on a standard 100-episode subset from the validation unseen split. An episode is successful if the agent stops within 3 meters of the target.

Evaluation Metrics. We use standard VLN metrics: Navigation Error (NE), the final distance to goal; Success Rate (SR),our primary metric for stopping within $3\mathrm{\;m}$ ; Oracle Success Rate (OSR), SR if stopping at the closest point on the path; and Success rate weighted by Path Length (SPL), which penalizes inefficient paths. To account for the inherent stochasticity in the outputs of MLLMs, we repeat 3 runs for each experiment over the 100-episode set and report the mean and standard deviation for all metrics.

Implementation Details. Our zero-shot framework requires no environment-specific training. For the high-level Language Action stage, we evaluate two leading MLLMs: Gemini-2.5-Pro and GPT-4o. For the Vision Action stage, we primarily use the efficient Qwen2.5-VL-32B, with other models explored in our ablation studies. The agent's observation is composed of posed ${640} \times  {480}$ RGB-D images, low-level path planning is executed using the Fast Marching Method (FMM) on a global map constructed from depth observations. All experiments were conducted on 8 NVIDIA RTX 4090 GPUs for parallel evaluation.

Inference Cost. To provide a clear picture of the computational cost, we report the token usage for GPT-4o + Qwen2.5- VL-32B over the 100-episode validation set. On average, each trajectory required approximately 32,682 tokens with 7.93 calls for the high-level planner (GPT-4o) and 8,050 tokens with 7.50 calls for the grounding model (Qwen2.5- VL-32B). Based on current API pricing, the total inference cost is approximately \$0.084 USD per episode. This highlights the efficiency of our hierarchical design, where an expensive, powerful model is used for high-impact decisions, while a lightweight model handles the low-level perceptual grounding task.

<!-- Media -->

TABLE I: Main results on the VLN-CE benchmark. LaViRA significantly outperforms all previous zero-shot methods. Best and second-best zero-shot results are highlighted.

<table><tr><td>$\mathbf{{Method}}$</td><td>NE↓</td><td>OSR↑</td><td>SR↑</td><td>SPL↑</td></tr><tr><td colspan="5">$\mathbf{{SupervisedLearning}}$</td></tr><tr><td>CMA [31]</td><td>6.92</td><td>45</td><td>37</td><td>32.2</td></tr><tr><td>RecBERT [31]</td><td>5.80</td><td>57</td><td>48</td><td>43.2</td></tr><tr><td>ETPNav [18]</td><td>5.15</td><td>58</td><td>52</td><td>52.2</td></tr><tr><td>BEVBert [32]</td><td>5.13</td><td>64</td><td>60</td><td>53.4</td></tr><tr><td colspan="5">Zero-Shot</td></tr><tr><td>Random</td><td>8.63</td><td>12</td><td>2</td><td>1.5</td></tr><tr><td>NavGPT-CE [33]</td><td>8.37</td><td>26.9</td><td>16.3</td><td>10.2</td></tr><tr><td>DiscussNav-CE [33]</td><td>7.77</td><td>15</td><td>11</td><td>10.5</td></tr><tr><td>MapGPT-CE [34]</td><td>8.16</td><td>21</td><td>7</td><td>5.0</td></tr><tr><td>Open-Nav [3]</td><td>6.70</td><td>23</td><td>19</td><td>16.1</td></tr><tr><td>SmartWay [4]</td><td>7.11</td><td>51</td><td>29</td><td>22.5</td></tr><tr><td>InstructNav [6]</td><td>6.89</td><td>47</td><td>31</td><td>24.0</td></tr><tr><td>CA-Nav [5]</td><td>7.58</td><td>48.0</td><td>25.3</td><td>10.8</td></tr><tr><td>GC-VLN [35]</td><td>7.30</td><td>41.8</td><td>33.6</td><td>16.3</td></tr><tr><td>LaViRA(GPT-4o)</td><td>${6.43} \pm  {0.28}$</td><td>${43.3} \pm  {3.2}$</td><td>${36.0} \pm  {1.7}$</td><td>${28.3} \pm  {0.8}$</td></tr><tr><td>LaViRA(Gemini-2.5-Pro)</td><td>${6.54} \pm  {0.27}$</td><td>${48.7} \pm  {2.1}$</td><td>${38.3} \pm  {0.6}$</td><td>${28.3} \pm  {0.9}$</td></tr></table>

<!-- Media -->

## B. Main Results

We compare LaViRA with a range of existing methods on the VLN-CE benchmark. As detailed in Table 1, our method sets a new state-of-the-art for zero-shot VLN-CE.

The LaViRA variant using Gemini-2.5-Pro as the high-level planner achieves a SR of ${38.3}\%$ and an SPL of ${28.3}\%$ . This represents a significant improvement over the previous leading zero-shot method, InstructNav [6], with a 7.3-point gain in SR and a 4.3-point gain in SPL. Furthermore, the low standard deviations across multiple runs underscore the robustness and stability of our framework, a key advantage in real-world applications where consistency is critical. Notably, our framework's performance surpasses even supervised methods in SR, underscoring the powerful reasoning and generalization capabilities unlocked by our hierarchical decomposition. The GPT-4o variant of LaViRA also shows strong and stable results. These findings validate our central hypothesis: by decomposing the navigation task and leveraging the advanced reasoning of MLLMs, we can achieve superior and robust zero-shot performance without relying on a pre-trained waypoint predictor.

## C. Ablation Studies

We performed a series of ablation studies to analyze LaViRA's performance and quantify the contribution of its core components. Our baseline for these studies is LaViRA (GPT-4o + Qwen2.5-VL-32B). Although the Gemini-2.5-Pro variant delivered superior performance, we used the GPT- 4o variant for ablations due to documented stability issues with the Gemini-2.5-Pro API during our experiments, which could have compromised the consistency of iterative testing. The GPT-4o model provided the necessary reliability for a rigorous and reproducible analysis.

Model Selection. Our framework is model-agnostic, allowing flexible combinations of MLLMs. As shown in Table II, our experiments confirm that pairing models of different scales according to task granularity is crucial. Using a powerful MLLM (e.g., GPT-4o) for high-level Language Action (LA) is key; replacing it with the smaller Qwen2.5-VL-72B leads to a 7.0-point drop in SPL. For the more focused Vision Action (VA) stage, an efficient model like Qwen2.5-VL- ${32}\mathrm{\;B}$ proves highly effective. This aligns with the principles of hierarchical VLN discussed in our related work II-C where specialized modules handle different levels of the task. Interestingly, using a powerful model like GPT-4o for both stages significantly degrades performance (SPL drops from ${28.3}\%$ to ${16.8}\%$ ). This suggests that simply using the largest model is not optimal, the best results-pairing a top-tier planning model with an efficient grounding model-validate our core hypothesis and demonstrate a key advantage of our zero-shot hierarchical approach.


<!-- Media -->

TABLE II: Ablation on model selection. Performance is maximized when a powerful MLLM handles high-level Language Actions (LAM) and an efficient MLLM handles focused Vision Actions (VAM). Qwen-32B and Qwen-72B are short for Qwen2.5-VL-32B and Qwen2.5-VL-72B.

<table><tr><td>$\mathbf{{LAM}}$</td><td>$\mathbf{{VAM}}$</td><td>$\mathbf{{NE} \downarrow  }$</td><td>OSR↑</td><td>SR↑</td><td>SPL↑</td></tr><tr><td>Qwen-32B</td><td>Qwen-32B</td><td>${8.04} \pm  {0.28}$</td><td>${25.3} \pm  {2.1}$</td><td>${19.7} \pm  {3.5}$</td><td>${14.3} \pm  {2.6}$</td></tr><tr><td>Qwen-72B</td><td>Qwen-32B</td><td>${7.18} \pm  {0.18}$</td><td>${33.3} \pm  {2.1}$</td><td>${27.7} \pm  {2.3}$</td><td>${21.3} \pm  {2.7}$</td></tr><tr><td>GPT-40</td><td>Qwen-32B</td><td>${6.43} \pm  {0.28}$</td><td>${43.3} \pm  {3.2}$</td><td>${36.0} \pm  {1.7}$</td><td>$\mathbf{{28.3} \pm  {0.8}}$</td></tr><tr><td>GPT-4o</td><td>Qwen-72B</td><td>${6.78} \pm  {0.11}$</td><td>${39.3} \pm  {2.1}$</td><td>${32.3} \pm  {1.2}$</td><td>${23.8} \pm  {0.7}$</td></tr><tr><td>GPT-4o</td><td>GPT-40</td><td>${7.47} \pm  {0.24}$</td><td>${26.0} \pm  {7.5}$</td><td>${20.7} \pm  {5.7}$</td><td>${16.8} \pm  {4.9}$</td></tr></table>

TABLE III: Ablation on framework design. A three-stage pipeline, rich visual history, and flexible backtracking are all crucial for robust navigation.

<table><tr><td>Configuration</td><td>$\mathbf{{NE} \downarrow  }$</td><td>OSR↑</td><td>SR↑</td><td>SPL↑</td></tr><tr><td>LaViRA (Full)</td><td>${6.43} \pm  {0.28}$</td><td>${43.3} \pm  {3.2}$</td><td>${36.0} \pm  {1.7}$</td><td>${28.3} \pm  {0.8}$</td></tr><tr><td colspan="5">Framework Decomposition</td></tr><tr><td>w/o LA</td><td>${8.94} \pm  {0.53}$</td><td>${13.0} \pm  {3.0}$</td><td>${6.7} \pm  {0.6}$</td><td>${4.4} \pm  {1.2}$</td></tr><tr><td>w/o VA</td><td>${7.28} \pm  {0.23}$</td><td>${34.0} \pm  {4.0}$</td><td>${23.0} \pm  {5.2}$</td><td>${13.9} \pm  {3.4}$</td></tr><tr><td>w/o LA+VA</td><td>${9.21} \pm  {0.11}$</td><td>${1.7} \pm  {0.6}$</td><td>${0.0} \pm  {0.0}$</td><td>${0.0} \pm  {0.0}$</td></tr><tr><td colspan="5">History Representation</td></tr><tr><td>text obs. + act.</td><td>${6.99} \pm  {0.32}$</td><td>${37.7} \pm  {5.5}$</td><td>${31.0} \pm  {3.6}$</td><td>${23.0} \pm  {2.8}$</td></tr><tr><td>only act.</td><td>${6.54} \pm  {0.11}$</td><td>${38.3} \pm  {1.2}$</td><td>${32.7} \pm  {2.1}$</td><td>${25.1} \pm  {1.8}$</td></tr><tr><td>only visual obs.</td><td>${6.64} \pm  {0.23}$</td><td>${36.0} \pm  {5.6}$</td><td>${29.3} \pm  {3.5}$</td><td>${22.5} \pm  {2.2}$</td></tr><tr><td>only text obs.</td><td>${6.96} \pm  {0.24}$</td><td>${35.7} \pm  {4.0}$</td><td>${27.7} \pm  {2.5}$</td><td>${21.8} \pm  {1.8}$</td></tr><tr><td>w/o history</td><td>${6.90} \pm  {0.46}$</td><td>${36.3} \pm  {7.0}$</td><td>${27.0} \pm  {5.6}$</td><td>${19.4} \pm  {7.3}$</td></tr><tr><td colspan="5">Backtracking Mechanism</td></tr><tr><td>w/o backtrack</td><td>${6.92} \pm  {0.32}$</td><td>${42.0} \pm  {3.0}$</td><td>${30.0} \pm  {4.4}$</td><td>${22.2} \pm  {4.0}$</td></tr><tr><td>last waypoint only</td><td>${6.65} \pm  {0.16}$</td><td>${41.7} \pm  {6.0}$</td><td>${31.7} \pm  {1.5}$</td><td>${23.5} \pm  {1.0}$</td></tr><tr><td colspan="5">Pixel Point Selection</td></tr><tr><td>direct output point</td><td>${7.00} \pm  {0.84}$</td><td>${45.3} \pm  {3.1}$</td><td>${31.0} \pm  {5.6}$</td><td>${19.4} \pm  {5.3}$</td></tr><tr><td>bbox median depth</td><td>${6.73} \pm  {0.09}$</td><td>${51.0} \pm  {2.6}$</td><td>${34.0} \pm  {1.0}$</td><td>${21.8} \pm  {1.2}$</td></tr></table>

<!-- Media -->

Framework Design Choices. We conducted further ablations to evaluate key design choices within our framework, as consolidated in Table III.

First, we ablated our hierarchical structure. An end-to-end baseline ("w/o LA+VA") that directly predicts actions fails completely,achieving an SPL of $0\%$ . Removing the high-level planner ("w/o LA") and using a single MLLM to directly output a bounding box yields a low ${4.4}\%$ SPL. Conversely, removing the perceptual grounding model ("w/o VA") and having the planner select only a direction achieves 13.9% SPL. Our full framework surpasses this by 14.4 absolute percentage points in SPL, proving the coarse-to-fine decomposition is critical for generating efficient and successful paths.

Next, we analyzed history representation. Replacing visual history with textual descriptions ("text obs. + act.") drops SPL by 5.3 points to 23.0%, showing MLLMs benefit more from raw visual data than summaries when planning efficient paths. Using only past actions ("only act.", 25.1% SPL) or only textual observations ("only text obs.", 21.8% SPL) is also less effective. This confirms that a combination of visual observations and actions provides the richest context for optimal navigation performance.

We then evaluated our backtracking mechanism. Disabling it ("w/o backtrack") causes a 6.1-point SPL drop. A restrictive policy allowing only backtracking to the last way-point ("last waypoint only") is still 4.8 points worse in SPL than our default strategy, which allows selecting any previous waypoint. This demonstrates that flexible, long-range error correction is crucial for improving path efficiency.

Finally, we ablated the pixel point selection method. Having the model directly output pixel coordinates ("direct output point") resulted in a ${19.4}\%$ SPL,8.9 points lower than our approach. By first identifying a bounding box and then applying a geometrically-grounded heuristic (bottom-center point), we achieve more reliable and efficient navigation.

## D. Qualitative Analysis

To offer qualitative insights into LaViRA's decision-making, Figure 4 shows a successful navigation run and common failures. In the success case, LaViRA demonstrates its coarse-to-fine approach: it first makes a high-level directional choice ("navigate left"), then grounds this in a visual landmark ("Black door with glass panels"), and finally selects a precise waypoint. This hierarchical process validates our design.

The failure cases illustrate common errors: (1) A Language Action error from an ambiguous instruction. For example, when the instruction asks to "walk to the door in the front" where there are several doors in agent's observation, LAM fails to identify the door desired. (2) A Vision Action error where the correct object description is grounded to the wrong image region. Though VAM exhibit a strong capability to ground objects in a bounding box, it sometimes fails to locate a relatively larger area like "hallway" or "living room" and chooses a meaningless region in the observation. (3) A simulation-induced error where depth reconstruction artifacts cause incorrect $3\mathrm{D}$ projection of the target. For instance, in the Habitat simulator, transparent objects like windows are not assigned depth values. This lack of data disrupts the agent's spatial perception, causing it to mislocate the target.

## V. REAL-WORLD EXPERIMENTS

To validate LaViRA's practicality beyond simulation, we deployed it on two distinct real-world robots: a Unitree Go1 quadruped and an Agilex Cobot Magic wheeled platform. These experiments tested the framework's sim-to-real transferability, requiring only the replacement of the low-level robot controller. The Unitree Go1 was equipped with a Jetson Orin NX and an Intel RealSense D435i camera, using its native velocity controller to navigate. The Agilex Cobot Magic platform used a chest-mounted Orbbec Dabai camera and a 2-DOF velocity controller for its mobile base. In both deployments, the onboard computers called the respective MLLM APIs for Language and Vision Actions, transmitting the resulting target pixel coordinates to the robot's navigation system.


<!-- Media -->

<!-- figureText: Instruction: Turn to face black door. Walk forward. Turn right to walk past stairs. Stop in front of door on right. LAM's Direction Error NAV: The stairs on the righ STOP side of the image on the right GT: front Walk to the door in the front and turn right .. VAM's Detection Error VAM:Doorway leading into hallway RAM's Localization Error t=144 t=165 Exit the kitchen, walk down the hall and .. NAV: Black door with glass NAV: The window on the right BACKTRACK: to waypoint 2 panels on the left side of the room on the right Topdown Time t=12 t=60 t=96 -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_40_09_40787f.jpg"/>

Fig. 4: Visualization examples. (Left) Navigation visualization: The outputs of Language Action model are denoted in blue text, Vision Action model outputs bounding boxes in green and target description in red. The robot's position and orientation are represented by a red dot with arrow, blue dots denote history waypoints, and the green cross marks the target position of the next waypoint. (Right) Failure cases visualization: Language action model misjudges direction due to ambiguous instruction; Vision action model gives correct target description but selects wrong region; Visual reconstruction errors in simulation cause incorrect target localization.

<!-- figureText: Instruction: Go to the shelf on the left, turn left and move to the black chair, then Instruction: Go until you reach the window, exit the open door at your right. Instruction: Go until you reach the window, exit the open door at your right stop by the white cabinet in front of you. Instruction: Go to the chair with a box in front of you, turn left, move to the tv in the front, and stop there. -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_40_09_219802.jpg"/>

Fig. 5: Real-world experiment examples. LaViRA guides a Unitree Go1 quadruped (top) and an Agilex Cobot Magic wheeled robot (bottom) in an office. The visualization shows the third-person view of the robot's trajectory alongside the agent's ego view and targets, demonstrating successful real-world adaptation on diverse platforms.

<!-- Media -->

As shown in Figure 5, both platforms successfully executed navigation tasks in complex indoor environments. This demonstrates LaViRA's modularity and robustness, as the high-level reasoning and perceptual grounding components functioned effectively across different robot morphologies and control systems without modification, confirming the framework's real-world applicability. More details will be provided in the video.


## VI. CONCLUSION

We introduce LaViRA, a novel, hierarchical framework for zero-shot Vision-and-Language Navigation in continuous environments. By decomposing the navigation process into a coarse-to-fine hierarchy of actions, LaViRA eliminates the need for pre-trained waypoint predictors and fully leverages the multi-granularity reasoning of MLLMs. Our experiments show that pairing a powerful MLLM for high-level planning with an efficient one for perceptual grounding sets a new state-of-the-art on the VLN-CE benchmark. LaViRA significantly outperforms existing zero-shot methods, demonstrating the potential of structured, model-driven reasoning for complex embodied navigation.

Despite its performance, LaViRA's success is dependent on the underlying MLLMs, which can fail to interpret ambiguous instructions or ground descriptions of large areas. Future work will focus on enhancing robustness by fine-tuning the MLLMs on in-domain navigation data and improving the system's resilience to sim-to-real gaps, pushing LaViRA towards reliable real-world deployment. REFERENCES

[1] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. Van Den Hengel, "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 3674-3683.

[2] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, "Beyond the nav-graph: Vision and language navigation in continuous environments," in Proceedings of the European Conference on Computer Vision, 2020, pp. 104-120.

[3] Y. Qiao, W. Lyu, H. Wang, Z. Wang, Z. Li, Y. Zhang, M. Tan, and Q. Wu, "Open-nav: Exploring zero-shot vision-and-language navigation in continuous environment with open-source llms," arXiv preprint arXiv:2409.18794, 2024.

[4] X. Shi, Z. Li, W. Lyu, J. Xia, F. Dayoub, Y. Qiao, and Q. Wu, "Smart-way: Enhanced waypoint prediction and backtracking for zero-shot vision-and-language navigation," arXiv preprint arXiv:2503.10069, 2025.

[5] K. Chen, D. An, Y. Huang, R. Xu, Y. Su, Y. Ling, I. Reid, and L. Wang, "Constraint-aware zero-shot vision-language navigation in continuous environments," IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025.

[6] Y. Long, W. Cai, H. Wang, G. Zhan, and H. Dong, "Instructnav: Zero-shot system for generic instruction navigation in unexplored environment," arXiv preprint arXiv:2406.04882, 2024.

[7] J. Li, D. Li, S. Savarese, and S. Hoi, "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models," in International Conference on Machine Learning, 2023, pp. 19730-19742.

[8] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang, "Matterport3d: Learning from rgb-d data in indoor environments," arXiv preprint arXiv:1709.06158, 2017.

[9] Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Y. Wang, C. Shen, and A. van den Hengel, "Reverie: Remote embodied visual referring expression in real indoor environments," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 9979-9988.

[10] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge, "Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding," arXiv preprint arXiv:2010.07954, 2020.

[11] Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, and S. Gould, "Vln bert: A recurrent vision-and-language bert for navigation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 1643-1653.

[12] Y. Qi, Z. Pan, Y. Hong, M.-H. Yang, A. van den Hengel, and Q. Wu, "The road to know-where: An object-and-room informed sequential bert for indoor vision-language navigation," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 1635-1644.

[13] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang, W. Y. Wang, and L. Zhang, "Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 6629-6638.

[14] J. Li, H. Tan, and M. Bansal, "Envedit: Environment editing for vision-and-language navigation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 15386-15396.

[15] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, and T. Darrell, "Speaker-follower models for vision-and-language navigation," in Advances in Neural Information Processing Systems, 2018, pp. 3318-3329.

[16] Z. Wang, J. Li, Y. Hong, Y. Wang, Q. Wu, M. Bansal, S. Gould, H. Tan, and Y. Qiao, "Scaling data generation in vision-and-language navigation," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 12009-12020.

[17] S. Chen, P.-L. Guhur, M. Tapaswi, C. Schmid, and I. Laptev, "Think global, act local: Dual-scale graph transformer for vision-and-language navigation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16516-16526.

[18] D. An, H. Wang, W. Wang, Z. Wang, Y. Huang, K. He, and L. Wang, "Etpnav: Evolving topological planning for vision-language navigation in continuous environments," IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1-16, 2024.

[19] Z. Wang, X. Li, J. Yang, Y. Liu, and S. Jiang, "Gridmm: Grid memory map for vision-and-language navigation," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 15625-15636.

[20] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.

[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., "Llama 2: Open foundation and fine-tuned chat models," arXiv preprint arXiv:2307.09288, 2023.

[22] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., "Learning transferable visual models from natural language supervision," in International Conference on Machine Learning, 2021, pp. 8748-8763.

[23] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al., "Gemini: a family of highly capable multimodal models," arXiv preprint arXiv:2312.11805, 2023.

[24] Y. Hong, Z. Wang, Q. Wu, and S. Gould, "Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 15418-15428.

[25] C. Gao, X. Peng, M. Yan, H. Wang, L. Yang, H. Ren, H. Li, and S. Liu, "Adaptive zone-aware hierarchical planner for vision-language navigation," in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 14911-14920.

[26] S. Zhang, X. Song, X. Yu, and et al., "HOZ++: Versatile hierarchical object-to-zone graph for object navigation," IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025.

[27] M. Z. Irshad, C.-Y. Ma, and Z. Kira, "Hierarchical cross-modal agent for robotics vision-and-language navigation," in IEEE International Conference on Robotics and Automation (ICRA), 2021.

[28] F. Johnson, B. B. Cao, A. Ashok, and et al., "Feudal networks for visual navigation," arXiv preprint arXiv:2402.12498, 2024.

[29] L. Zhang,X. Hao,Y. Tang,and et al.,"NavA ${}^{3}$ : Understanding any instruction, navigating anywhere, finding anything," arXiv preprint arXiv:2508.04598, 2025.

[30] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, et al., "Habitat: A platform for embodied ai research," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 9338-9346.

[31] Y. Hong, Z. Wang, Q. Wu, and S. Gould, "Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 15439-15449.


[32] D. An, Y. Qi, Y. Li, Y. Huang, L. Wang, T. Tan, and J. Shao, "Bevbert: Multimodal map pre-training for language-guided navigation," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 2737-2748.

[33] Y. Long, X. Li, W. Cai, and H. Dong, "Discuss before moving: Visual language navigation via multi-expert discussions," in IEEE International Conference on Robotics and Automation, 2023, pp. 17380-17387.

[34] J. Chen, B. Lin, R. Xu, Z. Chai, X. Liang, and K.-Y. K. Wong, "Mapgpt: Map-guided prompting with adaptive path planning for vision-and-language navigation," arXiv preprint arXiv:2401.07314, 2024.

[35] H. Yin, H. Wei, X. Xu, W. Guo, J. Zhou, and J. Lu, "GC-VLN: Instruction as graph constraints for training-free vision-and-language navigation," arXiv preprint arXiv:2509.10454, 2025.