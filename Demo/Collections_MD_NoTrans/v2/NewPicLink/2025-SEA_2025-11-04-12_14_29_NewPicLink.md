# SEA: Semantic Map Prediction for Active Exploration of Uncertain Areas

Hongyu Ding ${}^{1, * }$ ,Xinyue Liang ${}^{1, * }$ ,Yudong Fang ${}^{2}$ ,You Wu ${}^{1}$ ,

Jieqi ${\mathrm{{Shi}}}^{2, \dagger  }$ ,Jing ${\mathrm{{Huo}}}^{1, \dagger  }$ ,Wenbin ${\mathrm{{Li}}}^{2}$ ,Jing ${\mathrm{{Wu}}}^{3}$ ,Yu-Kun ${\mathrm{{Lai}}}^{3}$ ,Yang ${\mathrm{{Gao}}}^{2}$

${Abstract}$ - In this paper,we propose SEA,a novel approach for active robot exploration through semantic map prediction and a reinforcement learning-based hierarchical exploration policy. Unlike existing learning-based methods that rely on one-step waypoint prediction, our approach enhances the agent's long-term environmental understanding to facilitate more efficient exploration. We propose an iterative prediction-exploration framework that explicitly predicts the missing areas of the map based on current observations. The difference between the actual accumulated map and the predicted global map is then used to guide exploration. Additionally, we design a novel reward mechanism that leverages reinforcement learning to update the long-term exploration strategies, enabling us to construct an accurate semantic map within limited steps. Experimental results demonstrate that our method significantly outperforms state-of-the-art exploration strategies, achieving superior coverage ares of the global map within the same time constraints. Project page: https://robo-lavira.github.io/sea-active-exp/

## I. INTRODUCTION

With the advancement of embodied agents, research on active exploration in unknown environments has grown rapidly. Exploration tasks in embodied intelligence differ in focus, including discovering and cataloging objects [1], [2], locating specific targets [3]-[5], maximizing explored area within limited time [6]-[8], and achieving accurate environment reconstruction.

Traditional exploration algorithms rely on hand-crafted optimization methods to maximize coverage. Graph-based methods [9] are precise but inflexible, often falling into local optima, while sampling-based methods [10]-[12] handle high-dimensional spaces but cannot guarantee optimality. These approaches generally face a trade-off between global awareness and exploration efficiency. To overcome this, reinforcement learning (RL) and deep learning (DL) have been introduced, leveraging neural networks' implicit long-term memory to infer global structure [13]. However, studies on spatial intelligence reveal that neural networks often fail to capture topology and lack robust 3D perception [14], limiting their reliability in complex scenes.

<!-- Media -->

<!-- figureText: How to establish a large Global Semantic Map Mapper \\\$’xploration Exploring Areas of Uncertainty semantic map in limited time? observation action Unknown Scene -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_43_24_a1a265.jpg"/>

Fig. 1: Constructing a global semantic map within limited time steps, with the semantic map guiding active exploration. In our framework, we encourage the agent to take action to explore the uncertain areas on the semantic map to enhance exploration efficiency.

<!-- Media -->

To address these limitations, we integrate the adaptive planning capabilities of RL with explicit spatial reasoning to achieve more efficient and flexible exploration. Inspired by vision-and-language navigation (VLN), where semantic maps improve memory [15], [16], we argue that learning-based exploration should return to its core goal: actively and comprehensively covering unknown environments while building complete and accurate maps for decision-making.

The processes of explicit semantic map construction and unexplored area exploration operate in an iterative manner. The semantic map serves both as a structured representation of the environment and a form of spatial memory. It enables the agent to distinguish between explored and unexplored regions while continuously refining its action strategies for maximum efficiency. Simultaneously, the exploration process actively refines the map, ultimately ensuring the most rapid and comprehensive coverage of the environment. This interplay fosters an exploration strategy that balances exploration, which encourages enlarging the map, and correction, which encourages adding observations to refine uncertain areas.

To achieve this, we train a prediction-based completion network that estimates complete maps from local observations, guided by prior knowledge of room layouts, object semantics, and shapes. A confidence-aware fusion module integrates observations into a global map, where discrepancies between predicted and accumulated maps highlight unexplored regions. By converting these unknown areas into a probabilistic map, we constrain the exploration policy to uncertain regions lacking prior knowledge, which are also areas we regard as more valuable and worth exploring. A tailored reward function further encourages accurate semantic coverage, improving waypoint selection and overall efficiency.

---

<!-- Footnote -->

${}^{ * }$ Equal Contribution, ${}^{ \dagger  }$ Corresponding Author

${}^{1}$ Hongyu Ding,Xinyue Liang,You Wu and Jing Huo are with the School of Computer Science, Nanjing University, China. Emails: \{hongyuding, MF21330051, you\}@smail.nju.edu.cn, huojing@nju.edu.cn

${}^{2}$ Yudong Fang,Jieqi Shi,Wenbin Li and Yang Gao are with the School of Intelligence Science and Technology, Nanjing University, China. Emails: 231880023@smail.nju.edu.cn, \{isjieqi, liwenbin, gaoy\}@nju.edu.cn

${}^{3}$ Jing Wu and Yu-Kun Lai are with Cardiff University,United Kingdom. Emails: \{WuJ11, LaiY4\}@cardiff.ac.uk

<!-- Footnote -->

---

<!-- Meanless: name Server Projects-->


In summary, we propose SEA, a semantic-guided exploration framework that focuses on learning-based exploration and presents four key contributions: 1)We propose a prediction-based semantic map completion method to identify unexplored areas and guide long-term exploration strategies for faster map coverage. 2)We design a reward function that optimizes exploration waypoint selection by encouraging agents to prioritize uncertain regions. 3) Our approach efficiently explores complex environments while simultaneously generating an accurate global semantic map, enabling better navigation in unknown areas. 4)The proposed method outperforms state-of-the-art DRL-based exploration approaches on the Habitat dataset, demonstrating superior exploration efficiency and semantic map accuracy.

## II. RELATED WORK

## A. Semantic Scene Completion

A concept closely related to our work is Semantic Scene Completion (SSC), which aims to infer the complete geometry and semantics of a scene from sparse inputs like depth maps or monocular RGB images. Early work primarily focused on object-level completion, reconstructing full geometry from limited 2D/3D observations [17], [18]. With the rise of deep learning, research has shifted toward completing full semantic scenes by jointly predicting semantics and geometry [19]-[21]. However, a key distinction from our approach is that most SSC methods reconstruct only the current visible scene in dense 3D space, without predicting unseen areas beyond the field of view.

Several studies combine completion with reconstruction and exploration [22]-[24]. For instance, PredRecon [22] applies object-level completion to guide UAV waypoint selection for reconstruction, yet it focuses on object geometry rather than scene-level exploration. The heuristic-based method in [24] identifies unknown frontiers and applies rule-based strategies to guide exploration. Although it integrates unknown-area prediction, it only distinguishes between explored and unexplored regions and lacks perceptual priors for active exploration.

## B. Occupancy Mapping and Semantic Mapping

The objective of mapping is to construct a structured representation of an environment. Depending on whether semantic information is considered, this can be categorized into occupancy and semantic mapping. In occupancy mapping, Chaplot et al. introduced an active navigation framework that projects egocentric views to generate obstacle maps [6]. Other works construct obstacle maps [2], [25]-[27] or build topological maps capturing spatial relations among rooms and objects [28]-[34].

While some works [35], [36] attempt to enhance environmental observations by optimizing camera rotation for next-best-view selection, they do not guide the agent toward active exploration for improved semantic mapping. Similarly, studies on optimizing exploration perspectives for point cloud reconstruction [37]-[40] adopt a next-best-view approach but are constrained by the high storage cost and computational resources required for $3\mathrm{D}$ point clouds. As a result, many semantic mapping approaches struggle to integrate with autonomous exploration tasks.

## C. Active Exploration

Active exploration aims to enlarge the explored area or discover more objects in an environment using limited steps. Our proposed task falls into this category but places a distinct emphasis on combining exploration with semantic map construction, requiring more explicit environmental modeling.

Most existing exploration approaches [1], [2], [6]-[8], [26], [41] focus on maximizing coverage or object diversity but lack explicit semantic reasoning. The works most related to ours are SSCNav [42] and L2M [43], which leverage predicted semantic local maps and uncertainty estimation. However, they are limited to object-goal navigation. SSCNav discards local maps at each step, preventing global map accumulation, and its confidence maps are used to correct errors rather than guide exploration in uncertain regions. L2M also exploits uncertainty, but only for specific object categories relevant to its navigation task, without extending to global semantic completion.

The recent work GLEAM [44] focuses on large-scale training and cross-dataset policy generalization. Its focus on scalability is complementary to ours; while GLEAM advances generalizable exploration, our method addresses the critical need for semantic uncertainty and completion to construct high-fidelity maps under a limited budget. To our knowledge, no prior work simultaneously explores unseen indoor environments and constructs predictive semantic maps to guide the process. We argue that semantic reasoning, fundamental to how humans explore, is a highly promising direction for enhancing both efficiency and scene understanding in embodied AI.

## III. APPROACH

Our goal is to actively explore the environment while constructing an accurate semantic map within a limited number of steps. To achieve this, we propose a framework consisting of three key modules that facilitate both exploration and mapping. The overall approach is illustrated in Figure 2, Specifically, we introduce three major components: (i) ASC-based Local Mapper that performs local map prediction and confidence estimation. (ii) Two-stage Navigator that decouples the exploration task into long-term and short-term policies, allowing for more efficient and structured exploration. (iii) Confidence-aware Full Mapper that accumulates local maps while incorporating confidence-based adjustments.


<!-- Media -->

<!-- figureText: Forward / Turn _ow-Level Waypoint Reward Function Map Coverage Full Projected Full Completed Semantic Confidence Map ${M}_{proj}$ Map ${M}_{cmplt}$ Completion Network Confidence Local Completed Local Confidence Map ${m}_{cmplt}$ Map ${m}_{conf}$ action interaction ${obs}$ RGB Semantic Observation Segmentation proj. Local Projected Map ${m}_{proj}$ Depth image Robot Pose -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_43_24_fbdee1.jpg"/>

Fig. 2: Overview of SEA. After obtaining the egocentric observations of RGB-D image and the ground truth position from Habitat simulator as the input, we employ panoramic semantic segmentation and project the egocentric observations onto the 2D plane to build a local map. The projected local map is then processed by our Local Mapper and Full Mapper modules to generate local completed and confidence maps, which are subsequently accumulated into full projected and full completed maps. Besides, RL Navigator takes the local maps to select the long-term goal to explore, which is then passed to the short-term policy module to make the decision of the next action.

<!-- Media -->

## A. Task Definition

In an unknown environment, the agent begins at a random position, actively exploring the scene while building a semantic map. At each timestep $t$ ,the agent acquires RGB-D observation ${o}_{t}$ and an accurate pose ${p}_{t}$ from the environment. The agent is supposed to learn an exploration policy $\pi \left( {{a}_{t} \mid  {o}_{t},{p}_{t}}\right)$ which aims to explore as rapid as possible while constructing an accurate semantic map by executing action ${a}_{t} \in  \{$ MoveForward,RotateRight,Right,RotateLeft $\}$ within limited $k$ timesteps.

## B. ASC-based Local Mapper

Building a complete map to represent the global environment requires fusing information from egocentric views. Existing DRL-based approaches [45] directly accumulate maps by projecting only the areas visible to the agent, leading to inefficient mapping. We highlight that, with prior knowledge of indoor scenes, unseen areas can be predicted and completed, enhancing the semantic map with broader and more accurate coverage.

To address this, we propose the ASC-based Local Mapper, which projects local maps from egocentric observations and fills unseen areas with predicted semantics. To improve exploration efficiency, the agent is encouraged to explore regions with relatively inaccurate semantics. Additionally, the Local Mapper estimates semantic confidence and generates a confidence map to guide future exploration.

1) Panoramic Segmented Local Map: In this module, we first perform panoramic segmentation of RGB observation according to the 40 semantic categories of the MP3D dataset. Our panoramic semantic segmentation employs off-the-shelf ACNet [46] and leverages the pretrained panoramic segmentation model from SSCNav [42] which is trained with 209,200 RGB-D images of 40 object categories from MP3D training houses. As a comparison, SemExp [4] utilizes only 16 semantic categories relevant to the Habitat Object Goal Navigation task. In contrast, our local maps include all 40 semantic categories plus an additional void category. Moreover, our method provides a panoramic semantic view, excluding unseen areas such as floors and walls.

We combine depth observation and segmented RGB observation to compute each point in the egocentric $3\mathrm{D}$ point cloud. To project the 3D map into 2D space, we accumulate the semantic point cloud based on height and obtain the 2D semantic local projected map ${m}_{\text{proj }}$ of size $M \times  M$ . In addition,we determine the world coordinates of this $2\mathrm{D}$ local map using the ground-truth rotation and translation of the agents.

We emphasize that, although we do not explore real-world data expansion in this paper, we have aimed to replicate real-world noise conditions in this section to facilitate future research. To align with the semantic noise encountered in real-world applications, we employ a segmentation model for semantic segmentation instead of using the ground-truth annotations. Regarding pose estimation, while we obtain ground-truth poses directly from the simulator, this process can be replicated in the real world using a SLAM front end. Considering that common front-end methods like VINS can achieve an accuracy of $7\mathrm{\;{cm}}$ ATE in a ${147}\mathrm{\;m}$ trajectory [47], we believe this assumption is reasonable.


<!-- Media -->

<!-- figureText: ${m}_{proj}$ ${m}_{pred}$ ${\text{mask}}_{us} \cdot  {m}_{\text{pred}}$ ${m}_{cmplt}$ ${m}_{gt}$ ${\mathcal{L}}_{CE}$ ${\text{mask}}_{us}$ -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_43_24_55097a.jpg"/>

Fig. 3: The approach for building a local semantic completed map. We pass the local projected map ${m}_{proj}$ through the semantic completed network to get the semantic prediction map ${m}_{\text{pred }}$ ,and extract the unseen mask ${\operatorname{mask}}_{us}$ from the unseen part. As shown in Equation 1,we fill the unseen part of ${m}_{proj}$ with that in ${m}_{pred}$ to form ${m}_{cmplt}$ .

<!-- Media -->

2) Local Map Completion: By leveraging prior knowledge of indoor scenes and objects, unseen areas can be predicted and completed based on the projected map, expanding accurate semantic coverage within a limited number of steps and improving exploration efficiency. In our framework, we generate a completed semantic local map, covering the entire local map and filling unknown regions with predicted semantics. Unlike SSCNav's completion method, our approach maintains consistency between observed and predicted areas during training.

As illustrated in Figure 3, we calculate the unseen area in ${m}_{\text{proj }}$ to obtain a mask ${\text{ mask }}_{us} \in  \{ 0,1{\} }^{M \times  M}$ . We exploit ResNet with a fully convolutional neural network with 1 maxpooling layer, 4 down-sample residual blocks, and 5 up-sample residual blocks, following the same architecture in SSCNav [48]. Differently, we use only one observed projected map ${m}_{proj}$ to get the predicted map ${m}_{pred}$ of the same size,and fill the unseen part of ${m}_{proj}$ with that in ${m}_{\text{pred }}$ to form ${m}_{\text{cmplt }}$ for consistency. The loss is calculated as the Cross Entropy Loss between ${m}_{\text{cmplt }}$ and ${m}_{gt}$ .

$$
{m}_{\text{cmplt }} = \left( {1 - {\text{ mask }}_{us}}\right)  \odot  {m}_{\text{proj }} + {\text{ mask }}_{us} \odot  {m}_{\text{pred }} \tag{1}
$$

3) Local Map Confidence Estimation: If the predicted area of ${m}_{\text{cmplt }}$ lacks accuracy,semantic noise may introduce errors into the projected map, negatively affecting subsequent exploration. Since our goal is to maximize Accurate Semantic Coverage (ASC) within a limited number of steps, repeatedly exploring areas where the completed semantic map is already accurate or revisiting previously explored locations would be inefficient, limiting the expansion of full scene coverage.

To address this, we introduce a novel method that prioritizes areas where the completed semantic map is less accurate. We select such maps as high-potential regions for the next long-term exploration target. The agent is then guided by this target to explore these areas efficiently. In summary, since inaccurate regions require further exploration, we predict a semantic confidence map ${m}_{\text{conf }} \in  {\left\lbrack  0,1\right\rbrack  }^{M \times  M}$ for ${m}_{\text{cmplt }}$ to assess confidence levels and guide exploration.

We compare ${m}_{\text{cmplt }}$ and the ground truth local map ${m}_{\text{cmplt }}$ to generate a semantic accuracy map ${m}_{\text{acc }} \in$ $\{ 0,1{\} }^{M \times  M}$ ,which serves as supervision for the predicted confidence map. Using the same ResNet-based architecture as the completion network,we take ${m}_{\text{pred }}$ as the input during training and output ${m}_{\text{conf }}$ . We only apply the Cross Entropy Loss of the unseen part between prediction map for optimization. During navigation, repeated exploration can be avoided by using ${m}_{\text{conf }}$ as a guidance. With this approach, the RL Navigator based on ${m}_{\text{conf }}$ can efficiently construct a more accurate semantic map within a limited number of steps.

## C. Two-Stage Navigator

We divide the exploration task into two distinct stages. First, we select a long-term goal to guide the agent's navigation over the next $\mu$ steps. Then,we apply short-term policies for path planning and action selection, ensuring efficient movement toward the goal.

1) Confidence-aware Long-term Policy: Existing approaches [1], [6] select long-term goals across the entire map. However, our experiments show that these methods often select goals near map corners, limiting their ability to utilize environmental information effectively. This inefficiency negatively impacts the overall exploration process.

To address these limitations, we propose leveraging the Soft Actor-Critic (SAC) algorithm [49], an off-policy, model-free reinforcement learning method that enables more effective long-term goal selection. Within a single long-term goal period, the agent's reachable distance remains limited to the $M \times  M$ local map. This allows us to select a new long-term goal based on the current ${m}_{\text{proj }},{m}_{\text{cmplt }}$ and ${m}_{\text{conf }}$ . The interval for long-term goal planning is set to $\mu  = {25}$ . The outputs of the Local Mapper,including ${m}_{\text{proj }},{m}_{\text{cmplt }}$ and ${m}_{\text{conf }}$ ,are fed into the SAC module for exploration. Additionally, the embeddings of the agent's current position and orientation are included as inputs.


Meanwhile, the Confidence-Aware Full Mapper (Subsection III.D) updates the full projected map. The differences between the coverage ${\Delta }_{\text{cover }}$ of the full projected maps is computed to determine the coverage reward ${r}_{\text{cover }}$ . Since the predicted map area can theoretically expand indefinitely, maximizing its coverage alone is not meaningful. Therefore, we use the variance in Accurate Semantic Coverage (ASC) between consecutive steps of the completed map to compute the completion reward ${r}_{asc}$ . Besides,we calculate the confidence of the long-term goal ${r}_{\text{conf }}$ on ${m}_{\text{conf }}$ .

At time $t$ ,we define the following reward ${r}_{t}$ in reinforcement learning training,where ${\eta }_{c},{\eta }_{asc}$ and ${\eta }_{\text{conf }}$ are hyper parameters that balance different loss terms,

$$
{r}_{t} = {r}_{\text{cover }} + {r}_{\text{asc }} + {r}_{\text{conf }} \tag{2}
$$

$$
 = {\eta }_{c}{\Delta }_{c} + {\eta }_{asc}{\Delta }_{asc} + {\eta }_{conf}\operatorname{con}{f}_{t}
$$

where ${r}_{\text{cover }}$ is the change in projected map coverage $\left( {\Delta }_{c}\right)$ , ${r}_{asc}$ is the change in Accurate Semantic Coverage of the completed map $\left( {\Delta }_{asc}\right)$ ,and ${r}_{conf}$ is the confidence value of the chosen goal. The hyperparameters ${\eta }_{c},{\eta }_{asc}$ ,and ${\eta }_{\text{conf }}$ balance these terms.

2) Short-term Policy: For the short-term policy, a deterministic path planning method, Fast Marching Method (FMM) [50],is used. Based on ${m}_{proj}$ ,FMM plans a path toward the selected long-term goal and determines the agent's next action. It maintains an obstacle local map, allowing the agent to navigate efficiently while avoiding obstacles during exploration.

## D. Confidence-aware Full Mapper

Based on the output of the Local Mapper(Section III.B), we construct the full projected map ${M}_{proj}$ ,full completed map ${M}_{\text{cmplt }}$ and full confidence map ${M}_{\text{conf }}$ to obtain the semantic map of an indoor scene. Firstly,we establish ${M}_{\text{conf }}$ by accumulating local confidence map ${m}_{\text{conf }}$ according to the position. If a region overlaps with a previously explored area, we retain the higher confidence value. We define this process as confidence-aware accumulation. Simultaneously, we generate a mask for the retained parts of the current ${m}_{conf}$ ,denoted as ${\operatorname{mask}}_{conf}$ .

We then apply ${\operatorname{mask}}_{\text{conf }}$ to ${m}_{\text{proj }}$ and ${m}_{\text{cmplt }}$ before stitching them into ${M}_{\text{proj }}$ and ${M}_{\text{cmplt }}$ according to the world coordinates of the local maps. Finally, we evaluate the metrics using ${M}_{\text{proj }}$ and ${M}_{\text{cmplt }}$ .

<!-- Media -->

TABLE I: Results after 500 steps on MP3D datasets.

<table><tr><td>$\mathbf{{Method}}$</td><td>CovP $\left( {m}^{2}\right)  \uparrow$</td><td>ASCP $\left( {m}^{2}\right)  \uparrow$</td><td>$\mathbf{{CovC}}$ $\left( {m}^{2}\right)  \uparrow$</td><td>ASCC $\left( {m}^{2}\right)  \uparrow$</td></tr><tr><td>SemExp*</td><td>96.61</td><td>36.54</td><td>101.64</td><td>40.92</td></tr><tr><td>Impact</td><td>104.53</td><td>42.19</td><td>111.53</td><td>45.14</td></tr><tr><td>EE</td><td>102.79</td><td>40.95</td><td>109.47</td><td>46.48</td></tr><tr><td>SSCNav</td><td>14.61</td><td>-</td><td>14.61</td><td>-</td></tr><tr><td>ARiADNE*</td><td>104.25</td><td>-</td><td>104.25</td><td>-</td></tr><tr><td>SEA (Ours)</td><td>111.74</td><td>49.53</td><td>117.14</td><td>53.99</td></tr></table>

TABLE II: Ablation results on MP3D Val datasets.

<table><tr><td>$\mathbf{{Method}}$</td><td>CovP $\left( {m}^{2}\right)  \uparrow$</td><td>ASCP $\left( {m}^{2}\right)  \uparrow$</td><td>$\mathbf{{CovC}}$ $\left( {m}^{2}\right)  \uparrow$</td><td>ASCC $\left( {m}^{2}\right)  \uparrow$</td></tr><tr><td>w/o ${r}_{\text{cover }}$</td><td>104.20</td><td>43.10</td><td>106.36</td><td>44.15</td></tr><tr><td>w/o ${r}_{asc}$</td><td>111.03</td><td>48.76</td><td>115.08</td><td>51.66</td></tr><tr><td>w/o ${r}_{conf}$</td><td>110.59</td><td>48.07</td><td>114.74</td><td>50.49</td></tr><tr><td>w/o ${m}_{cmplt}$</td><td>109.11</td><td>47.87</td><td>112.29</td><td>49.10</td></tr><tr><td>w/o ${m}_{conf}$</td><td>106.63</td><td>45.80</td><td>108.18</td><td>46.88</td></tr><tr><td>SEA</td><td>111.74</td><td>49.53</td><td>117.14</td><td>53.99</td></tr></table>

<!-- Media -->

## IV. EXPERIMENT RESULTS

## A. Experiment Setup

Dataset and Simulator. We use Habitat [48] as the simulation platform and MP3D [51] as the dataset. MP3D is an RGB-D indoor dataset with 90 large-scale scenes: 61 for training, 11 for validation, and 18 for testing. Our method is trained with $1\mathrm{M}$ observation frames from the training split and evaluated with ${50}\mathrm{k}$ frames from the validation split. For semantic completion and confidence modeling, we adopt the local map completion dataset from SSCNav, which contains over ${52.3}\mathrm{k}$ pairs of ground-truth and projected maps. Ground-truth full maps ${M}_{gt}$ for evaluation are generated following SMNet [52] using MP3D semantics.

Metrics. To assess the effectiveness of semantic mapping, we define the following metrics. CovP is the area covered by ${M}_{\text{proj }}$ in ${m}^{2}$ and $\mathbf{{Cov}}\mathbf{C}$ is the coverage of ${M}_{\text{cmplt }}$ in ${m}^{2}$ . ASCP is accurate semantic coverage which is calculated as the interaction in ${m}^{2}$ between ${M}_{\text{proj }}$ and ${M}_{gt}$ while ASCC uses ${M}_{\text{cmplt }}$ .

Following [7], the maximum exploration step is set to 500. The local map resolution is ${128} \times  {128}$ ,and the full map is ${1280} \times  {1280}$ (corresponding to ${48} \times  {48},{m}^{2}$ in the physical world). Although insufficient for full exploration, this effectively highlights differences across strategies for semantic map construction. Long-term goals are updated every 10 steps. Empirically, ASC is about half of total coverage. We estimate ${\Delta }_{asc}$ by doubling it and compute ${\Delta }_{\text{conf }}$ by scaling ${\Delta }_{\text{asc }}$ by 1000. The final reward (Eq. 2) is calculated with hyperparameters ${\eta }_{c} = {0.6},{\eta }_{asc} = {0.4}$ ,and ${\eta }_{\text{conf }} = {0.4}$ .

## B. Baselines

In our simulation experiments, the agent's pose is obtained directly from Habitat's ground-truth position sensor, avoiding pose estimation and simplifying mapping. This allows us to focus on evaluating exploration speed, coverage integrity, and semantic certainty. Accordingly, we select baselines that emphasize map coverage.


<!-- Media -->

<!-- figureText: SemExp* EE Impact SEA (Ours) ARiADNE RGB Projected Map local view waypoints Ground Truth Semantic Map Full Projected Map Full Completed Map Trajectory -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_43_24_8d2d18.jpg"/>

Fig. 4: The qualitative evaluation of different methods during evaluation, including the full projected map and the full completed map. To provide a clearer view,we enlarge ${m}_{proj}$ and ${m}_{cmplt}$ built at a specific time point in the bottom left corner. Since ARiADNE has no semantic maps, we use different views according to its original paper for illustration.

<!-- Media -->

leftmargin $= {.5}\mathrm{\;{cm}}$

- SemExp* [4]: Originally designed for object-goal navigation, SemExp shares its architecture with ANS [6], which targets occupied coverage. We modify its reward to prioritize coverage and retrain it on the same dataset, effectively converting it into an exploration method.

- Impact [7]: Trained with purely intrinsic rewards, Impact explores to maximize occupied coverage. We evaluate it using the provided trained model under the same setting reported in the original paper.

- EE [2]: This work analyzes how reward design affects coverage and object detection. We adopt the released model with the best coverage performance for evaluation.

- ARiADNE [53] is a state-of-the-art RL-based exploration method that outperforms classical approaches like TARE [12] on grid maps. For a fair comparison, we adapted it to match our method's mapping scope and integrated our short-term policy, FMM, as its fine-grained exploration strategy in the continuous Habitat environment.

## C. Results on MP3D Datasets

As shown in Table 1, our method outperforms existing approaches in both global map coverage and semantic accuracy, for both projected and completed maps. After 500 steps, it achieves ${111.74}{\mathrm{\;m}}^{2}$ of occupied coverage and ${49.53}{\mathrm{\;m}}^{2}$ of accurate semantic coverage on the projected map.

ARiADNE, originally trained in simple, flat environments with regular layouts, performs poorly on MP3D, which contains diverse and irregular structures, often with multiple floors. For fairness, we report ARiADNE* on single-level scenes, where its performance is comparable to other baselines. These results highlight the advantage of our method in the selection of routes and the prediction of long-term semantics. By avoiding redundant exploration in already confident areas, it reallocates effort to uncertain regions, leading to more efficient exploration.

Notably, the best coverage reported in Impact [7] when evaluated on the MP3D Val split is ${144.64}{\mathrm{\;m}}^{2}$ . However,this difference arises because our experimental setup limits the depth range of map projection to $5\mathrm{\;m}$ ,whereas the mapper in Impact’s code uses ${10}\mathrm{\;m}$ . Consequently,the coverage of our semantic map mapper appears lower than in Impact's original paper. Nevertheless, since all experiments are conducted under a consistent setting in our study, the results remain valid and reliable.

Figure 4 further illustrates agent trajectories in an unseen MP3D scene. Our method yields the widest coverage and highest semantic accuracy. The close-up around the dining table shows how multi-view observations produce a more precise semantic map, surpassing SemExp*, EE, and Impact in detail and correctness.

Furthermore, to demonstrate the effectiveness of each module in our proposed method, we conducted a comprehensive ablation study. Specifically, we systematically removed each component from the global input of the RL navigator as well as individual reward mechanisms. As quantitatively presented in Table II, our experimental results demonstrate that each module contributes significantly to the semantic map construction process. The most substantial performance degradation was observed when ${m}_{\text{cmplt }}$ and ${m}_{\text{conf }}$ were excluded from the global input, indicating that the superior performance primarily stems from the map-based inputs to the reinforcement learning model for long-term goal selection, rather than relying solely on feedback rewards. Moreover,our analysis reveals that ${m}_{\text{conf }}$ and ${r}_{\text{conf }}$ ,which specifically identify and quantify the semantically uncertain regions in the currently explored map, thereby enabling more effective optimization of long-term goal selection. The differential impact between these components suggests that the explicit representation of uncertainty plays a crucial role in guiding the exploration process.


<!-- Media -->

<!-- figureText: t=64 t=88 t=112 t=144 t=168 t=192 t=240 t=272 cushior sink floor cabinet window misc 3rd-person Semantic door wall table chair counter sofa -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_43_24_d82be9.jpg"/>

Fig. 5: The qualitative evaluation of SEA in a real-world setting. The top row shows third-person views of the robot's navigation at different timesteps. The bottom row displays the corresponding semantic mapping results. In the semantic maps, the red circle indicates the robot's pose and orientation, the green cross marks the next waypoint predicted by our model, and the blue line represents the planned path to that waypoint. The legend at the bottom explains the color-coding for different object categories in the semantic map.

<!-- Media -->

## D. Real-world Deployment

To further validate the practicality of our approach, we conducted a real-world experiment in an indoor office environment using the Agilex Cobot Magic mobile platform. The robot was equipped with an Intel RealSense D435i camera at the front, and we utilized the robot's native wheel odometry for localization. The system demonstrated high efficiency,with an average inference time of ${0.08}\mathrm{\;s}$ for waypoint prediction and an average time of ${0.75}\mathrm{\;s}$ per frame for semantic map construction. Within a fixed number of steps, our method enabled the robot to actively explore the environment, producing a semantic map that captured both the structural layout and object categories with high fidelity. This demonstrates its robustness and effectiveness beyond simulation. Representative results of the constructed semantic map in the office environment are shown in Figure 5

## V. CONCLUSION

In this work, we introduced SEA, a visual exploration framework that prioritizes low-confidence regions on the semantic map to achieve rapid semantic coverage. By leveraging reinforcement learning for long-term goal selection, SEA guides the agent toward uncertain areas, enabling efficient observation of semantic objects, refinement of the map, and effective exploration of unseen environments within limited steps. Experimental results show that SEA consistently outperforms existing DL- and DRL-based exploration methods in both semantic map construction and indoor exploration.

Limitations and Future Work. Our study focused on predicting completed maps from local projections within a ${4.8} \times  {4.8}{m}^{2}$ region around the agent. Extending completion and confidence estimation to the global map remains challenging due to model size, runtime constraints, and the mismatch between long-term goal selection and feasible movement ranges. Additionally, noise artifacts caused by distant objects beyond the simulator's depth range highlight the need for more effective denoising methods that preserve semantic accuracy. We believe addressing these challenges will further advance semantic exploration and support broader applications of embodied AI in real-world environments. REFERENCES

[1] D. S. Chaplot, H. Jiang, S. Gupta, and A. Gupta, "Semantic curiosity for active visual learning," in Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VI 16. Springer, 2020, pp. 309-326.

[2] S. K. Ramakrishnan, D. Jayaraman, and K. Grauman, "An exploration of embodied visual exploration," International Journal of Computer Vision, vol. 129, pp. 1616-1649, 2021.

[3] J. Ye, D. Batra, A. Das, and E. Wijmans, "Auxiliary tasks and exploration enable objectgoal navigation," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 16117- 16126.

[4] D. S. Chaplot, D. P. Gandhi, A. Gupta, and R. R. Salakhutdinov, "Object goal navigation using goal-oriented semantic exploration," Advances in Neural Information Processing Systems, vol. 33, pp. 4247-4258, 2020.

[5] M. Zhu, B. Zhao, and T. Kong, "Navigating to objects in unseen environments by distance prediction," in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2022, pp. 10571-10578.

[6] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdi-nov, "Learning to explore using active neural slam," arXiv preprint arXiv:2004.05155, 2020.

[7] R. Bigazzi, F. Landi, S. Cascianelli, L. Baraldi, M. Cornia, and R. Cucchiara, "Focus on impact: indoor exploration with intrinsic motivation," IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 2985-2992, 2022.

[8] L. Juncheng, M. Brendan, and M. Steven, "Learning to explore by reinforcement over high-level options," arXiv preprint arXiv:2111.01364, 2021.

[9] D. Dolgov, S. Thrun, M. Montemerlo, and J. Diebel, "Practical search techniques in path planning for autonomous driving," 2008.

[10] B. Zhou, F. Gao, L. Wang, C. Liu, and S. Shen, "Robust and efficient quadrotor trajectory generation for fast autonomous flight," IEEE Robotics and Automation Letters, vol. 4, no. 4, pp. 3529-3536, 2019.

[11] B. Zhou, Y. Zhang, X. Chen, and S. Shen, "Fuel: Fast uav exploration using incremental frontier structure and hierarchical planning," IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 779-786, 2021.

[12] C. Cao, H. Zhu, H. Choset, and J. Zhang, "Tare: A hierarchical framework for efficiently exploring complex $3\mathrm{\;d}$ environments," Robotics: Science and Systems XVII, 2021. [Online]. Available: https://api.semanticscholar.org/CorpusID:235399646


[13] Y. Cao, J. Lew, J. Liang, J. Cheng, and G. Sartoretti, "Dare: Diffusion policy for autonomous robot exploration," arXiv preprint arXiv:2410.16687, 2024.

[14] J. Yang, S. Yang, A. W. Gupta, R. Han, L. Fei-Fei, and S. Xie, "Thinking in space: How multimodal large language models see, remember, and recall spaces," arXiv preprint arXiv:2412.14171, 2024.

[15] Z. Wang, X. Li, J. Yang, Y. Liu, and S. Jiang, "Gridmm: Grid memory map for vision-and-language navigation," 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 15579- 15590, 2023. [Online]. Available: https://api.semanticscholar.org/ CorpusID:260125382

[16] S. Chen, P.-L. Guhur, M. Tapaswi, C. Schmid, and I. Laptev, "Think global, act local: Dual-scale graph transformer for vision-and-language navigation," 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16516-16526, 2022. [Online]. Available: https://api.semanticscholar.org/CorpusID:247084359

[17] W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert, "Pcn: Point completion network," 2018 International Conference on 3D Vision (3DV), pp. 728-737, 2018. [Online]. Available: https: //api.semanticscholar.org/CorpusID:51908879

[18] L. P. Tchapmi, V. Kosaraju, H. Rezatofighi, I. D. Reid, and S. Savarese, "Topnet: Structural point cloud decoder," 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 383-392, 2019. [Online]. Available: https://api.semanticscholar.org/CorpusID: 198184513

[19] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. A. Funkhouser, "Semantic scene completion from a single depth image," 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 190-198, 2016. [Online]. Available: https://api.semanticscholar.org/CorpusID:20416090

[20] Y. Li, Z. Yu, C. B. Choy, C. Xiao, J. M. Álvarez, S. Fidler, C. Feng, and A. Anandkumar, "Voxformer: Sparse voxel transformer for camera-based 3d semantic scene completion," 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9087-9098, 2023. [Online]. Available: https://api.semanticscholar.org/CorpusID: 257102923

[21] H. Jiang, T. Cheng, N. Gao, H. Zhang, T. Lin, W. Liu, and X. Wang, "Symphonize 3d semantic scene completion with contextual instance queries," CVPR, 2024.

[22] C. Feng, H. Li, F. Gao, B. Zhou, and S. Shen, "Predrecon: A prediction-boosted planning framework for fast and high-quality autonomous aerial reconstruction," in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 1207- 1213.

[23] C. Gao, G. Yang, X. Chen, and B. M. Chen, "Active view planner for infrastructure 3d reconstruction," 2024 IEEE 18th International Conference on Control & Automation (ICCA), pp. 400-405, 2024. [Online]. Available: https://api.semanticscholar.org/CorpusID: 271462597

[24] Z. Xu, C. Suzuki, X. Zhan, and K. Shimada, "Heuristic-based incremental probabilistic roadmap for efficient uav exploration in dynamic environments," 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 11832-11838, 2023. [Online]. Available: https://api.semanticscholar.org/CorpusID:262043787

[25] T. Chen, S. Gupta, and A. Gupta, "Learning exploration policies for navigation," arXiv preprint arXiv:1903.01959, 2019.

[26] S. K. Ramakrishnan, Z. Al-Halah, and K. Grauman, "Occupancy anticipation for efficient exploration and navigation," in Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V 16. Springer, 2020, pp. 400-418.

[27] V. D. Sharma, J. Chen, A. Shrivastava, and P. Tokekar, "Occupancy map prediction for improved indoor robot navigation," arXiv preprint arXiv:2203.04177, 2022.

[28] S. D. Morad, S. Liwicki, R. Kortvelesy, R. Mecca, and A. Prorok, "Graph convolutional memory using topological priors," arXiv preprint arXiv:2106.14117, 2021.

[29] F. Wang, C. Zhang, F. Tang, H. Jiang, Y. Wu, and Y. Liu, "Lightweight object-level topological semantic mapping and long-term global localization based on graph matching," arXiv preprint arXiv:2201.05977, 2022.

[30] Y. Li, Y. Ma, X. Huo, and X. Wu, "Remote object navigation for service robots using hierarchical knowledge graph in human-centered environments," Intelligent Service Robotics, vol. 15, no. 4, pp. 459- 473, 2022.

[31] R. Dang, Z. Shi, L. Wang, Z. He, C. Liu, and Q. Chen, "Unbiased directed object attention graph for object navigation," in Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 3617-3627.

[32] D. S. Chaplot, R. Salakhutdinov, A. Gupta, and S. Gupta, "Neural topological slam for visual navigation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 12875-12884.

[33] Z. Ravichandran, L. Peng, N. Hughes, J. D. Griffith, and L. Carlone, "Hierarchical representations and explicit memory: Learning effective navigation policies on $3\mathrm{\;d}$ scene graphs using graph neural networks," in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 9272-9279.

[34] T. Zheng, Z. Duan, J. Wang, G. Lu, S. Li, and Z. Yu, "Research on distance transform and neural network lidar information sampling classification-based semantic segmentation of $2\mathrm{\;d}$ indoor room maps," Sensors, vol. 21, no. 4, p. 1365, 2021.

[35] D. Jayaraman and K. Grauman, "Look-ahead before you leap: end-to-end active recognition by forecasting the effect of motion," in Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14. Springer, 2016, pp. 489-505.

[36] ——, “Learning to look around: Intelligently exploring unseen environments for unknown tasks," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 1238-1247.

[37] A. Guédon, T. Monnier, P. Monasse, and V. Lepetit, "Macarons: Mapping and coverage anticipation with rgb online self-supervision," arXiv preprint arXiv:2303.03315, 2023.

[38] T. Cieslewski, E. Kaufmann, and D. Scaramuzza, "Rapid exploration with multi-rotors: A frontier selection method for high speed flight," in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017, pp. 2135-2142.

[39] J. I. Vasquez-Gomez, L. E. Sucar, R. Murrieta-Cid, and E. Lopez-Damian, "Volumetric next-best-view planning for 3d object reconstruction with positioning error," International Journal of Advanced Robotic Systems, vol. 11, no. 10, p. 159, 2014.

[40] J. I. Vasquez-Gomez, L. E. Sucar, and R. Murrieta-Cid, "View/state planning for three-dimensional object reconstruction under uncertainty," Autonomous Robots, vol. 41, pp. 89-109, 2017.

[41] S. Liu and T. Okatani, "Symmetry-aware neural architecture for embodied visual navigation," arXiv preprint arXiv:2112.09515, 2021.

[42] Y. Liang, B. Chen, and S. Song, "Sscnav: Confidence-aware semantic scene completion for visual semantic navigation," in 2021 IEEE international conference on robotics and automation (ICRA). IEEE, 2021, pp. 13 194-13 200.

[43] G. Georgakis, B. Bucher, K. Schmeckpeper, S. Singh, and K. Dani-ilidis, "Learning to map for active semantic goal navigation," arXiv preprint arXiv:2106.15648, 2021.

[44] X. Chen, T. Wang, Q. Li, T. Huang, J. Pang, and T. Xue, "Gleam: Learning generalizable exploration policy for active mapping in complex 3d indoor scenes," 2025.

[45] Z. Li, J. Xin, and N. Li, "Autonomous exploration and mapping for mobile robots via cumulative curriculum reinforcement learning," in 2023 IEEE/RSJ Ineternational Conference on Intelligent Robots and Systems (IROS). IEEE, 2023, pp. 7495-7500.

[46] X. Hu, K. Yang, L. Fei, and K. Wang, "Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation," in 2019 IEEE International Conference on Image Processing (ICIP). IEEE, 2019, pp. 1440-1444.

[47] C. Campos, R. Elvira, J. J. G. Rodr'iguez, J. M. M. Montiel, and J. D. Tardós, "Orb-slam3: An accurate open-source library for visual, visual-inertial, and multimap slam," IEEE Transactions on Robotics, vol. 37, pp. 1874-1890, 2020. [Online]. Available: https://api.semanticscholar.org/CorpusID:220713377

[48] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik et al., "Habitat: A platform for embodied ai research," in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 9339-9347.

[49] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor," in International conference on machine learning. PMLR, 2018, pp. 1861-1870.

[50] J. A. Sethian, "A fast marching level set method for monotonically advancing fronts." Proceedings of the National Academy of Sciences, vol. 93, no. 4, pp. 1591-1595, 1996.


[51] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang, "Matterport3d: Learning from rgb-d data in indoor environments," arXiv preprint arXiv:1709.06158, 2017.

[52] V. Cartillier, Z. Ren, N. Jain, S. Lee, I. Essa, and D. Batra, "Semantic mapnet: Building allocentric semantic maps and representations from egocentric views," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 2, 2021, pp. 964-972.

[53] Y. Cao, T. Hou, Y. Wang, X. Yi, and G. Sartoretti, "Ariadne: A reinforcement learning approach using attention-based deep networks for exploration," 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 10 219-10 225, 2023. [Online]. Available: https://api.semanticscholar.org/CorpusID:256358779