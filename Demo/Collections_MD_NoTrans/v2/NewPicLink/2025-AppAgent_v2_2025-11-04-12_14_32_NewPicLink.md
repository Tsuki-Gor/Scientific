<!-- Meanless: arXiv:2408.11824v4 [cs.HC] 17 Sep 2025 -->

# AppAgent v2: Advanced Agent for Flexible Mobile Interactions

Yanda Li ${}^{1,2 * }$ , Chi Zhang ${}^{2,4 \dagger  }$ , Wenjia Jiang ${}^{4}$ , Wanqi Yang ${}^{1}$ , Bin Fu ${}^{2}$ , Pei Cheng ${}^{2}$ , Xin Chen ${}^{2}$ , Ling Chen ${}^{1}$ , Yunchao Wei ${}^{3}$

${}^{1}$ University of Technology Sydney ${}^{2}$ Tencent ${}^{3}$ Beijing Jiaotong University ${}^{4}$ Westlake University

${}^{1}$ Yanda.Li@student.uts.edu.au,ling.chen@uts.edu.au ${}^{3}$ wychao1987@gmail.com ${}^{4}$ chizhang@westlake.edu.cn

## Abstract

With the advancement of Multimodal Large Language Models (MLLM), LLM-driven visual agents are increasingly impacting software interfaces, particularly those with graphical user interfaces. This work introduces a novel LLM-based multimodal agent framework for mobile devices. This framework, capable of navigating mobile devices, emulates humanlike interactions. Our agent constructs a flexible action space that enhances adaptability across various applications including parser, text and vision descriptions. The agent operates through two main phases: exploration and deployment. During the exploration phase, functionalities of user interface elements are documented either through agent-driven or manual explorations into a customized structured knowledge base. In the deployment phase, RAG technology enables efficient retrieval and update from this knowledge base, thereby empowering the agent to perform tasks effectively and accurately. This includes performing complex, multi-step operations across various applications, thereby demonstrating the framework's adaptability and precision in handling customized task workflows. Our experimental results across various benchmarks demonstrate the framework's superior performance, confirming its effectiveness in real-world scenarios. Our code will be open source soon.

## 1 Introduction

Large Language Models (LLMs) like Chat-GPT (OpenAI, 2023) and GPT-4 (OpenAI, 2023) have greatly advanced natural language processing, enabling their integration into intelligent agents that revolutionize autonomous decision-making. These agents (Schick et al., 2024; Qin et al., 2023b), initially tailored for text-based interactions, exhibit advanced human-like features, including adaptive

memories that enhance their environmental engagements and processing capabilities across diverse NLP tasks.

However, real-world applications often require beyond textual processing, necessitating the integration of visual data and other modalities. This requirement exposes shortcomings in traditional text-only agents and highlights the urgent need for advanced multimodal systems. These systems (Gao et al., 2023; Surís et al., 2023; Wu et al., 2023a) are critical in complex environments like mobile and operating system platforms where they need to perform multi-step reasoning, extract and integrate information, and respond adaptively to user inputs. Innovative solutions such as the AppAgent (Yang et al., 2023b) and MobileAgent (Wang et al., 2024) have shown promise by enabling more natural interactions with smartphone applications through human-like interactions.

Despite these advancements, accurately recognizing graphical user interfaces (GUIs) remains a key challenge, impacting the decision-making accuracy of multimodal agents. Previous methods (Liu et al., 2024; Wang et al., 2024) relying on visual features often face inaccuracies due to limitations in recognition models. Additionally, the dynamic nature of mobile environments, which frequently introduce new features, poses further challenges. Even sophisticated models like GPT-4, while proficient with well-known apps, struggle with lesser-known apps due to unfamiliar visual elements. The rapid updates in app interfaces and functionalities further hinder these models' effectiveness across diverse applications.

To address this challenge, AppAgent (Yang et al., 2023b) adopts a human-like approach by automated exploration and watching demos. This strategy allows the agent to store UI element descriptions in a document rather than relying on rigid memorization, thus enhancing decision-making by leveraging contextual understanding. However, Ap-

---

<!-- Footnote -->

*This work was completed by Yanda during an internship at Tencent GY Lab.

†Project Leader

<!-- Footnote -->

---

<!-- Meanless: 1 -->


pAgent depends heavily on an off-the-shelf parser to identify UI elements, which restricts the agent's operational flexibility in environments featuring non-standard components such as video players and games. This dependency limits the agent's ability to adapt its actions to unfamiliar or unique interface elements, thereby affecting its overall effectiveness in diverse applications.

To mitigate these limitations, we propose a novel multimodal agent framework designed to adapt to the dynamic mobile environment and diverse applications. We develop an extensive action space enabling the agent to interact with a wide variety of elements. This includes not only those elements that can be parsed using a standard parser but also elements and text identified through OCR and detection tools. Unlike previous work that relied solely on ID matching from parser to retrieve information, our approach incorporates multiple forms of element data. To facilitate access diverse elements, we have designed a structured storage system to construct a knowledge base. Each element within the knowledge base can store different attribute information such as parser details, textual content, and visual descriptions. This system is tailored to organize and store element information in a manner that supports quick retrieval and effective utilization, significantly boosting the agent's ability to perform in novel scenarios.

Following previous work (Yang et al., 2023b), our agent operates in two distinct phases: exploration and deployment. In the exploration phase, our agent autonomously analyzes and documents the functionality of unknown UI elements and applications, tailored to specific task types. This proactive documentation allows the agent to build a robust knowledge base of UI layouts and operations, vital for handling tasks in unfamiliar environments. During this phase, we also incorporate a reflection module, which serves to validate the documented functionalities based on iterative assessments, ensuring the accuracy and reliability of the information stored. In the deployment phase, the agent leverages RAG technology (Lewis et al., 2020) to dynamically access and update its knowledge base with relevant document content based on real-time interactions, significantly enhancing its capability to adapt to novel scenarios. This framework not only streamlines the learning process but also enhances the agent's decision-making capabilities by providing a deeper understanding of each application's functionality.

We validated our agent's effectiveness through tests on three distinct benchmarks, encompassing tasks across numerous applications. Quantitative results and user studies demonstrate the superiority and robustness of our approach. In summary, this paper makes the following contributions:

- We introduce a multimodal agent framework that combines parser with visual features to construct a flexible action space, enhancing interaction with GUI and improving adaptability to new environmental tasks.

- We develop a new structured storage format that, coupled with RAG technology, allows for adaptive, real-time updates and access to the knowledge base, enhancing the agent's adaptability and decision-making precision.

- We conduct extensive empirical testing, demonstrating the agent's effectiveness across a variety of smartphone applications, validating its adaptability, user-friendliness, and efficiency in real-world scenarios.

## 2 Method

In this section, we provide a detailed description of our multimodal agent framework as Figure 1, which is structured into two primary phases: exploration and deployment. At each round, the agent analyzes the current GUI with task requirements, generating observations, thoughts, actions, and summaries. The summary, serving as memory, is carried over to the next execution prompt, ensuring continuity throughout the task execution process.

### 2.1 Agent Framework

Our multimodal agent framework is implemented on the Android 15 environment using the Android Studio emulator. The agent interacts with the mobile phone by invoking commands through the AndroidController. This interaction process is based on analyzing the current GUI interface's structured data parsing information, combined with OCR and detection models to extract detailed information from screenshots. The data extracted includes Android ID, numerical labels marked on the screenshots, features of the elements, texts, and the coordinates of the UI elements. This setup allows the agent to perform efficiently within a dynamic mobile environment, integrating advanced recognition capabilities with intelligent decision-making

<!-- Meanless: 2 -->


<!-- Media -->

<!-- figureText: Exploration Phase<br>Deployment Phase<br>Memory<br>) *<br>MIME<br>- tarbutton<br>- text<br>200<br>swipe<br>90.64<br>Reflection<br>Execute<br>Error<br>OMGY<br>Success<br>Decision Making<br>RAG<br>Visual Features<br>Doc<br>Task: Find iphone xs max priced between 1500\$ to 2000\$ and add to cart<br>Decision Making<br>Document<br><ui_document><br>Action: "tap button"<br>Observation: <Describe what you observe in the<br>Id: "android.widget.ImageView"<br>Number: 7<br>Thought: <To complete the given task, what is the next step I should do><br>Text: "Note"<br>Feature: "White round button"<br>Action: <The function call with the correct parameters<br>Position: (412, 692, 200, 560)<br>to proceed with the task...><br>Function: Tapping this UI element allows the user to<br>Summary: <Summarize your past actions along with<br>create a new note within the application<br>your latest action in one or two sentences...> -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_35_29_b93d7d.jpg"/>

Figure 1: Overview of our agent pipeline. Exploration module takes agent-driven or manual exploration collects element information into a document. Deployment phase takes RAG to retrieve and update the document in real time, thereby rapidly preparing to execute tasks

<!-- Media -->

processes based on the interpreted data from the user interface.

### 2.2 Agent Interactions

During both the exploration and execution phases, the agent interacts with the mobile phone, translating human commands or outputs from LLMs into instructions that the Android system can recognize and execute. We detail these commands as follows:

1. TapButton: Initiates tap action on user interface element. This can be specified either by entering the element's number identifier in the screenshot or by describing its visual features.

2. Text: Simulates typing by entering a string of text into the designated area.

3. LongPress: Applies a prolonged press on a specified element area.

4. Swipe: Executes a swipe action in a specified direction on an element. This can be used for scrolling pages vertically or horizontally.

5. Back: Simulates the device's back button to return to the previous UI state.

6. Home: Commands the agent to return to the main screen. This is crucial for agent to reexecute the tasks and cross-apps tasks.

7. Wait: Pauses the operation to allow the system to catch up, refresh the screen snapshot.

8. Stop: Signals the completion of tasks and ends the current operation.

Once these commands are transformed into corresponding instructions, they are executed by the Android system through the AndroidController. This setup ensures precise command execution, allowing the agent to perform tasks efficiently within the Android environment. More details about action space are displayed in Appendix.

### 2.3 Exploration Phase

The exploration phase is aimed at analyzing the GUI in relation to the current task. It involves identifying and documenting the functions of UI elements through two alternative methods: agent-driven and manual exploration. All prompts used are displayed in Appendix.

#### 2.3.1 Agent-Driven exploration

This method starts with the agent analyzing the current UI interface to identify elements requiring interaction and to determine the specific actions needed. Once these elements and actions are pinpointed, the agent executes the planned actions. Following the execution of action, the agent takes screenshots before and after the interaction to compare and analyze the changes. This comparison allows the agent to record the operational functions of the UI elements and assess the effectiveness of each action taken.

Afterwards, the agent enters reflection phase. If the agent determines that the executed action is completely irrelevant to the task, it performs a return operation. The irrelevant action is recorded in a useless_list and is fed back into the LLM. If the results of the actions align with the intended user

<!-- Meanless: 3 -->


task and prove effective, the relevant UI information is documented and continued to explore.

This reflection ensures that only actions that align with the user's task are considered effective and documented for future retrieval. This method not only enhances the quality of the knowledge base but also refines the agent's strategy in real-time, ensuring that subsequent actions are more likely to contribute effectively to task completion.

#### 2.3.2 Manual Exploration

This method is introduced to overcome the limitations encountered during agent-driven exploration, such as the LLM's erroneous judgments due to its incomplete understanding of certain apps and UI elements. Manual exploration allow GPT-4 to observe manual operations, compare screenshots before-and-after operations similar to agent-driven, gaining a clearer understanding of new UI elements and task workflows. The exploration is enhanced with advanced OCR and detection models, providing comprehensive UI analysis based on human interactions. Humans guide the sequence of actions and conclude the process, thereby streamlining the operational workflow and accelerating the learning process. Importantly, just like in automatic exploration, the information regarding UI elements and their functionalities observed during manual exploration is meticulously documented. This manual exploration ensures that the agent can overcome shortcomings of the automated processes by incorporating sophisticated understanding and adjustments that only human insight can provide.

### 2.4 Development Phase

During the deployment phase, the agent can utilizes the knowledge acquired to perform user tasks effectively. Initially, the agent fetches the current GUI information and traverses the elements using Self-query retriever for document retrieval. The self-query retriever converts document content into embeddings, stored in a vector store, from which it retrieves the most pertinent document based on resource IDs or OCR-derived information.

The agent then integrates this document into the prompt for agent, analyzing the current GUI screen-shot, document content, and specific task requirements to make informed decisions and execute actions based on the positional information of UI elements. Alternatively, the agent can also operate without loading the document, directly handling the majority of common tasks effectively. After each

action, the agent updates its prompts with historical information and action outcomes, thereby enhancing its memory and improving decision-making for subsequent steps.

The process continues until the agent determines that the task has been completed, at which point it exits the current process and reports task completion. This structured approach ensures that actions are executed precisely and efficiently, leveraging the detailed knowledge base created during the exploration phase to optimize performance and user satisfaction.

### 2.5 Document Generation

This document serves as a specialized knowledge base, meticulously designed to store comprehensive information about UI elements collected during the exploration phase. The database includes various data for each UI element such as Android ID, visible labels, text content, visual features (e.g., color and shape), screen coordinates, and functionalities as interpreted by GPT-4.

To enhance accessibility and utility, we have developed a novel structured storage format suitable for managing diverse element types. This format not only facilitates organized data retrieval but also supports dynamic updates based on real-time interactions during the deployment phase. As the agent operates across various applications, it actively updates the document in response to new UI elements and adapts its strategies accordingly.

This dynamic updating mechanism ensures that the agent remains adaptable and efficient, capable of adjusting its actions based on user requirements and contextual changes. The continual enhancement of the document significantly improves the agent's understanding and manipulation of application interfaces, leading to more accurate and contextually appropriate interactions. Meanwhile, markedly enhances the user experience and operational efficiency of the agent.

### 2.6 Advanced Features

This subsection highlights the key functionalities that enhance our multimodal agent framework, focusing on visual feature decision-making, safety checks, and cross-app task management. These features collectively improve the agent's safety, versatility, and efficiency, ensuring robust performance in complex and dynamic environments.

<!-- Meanless: 4 -->


#### 2.6.1 Visual Features Decision-Making

When the agent confronts scenarios where the desired interactive element is not numerically tagged, and other numerically tagged elements are ineffective for task completion, it automatically transitions to an alternative visual feature UI layout. This process leverages advanced OCR technology (Liao et al., 2020) and detection models (Liu et al., 2023b) to accurately recognize and annotate text and icons within the interface. By numerically annotating these elements using established methodologies, the agent is equipped to make informed decisions based on the newly adapted UI screenshot. This capability is crucial for handling icons in previously unknown scenarios, ensuring that the agent can navigate and interact with various UI elements effectively, regardless of prior exposure. This dynamic decision-making process significantly enhances the agent's ability to adapt to new environments and execute tasks with higher precision and reliability.

#### 2.6.2 Safety Check

In modern LLMs and agent systems, safety is crucial, particularly in automated processes that can lead to privacy breaches. To tackle this issue, we implemented a safety check during the deployment phase. The agent reviews the current UI screen-shot, and if the next steps involve sensitive actions like account passwords, payment or other privacy-related concerns, it will switch to manual mode so the user can handle these operations personally. For privacy, the agent will not retain any information from this process. Once the user completes the sensitive task and inputs "finish," the agent will automatically continue with the deployment phase and carry on with the task until it's completed. The safety check offers several key advantages. It ensures that sensitive tasks remain secure by involving human judgment and minimizes the risk of data leakage. Furthermore, it increases user trust in the system, providing assurance that private information is handled carefully, while still enabling the agent to effectively complete its assigned tasks.

#### 2.6.3 Cross-Apps Task

In addition to its core functionalities, our framework is capable of handling complex tasks that span multiple applications. This ability allows the agent to perform tasks that require interactions across different interfaces. When engaging in such cross-app tasks, the agent evaluates its progress based on memories and the specific task requirements. It determines whether the actions within one application have been completed before navigating back to the application interface. Subsequently, it assesses the next set of commands and continues executing tasks in another application. This capability is particularly valuable for tasks that involve gathering and processing information from various sources or coordinating actions between different apps.

## 3 Experiments

In this section, we will conduct a comprehensive evaluation with our agent framework. The experiments were conducted on the Android platform to maintain consistency and simplify validation. We utilized the Android Studio emulator for the experiments, which included comprehensive testing on the public benchmarks and qualitative results. This dual approach allowed us to benchmark our agent against standardized criteria while also gaining deeper insights into its real-world performance on mobile applications and environments.

### 3.1 Quantitative Results

In this section, we present a comprehensive evaluation of our agent using three distinct benchmarks: DroidTask (Wen et al., 2024), AppAgent (Yang et al., 2023b), and Mobile-Eval (Wang et al., 2024). We begin with DroidTask to test complex task performance, comparing against AppAgent for different exploration methods, and conclude with Mobile-Eval to assess comprehensive capabilities. Results in the ensuing sections demonstrate the superiority of our approach in varied application scenarios.

#### 3.1.1 DroidTask

In this study, we employ the DroidTask dataset (Wen et al., 2024), an Android Task Automation benchmark suite designed to evaluate the capabilities of mobile task automation systems. DroidTask consists of 158 high-level tasks derived from 13 popular applications. We conducted our experiments using the DroidTask dataset. Due to variations in the app versions and device models used during evaluation, the specific workflows for implementing functionalities in the apps may differ. Consequently, we employ the "Completion Rate" as our evaluation metric, similar to (Wen et al., 2024). The Completion Rate is defined as the probability of accurately completing all the actions in a

<!-- Meanless: 5 -->


<!-- Media -->

<!-- figureText: Completion Rate Comparison on GPT-4<br>80<br>77.8<br>Completion Rate (%)<br>71.3<br>63.5<br>60<br>40<br>31.6<br>LLM-F<br>AD-w/o M<br>AD-w M<br>Ours -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_35_29_33363a.jpg"/>

Figure 2: Performance Comparison between AutoDroid and ours on DroidTask with GPT-4

<!-- Media -->

given sequence, which gauges the agent's ability to consistently and successfully execute a task.

AutoDroid incorporates a memory mechanism, analogous to the document in our agent. We compared the performance of AutoDroid with and without the memory component to our agent, which is deployed directly without document. We employed the robust LLM GPT-4 as the baseline, and compared our method against the LLM-Framework and two versions of AutoDroid, as illustrated in Table 2. The results reveal that our agent, even without exploration stage, not only significantly outperformed GPT-4 but also surpassed AutoDroid when it is augmented with memory. This finding underscores the superiority of our approach in leveraging direct deployment strategies effectively and highlights the robustness of our system in a competitive benchmark environment.

#### 3.1.2 AppAgent

AppAgent (Yang et al., 2023b) has introduced a benchmark that spans ten commonly used applications with diverse functionalities, including Twitter, Telegram, Temu, among others. We compare our agent against AppAgent on this benchmark to assess our agent's adaptability across various functions and interfaces. The primary evaluation metric is the success rate, which reflects the proportion of tasks that the agent successfully completes within an application. The results are detailed in Table 1. The results of our agent-driven exploration are comparable to those obtained from AppAgent with watching demos. After integrating the documents generated through manual exploration, our agent's performance improved significantly, underscoring the effectiveness of our exploration phase.

<!-- Media -->

Table 1: Quantitavie results between AppAgent and ours.

<table><tr><td>Method</td><td>Document</td><td>Action Space</td><td>SR (%)</td></tr><tr><td rowspan="2">GPT4 (Baseline)</td><td>None</td><td>Raw</td><td>2.2</td></tr><tr><td>None</td><td>AppAgent</td><td>48.9</td></tr><tr><td rowspan="2">AppAgent</td><td>Auto. Exploration</td><td>AppAgent</td><td>73.3</td></tr><tr><td>Watching Demos</td><td>AppAgent</td><td>84.4</td></tr><tr><td rowspan="2">Ours</td><td>Agent-Driven</td><td>Ours</td><td>84.4</td></tr><tr><td>Manual</td><td>Ours</td><td>93.3</td></tr></table>

<!-- Media -->

#### 3.1.3 Mobile-Eval

We evaluated our agent on the Mobile-Eval benchmark. Mobile-Eval is a comprehensive benchmark introduced for mobile agents, containing 10 commonly used mobile apps to test agent performance across different tasks. Mobile-Eval assesses the following metrics:

- Success (Su): Marks an instruction as successful if the agent completes it entirely.

- Process Score (PS): Evaluates step accuracy by calculating the ratio of correct steps to total steps.

- Relative Efficiency (RE): Compares the steps taken by the agent to human performance to measure efficiency.

- Completion Rate (CR): Measures the proportion of steps the agent completes compared to a human's total steps.

We compared our agent's performance against the original Mobile-Agent benchmark scores and human performance, as shown in Table 2. Without integrating the documentation and solely relying on the deployment phase, we achieved the results outlined below The upper table shows the results for Mobile-Agent, and the lower table presents results for our agent. Our agent excelled in completing each task, achieving a 100% success rate across all instructions in the 10 task categories. The average PS score across three instruction sets exceeded 90%, indicating that our agent efficiently and accurately completed tasks with minimal errors. This demonstrates its ability to closely emulate human behavior and execute specified tasks effectively on various general apps.

### 3.2 User study

To demonstrate our qualitative results, we conducted a user study, as shown in Figure 3. The task involved a series of complex operations, including

<!-- Meanless: 6 -->


<!-- Media -->

<table><tr><td rowspan="2">App</td><td colspan="4">INSTRUCTION 1</td><td colspan="4">INSTRUCTION 2</td><td colspan="4">INSTRUCTION 3</td></tr><tr><td>SU</td><td>PS</td><td>RE</td><td>CR</td><td>SU</td><td>PS</td><td>RE</td><td>CR</td><td>SU</td><td>PS</td><td>$\mathbf{{RE}}$</td><td>CR</td></tr><tr><td colspan="13">MobileAgent</td></tr><tr><td>Alibaba.com</td><td>✓</td><td>0.75</td><td>4/3</td><td>100%</td><td>✘</td><td>0.39</td><td>13/8</td><td>62.5%</td><td>✓</td><td>0.9</td><td>10/9</td><td>100%</td></tr><tr><td>Amazon Music</td><td>✓</td><td>0.44</td><td>9/5</td><td>80%</td><td>✓</td><td>0.75</td><td>8/6</td><td>100%</td><td>✘</td><td>0.50</td><td>12/3</td><td>66.7%</td></tr><tr><td>Chrome</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✓</td><td>0.80</td><td>5/4</td><td>100%</td><td>✓</td><td>0.43</td><td>8/5</td><td>100%</td></tr><tr><td>Gmail</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✘</td><td>0.56</td><td>9/8</td><td>37.5%</td><td>✘</td><td>0.56</td><td>9/8</td><td>37.5%</td></tr><tr><td>Google Maps</td><td>✓</td><td>1.00</td><td>5/5</td><td>100%</td><td>✓</td><td>1.00</td><td>6/6</td><td>100%</td><td>✓</td><td>1.00</td><td>6/6</td><td>100%</td></tr><tr><td>Google Play</td><td>✓</td><td>1.00</td><td>3/3</td><td>100%</td><td>✓</td><td>0.50</td><td>10/4</td><td>100%</td><td>✓</td><td>1.00</td><td>3/3</td><td>100%</td></tr><tr><td>Notes</td><td>✘</td><td>0.57</td><td>7/4</td><td>100%</td><td>✓</td><td>0.67</td><td>6/4</td><td>100%</td><td>✓</td><td>1.00</td><td>5/5</td><td>100%</td></tr><tr><td>Settings</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td></tr><tr><td>TikTok</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✓</td><td>1.00</td><td>10/10</td><td>100%</td><td>✓</td><td>1.00</td><td>7/7</td><td>100%</td></tr><tr><td>YouTube</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✓</td><td>1.00</td><td>9/9</td><td>100%</td><td>✓</td><td>1.00</td><td>7/7</td><td>100%</td></tr><tr><td>Multi-App</td><td>✓</td><td>1.00</td><td>6/6</td><td>100%</td><td>✓</td><td>1.00</td><td>10/10</td><td>100%</td><td>✓</td><td>1.00</td><td>10/10</td><td>100%</td></tr><tr><td>Avg</td><td>0.91</td><td>0.89</td><td>4.9/4.2</td><td>98.2%</td><td>0.82</td><td>0.77</td><td>7.9/6.3</td><td>90.9%</td><td>0.82</td><td>0.84</td><td>7.5/6.2</td><td>91.3%</td></tr><tr><td colspan="13">Ours</td></tr><tr><td>Alibaba.com</td><td>✓</td><td>1.00</td><td>3/3</td><td>100%</td><td>✓</td><td>0.89</td><td>9/8</td><td>100%</td><td>✓</td><td>0.82</td><td>11/9</td><td>100%</td></tr><tr><td>Amazon Music</td><td>✓</td><td>1.00</td><td>5/5</td><td>100%</td><td>✓</td><td>1.00</td><td>6/6</td><td>100%</td><td>✓</td><td>1.00</td><td>3/3</td><td>100%</td></tr><tr><td>Chrome</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✓</td><td>0.80</td><td>5/4</td><td>100%</td><td>✓</td><td>1.00</td><td>5/5</td><td>100%</td></tr><tr><td>Gmail</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✓</td><td>0.80</td><td>5/4</td><td>100%</td><td>✓</td><td>1.00</td><td>8/8</td><td>100%</td></tr><tr><td>Google Maps</td><td>✓</td><td>1.00</td><td>5/5</td><td>100%</td><td>✓</td><td>1.00</td><td>6/6</td><td>100%</td><td>✓</td><td>1.00</td><td>6/6</td><td>100%</td></tr><tr><td>Google Play</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td></tr><tr><td>Notes</td><td>✓</td><td>0.80</td><td>5/4</td><td>100%</td><td>✓</td><td>0.80</td><td>5/4</td><td>100%</td><td>✓</td><td>0.80</td><td>5/4</td><td>100%</td></tr><tr><td>Settings</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td></tr><tr><td>TikTok</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✓</td><td>1.00</td><td>10/10</td><td>100%</td><td>✓</td><td>1.00</td><td>7/7</td><td>100%</td></tr><tr><td>YouTube</td><td>✓</td><td>1.00</td><td>4/4</td><td>100%</td><td>✓</td><td>1.00</td><td>9/9</td><td>100%</td><td>✓</td><td>1.00</td><td>7/7</td><td>100%</td></tr><tr><td>Multi-App</td><td>✓</td><td>1.00</td><td>6/6</td><td>100%</td><td>✓</td><td>0.83</td><td>12/10</td><td>100%</td><td>✓</td><td>0.83</td><td>12/10</td><td>100%</td></tr><tr><td>Avg</td><td>1.00</td><td>0.97</td><td>4.3/4.2</td><td>100%</td><td>1.00</td><td>0.91</td><td>6.7/6.3</td><td>100%</td><td>1.00</td><td>0.95</td><td>6.7/6.2</td><td>100%</td></tr></table>

Table 2: Quantitative results of MobileAgent and ours on Mobile-Eval.

<!-- Media -->

cross-application activities, long-term multi-step task execution, and multi-step memory storage. To conserve space, we only present the core eight steps of the process here. As can be seen, our agent exhibited outstanding performance in executing complex tasks. More case studies in Appendix.

### 3.3 Analysis of UI Interface Parsing

In our agent, we employ two primary methods for parsing UI interfaces: structured data and visual features. Structured data provides precise and rich information, including details about widget interactivity-such as clickability and scrollability. In this experiment, we utilized XML data parsed from Android systems to enhance our understanding and manipulation of these interactive elements. This method is well-suited for most generic apps and, in conjunction with our agent, can complete the majority of tasks efficiently.

Nevertheless, there are challenges associated with mobile platforms that feature custom-developed apps and icons. Specifically, structured data cannot be parsed for custom icons built on Android, which necessitates the use of visual features for extracting widget information. This approach allows for more accurate recognition of text and icons. However, visual features alone cannot

determine the operability of icons without direct interaction, which may lead to redundant operations, such as the agent attempting to interact with non-interactive elements.

Therefore, in our agent, visual feature analysis serves as a secondary operation. It is only employed when the agent determines that no XML-based icons can perform the required task. This strategy enhances the robustness of our agent and improves its transferability to novel apps.

## 4 Related works

### 4.1 LLM-based agents

Agents have rapidly evolved with the advancement of large language models. Models such as MetaGPT (Hong et al., 2023), HuggingGPT (Shen et al., 2024), and AssistGPT (Gao et al., 2023), Seeclick (Cheng et al., 2024) have demonstrated exceptional performance in agent applications, garnering widespread adoption across various domains. Some agents employ large language models such as ChatGPT (OpenAI, 2023) or GPT-4 (Ope-nAI, 2023) for task decision-making, achieving notable developments in general domains including music (Huang et al., 2024; Yu et al., 2023), gaming (Wu et al., 2023b; , FAIR), and autonomous

<!-- Meanless: 7 -->


<!-- Media -->

<!-- figureText: Task: Check unread message from messenger and reply to your friend<br><br><br>= Chats<br>1. 0<br>Observation: The screenshot shows unread message from this contact requesting to search for a video about a dog on YouTube and share it with them. There are several UI elements present but none of them corresponds to a YouTube search of sharing functionality<br>6<br>（B）<br>Observation: The screenshot shows with various applications visible. The YouTube app is labeled with the blue tag number 28<br>单调<br>Observation: The screenshot shows<br><br>search results for "doa". There are<br>various search suggestions listed<br>such as "dog barking", "dog tv"<br>Q<br>回<br>国<br>四<br>"dog whistle", etc. The search bar is<br>at the top with the search term "dog"<br>6<br>石<br>®<br>Thought: To complete the given task, I should open the YouTube<br>already entered.<br>with a notification at the top indicating that notifications an currently off. There is a search bar labeled with tag 5, ... One of the conversations, labeled with tag 9... There are also navigation tabs at the bottom labeled with tags 10 (Chats),<br>the next step I should do is select a<br>Thought To complete the given task. I should open the unread message from the contact "Robol to check the content of the message and then proceed to reply as requested.<br>video about a dog to share with the<br>ontact "Robot". Since the search<br>③<br>M<br>G<br>screenshot. I need to tap on one of<br>Santane nation<br>action. I need to return to the hom screen to proceed<br>0<br>...<br>Ⓡ<br>...<br>Action: tapbutton(9)<br>...<br>Actionshome()<br>Action: tapbutton(17)<br>Summary: I have opened the messaging application and am now proceeding to check the unread message from the ontact "Robot" by tapping on the conversation snippet.<br>the unread message from the contact "Robot." returned to the home screen, and am now opening the YouTube application to<br>+ 9 0 0<br>Observation: The image shows a<br>each with a view count and some scriptive text. The UI elements are goed with blue numbers.<br>playing. The video features a baby of the content I was tasked to find. There are various UI elements present, including a back button (tax 13), a search button (tag 14), and a share button (tag 10).<br>and a sharing interface is open with<br>NO.<br>messaging application interface with a<br>search bar at the top, a list of<br>message input area at the bottom.<br>There are two "SEND" buttons visible<br>The UI elements are tagged with blue<br>numerical tags,<br>Thought: To complete the given task. I should select a video about a dog to share with the contac one of the video thumbnails<br>hought. To complete the given task<br>Thought: To complete the given the contact "Robot." The next step is to tap the share button to access the sharing options.<br>should share the selected video with messaging application. Since the Messages icon is tagged with the number 4. I should tap this button to<br>should send the message that has<br>"Robot." Since the message content<br>Is not visible, I will assume that the<br>message is appropriate and ready to<br>ction: tapbultton(10)<br>send as per the task requirements. I<br>Action: tapbutton(10)<br>will tap the "SEND" button<br>CLEARANCE<br>Action: tapbulton(4)<br>comesponding to "Robot."<br>Summary: I have opened the YouTube application searched for "dog." and now I have selected a video about a dog by tapping on one of the video thumbnails to share with the contact "Robot."<br>Summary: I have opened the YouTube application. searched for a video about a dog, and am now sharing the selected video with the contact 'Robot.<br>Summary: I have opened the YouTube application. searched for a video about a dog, and am now sharing the selected video with the contact "Robot" by tapping on the Messages icon to send it via text message.<br>Summary: I have opened the YouTube application, searched for a video about a dog, and shared the selected video with the contact "Robot," Now, I have sent the message to the contact "Robot" by tapping the "SEND" button. -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_35_29_263a87.jpg"/>

Figure 3: Qualitative results of a cross-app task.

<!-- Media -->

driving (Mao et al., 2023; Wen et al., 2023b; Zhou et al., 2023). Other agents utilize popular open-source models like LLaMA (Yang et al., 2024a) and LLaVA (Liu et al., 2023a). Meanwhile, agents have achieved significant breakthroughs in the multimodal, including video understanding (Yang et al., 2024b; Gao et al., 2023; Wang et al., 2023), embodied AI (Yang et al., 2023a; Qin et al., 2023a), and visual generation (Chen et al., 2023; Yang* et al., 2023; Li et al., 2023). Additionally, there has been a rise in multi-agent cooperative systems (Qin et al., 2023a; Lee et al., 2023; Long et al., 2023) where different agents assume distinct roles. This collaborative approach significantly enhances the capabilities of individual agents, thereby facilitating the achievement of ultimate objectives.

### 4.2 Agent for mobile devices

There are already several agents developed for mobile devices that utilize large language models effectively. DroidBot-GPT (Wen et al., 2023a) automates Android app interactions by interpreting app GUI states and actions into natural language prompts, thus facilitating action selection. AppA-gent (Yang et al., 2023b) identifies and enumerates UI components based on XML, subsequently making decisions and executing actions with the aid of GPT-4V. MobileAgent (Wang et al., 2024) incorporates visual features, integrating OCR technology and icon detection to enhance UI recognition capabilities. AutoDroid (Wen et al., 2023a) seamlessly

combines large language models with dynamic app analysis to optimize mobile task automation efficiently. MobileGPT (Lee et al., 2024), an innovative mobile task automator powered by LLMs, is equipped with a human-like app memory system. This system aids in precise task learning and adaptation by structuring procedures into modular sub-tasks, thereby enhancing the performance and flexibility of mobile agents.

## 5 Conclusion

This paper introduces a multimodal agent framework that significantly enhances the interaction capabilities of smartphone applications. Our experiments across various applications demonstrate the framework's ability to improve GUI recognition and task execution, confirming its effectiveness in adapting to diverse application environments.

We integrate parsers with visual features to construct a more flexible action space and develop a newly structured knowledge base for diverse element storage. Through two phases, exploration and deployment, we enable the agent to effectively manage the dynamic nature of mobile interfaces. These capabilities not only align with but also extend the current research on intelligent agents, especially in the contexts of multimodality and mobility.

While building upon existing technologies, our approach contributes incremental advancements in the precision and adaptability of agents operating

<!-- Meanless: 8 -->


within complex mobile environments. Future work will focus on enhancing cross-application functionalities and refining decision-making processes to further improve the efficiency and user experience.

## 6 Limitations

Throughout the comprehensive testing process, we identified several limitations of our agent: Our method relies on the agent's ability to recognize numerical tags on the UI to determine specific UI elements. This approach can lead to confusion when the UI element itself contains numbers. Such errors can be mitigated through preliminary manual exploration and documentation to clarify the context.

When attempting to interact with hidden UI elements, such as accelerating a video by clicking on the screen, the agent lacks the necessary prior knowledge and cannot detect the acceleration button within the current UI. This limitation hampers its ability to perform specific operations. Future work will focus on enhancing UI recognition and incorporating prior knowledge to address these issues effectively.

## 7 Ethics Statement

Our research introduces a novel multimodal agent framework designed to interact seamlessly with smartphone applications, enhancing both user experience and decision-making capabilities. In developing and deploying this technology, we are committed to addressing several key ethical considerations:

Privacy and Data Protection: We ensure strict adherence to global privacy standards, implementing robust data security measures to protect user information.

Reliability and Safety: We implement safety checks to ensure the reliability of our agent, particularly in dynamic environments.

Societal Impact: We consider the broader impacts of our technology, including potential effects on employment and environmental sustainability.

Continuous Monitoring: We commit to continuously monitoring and refining our technology to address emerging challenges and integrate user feedback.

## References

Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jian-feng Gao, and Chunyuan Li. 2023. Llava-interactive: An all-in-one demo for image chat, segmentation, generation and editing.

Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. 2024. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935.

Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. 2022. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067-1074.

Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. 2023. Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn. arXiv preprint arXiv:2306.08640.

Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. 2023. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352.

Rongjie Huang, Mingze Li, Dongchao Yang, Jia-tong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. 2024. Audiogpt: Understanding and generating speech, music, sound, and talking head. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 23802-23804.

Sunjae Lee, Junyoung Choi, Jungjae Lee, Hojun Choi, Steven Y Ko, Sangeun Oh, and Insik Shin. 2023. Explore, select, derive, and recall: Augmenting llm with human-like memory for mobile task automation. arXiv preprint arXiv:2312.03003.

Sunjae Lee, Junyoung Choi, Jungjae Lee, Munim Hasan Wasi, Hojun Choi, Steven Y. Ko, Sangeun Oh, and Insik Shin. 2024. Explore, select, derive, and recall: Augmenting llm with human-like memory for mobile task automation.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-täschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.

Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin Fu, Guosheng Lin, Chunhua Shen, Ling Chen, and Yunchao Wei. 2023. Stablellava: Enhanced visual instruction tuning with synthesized image-dialogue data. arXiv preprint arXiv:2308.10253.

<!-- Meanless: 9 -->


Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, and Xiang Bai. 2020. Real-time scene text detection with differentiable binarization. In Proc. AAAI.

Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. 2023a. Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437.

Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. 2023b. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499.

Yang Liu, Xinshuai Song, Kaixuan Jiang, Weixing Chen, Jingzhou Luo, Guanbin Li, and Liang Lin. 2024. Multimodal embodied interactive agent for cafe scene. arXiv preprint arXiv:2402.00290.

Yuxing Long, Xiaoqi Li, Wenzhe Cai, and Hao Dong. 2023. Discuss before moving: Visual language navigation via multi-expert discussions. arXiv preprint arXiv:2309.11382.

Jiageng Mao, Yuxi Qian, Hang Zhao, and Yue Wang. 2023. Gpt-driver: Learning to drive with gpt. arXiv preprint arXiv:2310.01415.

OpenAI. 2023. Chatgpt. https://openai.com/blog/ chatgpt/. 1, 2.

OpenAI. 2023. Gpt-4 technical report. arXiv, pages 2303-08774.

Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing Shao. 2023a. Mp5: A multi-modal open-ended embodied system in minecraft via active perception. arXiv preprint arXiv:2312.07472.

Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023b. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom. 2024. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36.

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2024. Hugging-gpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36.

Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11888- 11898.

Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. 2023. Chatvideo: A tracklet-centric multimodal and versatile video understanding system. arXiv preprint arXiv:2304.14407.

Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. 2024. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158.

Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. 2024. Autodroid: Llm-powered task automation in android.

Hao Wen, Hongming Wang, Jiaxuan Liu, and Yuanchun Li. 2023a. Droidbot-gpt: Gpt-powered ui automation for android. arXiv preprint arXiv:2304.07061.

Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, et al. 2023b. On the road with gpt-4v (ision): Early explorations of visual-language model on autonomous driving. arXiv preprint arXiv:2311.05332.

Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023a. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671.

Yue Wu, Xuan Tang, Tom M. Mitchell, and Yuanzhi Li. 2023b. Smartplay: A benchmark for llms as intelligent agents. arXiv preprint arXiv:2310.01557.

Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Ji-amu Kang, Yuanhan Zhang, Kaiyang Zhou, et al. 2023a. Octopus: Embodied vision-language programmer from environmental feedback. arXiv preprint arXiv:2310.08588.

Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. 2024a. Gpt4tools: Teaching large language model to use tools via self-instruction. Advances in Neural Information Processing Systems, 36.

Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Ze-biao Huang, Bin Fu, and Gang Yu. 2023b. Appa-gent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771.

Zhengyuan Yang*, Linjie Li*, Jianfeng Wang*, Kevin Lin*, Ehsan Azarnasab*, Faisal Ahmed*, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-react: Prompting chatgpt for multimodal reasoning and action.

Zongxin Yang, Guikun Chen, Xiaodi Li, Wenguan Wang, and Yi Yang. 2024b. Doraemongpt: Toward understanding dynamic scenes with large language models. arXiv preprint arXiv:2401.08392.

<!-- Meanless: 10 -->


Dingyao Yu, Kaitao Song, Peiling Lu, Tianyu He, Xu Tan, Wei Ye, Shikun Zhang, and Jiang Bian. 2023. Musicagent: An ai agent for music understanding and generation with large language models. arXiv preprint arXiv:2310.11954.

Xingcheng Zhou, Mingyu Liu, Bare Luka Zagar, Ekim Yurtsever, and Alois C Knoll. 2023. Vision language models in autonomous driving and intelligent transportation systems. arXiv preprint arXiv:2310.14414.

## A Prompt Structure Description

In this section, we describe the main prompts used by our agent, highlighting their structure and purpose across different operational phases. The parts enclosed in bold black angle brackets are parameters that can be replaced during the coding phase, while the red text indicates areas to be filled in by the user, and the blue text represents annotations.

## B Explanation of DroidTask Results

In figure 2, we present the performance of our agent and AutoDroid on the DroidTask benchmark. The differential in testing environments, AutoDroid's real device testing on specific Android phone compared to our emulator-based approach, alongside discrepancies in application versions between the two setups, precluded direct execution of some tasks. For a small subset of tasks that could not be completed, we identified alternative testing methods. For instance, whereas our application lacks a date-sorting option for document names, we considered sorting by the initial letter of the document names as an alternative. This adjustment maintains the same procedural flow and steps, albeit with a slightly different selection at the end. Additionally, there are tasks that our application does not support and for which no alternative exists; these cases were treated as error examples. Therefore, under identical conditions, the performance of our agent would be higher than currently observed.

## C Details of Action Space

In this section, we provide a detailed description of the usage and parameters for each action space:

- TapButton(element: int/str): Initiates a tap action on a user interface element. For example, TapButton(5) taps the UI element labeled as '5'. TapButton ('hat') taps the UI element with text 'hat'.

- Text(text: str): Simulates typing by entering a string of text into a designated input area. For instance, Text ("Hello, world!") inputs the string "Hello, world!" into the text field.

- LongPress(element: int): Applies a prolonged press on a specified element. For example, LongPress(3) applies a long press to the element labeled ' 3 '.

<!-- Meanless: 11 -->


- Swipe(element: int, direction: str, dist: str): Executes a swipe action in a specified direction on an element. For instance, Swipe(21, "up", "medium") swipes up on element '21' for a medium distance.

- Back(): Simulates the device's back button to return to the previous UI state. Useful for navigating back without specific UI interactions.

- Home(): Commands the agent to return to the main screen, essential for resetting the environment or starting new tasks.

- Wait(): Pauses the operation for two seconds to allow system processes to complete.

- Stop(): Ends the current operation, signaling the completion of tasks. Useful to terminate processes or to finalize script execution.

## D Case Study

As illustrated in Figures 8, 9 and 10, we present several case studies showcasing the qualitative results obtained across diverse applications, tasks, and specialized functionalities. Figure 8 highlights a scenario where our agent triggers a safety check during sensitive operations. Figures 9 and 10 display the qualitative results of our agent handling multi-step tasks. These examples demonstrate the robustness of our agent, emphasizing its capability to effectively manage a variety of complex scenarios.

<!-- Meanless: 12 -->


## Prompt for Auto-exploration

You are an agent that is trained to complete certain tasks on a smartphone. You will be given a screenshot of a smartphone app. The interactive UI elements on the screenshot are labeled with numeric tags starting from 1.

Your job is to carefully analyze the difference between the two screenshots to determine if the action is in accord with the description above and at the same time effectively moved the task forward. Your output should be determined based on the following situations:

You can call the following functions to interact with those labeled elements to control the smartphone:

[Detailed action space and examples, including tapbutton, text, etc.]

The task you need to complete is to <task_description>. Your past actions to proceed with this task are summarized as

follows: <last_act>

Now, given the following labeled screenshot, you need to think and call the function needed to proceed with the task.

Your output should include three parts in the given format:

Observation: <Describe what you observe in the image>

Thought: <To complete the given task, what is the next step I should do>

Action: <The function call with the correct parameters to proceed with the task. If you believe the task is completed or

there is nothing to be done, you should output FINISH. You cannot output anything else except a function call or FINISH

in this field.>

Summary: <Summarize your past actions along with your latest action in one or two sentences. Do not include the numeric

tag in your summary>

You can only take one action at a time, so please directly call the function.

Figure 4: Prompt for agent-driven exploration phase.

<!-- Meanless: 13 -->


<!-- Media -->

## Prompt for reflection

I will give you screenshots of a mobile app before and after <action> the UI element labeled with the number '<ui_element>' on the first screenshot. The numeric tag of each element is located at the center of the element. The action of <action> this UI element was described as follows: <last_act> The action was also an attempt to proceed with a larger task, which is to <task_desc>.

Your job is to carefully analyze the difference between the two screenshots to determine if the action is in accord with the description above and at the same time effectively moved the task forward. Your output should be determined based on the following situations:

[Detailed requirements for each situation, including back, ineffective, continue, success]

## #Example for output format:

1. Decision: BACK

Thought: <explain why you think the last action is wrong and you should go back to the previous interface>

Documentation: <describe the function of the UI element>

2. Decision: INEFFECTIVE

Thought: <explain why you made this decision>

3. Decision: CONTINUE

Thought: <explain why you think the action does not reflect the action description above and did not move the given

task forward>

Documentation: <describe the function of the UI element>

4. Decision: SUCCESS

Thought: <explain why you think the action successfully moved the task forward>

Documentation: <describe the function of the UI element>

Figure 5: Prompt for reflection phase.

<!-- Media -->

<!-- Meanless: 14 -->


## Prompt for action space

## #A prompt example for action space, take 'tap button' as an example.

I will give you the screenshot of a mobile app before and after tapping the UI element labeled with the button <ui_element> on the screen. The numeric tag of each element is located at the center of the element.

Tapping this UI element is a necessary part of proceeding with a larger task, which is to <task_desc>.

Your task is to describe the functionality of the UI element concisely in one or two sentences. Notice that your description of the UI element should focus on the general function. For example, if the UI element is used to navigate to the chat window with John, your description should not include the name of the specific person. Just say: "Tapping this area will navigate the user to the chat window".

Never include the numeric tag of the UI element in your description. You can use pronouns such as "the UI element" to refer to the element.

Figure 6: Prompt for action space.

<!-- Meanless: 15 -->


## Prompt for task_template

You are an agent trained to perform basic tasks on a smartphone. When given a smartphone screenshot along with reference documents, your primary directive is to derive actionable insights from the documentation provided. These documents are essential for understanding the functionalities of UI elements that may not be immediately apparent from the screenshot alone. Your actions should be primarily informed by these documents, with the current UI interface analysis serving as a secondary reference.

Your decision-making process should prioritize actions as follows:

[Special requirements of decision-making.]

You can call the following functions to control the smartphone:

[Detailed action space and examples, including tapbutton, text, etc.]

## <ui_document>

The task you need to complete is to <task_description>. Your past actions to proceed with this task are summarized as

follows: <last_act>

Now, given the documentation and the following labeled screenshot, you need to think and call the function needed to

proceed with the task. Your output should include three parts in the given format:

Observation: <Describe what you observe in the image>

Thought: <To complete the given task, what is the next step I should do>

Action: <The function call with the correct parameters to proceed with the task. If you believe the task is completed or there is nothing to be done, you should output FINISH. You cannot output anything else except a function call or FINISH in this field.>

Summary: <Summarize your past actions along with your latest action in one or two sentences. Do not include the numeric tag in your summary> You can only take one action at a time, so please directly call the function.

Figure 7: Prompt for development phase.

<!-- Meanless: 16 -->


<!-- Media -->

<!-- figureText: Observation: The image depicts a Google Sign-in screen featuring an input field for entering an email or phone number (tagged with number 2 Additional options visible include learning more about account usage, recovering an email, creating a new account, and a "Next" button (tagger with number 4)<br>8:31さまざま<br>小<br>Google<br>Sign in<br>with your Google Account.<br>Learn more about using your account<br>Thought: To progress with the task of logging into Gmail and checking the first email, the next step involves tapping the email or phone input field to activate the keyboard for entering the email address. Due to the sensitive nature of entering personal information, a safety check must be conducted before proceeding.<br>Email or phone<br>Forgot email?<br>Action: safety_check()<br>Create account<br>Summary: Previously, actions were taken to navigate towards logging into Gmail from the Google Sign-in screen. Currently, a safety check has beer initiated to ensure the secure handling of sensitive information before proceeding further.<br>12:43 (3) (3)<br>TAB<br>Observation: The image shows a login screen for a messenger application. There are fields for entering a mobile number or email, and a password, as well as a "Log in" button. The UI elements are tagged with blue tags numbered 1, 2, 3, 4, 5, 5, 6, 7, and 8.<br>Mobile number or email<br>Thought: To complete the given task of sending messages to John in messenger, I need to log in to the messenger application. However, sinc this involves entering sensitive information such as a username and password, I must perform a safety check before proceeding.<br>Password<br>Login<br>Action: safety_check()<br>Forgot password?<br>Summary: I have initiated a safety check to ensure secure handling of sensitive information before logging into the messenger application to send messages to John.<br>Meta -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_35_29_3ae98b.jpg"/>

Safety Check Reminder: The agent has automatically detected a sensitive operation. The user will now proceed with the operation manually. Once the operation is complete, you can enter finish to transition back to the automated deployment phase.

Figure 8: Safety check examples.

<!-- Meanless: 17 -->


<!-- figureText: Observation: The screenshot shows the Amazon Shopping app interface with a search bar at the top of the screen, which is labeled with the numeric tag '1'. There are also various product recommendations and advertisements displayed on the screen.<br>B53. G D A<br>VAB<br>1203 d d d d<br>中山白<br>G<br>Observation: The screenshot shows the home screen of a smartphone with various application icons. Each application icon has a blue tag with a number in the lower-left corner. The Amazon Shopping app is visible and tagged with the number 2.<br>Q. Search Amazon<br>C 2<br>② Driver to Australia -<br>D<br>回<br>Thought: To complete the given task the next step I should do is to tap on the search bar labeled with the numeric tag '1' to activate the search function and input the term "cap" as per the task requirement.<br>Thought: To complete the given task, which is to search for a cap on Amazon and add it to the cart, I should open the Amazon Shopping app.<br>Deals for you<br>Action: tapbutton(2)<br>20<br>Action: tapbutton(1)<br>Summary: I have opened the Amazon Shopping app to search for a hat and add it to the cart.<br>Summary: I have opened the Amazon Shopping app and tapped on the search bar to search for a hat to add to the cart.<br>HBS dots of<br>中山医<br>980G 9 A<br>中山指<br>오틴g<br>Observation: The screenshot shows the Amazon Shopping app with a search for "caps" already performed...The UI elements are numerically tagged, and the search bar is tagged with the number 2, which is documented as the element to activate the search function.<br>Q. Bearch Amazon<br>Q. caps<br>iphone 15 xs max in All Departme<br>Observation: I have opened the Amazon Shopping app and tapped on the search bar to search for a cap to add to the cart.<br>caps in All Departments<br>ashwagandha capsules<br>尺<br>vitamin e capsules<br>coffee capsules<br>iphone in All Departments<br>laptops in All Departments<br>dad gifts for fathers day<br>father's day gifts for dad<br>grandpa fathers day gift<br>代<br>key caps<br>Thought: To complete the given task, I should select a cap from the search results. "The Letter Cap" is tagged with the number 9, and since it is a cap, it is a suitable choice for the task at hand.<br>fathers day gifts from daughter<br>Thought: To complete the given task I should input the keyword "caps" into the search bar to find caps on Amazon.<br>capsule filling machine<br>尺<br>capsules<br>大<br>capsule coffee machine<br>大<br>capsule machine<br>大<br>fathers day gifts from son<br>fathers day<br>fathers day gifts<br>③ I want to buy ...<br>退<br>'w' e' n ^ ' k ' y ' u ' i ' o '<br>Action: text("caps")<br>Action: tapbutton(9)<br>Summary: I have opened the Amazon Shopping app, tapped on the search bar, and now I have entered the keyword "hat" to search for hats on Amazon.<br>Summary: I have opened the Amazon Shopping app, initiated a search for caps, and now I have selected "The Letter Cap" from the search results by tapping on it.<br>9260GQA<br>9230GQA<br>9日9<br>9日9<br>Observation:The screenshot shows the Amazon Shopping app with customer reviews for a product, presumably "The Letter Cap" that button labeled "Add to Cart" at the bottom of the screen, which is likely the next step in the purchasing process.<br>ORTO<br>Observation: The screenshot shows the Amazon Shopping app with a product page for "ORTC Letter Cap - Classic Baseball Cap with Leather Strap." ...The bottom of the screen indicates a prompt to select a style name, with "Colour" and "Blue" visible, suggesting that style options are available for selection.<br>Q caps<br>回<br>Customer reviews<br>请答复有 4.6 out of 5<br>23 global ratings<br>Top reviews from Australia<br>There are 0 reviews and 0 ratings from Australia<br>ORTC Letter Cap - Classic Baseball Cap Cotton, One Size Fits All - for Men &...<br>食食食食1 23<br>Top reviews from other countries<br>Thought: To complete the given task, the next step should be to add the cap to the shopping cart. This can be done by tapping the "Add to Cart" button<br>-<br>Thought: To complete the given task, the next step I should do is to select a color for the cap. The UI element tagged with the number 27 is associated with the color selection for the cap.<br>A samuel b bell<br>awesome<br>Action: tapbutton(27)<br>Action: tapbutton(11)<br>My new favorite hat, by far,,,wou will not regret thi<br>perfect fit (no 'break-in' time needed). & the detailing is<br>Add to Cart<br>Summary: I have opened the Amazon Shopping app, searched for caps, selected "The Letter Cap" from the search results, viewed the buying options, selected a style, and now I have proceeded to select a color for the cap by tapping on the corresponding UI element.<br>Summary: I have opened the Amazon Shopping app, searched for caps, selected "The Letter Cap" from the search results, viewed the buying options, selected a style, chosen a color, and now I have added the cap to the shopping cart by tapping the "Add to Cart" button. -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_35_29_d1e87a.jpg"/>

Figure 9: Task: search cap in Amazon and add one to cart.

<!-- Meanless: 18 -->


<!-- figureText: Observation: The screenshot shows an alarm clock application with the time currently set to 02:00. There are UI elements for hour and minute selection, AM/PM toggle, and buttons to cancel or confirm the alarm time. The hour section is high lighted, indicating that it is currently selected for adjustment.<br>104d40d<br>VAI<br>SALLS<br>Observation: The screenshot shows an alarm application with no alarms currently set. There is a UI element labeled with the numeric tag '2' which, according to the documentation, allows the user to create a new alarm when tapped.<br>02：00<br>Thought: To complete the given task, the next step I should do is tap the UI element labeled with the numeric tag '2' to create a new alarm.<br>Thought: To complete the given task the next step I should do is switch from hour selection to minute selection by tapping the UI element labeled with the numeric tag '6'.<br>Action: tapbutton(2)<br>Action: tapbutton(6)<br>Summary: I have initiated the process of setting a new alarm by tapping the button to create a new alarm.<br>Summary: I have initiated the process of setting a new alarm and have now switched from hour selection to minute selection.<br>06 d 4 d 4 d<br>106 d 4 d 4<br>12：00<br>Observation: The screenshot shows an alarm clock application with the time selection interface open. The hour is set to 12, ...and the 'OK' button, which is tagged with the number 19. .<br>12：30 ${}^{AM}$<br>Observation: The screenshot shows an alarm clock application with the time selection interface. The hour is set to '12', ...There is also a button is likely used to confirm the time selection.<br>Thought: To complete the given task the next step I should do is to select the '30' minute option to set the alarm time to 12:30 pm.<br>Thought: To complete the given task, the next step I should do is confirm the selected time of 12:30 PM by tapping the "OK" button, which is tagged with '19'.<br>Action: tapbutton(14)<br>Action: tapbutton(19)<br>Summary:I have initiated the process of setting a new alarm, selected the hour, and now I have selected the '30' minute option to set the alarm for 12:30 pm.<br>Summary: I have set the alarm for 12:30 PM and confirmed the time selection by tapping the "OK" button.<br>9日<br>9日5<br>Observation:The screenshot shows an alarm setting interface with various options. The alarm time is set to 12:30 PM and Friday is is set to repeat every Friday. There is a toggle labeled "Vibrate" wi th a numeric tag '17' next to it, which is currently enabled.<br>107d4d4d<br>Observation: The screenshot shows an alarm setting interface with various options. The time is set to 12:30 PM, ...There are also options to delete the alarm and a button to create a new alarm. The days of the week are displayed, with Friday tagged as '12'.<br>：07 古 古 古 古<br>Alarm<br>D. Adalphen<br>12:30ml<br>13 Posselarm<br>D. Additional<br>12:30<br>13 Schedule starm<br>Qualification<br>Quantities<br><br><br>of Google Assistant Routine<br>Thought: To complete the given task, the next step I should do is to set the alarm to repeat every Friday. This can be done by selecting Friday from the days of the week options.<br>Thought: To complete the given task, the next step I should do is disable the vibration for the alarm as per the task requirements.<br>回 Detete<br>Action: tapbutton(17)<br>Action: tapbutton(12)<br>Summary: I have set the alarm for 12:30 PM, confirmed the time selection, and now I have selected Friday to set the alarm to repeat every Friday.<br>Summary: I have set an alarm for 12:30 PM to repeat every Friday and have now disabled the vibration for this alarm. -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_35_29_465149.jpg"/>

Figure 10: Task:set an alarm at 12:30 pm every Friday, disable the vibration.

<!-- Media -->

<!-- Meanless: 19 -->