

<!-- Meanless: IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. MAY, 2025 1 indows Port. Port Discoming Res-->

# SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving

Muleilan Pei, Jiayao Shan, Peiliang Li, Jieqi Shi, Jing Huo, Yang Gao, and Shaojie Shen

Abstract-Online scene perception and topology reasoning are critical for autonomous vehicles to understand their driving environments, particularly for mapless driving systems that endeavor to reduce reliance on costly High-Definition (HD) maps. However, recent advances in online scene understanding still face limitations, especially in long-range or occluded scenarios, due to the inherent constraints of onboard sensors. To address this challenge, we propose a Standard-Definition (SD) Map Enhanced scene Perception and Topology reasoning (SEPT) framework, which explores how to effectively incorporate the SD map as prior knowledge into existing perception and reasoning pipelines. Specifically, we introduce a novel hybrid feature fusion strategy that combines SD maps with Bird's-Eye-View (BEV) features, considering both rasterized and vectorized representations, while mitigating potential misalignment between SD maps and BEV feature spaces. Additionally, we leverage the SD map characteristics to design an auxiliary intersection-aware keypoint detection task, which further enhances the overall scene understanding performance. Experimental results on the large-scale OpenLane-V2 dataset demonstrate that by effectively integrating SD map priors, our framework significantly improves both scene perception and topology reasoning, outperforming existing methods by a substantial margin.

Index Terms-Computer vision for transportation, deep learning for visual perception, intelligent transportation systems.

## I. INTRODUCTION

SCENE understanding is essential for autonomous vehicles, facilitating critical downstream tasks such as accurate motion prediction and decision-making. High-Definition (HD) maps play a pivotal role in this process, providing rich geometric and semantic information, as well as topology relationships. However, HD maps present significant challenges, including high annotation costs, scalability limitations, and ongoing maintenance demands [1], which underscore the increasing need for online scene perception and topology reasoning [2].

In recent years, vision-centric mapless driving approaches (i.e., driving without HD maps) have made significant strides [3], [4], especially within advanced driver assistance systems. These methods aim to reduce the heavy reliance on HD maps by leveraging onboard sensors to perceive the complex scene structure of driving environments in real time. Specifically, with multi-view images as input, a variety of tasks need to be addressed, including lane segment detection, traffic element recognition, and scene topology reasoning [5], [6].

<!-- Media -->

<!-- figureText: Multi-View Images Ours GT w/ SD Map Baseline -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_43_34_2185fe.jpg"/>

Fig. 1. Illustration of the SD map prior for enhancing online scene understanding in long-distance and occlusion scenarios. In this example, the front view is severely obstructed by a bus at the intersection (highlighted in the red box in the front-view image), and the left-back zone is distant. The baseline (LaneSegNet [6]) fails to correctly perceive the road structure (indicated by the red boxes in the top and bottom of the middle visualization), whereas our SEPT framework accurately predicts the road layout with the augmentation of the SD map prior. The Ground Truth (GT) of lane segments is shown in the left figure, with the green line representing the SD map.

<!-- Media -->

Nevertheless, due to the inherent limitations of onboard sensors, such as constrained perception range and restricted field of view, fully mapless driving systems often struggle to accurately reconstruct far-seeing or occluded road conditions. Given that human drivers typically perceive the surrounding scenarios by combining observations with navigation maps, also known as Standard-Definition (SD) maps [7], integrating SD maps as additional prior knowledge of road structures offers a promising solution to complement onboard sensory inputs. In general, the SD map provides a centerline skeleton of road networks without detailed and high-precision annotations [8], making it lightweight, scalable, easily accessible, and low-cost [9]. This basic geographic and road-level topological information can effectively augment online sensing capabilities, thereby enhancing scene perception and topology reasoning, particularly in long-distance or occlusion scenarios, as demonstrated in Fig. 1.

---

<!-- Footnote -->

This work was supported in part by the Hong Kong PhD Fellowship Scheme, and in part by the HKUST-DJI Joint Innovation Laboratory. (Corresponding author: Jieqi Shi.)

Muleilan Pei and Shaojie Shen are with the Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China (email: mpei@ust.hk; eeshaojie@ust.hk).

Jiayao Shan and Peiliang Li are with Zhuoyu Technology, Shenzhen, China (email: jiayao.shan@zyt.com; peiliang.li@zyt.com)

Jieqi Shi, Jing Huo, and Yang Gao are with State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China (email: isjieqi@nju.edu.cn; huojing@nju.edu.cn; gaoy@nju.edu.cn).

<!-- Footnote -->

---




<!-- Meanless: 2 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. MAY, 2025-->

Despite the substantial potential benefits of SD maps, the effective integration of such map priors into current online perception and reasoning paradigms remains an ongoing challenge. Existing approaches typically rely on relatively simple encoding strategies to represent SD maps, either in a rasterized [8] or vectorized [10] format. Each representation has distinct advantages: dense rasterization preserves spatial positional information and fine-grained local details, while sparse vectorization captures complex geometry and topology more efficiently. However, most methods either focus on one representation or combine the two in a simplistic manner [11], which limits effective feature extraction and results in suboptimal utilization or information loss from the SD map. To address this gap, we encode the SD map using a hybrid representation and propose a lightweight yet effective fusion module to augment the Bird's-Eye-View (BEV) features with SD map priors. Additionally, inherent inaccuracies in GPS signals often cause weak spatial misalignment between the SD map and BEV space [12]. While previous works tend to neglect this artifact or dismiss it as noise, we introduce a feature alignment mechanism to resolve this issue. Specifically, for rasterization, we design a feature transformation network that dynamically modulates the features through predicting a scaling factor and bias term for each feature channel; for vectorization, we adopt a cross-attention mechanism [13] that adaptively attends to corresponding features, ensuring better alignment with the BEV feature space.

Moreover, existing approaches overlook the importance of topological road structures in driving scenes. For example, intersections, including cross, merge, or diverge nodes, serve as critical topological attributes that signify changes in road networks. Such keypoints can be effectively identified from SD map priors, which provide valuable characteristics about road structures. To leverage this information, we introduce an auxiliary task focused on recognizing the distribution of road intersections derived from SD maps. This task enables BEV features to capture crucial road topology, thereby enhancing overall driving scene understanding.

In summary, the primary contributions of this letter are as follows: (1) We propose a novel hybrid fusion strategy for SD maps that combines both rasterized and vectorized representations, ensuring effective alignment with BEV features for improved synergy. (2) We introduce an auxiliary Intersection-aware KeyPoint Detection (IKPD) task conditioned on the SD map prior, further enhancing scene understanding capabilities. (3) Extensive experiments on the large-scale OpenLane-V2 dataset demonstrate that our SD map-enhanced framework, termed SEPT, significantly improves both scene perception and topology reasoning performance.

## II. RELATED WORK

## A. Online Scene Perception

Online HD map construction relies on the accurate perception of scene elements. Pioneering efforts have focused on laneline detection [14], [15] to capture road geometry, or centerline perception [2], [16] to recognize lane connectivity. Given the intertwined nature of these two representations, a comprehensive mapping format, lane segment [6], has been proposed to seamlessly integrate both geometric 3D lanelines and topological 3D lane centerlines, along with areas defined by road boundaries and pedestrian crossings. Additionally, traffic element recognition has also been extensively explored in the literature [17], [18] for driving scene understanding, including the detection of traffic lights, road signs, and their associated semantic attributes. Despite advances in detecting these map elements, current online scene perception systems still struggle with occlusions and long-range scenarios. To address these limitations, our work leverages SD map priors, serving as essential complementary prompts with the potential to improve performance in these challenging conditions.

## B. Scene Topology Reasoning

Scene topology information is significant for downstream trajectory prediction [19] and behavior planning [20] tasks, as it provides the topological relationships among lanes and between lanes and traffic elements. Nevertheless, research on topology reasoning has been limited until the emergence of the OpenLane-V2 benchmark [5], which utilizes adjacency matrices to characterize topological connectivity. Most existing methods rely on Multi-Layer Perceptrons (MLPs) [21] or Graph Neural Networks (GNNs) [2] to learn these connection relationships, or incorporate spatial position encoding [22] to enhance reasoning capabilities. These methods, however, are prone to disruption by endpoint shift issues. To address this, the calculation of geometric distance and semantic similarity [23] has been proposed to mitigate such effects. Moreover, since SD maps inherently contain the topological structure of driving scenes, recent works [24] have explored leveraging this prior knowledge to further improve topology reasoning.

### C.SD Map Prior for Autonomous Driving

SD maps, such as Google Maps, are widely used for urban navigation and have recently garnered increasing attention in autonomous driving tasks. Previous studies have primarily concentrated on leveraging SD map priors to enhance online map construction, particularly in long-range scenarios [25]. These methods typically involve rasterizing SD maps [7] and employing Convolutional Neural Networks (CNNs) to extract features. However, the intrinsic weak alignment between SD maps and BEV features remains a challenge [1], leading to the adoption of attention mechanisms [8]. Recent advances in topology reasoning have also incorporated SD maps by vectorizing them into polylines and using Transformer [10] or GNN [9] architectures to improve online lane topology understanding. To fully exploit both representations, a concurrent approach [11] combines these two distinct streams; however, its fusion strategy remains overly simplistic, limiting effectiveness. Considering the existing constraints in SD map utilization, our work further explores their potential by developing a powerful hybrid fusion module and introducing an auxiliary intersection-aware keypoint forecasting task.


<!-- Meanless: PEI et al.: SEPT: STANDARD-DEFINITION MAP ENHANCED SCENE PERCEPTION AND TOPOLOGY REASONING FOR AUTONOMOUS DRIVING 3-->

<!-- Media -->

<!-- figureText: PV2BEV BEV Constructor Decoder LaneSeg Area Cross HHDG Topology Keypoint Attention ${\mathcal{F}}_{B}^{ \vee  }$ Feature Transform ${\mathcal{F}}_{B}^{\mathrm{R}}$ Perception Result Encoder ${\mathcal{F}}_{B}$ Vector Multi-View Images Encoder ${\mathcal{F}}_{SD}^{\mathrm{V}}$ Raster Encoder SD Map -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_43_34_abb52e.jpg"/>

Fig. 2. Overview of the SEPT architecture, demonstrating how it enhances the existing perception and reasoning model for online scene understanding through the integration of the SD map prior.

<!-- Media -->

## III. METHODOLOGY

## A. Task Statement

The online driving scene understanding task involves both scene perception and topology reasoning, using multi-view images and the corresponding SD map priors as inputs. Scene perception includes detecting lane segments, drivable areas, and traffic elements. To be specific, lane segments comprise directed lane centerlines, left and right lane boundaries, and their associated line types (e.g., non-visible, solid, dashed). Drivable areas are represented by undirected curves or closed polygons corresponding to road boundaries and pedestrian crossings. Traffic elements encompass traffic lights and road signs visible in the front view, together with their relevant attributes. For topology reasoning, the goal is to infer the topological relationships among lane segments and between lane segments and traffic elements. This topological information is typically modeled as a lane graph, where nodes represent lane segments or traffic elements, and edges signify connectivity relationships. An adjacency matrix is employed to characterize the lane graph.

## B. Framework Overview

The overall pipeline of our SEPT framework is illustrated in Fig. 2, which improves the baseline model by incorporating SD map priors. Specifically, given multi-view images, the PV2BEV encoder first extracts visual information via the image backbone and then transforms the Perspective-View (PV) features into the BEV feature,denoted as ${\mathcal{F}}_{B}$ ,by view transformation. Additionally, the SD map prior is encoded in two distinct formats: rasterized features ${\mathcal{F}}_{SD}^{\mathrm{R}}$ and vectorized features ${\mathcal{F}}_{SD}^{\mathrm{V}}$ ,through a hybrid SD map encoding approach. These two representations are then leveraged to augment the BEV feature through a Feature Transformation (FT) module and a cross-attention network, respectively, producing the enhanced BEV features ${\mathcal{F}}_{B}^{\mathrm{R}}$ and ${\mathcal{F}}_{B}^{\mathrm{V}}$ . A lightweight yet effective Dual Gated Feature Fusion (DGFF) module is employed to fuse these two augmented features, generating the final enhanced BEV feature ${\mathcal{F}}_{B}^{\mathrm{{SD}}}$ . This feature is consequently decoded to address various subtasks by different heads, such as the lane segment head, area head, topology head, etc. Notably, we also introduce an additional keypoint head for an auxiliary task, which detects road intersections from SD maps, further enhancing scene understanding capabilities.

## C. Hybrid SD Map Encoding and Fusion

To fully leverage SD map priors, we introduce a hybrid encoding approach, utilizing both rasterized and vectorized formats. These two representations are incorporated to enhance the BEV feature while ensuring implicit alignment between them. In addition, we design an efficient and effective fusion strategy to seamlessly integrate these features, thereby improving overall performance. Herein, let the BEV feature be represented as ${\mathcal{F}}_{B} \in  {\mathbb{R}}^{H \times  W \times  C}$ ,where $H$ and $W$ correspond to the spatial dimensions of the BEV perception range,and $C$ denotes the feature dimension.

1) Vectorized SD Map Encoding: Given raw polylines of SD maps, we begin by uniformly resampling these sequences to obtain $M$ segments. For each segment,we further evenly sample a fixed number of points. Following the structure of the classical vectorized method, SMERF [10], we then vectorize the SD map and extract the initial vectorized feature ${\mathcal{F}}_{SD}^{\mathrm{V}} \in  {\mathbb{R}}^{M \times  C}$ using a Transformer-based encoder model. In this paradigm, spatial misalignment between the SD map tokens and the BEV space can be mitigated through a multihead cross-attention mechanism. Here, the BEV feature acts as query tokens, while the SD map tokens serve as keys and values. This enables the BEV queries to adaptively aggregate relevant SD map tokens conditioned on a learnable attention distribution. As a result, we obtain implicitly aligned BEV features ${\mathcal{F}}_{B}^{\mathrm{V}} \in  {\mathbb{R}}^{H \times  W \times  C}$ ,complemented by the vectorized SD map priors.

2) Rasterized SD Map Encoding: We first rasterize the SD map into an $H \times  W$ canvas with a binary representation, where each grid cell is assigned a value of 1 if occupied by a polyline, and 0 otherwise. Different road types, such as crosswalks and sidewalks, are encoded as separate channels. The original SD map features are then extracted using CNNs, yielding the rasterized feature ${\mathcal{F}}_{SD}^{\mathrm{R}} \in  {\mathbb{R}}^{H \times  W \times  C}$ . Note that this feature may be weakly misaligned with the BEV space. To address this, motivated by the T-Net in PointNet [26], we introduce a Feature Transformation (FT) module to align ${\mathcal{F}}_{SD}^{\mathrm{R}}$ with ${\mathcal{F}}_{B}^{\mathrm{V}}$ at the feature level. Specifically,we first project both features along the channel dimension and compute their feature difference ${\mathcal{F}}_{\Delta } \in  {\mathbb{R}}^{H \times  W \times  C}$ ,which represents a form of calibration error. We then apply a max-pooling operation on ${\mathcal{F}}_{\Delta }$ to obtain the global context vector ${\mathcal{F}}_{\Delta }^{\text{Global }} \in  {\mathbb{R}}^{C}$ . Next, we leverage Feature-wise Linear Modulation (FiLM) [27] to predict the scaling factor $\gamma  \in  {\mathbb{R}}^{C}$ and the bias term $\beta  \in  {\mathbb{R}}^{C}$ for each feature channel. Finally, these transformation parameters are applied to the rasterized feature ${\mathcal{F}}_{SD}^{\mathrm{R}}$ ,resulting in the enhanced BEV feature ${\mathcal{F}}_{B}^{\mathrm{R}} \in  {\mathbb{R}}^{H \times  W \times  C}$ ,with implicit spatial alignment, as follows:


<!-- Meanless: 4 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. MAY, 2025-->

$$
{\mathcal{F}}_{B}^{\mathrm{R}} = \gamma  \odot  {\mathcal{F}}_{SD}^{\mathrm{R}} + \beta , \tag{1}
$$

<!-- Media -->

<!-- figureText: ${\mathcal{F}}_{B}^{ \vee  }$ FFN ${\mathcal{F}}_{B}^{\mathrm{{SD}}}$ $\sigma$ ${\mathcal{F}}_{B}^{\mathrm{R}}$ -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_43_34_e045f9.jpg"/>

Fig. 3. The hybrid feature fusion process of the DGFF module.

<!-- Media -->

where $\odot$ denotes the Hadamard (element-wise) product,and all operations follow the broadcasting mechanism.

3) Dual Gated Feature Fusion: After obtaining the two BEV features augmented by rasterized and vectorized SD map features, it is essential to design an effective fusion strategy to combine these distinct features, as the characteristics of the two branches may differ significantly. To this end, we propose a lightweight yet powerful fusion network called the Dual Gated Feature Fusion (DGFF) module, which leverages the gated attention mechanism to aggregate the dual-branch features. As depicted in Fig. 3,the two features ${\mathcal{F}}_{B}^{\mathrm{R}}$ and ${\mathcal{F}}_{B}^{\mathrm{V}}$ are first concatenated along the feature dimension and passed through a feed-forward network to produce a fused feature ${\mathcal{F}}_{B}^{\mathrm{R} \boxplus  \mathrm{V}} \in  {\mathbb{R}}^{H \times  W \times  C}$ ,as follows:

$$
{\mathcal{F}}_{B}^{\mathrm{R} \boxplus  \mathrm{V}} = \mathrm{{FFN}}\left( {{\mathcal{F}}_{B}^{\mathrm{R}} \boxplus  {\mathcal{F}}_{B}^{\mathrm{V}}}\right) , \tag{2}
$$

where $\boxplus$ denotes concatenation along the feature dimension, and $\operatorname{FFN}\left( \cdot \right)$ represents the feed-forward network. Next,an element-wise gating mechanism is performed on each input stream using the sigmoid function, enabling the model to adaptively weight the contributions of rasterized and vectorized features. This is expressed as:

$$
{w}_{\mathrm{R}} = \sigma \left( {\mathcal{F}}_{B}^{\mathrm{R}}\right) ,\;{w}_{\mathrm{V}} = \sigma \left( {\mathcal{F}}_{B}^{\mathrm{V}}\right) , \tag{3}
$$

where $\sigma \left( \cdot \right)$ is the element-wise sigmoid function,producing attention weights for each input stream. Although simpler, the gating mechanism introduces nonlinearity and adaptability, allowing the model to capture richer feature interactions without increasing the number of learnable parameters. This strikes a balance between representational capacity and efficiency. Finally, two parallel projection networks refine each gated feature before merging them via a weighted averaging operation, generating the final enhanced BEV feature ${\mathcal{F}}_{B}^{\mathrm{{SD}}} \in  {\mathbb{R}}^{H \times  W \times  C}$ . This process can be formulated by the following expression:

$$
{\mathcal{F}}_{B}^{\mathrm{{SD}}} = \mu  \cdot  {\operatorname{Proj}}_{\mathrm{R}}\left( {{w}_{\mathrm{R}} \odot  {\mathcal{F}}_{B}^{\mathrm{R} \boxplus  \mathrm{V}}}\right)  + \nu  \cdot  {\operatorname{Proj}}_{\mathrm{V}}\left( {{w}_{\mathrm{V}} \odot  {\mathcal{F}}_{B}^{\mathrm{R} \boxplus  \mathrm{V}}}\right) , \tag{4}
$$

where ${\operatorname{Proj}}_{\mathrm{R}}$ and ${\operatorname{Proj}}_{\mathrm{V}}$ are the parallel projection networks. $\mu$ and $\nu$ are hyperparameters for balancing each term.

Overall, with the support of the DGFF module, our hybrid SD map encoding and fusion strategy can adaptively fuse heterogeneous feature representations, empowering the model to emphasize the most informative components from each branch. This substantially boosts the representation capabilities of both rasterized and vectorized SD map priors, while maintaining efficiency and expressiveness.

## D. Intersection-Aware Keypoint Detection

To further enhance the BEV feature representation and improve understanding of road topology and geometry, we introduce an Intersection-Aware Keypoint Detection (IKPD) task. This auxiliary task helps the model capture road interaction patterns by detecting the road interaction distribution.

1) Intersection Generation: The first step in implementing the IKPD task involves identifying the intersection points of roads from SD maps, which will serve as the ground truth for supervision. Since the SD map prior provides the essential skeleton of road networks, intersection locations (e.g., merging, diverging, and crossing points) can be easily extracted. However, intersection points are typically sparsely distributed across the scene, and directly using these points as supervision can lead to class imbalance during training. Additionally, due to intrinsic positional biases relative to the ground-truth HD maps in certain scenarios, the intersection points derived from the SD map may not perfectly align with the finer details of the road structure. To mitigate this issue, we represent the keypoints as Gaussian distributions, similar to the approach used in confidence-based keypoint detection [28], [29]. Specifically, for each scene, we construct a heatmap $\mathcal{H} \in  {\mathbb{R}}^{H \times  W \times  1}$ to model the ground-truth distribution of road intersections. Each intersection is represented as a Gaussian distribution centered at its location, with a certain radius reflecting the spatial uncertainty.

2) IKPD Head: Given the augmented BEV feature ${\mathcal{F}}_{B}^{\mathrm{{SD}}}$ , we aim to design a lightweight network capable of effectively decoding the road intersection heatmap, thereby enriching the BEV feature with crucial geometric and topological information about the road structure. The IKPD head follows an efficient design paradigm that emphasizes both local feature extraction and global context reasoning. Specifically, the BEV feature is first passed through a series of CNN blocks with residual connections. Each residual block comprises depthwise separable convolutions (i.e., a depthwise convolution followed by a pointwise convolution) [30], which decouple spatial and channel-wise operations for computational efficiency. Dilated convolutions are also incorporated for capturing broader spatial context information. After each convolution, the output feature is refined with the Squeeze-and-Excitation (SE) [31] attention, which recalibrates the channel-wise features by computing global statistics and adaptively weighting the importance of each channel. This allows the IKPD head to prioritize the most relevant features for keypoint detection, improving its ability to focus on critical patterns. Consequently, the final output is produced through a $1 \times  1$ convolution followed by a sigmoid activation function, generating a heatmap that represents the distribution of road intersections.

## E. Training Objectives

Following the baseline approaches [2], [6], the supervision is applied to each head to optimize distinct training objectives, including detection losses for lane segments, areas, and traffic elements,denoted as ${\mathcal{L}}_{\mathrm{{DET}}}$ ,and topology reasoning losses, denoted as ${\mathcal{L}}_{\text{TOP }}$ . Our proposed SEPT framework does not modify the baseline loss functions but introduces an additional loss term for the auxiliary IKPD head,denoted as ${\mathcal{L}}_{\text{IKPD }}$ . Given the road intersection distribution is sparse and imbalanced, we employ focal loss [28] to supervise the keypoint heatmap training. The overall loss $\mathcal{L}$ for SEPT is formulated as:


<!-- Meanless: PEI et al.: SEPT: STANDARD-DEFINITION MAP ENHANCED SCENE PERCEPTION AND TOPOLOGY REASONING FOR AUTONOMOUS DRIVING 5-->

$$
\mathcal{L} = {\mathcal{L}}_{\mathrm{{DET}}} + {\mathcal{L}}_{\mathrm{{TOP}}} + {\mathcal{L}}_{\mathrm{{IKPD}}}. \tag{5}
$$

<!-- Media -->

TABLE I

Quantitative results on the OLV2 validation split, benchmarked using $\mathtt{{OLS}}$ . All metrics follow the higher-the-better criterion. The official ranking metric is shaded in gray, and the best results are indicated in bold. A "-" denotes the absence of relevant DATA.

<table><tr><td rowspan="2">Method</td><td rowspan="2">${\mathrm{{DET}}}_{l} \uparrow$</td><td rowspan="2">${\mathrm{{DET}}}_{t} \uparrow$</td><td colspan="3">v1.0</td><td colspan="3">v1.1</td><td rowspan="2">Params</td></tr><tr><td>${\mathrm{{TOP}}}_{ll} \uparrow$</td><td>${\operatorname{TOP}}_{lt} \uparrow$</td><td>OLS↑</td><td>${\mathrm{{TOP}}}_{ll} \uparrow$</td><td>${\operatorname{TOP}}_{lt} \uparrow$</td><td>OLS↑</td></tr><tr><td>TopoNet [2]</td><td>28.6</td><td>48.6</td><td>4.1</td><td>20.3</td><td>35.6</td><td>10.9</td><td>23.8</td><td>39.8</td><td>62.6M</td></tr><tr><td>w/ OLV2 [9]</td><td>27.9</td><td>48.1</td><td>5.1</td><td>20.9</td><td>36.1</td><td>-</td><td>-</td><td>-</td><td>75.9M</td></tr><tr><td>w/ OSMG [9]</td><td>30.0</td><td>47.6</td><td>5.4</td><td>21.3</td><td>36.7</td><td>-</td><td>-</td><td>-</td><td>64.6M</td></tr><tr><td>w/ OSMR [9]</td><td>30.6</td><td>44.6</td><td>7.7</td><td>22.9</td><td>37.7</td><td>-</td><td>-</td><td>-</td><td>75.9M</td></tr><tr><td>w/ SMERF [10]</td><td>33.4</td><td>48.6</td><td>7.5</td><td>23.4</td><td>39.4</td><td>15.4</td><td>25.4</td><td>42.9</td><td>65.8M</td></tr><tr><td>w/ SEPT (Ours)</td><td>34.2 (+5.6)</td><td>49.8 (+1.2)</td><td>8.3 (+4.2)</td><td>23.8 (+3.5)</td><td>40.4 (+4.8)</td><td>19.5 (+8.6)</td><td>27.5 (+3.7)</td><td>45.2 (+5.4)</td><td>70.4M</td></tr><tr><td>TopoLogic [23]</td><td>29.2</td><td>46.5</td><td>18.0</td><td>20.6</td><td>40.9</td><td>23.6</td><td>24.2</td><td>43.4</td><td>61.8M</td></tr><tr><td>w/ SMERF [10]</td><td>31.0</td><td>48.7</td><td>21.2</td><td>22.4</td><td>43.3</td><td>26.9</td><td>26.2</td><td>45.7</td><td>65.1M</td></tr><tr><td>w/ SEPT (Ours)</td><td>34.3 (+5.1)</td><td>48.9 (+2.4)</td><td>25.1 (+7.1)</td><td>25.1 (+4.5)</td><td>45.8 (+4.9)</td><td>31.2 (+7.6)</td><td>29.7 (+5.5)</td><td>48.4 (+5.0)</td><td>69.6M</td></tr></table>

<!-- Media -->

## IV. EXPERIMENTS AND RESULTS

## A. Experiment Setups

1) Real-World Dataset: We train and evaluate our proposed approach on the large-scale OpenLane-V2 (OLV2) dataset [5], which, to the best of our knowledge, is currently the only benchmark for both scene perception and topology reasoning in autonomous driving. All experiments in our work are conducted on the primary subset of OLV2, subset_A, built upon the Argoverse 2 [32] dataset with additional annotations for lane segments, traffic elements, and lane topology, etc. The subset $A$ comprises $1\mathrm{\;k}$ scenes collected from six cities, with $2\mathrm{{Hz}}$ multi-view images and optional SD map information (including three categories: roads, crosswalks, and sidewalks) extracted from the OpenStreetMap (OSM) [33]. The training set consists of approximately ${27}\mathrm{\;k}$ frames,while the validation set contains around ${4.8}\mathrm{k}$ frames.

2) Evaluation Metrics: We evaluate the performance of perception and reasoning using the official metrics provided by OLV2 [5]. There are two primary benchmark categories, each with distinct evaluation metrics: OLV2 Score (OLS) and OLV2 UniScore (OLUS). Both scores are averages derived from various metrics across different subtasks. The main distinction between them is that OLS focuses exclusively on lane centerline perception, while OLUS emphasizes lane segment perception. Specifically,OLS includes four sub-metrics: ${\mathrm{{DET}}}_{l}$ , ${\mathrm{{DET}}}_{t},{\mathrm{{TOP}}}_{ll}$ ,and ${\mathrm{{TOP}}}_{lt}$ . ${\mathrm{{DET}}}_{l}$ measures the mean average precision (mAP) for lane centerline detection, based on the Fréchet distance with match thresholds of 1.0, 2.0, and 3.0. ${\mathrm{{DET}}}_{t}$ represents the mAP for traffic element recognition, conditioned on the average Intersection over Union (IoU) with a match threshold of 0.75 across various traffic attributes. ${\mathrm{{TOP}}}_{ll}$ and ${\mathrm{{TOP}}}_{lt}$ measure mAP for topology among lane centerlines and between lane centerlines and traffic elements, using the adjacency matrix. Notably, there are two versions for calculating TOP scores: V1.0 and V1.1, with the V1.0 calculation containing a potential loophole issue [22]. The OLS score is computed as the average of these four metrics, given by:

$$
\mathrm{{OLS}} = \frac{1}{4}\left\lbrack  {{\mathrm{{DET}}}_{l} + {\mathrm{{DET}}}_{t} + f\left( {\mathrm{{TOP}}}_{ll}\right)  + f\left( {\mathrm{{TOP}}}_{lt}\right) }\right\rbrack  , \tag{6}
$$

where $f$ represents the square root function.

In contrast,OLUS encompasses five sub-metrics: ${\mathrm{{DET}}}_{ls}$ , ${\mathrm{{DET}}}_{a},{\mathrm{{DET}}}_{te},{\mathrm{{TOP}}}_{lsls}$ ,and ${\mathrm{{TOP}}}_{lste}$ ,covering detection for lane segments, areas, and traffic elements, as well as topology reasoning among lane segments and between lane segments and traffic elements. These metrics follow a similar calculation procedure to OLS,with the addition of ${\mathrm{{DET}}}_{a}$ ,which is measured using Chamfer distance.

3) Implementation Details: We select two representative high-performance models as baselines: TopoNet [2] for OLS and LaneSegNet [6] for OLUS. To ensure a fair comparison, we retain the official implementations of both baseline models, incorporating only the modules designed specifically for the SD map prior into the codebase. All models employ the default ResNet-50 backbone,with BEV feature dimensions set to $H =$ 200 and $W = {100}$ . We train our model on eight GPUs with a total batch size of 8 . The training configuration, including the learning rate and optimizer, remains consistent with baseline settings, and all models share the same hyperparameters.

## B. Comparison with State-of-the-Art

We compare our SEPT framework with other state-of-the-art methods that incorporate SD maps as input on the OLV2 benchmark, using the OLS evaluation metric. TopoNet [2] serves as the baseline for centerline perception. TopoNet with OLV2, OSMG, and OSMR [9] represent approaches utilizing rasterized SD maps from OLV2, augmented with full OSM attributes (e.g., stop signs, speed limits), and graph-based SD maps with OSM augmentation, respectively. SMERF is a classical method that enhances lane-topology understanding with SD maps in a vectorized representation. We present results for both v1.0 and v1.1 metrics to provide a comprehensive comparison, as shown in the upper group of Tab. I. Since the v1.1 metric is a recent update, the results for TopoNet and SMERF on v1.1 have been reevaluated using their official checkpoints, while others are not available. Compared to the baseline, our method significantly improves performance across all subtasks by effectively integrating SD map priors without introducing excessive parameters. Specifically, we achieve a 4.8 OLS improvement for v1.0 and a 5.4 OLS improvement for $\mathrm{v}{1.1}$ ,with a notable ${8.6}{\mathrm{{TOP}}}_{ll}$ increase in topology reasoning, outperforming other existing methods augmented with SD maps. Moreover, we also apply our SEPT framework to the recent model, TopoLogic [23], which is designed to enhance lane topology reasoning. As shown in the lower group of Tab. I, our SEPT consistently improves performance across all subtasks, achieving a 4.9 OLS gain for v1.0 and a 5.0 OLS gain for v1.1.


<!-- Meanless: 6 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. MAY, 2025-->

<!-- Media -->

TABLE II

Quantitative results on the OLV2 validation split with map element buckets, benethmarked using OLUS.

<table><tr><td>Method</td><td>Raster</td><td>Vector</td><td>IKPD</td><td>${\mathrm{{DET}}}_{ls} \uparrow$</td><td>${\mathrm{{DET}}}_{a} \uparrow$</td><td>${\text{DET}}_{te} \uparrow$</td><td>${\mathrm{{TOP}}}_{lsls} \uparrow$</td><td>${\mathrm{{TOP}}}_{\text{lste }} \uparrow$</td><td>OLUS $\uparrow$</td><td>Params</td></tr><tr><td>LaneSegNet [6]</td><td/><td/><td/><td>30.9</td><td>20.0</td><td>36.7</td><td>25.6</td><td>20.8</td><td>36.7</td><td>61.8M</td></tr><tr><td>w/ Raster Only</td><td>✓</td><td/><td/><td>33.8 (+2.9)</td><td>${28.1}\left( {+{8.1}}\right)$</td><td>${38.1}\left( {+{1.4}}\right)$</td><td>27.5 (+1.9)</td><td>21.8 (+1.0)</td><td>39.9 (+3.2)</td><td>65.5M</td></tr><tr><td>w/ Vector Only</td><td/><td>✓</td><td/><td>35.3 (+4.4)</td><td>${22.3}\left( {+{2.5}}\right)$</td><td>39.2 (+2.3)</td><td>30.2 (+4.6)</td><td>${22.6}\left( {+{1.8}}\right)$</td><td>39.9 (+3.2)</td><td>65.8M</td></tr><tr><td>w/ Hybrid Fusion</td><td>✓</td><td>✓</td><td/><td>35.8 (+4.9)</td><td>${28.2}\left( {+{8.2}}\right)$</td><td>39.6 (+2.9)</td><td>31.0 (+5.4)</td><td>${22.7}\left( {+{1.9}}\right)$</td><td>41.4 (+4.7)</td><td>69.8M</td></tr><tr><td>w/ SEPT</td><td>✓</td><td>✓</td><td>✓</td><td>38.4 (+7.5)</td><td>29.0 (+9.0)</td><td>40.0 (+3.3)</td><td>32.2 (+6.6)</td><td>23.8 (+3.0)</td><td>42.6 (+5.9)</td><td>70.4M</td></tr></table>

TABLE III

EFFECT OF FT MODULE ON RASTERIZED SD MAP ENCODING.

<table><tr><td>FT</td><td>${\mathrm{{DET}}}_{ls}$</td><td>${\mathrm{{DET}}}_{a}$</td><td>${\mathrm{{DET}}}_{te}$</td><td>${\mathrm{{TOP}}}_{lsls}$</td><td>${\mathrm{{TOP}}}_{\text{lste}}$</td><td>OLUS</td></tr><tr><td>✘</td><td>32.2</td><td>24.5</td><td>36.7</td><td>26.7</td><td>21.2</td><td>38.2</td></tr><tr><td>✓</td><td>33.8</td><td>28.1</td><td>38.1</td><td>27.5</td><td>21.8</td><td>39.9</td></tr></table>

<!-- Media -->

In addition, we further evaluate our approach using the latest and more challenging OLUS evaluation metric, which focuses on lane segment perception and provides a more comprehensive assessment of the map element bucket. We directly apply our SEPT framework to LaneSegNet [6], a leading method for driving scene topology. As shown in Tab. II, even without further adaptation, our framework significantly enhances the baseline performance across all five subtasks, resulting in an overall improvement of 5.9 OLUS. This underscores the effectiveness and generalizability of our proposed framework.

## C. Ablation Study

We conduct ablation studies to validate the effectiveness of each proposed component of SEPT as well as FT and DGFF modules using the OLV2 validation split, employing the OLUS evaluation metric for a comprehensive assessment. LaneSegNet [6] serves as the baseline model.

1) Component Study of SEPT: We conduct an in-depth analysis of the contributions of each proposed component, as shown in Tab. II.

Hybrid Fusion. Compared to the baseline, integrating the SD map prior, whether in rasterized or vectorized representation, improves performance across all metrics, with both formats yielding comparable results. Specifically, the rasterized SD map significantly enhances area detection by ${8.1}{\mathrm{{DET}}}_{a}$ ,as rasterization captures more spatial information. In contrast, the vectorized format improves the lane segment detection and topology reasoning between lane segments by ${4.4}{\mathrm{{DET}}}_{ls}$ and ${4.6}{\mathrm{{TOP}}}_{\text{lals }}$ ,respectively,as vectorization better preserves the geometry and topology of the road structure. This highlights that both representations have distinct advantages and should complement each other,underscoring the superiority of employing our hybrid fusion strategy.

<!-- Media -->

TABLE IV

COMPARISON OF DIFFERENT FEATURE FUSION STRATEGIES.

<table><tr><td>Fusion Strategy</td><td>${\mathrm{{DET}}}_{ls}$</td><td>${\mathrm{{DET}}}_{a}$</td><td>${\mathrm{{DET}}}_{te}$</td><td>${\mathrm{{TOP}}}_{lsls}$</td><td>${\mathrm{{TOP}}}_{\text{lste}}$</td><td>OLUS</td></tr><tr><td>Addition</td><td>35.4</td><td>23.9</td><td>36.8</td><td>29.5</td><td>21.7</td><td>39.4</td></tr><tr><td>Concatenation</td><td>35.7</td><td>24.3</td><td>37.7</td><td>29.9</td><td>22.0</td><td>39.9</td></tr><tr><td>Cross-Attention</td><td>35.5</td><td>25.5</td><td>38.3</td><td>30.0</td><td>22.5</td><td>40.3</td></tr><tr><td>DGFF (Ours)</td><td>35.8</td><td>28.2</td><td>39.6</td><td>31.0</td><td>22.7</td><td>41.4</td></tr></table>

TABLE V

COMPARISON OF DIFFERENT FUSION WEIGHTS.

<table><tr><td>Raster</td><td>Vector</td><td>${\mathrm{{DET}}}_{ls}$</td><td>${\mathrm{{DET}}}_{a}$</td><td>${\mathrm{{DET}}}_{te}$</td><td>${\mathrm{{TOP}}}_{lsls}$</td><td>${\text{TOP}}_{lste}$</td><td>OLUS</td></tr><tr><td>0.2</td><td>0.8</td><td>35.5</td><td>27.1</td><td>39.2</td><td>30.1</td><td>22.4</td><td>40.8</td></tr><tr><td>0.5</td><td>0.5</td><td>35.8</td><td>28.2</td><td>39.6</td><td>31.0</td><td>22.7</td><td>41.4</td></tr><tr><td>0.8</td><td>0.2</td><td>35.2</td><td>27.5</td><td>38.9</td><td>30.1</td><td>22.3</td><td>40.7</td></tr></table>

<!-- Media -->

IKPD. As shown in the last two rows of Tab. II, incorporating the auxiliary IKPD task enables our method to fully exploit the potential of the SD map prior, leading to further improvements across all subtasks in scene perception and topology reasoning.

2) Effect of the FT Module: We first investigate the impact of spatial alignment in the rasterized SD map encoding process. In the rasterized SD map-only configuration, we remove the proposed FT module from the pipeline. As shown in Tab. III, the model with the FT module consistently outperforms its counterpart across all metrics. This highlights the importance of aligning the SD map and BEV feature space, with feature-level alignment effectively mitigating associated challenges.

3) Effect of the DGFF Module: We further evaluate the fusion capability of the proposed DGFF module. Given the two BEV features ${\mathcal{F}}_{B}^{\mathrm{R}}$ and ${\mathcal{F}}_{B}^{\mathrm{V}}$ ,enhanced by rasterized and vectorized SD maps, respectively, we replace the DGFF with various fusion strategies, including element-wise addition, concatenation (followed by FFN), and cross-attention. As shown in Tab. IV, simple combinations of these features either hinder overall performance or fail to achieve a synergistic effect. While cross-attention improves performance, it introduces significant computational overhead. In contrast, our DGFF module is both more efficient and effective, utilizing a gated attention mechanism to integrate the two features and deliver superior performance. Additionally, we also explore the impact of different combination weights (i.e., $\mu$ and $\nu$ in Eq. 4) for the gated rasterized and vectorized features. From Tab. V, we observe that a balanced combination of the two features yields the best performance, with the rasterized term slightly outperforming in area detection and the vectorized term excelling in lane segment detection. These results are consistent with the findings discussed in Section IV-C1.


<!-- Meanless: PEI et al.: SEPT: STANDARD-DEFINITION MAP ENHANCED SCENE PERCEPTION AND TOPOLOGY REASONING FOR AUTONOMOUS DRIVING 7-->

<!-- Media -->

<!-- figureText: Multi-View Images GT (Centerline) TopoNet TopoNet + SEPT GT (Lane Segment) LaneSegNet LaneSegNet + SEPT -->

<img src="https://raw.githubusercontent.com/Tsuki-Gor/Pic_Bed_Ob/main/Mixed/M2025/11/2025_11_04__12_43_34_db8750.jpg"/>

Fig. 4. Qualitative comparisons between the baselines with and without our SEPT module on the OLV2 validation split. From left to right, the figure presents the multi-view images, the ground truth (GT) for centerline perception and topology, the results of the baseline (TopoNet) with and without SEPT, the GT for lane segment perception and topology, and the results of the baseline (LaneSegNet) with and without SEPT. The green line indicates the corresponding SD map prior.

<!-- Media -->

## D. Qualitative Results

We present visualizations from the OLV2 validation split to demonstrate the improvements brought by our proposed SEPT framework over two baseline models: TopoNet [2] for lane centerline perception and LaneSegNet [6] for lane segment perception. As illustrated in Fig. 4, we highlight several representative cases. In the first row, we demonstrate that in long-range scenarios, the baseline models either fail to detect or inaccurately predict the left turn (highlighted in purple boxes), whereas our model successfully identifies it. In the second row, due to occlusion from a front vehicle, our approach notably enhances the prediction of the occluded road structure at the intersection. The third row illustrates how effectively incorporating the SD map improves lane topology reasoning. The final row presents an interesting case where the SD map provides outdated information, resulting in incorrect prior knowledge. However, our SEPT framework demonstrates the ability to prioritize online perception, yielding a correct prediction that aligns with current observations. This shows that our model strikes an effective balance between onboard sensing and SD map priors. Overall, our SEPT framework significantly improves both scene perception and topology reasoning. More qualitative results can be found in our supplementary video.


<!-- Meanless: 8 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. MAY, 2025-->

## V. CONCLUSION

In this letter, we present SEPT, a novel framework that integrates SD map priors into existing perception and reasoning models to enhance online scene understanding. SEPT effectively mitigates the misalignment issue in both rasterized and vectorized SD map representations and leverages the DGFF module to fuse these features for synergistic improvement. To further exploit the potential of SD maps, we introduce the auxiliary IKPD task, which enhances the model in capturing road interaction patterns. We apply our SEPT to two baseline methods and validate it on the large-scale OLV2 benchmark. The significant performance gains demonstrate the superiority of our framework. Future work will focus on incorporating additional SD map prior information, such as lane numbers and road directions, to further enhance scene perception and topology reasoning for mapless driving. REFERENCES

[1] J. Li, P. Jia, J. Chen, J. Liu, and L. He, "Local map construction methods with sd map: A novel survey," arXiv preprint arXiv:2409.02415, 2024.

[2] T. Li, L. Chen, H. Wang, Y. Li, J. Yang, X. Geng, S. Jiang, Y. Wang, H. Xu, C. Xu et al., "Graph-based topology reasoning for driving scenes," arXiv preprint arXiv:2304.05277, 2023.

[3] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang et al., "Planning-oriented autonomous driving," in CVPR, 2023, pp. 17853-17862.

[4] B. Jiang, S. Chen, Q. Xu, B. Liao, J. Chen, H. Zhou, Q. Zhang, W. Liu, C. Huang, and X. Wang, "Vad: Vectorized scene representation for efficient autonomous driving," in Proc. of the IEEE Intl. Conf. Comput. Vis. (ICCV), 2023, pp. 8340-8350.

[5] H. Wang, T. Li, Y. Li, L. Chen, C. Sima, Z. Liu, B. Wang, P. Jia, Y. Wang, S. Jiang et al., "Openlane-v2: A topology reasoning benchmark for unified 3d hd mapping," Advances in Neural Information Processing Systems, vol. 36, 2024.

[6] T. Li, P. Jia, B. Wang, L. Chen, K. Jiang, J. Yan, and H. Li, "Lanesegnet: Map learning with lane segment perception for autonomous driving," arXiv preprint arXiv:2312.16108, 2023.

[7] T. Ort, J. M. Walls, S. A. Parkison, I. Gilitschenski, and D. Rus, "Maplite 2.0: Online hd map inference using a prior sd map," IEEE Robotics and Automation Letters, vol. 7, no. 3, pp. 8355-8362, 2022.

[8] Z. Jiang, Z. Zhu, P. Li, H.-a. Gao, T. Yuan, Y. Shi, H. Zhao, and H. Zhao, "P-mapnet: Far-seeing map generator enhanced by both sdmap and hdmap priors," IEEE Robotics and Automation Letters, vol. 9, pp. 8539-8546, 2024.

[9] H. Zhang, D. Paz, y. Guo, A. Das, X. Huang, K. Haug, H. Christensen, and L. Ren, "Enhancing online road network perception and reasoning with standard definition maps," in Proc. of the IEEE/RSJ Intl. Conf. on Intell. Robots Syst.(IROS), 2024.

[10] K. Z. Luo, X. Weng, Y. Wang, S. Wu, J. Li, K. Q. Weinberger, Y. Wang, and M. Pavone, "Augmenting lane perception and topology understanding with standard definition navigation maps," in Proc. of the IEEE Intl. Conf. on Robot. and Autom. (ICRA), 2024, pp. 4029-4035.

[11] S. Yang, M. Jiang, Z. Fan, X. Xie, X. Tan, Y. Li, E. Ding, L. Wang, and J. Wang, "Toposd: Topology-enhanced lane segment perception with sdmap prior," arXiv preprint arXiv:2411.14751, 2024.

[12] H. Wu, Z. Zhang, S. Lin, X. Mu, Q. Zhao, M. Yang, and T. Qin, "Maplocnet: Coarse-to-fine feature registration for visual re-localization in navigation maps," in Proc. of the IEEE/RSJ Intl. Conf. on Intell. Robots Syst.(IROS). IEEE, 2024, pp. 13 198-13 205.

[13] A. Vaswani, "Attention is all you need," Advances in Neural Information Processing Systems, 2017.

[14] B. Liao, S. Chen, X. Wang et al., "Maptr: Structured modeling and learning for online vectorized hd map construction," in International Conference on Learning Representations, 2023.

[15] Q. Li, Y. Wang, Y. Wang, and H. Zhao, "Hdmapnet: An online hd map construction and evaluation framework," in Proc. of the IEEE Intl. Conf. on Robot. and Autom. (ICRA), 2022, pp. 4628-4634.

[16] Z. Xu, Y. Liu, Y. Sun, M. Liu, and L. Wang, "Centerlinedet: Centerline graph detection for road lanes with vehicle-mounted sensors by transformer for hd map generation," in Proc. of the IEEE Intl. Conf. on Robot. and Autom. (ICRA), 2023, pp. 3553-3559.

[17] T. Langenberg, T. Lüddecke, and F. Wörgötter, "Deep metadata fusion for traffic light to lane assignment," IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. 973-980, 2019.

[18] Y. Liu, T. Wang, X. Zhang, and J. Sun, "Petr: Position embedding transformation for multi-view 3d object detection," in European Conference on Computer Vision. Springer, 2022, pp. 531-548.

[19] M. Liang, B. Yang, R. Hu, Y. Chen, R. Liao, S. Feng, and R. Urtasun, "Learning lane graph representations for motion forecasting," in ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16. Springer, 2020, pp. 541-556.

[20] H. Liu, L. Chen, Y. Qiao, C. Lv, and H. Li, "Reasoning multi-agent behavioral topology for interactive autonomous driving," in Annual Conference on Neural Information Processing Systems, 2024.

[21] Y. B. Can, A. Liniger, D. P. Paudel, and L. Van Gool, "Structured bird's-eye-view traffic scene understanding from onboard images," in Proc. of the IEEE Intl. Conf. Comput. Vis. (ICCV), 2021, pp. 15661-15670.

[22] D. Wu, J. Chang, F. Jia, Y. Liu, T. Wang, and J. Shen, "Topomlp: A simple yet strong pipeline for driving topology reasoning," in The Twelfth International Conference on Learning Representations, 2024.

[23] Y. Fu, W. Liao, X. Liu, H. Xu, Y. Ma, Y. Zhang, and F. Dai, "Topologic: An interpretable pipeline for lane topology reasoning on driving scenes," Advances in Neural Information Processing Systems, vol. 37, pp. 61658-61676, 2024.

[24] Z. Ma, S. Liang, Y. Wen, W. Lu, and G. Wan, "Roadpainter: Points are ideal navigators for topology transformer," in European Conference on Computer Vision. Springer, 2025, pp. 179-195.

[25] H. Wu, Z. Zhang, S. Lin, T. Qin, J. Pan, Q. Zhao, C. Xu, and M. Yang, "Blos-bev: Navigation map enhanced lane segmentation network, beyond line of sight," in 2024 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2024, pp. 3212-3219.

[26] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, "Pointnet: Deep learning on point sets for $3\mathrm{\;d}$ classification and segmentation," in CVPR,2017,pp. 652-660.

[27] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville, "Film: Visual reasoning with a general conditioning layer," in Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, 2018.

[28] H. Law and J. Deng, "Cornernet: Detecting objects as paired keypoints," in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 734-750.

[29] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian, "Centernet: Keypoint triplets for object detection," in CVPR, 2019, pp. 6569-6578.

[30] F. Chollet, "Xception: Deep learning with depthwise separable convolutions," in CVPR, 2017, pp. 1251-1258.

[31] J. Hu, L. Shen, and G. Sun, "Squeeze-and-excitation networks," in CVPR, 2018, pp. 7132-7141.

[32] B. Wilson, W. Qi, T. Agarwal et al., "Argoverse 2: Next generation datasets for self-driving perception and forecasting," arXiv preprint arXiv:2301.00493, 2023.

[33] M. Haklay and P. Weber, "Openstreetmap: User-generated street maps," IEEE Pervasive computing, vol. 7, no. 4, pp. 12-18, 2008.