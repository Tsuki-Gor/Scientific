# AppAgent: Multimodal Agents as Smartphone Users

Chi Zhang* Zhao Yang* Jiaxuan Liu* Yucheng Han Xin Chen

Zebiao Huang Bin Fu Gang ${\mathrm{{Yu}}}^{ \dagger  }$

Tencent

\{johnczhang, jayzyang, jiaxuanliu, yuchenghan, shingxchen, zebiaohuang, brianfu, skicyyu\}@tencent.com https://appagent-official.github.io/

<!-- Media -->

<!-- figureText: Telegram<br>Chat with the user Agent Liu and answer his question<br>YouTube<br>Search music video song Wonderful Tonight and leave a praising comment<br>Gmail<br>end an email to<br>anedoe@email.com to<br>ask her about her new job<br>Telecramist<br><br>暑 \$@schminm 顓<br>⓷<br>Primary<br>Wonderful Tonight<br><br>83 | The Im<br>qwertyu i op<br>qwertyu i op<br>q w e r t y u i o p<br>asdfghjkl<br>a s d f g h j k l<br>a s d f g h j k l<br>� z x c v b n m @<br>2 x c v b n m @<br>合 zhíc v b n m @<br>1929 英 1928 接行<br>1923 英 1923 接行<br>略<br>7123 英<br><br><br>Observation: The screenshot shows the Telegram app, a conversation from "Agent Liu" with a message... Thought: To complete this task, I should input the text with this answer into the input field labeled with the tag '7' : text("The Toronto Raptors won the 2019 NBA ... ")<br>Observation: The screenshot shows a YouTube search result for "Wonderful Tonight" ...<br>Observation: The screenshot shows a smartphone's email application ...<br>Thought: To complete this task, I should insert a praising comment into the text input field labeled '2'. : text("This is such a timeless piece...")<br>Thought: To complete this task, I should type the content of the email asking Jane Doe about her new job. : text("Dear Jane Doe,\\n\\nI am writing to ...")<br>Lightroom<br>Improve the quality of this image until it looks good to you<br>Clock<br>Set an alarm at 12:30 pr every Friday and Sunday, and disable the vibration<br>TEMU<br>Search for a gaming eadset and add it to my shopping cart.<br>4.5 厚度 甲 乙 甲 丙 甲 乙 甲 自 变D<br>飞船电<br>2、2、4、<br>又<br>of free shipping - ${\omega }^{2}$ free ${\Omega }^{2}$ distinct within 30 days<br>12:30 ${}_{PM}$<br>a graph in region of the map is a part of a graph<br>${05} : {00}$<br>Object<br>Simple 如前研制面<br>南京奥地利设计站<br>＃＃＃<br><<br>Observation: The screenshot shows a photo editing app on a smartphone with an image displayed on the screen... ought: To complete this task, I should adjust the noise reduction level using the slider. : Action: swipe(23, "right", "medium")<br>Observation: The image shows a screenshot of an alarm application on a smartphone.. Thought: To complete this task, I should disable the vibration feature by tapping on the UI element. : tap(17)<br>Observation: The screenshot displays a shopping app with a variety of headsets listed ...<br>Thought: To complete this task, I should add the selected headset into my shopping cart, ALBORE tap(14) -->

<img src="https://cdn.noedgeai.com/bo_d44na0k601uc738n6lug_0.jpg?x=224&y=545&w=1199&h=1099&r=0"/>

Figure 1: Diverse applications of our multimodal agent framework for smartphone App operation. We evaluate the effectiveness of our agent model on 50 tasks across 10 different Apps, highlighting its adaptability and effectiveness in a real-world context.

<!-- Media -->

## Abstract

Recent advancements in large language models (LLMs) have led to the creation of intelligent agents capable of performing complex tasks. This paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications. Our framework enables the agent to operate smartphone applica-

tions through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps. Central to our agent's functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates

---

<!-- Footnote -->

*Equal contributions.

${}^{ \dagger  }$ Corresponding Author.

<!-- Footnote -->

---

<!-- Meanless: ETOT POTTE TATTA<br>EZOZ 38A 77 [AD'SO] ZAILLETTIET:AIXIE<br>1 -->


a knowledge base that the agent refers to for executing complex tasks across different applications. To demonstrate the practicality of our agent, we conducted extensive testing over 50 tasks in 10 different applications, including social media, email, maps, shopping, and sophisticated image editing tools. The results affirm our agent's proficiency in handling a diverse array of high-level tasks.

## 1 Introduction

The emergence of large language models (LLMs), such as ChatGPT (OpenAI, 2021) and GPT-4 (Ope-nAI, 2023), marks a significant milestone in the field of artificial intelligence and natural language processing. These advanced models represent a fundamental change in how machines understand and generate human language, exhibiting a level of sophistication and versatility previously unattainable. One of the most exciting developments in this field is the capability of LLMs to function not just as language processors, but as agents capable of performing complex tasks. This evolution is evident in initiatives such as AutoGPT (Yang et al., 2023a) and MetaGPT (Hong et al., 2023), which showcase the practical applications of LLMs in tasks requiring advanced cognitive functions like reasoning, planning, and collaboration. The significance of these developments cannot be overstated, as they extend the utility of LLMs beyond simple language tasks, revolutionizing various aspects of technology and daily life.

However, a key limitation of these LLM-based agents has been their reliance solely on text-based information. This restriction has historically curtailed their perception and interaction with their environment. The introduction of models equipped with vision capabilities, such as the latest iteration of GPT-4, marks a pivotal breakthrough. By integrating the ability to process and interpret visual information, these models can now understand aspects of their surroundings that are difficult or impossible to convey through text alone. This extended capability enables LLMs to interpret context, recognize patterns, and respond to visual cues, thus providing a more holistic and interactive experience with the world.

In our work, we focus on building a multimodal agent leveraging the vision capabilities of multimodal large language models to undertake tasks previously unachievable by text-only agents. In particular, we explore an interesting but challenging application that builds an agent to operate any smartphone application (App) in the mobile operating system. Our approach differs significantly from existing intelligent phone assistants like Siri, which operate through system back-end access and function calls. Instead, our agent interacts with smartphone apps in a human-like manner, using low-level operations such as tapping and swiping on the graphical user interface (GUI). The proposed agent offers multiple advantages. Firstly, it eliminates the need for system back-end access, making our agent universally applicable across various applications. Additionally, this approach enhances security and privacy, as the agent does not require deep system integration. Furthermore, by operating on the GUI level, our agent can adapt to changes in app interfaces and updates, ensuring long-term applicability and flexibility.

However, creating a multimodal agent capable of operating diverse smartphone apps presents significant challenges. Existing research indicates that adapting current models for embodied tasks necessitates extensive training data, and collecting a large dataset of app demonstrations for training is a formidable task. Moreover, different apps have unique GUIs with varying icon meanings and operational logic, and it remains uncertain whether these adapted models can effectively generalize to unseen apps.

In this paper, we introduce a multimodal agent framework aimed at operating any smartphone app like human users. The learning of our framework involves an exploration phase where the agent interacts autonomously with apps through a set of pre-defined actions and learns from their outcomes. These interactions are documented, which assists the agent in navigating and operating the apps. This learning process can be accelerated by observing a few human demonstrations. Following this exploratory phase, the agent can operate the app by consulting the constructed document based on its current state, eliminating the need to adapt the parameters of the LLMs or collect extensive training data for each app.

To validate its effectiveness, we tested our agent on 50 tasks across 10 different apps, ranging from social media and messaging to email, maps, shopping, and even complex image editing apps. Both quantitative results and user studies underscore the advantages of our design, particularly its adaptability, user-friendliness, and efficient learning and

<!-- Meanless: 2 -->


operating capabilities across a wide range of applications. This underlines the potential of our agent as a versatile and effective tool in the realm of smartphone app operation.

In summary, this paper makes the following contributions:

- We open-source a multimodal agent framework, focusing on operating smartphone applications with our developed action space.

- We propose an innovative exploration strategy, which enables the agent to learn to use novel apps.

- Through extensive experiments across multiple apps, we validate the advantages of our framework, demonstrating its potential in the realm of AI-assisted smartphone app operation.

## 2 Related Work

### 2.1 Large language models

The development of ChatGPT (OpenAI, 2021) and GPT-4 (OpenAI, 2023) represents a crucial advancement in natural language processing. Unlike earlier large language models (LLMs), these new models (Touvron et al., 2023a,b; Zeng et al., 2022; Taori et al., 2023; Zheng et al., 2023) enable multi-round conversations and have the impressive ability to follow complex instructions. The integration of vision capabilities in GPT-4V (Yang et al., 2023b) is a further milestone, enabling the language model to process and interpret visual data. This addition has broadened the scope of potential AI applications, allowing GPT-4 to undertake diverse tasks such as problem-solving, logical reasoning, tool usage, API calls, and coding. Recent studies (Yang et al., 2023c; Yan et al., 2023) have shown that GPT-4V can understand various types of images, including simple user interfaces (UIs) in popular smartphone apps. However, challenges arise when the apps are new and their UIs are less typical, which highlights a major problem that our work aims to address. Among open-source efforts from the industry and research community, the LLaMA series (Touvron et al., 2023a,b) are the most popular equivalents and have been fine-tuned to acquire conversational abilities, employing a decoder-only architecture similar to ChatGPT (Taori et al., 2023; Zheng et al., 2023). Building upon LLaMA, many multimodal LLMs, such as LLaVA (Liu et al.,

2023b,a), ChartLlama (Han et al., 2023), and Sta-bleLLaVA (Li et al., 2023), also demonstrate vision understanding capabilities akin to those of GPT-4V. Nevertheless, a performance gap persists between these open-source models and GPT-4V, suggesting potential areas for further development.

### 2.2 LLMs as agents

The use of LLMs as agents for executing complex tasks has gained increasing attention. Initiatives like AutoGPT (Yang et al., 2023a), Hug-gingGPT (Shen et al., 2023), and MetaGPT (Hong et al., 2023) illustrate this trend, and many projects demonstrate impressive capabilities, moving beyond basic language tasks to engaging in activities requiring higher cognitive functions, such as software development (Qian et al., 2023; Chen et al., 2021) and gaming (FAIR et al., 2022; Park et al., 2023; Xu et al., 2023). In this context, Yao et al. (Yao et al., 2023) introduce an innovative approach that synergizes reasoning and acting in LLMs, significantly enhancing their decision-making and interactive capabilities. LLM-based agents are designed to utilize the advanced language and reasoning skills of LLMs to interact with and manipulate their environment (Liu et al., 2023c; Gur et al., 2023; Xie et al., 2023). This includes performing tasks that require understanding context, making decisions, and learning from interactions (Xi et al., 2023; Hu and Shu, 2023). Such agents are pivotal in applications where human-like cognitive abilities are essential.

The emergence of multimodal LLM agents (Wang et al., 2023; Furuta et al., 2023; Brohan et al., 2022, 2023; Reed et al., 2022), capable of processing various inputs including text, images, audio, and video, has further broadened the scope of LLM applications. This versatility is particularly beneficial for LLM-based agents, enabling them to interact more effectively with their environment and complete more complex tasks, be it completing household tasks in a physical world (Ahn et al., 2022), generating 3D assets via procedural tool use (Sun et al., 2023), or mastering over 600 tasks across different domains at the same time (Reed et al., 2022). Our research contributes to this area by focusing on an agent designed to operate smartphone applications. This agent's ability to interpret screenshots from the operating system demonstrates its flexibility and adaptability, making it a valuable tool in a wide

<!-- Meanless: 3 -->


<!-- Media -->

<!-- figureText: & Exploration Phase<br>Deployment Phase:<br>hase: "beautify this photo for me..."<br>Swipe<br>...<br>2.0<br>2.2<br>"Tapping this UI element opens the color settings to adjust the color of the image."<br>"Swiping this UI element allows the user to adjust the colo temperature of the image."<br><br><br><br>Doc for App<br>Observe, Think, Act, and Summarize -->

<img src="https://cdn.noedgeai.com/bo_d44na0k601uc738n6lug_3.jpg?x=210&y=193&w=1240&h=507&r=0"/>

Figure 2: Overview of our multimodal agent framework designed to operate smartphone applications. The figure illustrates the two-phase approach of our framework. In the exploration phase, the agent interacts with a smartphone application and learns from their outcomes to create a comprehensive reference document. In the deployment phase, the agent utilizes the information compiled in this document to operate and navigate the apps effectively.

<!-- Media -->

range of applications.

## 3 Method

This section details the methodology behind our innovative multimodal agent framework. This framework enables an agent to interact with smartphone applications in a manner akin to human behavior. We first describe the experimental environment and action space, which are foundational elements of our system. Next, we discuss the exploration phase, where the agent learns app functionalities either through autonomous interactions or by observing human demonstrations. Finally, we outline the deployment phase, explaining how the agent applies its acquired knowledge to execute high-level tasks.

### 3.1 Environment and Action Space

Experimental Environment: Our experimental environment is built on a command-line interface (CLI), allowing the agent to interact with smart-phone apps. We chose the Android operating system for our experiments. The agent receives two key inputs: a real-time screenshot showing the app's interface and an XML file detailing the interactive elements. To enhance the agent's ability to identify and interact with these elements seamlessly, we assign each element a unique identifier. These identifiers are derived either from the resource ID in the XML file (if provided) or are constructed by combining the class name, size, and content of the element. These elements are overlaid as semi-transparent numbers on the screenshot.

This helps the agent to interact accurately without needing to specify exact positions on the screen and enhances the agent's precision in controlling the phone.

Action Space: Our agent's action space mirrors common human interactions with smartphones: taps and swipes. We designed four basic functions:

- Tap(element : int) : This function simulates a tap on the UI element numbered on the screen. For example, tap(5) would tap the element labeled '5'.

- Long_press(element : int) : This function emulates a long press (for 1 second) on a UI element.

- Swipe ( element : int, direction : str, dist : str): It allows the agent to swipe on an element in a specified direction (up, down, left, right) and distance (short, medium, long). For instance, swipe(21, "up", "medium") would swipe up on element '21' for a medium distance.

- Text(text : str) : To bypass inefficient virtual keyboard typing, this function inputs text directly into an input field when a virtual keyboard is visible. For example, text("Hello, world!") inputs the string "Hello, world!".

- Back() : A system-level function that helps the agent return to the previous UI page, especially useful for exiting irrelevant pages.

<!-- Meanless: 4 -->


- Exit() : A specialized function is employed to conclude processes, typically invoked upon successful task completion.

These predefined actions are designed to simplify the agent's interactions, particularly by eliminating the need for precise screen coordinates, which can pose challenges for language models in accurately predicting.

### 3.2 Exploration Phase

Exploring by autonomous interactions. The Exploration Phase is central to our framework. Here, the agent learns about the functionalities and features of smartphone apps through trial and error. In this phase, the agent is assigned a task and starts interacting autonomously with the UI elements. It uses different actions and observes the resulting changes in the app interface to understand how it works. The agent, driven by a large language model, attempts to figure out the functions of UI elements and the effects of specific actions by analyzing screenshots before and after each action. This information is compiled into a document that records the effects of actions applied to different elements. When a UI element is acted upon multiple times, the agent will update the document based on past documents and current observations to improve quality. To make exploration more efficient, the agent stops further exploring UI elements if the current UI page seems unrelated to the main tasks of the app, like advertisement pages. In such cases, it uses the Android system's Back() function to return to the previous UI page. Compared with random exploration, such as Depth-First Search and Breadth-First Search, this goal-oriented exploration approach ensures that the agent focuses on elements crucial for the effective operation of the app. The agent also utilizes the LLM's existing knowledge about user interfaces to improve exploration efficiency. The exploration stops when the agent completes the assigned task.

Exploring by watching demos. An alternative and often more effective exploration method involves the agent observing human demonstrations. These demonstrations provide the agent with examples of efficient app usage, especially for understanding complex functionalities that might be challenging to discover through autonomous interactions. In this method, a human user operates the apps while the agent observes, recording only the elements and actions employed by the human.

This strategy narrows down the exploration space and prevents the agent from engaging with irrelevant app pages, making it a more streamlined and efficient approach compared to autonomous interactions.

### 3.3 Deployment Phase

Following the exploration phase, the agent is well-equipped to execute complex tasks based on its accrued experience. The agent adheres to a step-by-step approach when given a task, with each step encompassing access to a screenshot of the current UI and a dynamically generated document detailing the functions of UI elements and the actions' effects on the current UI page. The prompts also provide detailed explanations of all available actions. In each step, the agent is first tasked with providing its observations of the current UI, followed by articulating its thought process concerning the task and current observations. Subsequently, the agent proceeds to execute actions by invoking available functions. After each action, the agent summarizes the interaction history and the actions taken during the current step. This information is incorporated into the next prompt, which provides the agent with a form of memory. This meticulous approach enhances the reliability and interpretability of the agent's actions, thereby facilitating more informed decision-making. The deployment phase stops when the agent determines that the task has been accomplished, at which point it can exit the process by taking the Exit() action.

## 4 Experiments

In this section, we will present our evaluation of the multimodal agent framework through a combination of quantitative and qualitative experiments. Our primary goal is to assess the agent's performance and its ability to operate a diverse set of smartphone applications effectively.

### 4.1 Experimental Setup

To comprehensively evaluate our method, we construct a benchmark that includes 10 popular applications, each serving various purposes. These applications include Google Maps, Twitter, Telegram, YouTube, Spotify, Yelp, Gmail, TEMU, Clock, and Lightroom. We have intentionally chosen this diverse set of apps to test the agent's adaptability across various functions and interfaces. In particular, to gain a more comprehensive insight into the

<!-- Meanless: 5 -->


<!-- Media -->

<!-- figureText: Google Maps: navigate to Tencent Shanghai Branch<br><br><br><br>Observation<br>鱼 Tencent Shanghai Branch<br>Observation<br>Observation<br>- 上海任意不然探切割线之包<br>The search field is focused and a keyboard is present on the lower half of the screen. UI element labeled ' 6 ' appears to be the search input field where text can be input to find locations, addresses, or points of interest.<br>A keyboard and a search result for "Tencent Shanghai Branch" with address details and a distance measur are displayed.<br>The screenshot shows a map application with the location "Tencent Shanghai Branch" pinpointed with a red marker. There are two buttons at the bottom of the screen, one labeled "Directions" and the other labeled "Start", with numeric tags 1 and 3 respectively.<br>To complete the given task, I should enter the text "Tencent Shanghai Branch" in the search field to proceed with the search for the destination.<br>D. 说出您想去的地点<br>To complete the given task, I should confirm the search for "Tencent Shanghai Branch". In this screenshot, the search confirm button is represented by the blue button with the text "\\u641e\\u7d22" ("Search" in Chinese) on it<br>To complete the given task, which is to navigate to Tencent Shanghai Branch, I should tap the "Directions" button to initiate the route finding process.<br>Branch Branches Brunch (8)<br>q w e r t y u i o p<br>qwe r t y u i o p<br>a s d f g h j k l<br>Action<br>a s d f g h j k l<br>Action<br>< z x c v b n m @<br>text("Tencent Shanghai Branch")<br>- 2 | c v b n m @<br>tap(1)<br>tap(2)<br>mea 英： ： ： ： ？ ：<br>7123 黄色 1327 1238 1125 1125<br>Gmail: send an email to janedoe@email.com to ask her about her new job 药盒 性格B: 常数回归4<br>(2) 设 ${OD}$ 的 空数 ${DD}$ 中<br>Observation<br>Observation<br>Inquiry About Your New Position<br>he recipient's email address is already filled Your New Position." The body of the email is not visible, and there's a UI element labeled '11' which is used to collapse the keyboard.<br>The recipient's email address is already filled in as janedoe@email.com and the subject line is "Inquiry About Your New Position." The cursor is blinking in the main body area of the email, indicating that it is ready to receive text input. The on-screen keyboard is<br>The email body is typed with a message asking Jane Doe about her recent job and expressing eagerness to hear from her. There is a send button labeled with the numeric tag ' 3 '.<br>Thought<br>recently started. Could you please<br>Thought<br>Thought<br>provide some details about your new<br>To complete the task, the next step should be to collapse the keyboard so that I can access the body of the email to type the message to<br>To complete the given task, the next step I should do is to type the content of the email asking Jane Doe about her new job.<br>To complete the given task, the next step I should take is to send the email by tapping the send button<br>Position Positions Positioned $\otimes$<br>qwer ty u i o p<br>q w e r t y u i o p<br>q w e r t y u i o p<br>a s d f g h j k l<br>Action<br>a s d f g h j k l<br>Action<br>a s d f g h j k l<br>tap(11)<br>台 z x c v b n m 回<br>text("Dear Jane Doe, n\\n1 am writing to inquire about your new position that you recently started..<br>7123 英 ： ： ： ： ： （ ） 下一５<br>M23 其中，<br>VO O O O<br>VO DO<br>I O O O<br>Lightroom: improve the quality of this image until it looks good to you<br>NoSIMS:Cl（簡易開發） [頁]2014, [頁]201<br><br>Observation<br><br><br>Observation<br>The screenshot shows a photo editing application interface with various adjustment settings for These adjustments have numeric tags 20, 21, 22, and 23 respectively. The settings are currently at their default values, and the image appears underexposed, needing improvement in lighting.<br>The exposure level has been increased, as indicated by the slider tagged with number 20 being to the right. The with their sliders at the neutral position. There are also other adjustment options available, such as color and detail which could further improve the image quality if needed.<br>The exposure and contrast sliders are visible with the exposure increased to $+ {2.20}$ and the shadows sliders are at0,indicating no adjustment has been made to them.<br>Thought<br>Thought<br>Thought<br>underexposure. I need to increase the exposure. The slider for the exposure setting interact with to adjust the exposure level. I will swipe right on this slider to increase the exposure<br>to adjust the contrast, as increasing contrast can often make an image appear more vivid and can complement interacting with UI element labeled 21 will adjust the contrast level of the image.<br>and adjusted the contrast slightly, the next step could be to adjust the highlights or shadows to further improve the image quality. Adjusting the highlights might help in bringing out more detail in the brighter parts of the image without affecting the overall exposure too much.<br>lib bmw m w<br>swipe(20, "right", "medium")<br><br>swipe(21. "right", "short")<br>tap(22) -->

<img src="https://cdn.noedgeai.com/bo_d44na0k601uc738n6lug_5.jpg?x=186&y=181&w=1274&h=1168&r=0"/>

Figure 3: Qualitative Task Evaluation Across Three Apps. This figure presents qualitative results for three distinct tasks conducted on Google Maps, Gmail, and Lightroom. It showcases AppAgent's ability to accurately perceive, reason, and execute tasks, demonstrating its competence in various application contexts. Due to space constraints, some less critical details have been omitted from the description.

<!-- Media -->

vision capabilities of our agent, we conducted an in-depth case study using Adobe Lightroom, an image-editing application. This specific case study allowed us to evaluate the agent's proficiency in handling visual tasks and its ability to interpret and manipulate images within the app. For the exploration phase, we capped the maximum number of steps at 40. During testing, we limited the maximum number of steps to 10. For these experiments, we utilized the state-of-the-art multimodal large language model, GPT-4. GPT-4 is equipped to process interleaved image-and-text inputs effectively. This unique capability enables our agent to interpret and interact with both visual and textual information seamlessly within the applications.

### 4.2 Design and Analysis

Baselines. To comprehensively evaluate our multimodal agent framework, we considered various design choices and their impact on performance. We conducted experiments using different configurations to provide valuable insights into the agent's behavior. We started with GPT-4 without any reference documents during testing and examined its performance both with the raw action API and our simplified action space. Next, we explored different ways to generate guiding documents for the agent. These included documents generated through autonomous exploration, watching human demonstrations, and the manually crafted document as an oracle benchmark.

To effectively compare the performance of different methods, we employed three key metrics:

<!-- Meanless: 6 -->


<!-- Media -->

<table><tr><td>Method</td><td>Document</td><td>Action Space</td><td>SR↑</td><td>Reward ↑</td><td>Avg. Steps</td></tr><tr><td rowspan="2">GPT4 (Baseline)</td><td>None</td><td>Raw</td><td>2.2%</td><td>0.6</td><td>4.0</td></tr><tr><td>None</td><td>Ours</td><td>48.9%</td><td>3.5</td><td>6.9</td></tr><tr><td rowspan="3">AppAgent</td><td>Auto. Exploration</td><td>Ours</td><td>73.3%</td><td>5.1</td><td>4.4</td></tr><tr><td>Watching Demos</td><td>Ours</td><td>84.4%</td><td>4.7</td><td>5.1</td></tr><tr><td>Manually Crafted</td><td>Ours</td><td>95.6%</td><td>5.5</td><td>5.5</td></tr></table>

Table 1: Evaluating Design Choices in AppAgent Performance. This table contrasts different design elements within AppAgent. Key findings include: our custom-developed action space surpasses the raw action space in efficiency; the exploration phase, incorporating both autonomous interaction and observation of human demonstrations, significantly enhances agent performance; and the auto-generated documentation yields outcomes on par with those derived from manually crafted documents.

<table><tr><td>Method</td><td>Document</td><td>Action Space</td><td>Avg. Rank $\downarrow$</td><td>Num. Tools</td></tr><tr><td>GPT4 (Baseline)</td><td>None</td><td>Ours</td><td>2.30</td><td>2.4</td></tr><tr><td rowspan="2">AppAgent</td><td>Watching Demos</td><td>Ours</td><td>1.95</td><td>5.8</td></tr><tr><td>Manually Crafted</td><td>Ours</td><td>1.75</td><td>4.0</td></tr></table>

Table 2: Case study on image editing tasks with Lightroom App. We conduct a user study to rank the image editing results of different methods. Our agents produce better results than the GPT-4 baseline.

<!-- Media -->

Successful Rate (SR): This metric measures the average rate at which the agent successfully completes tasks within an app. If the agent fails to finish the task in 10 steps, it is considered a failure.

Reward: To provide a more fine-grained measurement, we developed a reward model to assess performance. For each task within an app, we scored different UI pages. The closer the UI page was to the objective, the higher the score received. This means that even if the agent failed to complete the task, it would still receive credit based on its final state.

Average Steps: We also reported the average number of steps required to successfully finish tasks across the selected applications.

Results. The comparison of our experimental results is presented in Table 1. We report the average performance of 45 tasks on 9 of the 10 previously described apps. Notably, we excluded Lightroom from this evaluation, as assessing task completion in this application presented inherent ambiguities. As demonstrated, our simplified action space significantly improves the performance of the GPT-4 baseline. Our observations indicate that LLM struggles with producing accurate xy coordinates, while our simplified action space eliminates this challenging requirement. Additionally, documents generated through autonomous exploration and observ-

ing human demonstrations proved to be highly effective. Their results consistently outperformed the GPT-4 baseline and are comparable to the results of human-written documents, which highlights the efficacy of our design in enhancing the agent's performance across a diverse set of applications.

Qualitative results. In Fig. 3, we provide examples showcasing the agent's execution process for various tasks. This qualitative analysis serves to demonstrate the agent's capacity to accurately perceive, reason, and act in response to given tasks. For a more comprehensive understanding of our agent's capabilities, please refer to our project page, which includes additional demonstration videos.

### 4.3 Case Study

To gain deeper insights into the vision capabilities of our agent, we conducted an extensive case study using Adobe Lightroom, an image-editing application. This specific case study allowed us to evaluate the agent's proficiency in handling visual tasks, which was previously impossible for text-only agent models. Lightroom, as an image-editing app with various editing tools, demands a wide range of operations, such as selecting appropriate tools and manipulating image parameters. This case study provides a robust evaluation of the agent's overall capabilities. Additionally, the open-ended nature of image editing tasks allows us to

<!-- Meanless: 7 -->


assess the agent's problem-solving abilities. We prepared five images with visual issues, such as low contrast and overexposure. Various variants of our model, as previously illustrated, were used to edit these images. A user study was conducted to rank the editing results produced by different methods. We also reported the average number of tools used for image editing, providing an additional reference to the editing process's complexity. All models were assigned the task of "fix this image until it looks good to you" without specifying the image's problems. The comparison of the results is presented in Table 2. As we can see, our agent model with documents yields consistently better results than the GPT-4 baseline, which emphasizes the influence of documents in our design. The generated documents by watching the demonstration produced comparable results with the results of manually crafted documents, which suggests the effectiveness of the exploration phase. We also find that with a document, the agent tends to use various tools to improve the image quality, while the GPT-4 baseline uses fewer tools.

## 5 Conclusion

In this paper, we have introduced a novel multimodal agent framework that leverages the vision capabilities of large language models to operate smartphone applications in a human-like manner. Our approach eliminates the need for system back-end access and offers security, adaptability, and flexibility advantages. Our exploration-based learning strategy allows the agent to quickly adapt to new applications with unfamiliar user interfaces, making it a versatile tool for various tasks. Our extensive experiments across various apps highlight our agent's ability to handle diverse high-level tasks and underscore its adaptability and learning efficiency.

Limitation. We have adopted a simplified action space for smartphone operations, which means that advanced controls such as multi-touch and irregular gestures are not supported. This limitation may restrict the agent's applicability in some challenging scenarios. Nevertheless, we recognize this as an avenue for future research and development.

## References

Michael Ahn, Anthony Brohan, Noah Brown, Yev-gen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol

Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-month, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. 2022. Do as i can and not as i say: Grounding language in robotic af-fordances. In arXiv preprint arXiv:2204.01691.

Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. 2023. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818.

Anthony Brohan, Noah Brown, Justice Carbajal, Yev-gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. 2022. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.

Meta FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. 2022. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067-1074.

Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur. 2023. Multimodal web navigation with instruction-finetuned foundation models.

Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksan-dra Faust. 2023. A real-world webagent with planning, long context understanding, and program synthesis.

Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. 2023. Chartllama: A multimodal llm for chart understanding and generation.

Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2023. Metagpt: Meta programming for a multi-agent collaborative framework.

<!-- Meanless: 8 -->


Zhiting Hu and Tianmin Shu. 2023. Language models, agent models, and world models: The law for machine reasoning and planning. arXiv preprint arXiv:2312.05230.

Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin Fu, Guosheng Lin, Chunhua Shen, Ling Chen, and Yunchao Wei. 2023. Stablellava: Enhanced visual instruction tuning with synthesized image-dialogue data.

Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning.

Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning.

Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Ao-han Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023c. Agent-Bench: Evaluating LLMs as agents. arXiv preprint arXiv: 2308.03688.

OpenAI. 2021. Chatgpt. https://openai.com/ research/chatgpt.

OpenAI. 2023. Gpt-4 technical report.

Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-22.

Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. Communicative agents for software development. arXiv preprint arXiv:2307.07924.

Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. 2022. A generalist agent. arXiv preprint arXiv:2205.06175.

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugging-gpt: Solving ai tasks with chatgpt and its friends in huggingface. In Advances in Neural Information Processing Systems.

Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould. 2023. 3d-gpt: Procedural 3d modeling with large language models. arXiv preprint arXiv:2310.12945.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.

Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jin-bing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, and Yitao Liang. 2023. Jarvis-1: Open-world multitask agents with memory-augmented multimodal language models.

Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864.

Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Lu-oxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, and Tao Yu. 2023. Openagents: An open platform for language agents in the wild.

Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xi-aolong Wang, Weidong Liu, and Yang Liu. 2023. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658.

An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, Zicheng Liu, and Lijuan Wang. 2023. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv: 2311.07562.

<!-- Meanless: 9 -->


Hui Yang, Sifu Yue, and Yunzhong He. 2023a. Auto-gpt for online decision making: Benchmarks and additional opinions.

Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023b. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421.

Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023c. The dawn of Imms: Preliminary explorations with gpt-4v(ision). arXiv preprint arXiv: 2309.17421.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing reasoning and acting in language models. In ICLR.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.

<!-- Meanless: 10 -->