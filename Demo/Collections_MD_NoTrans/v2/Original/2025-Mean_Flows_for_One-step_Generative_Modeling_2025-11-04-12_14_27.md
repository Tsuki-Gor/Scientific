

<!-- Meanless: のSNSSS DHSS EKSSSSORSKS-->

# Mean Flows for One-step Generative Modeling

Zhengyang Geng ${}^{1 * }$ Mingyang Deng ${}^{2}$ Xingjian Bai ${}^{2}$ J. Zico Kolter ${}^{1}$ Kaiming He ${}^{2}$

${}^{1}$ CMU ${}^{2}$ MIT

## Abstract

We propose a principled and effective framework for one-step generative modeling. We introduce the notion of average velocity to characterize flow fields, in contrast to instantaneous velocity modeled by Flow Matching methods. A well-defined identity between average and instantaneous velocities is derived and used to guide neural network training. Our method, termed the MeanFlow model, is self-contained and requires no pre-training, distillation, or curriculum learning. MeanFlow demonstrates strong empirical performance: it achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet ${256} \times  {256}$ trained from scratch,significantly outperforming previous state-of-the-art one-step diffusion/flow models. Our study substantially narrows the gap between one-step diffusion/flow models and their multi-step predecessors, and we hope it will motivate future research to revisit the foundations of these powerful models.

<!-- Media -->

<!-- figureText: 35 iCT-XL Shortcut-XL MF-XL ${2}^{38}$ ${2}^{39}$ Training Compute (GFLOPs, log-scale) 131M 30 308M 25 459M 1-step FID XL: 675M 15 10 MF-B MF-M MF-L ${2}^{36}$ ${2}^{37}$ -->

<img src="https://cdn.noedgeai.com/bo_d40v6djef24c73d3k6eg_0.jpg?x=315&y=1111&w=573&h=427&r=0"/>

<img src="https://cdn.noedgeai.com/bo_d40v6djef24c73d3k6eg_0.jpg?x=899&y=1109&w=589&h=121&r=0"/>

Figure 1: One-step generation on ImageNet ${256} \times  {256}$ from scratch. Our MeanFlow (MF) model achieves significantly better generation quality than previous state-of-the-art one-step diffusion/flow methods. Here, iCT [43], Shortcut [13], and our MF are all 1-NFE generation, while IMM's 1-step result [52] involves 2-NFE guidance. Detailed numbers are in Tab. 2. Images shown are generated by our 1-NFE model.

<!-- Media -->

## 1 Introduction

The goal of generative modeling is to transform a prior distribution into the data distribution. Flow Matching $\left\lbrack  {{28},2,{30}}\right\rbrack$ provides an intuitive and conceptually simple framework for constructing flow paths that transport one distribution to another. Closely related to diffusion models [42, 44, 19], Flow Matching focuses on the velocity fields that guide model training. Since its introduction, Flow Matching has seen widespread adoption in modern generative modeling [11, 33, 35].

Both Flow Matching and diffusion models perform iterative sampling during generation. Recent research has paid significant attention to few-step—and in particular, one-step, feedforward—generative models. Pioneering this direction, Consistency Models [46, 43, 15, 31] introduce a consistency constraint to network outputs for inputs sampled along the same path. Despite encouraging results, the consistency constraint is imposed as a property of the network's behavior, while the properties of the underlying ground-truth field that should guide learning remain unknown. Consequently, training can be unstable and requires a carefully designed "discretization curriculum" $\left\lbrack  {{46},{43},{15}}\right\rbrack$ to progressively constrain the time domain. In this work, we propose a principled and effective framework, termed MeanFlow, for one-step generation. The core idea is to introduce a new ground-truth field representing the average velocity, in contrast to the instantaneous velocity typically modeled in Flow Matching. Average velocity is defined as the ratio of displacement to a time interval, with displacement given by the time integral of the instantaneous velocity. Solely originated from this definition, we derive a well-defined, intrinsic relation between the average and instantaneous velocities, which naturally serves as a principled basis for guiding network training.

---

<!-- Footnote -->

*Work partly done when visiting MIT.

<!-- Footnote -->

---


Building on this fundamental concept, we train a neural network to directly model the average velocity field. We introduce a loss function that encourages the network to satisfy the intrinsic relation between average and instantaneous velocities. No extra consistency heuristic is needed. The existence of the ground-truth target field ensures that the optimal solution is, in principle, independent of the specific network, which in practice can lead to more robust and stable training. We further show that our framework can naturally incorporate classifier-free guidance (CFG) [18] into the target field, incurring no additional cost at sampling time when guidance is used.

Our MeanFlow Models demonstrate strong empirical performance in one-step generative modeling. On ImageNet 256 $\times  {256}$ [7],our method achieves an FID of 3.43 using 1-NFE (Number of Function Evaluations) generation. This result significantly outperforms previous state-of-the-art methods in its class by a relative margin of ${50}\%$ to ${70}\%$ (Fig. 1). In addition,our method stands as a self-contained generative model: it is trained entirely from scratch, without any pre-training, distillation, or curriculum learning. Our study largely closes the gap between one-step diffusion/flow models and their multi-step predecessors, and we hope it will inspire future work to reconsider the foundations of these powerful models.

## 2 Related Work

Diffusion and Flow Matching. Over the past decade, diffusion models [42, 44, 19, 45] have been developed into a highly successful framework for generative modeling. These models progressively add noise to clean data and train a neural network to reverse this process. This procedure involves solving stochastic differential equations (SDE), which is then reformulated as probability flow ordinary differential equations (ODE) [45, 22]. Flow Matching methods [28, 2, 30] extend this framework by modeling the velocity fields that define flow paths between distributions. Flow Matching can also be viewed as a form of continuous-time Normalizing Flows [36].

Few-step Diffusion/Flow Models. Reducing sampling steps has become an important consideration from both practical and theoretical perspectives. One approach is to distill a pre-trained many-step diffusion model into a few-step model, e.g., [39, 14, 41] or score distillation [32, 50, 53]. Early explorations into training few-step models [46] are built upon the evolution of distillation-based methods. Meanwhile, Consistency Models [46] are developed as a standalone generative model that does not require distillation. These models impose consistency constraints on network outputs at different time steps, encouraging them to produce the same endpoints along the trajectory. Various consistency models and training strategies [46, 43, 15, 31, 49] have been investigated.

In recent work, several methods have focused on characterizing diffusion-/flow-based quantities with respect to two time-dependent variables. In [3], a Flow Map is defined as the integral of the flow between two time steps, with several forms of matching losses developed for learning. In comparison to the average velocity our method is based on, the Flow Map corresponds to displacement. Shortcut Models [13] introduce a self-consistency loss function in addition to Flow Matching, which captures relationships between the flows at different discrete time intervals. Inductive Moment Matching [52] models the self-consistency of stochastic interpolants at different time steps.

## 3 Background: Flow Matching

Flow Matching $\left\lbrack  {{28},{30},1}\right\rbrack$ is a family of generative models that learn to match the flows,represented by velocity fields,between two probabilistic distributions. Formally,given data $x \sim  {p}_{\text{data }}\left( x\right)$ and prior $\epsilon  \sim  {p}_{\text{prior }}\left( \epsilon \right)$ ,a flow path can be constructed as ${z}_{t} = {a}_{t}x + {b}_{t}\epsilon$ with time $t$ ,where ${a}_{t}$ and ${b}_{t}$ are predefined schedules. The velocity ${v}_{t}$ is defined as ${v}_{t} = {z}_{t}^{\prime } = {a}_{t}^{\prime }x + {b}_{t}^{\prime }\epsilon$ ,where ${}^{\prime }$ denotes the time derivative. This velocity is referred to as the conditional velocity in [28],denoted by ${v}_{t} = {v}_{t}\left( {{z}_{t} \mid  x}\right)$ . See Fig. 2 left. A commonly used schedule is ${a}_{t} = 1 - t$ and ${b}_{t} = t$ ,which leads to ${v}_{t} = \epsilon  - x$ .

<!-- Meanless: 2-->




<!-- Media -->

Figure 2: Velocity fields in Flow Matching [28]. Left: con-

<!-- figureText: $v\left( {{z}_{t},t}\right)$ -->

<img src="https://cdn.noedgeai.com/bo_d40v6djef24c73d3k6eg_2.jpg?x=221&y=198&w=583&h=274&r=0"/>

ditional flows [28]. A given ${z}_{t}$ can arise from different $\left( {x,\epsilon }\right)$ pairs,resulting in different conditional velocities ${v}_{t}$ . Right: marginal flows [28], obtained by marginalizing over all possible conditional velocities. The marginal velocity field serves as the underlying ground-truth field for network training. All velocities shown here are essentially instantaneous velocities. Illustration follows [12]. (Gray dots: samples from prior; red dots: samples from data.)

<!-- Media -->

Because a given ${z}_{t}$ and its ${v}_{t}$ can arise from different $x$ and $\epsilon$ ,Flow Matching essentially models the expectation over all possibilities, called the marginal velocity [28] (Fig. 2 right):

$$
v\left( {{z}_{t},t}\right)  \triangleq  {\mathbb{E}}_{{p}_{t}\left( {{v}_{t} \mid  {z}_{t}}\right) }\left\lbrack  {v}_{t}\right\rbrack  . \tag{1}
$$

A neural network ${v}_{\theta }$ parameterized by $\theta$ is learned to fit the marginal velocity field: ${\mathcal{L}}_{\mathrm{{FM}}}\left( \theta \right)  =$ ${\mathbb{E}}_{t,{p}_{t}\left( {z}_{t}\right) }{\begin{Vmatrix}{v}_{\theta }\left( {z}_{t},t\right)  - v\left( {z}_{t},t\right) \end{Vmatrix}}^{2}$ . Although computing this loss function is infeasible due to the marginalization in Eq. (1), it is proposed to instead evaluate the conditional Flow Matching loss [28]: ${\mathcal{L}}_{\mathrm{{CFM}}}\left( \theta \right)  = {\mathbb{E}}_{t,x,\epsilon }{\begin{Vmatrix}{v}_{\theta }\left( {z}_{t},t\right)  - {v}_{t}\left( {z}_{t} \mid  x\right) \end{Vmatrix}}^{2}$ ,where the target ${v}_{t}$ is the conditional velocity. Minimizing ${\mathcal{L}}_{\mathrm{{CFM}}}$ is equivalent to minimizing ${\mathcal{L}}_{\mathrm{{FM}}}$ [28].

Given a marginal velocity field $v\left( {{z}_{t},t}\right)$ ,samples are generated by solving an ODE for ${z}_{t}$ :

$$
\frac{d}{dt}{z}_{t} = v\left( {{z}_{t},t}\right)  \tag{2}
$$

starting from ${z}_{1} = \epsilon  \sim  {p}_{\text{prior }}$ . The solution can be written as: ${z}_{r} = {z}_{t} - {\int }_{r}^{t}v\left( {{z}_{\tau },\tau }\right) {d\tau }$ ,where we use $r$ to denote another time step. In practice,this integral is approximated numerically over discrete time steps. For example, the Euler method, a first-order ODE solver, computes each step as: ${z}_{{t}_{i + 1}} = {z}_{{t}_{i}} + \left( {{t}_{i + 1} - {t}_{i}}\right) v\left( {{z}_{{t}_{i}},{t}_{i}}\right)$ . Higher-order solvers can also be applied.

It is worth noting that even when the conditional flows are designed to be straight ("rectified") [28, 30], the marginal velocity field (Eq. (1)) typically induces a curved trajectory. See Fig. 2 for illustration. We also emphasize that this non-straightness is not only a result of neural network approximation, but rather arises from the underlying ground-truth marginal velocity field. When applying coarse discretizations over curved trajectories, numerical ODE solvers lead to inaccurate results.

## 4 MeanFlow Models

### 4.1 Mean Flows

The core idea of our approach is to introduce a new field representing average velocity, whereas the velocity modeled in Flow Matching represents the instantaneous velocity.

Average Velocity. We define average velocity as the displacement between two time steps $t$ and $r$ (obtained by integration) divided by the time interval. Formally,the average velocity $u$ is:

$$
u\left( {{z}_{t},r,t}\right)  \triangleq  \frac{1}{t - r}{\int }_{r}^{t}v\left( {{z}_{\tau },\tau }\right) {d\tau }. \tag{3}
$$

To emphasize the conceptual difference,throughout this paper,we use the notation $u$ to denote average velocity,and $v$ to denote instantaneous velocity. $u\left( {{z}_{t},r,t}\right)$ is a field that is jointly dependent on(r,t). The field of $u$ is illustrated in Fig. 3. Note that in general,the average velocity $u$ is the result of a functional of the instantaneous velocity $v$ : that is, $u = \mathcal{F}\left\lbrack  v\right\rbrack   \triangleq  \frac{1}{t - r}{\int }_{r}^{t}{vd\tau }$ . It is a field induced by $v$ ,not depending on any neural network. Conceptually,just as the instantaneous velocity $v$ serves as the ground-truth field in Flow Matching,the average velocity $u$ in our formulation provides an underlying ground-truth field for learning.

By definition,the field of $u$ satisfies certain boundary conditions and "consistency" constraints (generalizing the terminology of [46]). As $r \rightarrow  t$ ,we have: $\mathop{\lim }\limits_{{r \rightarrow  t}}u = v$ . Moreover,a form of "consistency" is naturally satisfied: taking one larger step over $\left\lbrack  {r,t}\right\rbrack$ is "consistent" with taking two smaller consecutive steps over $\left\lbrack  {r,s}\right\rbrack$ and $\left\lbrack  {s,t}\right\rbrack$ ,for any intermediate time $s$ . To see this,observe that $\left( {t - r}\right) u\left( {{z}_{t},r,t}\right)  = \left( {s - r}\right) u\left( {{z}_{s},r,s}\right)  + \left( {t - s}\right) u\left( {{z}_{t},s,t}\right)$ ,which follows directly from the additivity of the integral: ${\int }_{r}^{t}{vd\tau } = {\int }_{r}^{s}{vd\tau } + {\int }_{s}^{t}{vd\tau }$ . Thus,a network that accurately approximates the true $u$ is expected to satisfy the consistency relation inherently, without the need for explicit constraints.

<!-- Meanless: 3-->




<!-- Media -->

<!-- figureText: $u\left( {z,r,t}\right)$ $u\left( {z,r,t}\right)$ $u\left( {z,r,t}\right)$ $t = {0.7}$ $t = {1.0}$ $\left( {t - r}\right) u\left( {z,r,t}\right)$ $u\left( {z,r,t}\right)$ $t = {0.5}$ -->

<img src="https://cdn.noedgeai.com/bo_d40v6djef24c73d3k6eg_3.jpg?x=316&y=156&w=1168&h=219&r=0"/>

Figure 3: The field of average velocity $u\left( {z,r,t}\right)$ . Leftmost: While the instantaneous velocity $v$ determines the tangent direction of the path,the average velocity $u\left( {z,r,t}\right)$ ,defined in Eq. (3), is generally not aligned with $v$ . The average velocity is aligned with the displacement,which is $\left( {t - r}\right) u\left( {z,r,t}\right)$ . Right three subplots: The field $u\left( {z,r,t}\right)$ is conditioned on both $r$ and $t$ ,and is shown here for $t = {0.5},{0.7}$ ,and 1.0 .

<!-- Media -->

The ultimate aim of our MeanFlow model will be to approximate the average velocity using a neural network ${u}_{\theta }\left( {{z}_{t},r,t}\right)$ . This has the notable advantage that,assuming we approximate this quantity accurately,we can approximate the entire flow path using a single evaluation of ${u}_{\theta }\left( {\epsilon ,0,1}\right)$ . In other words, and as we will also demonstrate empirically, the approach is much more amenable to single or few-step generation, as it does not need to explicitly approximate a time integral at inference time, which was required when modeling instantaneous velocity. However, directly using the average velocity defined by Eq. (3) as ground truth for training a network is intractable, as it requires evaluating an integral during training. Our key insight is that the definitional equation of average velocity can be manipulated to construct an optimization target that is ultimately amenable to training, even when only the instantaneous velocity is accessible.

The MeanFlow Identity. To have a formulation amenable to training, we rewrite Eq. (3) as:

$$
\left( {t - r}\right) u\left( {{z}_{t},r,t}\right)  = {\int }_{r}^{t}v\left( {{z}_{\tau },\tau }\right) {d\tau }. \tag{4}
$$

Now we differentiate both sides with respect to $t$ ,treating $r$ as independent of $t$ . This leads to:

$$
\frac{d}{dt}\left( {t - r}\right) u\left( {{z}_{t},r,t}\right)  = \frac{d}{dt}{\int }_{r}^{t}v\left( {{z}_{\tau },\tau }\right) {d\tau } \Rightarrow  u\left( {{z}_{t},r,t}\right)  + \left( {t - r}\right) \frac{d}{dt}u\left( {{z}_{t},r,t}\right)  = v\left( {{z}_{t},t}\right) , \tag{5}
$$

where the manipulation of the left hand side employs the product rule and the right hand side uses the fundamental theorem of calculus ${}^{2}$ . Rearranging terms,we obtain the identity:

$$
\underset{\text{average vel. }}{\underbrace{u\left( {{z}_{t},r,t}\right) }} = \underset{\text{instant. vel. }}{\underbrace{v\left( {{z}_{t},t}\right) }} - \left( {t - r}\right) \underset{\text{time derivative }}{\underbrace{\frac{d}{dt}u\left( {{z}_{t},r,t}\right) }} \tag{6}
$$

We refer to this equation as the "MeanFlow Identity",which describes the relation between $v$ and $u$ . It is easy to show that Eq. (6) and Eq. (4) are equivalent (see Appendix B.3).

The right hand side of Eq. (6) provides a "target" form for $u\left( {{z}_{t},r,t}\right)$ ,which we will leverage to construct a loss function to train a neural network. To serve as a suitable target, we must also further decompose the time derivative term, which we discuss next.

Computing Time Derivative. To compute the $\frac{d}{dt}u$ term in Eq. (6),note that $\frac{d}{dt}$ denotes a total derivative, which can be expanded in terms of partial derivatives:

$$
\frac{d}{dt}u\left( {{z}_{t},r,t}\right)  = \frac{d{z}_{t}}{dt}{\partial }_{z}u + \frac{dr}{dt}{\partial }_{r}u + \frac{dt}{dt}{\partial }_{t}u. \tag{7}
$$

With $\frac{d{z}_{t}}{dt} = v\left( {{z}_{t},t}\right)$ (see Eq. (2)), $\frac{dr}{dt} = 0$ ,and $\frac{dt}{dt} = 1$ ,we have another relation between $u$ and $v$ :

$$
\frac{d}{dt}u\left( {{z}_{t},r,t}\right)  = v\left( {{z}_{t},t}\right) {\partial }_{z}u + {\partial }_{t}u, \tag{8}
$$

---

<!-- Footnote -->

${}^{2}$ If $r$ depends on $t$ ,the Leibniz rule [26] gives: $\frac{d}{dt}{\int }_{r}^{t}v\left( {{z}_{\tau },\tau }\right) {d\tau } = v\left( {{z}_{t},t}\right)  - v\left( {{z}_{r},r}\right) \frac{dr}{dt}$ .

<!-- Footnote -->

---

<!-- Meanless: 4-->


This equation shows that the total derivative is given by the Jacobian-vector product (JVP) between $\left\lbrack  {{\partial }_{z}u,{\partial }_{r}u,{\partial }_{t}u}\right\rbrack$ (the Jacobian matrix of the function $u$ ) and the tangent vector $\left\lbrack  {v,0,1}\right\rbrack$ . In modern libraries, this can be efficiently computed by the jvp interface, such as torch. func. jvp in PyTorch or jax. jvp in JAX, which we discuss later.

Training with Average Velocity. Up to this point, the formulations are independent of any network parameterization. We now introduce a model to learn $u$ . Formally,we parameterize a network ${u}_{\theta }$ and encourage it to satisfy the MeanFlow Identity (Eq. (6)). Specifically, we minimize this objective:

$$
\mathcal{L}\left( \theta \right)  = \mathbb{E}{\begin{Vmatrix}{u}_{\theta }\left( {z}_{t},r,t\right)  - \mathrm{{sg}}\left( {u}_{\mathrm{{tgt}}}\right) \end{Vmatrix}}_{2}^{2}, \tag{9}
$$

$$
\text{where}\;{u}_{\mathrm{{tgt}}} = v\left( {{z}_{t},t}\right)  - \left( {t - r}\right) \left( {v\left( {{z}_{t},t}\right) {\partial }_{z}{u}_{\theta } + {\partial }_{t}{u}_{\theta }}\right) \text{,} \tag{10}
$$

The term ${u}_{\text{tgt }}$ serves as the effective regression target,which is driven by Eq. (6). This target uses the instantaneous velocity $v$ as the only ground-truth signal; no integral computation is needed. While the target should involve derivatives of $u$ (that is, $\partial u$ ),they are replaced by their parameterized counterparts (that is, $\partial {u}_{\theta }$ ). In the loss function,a stop-gradient (sg) operation is applied on the target ${u}_{\text{tgt }}$ ,following common practice [46,43,15,31,13]: in our case,it eliminates the need for "double backpropagation" through the Jacobian-vector product, thereby avoiding higher-order optimization Despite these practices for optimizability,if ${u}_{\theta }$ were to achieve zero loss,it is easy to show that it would satisfy the MeanFlow Identity (Eq. (6)), and thus satisfy the original definition (Eq. (3)).

The velocity $v\left( {{z}_{t},t}\right)$ in Eq. (10) is the marginal velocity in Flow Matching [28] (see Fig. 2 right). We follow [28] to replace it with the conditional velocity (Fig. 2 left). With this, the target is:

$$
{u}_{\text{tgt }} = {v}_{t} - \left( {t - r}\right) \left( {{v}_{t}{\partial }_{z}{u}_{\theta } + {\partial }_{t}{u}_{\theta }}\right) . \tag{11}
$$

Recall that ${v}_{t} = {a}_{t}^{\prime }x + {b}_{t}^{\prime }\epsilon$ is the conditional velocity [28],and by default, ${v}_{t} = \epsilon  - x$ .

Pseudocode for minimizing the loss function Eq. (9) is presented in Alg. 1. Overall, our method is conceptually simple: it behaves similarly to Flow Matching, with the key difference that the matching target is modified by $- \left( {t - r}\right) \left( {{v}_{t}{\partial }_{z}{u}_{\theta } + {\partial }_{t}{u}_{\theta }}\right)$ ,arising from our consideration of the average velocity. In particular,note that if we were to restrict to the condition $t = r$ ,then the second term vanishes, and the method would exactly match standard Flow Matching.

In Alg. 1, the jvp operation is highly efficient. In essence,computing $\frac{d}{dt}u$ via jvp requires only a single backward pass, similar to standard back-propagation in neural networks. Because $\frac{d}{dt}u$ is part of the target ${u}_{\text{tgt }}$ and thus subject to stopgrad (w.r.t. $\theta$ ),the backpropagation for neural network optimization (w.r.t. $\theta$ ) treats $\frac{d}{dt}u$ as a constant, incurring no higher-order gradient computation. Consequently, jvp introduces only a single extra backward pass, and its cost is comparable to that of backpropagation. In our JAX implementation of Alg. 1, the overhead is less than 20% of the total training time (see appendix).

<!-- Media -->

Algorithm 1 MeanFlow: Training.

Note: in PyTorch and JAX, jvp returns the function output and JVP.

---

					#fn(z, r, t): function to predict u
				#x: training batch
t, r = sample_t_r(   )
	e = randn_like(x)
$z = \left( {1 - t}\right)  * x + t * e$
$\mathrm{v} = \mathrm{e} - \mathrm{x}$
u, dudt = jvp(fn, (z, r, t), (v, 0, 1))
	u_tgt = v - (t - r) * dudt
	error = u - stopgrad(u_tgt)
loss = metric(error)

---

<!-- Media -->

Sampling. Sampling using a MeanFlow model is performed simply by replacing the time integral with the average velocity:

$$
{z}_{r} = {z}_{t} - \left( {t - r}\right) u\left( {{z}_{t},r,t}\right)  \tag{12}
$$

In the case of 1-step sampling, we simply have ${z}_{0} = {z}_{1} - u\left( {{z}_{1},0,1}\right)$ ,where ${z}_{1} = \epsilon  \sim  {p}_{\text{prior }}\left( \epsilon \right)$ . Alg. 2 provides the pseudocode. Although one-step sampling is the main focus on this work, we emphasize that few step sampling is also straightforward given this equation.

<!-- Media -->

Algorithm 2 MeanFlow: 1-step Sampling

---

	e = randn ( x_shape )
$\mathrm{x} = \mathrm{e} - \operatorname{fn}\left( {\mathrm{e},\mathrm{r} = 0,\mathrm{t} = 1}\right)$

---

<!-- Media -->

Relation to Prior Work. While related to previous one-step generative models [46, 43, 15, 31, 49, ${23},{13},{52}\rbrack$ ,our method provides a more principled framework. At the core of our method is the functional relationship between two underlying fields $v$ and $u$ ,which naturally leads to the MeanFlow Identity that $u$ must satisfy (Eq. (6)). This identity does not depend on the introduction of neural networks. In contrast, prior works typically rely on extra consistency constraints, imposed on the behavior of the neural network. Consistency Models [46, 43, 15, 31] are focused on paths anchored at the data side: in our notations,this corresponds to fixing $r \equiv  0$ for any $t$ . As a result,Consistency Models are conditioned on a single time variable, unlike ours. On the other hand, the Shortcut [13] and IMM [52] models are conditioned on two time variables: they introduce additional two-time self-consistency constraints. In contrast, our method is solely driven by the definition of average velocity, and the MeanFlow Identity (Eq. (6)) used for training is naturally derived from this definition, with no extra assumption.

<!-- Meanless: 5-->




### 4.2 Mean Flows with Guidance

Our method naturally supports classifier-free guidance (CFG) [18]. Rather than naïvely applying CFG at sampling time, which would double NFE, we treat CFG as a property of the underlying ground-truth fields. This formulation allows us to enjoy the benefits of CFG while maintaining the 1-NFE behavior during sampling.

Ground-truth Fields. We construct a new ground-truth field ${v}^{\text{cfg }}$ :

$$
{v}^{\mathrm{{cfg}}}\left( {{z}_{t},t \mid  \mathbf{c}}\right)  \triangleq  {\omega v}\left( {{z}_{t},t \mid  \mathbf{c}}\right)  + \left( {1 - \omega }\right) v\left( {{z}_{t},t}\right) , \tag{13}
$$

which is a linear combination of a class-conditional and a class-unconditional field:

$$
v\left( {{z}_{t},t \mid  \mathbf{c}}\right)  \triangleq  {\mathbb{E}}_{{p}_{t}\left( {{v}_{t} \mid  {z}_{t},\mathbf{c}}\right) }\left\lbrack  {v}_{t}\right\rbrack  \;\text{ and }\;v\left( {{z}_{t},t}\right)  \triangleq  {\mathbb{E}}_{\mathbf{c}}\left\lbrack  {v\left( {{z}_{t},t \mid  \mathbf{c}}\right) }\right\rbrack  , \tag{14}
$$

where ${v}_{t}$ is the conditional velocity [28] (more precisely,sample-conditional velocity in this context). Following the spirit of MeanFlow,we introduce the average velocity ${u}^{\text{cfg }}$ corresponding to ${v}^{\text{cfg }}$ . As per the MeanFlow Identity (Eq. (6)), ${u}^{\mathrm{{cfg}}}$ satisfies:

$$
{u}^{\mathrm{{cfg}}}\left( {{z}_{t},r,t \mid  \mathbf{c}}\right)  = {v}^{\mathrm{{cfg}}}\left( {{z}_{t},t \mid  \mathbf{c}}\right)  - \left( {t - r}\right) \frac{d}{dt}{u}^{\mathrm{{cfg}}}\left( {{z}_{t},r,t \mid  \mathbf{c}}\right) . \tag{15}
$$

Again, ${v}^{\text{cfg }}$ and ${u}^{\text{cfg }}$ are underlying ground-truth fields that do not depend on neural networks. Here, ${v}^{\mathrm{{cfg}}}$ ,as defined in Eq. (13),can be rewritten as:

$$
{v}^{\mathrm{{cfg}}}\left( {{z}_{t},t \mid  \mathbf{c}}\right)  = {\omega v}\left( {{z}_{t},t \mid  \mathbf{c}}\right)  + \left( {1 - \omega }\right) {u}^{\mathrm{{cfg}}}\left( {{z}_{t},t,t}\right) , \tag{16}
$$

where we leverage the relation ${}^{3} : v\left( {{z}_{t},t}\right)  = {v}^{\mathrm{{cfg}}}\left( {{z}_{t},t}\right)$ ,as well as ${v}^{\mathrm{{cfg}}}\left( {{z}_{t},t}\right)  = {u}^{\mathrm{{cfg}}}\left( {{z}_{t},t,t}\right)$ .

Training with Guidance. With Eq. (15) and Eq. (16), we construct a network and its learning target. We directly parameterize ${u}^{\text{cfg }}$ by a function ${u}_{\theta }^{\text{cfg }}$ . Based on Eq. (15),we obtain the objective:

$$
\mathcal{L}\left( \theta \right)  = \mathbb{E}{\begin{Vmatrix}{u}_{\theta }^{\mathrm{{cfg}}}\left( {z}_{t},r,t \mid  \mathbf{c}\right)  - \mathrm{{sg}}\left( {u}_{\mathrm{{tgt}}}\right) \end{Vmatrix}}_{2}^{2}, \tag{17}
$$

$$
\text{where}{u}_{\mathrm{{tgt}}} = {\widetilde{v}}_{t} - \left( {t - r}\right) \left( {{\widetilde{v}}_{t}{\partial }_{z}{u}_{\theta }^{\mathrm{{cfg}}} + {\partial }_{t}{u}_{\theta }^{\mathrm{{cfg}}}}\right) \text{.} \tag{18}
$$

This formulation is similar to Eq. (9),with the only difference that it has a modified ${\widetilde{v}}_{t}$ :

$$
{\widetilde{v}}_{t} \triangleq  \omega {v}_{t} + \left( {1 - \omega }\right) {u}_{\theta }^{\mathrm{{cfg}}}\left( {{z}_{t},t,t}\right) , \tag{19}
$$

which is driven by Eq. (16): the term $v\left( {{z}_{t},t \mid  \mathbf{c}}\right)$ in Eq. (16),which is the marginal velocity,is replaced by the (sample-)conditional velocity ${v}_{t}$ ,following [28]. If $\omega  = 1$ ,this loss function degenerates to the no-CFG case in Eq. (9).

To expose the network ${u}_{\theta }^{\text{cfg }}$ in Eq. (17) to class-unconditional inputs,we drop the class condition with ${10}\%$ probability,following [18]. Driven by a similar motivation,we can also expose ${u}_{\theta }^{\mathrm{{cfg}}}\left( {{z}_{t},t,t}\right)$ in Eq. (19) to both class-unconditional and class-conditional versions: the details are in Appendix B.1.

Single-NFE Sampling with CFG. In our formulation, ${u}_{\theta }^{\text{cfg }}$ directly models ${u}^{\text{cfg }}$ ,which is the average velocity induced by the CFG velocity ${v}^{\text{cfg }}$ (Eq. (13)). As a result,no linear combination is required during sampling: we directly use ${u}_{\theta }^{\text{cfg }}$ for one-step sampling (see Alg. 2),with only a single NFE. This formulation preserves the desirable single-NFE behavior.

---

<!-- Footnote -->

${}^{3}$ Observe that: ${v}^{\mathrm{{cfg}}}\left( {{z}_{t},t}\right)  \triangleq  {\mathbb{E}}_{\mathbf{c}}\left\lbrack  {{v}^{\mathrm{{cfg}}}\left( {{z}_{t},t \mid  \mathbf{c}}\right) }\right\rbrack   = \omega {\mathbb{E}}_{\mathbf{c}}\left\lbrack  {v\left( {{z}_{t},t \mid  \mathbf{c}}\right) }\right\rbrack   + \left( {1 - \omega }\right) v\left( {{z}_{t},t}\right)  = v\left( {{z}_{t},t}\right)$ .

<!-- Footnote -->

---

<!-- Meanless: 6-->


### 4.3 Design Decisions

Loss Metrics. In Eq. (9), the metric considered is the squared L2 loss. Following [46, 43, 15], we investigate different loss metrics. In general,we consider the loss function in the form of $\mathcal{L} = \parallel \Delta {\parallel }_{2}^{2\gamma }$ , where $\Delta$ denotes the regression error. It can be proven (see [15]) that minimizing $\parallel \Delta {\parallel }_{2}^{2\gamma }$ is equivalent to minimizing the squared L2 loss $\parallel \Delta {\parallel }_{2}^{2}$ with "adapted loss weights". Details are in the appendix. In practice,we set the weight as $w = 1/{\left( \parallel \Delta {\parallel }_{2}^{2} + c\right) }^{p}$ ,where $p = 1 - \gamma$ and $c > 0$ (e.g., ${10}^{-3}$ ). The adaptively weighted loss is $\operatorname{sg}\left( w\right)  \cdot  \mathcal{L}$ ,with $\mathcal{L} = \parallel \Delta {\parallel }_{2}^{2}$ . If $p = {0.5}$ ,this is similar to the Pseudo-Huber loss in [43]. We compare different $p$ values in experiments.

Sampling Time Steps(r,t). We sample the two time steps(r,t)from a predefined distribution We investigate two types of distributions: (i) a uniform distribution, $\mathcal{U}\left( {0,1}\right)$ ,and (ii) a logit-normal (lognorm) distribution [11],where a sample is first drawn from a normal distribution $\mathcal{N}\left( {\mu ,\sigma }\right)$ and then mapped to(0,1)using the logistic function. Given a sampled pair,we assign the larger value to $t$ and the smaller to $r$ . We set a certain portion of random samples with $r = t$ .

Conditioning on(r,t). We use positional embedding [48] to encode the time variables,which are then combined and provided as the conditioning of the neural network. We note that although the field is parameterized by ${u}_{\theta }\left( {{z}_{t},r,t}\right)$ ,it is not necessary for the network to directly condition on(r,t) For example,we can let the network directly condition on $\left( {t,{\Delta t}}\right)$ ,with ${\Delta t} = t - r$ . In this case,we have ${u}_{\theta }\left( {\cdot ,r,t}\right)  \triangleq  \operatorname{net}\left( {\cdot ,t,t - r}\right)$ where net is the network. The JVP computation is always w.r.t. the function ${u}_{\theta }\left( {\cdot ,r,t}\right)$ . We compare different forms of conditioning in experiments.

## 5 Experiments

Experiment Setting. We conduct our major experiments on ImageNet [7] generation at 256 $\times  {256}$ resolution. We evaluate Fréchet Inception Distance (FID) [17] on ${50}\mathrm{\;K}$ generated images. We examine the number of function evaluations (NFE) and study 1-NFE generation by default. Following $\left\lbrack  {{34},{13},{52}}\right\rbrack$ ,we implement our models on the latent space of a pre-trained VAE tokenizer [37]. For ${256} \times  {256}$ images,the tokenizer produces a latent space of ${32} \times  {32} \times  4$ ,which is the input to the model Our models are all trained from scratch. Implementation details are in Appendix A.

In our ablation study, we use the ViT-B/4 architecture (namely, "Base" size with a patch size of 4) [9] as developed in [34], trained for 80 epochs (400K iterations). As a reference, DiT-B/4 in [34] has 68.4 FID, and SiT-B/4 [33] (in our reproduction) has 58.9 FID, both using 250-NFE sampling.

### 5.1 Ablation Study

We investigate the model properties in Tab. 1, analyzed next:

From Flow Matching to Mean Flows. Our method can be viewed as Flow Matching with a modified target (Alg. 1),and it reduces to standard Flow Matching when $r$ always equals $t$ . Tab. 1a compares the ratio of randomly sampling $r \neq  t$ . A 0% ratio of $r \neq  t$ (reducing to Flow Matching) fails to produce reasonable results for 1-NFE generation. A non-zero ratio of $r \neq  t$ enables MeanFlow to take effect, yielding meaningful results under 1-NFE generation. We observe that the model balances between learning the instantaneous velocity $\left( {r = t}\right)$ vs. propagating into $r \neq  t$ via the modified target Here,the optimal FID is achieved at a ratio of 25%, and a ratio of 100% also yields a valid result.

JVP Computation. The JVP operation Eq. (8) serves as the core relation that connects all(r,t) coordinates. In Tab. 1b, we conduct a destructive comparison in which incorrect JVP computation is intentionally performed. It shows that meaningful results are achieved only when the JVP computation is correct. Notably,the JVP tangent along ${\partial }_{z}u$ is $d$ -dimensional,where $d$ is the data dimension (here, ${32} \times  {32} \times  4$ ),and the tangents along ${\partial }_{r}u$ and ${\partial }_{t}u$ are one-dimensional. Nevertheless,these two time variables determine the field $u$ ,and their roles are therefore critical even though they are only one-dimensional.

Conditioning on(r,t). As discussed in Sec. 4.3,we can represent ${u}_{\theta }\left( {z,r,t}\right)$ by various forms of explicit positional embedding,e.g., ${u}_{\theta }\left( {\cdot ,r,t}\right)  \triangleq  \operatorname{net}\left( {\cdot ,t,t - r}\right)$ . Tab. 1c compares these variants. Tab. 1c shows that all variants of(r,t)embeddings studied yield meaningful 1-NFE results,demonstrating the effectiveness of MeanFlow as a framework. Embedding(t,t - r),that is,time and interval,achieves the best result,while directly embedding(r,t)performs almost as well. Notably, even embedding only the interval $t - r$ yields reasonable results.

<!-- Meanless: 7-->




<!-- Media -->

<table><tr><td>$\%$ of $r \neq  t$</td><td>FID, 1-NFE</td></tr><tr><td>$0\% \left( { = \mathrm{{FM}}}\right)$</td><td>328.91</td></tr><tr><td>25%</td><td>61.06</td></tr><tr><td>50%</td><td>63.14</td></tr><tr><td>100%</td><td>67.32</td></tr></table>

(a) Ratio of sampling $r \neq  t$ . The 0% entry reduces to the standard Flow Matching baseline.

<table><tr><td>jvp tangent</td><td>FID, 1-NFE</td></tr><tr><td>(v,0,1)</td><td>61.06</td></tr><tr><td>(v,0,0)</td><td>268.06</td></tr><tr><td>(v,1,0)</td><td>329.22</td></tr><tr><td>(v,1,1)</td><td>137.96</td></tr></table>

(b) JVP computation. The correct jvp tangent is(v,0,1)for Jacobian $\left( {{\partial }_{z}u,{\partial }_{r}u,{\partial }_{t}u}\right)$ .

<table><tr><td>pos. embed</td><td>FID, 1-NFE</td></tr><tr><td>(t,r)</td><td>61.75</td></tr><tr><td>(t,t - r)</td><td>61.06</td></tr><tr><td>(t,r,t - r)</td><td>63.98</td></tr><tr><td>$t - r$ only</td><td>63.13</td></tr></table>

(c) Positional embedding. The network is conditioned on the embeddings applied to the specified variables.

<table><tr><td>$t,r$ sampler</td><td>FID, 1-NFE</td></tr><tr><td>uniform(0,1)</td><td>65.90</td></tr><tr><td>lognorm(-0.2,1.0)</td><td>63.83</td></tr><tr><td>lognorm(-0.2,1.2)</td><td>64.72</td></tr><tr><td>lognorm(-0.4, 1.0)</td><td>61.06</td></tr><tr><td>lognorm(-0.4,1.2)</td><td>61.79</td></tr></table>

<table><tr><td>$p$</td><td>FID, 1-NFE</td></tr><tr><td>0.0</td><td>79.75</td></tr><tr><td>0.5</td><td>63.98</td></tr><tr><td>1.0</td><td>61.06</td></tr><tr><td>1.5</td><td>66.57</td></tr><tr><td>2.0</td><td>69.19</td></tr></table>

<table><tr><td>$\omega$</td><td>FID, 1-NFE</td></tr><tr><td>1.0 (w/o cfg)</td><td>61.06</td></tr><tr><td>1.5</td><td>33.33</td></tr><tr><td>2.0</td><td>20.15</td></tr><tr><td>3.0</td><td>15.53</td></tr><tr><td>5.0</td><td>20.75</td></tr></table>

(d) Time samplers. $t$ and $r$ are sampled (e) Loss metrics. $p = 0$ is squared L2 (f) CFG scale:. Our method supports from the specific sampler. loss. $p = {0.5}$ is Pseudo-Huber loss. 1-NFE CFG sampling.

Table 1: Ablation study on 1-NFE ImageNet 256 $\times  {256}$ generation. FID-50K is evaluated. Default configurations are marked in gray : B/4 backbone, 80-epoch training from scratch.

<!-- Media -->

Time Samplers. Prior work [11] has shown that the distribution used to sample $t$ influences the generation quality. We study the distribution used to sample(r,t)in Tab. 1d. Note that(r,t)are first sampled independently,followed by a post-processing step that enforces $t > r$ by swapping and then caps the proportion of $r \neq  t$ to a specified ratio. Tab. 1d reports that a logit-normal sampler performs the best, consistent with observations on Flow Matching [11].

Loss Metrics. It has been reported [43] that the choice of loss metrics strongly impacts the performance of few-/one-step generation. We study this aspect in Tab. 1e. Our loss metric is implemented via adaptive loss weighting [15] with power $p$ (Sec. 4.3). Tab. 1e shows that $p = 1$ achieves the best result,whereas $p = {0.5}$ (similar to Pseudo-Huber loss [43]) also performs competitively. The standard squared L2 loss (here, $p = 0$ ) underperforms compared to other settings,but still produces meaningful results, consistent with observations in [43].

Guidance Scale. Tab. 1f reports the results with CFG. Consistent with observations in multi-step generation [34], CFG substantially improves generation quality in our 1-NFE setting too. We emphasize that our CFG formulation (Sec. 4.2) naturally support 1-NFE sampling.

Scalability. Fig. 4 presents the 1-NFE FID results of MeanFlow across larger model sizes and different training durations. Consistent with the behavior of Transformer-based diffusion/flow models (DiT [34] and SiT [33]), MeanFlow models exhibit promising scalability for 1-NFE generation.

### 5.2 Comparisons with Prior Work

ImageNet ${256} \times  {256}$ Comparisons. In Fig. 1 we compare with previous one-step diffusion/flow models, which are also summarized in Tab. 2 (left). Overall, MeanFlow largely outperforms previous methods in its class: it achieves 3.43 FID, which is an over 50% relative improvement vs. IMM's one-step result of 7.77 [52]; if we compare only 1-NFE (not just one-step) generation, MeanFlow has nearly ${70}\%$ relative improvement ${vs}$ . the previous state-of-the-art (10.60,Shortcut [13]). Our method largely closes the gap between one-step and many-step diffusion/flow models.

In 2-NFE generation, our method achieves an FID of 2.20 (Tab. 2, bottom left). This result is on par with the leading baselines of many-step diffusion/flow models, namely, DiT [34] (FID 2.27) and SiT [33] (FID 2.15),both having an NFE of ${250} \times  2$ (Tab. 2,right),under the same XL/2 backbone. Our results suggest that few-step diffusion/flow models can rival their many-step predecessors. Orthogonal improvements, such as REPA [51], are applicable, which are left for future work.

<!-- Meanless: 8-->




<!-- Media -->

<!-- figureText: 1-NFE FID B/2 M/2 L/2 6.17 3.43 240 Training Epochs -->

<img src="https://cdn.noedgeai.com/bo_d40v6djef24c73d3k6eg_8.jpg?x=315&y=191&w=584&h=334&r=0"/>

Figure 4: Scalability of MeanFlow models on ImageNet 256 $\times  {256.1}$ -NFE generation FID is reported. All models are trained from scratch. CFG is applied while maintaining the 1-NFE sampling behavior. Our method exhibits promising scalability with respect to model size.

<table><tr><td>method</td><td>params</td><td>NFE</td><td>FID</td></tr><tr><td colspan="4">1-NFE diffusion/flow from scratch</td></tr><tr><td>iCT-XL/2 [43] ${}^{ \dagger  }$</td><td>675M</td><td>1</td><td>34.24</td></tr><tr><td>Shortcut-XL/2 [13]</td><td>675M</td><td>1</td><td>10.60</td></tr><tr><td>MeanFlow-B/2</td><td>131M</td><td>1</td><td>6.17</td></tr><tr><td>MeanFlow-M/2</td><td>308M</td><td>1</td><td>5.01</td></tr><tr><td>MeanFlow-L/2</td><td>459M</td><td>1</td><td>3.84</td></tr><tr><td>MeanFlow-XL/2</td><td>676M</td><td>1</td><td>3.43</td></tr><tr><td colspan="4">2-NFE diffusion/flow from scratch</td></tr><tr><td>iCT-XL/2 [43] ${}^{ \dagger  }$</td><td>675M</td><td>2</td><td>20.30</td></tr><tr><td>iMM-XL/2 [52]</td><td>675M</td><td>$1 \times  2$</td><td>7.77</td></tr><tr><td>MeanFlow-XL/2</td><td>676M</td><td>2</td><td>2.93</td></tr><tr><td>MeanFlow-XL/2+</td><td>676M</td><td>2</td><td>2.20</td></tr></table>

<table><tr><td>method</td><td>params</td><td>NFE</td><td>FID</td></tr><tr><td colspan="4">${GANs}$</td></tr><tr><td>BigGAN [5]</td><td>112M</td><td>1</td><td>6.95</td></tr><tr><td>GigaGAN [21]</td><td>569M</td><td>1</td><td>3.45</td></tr><tr><td>StyleGAN-XL [40]</td><td>166M</td><td>1</td><td>2.30</td></tr><tr><td colspan="4">autoregressive/masking</td></tr><tr><td>AR w/ VQGAN [10]</td><td>227M</td><td>1024</td><td>26.52</td></tr><tr><td>MaskGIT [6]</td><td>227M</td><td>8</td><td>6.18</td></tr><tr><td>VAR-d30 [47]</td><td>2B</td><td>${10} \times  2$</td><td>1.92</td></tr><tr><td>MAR-H [27]</td><td>943M</td><td>${256} \times  2$</td><td>1.55</td></tr><tr><td colspan="4">diffusion/flow</td></tr><tr><td>ADM [8]</td><td>554M</td><td>${250} \times  2$</td><td>10.94</td></tr><tr><td>LDM-4-G [37]</td><td>400M</td><td>${250} \times  2$</td><td>3.60</td></tr><tr><td>SimDiff [20]</td><td>2B</td><td>${512} \times  2$</td><td>2.77</td></tr><tr><td>DiT-XL/2 [34]</td><td>675M</td><td>${250} \times  2$</td><td>2.27</td></tr><tr><td>SiT-XL/2 [33]</td><td>675M</td><td>${250} \times  2$</td><td>2.06</td></tr><tr><td>SiT-XL/2+REPA [51]</td><td>675M</td><td>${250} \times  2$</td><td>1.42</td></tr></table>

Table 2: Class-conditional generation on ImageNet-256 $\times  {256}$ . All entries are reported with CFG, when applicable. Left: 1-NFE and 2-NFE diffusion/flow models trained from scratch. Right: Other families of generative models as a reference. In both tables, " $\times  2$ " indicates that CFG incurs an NFE of 2 per sampling step. Our MeanFlow models are all trained for 240 epochs, except that "MeanFlow-XL+" is trained for more epochs and with configurations selected for longer training, specified in appendix. ${}^{ \dagger  }$ : iCT [43] results are reported by [52].

<!-- Media -->

Notably, our method is self-contained and trained entirely from scratch. It achieves the strong results without using any pre-training,distillation,or the curriculum learning adopted in [43, 15, 31].

CIFAR-10 Comparisons. We report unconditional generation results on CIFAR-10 [25] (32) in Tab. 3. FID-50K is reported with 1-NFE sampling. All entries are with the same U-net [38] developed from [44] (~55M), applied directly on the pixel space. All other competitors are with the EDM-style pre-conditioner [22], and ours has no preconditioner. Implementation details are in the appendix. On this dataset, our method is competitive with prior approaches.

<!-- Media -->

<table><tr><td>method</td><td>precond</td><td>NFE</td><td>FID</td></tr><tr><td>iCT [43]</td><td>EDM</td><td>1</td><td>2.83</td></tr><tr><td>ECT [15]</td><td>EDM</td><td>1</td><td>3.60</td></tr><tr><td>sCT [31]</td><td>EDM</td><td>1</td><td>2.97</td></tr><tr><td>IMM [52]</td><td>EDM</td><td>1</td><td>3.20</td></tr><tr><td>MeanFlow</td><td>none</td><td>1</td><td>2.92</td></tr></table>

Table 3: Unconditional CIFAR-10.

<!-- Media -->

## 6 Conclusion

We have presented MeanFlow, a principled and effective framework for one-step generation. Broadly speaking, the scenario considered in this work is related to multi-scale simulation problems in physics that may involve a range of scales, lengths, and resolution, in space or time. Carrying out numerical simulation is inherently limited by the ability of computers to resolve the range of scales. Our formulation involves describing the underlying quantity at coarsened levels of granularity, a common theme that underlies many important applications in physics. We hope that our work will bridge research in generative modeling, simulation, and dynamical systems in related fields.

<!-- Meanless: 9-->




## Acknowledgement

We greatly thank Google TPU Research Cloud (TRC) for granting us access to TPUs. Zhengyang Geng is partially supported by funding from the Bosch Center for AI. Zico Kolter gratefully acknowledges Bosch's funding for the lab. Mingyang Deng and Xingjian Bai are partially supported by the MIT-IBM Watson AI Lab funding award. We thank Runqian Wang, Qiao Sun, Zhicheng Jiang, Hanhong Zhao, Yiyang Lu, and Xianbang Wang for their help on the JAX and TPU implementation.

## References

[1] Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. 2

[2] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In International Conference on Learning Representations (ICLR), 2023. 1, 2

[3] Nicholas M Boffi, Michael S Albergo, and Eric Vanden-Eijnden. Flow map matching. arXiv preprint arXiv:2406.07507, 2024. 2

[4] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.16

[5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations (ICLR), 2019.9

[6] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 9

[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 2, 7

[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Neural Information Processing Systems (NeurIPS), 34, 2021. 9

[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. 7, 14

[10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.9

[11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 1, 7, 8

[12] Tor Fjelde, Emile Mathieu, and Vincent Dutordoir. An introduction to flow matching. https: //mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html, January 2024. Cambridge Machine Learning Group Blog. 3

[13] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. In International Conference on Learning Representations (ICLR), 2025. 1, 2, 5, 6, 7, 8, 9

<!-- Meanless: 10-->




[14] Zhengyang Geng, Ashwini Pokle, and J Zico Kolter. One-step diffusion distillation via deep equilibrium models. Neural Information Processing Systems (NeurIPS), 36, 2024. 2

[15] Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and J Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. 1, 2, 5, 6, 7, 8, 9, 15

[16] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 14

[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Neural Information Processing Systems (NeurIPS), 2017. 7

[18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2, 6, 14, 15

[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Neural Information Processing Systems (NeurIPS), 2020. 1, 2

[20] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning (ICML), 2023. 9

[21] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 9

[22] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Neural Information Processing Systems (NeurIPS), 2022. 2, 9, 14

[23] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ODE trajectory of diffusion. In International Conference on Learning Representations (ICLR), 2024. 5

[24] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015. 14

[25] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. URL https: //www.cs.toronto.edu/~kriz/cifar.html. 9

[26] Gottfried Wilhelm Leibniz. Epistola LXXI ad johannem bernoullium, 5 aug 1697. In Johann Bernoulli, editor, Virorum celeberrimorum G. G. Leibnitti et Johannis Bernoulli Commercium philosophicum et mathematicum, volume I, pages 368-370. Marc-Michel Bousquet, Lausanne & Geneva, 1745. URL https://archive.org/details/bub_gb_103w0MxjoF8C/page/368.First explicit statement of the Leibniz integral rule. 4

[27] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Neural Information Processing Systems (NeurIPS), 2024.9

[28] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In International Conference on Learning Representations (ICLR),2023.1,2,3,5,6

[29] Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, and Itai Gat. Flow matching guide and code, 2024. URL https://arxiv.org/abs/2412.06264.14

[30] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations (ICLR), 2023. 1, 2, 3

<!-- Meanless: 11-->




[31] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. In International Conference on Learning Representations (ICLR), 2025. 1, 2, 5, 6, 9

[32] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: A universal approach for transferring knowledge from pre-trained diffusion models. Neural Information Processing Systems (NeurIPS), 2024. 2

[33] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision (ECCV), 2024. 1, 7, 8, 9

[34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 7, 8, 9, 14

[35] Adam Polyak, , et al. Movie Gen: A cast of media foundation models, 2025. 1

[36] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International Conference on Machine Learning (ICML), 2015. 2

[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 7, 9

[38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention (MICCAI), 2015. 9, 14

[39] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations (ICLR), 2022. 2

[40] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM Transactions on Graphics (SIGGRAPH), 2022. 9

[41] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision (ECCV), 2024. 2

[42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning (ICML), 2015. 1, 2

[43] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. In International Conference on Learning Representations (ICLR), 2024. 1, 2, 5, 6, 7, 8, 9, 15

[44] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Neural Information Processing Systems (NeurIPS), 2019. 1, 2, 9, 14

[45] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations (ICLR), 2021. 2

[46] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning (ICML), 2023. 1, 2, 3, 5, 6, 7

[47] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Neural Information Processing Systems (NeurIPS), 2024. 9

[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Neural Information Processing Systems (NeurIPS), 2017. 7

[49] Ling Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, and Bin Cui. Consistency flow matching: Defining straight flows with velocity consistency. arXiv preprint arXiv:2407.02398, 2024. 2, 5

<!-- Meanless: 12-->




[50] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Frédo Durand, William T Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2

[51] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In International Conference on Learning Representations (ICLR), 2025. 9

[52] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. arXiv preprint arXiv:2503.07565, 2025. 1, 2, 5, 6, 7, 8, 9

[53] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In International Conference on Machine Learning (ICML), 2024. 2

<!-- Meanless: 13-->




## Appendix

## A Implementation

<!-- Media -->

Table 4: Configurations on ImageNet 256 $\times  {256}$ . B/4 is our ablation model.

<table><tr><td>configs</td><td>B/4</td><td>B/2</td><td>M/2</td><td>L/2</td><td>XL/2</td><td>XL/2+</td></tr><tr><td>params (M)</td><td>131</td><td>131</td><td>497.8</td><td>459</td><td>676</td><td>676</td></tr><tr><td>FLOPs (G)</td><td>5.6</td><td>23.1</td><td>54.0</td><td>119.0</td><td>119.0</td><td>119.0</td></tr><tr><td>depth</td><td>12</td><td>12</td><td>16</td><td>24</td><td>28</td><td>28</td></tr><tr><td>hidden dim</td><td>768</td><td>768</td><td>1024</td><td>1024</td><td>1152</td><td>1152</td></tr><tr><td>heads</td><td>12</td><td>12</td><td>16</td><td>16</td><td>16</td><td>16</td></tr><tr><td>patch size</td><td>$4 \times  4$</td><td>$2 \times  2$</td><td>$2 \times  2$</td><td>$2 \times  2$</td><td>$2 \times  2$</td><td>$2 \times  2$</td></tr><tr><td>epochs</td><td>80</td><td>240</td><td>240</td><td>240</td><td>240</td><td>1000</td></tr><tr><td>batch size</td><td colspan="6">256</td></tr><tr><td>dropout</td><td/><td/><td/><td>0.0</td><td/><td/></tr><tr><td>optimizer</td><td colspan="6">Adam [24]</td></tr><tr><td>lr schedule</td><td colspan="6">constant</td></tr><tr><td>lr</td><td colspan="6" rowspan="2">0.0001 (0.9, 0.95)</td></tr><tr><td>Adam $\left( {{\beta }_{1},{\beta }_{2}}\right)$</td></tr><tr><td>weight decay</td><td/><td/><td/><td>0.0</td><td/><td/></tr><tr><td>ema decay</td><td/><td/><td/><td>0.9999</td><td/><td/></tr><tr><td>ratio of $r \neq  t$</td><td>Tab. 1a</td><td/><td/><td>25%</td><td/><td/></tr><tr><td>(r,t)cond</td><td>Tab. 1c</td><td/><td colspan="4">(t,t - r)</td></tr><tr><td>(r,t)sampler</td><td>Tab. 1d</td><td/><td colspan="4">lognorm(-0.4,1.0)</td></tr><tr><td>$p$ for adaptive weight</td><td>Tab. 1e</td><td/><td colspan="4">1.0</td></tr><tr><td>CFG effective scale ${\omega }^{\prime }$</td><td>Tab. 1f</td><td>2.0</td><td>2.0</td><td>2.5</td><td>2.5</td><td>2.0</td></tr><tr><td>CFG $\omega$ ,Eq. (21)</td><td>$\omega  = {\omega }^{\prime }$</td><td>1.0</td><td>1.0</td><td>0.2</td><td>0.2</td><td>1.0</td></tr><tr><td>CFG $\kappa$ ,Eq. (21)</td><td/><td/><td/><td>$\kappa  = 1 - \omega /{\omega }^{\prime }$</td><td/><td/></tr><tr><td>CFG cls-cond drop [18]</td><td/><td/><td/><td>0.1 [18]</td><td/><td/></tr><tr><td>CFG triggered if $t$ is in:</td><td>$\left\lbrack  {{0.0},{1.0}}\right\rbrack$</td><td>$\left\lbrack  {{0.0},{1.0}}\right\rbrack$</td><td>$\left\lbrack  {{0.0},{1.0}}\right\rbrack$</td><td>$\left\lbrack  {{0.0},{0.8}}\right\rbrack$</td><td>$\left\lbrack  {{0.0},{0.75}}\right\rbrack$</td><td>$\left\lbrack  {{0.3},{0.8}}\right\rbrack$</td></tr></table>

<!-- Media -->

ImageNet ${256} \times  {256}$ . We use a standard VAE tokenizer to extract the latent representations. ${}^{4}$ The latent size is ${32} \times  {32} \times  4$ ,which is the input to the model. The backbone architectures follow DiT [34], which are based on ViT [9] with adaLN-Zero [34] for conditioning. To condition on two time variables $\left( {e.g.,\left( {r,t}\right) }\right)$ ,we apply positional embedding on each time variable,followed by a 2-layer MLP, and summed. We keep the the DiT architecture blocks untouched, while architectural improvements are orthogonal and possible. The configuration specifics are in Tab. 4.

CIFAR-10. We experiment with class-unconditional generation on CIFAR-10. Our implementation follows standard Flow Matching practice [29]. The input to the model is ${32} \times  {32} \times  3$ in the pixel space. The network is a U-net [38] developed from [44] (~55M),which is commonly used by other baselines we compare. We apply positional embedding on the two time variables (here,(t,t - r)) and concatenate them for conditioning. We do not use any EDM preconditioner [22].

We use Adam with learning rate 0.0006,batch size 1024, $\left( {{\beta }_{1},{\beta }_{2}}\right)  = \left( {{0.9},{0.999}}\right)$ ,dropout 0.2,weight decay 0, and EMA decay of 0.99995 . The model is trained for 800K iterations (with 10K warm-up [16]). The(r,t)sampler is $\operatorname{lognorm}\left( {-{2.0},{2.0}}\right)$ . The ratio of sampling $r \neq  t$ is 75%. The power $p$ for adaptive weighting is 0.75 . Our data augmentation setup follows [22], with vertical flipping and rotation disabled.

---

<!-- Footnote -->

${}^{4}$ https://huggingface.co/pcuenq/sd-vae-ft-mse-flax

<!-- Footnote -->

---

<!-- Meanless: 14-->


<!-- Media -->

<table><tr><td>$\kappa$</td><td>FID, 1-NFE</td></tr><tr><td>0.0</td><td>20.15</td></tr><tr><td>0.5</td><td>19.15</td></tr><tr><td>0.8</td><td>19.10</td></tr><tr><td>0.9</td><td>18.63</td></tr><tr><td>0.95</td><td>19.17</td></tr></table>

<!-- Media -->

Table 5: Improved CFG for MeanFlow. $\kappa$ is as defined in Eq. (20),whose goal is to enable both class-conditional ${u}^{\mathrm{{cfg}}}\left( {\cdot  \mid  \mathbf{c}}\right)$ and class-unconditional ${u}^{\mathrm{{cfg}}}\left( \cdot \right)$ to appear in the target. In this table,we fix the effective guidance scale ${\omega }^{\prime }$ ,given by ${\omega }^{\prime } = \omega /\left( {1 - \kappa }\right)$ ,as 2.0 . Accordingly,for different $\kappa$ values,we set $\omega$ by $\omega  = \left( {1 - \kappa }\right)  \cdot  {\omega }^{\prime }$ . If $\kappa  = 0$ ,it falls back to the CFG case in Eq. (19) (see also Tab. 1f). Similar to standard CFG's practice of randomly dropping class conditions [18], we observe that mixing class-conditional and class-unconditional ${u}^{\mathrm{{cfg}}}$ in our target improves generation quality.

## B Additional Technical Details

### B.1 Improved CFG for MeanFlow

In Sec. 4.2, we have discussed how to naturally extend our method to supporting CFG. The only change needed is to revise the target by Eq. (19). We notice that only the class-unconditional ${u}^{\mathrm{{cfg}}}$ is presented in Eq. (19). In the original CFG [18], it is a standard practice to mix class-conditional and class-unconditional predictions, approached by random dropping. We observe that a similar idea can be applied to our regression target as well.

Formally,we introduce a mixing scale $\kappa$ and rewrite Eq. (16) as:

$$
{v}^{\mathrm{{cfg}}}\left( {{z}_{t},t \mid  \mathbf{c}}\right)  = {\omega v}\left( {{z}_{t},t \mid  \mathbf{c}}\right)  + \kappa {u}^{\mathrm{{cfg}}}\left( {{z}_{t},t,t \mid  \mathbf{c}}\right)  + \left( {1 - \omega  - \kappa }\right) {u}^{\mathrm{{cfg}}}\left( {{z}_{t},t,t}\right) . \tag{20}
$$

Here,the role of $\kappa$ is to mix with ${u}^{\mathrm{{cfg}}}\left( {\cdot  \mid  \mathbf{c}}\right)$ on the right hand side. We can show that Eq. (20) satisfies the original CFG formulation (Eq. (13)) with the effective guidance scale of ${\omega }^{\prime } = \frac{\omega }{1 - \kappa }$ ,leveraging the relation $v\left( {{z}_{t},t}\right)  = {v}^{\mathrm{{cfg}}}\left( {{z}_{t},t}\right)$ and ${v}^{\mathrm{{cfg}}}\left( {{z}_{t},t}\right)  = {u}^{\mathrm{{cfg}}}\left( {{z}_{t},t,t}\right)$ that we have used for deriving Eq. (16). With this, Eq. (19) is rewritten as

$$
{\widetilde{v}}_{t} \triangleq  \omega \underset{\text{sample }{v}_{t}}{\underbrace{\left( \epsilon  - x\right) }} + \underset{\text{cls-cond output }}{\underbrace{\kappa {u}_{\theta }^{\mathrm{{cfg}}}\left( {{z}_{t},t,t \mid  \mathbf{c}}\right) }} + \underset{\text{cls-uncond output }}{\underbrace{\left( {1 - \omega  - \kappa }\right) {u}_{\theta }^{\mathrm{{cfg}}}\left( {{z}_{t},t,t}\right) }}. \tag{21}
$$

The loss function is the same as defined in Eq. (17).

The influence of introducing $\kappa$ is explored in Tab. 5,where we fix the effective guidance scale ${\omega }^{\prime }$ as 2.0 and vary $\kappa$ . It shows that mixing by $\kappa$ can further improve generation quality. We note that the ablation in Tab. 1f in the main paper did not involve this improvement,i.e., $\kappa$ was set as 0 .

### B.2 Loss Metrics

The squared L2 loss is given by $\mathcal{L} = \parallel \Delta {\parallel }_{2}^{2}$ ,where $\Delta  = {u}_{\theta } - {u}_{\text{tgt }}$ denotes the regression error. Generally,one can adopt the powered L2 loss ${\mathcal{L}}_{\gamma } = \parallel \Delta {\parallel }_{2}^{2\gamma }$ ,where $\gamma$ is a user-specified hyper-parameter. Minimizing this loss is equivalent to minimizing an adaptively weighted squared L2 loss (see [15]): $\frac{d}{d\theta }{\mathcal{L}}_{\gamma } = \gamma {\left( \parallel \Delta {\parallel }_{2}^{2}\right) }^{\left( \gamma  - 1\right) } \cdot  \frac{d\parallel \Delta {\parallel }_{2}^{2}}{d\theta }$ . This can be viewed as weighting the squared L2 loss $\left( {\parallel \Delta {\parallel }_{2}^{2}}\right)$ by a loss-adaptive weight $\lambda  \propto  \parallel \Delta {\parallel }_{2}^{2\left( {\gamma  - 1}\right) }$ . In practice,we follow [15] and weight by:

$$
w = 1/{\left( \parallel \Delta {\parallel }_{2}^{2} + c\right) }^{p}, \tag{22}
$$

where $p = 1 - \gamma$ and $c > 0$ is a small constant to avoid division by zero. If $p = {0.5}$ ,this is similar to the Pseudo-Huber loss in [43]. The adaptively weighted loss is $\mathrm{{sg}}\left( w\right)  \cdot  \mathcal{L}$ ,where sg denotes the stop-gradient operator.

### B.3 On the Sufficiency of the MeanFlow Identity

In the main paper, starting from the definitional Eq. (4), we have derived the MeanFlow Identity Eq. (6). This indicates that "Eq. (4) $\Rightarrow$ Eq. (6)",that is,Eq. (6) is a necessary condition for Eq. (4) Next,we show that it is also a sufficient condition,that is,"Eq. (6) $\Rightarrow$ Eq. (4)".

In general, equality of derivatives does not imply equality of integrals: they may differ by a constant. In our case,we show that the constant is canceled out. Consider a "displacement field" $S$ written as:

$$
S\left( {{z}_{t},r,t}\right)  = \left( {t - r}\right) u\left( {{z}_{t},r,t}\right) . \tag{23}
$$

<!-- Meanless: 15-->


If we treat $S$ as an arbitrary function,then in general,equality of derivatives can only lead to equality of integrals, up to some constants:

$$
\frac{d}{dt}S\left( {{z}_{t},r,t}\right)  = v\left( {{z}_{t},t}\right)  \Rightarrow  S\left( {{z}_{t},r,t}\right)  + {C}_{1} = {\int }_{r}^{t}v\left( {{z}_{\tau },\tau }\right) {d\tau } + {C}_{2}. \tag{24}
$$

<!-- Media -->

<img src="https://cdn.noedgeai.com/bo_d40v6djef24c73d3k6eg_15.jpg?x=330&y=201&w=1141&h=437&r=0"/>

Figure 5: 1-NFE Generation Results. We show curated examples of class-conditional generation on ImageNet 256 $\times  {256} \times  {256}$ using our 1-NFE model (MeanFlow-XL/2,3.43 FID).

<!-- Media -->

However,the definition of ${\left. S\text{ gives }S\right| }_{t = r} = 0$ ,and we also have ${\int }_{r}^{t}{vd\tau } = 0$ when $t = r$ ,which gives ${C}_{1} = {C}_{2}$ . This indicates "Eq. (6) $\Rightarrow$ Eq. (4)".

We note that this sufficiency is a consequence of modeling the average velocity $u$ ,rather than directly the displacement $S$ . Enforcing the equality of derivatives on the displacement,for example, $\frac{d}{dt}S = v$ , does not automatically yield $S = {\int }_{r}^{t}{vd\tau }$ . If we were to parameterize $S$ directly,an extra boundary condition ${\left. S\right| }_{t = r} = 0$ is needed. Our formulation can automatically satisfy this condition.

### B.4 Analysis on Jacobian-Vector Product (JVP) Computation

While JVP computation can be seen as a concern in some methods, it is very lightweight in ours. In our case, as the computed product (between the Jacobian matrix and the tangent vector) is subject to stop-gradient (Eq. (10)), it is treated as a constant when conducting SGD using backpropagation w.r.t. $\theta$ . Consequently,it does not add any overhead to the $\theta$ -backpropagation.

As such, the extra computation incurred by computing this product is the "backward" pass performed by the jvp operation. This backward is analogous to the standard backward pass used for optimizing neural networks (w.r.t. $\theta$ )—in fact,in some deep learning libraries,such as JAX [4],they use similar interfaces to compute the standard backpropagation (w.r.t. θ). In our case, it is even less costly than a standard backpropagation, as it only needs to backpropagate to the input variables, not to the parameters $\theta$ . This overhead is small.

We benchmark the overhead caused by JVP in our ablation model, B/4. We implement the model in JAX and benchmark in v4-8 TPUs. We compare MeanFlow with its Flow Matching counterpart, where the only overhead is the backward pass incurred by JVP; notice that the forward pass of JVP is the standard forward propagation, which Flow Matching (or any typical objective function) also needs to perform. In our benchmarking, the training cost is 0.045 sec/iter using Flow Matching, and 0.052 sec/iter using MeanFlow, which is a merely 16% wall-clock overhead.

## C Qualitative Results

Fig. 5 shows curated generation examples on ImageNet 256 $\times  {256}$ using our 1-NFE model.

<!-- Meanless: 16-->

