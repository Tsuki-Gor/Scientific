# ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long- Horizon Tasks

Yuanyi Song ${}^{1 *  \dagger  }$ ,Heyuan Huang ${}^{2}$ ; Qiqiang Lin ${}^{2}$ ,Yin Zhao ${}^{2}$ ,Xiangmou Qu ${}^{2}$ , Jun Wang ${}^{2}$ , Xingyu Lou ${}^{2 \ddagger  }$ , Weiwen Liu ${}^{1}$ , Zhuosheng Zhang ${}^{1}$ , Jun Wang ${}^{2}$ , Yong Yu ${}^{1}$ , Weinan Zhang ${}^{1 \ddagger  }$ , Zhaoxiang Wang ${}^{2 \ddagger  }$

${}^{1}$ Shanghai Jiao Tong University ${}^{2}$ OPPO

norsheep919@sjtu.edu.cn louxingyu@oppo.com

wnzhang@sjtu.edu.cn steven.wangzx@gmail.com

## Abstract

The rapid advancement of multimodal large language models has enabled agents to operate mobile devices by directly interacting with graphical user interfaces, opening new possibilities for mobile automation. However, real-world mobile tasks are often complex and allow for multiple valid solutions. This contradicts current mobile agent evaluation standards: offline static benchmarks can only validate a single predefined "golden path", while online dynamic testing is constrained by the complexity and non-reproducibility of real devices, making both approaches inadequate for comprehensively assessing agent capabilities. To bridge the gap between offline and online evaluation and enhance testing stability, this paper introduces a novel graph-structured benchmarking framework. By modeling the finite states observed during real-device interactions, it achieves static simulation of dynamic behaviors. Building on this, we develop ColorBench, a benchmark focused on complex long-horizon tasks. It supports evaluation of multiple valid solutions, subtask completion rate statistics, and atomic-level capability analysis. ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average length of over 13 steps. Each task includes at least two correct paths and several typical error paths, enabling quasi-dynamic interaction. By evaluating ColorBench across various baselines, we discover limitations of existing models and propose improvement directions and feasible technical pathways to enhance agents' performance on complex, long-horizon problems based on experimental results. Code and data are available at: ColorBench.

## 1 Introduction

As one of the primary human-computer interaction entry points for today's internet, mobile devices present an urgent need to enhance internet service accessibility and user experience by exploring their automation, calling for AI agents' graphical user interface (GUI) interaction capabilities (Wen et al., 2024; Hong et al., 2024; Li et al., 2025; Chen et al., 2025a). With the advancement of artificial intelligence, utilizing multimodal large language models (MLLMs) as agents to operate graphical user interfaces GUIs for mobile tasks has gained increasing attention (Jiang et al., 2025; Cheng et al., 2025; Wang et al., 2024b;a; Wu et al., 2025), giving rise to numerous outstanding benchmarks for mobile tasks (Rawles et al., 2023; Lu et al., 2024; Sun et al., 2022; Li et al., 2024a; Liu et al., 2025a; Huang et al., 2025).

---

*This work was done during Yuanyi Song's internship at OPPO.

${}^{ \dagger  }\mathrm{Y}$ . Song and H. Huang contributed equally to this research.

${}^{ \ddagger  }\mathrm{X}$ . Lou,W. Zhang and Z. Wang are the corresponding authors.

---

Existing mobile GUI agent benchmarks can be broadly categorized into two paradigms (Zhang et al., 2024a; Xu et al., 2025):

1) Offline Static Evaluation: This paradigm involves assessing agent performance using static image trajectory data. Although widely adopted due to the feasibility of rapid, large-scale data collection, this method is subject to several prominent limitations: a) Rigid Assessment. It relies on fixed trajectories for evaluation, failing to assess multiple potential solutions and leading to potential misjudgments. b) Oversimplified Metrics. Its evaluation metrics are singular, overemphasizing step-level success rates while lacking a comprehensive assessment of overall task completion. c) Coarse-Grained Diagnosis. Its evaluation dimensions are broad, considering only task-wise completion status and lacking fine-grained analysis of atomic capabilities. These issues cause a significant "offline-online evaluation discrepancy" during static testing, which means an agent's performance on offline static evaluations may not positively correlate with its actual device performance (Li et al., 2024a; Sun et al., 2022).

<img src="https://cdn.noedgeai.com/bo_d4nfss77aajc73frs9l0_1.jpg?x=924&y=340&w=592&h=468&r=0"/>

Figure 1: Structure of the ColorBench. It illustrates multi-path solutions, reflective backtracking and automated evaluation milestones, demonstrating how the graph is structured and utilized. Each node represents a screen list.

2) Online Dynamic Evaluation: This paradigm assesses models within a dynamic environment to deliver more authentic performance metrics. It can be further categorized into two types: virtual environment evaluation and real-device evaluation. However, existing virtual testing environments like AndroidWorld (Rawles et al., 2024) and AndroidLab (Xu et al., 2024) involve applications and interaction patterns that differ from real-world scenarios, making it challenging to accurately reflect an agent's true deployment performance. Moreover, its implementation is also relatively challenging. While real-device evaluation is highly valued for its ability to accurately reflect user scenarios, it suffers from the following defects: a) Instability. Variations in page loading delays and sudden ad pop-ups may cause unexpected interruptions, leading to ambiguous evaluation criteria; b) Inefficient Result Assessment. The highly dynamic GUI makes automated checks difficult, forcing reliance on time-consuming manual verification (Dai et al., 2025; Hu et al., 2024). c) Security Risks. Most applications require account login to access full functionality, posing risks of unintended payments, data deletion, or personal information leaks during evaluations (Ma et al., 2024; Chen et al., 2025c; Zhang et al., 2024c). These factors directly result in poor reproducibility and low efficiency during dynamic testing.

To address these problems, we propose ColorBench, a novel graph-structured mobile agent benchmark for complex long-horizon tasks. As presented in Table 1, ColorBench aims to strike a balance between offline static and online dynamic evaluation through a finite-state simulation, maintaining the stability of the former while incorporating the flexibility of the latter. Moreover, we focus on complex long-horizon tasks due to their composite nature, which is characterized by multiple atomic subtasks executed in sequential, parallel, or recursive patterns, along with the existence of multiple path solutions, thereby making a graph-structured benchmark ideally suited for this context.

We organize the colorbench into a strongly connected graph as shown in Figure 1, where mobile screen states serve as nodes and action transition relationships between nodes serve as edges, multiple solutions (paths of different colors), reflective backtracking (the blue path), and automated evaluation milestones (the red flag). Specifically, we designed an efficient graph-structured benchmark construction methodology, enabling subsequent researchers to expand or reconstruct graphs. It

Table 1: Comparison between ColorBench and other mobile agents benchmarks. Our ColorBench is a static graph environment but supports interaction similar to real world. The column "Interaction" indicates whether to support the agent to interact with provided environment when evaluating. The column "AtomCap" means whether to support atomic capability assessment. The "Step" only calculates the optimal solution for each task and does not account for randomly appearing advertisements or longer correct paths. It is worth mentioning that the benchmark statistics in the third section include the training set. collects static trajectories via breadth-first and depth-first algorithms, merges nodes and edges by computing screenshot similarity, and automatically labels bounding boxes as transition conditions. Manual validation is integrated throughout to ensure data quality.

<table><tr><td>Dataset</td><td>#Tasks</td><td>#Step</td><td>#Apps</td><td>Interaction</td><td>Multiple Solution</td><td>Atom-Cap</td></tr><tr><td>AndroidLab (Xu et al., 2024)</td><td>138</td><td>8.5</td><td>9</td><td>✓</td><td>✓</td><td>✘</td></tr><tr><td>AndroidWorld (Rawles et al., 2024)</td><td>116</td><td>-</td><td>20</td><td>✓</td><td>✓</td><td>✘</td></tr><tr><td>Mobile-Env (Zhang et al., 2023)</td><td>150</td><td>-</td><td>-</td><td>✓</td><td>✓</td><td>✘</td></tr><tr><td>MobileAgentBench (Wang et al., 2024c)</td><td>100</td><td>-</td><td>10</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>UI-NEXUS (Guo et al., 2025)</td><td>100</td><td>14.05</td><td>50</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>Mobile-Eval (Wang et al., 2024b)</td><td>33</td><td>5.5</td><td>10</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>Mobile-Eval-E (Wang et al., 2025b)</td><td>25</td><td>14.56</td><td>15</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>SPA-BENCH (Chen et al., 2024a)</td><td>340</td><td>8.2</td><td>66</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>MVISU-Bench (Huang et al., 2025)</td><td>404</td><td>-</td><td>137</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>AppAgent (Zhang et al., 2025)</td><td>50</td><td>-</td><td>10</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>AndroidControl (Li et al., 2024a)</td><td>15283</td><td>4.8</td><td>833</td><td>✘</td><td>✘</td><td>✘</td></tr><tr><td>GUI-Odyssey (Lu et al., 2024)</td><td>7735</td><td>15.4</td><td>201</td><td>✘</td><td>✘</td><td>✘</td></tr><tr><td>Meta-GUI (Sun et al., 2022)</td><td>1125</td><td>4.3</td><td>-</td><td>✘</td><td>✘</td><td>✘</td></tr><tr><td>Mobile-Bench-v2 (Xu et al., 2025)</td><td>12,856</td><td>7.28</td><td>49</td><td>✘</td><td>✓</td><td>✘</td></tr><tr><td>ColorBench</td><td>175</td><td>13.13+</td><td>21</td><td>✓</td><td>✓</td><td>✓</td></tr></table>

The graph-structured benchmark effectively resolves the aforementioned limitations present in the existing benchmarks. As shown in Figure 2, it mainly has the following key advantages: 1) Multi-Solution Evaluation: It supports the evaluation of multiple valid solutions for a single task; 2) Enhanced Collaboration: It enables agents to fully utilize collaborative abilities such as reflection and backtracking, thereby preventing the underestimation of model capabilities due to environmental constraints; 3) Atomic Assessment: By setting milestones for subtasks, it enables stable and automated evaluation as well as fine-grained assessment of atomic task capabilities for diagnosing atomic-task level weaknesses that lead to agent failures; 4) Controllable Environment: It provides a secure, stable, and controllable testing environment that remains statically reliable while supporting rich interactions. Together, these properties allow the framework to effectively bridge the gap between offline and online evaluation.

We carefully select three commonly used closed-source models and ten open-source models, alongside two customized baselines, for a comprehensive evaluation on ColorBench. Through extensive experiments, we systematically reveal the limitations of existing agents in tackling complex long-horizon tasks and diagnose their underlying causes. Based on these findings, we provide concrete recommendations for developing more capable agents suitable for such challenging scenarios.

Overall, the primary contributions of our work can be summarized as follows:

- Graph-Structured Benchmark. We propose a benchmark framework of graph structure that bridges the discrepancy between offline and online tests We design an effective construction methodology that balances quality and efficiency and verify the significance and feasibility of the graph paradigm by statistical experiments.

- ColorBench for Complex Long-Horizon Task. It is the first comprehensive graph-structured mobile agent benchmark for complex long-horizon tasks. By extending step-level evaluation to the atomic-task level, we assess and pinpoint models' weaknesses in atomic capabilities through evaluations within complete complex tasks.

- Pathways and Insights for ColorBench Solutions. We conduct systematic experiments and analyses on ColorBench and offer improvement directions and insights for future complex long-horizon task solutions based on experimental findings.

## 2 Related Work

### 2.1 Mobile GUI Agent Benchmark

Traditional GUI agent evaluation methods can be broadly categorized into two types: offline static evaluation based on trajectory chains, and end-to-end online dynamic evaluation (Rawles et al., 2023; Sun et al., 2022; Li et al., 2024a). Dynamic evaluation can be further subdivided into virtual sandbox environments and real devices (Rawles et al., 2024; Xu et al., 2024; Zhang et al., 2023). For applications requiring login credentials, there is no fundamental difference in their impact on the real world. Static evaluation is straightforward and comprehensive. Using image-answer pairs alone, benchmarks can be created for step tasks such as UI recognition, grounding capabilities(Deka et al., 2017; Wang et al., 2021; Li et al., 2020), page transition relationship identification (Chen et al., 2024b), and simple atomic tasks (Li et al., 2024a; Deng et al., 2023; Taleby Ahvanooey et al., 2016; Zhang et al., 2024b; Wu et al., 2025), as well as demonstration learning capabilities (Liu et al., 2025a). However, for complex long-horizon tasks composed of multiple atomic tasks or even spanning multiple apps, multiple solutions often exist (Guo et al., 2025; Lu et al., 2024; Wang et al., 2025b). Static datasets, constrained by their single standard answer, struggle to accurately assess real-world model capabilities (Wang et al., 2025a). Existing GUI benchmarks for complex long-horizon tasks across various platforms and scenarios typically employ dynamic testing in real or simulated environments (Liu et al., 2025b; Ye et al., 2025b). Unlike the web environment, mobile GUI operations lack accessible URLs, and inherent issues with mobile applications make dynamic benchmarks unsuitable as unified evaluation standards. Addressing this challenge is crucial for advancing mobile GUI agent deployment. Therefore, we innovatively propose a graph-structured benchmark for complex long-horizon tasks on mobile devices, which bridges the offline-online test discrepancy with enhanced stability compared to dynamic evaluation, addressing the shortcomings of this research. Table 1 compares ColorBench with other mobile GUI agent benchmarks.

### 2.2 Comparison with Prior Mobile Agent Graph

The graph-structured benchmark and the User Interaction Transition Graph (UTG) share a similar high-level structure but differ critically in their granularity and purpose. To meet benchmarking standards, the graph-structured benchmark constructs the topology of state transitions, whereas the UTG defines them based on page relationships. Consequently, the UTGs are primarily used for knowledge exploration during the execution of a single task (Wen et al., 2024; Fan et al., 2025), representing a subset of the evaluable environment. In contrast, the graph-structured benchmark is designed for evaluating multiple tasks. It aggregates all possible execution paths, including error states, into a comprehensive evaluation environment that serves as a subset of the real world. As such, the benchmark constitutes a superset of the exploration space covered by any single UTG.

Additionally, graphs have extensive application in other aspects of GUI agents. For task generation and evaluation, OmniBench automates the synthesis of complex tasks by combining subtasks based on graphs and utilizes these subtask structure graphs for evaluation (Li et al., 2024b). MobiFlow models tasks as directed acyclic graphs for evaluation purpose(Bera et al., 2018). For model training, MobileM3 connects pages collected through breadth-first exploration into graphs to learn UI transition relationships (Wu et al., 2024a). Methodologically, Xplore-Agent constructs graph-structured page relationships during exploration (Sun et al., 2025), while PG-Agent pre-builts page-relationship graphs of specific applications for RAG to augment knowledge (Chen et al., 2025b). These works demonstrate the advantages of graphs in GUI tasks: precise action transition relationships, natural page navigation modeling, and a clear, controllable global perspective. Mobile-Bench-v2 utilizes MobileM3's graphs to generate multi-path tasks, introducing a graph-structured benchmark for the first time, but it lacks systematic explanations (Xu et al., 2025). Our work fills this research gap and proposes a feasible methodology for building a graph-structured benchmark from scratch.

## 3 Graph-Structured Benchmark

### 3.1 Definition of Graph-Structured Benchmark

The graph abstracts the finite states of real mobile environment into a strongly connected directed graph $G = \left( {V,E}\right)$ ,comprising two main components:

- $V = \left\{  {{N}_{1}\left\lbrack  {{p}_{1},{p}_{2},\ldots }\right\rbrack  ,{N}_{2}\left\lbrack  \ldots \right\rbrack  ,\ldots }\right\}$ : The node set,modeling all screen states ${N}_{i}$ that may be encountered in actual evaluations. From the HOME page to detailed app screens and inter-app navigation pages, different screen states are modeled as distinct nodes at the granularity of action transitions. For screens with random elements,they are treated as different screens ${p}_{1},{p}_{2},\ldots$ within a unified node ${N}_{i}$ when recommended content does not involve action transitions. This effectively models the randomness observed in real-device testing.

- $E = \left\{  {\left( {{N}_{i},{N}_{j},a}\right) ,\ldots }\right\}$ : The edge set models the action transition relationships between screen states at the node granularity. Each edge corresponds to a specific action a transitioning from one node ${N}_{i}$ to another ${N}_{j}$ ,such as "click bbox[x1,y1,x2,y2]" or "type[TEXT]",aligning one-to-one with the action space used in real-device evaluations.

It integrates all tasks into a single, invariant graph structure, simplifying the evaluation pipeline. When evaluating, each task originates from a unified "HOME" node. The agent determines actions based on user queries and the current screen, then transitions to the next screen via actions within the graph.

### 3.2 Advantages of Graph-Structured Benchmark

In this section, we detail the advantages of the graph-structured benchmark over the other two types of evaluation paradigms. Compared to static evaluation, the graph structure inherently simulates page transitions from a real mobile environment, which allows it to naturally support multi-path solutions to a problem. It is designed to encompass not only the optimal path but also sub-optimal ones and, importantly, recovery paths where the agent corrects its errors. Therefore, graphs offer superior fault tolerance, allowing models to reflect on past errors and return to re-execute without terminating the evaluation. This enables the assessment of collaborative intelligence within agent systems. Compared to dynamic evaluation, the static data construction ensures a stable, controllable, and reproducible testing process. It enables automated evaluation through predefined nodes, which represent task completion, while also avoiding security risks such as unintended payments or information leaks associated with real-world third-party app logins.

In terms of evaluation metrics, this approach breaks through the limitations of static step-level and dynamic result-level assessments. By setting sub-task milestones on the graph, it avoids misjudgments of single-step execution details. This enables effective evaluation of completion rates for complex, long-horizon tasks composed of multiple atomic tasks, while pinpointing the completion status of subtasks. Consequently, it achieves evaluation at the atomic task capability level, allowing targeted identification of an agent's shortcomings in specific tasks. Furthermore, to simulate real-world environmental variability, multiple images of functionally identical pages are retained. This approach replicates random effects in practical scenarios, striking a balance between the stability of static evaluation and the randomness of dynamic testing.

<img src="https://cdn.noedgeai.com/bo_d4nfss77aajc73frs9l0_5.jpg?x=261&y=211&w=1279&h=800&r=0"/>

Figure 2: Advantage of graph-structured benchmark. Our constructed ColorBench possesses these advantages. The upper-left corner demonstrates multiple solutions to one problem, supporting diverse execution paths for the same task. The lower-left corner illustrates how the graph's strongly connected structure effectively enables model collaboration capabilities such as backtracking and reflection. The upper-right corner showcases the graph's inherent suitability for complex long-horizon tasks and its support for atomic capability evaluation. The lower-right corner highlights the graph's superior controllability and reproducibility compared to dynamic evaluation.

## 4 ColorBench

### 4.1 Overview and Statistical Analysis

To construct complex, long-horizon tasks with multiple valid solutions, we employed partially ambiguous instructions, similar to those in Mobile-Bench-v2 (Xu et al., 2025), to create a finite and controlled set of correct solutions. Our resulting benchmark, ColorBench, comprises 175 such tasks -101 cross-app and 74 single-app. After excluding random advertisements, pop-ups, and sub-optimal paths, the average optimal path length exceeds 13.13 steps. Representative tasks include price comparison and multi-content sharing (cross-app), as well as food ordering and specific content queries (single-app). The diverse combinations and strong inter-dependencies among subtasks mean each one critically influences the final outcome, thus realistically simulating real-world interactions and narrowing the gap between offline and online evaluation. From the commonalities of these subtasks, we have summarized 15 atomic task capabilities. Figure 3 displays the statistics of ColorBench, and the supported action space is detailed in Appendix A.1.

### 4.2 Dataset Construction

We design a graph construction strategy for benchmarking that balances quality and automation. The strategy comprises two primary phases: trajectory collection and graph merging. The trajectory collection phase aims to fully capture UI elements with high interaction probabilities on key pages within evaluation paths. It integrates both breadth-based and depth-based trajectory collection methods, simultaneously covering high-frequency short-distance tasks and complex long-distance tasks. The graph merging phase constructs an interactive evaluation environment from the collected high-quality trajectories. This phase employs semantic and visual criteria, utilizing action transitions as the key basis for distinguishing different page nodes, thereby accurately identifying effective interaction boundaries between interface states. Subsequently, we employ models to automatically annotate complete UI bounding boxes, with the detailed annotation method provided in Appendix A.2. All aforementioned processes underwent final manual quality inspection and refinement. Details regarding the models used in the automation strategy, data collection scales, and cleaning results are introduced in Appendix A.3. Throughout the construction process, we prioritized the fidelity of the graph in simulating dynamic environments. The following subsections will elaborate on the complete construction process of ColorBench.

<img src="https://cdn.noedgeai.com/bo_d4nfss77aajc73frs9l0_6.jpg?x=250&y=226&w=1301&h=405&r=0"/>

Figure 3: ColorBench statistics. The results do not include actions "navigate back", "navigate home", or "open app".

#### 4.2.1 Trajectory Collection

Step 1: Breadth-First Search (BFS) BFS trajectory collection focuses on capturing interactions across shallow-level pages within each application. These pages exhibit high frequency and short-distance characteristics in daily user operations, ensuring the dataset covers the most common trajectories within the target app. Specifically, we employ two VLMs: VLM $A$ handles the identification of interactive UI elements. Upon entering a new shallow-level page within the target application, VLM $A$ first identifies and stores all interactive UI elements on the page. Subsequently, VLM $B$ performs interaction operations on each identified UI element and records the resulting trajectory. This dual-model collaboration ensures accuracy and consistency in the collection process while reducing manual intervention.

Step 2: Depth-First Search (DFS) DFS trajectory collection targets complex long-horizon tasks that require sustained interaction across multiple applications. Automation tools struggle to capture the complete trajectory of such tasks. To address this, we designed a trajectory collection method based on "screenshot-based action completion" and "branch trajectory supplementation." This approach effectively reduces the difficulty of collecting complex trajectories while ensuring data integrity and authenticity.

Screenshot-Based Action Completion. We manually capture screenshots of each step in long-horizon tasks, then use VLM to fill in missing actions between screenshots, thereby reconstructing the complete task trajectory. The process involves three steps: 1) Task Definition and Screenshot Capture: An expert annotation team designs a set of typical, complex long-horizon tasks supporting multiple paths. For each task, trained operators manually execute it on real mobile devices, collecting multiple task completion trajectories and capturing screenshots corresponding to each action. 2) Trajectory Construction: For the acquired sequence of captured screenshots $\left( {{S}_{1},{S}_{2},\ldots ,{S}_{n}}\right)$ ,each input consecutive screenshots ${S}_{i}$ and ${S}_{i + 1}$ . The action completion model predicts all correct interaction actions ${A}_{i}$ . Subsequently,the sequences $\left( {{S}_{1} \rightarrow  {A}_{1} \rightarrow  {S}_{2} \rightarrow  {A}_{2} \rightarrow  \cdots  \rightarrow  {A}_{n - 1} \rightarrow  {S}_{n}}\right)$ are combined to form the complete trajectory. 3) Manual Verification: After the model predicts the action, the research team manually verifies all trajectories to ensure their accuracy.

Branch Trajectory Supplementation. The trajectories obtained through the above steps represent only multiple possible operational paths when humans execute complex long-horizon tasks. Additionally, there may be some correct paths with high model click probabilities (this portion is extremely rare) and erroneous options. These branching paths constitute a crucial component of the model's potential click space, enabling a more realistic simulation of real scenarios. Therefore, we supplement the existing trajectories through the following steps: 1) Let the model perform the same tasks and compare its execution trajectories with the annotated trajectories; 2) Treat other erroneous regions clicked by the model as high-probability potential trajectory spaces, and construct corresponding branch trajectories to supplement the dataset.

#### 4.2.2 Merge Trajectories into a Graph

The graph merging strategy aims to automatically construct evaluation-ready interaction graphs from quality interaction traces. It distinguishes page nodes by identifying action transitions that cause substantive interface state changes. To achieve this goal, we design a two-stage filtering pipeline based on VLMs, with the following workflow:

Semantics-Based Coarse Screening. First, we employ large language models to generate semantic descriptions for each screenshot, covering page layout and core functionalities. Subsequently, we compute embedding vectors for all descriptive texts and filter image pairs with semantic similarity exceeding a preset threshold through pairwise comparisons. This step efficiently narrows the candidate pool, focusing on interfaces that are highly similar in both visual appearance and functionality.

Action Transition-Based Node Discrimination. For image pairs that pass semantic filtering, we feed them into a VLM for fine-grained discrimination. Since candidate pairs are already semantically highly similar, the model's core discrimination criterion shifts from content to whether an "action transition" that drives state change exists between them. For instance, interactive operations like "Click to Follow" or "Add to Cart" may not significantly alter page layouts but trigger substantive updates to interface states, such as button status changes and product count updates. The model classifies image pairs exhibiting such action transitions as belonging to different page nodes, otherwise the same node.

Manual Verification and Graph Enhancement. After automated merging, we introduced a manual verification process to further enhance the quality of the graph. This step primarily accomplishes two tasks: first, it corrects node classifications that may have been misjudged during automation; second, it supplements annotations for executable common actions across different state nodes on the same page. Ultimately, we obtained a verified, fully annotated graph that serves as the evaluation environment for ColorBench.

## 5 Experiments

In this section, we conduct extensive experiments to answer the following research questions:

RQ1 Why graph-structured benchmark are necessary and feasible?

RQ2 How do existing models perform on complex long-horizon tasks?

RQ3 What capabilities do existing models lack in complex long-horizon tasks?

RQ4 Which modules are essential for complex long-horizon tasks?

### 5.1 Experiment Setup

#### 5.1.1 Evaluation Metrics

To evaluate the performance of models on ColorBench, we leverage SR (success rate) and CR (completion rate) (Liu et al., 2025b; Zhang et al., 2024a), as well as Atomic Task Capability (AC), which we propose to diagnose capability-level weaknesses that lead to agent failures. In complex long-horizon tasks, each milestone represents the completion of a subtask, and the number of milestones reached reflects the CR of the task. When all milestones are reached, the task is considered successful; otherwise, it is considered a failure. By extracting common characteristics across all milestone points, we categorize them into 15 atomic task capabilities. For each atomic task, the AC is calculated as:

$$
\mathrm{{AC}} = \frac{\# \text{ Successfully Reached milestones of an Atomic Task Executed During Evaluation }}{\# \text{ the Atomic Task Executed During Evaluation }}.
$$

The denominator excludes subtasks that were never executed due to failure in preceding subtasks.

#### 5.1.2 Baselines

We evaluated Colorbench on open-source models and closed-source models commonly used in mobile GUI agents. Common open-source models for GUI tasks can be categorized into two types: general VLMs such as the Qwen-VL (Bai et al., 2023; Team, 2025) series, and specialized foundation models fine-tuned on extensive GUI data based on general VLMs, including UI-TARS (Qin et al., 2025), OS-Atlas (Wu et al., 2024b), and GUI-OWL series (Ye et al., 2025a). Turning to closed-source VLMs, we carefully selected three models: Qwen-VL Max (Team, 2023), GLM-4.5V (AI, 2024), and GPT-40 (OpenAI, 2024), which are commonly used in mobile GUI agents. Due to the poor grounding capabilities of closed-source models, we employed Qwen2.5-VL-7b (Bai et al., 2025) as the grounding model to identify the target UI element for click and long-press actions.

Moreover, we evaluated two common approaches: 1) we finetuned the base GUI-OWL (Ye et al., 2025a) model using our static Chinese app training dataset to supplement its knowledge of Chinese applications; 2) we designed a simple multi-agent system incorporating planning, reflection, and memory modules. To investigate the function of these modules for solving complex long-horizon tasks, we conducted ablation experiments on them. Comprehensively considering the model's professional and general ability, we selected Qwen2.5-VL-32B (Bai et al., 2025) and GUI-OWL-32B (Ye et al., 2025a) for the ablation study. Due to the page limitation, we further provide implementation details in Appendix B.1.

### 5.2 Significance of Graph Structure (RQ1)

The real-world mobile environment is a vast, strongly connected directed graph with an infinite number of nodes and edges, exhibiting immense temporal randomness simultaneously. Replicating such an environment one-to-one is impractical. Given this, a critical question arises: Can graph-structured evaluation datasets effectively simulate dynamic testing environments to bridge the gap between dynamic and static evaluations?

<img src="https://cdn.noedgeai.com/bo_d4nfss77aajc73frs9l0_8.jpg?x=922&y=1414&w=560&h=349&r=0"/>

Figure 4: Statistics on the number of actions that all experienced models at each node in the graph tend to perform, do not include "navigate back", "navigate home" and "open app" which can be executed on any interface.

To address this question, we conducted two statistical experiments: first, we quantified the potential execution paths for the model operating in a real environment, and second, we compared the outcomes of identical tasks evaluated on a physical device versus on a graph. Specifically, for the first experiment, we analyzed the execution trajectories of all models on ColorBench. By calculating the potential action space at each node (excluding "navigate back" and "home" actions), we determined the number of successor nodes required at that node in the graph. As shown in Figure 4, the maximum value is 17 corresponding to the APP's home page, and the minimum value

Table 2: Performance comparison of closed-source and open-source models on our Col-orBench. SR means task success rate and CR means task completion rate (proportion of completed subtasks). Bold represents optimal performance, while underlining represents suboptimal. The results of atomic capabilities are presented in Table 5 in the appendix.

<table><tr><td rowspan="2">Baseline</td><td rowspan="2">Model</td><td colspan="2">Single APP</td><td colspan="2">Cross APP</td><td colspan="2">Average</td></tr><tr><td>SR(%)</td><td>CR(%)</td><td>SR(%)</td><td>CR(%)</td><td>SR(%)</td><td>CR(%)</td></tr><tr><td rowspan="3">Closed-source Model with Grounding</td><td>GPT-40</td><td>20.27</td><td>27.59</td><td>11.88</td><td>21.53</td><td>15.43</td><td>24.10</td></tr><tr><td>Qwen-VL Max</td><td>20.27</td><td>32.55</td><td>14.85</td><td>29.42</td><td>17.14</td><td>30.74</td></tr><tr><td>GLM-4.5V</td><td>36.49</td><td>56.64</td><td>23.23</td><td>47.72</td><td>28.57</td><td>51.54</td></tr><tr><td rowspan="10">Open-source Model</td><td>OS-Atlas-Pro-7B</td><td>10.81</td><td>17.91</td><td>3.96</td><td>19.87</td><td>6.86</td><td>19.04</td></tr><tr><td>UI-TARS-1.5-7B</td><td>9.46</td><td>16.89</td><td>0.00</td><td>13.88</td><td>4.00</td><td>15.15</td></tr><tr><td>UI-TARS-7B-DPO</td><td>8.11</td><td>12.05</td><td>0.99</td><td>11.82</td><td>4.00</td><td>11.91</td></tr><tr><td>GUI-OWL-7B</td><td>25.68</td><td>39.19</td><td>16.83</td><td>34.16</td><td>20.57</td><td>36.29</td></tr><tr><td>Qwen2.5-VL-7B</td><td>22.97</td><td>34.80</td><td>9.90</td><td>32.57</td><td>15.43</td><td>33.51</td></tr><tr><td>GUI-OWL-32B</td><td>36.49</td><td>47.52</td><td>22.77</td><td>39.31</td><td>28.57</td><td>42.78</td></tr><tr><td>Qwen2.5-VL-32B</td><td>24.32</td><td>36.49</td><td>11.88</td><td>29.62</td><td>17.14</td><td>32.53</td></tr><tr><td>UI-TARS-72B-DPO</td><td>33.78</td><td>46.85</td><td>23.76</td><td>45.40</td><td>28.00</td><td>46.01</td></tr><tr><td>Qwen2.5-VL-72B</td><td>21.62</td><td>34.23</td><td>16.83</td><td>32.38</td><td>18.86</td><td>33.16</td></tr><tr><td>Qwen3-VL-235B-A22B-Instruct</td><td>35.14</td><td>49.66</td><td>27.72</td><td>43.42</td><td>30.86</td><td>46.06</td></tr><tr><td rowspan="2">Fine-tuned with ColorBench-train</td><td>GUI-OWL-7B-RL</td><td>31.08</td><td>46.62</td><td>19.80</td><td>41.79</td><td>24.57</td><td>43.83</td></tr><tr><td>GUI-OWL-32B-RL</td><td>40.54</td><td>53.72</td><td>26.73</td><td>42.25</td><td>32.57</td><td>47.10</td></tr><tr><td rowspan="2">Multi-Agent System</td><td>GUI-OWL-32B</td><td>43.24</td><td>55.86</td><td>22.77</td><td>43.40</td><td>31.43</td><td>48.67</td></tr><tr><td>Qwen2.5-VL-32B</td><td>40.54</td><td>55.32</td><td>20.79</td><td>40.18</td><td>29.14</td><td>46.58</td></tr></table>

is 0 , indicating a task endpoint. The average value is 1.9 , reflecting the constructibility of the graph. Consequently, we have demonstrated that the variety of trajectories and screen states that models experienced in real-world evaluations is limited, which validates both the conceptual soundness and practical feasibility of our graph-structured benchmark.

For the second experiment, we selected a subset of executable tasks from ColorBench for real-device testing, adapting to the dynamically changing real-world information. Partial results are presented in Table 6 in the appendix. These examples demonstrate that ColorBench covers the primary causes of task failure in real-device evaluations. Therefore, we can assess the authenticity of the graph and demonstrate that a graph-structured benchmark can effectively reduce the gap between offline and online assessments. In fact, when constructing the graph, some of the erroneous trajectories supported by the graph originate from the model's actual mistakes, which essentially creates a customized testing environment for the model and further ensures the validity of the graph.

### 5.3 Overall Performance (RQ2)

Table 2 presents the main experimental results. For closed-source models, we observe that:

- GLM-4.5V significantly outperforms both Qwen-VL Max and GPT-4o. Although its SR is not the highest, its notably high CR demonstrates stronger capabilities in comprehending and planning for complex, long-horizon tasks. Furthermore, Table 5 in the appendix indicates that GLM-4.5V can actively memorize essential historical information, an ability that surpasses most compared models. In contrast, GPT-40 underperforms due to a lack of training on relevant operational knowledge, while Qwen-VL Max exhibits deficiencies in decomposing and planning complex tasks. Additionally, all three models demonstrate generally weak UI grounding abilities.

- A lack of knowledge about mobile phone operation severely impedes task execution. For instance, despite its powerful multi-modal understanding, GPT-40's unfamiliarity with basic mobile actions (i.e., copying, pasting, and sharing) often results in it providing only high-level plans without being able to execute the specific operational steps required.

As for open-source models, we can draw the following observations from Table 2:

- Models with larger parameter scales generally achieve better performance on complex long-horizon tasks. For instance, both the GUI-OWL and Qwen series show improved results as the model size increases, and their largest models, alongside GLM-4.5V, are the only three capable of actively memorizing essential historical information, as shown in Table 5 in the appendix. This stems from their stronger general capabilities, enabling better comprehension and planning for complex tasks.

- Specialized foundation models do not necessarily outperform general-purpose models. Fine-tuning can easily lead to overfitting, reducing their ability to generalize to complex long-horizon tasks, as evidenced by the results of UI-TARS and OS-Atlas-Pro (7B). In contrast, the strong performance of the GUI-OWL series indicates that specialized foundation models remain promising and warrant further exploration.

- The foundational mobile GUI abilities of these models remain unstable. Although the open-source models have been trained to varying degrees on mobile GUI operations, we still observe issues such as execution step errors, recognition and grounding deviations, and instruction-following failures. Therefore, basic mobile operation capabilities still require improvement. Comparisons with closed-source models further indicate that domain-specific knowledge forms a critical foundation for successfully accomplishing these tasks.

As shown in Table 5, models fine-tuned with app-specific data demonstrate higher task accuracy due to their improved familiarity with application layouts, content, and functionality. In multi-agent systems, decomposing complex tasks into structured modules with enforced execution further enhances performance reliability. Additionally, the appendix records each model's atomic capability scores, which directly influence the success or failure of the task. Further analysis of these atomic capabilities is provided in Appendix B.2.

### 5.4 Key Capabilities (RQ3)

We manually examined the task execution logs of each model and analyzed their performance in essential high-level cognitive capabilities (those requiring reasoning). We identified the following common issues in existing models: incomplete decomposition of complex long-horizon tasks, vague memory of essential historical information, and a lack of effective reflection on recurring erroneous actions.

For example, in the task "Go to Xiaohongshu to search for 'AI Agent', share the paper on 'optimized memory management' with WeChat Contact 1, then go to Baidu Arxiv to search and download the PDF of that paper, and finally send it to Contact 1", some models incorrectly considered the task complete after executing the "share with contact" subtask. Other models reached the Arxiv website but failed to retain the specific paper title identified earlier on Xiaohongshu, causing the task to fall into an infinite loop. Most models were unable to recognize their own errors in these situations.

In conclusion, to handle complex long-horizon tasks, models require not only basic GUI capabilities but also the following key advanced competencies: the ability to comprehend and analyze complex task requirements, the capacity to decompose long-horizon tasks into structured subtasks, the capability to actively memorize and recall critical information across tasks, and the faculty for reflection and self-correction. Therefore, the accurate execution of atomic tasks is the foundation for solving complex long-horizon tasks, while high-quality planning, reflection, and memory are the critical support that effectively string these atomic tasks together and ensure their stable execution.

### 5.5 Ablation Study (RQ4)

Ablation studies on individual modules reveal their distinct contributions to solving complex long-horizon tasks. Table 3, while each module improves overall performance, the extent of this improvement varies across models. For instance, introducing only the reflection module brings substantial gains for Qwen-2.5-VL-32B but limited improvement for GUI-Owl-32B, reflecting differences in their inherent capabilities.

Table 3: Ablation experiment results of the multi-agent system modules on our Color-Bench. The numbers in the footnote indicate changes relative to the baseline.

<table><tr><td rowspan="2">Base Model</td><td colspan="3">Module</td><td colspan="2">Single APP</td><td colspan="2">Cross APP</td><td colspan="2">Average</td></tr><tr><td>Plan</td><td>Reflection</td><td>Memory</td><td>SR(%)</td><td>CR(%)</td><td>SR(%)</td><td>CR(%)</td><td>SR(%)</td><td>CR(%)</td></tr><tr><td rowspan="6">Qwen2.5-VL (32B)</td><td>✘</td><td>✘</td><td>✘</td><td>24.32</td><td>36.49</td><td>11.88</td><td>29.62</td><td>17.14</td><td>32.53</td></tr><tr><td>✓</td><td>✘</td><td>✘</td><td>32.43+8.11</td><td>46.28+9.79</td><td>17.82 +5.94</td><td>35.54+5.92</td><td>24.00 +6.86</td><td>40.09+7.56</td></tr><tr><td>✘</td><td>✓</td><td>✘</td><td>37.84+13.52</td><td>48.65+12.16</td><td>15.84 +3.96</td><td>33.48+3.86</td><td>25.14+8.00</td><td>39.90+7.37</td></tr><tr><td>✘</td><td>✘</td><td>✓</td><td>31.08+6.76</td><td>47.52+11.03</td><td>18.81 +6.93</td><td>35.30+5.68</td><td>24.00+6.86</td><td>40.47+7.94</td></tr><tr><td>✓</td><td>✓</td><td>✘</td><td>35.14+10.82</td><td>51.35+14.86</td><td>19.80+7.92</td><td>39.01+9.39</td><td>26.29+9.15</td><td>44.23+11.70</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>40.54</td><td>55.32+18.83</td><td>20.79+8.91</td><td>40.18+10.56</td><td>29.14+12.00</td><td>46.58+14.05</td></tr><tr><td rowspan="6">GUI-OWL (32B)</td><td>✘</td><td>✘</td><td>✘</td><td>36.49</td><td>47.52</td><td>22.77</td><td>39.31</td><td>28.57</td><td>42.78</td></tr><tr><td>✓</td><td>✘</td><td>✘</td><td>${35.14}_{-{1.35}}$</td><td>49.66+2.14</td><td>22.770.00</td><td>46.03+6.72</td><td>28.00 -0.57</td><td>47.56+4.78</td></tr><tr><td>✘</td><td>✓</td><td>✘</td><td>${37.84}_{+{1.35}}$</td><td>51.35+3.83</td><td>27.55+4.78</td><td>46.26+6.95</td><td>31.43+2.86</td><td>48.45+5.67</td></tr><tr><td>✘</td><td>✘</td><td>✓</td><td>36.490.00</td><td>49.55+2.03</td><td>25.74+2.97</td><td>43.86+4.55</td><td>30.29+1.72</td><td>46.27+3.49</td></tr><tr><td>✓</td><td>✓</td><td>✘</td><td>41.89+5.40</td><td>51.91+4.39</td><td>22.770.00</td><td>44.82+5.51</td><td>30.86+2.29</td><td>47.82+5.04</td></tr><tr><td>✓</td><td>✓</td><td>✓</td><td>43.24+6.75</td><td>55.86+8.34</td><td>22.770.00</td><td>43.40+4.09</td><td>31.43+2.86</td><td>48.67+5.89</td></tr></table>

We also observed nuanced interactions between modules. When Qwen-2.5-VL-32B evolves from using only reflection to incorporating both reflection and planning, SR on single-app tasks decreases. Meanwhile, CR increases and remains higher than when planning alone is used. Conversely, GUI-Owl-32B's CR on cross-app tasks declines as more modules are added, though it still exceeds the baseline. This highlights instability introduced by multi-agent systems. Overly complex module combinations can cause erroneous coupling and error accumulation. As a result, tasks that were initially solvable may become unresolvable.

We attribute this to imbalanced capability distributions, where weaker modules can inhibit stronger ones. This occurs because increased low-quality information raises systemic entropy, while high-quality signals are underutilized. Therefore, multi-agent approaches to long-horizon tasks require not only individual module effectiveness but also balanced integration - otherwise, they risk degrading performance.

## 6 Conclusion

To bridge the gap between offline static evaluation and online dynamic evaluation for Mobile GUI Agents, we propose a graph-structured benchmark with an effective construction methodology. Based on this framework, we develop ColorBench, a benchmark specifically designed for complex long-horizon tasks that balances static stability and dynamic randomness through finite-state modeling of a dynamic environment. ColorBench supports multiple valid solutions for individual tasks and extends evaluation from step-level and result-level metrics to atomic task capability assessment, enabling effective diagnosis of model deficiencies. We extensively evaluate ColorBench across numerous models, validating the necessity and feasibility of the graph-based benchmark. Based on experimental results, we analyze limitations of existing models and provide concrete suggestions and potential approaches to enhance agents' capabilities in solving complex long-horizon problems.

## References

Zhipu AI. Glm-4.5v technical report. https://zhipu.ai/en/blog/glm-4-5v, 2024. Accessed: 2024-12-19.

Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023. URL https://arxiv.org/abs/2308.12966.

Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.

Samaresh Bera, Sudip Misra, and Mohammad S Obaidat. Mobi-flow: Mobility-aware adaptive flow-rule placement in software-defined access network. IEEE Transactions on Mobile Computing, 18 (8):1831-1842, 2018.

Gongwei Chen, Xurui Zhou, Rui Shao, Yibo Lyu, Kaiwen Zhou, Shuai Wang, Wentao Li, Yinchuan Li, Zhongang Qi, and Liqiang Nie. Less is more: Empowering gui agent with context-aware simplification. arXiv preprint arXiv:2507.03730, 2025a.

Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, et al. Spa-bench: A comprehensive benchmark for smartphone agent evaluation. In NeurIPS 2024 Workshop on Open-World Agents, 2024a.

Weizhi Chen, Ziwei Wang, Leyang Yang, Sheng Zhou, Xiaoxuan Tang, Jiajun Bu, Yong Li, and Wei Jiang. Pg-agent: An agent powered by page graph. arXiv preprint arXiv:2509.03536, 2025b.

Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024b.

Yurun Chen, Xavier Hu, Yuhan Liu, Keting Yin, Juncheng Li, Zhuosheng Zhang, and Shengyu Zhang. Harmonyguard: Toward safety and utility in web agents via adaptive policy enhancement and dual-objective optimization. arXiv preprint arXiv:2508.04010, 2025c.

Pengzhou Cheng, Zheng Wu, Zongru Wu, Aston Zhang, Zhuosheng Zhang, and Gongshen Liu. Os-kairos: Adaptive interaction for mllm-powered gui agents. arXiv preprint arXiv:2503.16465, 2025.

Gaole Dai, Shiqi Jiang, Ting Cao, Yuanchun Li, Yuqing Yang, Rui Tan, Mo Li, and Lili Qiu. Advancing mobile gui agents: A verifier-driven approach to practical deployment. arXiv preprint arXiv:2503.15937, 2025.

Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: A mobile app dataset for building data-driven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology, pp. 845-854, 2017.

Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 36:28091-28114, 2023.

Yue Fan, Handong Zhao, Ruiyi Zhang, Yu Shen, Xin Eric Wang, and Gang Wu. Gui-bee: Align gui action grounding to novel environments via autonomous exploration. arXiv preprint arXiv:2501.13896, 2025.

Yuan Guo, Tingjia Miao, Zheng Wu, Pengzhou Cheng, Ming Zhou, and Zhuosheng Zhang. Atomic-to-compositional generalization for mobile agents with a new benchmark and scheduling system. arXiv preprint arXiv:2506.08972, 2025.

Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14281-14290, 2024.

Yongxiang Hu, Xuan Wang, Yingchuan Wang, Yu Zhang, Shiyu Guo, Chaoyi Chen, Xin Wang, and Yangfan Zhou. Auitestagent: Automatic requirements oriented gui function testing. arXiv preprint arXiv:2407.09018, 2024.

Zeyu Huang, Juyuan Wang, Longfeng Chen, Boyi Xiao, Leng Cai, Yawen Zeng, and Jin Xu. Mvisu-bench: Benchmarking mobile agents for real-world tasks by multi-app, vague, interactive, single-app and unethical instructions. arXiv preprint arXiv:2508.09057, 2025.

Wenjia Jiang, Yangyang Zhuang, Chenxi Song, Xu Yang, Joey Tianyi Zhou, and Chi Zhang. Ap-pagentx: Evolving gui agents as proficient smartphone users. arXiv preprint arXiv:2503.02268, 2025.

Hongxin Li, Jingfan Chen, Jingran Su, Yuntao Chen, Qing Li, and Zhaoxiang Zhang. Auto-gui: Scaling gui grounding with automatic functionality annotations from llms. arXiv preprint arXiv:2502.01977, 2025.

Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents. arXiv e-prints, pp. arXiv-2406, 2024a.

Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements. arXiv preprint arXiv:2010.04295, 2020.

Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Wang, Jian Yang, et al. Omnibench: Towards the future of universal omni-language models. arXiv preprint arXiv:2409.15272, 2024b.

Guangyi Liu, Pengxiang Zhao, Liang Liu, Zhiming Chen, Yuxiang Chai, Shuai Ren, Hao Wang, Shibo He, and Wenchao Meng. Learnact: Few-shot mobile gui agent with a unified demonstration benchmark. arXiv preprint arXiv:2504.13805, 2025a.

Shunyu Liu, Minghao Liu, Huichi Zhou, Zhenyu Cui, Yang Zhou, Yuhao Zhou, Wendong Fan, Ge Zhang, Jiajun Shi, Weihao Xuan, et al. Verigui: Verifiable long-chain gui dataset. arXiv preprint arXiv:2508.04026, 2025b.

Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: A comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024.

Xinbei Ma, Yiting Wang, Yao Yao, Tongxin Yuan, Aston Zhang, Zhuosheng Zhang, and Hai Zhao. Caution for the environment: Multimodal agents are susceptible to environmental distractions. arXiv preprint arXiv:2408.02544, 2024.

OpenAI. Gpt-40 system card. https://cdn.openai.com/gpt-40-system-card.pdf, 2024.

Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025.

Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. An-droidinthewild: A large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:59708-59728, 2023.

Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: A dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024.

Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. Meta-gui: Towards multi-modal conversational agents on mobile gui. arXiv preprint arXiv:2205.11029, 2022.

Yuchen Sun, Shanhui Zhao, Tao Yu, Hao Wen, Samith Va, Mengwei Xu, Yuanchun Li, and Chongyang Zhang. Gui-xplore: Empowering generalizable gui agents with one exploration. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 19477-19486, 2025.

Milad Taleby Ahvanooey, Hassan Dana Mazraeh, and Seyed Hashem Tabasi. An innovative technique for web text watermarking (aitw). Information Security Journal: A Global Perspective, 25(4-6): 191-196, 2016.

Qwen Team. Qwen-vl max, 2023. URL https://github.com/QwenLM/Qwen-VL.

Qwen Team. Qwen3-vl, 2025. URL https://github.com/QwenLM/Qwen3-VL.

Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words: Automatic mobile ui summarization with multimodal learning. In The 34th Annual ACM Symposium on User Interface Software and Technology, pp. 498-510, 2021.

Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. Advances in Neural Information Processing Systems, 37:2686-2710, 2024a.

Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024b.

Luyuan Wang, Yongyu Deng, Yiwei Zha, Guodong Mao, Qinmin Wang, Tianchen Min, Wei Chen, and Shoufa Chen. Mobileagentbench: An efficient and user-friendly benchmark for mobile llm agents. arXiv preprint arXiv:2406.08184, 2024c.

Weixuan Wang, Dongge Han, Daniel Madrigal Diaz, Jin Xu, Victor Rühle, and Saravan Rajmohan. Odysseybench: Evaluating llm agents on long-horizon complex office application workflows. arXiv preprint arXiv:2508.09124, 2025a.

Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. Mobile-agent-e: Self-evolving mobile assistant for complex tasks. arXiv preprint arXiv:2501.11733, 2025b.

Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Autodroid: Llm-powered task automation in android. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking, pp. 543-557, 2024.

Qinzhuo Wu, Weikai Xu, Wei Liu, Tao Tan, Jianfeng Liu, Ang Li, Jian Luan, Bin Wang, and Shuo Shang. Mobilevlm: A vision-language model for better intra-and inter-ui understanding. arXiv preprint arXiv:2409.14818, 2024a.

Zheng Wu, Heyuan Huang, Yanjia Yang, Yuanyi Song, Xingyu Lou, Wei̇wen Liu, Weinan Zhang, Jun Wang, and Zhuosheng Zhang. Quick on the uptake: Eliciting implicit intents from human demonstrations for personalized mobile-use agents. arXiv preprint arXiv:2508.08645, 2025.

Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: A foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024b.

Weikai Xu, Zhizheng Jiang, Yuxuan Liu, Pengzhi Gao, Wei Liu, Jian Luan, Yuanchun Li, Yunxin Liu, Bin Wang, and Bo An. Mobile-bench-v2: A more realistic and comprehensive benchmark for vlm-based mobile agents. arXiv preprint arXiv:2505.11891, 2025.

Yifan Xu, Xiao Liu, Xueqiao Sun, Siyi Cheng, Hao Yu, Hanyu Lai, Shudan Zhang, Dan Zhang, Jie Tang, and Yuxiao Dong. Androidlab: Training and systematic benchmarking of android autonomous agents. arXiv preprint arXiv:2410.24024, 2024.

Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-agent-v3: Foundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025a.

Suyu Ye, Haojun Shi, Darren Shih, Hyokun Yun, Tanya Roosta, and Tianmin Shu. Realwebassist: A benchmark for long-horizon web assistance with real-world users. arXiv preprint arXiv:2504.10445, 2025b.

Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents: A survey. arXiv preprint arXiv:2411.18279, 2024a.

Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 1-20, 2025.

Danyang Zhang, Zhennan Shen, Rui Xie, Situo Zhang, Tianbao Xie, Zihan Zhao, Siyuan Chen, Lu Chen, Hongshen Xu, Ruisheng Cao, et al. Mobile-env: Building qualified evaluation benchmarks for llm-gui interaction. arXiv preprint arXiv:2305.08144, 2023.

Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents. arXiv preprint arXiv:2403.02713, 2024b.

Zaibin Zhang, Yongting Zhang, Lijun Li, Hongzhi Gao, Lijun Wang, Huchuan Lu, Feng Zhao, Yu Qiao, and Jing Shao. Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety. arXiv preprint arXiv:2401.11880, 2024c.

## A Additional Information of ColorBench

### A.1 Action Space

Our ColorBench includes nine actions, and during evaluation, the outputs of different models need to be aligned to the action space.

Table 4: Action space of ColorBench.

<table><tr><td>Action Type</td><td>Parameter</td></tr><tr><td>click</td><td>coordinate $= \left( {x,y}\right)$</td></tr><tr><td>long press</td><td>coordinate $= \left( {x,y}\right)$</td></tr><tr><td>swipe</td><td>direction $\in  \{$ up,down,left,right $\}$</td></tr><tr><td>type</td><td>content=[TEXT]</td></tr><tr><td>wait</td><td>coordinate= $\left( {x,y}\right)$</td></tr><tr><td>open</td><td>app= app name</td></tr><tr><td>navigate back</td><td>none</td></tr><tr><td>navigate home</td><td>none</td></tr><tr><td>complete</td><td>answer=[TEXT]</td></tr></table>

### A.2 Annotation of Bounding Box

Accurate bounding boxes are crucial for GUIs, as they define the precise spatial scope of interactive UI elements, preventing mapping errors between user actions and irrelevant areas. Therefore, this paper proposes an annotation method combining multiple VLM integration with human verification, divided into three concise steps:

- Step 1: Input actual interaction point coordinates and corresponding images into two VLMs, each generating an interaction region. One VLM outputs a larger bounding box (ensuring complete target coverage), while the other outputs a smaller bounding box (minimizing background inclusion).

- Step 2: A third VLM evaluates the two candidate boxes and selects the optimal one. If neither candidate is satisfactory, a new bounding box is generated.

- Step 3: Domain experts verify the preliminary results from Step 2, correcting errors (e.g., missing target parts, redundant background) and confirming the final annotation results.

### A.3 Supplement on ColorBench Construction

#### A.3.1 Automated Strategy Model Selection

Our automated pipeline leverages specialized models according to their core capabilities. Qwen2.5- VL-72B, chosen for its strong general reasoning, handles UI element identification and interaction during broad-coverage trajectory collection, predicts the correct action between model outputs for deep trajectory action annotation, and generates page state descriptions with visual similarity judgment during graph merging. Page descriptions are encoded by the "models-BAAI-bge-large-zh-v1.5" model for semantic similarity calculation. GUI-OWL-32B, recognized for its exceptional visual grounding capability, is employed exclusively for bounding box annotation.

#### A.3.2 Data Collection and Quality Assurance Results

During the BFS phase, trajectory collection was performed for four individual applications: Weixin, Meituan, Jingdong, Xiaohongshu, yielding 6300 trajectory screenshots. During the DFS phase, an additional 1343 trajectory screenshots for complex long-horizon tasks (including single-app and cross-app ) were collected. Using a graph merging strategy to integrate these trajectories into a unified graph structure, we conducted manual quality control to remove near-duplicate images from identical nodes, retaining only a limited set of representative screenshots. To address coverage gaps in the graph, we manually supplemented 50 screenshots. The final constructed graph contains 1989 carefully curated screenshots.

#### A.3.3 System Prompt in Graph Merging

Figure 5 presents the system prompt template used in our automated graph merging process. Due to page limitations, additional implementation details will be made available in our future open-source release.

## B Experimental Supplement

### B.1 Implementation Details

All open-source models tested in our experiments were deployed using VLLM(0.10.1) on A100 80G hardware. Since each node in the image dataset may contain multiple images, we implemented random returns and set the random seed to 2025. We configured model inference parameters as follows: temperature $= {0.1}$ ,top_k=5,top_p=0.9,max_tokens $= {1024}$ . To ensure experimental fairness,we uniformly employed the open-source prompts provided by each model while preserving their original action spaces and performing additional alignment on outputs. We also designed identical prompts for closed-source models. For these, we supplemented each input with the model's historical output records and captured its current reasoning and actions. We will open-source the project code in the future.

## Prompt 1: Generating Page Description fonttitle

You are a GUI AGENT. Please define in one sentence what page the given mobile screenshot represents, and provide a general description of the page layout, its purpose, and other key information. You should ignore changes in the screenshot caused by time or updates, and ultimately form a brief textual description.

## Prompt 2: Judgement of Same Screen State fonttitle

You are a GUI AGENT. Please ignore changes in the screenshots caused by time, and judge whether the two given images belong to the same page state based on aspects such as page layout, page function, and action relationships.

Notes: 1. Treat all mobile home screen pages as the same page;

2. Ignore changes in the page caused by time and updates to recommended content, and focus only on changes caused by actions;

3. Pay special attention to navigation bar tabs. In the application, different navigation bar tabs indicate different pages;

4. Page state changes caused by actions should be considered different pages. If performing an action on one image is required to turn it into the other image, then these two images represent different page states.

Figure 5: System prompt templates for merging graph. In practical use, given the language of the application, we employed Chinese-language prompts.

### B.2 Result of Atomic Capabilities

Table 5 presents evaluation results for atomic task capabilities across open-source and closed-source models. A "-" in the table indicates the model never executed that subtask, having failed at a preceding subtask, thus rendering the subtask capability unverifiable. Higher scores in the table do not directly reflect the model's capability; rather, it is the poorer results that effectively reveal the causes of task failure. For example, GLM-4.5V exhibits high SR and CR but a "save" score of 0, indicating it cannot perform the save action. UI-TARS-72B-DPO also exhibits high SR and CR, yet its "filter" score is as low as 22.22, indicating this is the critical factor constraining the model's success.

Table 5: Success rate of atomic capabilities of closed-source and open-source models on our ColorBench. All values are in percentage (%). The symbol '-' indicates that the model did not encounter this type of subtask during the evaluation process, usually due to the failure of a preceding task.

<table><tr><td>Model</td><td>follow</td><td>pay</td><td>save</td><td>search</td><td>share</td><td>set</td><td>find</td><td>copy</td><td>filter</td><td>like</td><td>send</td><td>location</td><td>navigation</td><td>others</td><td>memory</td></tr><tr><td>GPT-40</td><td>100.00</td><td>50.00</td><td>50.00</td><td>33.06</td><td>76.47</td><td>84.62</td><td>56.67</td><td>0.00</td><td>18.75</td><td>80.00</td><td>25.00</td><td>25.00</td><td>0.00</td><td>62.50</td><td>0.00</td></tr><tr><td>Qwen-VL Max</td><td>40</td><td>33.33</td><td>16.67</td><td>43.8</td><td>70.37</td><td>83.33</td><td>60.56</td><td>33.33</td><td>36.36</td><td>80</td><td>50.00</td><td>25.00</td><td>-</td><td>68.57</td><td>0.00</td></tr><tr><td>GLM-4.5V</td><td>90.91</td><td>55.56</td><td>0.00</td><td>72.95</td><td>90.62</td><td>75.00</td><td>76.34</td><td>50.00</td><td>26.32</td><td>81.82</td><td>77.78</td><td>66.67</td><td>75.00</td><td>60.00</td><td>45.45</td></tr><tr><td>OS-Atlas-Pro-7B</td><td>100.00</td><td>0.00</td><td>75.00</td><td>43.22</td><td>35.71</td><td>66.67</td><td>35.94</td><td>0.00</td><td>0.00</td><td>71.43</td><td>25.00</td><td>40.00</td><td>100.00</td><td>26.09</td><td>0.00</td></tr><tr><td>UI-TARS-1.5-7B</td><td>50.00</td><td>0.00</td><td>33.33</td><td>30.25</td><td>7.69</td><td>40.00</td><td>37.93</td><td>100.00</td><td>20.00</td><td>100.00</td><td>0.00</td><td>20.00</td><td>0.00</td><td>40.00</td><td>0.00</td></tr><tr><td>UI-TARS-7B-DPO</td><td>50.00</td><td>0.00</td><td>50.00</td><td>24.58</td><td>37.50</td><td>66.67</td><td>25.49</td><td>0.00</td><td>22.22</td><td>71.43</td><td>0.00</td><td>0.00</td><td>-</td><td>34.78</td><td>0.00</td></tr><tr><td>GUI-OWL-7B</td><td>85.71</td><td>60.00</td><td>25.00</td><td>57.85</td><td>88.46</td><td>69.23</td><td>62.67</td><td>50.00</td><td>11.11</td><td>75.00</td><td>50.00</td><td>33.33</td><td>0.00</td><td>58.97</td><td>0.00</td></tr><tr><td>Qwen2.5-VL-7B</td><td>83.33</td><td>75</td><td>16.67</td><td>61.48</td><td>50.00</td><td>75.00</td><td>52.00</td><td>100.00</td><td>25</td><td>77.78</td><td>83.33</td><td>16.67</td><td>0</td><td>51.52</td><td>0.00</td></tr><tr><td>GUI-OWL-32B</td><td>87.50</td><td>62.50</td><td>0.00</td><td>64.23</td><td>81.82</td><td>84.62</td><td>64.10</td><td>50.00</td><td>31.25</td><td>100.00</td><td>57.14</td><td>40.00</td><td>66.67</td><td>69.23</td><td>0.00</td></tr><tr><td>Qwen2.5-VL-32B</td><td>33.33</td><td>83.33</td><td>0.00</td><td>54.55</td><td>71.43</td><td>84.62</td><td>52.05</td><td>50.00</td><td>16.67</td><td>88.89</td><td>75.00</td><td>50.00</td><td>50.00</td><td>59.38</td><td>0.00</td></tr><tr><td>UI-TARS-72B-DPO</td><td>77.78</td><td>55.56</td><td>28.57</td><td>68.50</td><td>85.00</td><td>85.71</td><td>63.29</td><td>100.00</td><td>22.22</td><td>88.89</td><td>66.67</td><td>50.00</td><td>66.67</td><td>73.17</td><td>28.57</td></tr><tr><td>Qwen2.5-VL-72B</td><td>42.86</td><td>75.00</td><td>0.00</td><td>61.48</td><td>66.67</td><td>84.62</td><td>52.00</td><td>50.00</td><td>6.67</td><td>90.00</td><td>60.00</td><td>20.00</td><td>50.00</td><td>54.84</td><td>0.00</td></tr><tr><td>Qwen3-VL-235B-A22B-Instruct</td><td>90.00</td><td>71.43</td><td>28.57</td><td>62.90</td><td>86.49</td><td>92.31</td><td>68.75</td><td>100.00</td><td>35.00</td><td>90.91</td><td>83.33</td><td>16.67</td><td>0.00</td><td>71.43</td><td>50.00</td></tr></table>

Table 6: Comparison of some failure results evaluated using the ColorBench task on real devices and on the graph dataset. Failure reasons are recorded separately for the specific reasons under the two evaluation environments. The tasks in the table are actually evaluated in Chinese.

<table><tr><td>Task</td><td>Failure Reason of Real-Device</td><td>Failure Reason of Graph</td></tr><tr><td>Search for "Guangzhou travel guides" on Xiao-hongshu, share the first one with Xiaohongshu friend Hh Yuan, and ask "Which place do you want to visit the most?" Then use Xiaohongshu to search for food near Canton Tower, remember the name of the first restaurant, search for that restaurant on Meituan and order a set meal for two.</td><td>Accidentally clicked on the advertisement when searching for the store on Meituan.</td><td>Failed to search for food on Meituan.</td></tr><tr><td>Search for "Guangzhou travel guides" on Xi-aohongshu, find the first one, check the blog-ger's followers, likes, and favorites, and then tell WeChat friend 1.</td><td>There is an error in the data sent to WeChat friends.</td><td>Did not enter the blogger's homepage.</td></tr><tr><td>Search for "Guangzhou travel guides" on Xiao-hongshu, use the "Q&A" to let AI reply, and save the generated content as an image to the album.</td><td>Fail to find "Q&A".</td><td>Failed to save as an image.</td></tr><tr><td>Compare the price of the "Xin Dou Ji Guangzhou Tower" double set meal on Meituan, Dazhong Dianping, and TikTok, then forward the cheapest one to friend 1.</td><td>It ends after executing on Meituan.</td><td>Failed to search after Meituan.</td></tr><tr><td>Search for "Baheli Beef Hotpot" on Meituan, forward all double set meals to WeChat friend 1, and ask him to "choose one" after sharing.</td><td>0 No sharing, just repeatedly clicking to grab the deal.</td><td>Only shared one set meal.</td></tr><tr><td>Order takeaway from "Baheli Beef Hotpot" on Meituan: one dry-fried beef, without dinnerware.</td><td>Mistakenly viewed the in-store set meal instead of selecting takeout.</td><td>Did not accurately find takeout.</td></tr><tr><td>Search for "Baheli Beef Hotpot" on Meituan, choose the first restaurant and navigate directly using Gaode Map.</td><td>Did not use Gaode Map navigation.</td><td>Did not switch to Gaode Map.</td></tr><tr><td>Go to JD xxx official flagship store, search for "a certain mobile phone" in the store, check what colors are available for the 16+512GB configuration, and then tell WeChat friend 1.</td><td>Did not enter the official flagship store.</td><td>Did not search in the official flagship store.</td></tr><tr><td>Go to JD xxx official flagship store, forward the store detail page to Moments.</td><td>Could not find the store details page, only accessed the homepage.</td><td>Could not find the store details page.</td></tr><tr><td>Go to JD xxx official flagship store, share the homepage with WeChat friend 1.</td><td>Could not find the share UI.</td><td>Could not find the share option.</td></tr></table>