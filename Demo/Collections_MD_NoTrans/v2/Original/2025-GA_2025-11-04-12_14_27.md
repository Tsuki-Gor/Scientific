Longzhao Huang ${}^{1}$ , Jun Liu ${}^{1}$ , Changwei Wang ${}^{2,3}$ , Rongtao Xu ${}^{4}$ , Wenhao Xu ${}^{1}$ , Zhiwei Xu ${}^{5}$ , Qi Zhang ${}^{6}$ , Yu Zhang ${}^{6}$ , Kexue Fu ${}^{2,6}$ , Longxiang Gao ${}^{2}$ , Yanran Xu ${}^{7,8}$ , Lei Zhang ${}^{9}$ , Li Guo ${}^{1}$ , and Shibiao ${\mathrm{{Xu}}}^{1}$

${}^{1}$ School of Artificial Intelligence,Beijing University of Posts and Telecommunications

${}^{2}$ Ministry of Education,Shandong Computer Science Center (National Supercomputer

Center in Jinan), Key Laboratory of Computing Power Network and Information Security, Qilu University of Technology (Shandong Academy of Sciences)

${}^{3}$ Shandong Fundamental Research Center for Computer Science,Shandong Provincial Key Laboratory of Computer Networks

${}^{4}$ Institute of Automation,The State Key Laboratory of Multimodal Artificial Intelligence Systems, Chinese Academy of Sciences

${}^{5}$ School of Artificial Intelligence,Shandong University

${}^{6}$ Tongji University

${}^{7}$ Transport Engineering and Mobility

8RWTH Aachen

${}^{9}$ Institute of Computing Technology,Chinese Academy of Sciences

July 22, 2025

## Abstract

The Graphical User Interface (GUI) is a visual method that allows users to interact with computers and mobile devices. Nowadays, users rely on GUI for completing some tasks, such as browsing web or using mobile applications. Users often meet some needs such as setting an alarm for 8:00 AM to wake them up and checking the weather for tomorrow. Some commercial agents have been integrated into users personal phones to help the user accomplish a series of basic tasks. Unfortunately, these commercial agents often relied on fixed templates or program scripts to ensure reliability. This also limited their functionality to some basic system applications. Recently large language models (LLMs) have made significant breakthroughs in natural language processing (NLP). Astonishingly, LLMs have demonstrated not only a strong ability to understand and generate text but also planning and reasoning capabilities. Some researchers have considered using LLMs as the agent's brain, equipping these agents with corresponding capabilities. LLM-based agents are also being applied to help users automate tasks on their personal phones and computers. These agents often can understand the GUI environment on personal phones and computers, allowing them to make decisions to complete tasks. This is also the origin of the term "GUI Agent". Our review surveys recent research on LLM-based GUI Agents. We summarize the capabilities of existing GUI Agents and also discuss the GUI Agent task automation pipeline. A comprehensive list of studies in this paper will be available at a GitHub repositories.

<!-- Meanless: 1 -->


# GA: A Comprehensive Survey on LLM-based GUI Agent

Longzhao Huang ${}^{\mathrm{a}}$ ,Jun Liu ${}^{\mathrm{a}}$ ,Changwei Wang ${}^{\mathrm{b},\mathrm{d}}$ ,Rongtao ${\mathrm{{Xu}}}^{\mathrm{c}}$ ,Wenhao ${\mathrm{{Xu}}}^{\mathrm{a}}$ ,Zhiwei ${\mathrm{{Xu}}}^{\mathrm{h}}$ ,Qi Zhang ${}^{\mathrm{e}}$ ,Yu Zhang ${}^{\mathrm{e}}$ ,Kexue ${\mathrm{{Fu}}}^{\mathrm{b}}$ ,Longxiang Gao ${}^{\mathrm{b}}$ , YanRan ${\mathrm{{Xu}}}^{\mathrm{f}}$ ,Lei Zhang ${}^{\mathrm{g}}$ ,Li Guo ${}^{\mathrm{a}}$ ,Shibiao ${\mathrm{{Xu}}}^{\mathrm{a}}$

${}^{a}$ School of Artificial Intelligence,Beijing University of Posts and Telecommunications, BeiJing, China

${}^{b}$ Key Laboratory of Computing Power Network and Information Security,Ministry of Education, Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), JiNan, China

${}^{c}$ The State Key Laboratory of Multimodal Artificial Intelligence Systems,Institute of Automation, Chinese Academy of Sciences, China, BeiJing, China

${}^{d}$ Shandong Provincial Key Laboratory of Computer Networks,Shandong Fundamental

Research Center for Computer Science, JiNan, China e Tongji University, Shanghai, China

${}^{f}$ Transport Engineering and Mobility, Civil Engineering, RWTH Aachen, Aachen, Germany

${}^{9}$ Institute of Computing Technology, Chinese Academy of Sciences, BeiJing, China ${}^{h}$ School of Artificial Intelligence,Shandong University,JiNan,China

## Abstract

The Graphical User Interface (GUI) is a visual method that allows users to interact with computers and mobile devices. Nowadays, users rely on GUI for completing some tasks, such as browsing web or using mobile applications. Users often meet some needs such as setting an alarm for 8:00 AM to wake them up and checking the weather for tomorrow. Some commercial agents have been integrated into users personal phones to help the user accomplish a series of basic tasks. Unfortunately, these commercial agents often relied on fixed templates or program scripts to ensure reliability. This also limited their functionality to some basic system applications. Recently large language models (LLMs) have made significant breakthroughs in natural language processing (NLP). Astonishingly, LLMs have demonstrated not only a strong ability to understand and generate text but also planning and

---

<!-- Footnote -->

*Shibiao $\mathrm{{Xu}}$ is the corresponding author (shibiaoxu@bupt.edu.cn).

<!-- Footnote -->

---

<!-- Meanless: Preprint submitted to Applied Soft Computing<br>June 26, 2025 -->


reasoning capabilities. Some researchers have considered using LLMs as the agent's brain, equipping these agents with corresponding capabilities. LLM-based agents are also being applied to help users automate tasks on their personal phones and computers. These agents often can understand the GUI environment on personal phones and computers, allowing them to make decisions to complete tasks. This is also the origin of the term "GUI Agent". Our review surveys recent research on LLM-based GUI Agents. We summarize the capabilities of existing GUI Agents and also discuss the GUI Agent task automation pipeline. A comprehensive list of studies in this paper will be available at a GitHub repositories.

Keywords: GUI Agent, Large Language Model, Task Automation, GUI Environment Understanding

## 1. Introduction

GUI is a visual user interface that allows users to interact with computers and mobile electronic devices using graphical icons, visual indicators, and interactive elements such as buttons and menus. GUI has become essential in modern computing, making interactions intuitive and accessible across various platforms, including computer operating systems [1], mobile applications like Android [2], and web applications [3]. Users typically need to repeatedly perform some basic daily tasks (such as setting an alarm). In this process, some current commercial agents [4] can assist users complete these basic tasks. These commercial agents find it difficult to execute multiple complex steps to accomplish high-level tasks (such as querying today's gold price and sending it to JACK via WeChat). Leveraging agents to assist users in navigating these high-level tasks has become a very important field [5].

The exploration of commercial agents has seen significant advancements. On smartphones, Apple integrated Siri [4] into the iPhone 4S as early as 2011. Siri offers a convenient user experience through voice interaction, assisting users with various daily tasks such as information retrieval, navigation, and translation. For personal computers, Microsoft introduced Copilot [6] to assist Windows users automating document drafting, creating presentations, summarizing emails, and more, thereby enhancing productivity. Commercial agents [4, 7] primarily adopt template-based approaches to automate tasks. Template-based methods involve predefining a series of task templates, each template of which contains detailed descriptions. When a user sends a command to the agent, the agent maps this command to the corresponding template and extracts the corresponding template parameter from the command. While template-based methods are precise and reliable, they lack scalability and flexibility, making it difficult to support complex tasks or other application scenarios.

<!-- Meanless: 2 -->


Moving beyond the template-based methods, the introduction of GUI Agents marks a significant evolution in the field of task automation. Unlike template-based methods, GUI Agents are designed to understand the GUI environment in real-time, and then select and execute the next action. We have defined the task automation pipeline involving the GUI Agent in Figure 1. There are three key basic roles: the device, the user, and the GUI Agent. In Figure 1, the user gives the agent an initialization task (Search for "GUI Agent" in Google Search), which could be a voice command or text input. The next step for the agent is to then integrate the task instruction and the GUI environment as the input command for the agent and decide the next action. The agent needs to determine whether the task is completed independently.

There are many methods to implement the GUI Agents in the past, such as demonstrations learning methods [8, 9, 10], supervised learning methods 11, 12], and reinforcement learning methods [13, 14, 15]. These methods often require extensive manual data collection or the handcrafted design of reward functions. Despite their potential, the reliance on significant human intervention remains a considerable challenge for broader implementation and adaptability. Recently, LLMs [16, 17, 18, 19] have achieved remarkable success in NLP. Many consider LLMs to be the dawn of artificial general intelligence. LLMs not only exhibit strong capabilities in understanding and generating natural language but also demonstrate impressive performance in planning and reasoning tasks [20]. Some researchers [21, 22, 23] are now attempting to endow LLMs with perception abilities and action spaces, thereby transforming them into agents. Some researchers [24, 25, 26] have also started exploring LLM-based GUI Agents. Compared to traditional agents, LLM-based GUI agents utilize in-contextual learning to automate task [27]. These GUI Agents [24, 25] avoid the need to collect large amounts of data for training, while also increasing the flexibility of GUI agents [26].

From the GUI Agent role in task automation in Figure 1, GUI Agents must have three basic capabilities: GUI environment comprehension, device control, and user interaction. Based on different ways to understand the GUI environment, we propose a taxonomy that divides GUI Agents into three categories: Vision-Based, Text-Based, and Hybrid Text-Vision. After comprehending the GUI environment, agents can generate automated scripts or select action from the action space to control the device. During the task execution process, agents can chat with users in real-time to refine task information. In addition to the three basic capabilities, agents can provide personalized services to users to enhance the user experience. Furthermore, some studies have applied multi-agents in the task automation field [28]. Consequently, collaborating with other agents is also important for GUI Agents. Our paper surveys numerous recent GUI Agents, and discusses the relevant capabilities of the agents in Section 3.

<!-- Meanless: 3 -->


<!-- Media -->

<!-- figureText: Search "GUI Agent" in google browser<br>Initial Task Chat<br>GUI Agent<br>User<br>GUI Agent<br>Action<br>User<br>Device<br>GUI Agent<br>State0<br>StateN -->

<img src="https://cdn.noedgeai.com/bo_d41gja3ef24c73d3tvo0_4.jpg?x=310&y=370&w=1175&h=428&r=0"/>

Figure 1: The Basic Pipeline of GUI Agent Task Automation.

<!-- Media -->

Besides the capabilities of the GUI Agents, the task automation pipeline is also important for task efficiency. Most agents follow a single-stage pipeline, where the agents directly performs tasks within the GUI environment. Such agents rely solely on knowledge obtained from prior training or information described by humans. However, due to the diversity of GUI scenarios, agents often face unknown manipulation methods within the new GUI environment. To better alleviate this issue, some researchers have proposed a two-stage pipeline (from exploration to exploitation). In the exploration stage, agents first freely explore the GUI environment to form a knowledge database specific to that environment. And then agents exploit this knowledge database to complete tasks within the GUI environment. We discuss the two-stage task automation pipeline in Section 4.

Our paper surveys numerous recent studies on LLM-based GUI Agents, providing a detailed overview of the current state of research in this area. We summarize the current basic capabilities of GUI Agents and explore the task automation pipeline. We also summarize relevant evaluation benchmarks. Finally, we summarize the challenges and potential opportunities. Overall, this paper makes the following contributions:

<!-- Meanless: 4 -->


1. Our paper surveys numerous recent studies on LLM-based GUI Agents, providing a detailed overview in this area. A comprehensive list of studies in this paper will be available at https://github.com/longzhaohuang/ GUI-Agent-Survey.

2. From the role of GUI Agents in task automation, our paper summarizes the relevant capabilities, covering GUI environment comprehension, device control, user interaction, personalized services, and collaboration with other agents. We also provide a taxonomy based on the way to understand the GUI environment.

3. Our paper discusses the two-stage task automation pipeline involved by GUI Agents: from exploration to exploitation.

4. Our paper summarizes the remaining challenges in this field and explores potential opportunities.

The remaining structure of this paper is as follows: In Section 2 we provide essential background knowledge on GUI Agents, enabling readers to understand the subsequent paper better; In Section 3, we summarize the relevant capabilities of GUI Agents and propose a taxonomy based on the way to understand the environment; In Section 4, we summarize the two-stage task automation pipeline involved by GUI Agents; In Section 5, we summarize the commonly used evaluation datasets and their corresponding evaluation criteria; In Section 6 we discuss the existing challenges faced by GUI Agents and opportunity in this era; In Section 7, we conclude the paper by summarizing the key contributions and overall content.

## 2. Background Knowledge

This section expounds the background knowledge of GUI Agents. In Section 2.1, we introduce the fundamental concepts of LLMs. While LLMs is adept at processing discrete textual data, leveraging visual perception is crucial in GUI task automation. Given the limitations of LLMs in handling only textual data, we discuss the integration of LLMs with multi-modal perception capabilities in section 2.2, which enable LLMs to interact with and understand GUI environment more effectively. The section 2.3 introduces the concept of LLM-based agents, outlining their characteristics and capacity.

<!-- Meanless: 5 -->


### 2.1. Fundamental Concept of Large Language Model

Recently, LLMs have achieved unprecedented advancements in NLP, and has demonstrated exceptional capabilities in both understanding and generating natural language. In the context of GUI automation task, LLMs can simultaneously comprehend the GUI environment represented by view hierarchy (VH) and document object model (DOM) structure repersentation and the instructions provided by users.

The LLMs architecture are typically based on the transformer 29. Traditional transformers generally consist of an encoder component and a decoder component. Based on the components utilized by LLMs, LLMs can typically be categorized into three types: encoder-only, decoder-only, and encoder-decoder type. The encoder-only method, exemplified by the BERT series [30], uses the encoder component to transform text sequences into embedding representations. Then encoder-only method extracts a single embedding to perform global comprehension tasks such as emotional prediction. Despite excelling in global comprehension, the heavy condensation of information into one single embedding can lead to information loss. This makes this architecture unsuitable for text generation and not the preferred choice for contemporary LLMs architectures. The decoder-only method generates output sequences in an autoregressive manner. Based on the attention mechanism, decoder-only methods are divided into causal-decoder method [27] and prefix-decoder method [31]. Compared to the causal-decoder method, the prefix-decoder method employs bidirectional attention on the prefix part of the sequence. Encoder-decoder methods, such as T5 [32], follow the classical transformer design, combining both encoding and decoding processes. They can understand input text and, through a cross-attention mechanism, autoregressive generate the target sequence. This dual capability enables them to handle both text understanding and generation.

The remarkable capabilities of LLMs are largely attributable to their enormous parameter and their extensive training datasets. According to the Scaling Law [33], increasing the scale of parameters and data effectively enhances LLMs performance on downstream tasks. Researchers [18, 34] typically begin by expanding LLMs to tens of billions of parameters, even hundreds of billions. Then they train LLMs following the two-stage approach: "pre-training and fine-tuning." In the pre-training phase, LLMs are initially trained on a large-scale corpus using specially designed pre-training tasks. This stage helps the model learn a wide range of linguistic features and patterns. After a pre-training phase, LLMs are fine-tuned on specific downstream tasks to adapt to the unique requirements of these tasks.

<!-- Meanless: 6 -->


### 2.2. Equipping Large Language Model with Perceptual Abilities

Despite LLMs impressive performance across various natural language processing tasks, they are inherently limited in handling visual information. While LLMs can assist GUI Agents by parsing VH and DOM structure representation and understanding user commands, they face challenges such as excessive input length and loss of rich visual details. Consequently, researchers [35, 36] have begun to explore different ways to endow LLMs with perceptual capabilities. This enhancement allows LLMs to process visual information, thereby overcoming the limitations of traditional LLMs in GUI automation tasks.

In the early stages, LLMs [35, 37, 38] achieved multimodal capabilities by integrating external expert vision tools [39, 40]. For instance, VISPROG [35] leverages the ICL abilities of LLMs to generate Python-like programs that call various expert vision models or image processing routines. At that moment, VISPROG demonstrated significant flexibility across four tasks: compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. Currently, most LLMs [36, 41, 42] are typically designed in an end-to-end trainable format. This enables the LLMs to process visual information without relying on external expert visual models, commonly referred as Multi-modal Large Language Models (MLLMs). These MLLMs consist of three main components: a vision encoder, a learnable mapping layer, and a LLM. Initially, MLLMs use a frozen vision encoder to extract image information, which is then aligned with textual data via a learnable mapping layer. Finally, MLLMs use a LLM to auto-regressively generate the target text sequence. The learnable mapping layer can be implemented using techniques such as MLP, cross-attention mechanisms, or Q-Former [41]. LLava [43] employs a simple linear mapping layer to align visual and textual representations, subsequently concatenating these aligned representations to input LLM. Flamingo [36] uses a cross-attention mechanism as its learnable mapping layer, specifically, flamingo leverages visual features to enhance text representations within the cross-attention layers. This eliminates the need for LLM to handle additional visual tokens. Q-Former, initially introduced in the BLIP series [41], uses learnable queries to record visual information related to textual information, and then integrate this information in transformed layer to effectively align these two patterns. Due to significant performance improvements in MLLMs, Q-Former has been widely adopted in subsequent research [44, 45, 46].

<!-- Meanless: 7 -->


### 2.3. Relevant Concepts of the Agent

This section elucidates the concepts related to agents and describes the transition from LLMs to intelligent agents. This section is to provide readers with an understanding of LLM-based agents so that readers can better understand the subsequent content. An agent refers to an entity capable of perceiving its environment, formulating strategies, and executing tasks, these entities typically exhibit autonomy, reactivity, proactivity, and even social abilities [20].

Early agent methodologies primarily focused on symbolic agent and reactive agents. Symbolic agent, epitomized by knowledge-based expert systems 47, 48, 49, 50], relied on symbolic logic and strict rules. These agents excelled in logical reasoning but struggled to handle the complexities and uncertainties of the real world. Reactive agents [51, 52], on the other hand, operated efficiently through a perception-action loop mechanism, enabling immediate responses to environmental. With advancements in deep learning and computational hardware, reinforcement learning (RL) agents emerged. These agents continually interact with their environment, learning to make decisions that maximize cumulative expected rewards. Unlike symbolic and reactive agents, RL agents [53, 54, 55, 56] can adapt to complex and dynamically changing environments. However, they also face challenges such as low sample efficiency and unstable training processes.

As LLMs have demonstrated remarkable capabilities, researchers [21, 22, 57 have begun exploring the potential of agents driven by LLMs. LLM-based Agents have several key abilities: planning, memory, and tool usage [58]. Before executing tasks, these agents utilize Chain of Thoughts (CoT) [59] and Tree of Thoughts (ToT) [60] for planning. The CoT technique guides the agent to think step-by-step, decomposing complex tasks into subtasks. In contrast, ToT generates multiple thoughts at each step, creating a tree structure for thorough exploration.

During task execution, agents employ ReAct [61] and Reflexion [62] methodologies, iteratively refining past decisions and correcting errors to accomplish tasks. Agents also leverage both Short Term Memory and Long Term Memory to assist in task completion. Short Term Memory is used for ICL [27], where the content is fed into the agent along with the task context, albeit limited by the transformer context window size. Long Term Memory, stored in external vector databases, is accessible via rapid retrieval, providing a broader context beyond the immediate task. Furthermore, agents enhance their action space by configuring and utilizing tools relevant to the task at hand. They call these tools by outputting the correct API formats, thereby expanding their functional capabilities.

<!-- Meanless: 8 -->


While single-agent systems have shown remarkable performance, some researchers [63] have integrated multiple agents to enhance overall capabilities. Currently, multiagent system paradigms are generally classified into three types: cooperation, discussion, and competition [64]. In the cooperative paradigm [65], agents work together to achieve a common goal or complete a task collectively. In the debate paradigm [66], each agent presents its own opinion, and through exchanging viewpoints, they reach a consensus. In the competitive paradigm [67], agents pursue their individual goals, which are often in conflict with each other, thus creating a competitive environment.

<!-- Media -->

<!-- figureText: Ⓡ<br>(a) GUI Environment Understanding<br>(b) Device Control<br>(c) User Interaction<br>(d) Perosnal Service<br>(e) Agent Synergy -->

<img src="https://cdn.noedgeai.com/bo_d41gja3ef24c73d3tvo0_9.jpg?x=332&y=1004&w=1157&h=646&r=0"/>

Figure 2: The core capability of GUI Agent

<!-- Media -->

### 3.The core capability of GUI Agent

Based on the GUI Agent role in task automation as depicted in Figure 1, this section explores the core capability of GUI Agents. These capabilities are divided into two categories: basic and advanced. The basic capabilities include GUI environment comprehension, device control, and user interaction. GUI Agents must have these three basic capabilities to achieve fundamental task automation. Section 3.1 delves into these basic capabilities. Moreover, Section 3.2 explores two advanced capabilities: personalized services and agent synergy. We illustrate these five capabilities in the Figure 2.

<!-- Meanless: 9 -->


<!-- Media -->

Table 1: Summary of GUI Agents. Distinguish the existing works based on their platform, the methods they use for GUI environment understanding, the types of control devices they employ, and the publication year.

<table><tr><td>Platform</td><td>Work</td><td>GUI Environment Understanding</td><td>Control Device</td><td>Publication Year</td></tr><tr><td rowspan="11">Mobile</td><td>SpotLight [68]</td><td>Vision-Based</td><td>UI-based</td><td>2022</td></tr><tr><td>Meta-GUI 69</td><td>Vision-Based</td><td>UI-based</td><td>2022</td></tr><tr><td>AutoUI 70</td><td>Vision-Based</td><td>UI-based</td><td>2023</td></tr><tr><td>MobileGPT 71</td><td>Vision-Based</td><td>UI-based</td><td>2023</td></tr><tr><td>AppAgent 72</td><td>Hybrid Text-Vision</td><td>UI-based</td><td>2023</td></tr><tr><td>MM-Navigator 24</td><td>Vision-Based</td><td>UI-based</td><td>2023</td></tr><tr><td>DroidBot-GPT 73</td><td>Text-Based</td><td>UI-based</td><td>2023</td></tr><tr><td>Mobile-Agent 74</td><td>Vision-Based</td><td>UI-based</td><td>2024</td></tr><tr><td>Mobile-Agent-v2 75</td><td>Vision-Based</td><td>UI-based</td><td>2024</td></tr><tr><td>SeeClick [76]</td><td>Vision-Based</td><td>UI-based</td><td>2024</td></tr><tr><td>CocoAgent [77]</td><td>Vision-Based</td><td>UI-based</td><td>2024</td></tr><tr><td rowspan="14">Computer</td><td>WebGPT [78]</td><td>Text-Based</td><td>UI-based</td><td>2021</td></tr><tr><td>Pix2Act 79</td><td>Vision-Based</td><td>UI-based</td><td>2023</td></tr><tr><td>WebGUM [80]</td><td>Hybrid Text-Vision</td><td>UI-based</td><td>2023</td></tr><tr><td>MindAct 81</td><td>Text-Based</td><td>UI-based</td><td>2024</td></tr><tr><td>WebAgent 82</td><td>Text-Based</td><td>Code-based</td><td>2023</td></tr><tr><td>RCI 83</td><td>Text-Based</td><td>UI-based</td><td>2024</td></tr><tr><td>AdaPlanner 84</td><td>Text-Based</td><td>Code-based</td><td>2024</td></tr><tr><td>WEBWISE 25</td><td>Vision-Based</td><td>Code-based</td><td>2023</td></tr><tr><td>SeeAct 85</td><td>Hybrid Text-Vision</td><td>UI-based</td><td>2024</td></tr><tr><td>WebVoyager 86</td><td>ScreenShot</td><td>UI-based</td><td>2024</td></tr><tr><td>DUAL-VCR 87</td><td>Hybrid Text-Vision</td><td>UI-based</td><td>2024</td></tr><tr><td>UFO 88</td><td>Vision-Based</td><td>UI-based</td><td>2024</td></tr><tr><td>AutoWebGLM [89]</td><td>Text-Based</td><td>UI-based</td><td>2024</td></tr><tr><td>MMAC-Copilot 90</td><td>Vision-Based</td><td>UI-based, Code-based</td><td>2024</td></tr></table>

<!-- Media -->

<!-- Meanless: 10 -->


3.1. Basic capability

3.1.1. GUI environment comprehension

<!-- Media -->

<!-- figureText: Input<br>Input<br>Task<br>DOM / VH<br>Task<br>ScreenShot<br>了<br>了<br>乃<br>乃<br>Brain<br>Large Language Model<br>Brain<br>LLM<br>⓽<br>OR<br>GUI Agent<br>MLLM<br>GUI Agent<br>Tool<br>臼<br>Tool<br>Convertor<br>Filter<br>Parser<br>(a) Text-Based Understanding by VH or DOM structure information<br>(b) Vision-Based Understanding by screen-shot<br>Input<br>Task<br>DOM / VH<br>ScreenShot<br>GUI Agent<br>(c) Hybrid Text-Vision Based Understanding -->

<img src="https://cdn.noedgeai.com/bo_d41gja3ef24c73d3tvo0_11.jpg?x=317&y=510&w=1160&h=822&r=0"/>

Figure 3: Three Basic Paradigms for Understanding GUI Environments

<!-- Media -->

The GUI environment comprehension is one of the essential foundational capabilities of GUI Agents and the basis for its name origin. In our paper, Figure 3 illustrates three fundamental approaches to understanding the GUI environment: Text-Based, Vision-Based, and Hybrid Text-Vision. As shown in Figure 3(a), the Text-Based approach involves directly understanding the VH/DOM structure representation. The Vision-Based approach, as depicted in Figure 3(b), interprets the GUI environment through screenshots. The Hybrid Text-Vision approach, as shown in Figure 3(c), combines the characteristics of both methods to comprehend the GUI environment. Table 1 in our paper summarizes the methods employed by GUI Agents to understand their environments.

In the textual paradigm for understanding the GUI environment, GUI Agents typically do not directly input the raw information as text into the LLMs. GUI Agents preprocesses the text through the Convertor and Filter stage. The Convertor stage transforms the raw input into a form that is more comprehensible to the LLM. This stage generally applies to non-HTML form textual input. These studies [91, 73] set the Convertor stage to transform the raw input into HTML or natural language form. Wang et al. [91] employed a depth-first search traversal to convert the Android UI VH structure representation into HTML, which was then fed into PaLM [31. DroidBot-GPT [73] converted structural representation into natural language sentences for the LLM input. DroidBot-GPT first used Droidbot [92] to extract structure representation from the Android UI VH, then translated this information into understandable natural language sentences, which were finally input into ChatGPT [16]. The Filter stage aims to extract task-relevant text content from lengthy raw inputs. These studies [81, 82] set the Filter step to task-relevant text content. MindAct [81] investigated using HTML text as input. To address the issue of lengthy HTML text, MindAct proposed a two-stage paradigm. Mind2Act used a small language model (DeBERTa [93]) first to filter raw text and obtain candidate elements. MindAct combined these elements into HTML fragments, which were input into the LLM (Flan-T5 [94]) to predict actions. WebAgent [82] inherit the two-stage paradigm of Min-dAct. WebAgent first utilized a domain-specific model HTML-T5 to summarize task-relevant snippets from HTML documents, then used FLAN-U-PaLM [31, 94] to generate Python programs from these snippets to perform actions on the website.

<!-- Meanless: 11 -->


GUI Agents show potential in understanding GUI environment through VH/DOM structure representation, but several challenges persist in text-based approach. First, VH/DOM structure representation is not always accessible. Second, the verbosity of such structured representation creates inefficient contexts for LLMs, leading to the omission of crucial details. Finally, significant information including icons, images, charts, and spatial relationships cannot be easily conveyed through VH/DOM structure representation alone. To address the limitations of the text-based approach, agents use screenshots to understand the GUI environment visually. Figure 3(b) in this paper illustrates this approach where agents comprehend the GUI environment through visual perception. Specifically, agents can achieve this paradigm in two ways: (a) Parsing the screenshot using external tools; (b) Direct Understanding via MLLMs.

WebWISE [25] leverages parsers to convert GUI interfaces into textual elements, which are then input into the LLM. Specifically, WebWISE employs Pix2Struct [95] to extract the DOM structure representation from web screenshots and subsequently inputs this information into ChatGPT to generate the Python script. Pix2Struct is a specialized MLLM designed for web screenshot simplification. Pix2struct has two pre-training tasks, web screenshot mask completion and web screenshot structure information extraction. Compared to WebWISE which relies on external visual tools, these approaches [79, 70, 68] utilize the multi-modal capability of MLLMs to directly interpret screenshots. Pix2Act [79] fine-tuned Pix2Struct to output action operation. Auto-GUI [70] train an MLLM from scratch. Auto-GUI employs the cross-attention mechanism to query screenshot clues with command text token, thereby avoiding the additional computational cost brought by visual tokens. Spotlight [68] introduces an additional region summary in the MLLM, which directs the MLLM to focus more on key areas of the screen-shot. The ability of MLLMs is sufficient to help agents understand contextual information. However, these agents face two challenges: fine-grained localization and high-resolution input. Screenshots often contain small text links and icons that require fine-grained localization, "The fine-grained localization capability of MLLMs is important for task execution. Moreover, the high resolution of screenshots poses a challenge. This challenge mainly involves increased computational resource overhead and complexity. For instance, encoding a 1120 $\times$ 1120 screenshot with a 14-patch setting transformer architecture would require an additional 6400 visual tokens.

<!-- Meanless: 12 -->


The challenge of fine-grained localization is not unique to GUI Agents, but is a common issue across most MLLMs [96]. Researchers [97, 98, 99, 100, 101, 102] have proposed various strategies to enhance the fine-grained localization capability of MLLMs. For commercial like GPT-4V [103], the "Image Caption" strategy [97, 98, 99] is commonly employed. In Figure 4, we illustrate the application of the "Image Caption" strategy. This strategy involves using a red bounding box to highlight elements in the original image and annotating them with red English letters. This approach is widely adopted in research [24, 72, 85, 76, 71] utilizing commercial models like GPT-4V as the GUI Agent brain. For instance, MM-navigator [24] and AppAgent [72] first use iconnet [104] to locate icons, marking them with numbers before inputting the annotated images into GPT-4V for interpretation. However, excessive visual markers can clutter the original image, hindering the MLLM's comprehension. SeeAct [85] adopts a more refined method by using MindAct to filter out candidate element before applying the "Image Caption" strategy. For trainable open-source MLLMs, researches [102, 105] have developed a more intuitive strategy to enhance fine-grained visual localization. This involves having MLLMs output the coordinates of target bounding boxes in text form. Both SeekClick [76] and CogAgent [106] have retrained their respective MLLMs to output these location coordinates effectively.

<!-- Meanless: 13 -->


<!-- Media -->

<img src="https://cdn.noedgeai.com/bo_d41gja3ef24c73d3tvo0_14.jpg?x=720&y=571&w=353&h=765&r=0"/>

Figure 4: Enhancing Visual Localization Capabilities in MLLMs via "Image Caption" Strategy

<!-- Media -->

Compared with the challenge of fine-grained localization, there are still fewer studies [106, 70] exploring solutions to the challenge of high-resolution input. A more intuitive approach is to directly resize the high-resolution image into a low-resolution image. However, reducing the high resolution often makes it more difficult to locate some tiny icons and text links. CogA-gent 106 additionally utilizes a lightweight high-resolution image encoder to support 1120 × 1120 high-resolution input, and uses a cross-attention mechanism to pass high-resolution information to the transformer decoder, thereby avoiding high-resolution visual tokens input. Another method [70] is to not concatenate visual tokens and text tokens together, but directly use the cross-attention mechanism to enhance text tokens with visual tokens.

We illustrate the Hybird text-vision for understanding GUI environment in Figure 3(c), which learns both GUI screenshots and structured representation. These methods [80, 87, 86, 107] can supplement the relevant information of small page elements and text links by utilizing structured information, and can also provide visual information through screenshots. WebGUM [80] combines the visual web screenshots and linguistic HTML pages to perform web navigation tasks. Specifically, WebGUM tokenization the HTML as part of the textual information and then concatenates it with visual tokens as input to the MLLM. Similar to WebGUM, BUI-Bert [107] also simply concatenates the two types of information and inputs them into the MLLM. DUAL-VCR [87] proposed a dual-view approach that uses GUI screenshots as contextual clues for agents to better understand HTML. WebVoyager [86] uses GPT-4V as the Brain, and additionally inputs GUI layout information as a supplement while taking GUI screenshots.

<!-- Meanless: 14 -->


#### 3.1.2. Device control

Compared to using text directly for input and output, some studies [108], 109, 110, 111 propose using the LLM to synthesize programs. Another fundamental capability of a GUI Agents is device control. It is very intuitive to categorize GUI Agents into two types from the device control perspective: Code-based agents and UI-based agents [112]. Code-based agents either rely on the LLM to generate suitable code for interacting with device APIs [82] or are fine-tuned to learn how to call device APIs directly [78]. For instance, We-bAgent [82] extracts task-related segments from HTML text and uses these to generate Python scripts to perform operations on websites. Similarly, We-bGPT [78] fine-tunes a GPT-3 [27] to answer long-form questions by calling the Microsoft Bing Web Search API [113]. However, not all application APIs are accessible, and there can be significant differences between APIs of different applications. UI-based agents overcome these limitations by mimicking human actions to control devices. UI-based agents require a pre-defined set of actions as an action space and then select these actions in natural language form. These agents perform device control by simulating human interactions with the GUI. Currently, the majority of GUI Agents [91, 83] employ this control method. We illustrate in Table 1 in this paper presents the methods used by GUI Agents to control the device.

#### 3.1.3. User interaction

User interaction is the final basic capability of GUI Agents. From the research perspective, GUI Agents generally support interaction with users in the form of text-based communication. This work [112] mentions additional interaction methods, such as voice interaction, GUI interaction, and virtual reality (VR) interaction. Voice interaction is considered the most popular form of interaction, widely accepted to human communication. Traditional virtual assistants [4, 7] have demonstrated the popularity of this interaction method. GUI interaction involves interacting with agents through actions like clicking buttons or selecting menus. Finally, the VR interaction method is based on an immersive 3D environment, where users can interact using headsets and gesture controllers.

<!-- Meanless: 15 -->


From another perspective of interaction forms, most GUI Agents nowadays interact with users in the form of simple dialogue (Text or voice). Most existing GUI Agents [81, 82, 114] follow a basic workflow: the user provides an initial task command to the GUI Agents. And then agent automatically completes the specified task and return the final status back to the user. However, this basic workflow imposes two requirements on the user: (1) the user must provide all task-related information at the beginning, and (2) the user must understand how the GUI environment operates to ensure task success. In fact, users may not have all the necessary information at the beginning. Meanwhile, if the agent initially insists that users think exhaustively about what additional information is required to complete the task, it will significantly diminish the user experience. To address this issues, researchers [69, 115, 116] have proposed the concept of multi-turn dialogues. Building on multi-turn dialogues, WebLINX 115 introduced the concept of task-oriented dialogue. Figure 5 in this paper illustrates a comparison between simple dialogue and task-oriented dialogue. Figure 5(a) shows the single-turn interaction method, while Figure 5(b) shows the task-oriented dialogue method. In task-oriented dialogues, the agent engages in a conversational exchange with the user, gradually acquiring the necessary information to complete the task, thus enhancing the overall user experience by making the interaction more intuitive and less burdensome. Task-oriented dialogues also improves the success rate of the task.

### 3.2. Advanced capability

#### 3.2.1. Personalized services

Personalization capability is one of the advanced capabilities. While it may seem to be in conflict with privacy protection, it is relatively important in task automation. Various users display distinct preferences, needs, and habits when interacting with applications or web interfaces. Personalization can accommodate these unique characteristics, thus more effectively fulfilling user needs. By employing personalization, GUI Agents can swiftly discern user intent and execute corresponding actions with greater efficiency and accuracy. Presently, personalization is usually realized through the memory modules of the agents. For instance, Friday [117] records user profiles in declarative memory, capturing preferences related to conversation style, tool usage habits, and music/video preferences. This capability allows agents to tailor their responses and actions, thereby enhancing the overall user experience.

<!-- Meanless: 16 -->


<!-- Media -->

<!-- figureText: User<br>i wanna book a hotel near buddhas universal church. I want to check in 6 days later. And I will live there for 3 days. i want to book 2 rooms for 4 adults ...<br>OK<br>GUI Agent<br>(a) Single-Turn Interaction Approach<br>i wanna book a hotel near buddhas universal church.<br>User<br>When will you check in?<br>GUI Agent<br>6 days later. And I will live there for 3 days.<br>User<br>How many rooms do you want to book, and how many adults and children?<br>GUI Agent<br>-<br>-<br>You are welcome. Let me know if there is any thing I can help you with.<br>GUI Agent<br>(b) Task-Oriented Dialogue Approach -->

<img src="https://cdn.noedgeai.com/bo_d41gja3ef24c73d3tvo0_17.jpg?x=589&y=350&w=612&h=1045&r=0"/>

Figure 5: Two Distinct User Interaction Forms

<!-- Media -->

<!-- Meanless: 17 -->


#### 3.2.2. Agent Synergy

As some studies 28 have applied the multi-agent concept to task automation, multi-agent systems have shown great potential. Compared to traditional single-agent approaches, multi-agent systems break down the entire task automation process into several relatively simple subtasks. Each agent focuses on a specific subtask and optimizes the process based on its unique functionality and capabilities. This refinement and division of labor make task processing more efficient and accurate. Therefore, Agent Synergy is regarded as one of the advanced capabilities of GUI Agents.

Currently, there are a series of studies on agent clusters in GUI Agents [88, 90, 75]. UFO [88] is specifically designed for interacting with applications within the Windows operating system. It is a dual-agent system consisting of AppAgent and ActAgent. AppAgent is responsible for selecting the appropriate application to fulfill user requests, while ActAgent performs specific actions within the selected application to meet the user's needs. MMAC-Copilot [90] aims to enhance interaction with operating systems by leveraging the collective expertise of diverse agents. MMAC-Copilot introduces a team collaboration chain, allowing each participating agent to contribute insights based on their domain knowledge, effectively reducing hallucinations caused by knowledge gaps.

In contrast to UFO and MMAC-Copilot, which are multi-agent frameworks for the computer environment, Mobile-Agent-v2 [75] is a multi-agent framework designed for mobile devices. Mobile-Agent-v2 consists of three specialized agents: Planning Agent, Decision Agent, and Reflection Agent. The Planning Agent compresses historical operations and screen summaries into a plain text task progress format, making it easier for the Decision Agent to navigate the task flow. The Decision Agent generates and executes actions based on the current task progress, the current screen state, and feedback from the Reflection Agent (if the previous action was incorrect). The Reflection Agent monitors the screen changes before and after the Decision Agent's actions to determine whether the actions were successful. If the actions are not as expected, it takes appropriate measures to re-execute the actions.

## 4. Task Automation Pipeline

This section discusses a key issue: "How can agents efficiently automate complex tasks?" Most task automation agents follow a single-stage automation pipeline, where automation relies solely on pre-trained knowledge or task information provided by humans. Recently, researchers [28] proposed a dual-agent system that divides task automation into two key stages: from Exploration to Exploitation. In the Exploration stage, a weaker agent randomly explores the environment to gather and record knowledge, while in the Exploitation stage, a stronger agent utilizes the knowledge gained during the Exploration stage to execute tasks. GUI Agents task automation can also follow this two-stage pipeline: from Exploration to Exploitation. The two stages is shown in Figure 6 In the Exploration stage, the GUI Agents explore an unfamiliar GUI environment, recording interaction methods and actionable subtasks. In the Exploitation stage, GUI Agents utilize the knowledge acquired during the Exploration stage to execute tasks. We believe that exploration is an essential step in efficient task automation processes. Given the diversity of currently available applications and websites, it is impractical to collect a large amount of data for training in every unfamiliar GUI environment. In addition, the operation methods behind each GUI environment are different, and the same icon may represent different functions in different GUI contexts. The exploration stage provides a feasible solution to these challenges by requiring GUI Agents to explore the GUI environment before executing tasks.

<!-- Meanless: 18 -->


<!-- Media -->

<!-- figureText: Explore Phase<br>Exploitation Phase<br>道<br>GUI Agent<br>App<br>Planning<br>Action<br>Reflection<br>Memory -->

<img src="https://cdn.noedgeai.com/bo_d41gja3ef24c73d3tvo0_19.jpg?x=312&y=354&w=1169&h=335&r=0"/>

Figure 6: Two-Stage Task Automation Pipeline of GUI Agents: From Exploration to Exploitation.

<!-- Media -->

Currently, many researchers [72, 71, 26] have integrated the exploration stage before GUI Agents perform tasks. For example, AppAgent [72] explores the functionality and characteristics of smartphone applications through trial and error. At this stage, AppAgent is assigned a task and begins to interact autonomously with UI elements, using different operations and observing changes in the application interface to understand its working principle. Mo-bileGPT [71] uses two software tools during the exploration stage - a random explorer [118] and a user trace monitor [119] - to access and analyze as many application screens as possible. For each screen accessed, MobileGPT requests the LLM to generate a list of available subtasks on that screen for use during the deployment stage. AutoDroid [26] summarized the functionality of all UI elements through random exploration to gain a comprehensive understanding of the tasks that can be executed in an application and determine the corresponding UI elements required to perform these tasks.

<!-- Meanless: 19 -->


During the exploitation stage, GUI Agents can further refine tasks to improve their success rate. This process typically includes the following steps: planning, action, and reflection 117. The GUI Agent first executes task planning, which may break down the task into several sub-tasks and complete them one by one. During the execution of subtasks, the GUI Agent utilizes knowledge obtained from memory or files during the exploration stage to assist in completing the task and continuously updates this knowledge. After each subtask is completed, the GUI Agent can set a Critic to automatically evaluate the completion status of the subtasks. GUI proxies typically use a LLM as the Critic, which not only determines whether the current subtask is completed, but also provides corrective suggestions and evaluates the necessity of reorganizing subtasks. Recently, MMAC-Copilot [90] proposed a multi-agent system to deepen the implementation of the above in the deployment stage. MMAC-Copilot has six kind agent roles: Planner, Librarian, Programmer, Viewer, Video Analyst, Mentor. Planner observes tasks and decomposes them into rough subtasks, which are then further refined by Viewer and Video Analyst. After determining the subtask, the Viewer and Programmer will execute the subtask, and the Mentor will evaluate the execution results and provide feedback to the Planner, indicating whether the subtask needs to be changed. During this process, the Librarian is responsible for conducting information retrieval, enabling them to answer queries and provide basic knowledge.

## 5. Evaluation Benchmark

### 5.1. Evaluation Dataset

We present GUI automation task dataset at the current stage in Table 2. Some datasets [129, 130] initially focused on low-level GUI automation tasks, which involved selecting web page elements given human instruction. PhraseNode [129] recommends selecting the element $e$ described by command $c$ when provided with a set of elements ${e1},\ldots ,{e}_{k}$ and a web environment $w$ . UIBert [130] introduced two downstream tasks for element selection: similar GUI component retrieval and referrring component retrieval. In similar GUI component retrieval tasks, the goal is to select the candidate components that are most functionally similar to the given GUI environment and components used as queries, and to search for the GUI and a set of candidate components. The referring component retrieval task is similar to the one proposed by PhraseNode, aiming to retrieve the components indicated by expressions from a set of GUI components detected on the screen given a reference expression and GUI image. SeekClick [76] extends this by introducing another baseline dataset called ScreenSpot, specifically designed to assess the capability of MLLMs in locating elements in GUI environment

<!-- Meanless: 20 -->


<!-- Media -->

Table 2: Overview of Datasets for GUI Task Automation. The columns indicate: the platform used (Platform), the method of environment observation (Observe), support for multi-turn dialogue (Chat), whether tasks need multistep to complete (High-Level), the number of different domains included in the dataset (Domain), and the number of instances in the dataset (Instance).

<table><tr><td>Dataset</td><td>Platform</td><td>Observe</td><td>Chat</td><td>High-Level</td><td>Domain</td><td>Instance</td></tr><tr><td>Meta-GUI 69</td><td>Mobile</td><td>ScreenShot, VH</td><td>✓</td><td>✓</td><td>11</td><td>1125</td></tr><tr><td>MobileGPT 71</td><td>Mobile</td><td>ScreenShot, VH</td><td>✓</td><td>✓</td><td>8</td><td>160</td></tr><tr><td>PixelHelp 11</td><td>Mobile</td><td>ScreenShot, VH</td><td>✘</td><td>✓</td><td>-</td><td>187</td></tr><tr><td>RICOSCA 11</td><td>Mobile</td><td>ScreenShot, VH</td><td>✘</td><td>✘</td><td>9.7k</td><td>25,677</td></tr><tr><td>MoTiF 120</td><td>Mobile</td><td>ScreenShot, VH</td><td>✘</td><td>✓</td><td>125</td><td>61K</td></tr><tr><td>UGIF 121</td><td>Mobile</td><td>ScreenShot, VH</td><td>✘</td><td>✓</td><td>11</td><td>4184</td></tr><tr><td>MobileAgentBench 122</td><td>Mobile</td><td>ScreenShot, VH</td><td>✘</td><td>✓</td><td>10</td><td>100</td></tr><tr><td>DroidTask [123]</td><td>Mobile</td><td>ScreenShot, VH</td><td>✘</td><td>✓</td><td>13</td><td>158</td></tr><tr><td>AndroidEnv 118</td><td>Mobile</td><td>ScreenShot</td><td>✘</td><td>✓</td><td>30</td><td>100</td></tr><tr><td>MobileEnv 124</td><td>Mobile</td><td>ScreenShot, VH</td><td>✘</td><td>✓</td><td>-</td><td>856,045</td></tr><tr><td>MobileAgentBench 122</td><td>Mobile</td><td>ScreenShot, VH</td><td>✘</td><td>✓</td><td>10</td><td>100</td></tr><tr><td>LlamaTouch 125</td><td>Mobile</td><td>ScreenShot, VH</td><td>✘</td><td>✓</td><td>57</td><td>496</td></tr><tr><td>WebArena 126</td><td>Computer</td><td>ScreenShot, DOM</td><td>✘</td><td>✓</td><td>6</td><td>812</td></tr><tr><td>VWA 127</td><td>Computer</td><td>ScreenShot, DOM</td><td>✘</td><td>✓</td><td>3</td><td>910</td></tr><tr><td>WebVoyager 86</td><td>Computer</td><td>ScreenShot, DOM</td><td>✘</td><td>✓</td><td>15</td><td>300</td></tr><tr><td>WebShop 128</td><td>Computer</td><td>ScreenShot, DOM</td><td>✘</td><td>✓</td><td>1</td><td>12,087</td></tr><tr><td>MninWoB++ 13</td><td>Computer</td><td>ScreenShot, DOM</td><td>✘</td><td>✘</td><td>100</td><td>100</td></tr><tr><td>WebLINX 115</td><td>Computer</td><td>ScreenShot, DOM</td><td>✓</td><td>✓</td><td>155</td><td>2337</td></tr><tr><td>RUSS 116</td><td>Computer</td><td>ScreenShot, DOM</td><td>✓</td><td>✓</td><td>22</td><td>80</td></tr><tr><td>PharseNode 129</td><td>Computer</td><td>ScreenShot, DOM</td><td>✘</td><td>✘</td><td>-</td><td>51,663</td></tr><tr><td>UIBert 130</td><td>Computer</td><td>ScreenShot, DOM</td><td>✘</td><td>✘</td><td>-</td><td>16,660</td></tr><tr><td>Mind2Web 81</td><td>Computer</td><td>ScreenShot, DOM</td><td>✘</td><td>✓</td><td>137</td><td>2,350</td></tr><tr><td>AssistGUI 131</td><td>Computer</td><td>ScreenShot, Metadata</td><td>✘</td><td>✓</td><td>9</td><td>100</td></tr><tr><td>AITW 132</td><td>Mobile, Computer</td><td>ScreenShot</td><td>✘</td><td>✓</td><td>357</td><td>30K</td></tr><tr><td>ScreenSpot 76</td><td>Mobile, Computer</td><td>ScreenShot</td><td>✘</td><td>✘</td><td>-</td><td>1200</td></tr></table>

<!-- Media -->

<!-- Meanless: 21 -->


Other datasets [81, 11, 132] have proposed high-level GUI automation tasks, which require the agent to preform multiple steps to complete instructions. For example, Li et al. [11] proposed a task that maps text instructions to the operations people need to perform in GUI environment. Given a multistep instruction $I$ ,this task entails generating a sequence of automatically executable actions ${a}_{1} : m$ on the screenshot $S$ . The tasks proposed in the above research focus on individual queries and step-by-step operations, but in practical scenarios, users may need to chat with the agent to provide the necessary task information. Therefore, some datasets [69, 115, 116] have proposed chat datasets for GUI task automation. In this setting, given an initial user instruction scenario, the agent must engage in a multi-turn conversation with the user to complete the task.

In addition, AssistGUI [131] also propose a novel task (desktop task automation). In desktop task automation, agents perform a series of operations to complete user queries under a teaching video that provides more detailed instructions on how to complete tasks and related applications.

### 5.2. Evaluation Metrics

#### 5.2.1. Common Evaluation Criteria

For Low-Level tasks, PhraseNode [129] and UIBert [130] employ the proportion of correctly selected elements as evaluation metrics. Following these studies [133, 134], SeekClick [76] uses click accuracy as the metric. Click accuracy is defined as the proportion of test samples where the model-predicted location falls in the ground truth element bounding box

Success rate [81, 13, 128] is one of the most common metrics for assessing GUI Agents, defined as the proportion of instances where the model reaches the desired final state. However, success rate is very strict and unable to evaluate the degree of execution of failed tasks. Step Successful Rate is proposed in Mind2Web [81], defined as the proportion of successful steps to total steps. However, these metrics are not suitable for dialogue task systems 115, 135, 136]. The objective in dialogue task systems is not fully defined in the first turn or later turns; instead, it evolves as the conversation proceeds. WebLINX [115] following established approaches [135, 137 in dialogue system leverage turn-level automatic evaluation metrics: element similarity and text similarity. Element similarity and text similarity both rely on intent matching (IM),where the given predicted action ${a}^{\prime }$ and reference action $a$ ,if the intent is consistent, ${IM}\left( {{a}^{\prime },a}\right)  = 1$ ; otherwise, ${IM}\left( {{a}^{\prime },a}\right)  = 0$ .

<!-- Meanless: 22 -->


The measure of element similarity is specific to operations that take elements as parameters (such as clicking, text input, submission). It calculates the similarity between elements by measuring the intersection over the union [138] between bounding boxes, using the following formula:

$$
{IM}\left( {{a}^{\prime },a}\right)  \times  \frac{{B}_{\text{reference }} \cup  {B}_{\text{predicted }}}{{B}_{\text{reference }} \cap  {B}_{\text{predicted }}} \tag{1}
$$

Here, ${B}_{\text{reference }}$ and ${B}_{\text{predicted }}$ are the coordinates of the ground truth and predicted bounding boxes, respectively.

To assess Text Similarity, the authors compute the chrF value [139] between the text generated agent and the ground truth text, i.e., the F1 score for character n-gram matching (using the default setting $n = 6$ ). Similar to Element Similarity,Text Similarity also scales using ${IM}$ ,resulting in the final formula:

$$
{IM}\left( {{a}^{\prime },a}\right)  \times  {CHRF}\left( {{a}^{\prime },a}\right)  \tag{2}
$$

Finally, WebLINX evaluates the performance of the element group (EG), including click, text input, and submit, using Element Similarity, while the text group (TG), including load, say, and text input, is evaluated using Text Similarity.

#### 5.2.2. Other Evaluation Methods

However, since there can be different ways to accomplish the same task, using a single operation way as the ground truth for evaluating accuracy may not be entirely correct. Therefore, some studies propose evaluation criteria that involve manually designing key step rewards [124]. While manually designing reward functions can provide a more accurate assessment of model performance, it can also be more cumbersome. In this work [86], the use of GPT-4V for automatically evaluating these trajectories. The results indicated that GPT-4V achieved an consistency of 85.3% compared to manual evaluations. This demonstrates the enormous potential of model evaluation methods.

<!-- Meanless: 23 -->


Compared to the method of directly inspecting action sequences mentioned above, MobileAgentBench [122] and LlamaTouch [125] evaluate success by comparing specific states within the task. MobileAgentBench provides a low-intrusion integration with existing agents, allowing evaluation with just a few lines of code. It determines task success by examining the final User Interface (UI) state after task execution. LlamaTouch, on the other hand, is capable of matching various UI states at different levels, ensuring a more faithful evaluation of tasks in real dynamic environments. Table 3 presents the performance of state-of-the-art methods on the LlamaTouch and MobileAgentBench frameworks, using Success Rate (SR) and Step Success Rate (SSR) as evaluation metrics.

## 6. Challenges and Opportunities

In this section, we discuss the current challenges and opportunities of LLM-Based GUI Agents. In Section 6.1, we discuss the computation cost issues associated with GUI Agents. This issue in fact poses a significant bottleneck to GUI Agents development, particularly for GUI Agents equipped with visual capabilities. We then continue in Section 6.2 to discuss the feasibility, completeness, and security concerns in task automation. Finally, in Section 6.3, we explore the connection between AI operating system(AIOS) agents and GUI Agents.

### 6.1. Cost Issues: A Bottleneck in GUI Agents Development

LLM-based GUI Agents may incur significant costs in completing GUI tasks, which severely limits their commercial application. The price for calling the ChatGPT API [16] by commercial LLMs is \$1.5 for inputting, 1000K tokens. Even if the Agent chooses the open-source LLM, the computational cost is still very high. For example, a 7B LLaMA [140] inference one token requires 6.7 billion floating-point operations. Completing a GUI task typically requires 2-8 steps.

One way to reduce the cost of task automation is to use tiny backbone architecture LLMs. At present, many researchers have attempted to integrate smaller backbone architecture into the foundation model (LLMs [141], 142, 143], MLLMs [144, 145, 146]), which have smaller parameters and can even achieve results with larger parameters on specific tasks. However there is little research exploring the application of small backbone architecture in GUI automation tasks. Another way to reduce inference overhead is through LLMs compression. Currently, model compression techniques are quite mature, including quantization [147, 148], pruning [149, 150], knowledge distillation [151, 152], and low-rank decomposition [153, 154].

<!-- Meanless: 24 -->


Deploying large generative language models (LLMs) directly on resource-constrained hardware is infeasible due to their high computational costs. Recent research [155, 156, 157] has focused on optimizing LLMs for mobile and embedded devices. Specifically, the study [155] introduces LLMS to minimize context-switching overheads for LLMs under tight memory budgets on mobile devices. It achieves this by separating memory management of application and LLM contexts from the key ideas of fine-grained, block-level, globally optimized KV cache compression and swapping.

### 6.2. Feasibility, Completeness, and Security in Task Automation

In task automation, feasibility, completeness, and security were mentioned early in ResponsibleTA [158]. Feasibility refers to predicting whether the low-level commands generated by the LLM are executable by the executor. If the command is not feasible, the system will request a replan to avoid executing unattainable instructions, thereby improving the controllability and efficiency of task automation. Completeness refers to checking whether the execution results of low-level commands align with the target of the given command and providing timely feedback so that the LLM coordinator can more reasonably rearrange the next steps. Lastly, security emphasizes the protection of users' sensitive information, which is especially important for GUI automation tasks involving personal devices. To ensure feasibility and completeness, ResponsibleTA proposed the Feasibility Predictor and Completeness Verifier to assess commands before and after execution. Regarding security, ResponsibleTA uses a named entity recognition (NER) model to automatically detect sensitive information in user instructions, replacing it with predefined placeholders, which are then stored in edge-deployed memory. Although these three properties are critical in task automation, few studies have explored them.

One of the key reasons that reduces the feasibility and completeness of automated tasks is the hallucination phenomena. The hallucination effect refers to the generation of content by foundational models (LLMs [159], MLLMs [160]) that is syntactically correct but factually or logically incorrect. Hallucination phenomena can be divided into factual and faithful hallucinations [161]. Factual hallucinations can be effectively detected by retrieving external facts and estimating the uncertainty of the factual content generated by the LLM. For faithful hallucinations, a simple strategy is to use the LLM as an evaluator to assess the fidelity of the generated content. Additionally, the self-reflection mechanism can be utilized to reduce the generation of erroneous content and promote self-improvement.

<!-- Meanless: 25 -->


For privacy protection, ResponsibleTA adopts a Data Masking method, which replaces sensitive information with placeholders before sending the data to cloud inference devices. Another way to protect privacy is to localize the LLM, reducing potential risks during data transmission, though this often faces challenges related to computational resource limitations.

#### 6.3.The Connection Between AIOS and GUI Agents

In fact, AI-OS [162] is a cutting-edge concept that considers LLMs as the core of the operating system, providing a computing environment fundamentally different from traditional operating systems (OS). In this framework, the LLM serves as the kernel of the OS, responsible for managing and scheduling the system's core functions. In AI-OS, agents are treated as applications, capable of performing various tasks and demonstrating intelligent problem-solving abilities across different scenarios.

The GUI Agent is an AI system that can understand and operate GUI. GUI represents the primary means of interaction between users and computer programs, mobile apps, or other digital devices, typically comprising windows, buttons, menus, icons, and other elements. The goal of the GUI Agent is to automate tasks within the user interface, such as filling out forms, clicking buttons, conducting searches, or browsing the web.

Intuitively, the GUI Agent can take on the role of an application within the AI-OS framework, enhancing its capabilities and functionalities through efficient management provided by this framework. For instance, in scenarios that span multiple applications, it can facilitate communication between clusters of agents.

## 7. conclusion

Our review surveys recent papers on LLM-based GUI task automation, aiming to provide readers with a comprehensive overview of GUI Agents. We then summarize the capabilities of GUI Agents, covering three fundamental abilities (GUI Environment Comprehension, Device Control, and User Interaction) and two advanced abilities (Personalized services and Agent synergy). Additionally, we categorize existing GUI Agents based on their methods of understanding the environment. Furthermore, we discuss two-stage efficient GUI task automation pipeline. Finally, this paper highlights the remaining challenges and future directions in GUI task automation, hoping to offer valuable insights to researchers and engineers in this field.

<!-- Meanless: 26 -->


## References

[1] A. Iannessi, P.-Y. Marcy, O. Clatz, A.-S. Bertrand, M. Sugimoto, A review of existing and potential computer user interfaces for modern radiology, Insights into imaging 9 (4) (2018) 599-609.

[2] L. Punchoojit, N. Hongwarittorrn, et al., Usability studies on mobile user interface design patterns: a systematic literature review, Advances in Human-Computer Interaction 2017 (2017).

[3] J. Guo, W. Zhang, T. Xia, Impact of shopping website design on customer satisfaction and loyalty: The mediating role of usability and the moderating role of trust, Sustainability 15 (8) (2023) 6347.

[4] Apple, Siri, https://www.apple.com/siri/ (2024).

[5] Y. Jiang, Y. Lu, T. Knearem, C. E. Kliman-Silver, C. Lutteroth, T. J.- J. Li, J. Nichols, W. Stuerzlinger, Computational methodologies for understanding, automating, and evaluating user interfaces, in: Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 2024, pp. 1-7.

[6] Y. Mehdi, Announcing microsoft copilot, your everyday ai companion (2023).

[7] Google, Google assistant for android, https://developer.android.com/guide/app-actions/overview (2024).

[8] A. Sugiura, Y. Koseki, Internet scrapbook: automating web browsing tasks by demonstration, in: Proceedings of the 11th annual ACM symposium on User interface software and technology, 1998, pp. 9-18.

[9] C. Bernal-Cárdenas, N. Cooper, K. Moran, O. Chaparro, A. Marcus, D. Poshyvanyk, Translating video recordings of mobile app usages into replayable scenarios, in: Proceedings of the ACM/IEEE 42nd international conference on software engineering, 2020, pp. 309-321.

<!-- Meanless: 27 -->


[10] J. Chen, A. Swearngin, J. Wu, T. Barik, J. Nichols, X. Zhang, Extracting replayable interactions from videos of mobile app usage, arXiv preprint arXiv:2207.04165 (2022).

[11] Y. Li, J. He, X. Zhou, Y. Zhang, J. Baldridge, Mapping natural language instructions to mobile ui action sequences, arXiv preprint arXiv:2005.03776 (2020).

[12] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus-ter, T. Wang, Q. Liu, P. S. Koura, et al., Opt-iml: Scaling language model instruction meta learning through the lens of generalization, arXiv preprint arXiv:2212.12017 (2022).

[13] T. Shi, A. Karpathy, L. Fan, J. Hernandez, P. Liang, World of bits: An open-domain platform for web-based agents, in: International Conference on Machine Learning, PMLR, 2017, pp. 3135-3144.

[14] E. Z. Liu, K. Guu, P. Pasupat, T. Shi, P. Liang, Reinforcement learning on web interfaces using workflow-guided exploration, arXiv preprint arXiv:1802.08802 (2018).

[15] P. C. Humphreys, D. Raposo, T. Pohlen, G. Thornton, R. Chhaparia, A. Muldal, J. Abramson, P. Georgiev, A. Santoro, T. Lillicrap, A data-driven approach for learning to control computers, in: International Conference on Machine Learning, PMLR, 2022, pp. 9466-9482.

[16] OpenAI, Introduce ChatGPT, https://openai.com/blog/chatgpt (2022).

[17] S. Pichai, D. Hassabis, Introducing gemini: our largest and most capable ai model, Google. Retrieved December 8 (2023) 2023.

[18] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al., Opt: Open pre-trained transformer language models, arXiv preprint arXiv:2205.01068 (2022).

[19] J. Goswami, K. K. Prajapati, A. Saha, A. K. Saha, Parameter-efficient fine-tuning large language model approach for hospital discharge paper summarization, Applied Soft Computing 157 (2024) 111531.

<!-- Meanless: 28 -->


[20] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou, et al., The rise and potential of large language model based agents: A survey, arXiv preprint arXiv:2309.07864 (2023).

[21] X. Team, Xagent: An autonomous agent for complex task solving (2023).

[22] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, Y. Zhuang, Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face, Advances in Neural Information Processing Systems 36 (2024).

[23] S. Wu, H. Fei, L. Qu, W. Ji, T.-S. Chua, Next-gpt: Any-to-any multimodal llm, arXiv preprint arXiv:2309.05519 (2023).

[24] A. Yan, Z. Yang, W. Zhu, K. Lin, L. Li, J. Wang, J. Yang, Y. Zhong, J. McAuley, J. Gao, et al., Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation, arXiv preprint arXiv:2311.07562 (2023).

[25] H. Tao, S. TV, M. Shlapentokh-Rothman, D. Hoiem, H. Ji, Webwise: Web interface control and sequential exploration with large language models, arXiv preprint arXiv:2310.16042 (2023).

[26] H. Wen, Y. Li, G. Liu, S. Zhao, T. Yu, T. J.-J. Li, S. Jiang, Y. Liu, Y. Zhang, Y. Liu, Autodroid: Llm-powered task automation in android, in: Proceedings of the 30th Annual International Conference on Mobile Computing and Networking, 2024, pp. 543-557.

[27] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models are few-shot learners, Advances in neural information processing systems 33 (2020) 1877-1901.

[28] X. Huang, W. Liu, X. Chen, X. Wang, D. Lian, Y. Wang, R. Tang, E. Chen, Wese: Weak exploration to strong exploitation for llm agents, arXiv preprint arXiv:2404.07456 (2024).

[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural information processing systems 30 (2017).

<!-- Meanless: 29 -->


[30] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805 (2018).

[31] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., Palm: Scaling language modeling with pathways, Journal of Machine Learning Research 24 (240) (2023) 1-113.

[32] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, Journal of machine learning research 21 (140) (2020) 1-67.

[33] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, D. Amodei, Scaling laws for neural language models, arXiv preprint arXiv:2001.08361 (2020).

[34] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained model, arXiv preprint arXiv:2210.02414 (2022).

[35] T. Gupta, A. Kembhavi, Visual programming: Compositional visual reasoning without training, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 14953- 14962.

[36] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al., Flamingo: a visual language model for few-shot learning, Advances in neural information processing systems 35 (2022) 23716-23736.

[37] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, N. Duan, Visual chatgpt: Talking, drawing and editing with visual foundation models, arXiv preprint arXiv:2303.04671 (2023).

[38] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu, M. Zeng, L. Wang, Mm-react: Prompting chatgpt for multimodal reasoning and action, arXiv preprint arXiv:2303.11381 (2023).

<!-- Meanless: 30 -->


[39] M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Mahendran, A. Arnab, M. Dehghani, Z. Shen, et al., Simple open-vocabulary object detection, in: European Conference on Computer Vision, Springer, 2022, pp. 728-755.

[40] W. Kim, B. Son, I. Kim, Vilt: Vision-and-language transformer without convolution or region supervision, in: International conference on machine learning, PMLR, 2021, pp. 5583-5594.

[41] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, in: International conference on machine learning, PMLR, 2023, pp. 19730-19742.

[42] D. Zhu, J. Chen, X. Shen, X. Li, M. Elhoseiny, Minigpt-4: Enhancing vision-language understanding with advanced large language models, arXiv preprint arXiv:2304.10592 (2023).

[43] H. Liu, C. Li, Q. Wu, Y. J. Lee, Visual instruction tuning, Advances in neural information processing systems 36 (2024).

[44] H. Zhang, X. Li, L. Bing, Video-llama: An instruction-tuned audio-visual language model for video understanding, arXiv preprint arXiv:2306.02858 (2023).

[45] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. N. Fung, S. Hoi, Instructblip: Towards general-purpose vision-language models with instruction tuning, Advances in Neural Information Processing Systems 36 (2024).

[46] F. Chen, M. Han, H. Zhao, Q. Zhang, J. Shi, S. Xu, B. Xu, X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages, arXiv preprint arXiv:2305.04160 (2023).

[47] W. Van Melle, Mycin: a knowledge-based consultation program for infectious disease diagnosis, International journal of man-machine studies 10 (3) (1978) 313-322.

[48] S. K. Sarma, K. R. Singh, A. Singh, An expert system for diagnosis of diseases in rice plant, International Journal of Artificial Intelligence 1 (1) (2010) 26-31.

<!-- Meanless: 31 -->


[49] M. F. Zarandi, S. Soltanzadeh, A. Mohammadi, O. Castillo, Designing a general type-2 fuzzy expert system for diagnosis of depression, Applied Soft Computing 80 (2019) 329-341.

[50] D. Içen, S. Günay, Design and implementation of the fuzzy expert system in monte carlo methods for fuzzy linear regression, Applied Soft Computing 77 (2019) 399-411.

[51] R. A. Brooks, Intelligence without representation, Artificial intelligence 47 (1-3) (1991) 139-159.

[52] P. Maes, Designing autonomous agents: Theory and practice from biology to engineering and back, MIT press, 1990.

[53] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M. Riedmiller, Playing atari with deep reinforcement learning, arXiv preprint arXiv:1312.5602 (2013).

[54] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru, R. Vicente, Multiagent cooperation and competition with deep reinforcement learning, PIoS one 12 (4) (2017) e0172395.

[55] J. Carapuço, R. Neves, N. Horta, Reinforcement learning applied to forex trading, Applied Soft Computing 73 (2018) 783-794.

[56] Z. Hu, X. Yu, Reinforcement learning-based comprehensive learning grey wolf optimizer for feature selection, Applied Soft Computing 149 (2023) 110959.

[57] H. Yang, S. Yue, Y. He, Auto-gpt for online decision making: Benchmarks and additional opinions, arXiv preprint arXiv:2306.02224 (2023).

[58] Langchain, Langchain, https://github.com/langchain-ai/ langchain (2023).

[59] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al., Chain-of-thought prompting elicits reasoning in large language models, Advances in neural information processing systems 35 (2022) 24824-24837.

<!-- Meanless: 32 -->


[60] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, K. Narasimhan, Tree of thoughts: Deliberate problem solving with large language models, Advances in Neural Information Processing Systems 36 (2024).

[61] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, Y. Cao, React: Synergizing reasoning and acting in language models, arXiv preprint arXiv:2210.03629 (2022).

[62] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, S. Yao, Reflexion: Language agents with verbal reinforcement learning, Advances in Neural Information Processing Systems 36 (2024).

[63] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li, L. Jiang, X. Zhang, C. Wang, Autogen: Enabling next-gen llm applications via multi-agent conversation framework, arXiv preprint arXiv:2308.08155 (2023).

[64] T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla, O. Wiest, X. Zhang, Large language model based multi-agents: A survey of progress and challenges, arXiv preprint arXiv:2402.01680 (2024).

[65] C. Qian, X. Cong, C. Yang, W. Chen, Y. Su, J. Xu, Z. Liu, M. Sun, Communicative agents for software development, arXiv preprint arXiv:2307.07924 (2023).

[66] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, I. Mordatch, Improving factuality and reasoning in language models through multiagent debate, arXiv preprint arXiv:2305.14325 (2023).

[67] Q. Zhao, J. Wang, Y. Zhang, Y. Jin, K. Zhu, H. Chen, X. Xie, Com-peteai: Understanding the competition behaviors in large language model-based agents, arXiv preprint arXiv:2310.17512 (2023).

[68] G. Li, Y. Li, Spotlight: Mobile ui understanding using vision-language models with a focus, arXiv preprint arXiv:2209.14927 (2022).

[69] L. Sun, X. Chen, L. Chen, T. Dai, Z. Zhu, K. Yu, Meta-gui: towards multi-modal conversational agents on mobile gui, arXiv preprint arXiv:2205.11029 (2022).

<!-- Meanless: 33 -->


[70] Z. Zhan, A. Zhang, You only look at screens: Multimodal chain-of-action agents, arXiv preprint arXiv:2309.11436 (2023).

[71] S. Lee, J. Choi, J. Lee, H. Choi, S. Y. Ko, S. Oh, I. Shin, Explore, select, derive, and recall: Augmenting llm with human-like memory for mobile task automation, arXiv preprint arXiv:2312.03003 3 (7) (2023) 8.

[72] Z. Yang, J. Liu, Y. Han, X. Chen, Z. Huang, B. Fu, G. Yu, Appagent: Multimodal agents as smartphone users, arXiv preprint arXiv:2312.13771 (2023).

[73] H. Wen, H. Wang, J. Liu, Y. Li, Droidbot-gpt: Gpt-powered ui automation for android, arXiv preprint arXiv:2304.07061 (2023).

[74] J. Wang, H. Xu, J. Ye, M. Yan, W. Shen, J. Zhang, F. Huang, J. Sang, Mobile-agent: Autonomous multi-modal mobile device agent with visual perception, arXiv preprint arXiv:2401.16158 (2024).

[75] J. Wang, H. Xu, H. Jia, X. Zhang, M. Yan, W. Shen, J. Zhang, F. Huang, J. Sang, Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration, arXiv preprint arXiv:2406.01014 (2024).

[76] K. Cheng, Q. Sun, Y. Chu, F. Xu, Y. Li, J. Zhang, Z. Wu, Seeclick: Harnessing gui grounding for advanced visual gui agents, arXiv preprint arXiv:2401.10935 (2024).

[77] X. Ma, Z. Zhang, H. Zhao, Coco-agent: A comprehensive cognitive mllm agent for smartphone gui automation, arXiv preprint arXiv:2402.11941 (2024).

[78] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al., Webgpt: Browser-assisted question-answering with human feedback, arXiv preprint arXiv:2112.09332 (2021).

[79] P. Shaw, M. Joshi, J. Cohan, J. Berant, P. Pasupat, H. Hu, U. Khan-delwal, K. Lee, K. N. Toutanova, From pixels to ui actions: Learning to follow instructions via graphical user interfaces, Advances in Neural Information Processing Systems 36 (2023) 34354-34370.

<!-- Meanless: 34 -->


[80] H. Furuta, K.-H. Lee, O. Nachum, Y. Matsuo, A. Faust, S. S. Gu, I. Gur, Multimodal web navigation with instruction-finetuned foundation models, arXiv preprint arXiv:2305.11854 (2023).

[81] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, Y. Su, Mind2web: Towards a generalist agent for the web, Advances in Neural Information Processing Systems 36 (2024).

[82] I. Gur, H. Furuta, A. Huang, M. Safdari, Y. Matsuo, D. Eck, A. Faust, A real-world webagent with planning, long context understanding, and program synthesis, arXiv preprint arXiv:2307.12856 (2023).

[83] G. Kim, P. Baldi, S. McAleer, Language models can solve computer tasks, Advances in Neural Information Processing Systems 36 (2024).

[84] H. Sun, Y. Zhuang, L. Kong, B. Dai, C. Zhang, Adaplanner: Adaptive planning from feedback with language models, Advances in Neural Information Processing Systems 36 (2024).

[85] B. Zheng, B. Gou, J. Kil, H. Sun, Y. Su, Gpt-4v (ision) is a generalist web agent, if grounded, arXiv preprint arXiv:2401.01614 (2024).

[86] H. He, W. Yao, K. Ma, W. Yu, Y. Dai, H. Zhang, Z. Lan, D. Yu, Webvoyager: Building an end-to-end web agent with large multimodal models, arXiv preprint arXiv:2401.13919 (2024).

[87] J. Kil, C. H. Song, B. Zheng, X. Deng, Y. Su, W.-L. Chao, Dual-view visual contextualization for web navigation, arXiv preprint arXiv:2402.04476 (2024).

[88] C. Zhang, L. Li, S. He, X. Zhang, B. Qiao, S. Qin, M. Ma, Y. Kang, Q. Lin, S. Rajmohan, et al., Ufo: A ui-focused agent for windows os interaction, arXiv preprint arXiv:2402.07939 (2024).

[89] H. Lai, X. Liu, I. L. Iong, S. Yao, Y. Chen, P. Shen, H. Yu, H. Zhang, X. Zhang, Y. Dong, et al., Autowebglm: Bootstrap and reinforce a large language model-based web navigating agent, arXiv preprint arXiv:2404.03648 (2024).

[90] Z. Song, Y. Li, M. Fang, Z. Chen, Z. Shi, Y. Huang, Mmac-copilot: Multi-modal agent collaboration operating system copilot, arXiv preprint arXiv:2404.18074 (2024).

<!-- Meanless: 35 -->


[91] B. Wang, G. Li, Y. Li, Enabling conversational interaction with mobile ui using large language models, in: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, 2023, pp. 1-17.

[92] Y. Li, Z. Yang, Y. Guo, X. Chen, Droidbot: a lightweight ui-guided test input generator for android, in: 2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C), IEEE, 2017, pp. 23-26.

[93] P. He, X. Liu, J. Gao, W. Chen, Deberta: Decoding-enhanced bert with disentangled attention, arXiv preprint arXiv:2006.03654 (2020).

[94] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, et al., Scaling instruction-finetuned language models, Journal of Machine Learning Research 25 (70) (2024) 1-53.

[95] K. Lee, M. Joshi, I. R. Turc, H. Hu, F. Liu, J. M. Eisenschlos, U. Khan-delwal, P. Shaw, M.-W. Chang, K. Toutanova, Pix2struct: Screenshot parsing as pretraining for visual language understanding, in: International Conference on Machine Learning, PMLR, 2023, pp. 18893- 18912.

[96] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774 (2023).

[97] A. Shtedritski, C. Rupprecht, A. Vedaldi, What does clip know about a red circle? visual prompt engineering for vlms, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 11987-11997.

[98] L. Yang, Y. Wang, X. Li, X. Wang, J. Yang, Fine-grained visual prompting, Advances in Neural Information Processing Systems 36 (2024).

[99] J. Yang, H. Zhang, F. Li, X. Zou, C. Li, J. Gao, Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v, arXiv preprint arXiv:2310.11441 (2023).

<!-- Meanless: 36 -->


[100] Z. Peng, W. Wang, L. Dong, Y. Hao, S. Huang, S. Ma, F. Wei, Kosmos- 2: Grounding multimodal large language models to the world, arXiv preprint arXiv:2306.14824 (2023).

[101] Y. Zhao, Z. Lin, D. Zhou, Z. Huang, J. Feng, B. Kang, Bubogpt: Enabling visual grounding in multi-modal llms, arXiv preprint arXiv:2307.08581 (2023).

[102] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, R. Zhao, Shikra: Unleashing multimodal llm's referential dialogue magic, arXiv preprint arXiv:2306.15195 (2023).

[103] OpenAI, Gpt-4v(ision) system card (2023). URL https://cdn.openai.com/papers/GPTV_System_Card.pdf

[104] S. Sunkara, M. Wang, L. Liu, G. Baechler, Y.-C. Hsiao, A. Sharma, J. Stout, et al., Towards better semantic understanding of mobile interfaces, arXiv preprint arXiv:2210.02663 (2022).

[105] K. You, H. Zhang, E. Schoop, F. Weers, A. Swearngin, J. Nichols, Y. Yang, Z. Gan, Ferret-ui: Grounded mobile ui understanding with multimodal llms, arXiv preprint arXiv:2404.05719 (2024).

[106] W. Hong, W. Wang, Q. Lv, J. Xu, W. Yu, J. Ji, Y. Wang, Z. Wang, Y. Dong, M. Ding, et al., Cogagent: A visual language model for gui agents, arXiv preprint arXiv:2312.08914 (2023).

[107] T. Iki, A. Aizawa, Do berts learn to use browser user interface? exploring multi-step tasks with unified vision-and-language berts, arXiv preprint arXiv:2203.07828 (2022).

[108] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al., Evaluating large language models trained on code, arXiv preprint arXiv:2107.03374 (2021).

[109] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, et al., Codebert: A pre-trained model for programming and natural languages, arXiv preprint arXiv:2002.08155 (2020).

<!-- Meanless: 37 -->


[110] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al., Competition-level code generation with alphacode, Science 378 (6624) (2022) 1092-1097.

[111] Y. Wang, W. Wang, S. Joty, S. C. Hoi, Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation, arXiv preprint arXiv:2109.00859 (2021).

[112] Y. Li, H. Wen, W. Wang, X. Li, Y. Yuan, G. Liu, J. Liu, W. Xu, X. Wang, Y. Sun, et al., Personal llm agents: Insights and survey about the capability, efficiency and security, arXiv preprint arXiv:2401.05459 (2024).

[113] Microsoft, Bing web search api, https://www.microsoft.com/en-us/ bing/apis/bing-web-search-api (2023).

[114] W. Li, F.-L. Hsu, W. Bishop, F. Campbell-Ajala, O. Riva, M. Lin, Uinav: A maker of ui automation agents, arXiv preprint arXiv:2312.10170 (2023).

[115] X. H. Lù, Z. Kasner, S. Reddy, Weblinx: Real-world website navigation with multi-turn dialogue, arXiv preprint arXiv:2402.05930 (2024).

[116] N. Xu, S. Masling, M. Du, G. Campagna, L. Heck, J. Landay, M. S. Lam, Grounding open-domain instructions to automate web support tasks, arXiv preprint arXiv:2103.16057 (2021).

[117] Z. Wu, C. Han, Z. Ding, Z. Weng, Z. Liu, S. Yao, T. Yu, L. Kong, Os-copilot: Towards generalist computer agents with self-improvement, arXiv preprint arXiv:2402.07456 (2024).

[118] D. Toyama, P. Hamel, A. Gergely, G. Comanici, A. Glaese, Z. Ahmed, T. Jackson, S. Mourad, D. Precup, Androidenv: A reinforcement learning platform for android, arXiv preprint arXiv:2105.13231 (2021).

[119] A. Developers, Create your own accessibility service (2020).

[120] A. Burns, D. Arsan, S. Agrawal, R. Kumar, K. Saenko, B. A. Plummer, A dataset for interactive vision-language navigation with unknown command feasibility, in: European Conference on Computer Vision, Springer, 2022, pp. 312-328.

<!-- Meanless: 38 -->


[121] S. G. Venkatesh, P. Talukdar, S. Narayanan, Ugif: Ui grounded instruction following, arXiv preprint arXiv:2211.07615 (2022).

[122] L. Wang, Y. Deng, Y. Zha, G. Mao, Q. Wang, T. Min, W. Chen, S. Chen, Mobileagentbench: An efficient and user-friendly benchmark for mobile llm agents, arXiv preprint arXiv:2406.08184 (2024).

[123] H. Wen, Y. Li, G. Liu, S. Zhao, T. Yu, T. J.-J. Li, S. Jiang, Y. Liu, Y. Zhang, Y. Liu, Empowering llm to use smartphone for intelligent task automation, arXiv preprint arXiv:2308.15272 (2023).

[124] D. Zhang, L. Chen, Z. Zhao, R. Cao, K. Yu, Mobile-env: An evaluation platform and benchmark for interactive agents in llm era, arXiv preprint arXiv:2305.08144 (2023).

[125] L. Zhang, S. Wang, X. Jia, Z. Zheng, Y. Yan, L. Gao, Y. Li, M. Xu, Llamatouch: A faithful and scalable testbed for mobile ui automation task evaluation, arXiv preprint arXiv:2404.16054 (2024).

[126] S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, Y. Bisk, D. Fried, U. Alon, et al., Webarena: A realistic web environment for building autonomous agents, arXiv preprint arXiv:2307.13854 (2023).

[127] J. Y. Koh, R. Lo, L. Jang, V. Duvvur, M. C. Lim, P.-Y. Huang, G. Neu-big, S. Zhou, R. Salakhutdinov, D. Fried, Visualwebarena: Evaluating multimodal agents on realistic visual web tasks, arXiv preprint arXiv:2401.13649 (2024).

[128] S. Yao, H. Chen, J. Yang, K. Narasimhan, Webshop: Towards scalable real-world web interaction with grounded language agents, Advances in Neural Information Processing Systems 35 (2022) 20744-20757.

[129] P. Pasupat, T.-S. Jiang, E. Z. Liu, K. Guu, P. Liang, Mapping natural language commands to web elements, arXiv preprint arXiv:1808.09132 (2018).

[130] C. Bai, X. Zang, Y. Xu, S. Sunkara, A. Rastogi, J. Chen, et al., Uib-ert: Learning generic multimodal representations for ui understanding, arXiv preprint arXiv:2107.13731 (2021).

<!-- Meanless: 39 -->


[131] D. Gao, L. Ji, Z. Bai, M. Ouyang, P. Li, D. Mao, Q. Wu, W. Zhang, P. Wang, X. Guo, et al., Assistgui: Task-oriented desktop graphical user interface automation, arXiv preprint arXiv:2312.13108 (2023).

[132] C. Rawles, A. Li, D. Rodriguez, O. Riva, T. Lillicrap, Android in the wild: A large-scale dataset for android device control, arXiv preprint arXiv:2307.10088 (2023).

[133] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al., Mmbench: Is your multi-modal model an all-around player?, arXiv preprint arXiv:2307.06281 (2023).

[134] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, L. Wang, Mm-vet: Evaluating large multimodal models for integrated capabilities, arXiv preprint arXiv:2308.02490 (2023).

[135] A. Rastogi, X. Zang, S. Sunkara, R. Gupta, P. Khaitan, Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset, in: Proceedings of the AAAI conference on artificial intelligence, Vol. 34, 2020, pp. 8689-8696.

[136] H. Ji, J. C. Park, R. Xia, Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing: System demonstrations, in: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, 2021.

[137] Y. Zhang, S. Sun, M. Galley, Y.-C. Chen, C. Brockett, X. Gao, J. Gao, J. Liu, B. Dolan, Dialogpt: Large-scale generative pre-training for conversational response generation, arXiv preprint arXiv:1911.00536 (2019).

[138] P. Jaccard, The distribution of the flora in the alpine zone. 1, New phytologist 11 (2) (1912) 37-50.

[139] M. Popović, chrf: character n-gram f-score for automatic mt evaluation, in: Proceedings of the tenth workshop on statistical machine translation, 2015, pp. 392-395.

<!-- Meanless: 40 -->


[140] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971 (2023).

[141] P. Zhang, G. Zeng, T. Wang, W. Lu, Tinyllama: An open-source small language model, arXiv preprint arXiv:2401.02385 (2024).

[142] M. Javaheripi, S. Bubeck, M. Abdin, J. Aneja, S. Bubeck, C. C. T. Mendes, W. Chen, A. Del Giorno, R. Eldan, S. Gopi, et al., Phi-2: The surprising power of small language models, Microsoft Research Blog (2023).

[143] S. Hu, Y. Tu, X. Han, C. He, G. Cui, X. Long, Z. Zheng, Y. Fang, Y. Huang, W. Zhao, et al., Minicpm: Unveiling the potential of small language models with scalable training strategies, arXiv preprint arXiv:2404.06395 (2024).

[144] Z. Yuan, Z. Li, L. Sun, Tinygpt-v: Efficient multimodal large language model via small backbones, arXiv preprint arXiv:2312.16862 (2023).

[145] Y. Li, Y. Zhang, C. Wang, Z. Zhong, Y. Chen, R. Chu, S. Liu, J. Jia, Mini-gemini: Mining the potential of multi-modality vision language models, arXiv preprint arXiv:2403.18814 (2024).

[146] Y. Zhu, M. Zhu, N. Liu, Z. Ou, X. Mou, J. Tang, Llava-phi: Efficient multi-modal assistant with small language model, arXiv preprint arXiv:2401.02330 (2024).

[147] S.-y. Liu, Z. Liu, X. Huang, P. Dong, K.-T. Cheng, Llm-fp4: 4-bit floating-point quantized transformers, arXiv preprint arXiv:2310.16836 (2023).

[148] T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, Gpt3. int8 (   ): 8- bit matrix multiplication for transformers at scale, Advances in Neural Information Processing Systems 35 (2022) 30318-30332.

[149] X. Ma, G. Fang, X. Wang, Llm-pruner: On the structural pruning of large language models, Advances in neural information processing systems 36 (2023) 21702-21720.

<!-- Meanless: 41 -->


[150] M. Xia, T. Gao, Z. Zeng, D. Chen, Sheared llama: Accelerating language model pre-training via structured pruning, arXiv preprint arXiv:2310.06694 (2023).

[151] E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, Gptq: Accurate post-training quantization for generative pre-trained transformers, arXiv preprint arXiv:2210.17323 (2022).

[152] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, S. Han, Smoothquant: Accurate and efficient post-training quantization for large language models, in: International Conference on Machine Learning, PMLR, 2023, pp. 38087-38099.

[153] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, Lora: Low-rank adaptation of large language models, arXiv preprint arXiv:2106.09685 (2021).

[154] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: Efficient finetuning of quantized llms, Advances in Neural Information Processing Systems 36 (2024).

[155] W. Yin, M. Xu, Y. Li, X. Liu, Llm as a system service on mobile devices, arXiv preprint arXiv:2403.11805 (2024).

[156] R. Jin, Q. Xu, M. Wu, Y. Xu, D. Li, X. Li, Z. Chen, Llm-based knowledge pruning for time series data analytics on edge-computing devices, arXiv preprint arXiv:2406.08765 (2024).

[157] Y. Zhao, M. Lin, H. Tang, Q. Wu, J. Wang, Merino: Entropy-driven design for generative language models on iot devices, arXiv preprint arXiv:2403.07921 (2024).

[158] Z. Zhang, X. Zhang, W. Xie, Y. Lu, Responsible task automation: Empowering large language models as responsible task automators, arXiv preprint arXiv:2306.01242 (2023).

[159] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, P. Fung, Survey of hallucination in natural language generation, ACM Computing Surveys 55 (12) (2023) 1-38.

<!-- Meanless: 42 -->


[160] V. Rawte, A. Sheth, A. Das, A survey of hallucination in large foundation models, arXiv preprint arXiv:2309.05922 (2023).

[161] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, et al., A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, arXiv preprint arXiv:2311.05232 (2023).

[162] Y. Ge, Y. Ren, W. Hua, S. Xu, J. Tan, Y. Zhang, Llm as os, agents as apps: Envisioning aios, agents and the aios-agent ecosystem, arXiv e-prints (2023) arXiv-2312.

<!-- Meanless: 43 -->