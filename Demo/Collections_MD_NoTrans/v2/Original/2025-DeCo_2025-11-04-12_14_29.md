

<!-- Meanless: OSASS IOSS IRASSSSSSSS-->

# DeCo: Task Decomposition and Skill Composition for Zero-Shot Generalization in Long-Horizon 3D Manipulation

Zixuan Chen ${}^{1, * }$ Junhui Yin ${}^{1, * }$ Yangtao Chen ${}^{1}$ Jing Huo ${}^{1, \dagger  }$

Pinzhuo Tian ${}^{2}$ Jieqi Shi ${}^{1}$ Yiwen Hou ${}^{3}$ Yinchuan Li ${}^{4}$ Yang Gao ${}^{1}$ ${}^{1}$ Nanjing University ${}^{2}$ Shanghai University

${}^{3}$ National University of Singapore ${}^{4}$ Huawei

Abstract: Generalizing language-conditioned multi-task imitation learning (IL) models to novel long-horizon 3D manipulation tasks remains a significant challenge. To address this, we propose DeCo (Task Decomposition and Skill Composition), a model-agnostic framework compatible with various multi-task IL models, designed to enhance their zero-shot generalization to novel, compositional, long-horizon 3D manipulation tasks. DeCo first decomposes IL demonstrations into a set of modular atomic tasks based on the physical interaction between the gripper and objects, and constructs an atomic training dataset that enables models to learn a diverse set of reusable atomic skills during imitation learning. At inference time, DeCo leverages a vision-language model (VLM) to parse high-level instructions for novel long-horizon tasks, retrieve the relevant atomic skills, and dynamically schedule their execution; a spatially-aware skill-chaining module then ensures smooth, collision-free transitions between sequential skills. We evaluate DeCo in simulation using DeCoBench, a benchmark specifically designed to assess zero-shot generalization of multi-task IL models in compositional long-horizon 3D manipulation. Across three representative multitask IL models—RVT-2, 3DDA, and ARP—DeCo achieves success rate improvements of ${66.67}\% ,{21.53}\%$ ,and ${57.92}\%$ ,respectively,on 12 novel compositional tasks. Moreover, in real-world experiments, a DeCo-enhanced model trained on only 6 atomic tasks successfully completes 9 novel long-horizon tasks, yielding an average success rate improvement of 53.33% over the base multi-task IL model. Video demonstrations are available at: https://deco226.github.io

Keywords: Multi-task Imitation Learning, Task Decomposition, Skill Composition, Long-Horizon 3D Manipulation, Generalization

## 1 Introduction

In recent years, imitation learning (IL) has emerged as a mainstream approach for robotic manipulation. By leveraging visual demonstrations and language instructions, IL trains language-conditioned multi-task control policies, enabling robots to acquire diverse skills and perform complex tasks in unstructured 3D environments. However, current multi-task IL models still suffer from limited generalization $\left\lbrack  {1,2,3,4,5}\right\rbrack$ ,particularly when facing novel long-horizon 3D manipulation tasks [6]- even when such tasks are merely sequential compositions of previously learned skills. For instance, a model may have learned to follow individual instructions such as "open drawer", "put block in opened drawer", and "close drawer", yet still fail to execute the composed instruction "put block into the closed drawer and then close drawer". This failure stems from the model's inability to decompose novel tasks and to retrieve, schedule, and perform the correct composition of its learned skills-failing to recognize that the task can be completed by sequentially executing three known skills: opening the drawer, placing the block, and then closing the drawer. Such limitations in task decomposition and skill composition severely undermine the real-world applicability and scalability of current multi-task IL models. Although vision-language models (VLMs) have been used to generate subtasks for long-horizon tasks via instruction plans $\left\lbrack  {7,8,6}\right\rbrack$ ,executable code $\left\lbrack  {9,{10}}\right\rbrack$ ,spatial keypoints [11], or affordance maps [12, 13], they often fail to align high-level semantic plans with low-level execution. Low-level tasks are typically limited to simple motion planning or pretrained skills, and the semantic decomposition does not directly map to the physical skill space. This gap limits the effective composition of low-level skills, ultimately hindering zero-shot performance on long-horizon 3D manipulation tasks [14, 15, 16].

---

<!-- Footnote -->

*: denotes equal contribution

${}^{ \dagger  }$ :Correspondence to: huojing@nju.edu.cn

<!-- Footnote -->

---


<!-- Media -->

<!-- figureText: New Perspective on the Training Dataset Zero-Shot Generalization Put the yellow block into the closed top drawer then close drawer. ............ Take the yellow block out of the bottom drawer and take the blue bloc out of the top drawer, then place them on the drawer's surface. (b) DeCo-Enhanced Models Zero-Shot Execution of Real-World Long-Horizon Tasks Traditional IL Training Dataset Traditional Multi-task Fali to generalize Long Horizon Manipulation Tasks Generalize DeCo-Enhanced IL Training Multi-task IL Models Dataset Take out.. Decomposed Atomic Tasks based on Gripper's Physical Interaction Multiple DeCo Training Dataset Atomic Skills (a) Insight and Performance of DeCo Framework -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_1.jpg?x=308&y=200&w=1176&h=380&r=0"/>

Figure 1: We present DeCo, a model-agnostic framework that enables diverse multi-task IL models to zero-shot generalize to novel yet compositional long-horizon 3D manipulation tasks.

<!-- Media -->

In this paper, we aim to enhance the zero-shot generalization of multi-task IL models to novel, compositional long-horizon 3D manipulation tasks—tasks that are unseen during training but can be solved by composing previously learned skills through semantic reasoning. To achieve this, we propose a model-agnostic framework for skill learning and composition, compatible with a wide range of multi-task IL models. This framework enables models to decompose novel tasks into reusable atomic skills, flexibly schedule them, and execute without additional training. The core question we address is: How can long-horizon tasks be decomposed into learned skills such that multi-task IL models can interpret their structure, plan accordingly, and successfully complete the overall task in a 3D environment? To this end, we propose DeCo (Task Decomposition and Skill Compositon), a model-agnostic framework compatible with various multi-task IL models, enabling zero-shot generalization to novel yet compositional long-horizon 3D manipulation tasks, as illustrated in Figure 1. DeCo consists of three key components: First, inspired by how humans decompose long-horizon tasks through hand-object interactions, DeCo proposes a new perspective on training datasets for multi-task imitation learning, based on prior methods of subtask discovery [13, 17]. It preprocesses original IL demonstrations by analyzing the physical interactions between the gripper and objects, decomposing them into a set of modular and reusable atomic tasks. Each task is paired with a natural language instruction and a goal pose, forming an atomic training dataset for training a multi-task IL model to acquire diverse skills. Second, during testing, DeCo uses VLMs to parse the novel language instructions and visual inputs, retrieve relevant atomic instructions from the atomic training dataset, and generate an execution plan. The multi-task IL model sequentially executes the skills, while DeCo monitors task progress via gripper interactions, enabling dynamic scheduling and flexible skill composition. Finally, to ensure smooth transitions between skills, DeCo builds a spatially aware cost map for the scene to calculate collision-free chaining poses, guiding the robotic arm between sequential skills and ensuring motion continuity and safety.

We carry out extensive evaluations in both simulated and real-world settings. In simulation, we introduce DeCoBench, a new benchmark built upon RLBench [18], designed to systematically evaluate the zero-shot generalization capabilities of multi-task IL models on novel yet compositional long-horizon 3D manipulation tasks. We equip DeCo with three representative multi-task IL models—RVT-2 [3], 3DDA [19], and ARP [5]—and evaluate their performance on DeCoBench. Experiments show that DeCo significantly boosts the generalization performance across all three models. Beyond simulation, we design 6 atomic tasks grounded in physical interaction to train a multi-task IL model, and evaluate it on 9 novel yet compositional long-horizon tasks. The DeCo-enhanced model demonstrates strong zero-shot generalization, validating the practicality of DeCo.

<!-- Meanless: 2-->




Our main contributions are follows: (1) We introduce DeCo, a model-agnostic framework that equips diverse multi-task IL models with zero-shot generalization capabilities for novel yet compositional long-horizon 3D manipulation tasks. (2) We introduce DeCoBench, a benchmark for systematically evaluating zero-shot generalization in multi-task IL models on compositional long-horizon 3D manipulation tasks. Extensive experiments on DeCoBench show that DeCo significantly improves the generalization of three representative multi-task IL models, validating its effectiveness. (3) We validate DeCo in real-world settings by constructing 6 atomic tasks and 9 novel yet composi-tonal long-horizon tasks. Results demonstrate that DeCo effectively enables multi-task IL models to achieve zero-shot generalization in novel long-horizon tasks, underscoring its practical applicability.

## 2 Related Work

Learning Manipulation Policies from Demonstrations Learning manipulation policies from offline visual demonstrations has garnered significant attention, fueled by advances in visual perception [20, 21]. Early 2D-based approaches [22, 23, 24, 25, 26, 14, 27, 28] have demonstrated success in simple pick-and-place tasks, benefiting from fast training, low hardware requirements, and modest computational demands. However, their reliance on pretrained image encoders and limited spatial understanding makes them less effective for tasks requiring high-precision and robust 3D interactions. To address this, works such as C2F-ARM [29] and PerAct [1] extend learning to 6-DoF actions in 3D environments, but they still require training separate task-specific policies. More recent efforts $\left\lbrack  {2,{30},{31},{32},{13},3,{19},5,6}\right\rbrack$ aim to develop unified multi-task imitation learning (IL) models that can perform diverse tasks from heterogeneous demonstrations. This shift is crucial for building general-purpose robotic agents. However, most of these models are limited to tasks observed during training, and particularly struggle to generalize to novel long-horizon scenarios, which hinders their deployment in real-world applications [6]. To address this limitation, we propose a model-agnostic framework that is compatible with existing representative multi-task imitation learning (IL) models, enabling them to achieve zero-shot generalization to novel long-horizon 3D manipulation tasks.

Methods for Long-Horizon Manipulation A common strategy for long-horizon manipulation is to decompose complex tasks into sequential subtasks using predefined action primitives (e.g., grasp, place, pull) [33, 34, 35] or environment-specific cues [14, 36, 37, 38, 39, 40]. While effective in structured settings, these methods lack compositional flexibility and generalization, making them fragile to goal shifts and environmental changes, and limiting scalability in multi-task IL. Recent work leverages Vision-Language Models (VLMs) to enhance subtask generation by exploiting their semantic understanding and decomposition capabilities. VLMs facilitate high-level planning via natural language,executable code,spatial keypoints,or affordance maps [7,8,9,10,12,13]. However, they often fail to align high-level semantic plans with low-level execution. Low-level behaviors remain constrained to motion primitives or pretrained skills, and semantic decomposition seldom maps directly to the physical skill space. This misalignment limits skill composition and hinders generalization in long-horizon 3D manipulation [14, 15, 16]. To address these issues, we propose a new atomic task construction with modularity and reusability, enabling consistent decomposition across diverse scenarios and compatibility with diverse multi-task IL models. We also introduce a spatially-aware skill chaining module with collision avoidance. Combined with VLM-guided planning, our framework improves generalization and robustness in compositional long-horizon tasks.

## 3 Method

In this section, we introduce DeCo (Task Decomposition and Skill Composition), a model-agnostic framework compatible with diverse existing multi-task IL models, enabling zero-shot generalization

<!-- Meanless: 3-->




<!-- Media -->

<!-- figureText: Physical Interaction-based Task Decomposition VLM-Guided Planning and Spatially-Aware Skill Scheduling (Sec. 3.3) Skill Chaining (Sec. 3.4) Novel yet Compositional Long-horizon Tas Next Skill Novel Language Instruction "Place the block in the closed top drawer, and then close the drawer" Spatially-Aware Cost Map Atomic Generation Instruction Library Spatially Atomic Instruction Language retrieve Model Cost Map Atomic Instruction (GPT40) Open the top drawer Place the block in the top Close the top drawer Library IL Model Skill Schedule Spatially-Aware Skill Chaining Module Start Pose Motion Planning Path ☉奇霓捐牓扃搶堵裝置薋薋睹碳嘧嘧朦ˋ駕察帑帑晷睯晷晁晁晁填荷储蓄屋堡\{ and Skill Learning (Sec. 3.2) (1) Physical Interaction-based Task Decomposition Original IL Training Tasks Demo Atomic Tasks open Atomic Training Dataset Atomic Instruction Keyframe RGB-D _____(optional) Discovery Atomic instruction Task Plan. Diffusion-based Model Various ...... Multi-task Multiple Autoregressive- based Model (2) Atomic Skill Learning with Multi-task IL Models loop -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_3.jpg?x=362&y=203&w=1064&h=582&r=0"/>

Figure 2: An overview of DeCo framework.

<!-- Media -->

to novel yet compositional long-horizon 3D manipulation tasks. Figure 2 shows DeCo's workflow within multi-task IL models. Sec. 3.1 introduces the problem formulation based on gripper-object physical interactions. Sec. 3.2 explains how DeCo processes the original training demonstrations of multi-task imitation learning (IL) models into atomic tasks and constructs a paired atomic instruction library and training dataset to achieve atomic skill learning. Sec. 3.3 details how DeCo utilizes vision-language models (VLMs) to plan novel task instructions and schedule atomic skills accordingly. Finally, Sec. 3.4 presents the spatially-aware skill chaining module, ensuring collision-free execution of long-horizon sequence.

<!-- Media -->

<!-- figureText: Put the yellow block into the bottom drawer (w/o close) Put yellow block in opened bottom drawer (full) Pick yellow block (half 1) Place it in bottom drawer (half 2) Open bottom drawer (full) Grasp bottom handle (half 1) Pull it out that -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_3.jpg?x=361&y=1149&w=1065&h=304&r=0"/>

Figure 3: Visual example of full and half interactions.

<!-- Media -->

### 3.1 Problem Formulation

We define the physical interaction as the contact event between a robotic gripper and an object, identified by changes in the gripper's openness. A single change in the gripper (from open to closed or vice versa) is a cycle. A full interaction,denoted as ${p}^{\text{full }}$ ,consists of two cycles: open $\rightarrow$ closed $\rightarrow$ open. A half interaction, ${p}^{\text{half }}$ ,represents a single change (one cycle) from open to closed or vice versa: ${p}^{\text{full }} = {p}_{o \rightarrow  c}^{\text{half }} + {p}_{c \rightarrow  o}^{\text{half }}$ ,where ${p}_{o \rightarrow  c}^{\text{half }}$ and ${p}_{c \rightarrow  o}^{\text{half }}$ represent the two sub-phases of the gripper transition: from open to closed, and from closed to open, respectively. A visual illustration of the definitions and relationship between full and half interactions is shown in Figure 3. Further differences are discussed in Appendix A. We assume access to a training task set ${\mathcal{T}}^{o} = \left\{  {{T}_{1}^{o},{T}_{2}^{o},\ldots ,{T}_{n}^{o}}\right\}$ , each paired with a natural language instruction ${\ell }_{i}^{o}$ . However,the physical interaction phases within each ${T}_{i}^{o}$ are often inconsistent. By decomposing tasks using predefined interaction boundaries, we construct an atomic task set ${\mathcal{T}}^{a} = \left\{  {{T}_{1}^{a},{T}_{2}^{a},\ldots ,{T}_{m}^{a}}\right\}$ and a corresponding instruction library ${\mathcal{L}}^{a} = \left\{  {{\ell }_{1}^{a},{\ell }_{2}^{a},\ldots ,{\ell }_{m}^{a}}\right\}$ . Each atomic task contains a consistent interaction cycle,either ${p}^{\text{full }}$ or ${p}^{\text{half }}$ . We frame 3D manipulation as keypose prediction [41,29,2,1,13]. A language-conditioned multi-task IL model $\mathcal{M}$ takes as input the observation ${o}_{t}$ (RGB-D) and instruction $\ell$ ,and predicts a 7- DoF action ${a}_{t}$ : a 6-DoF end-effector pose and a 1-DoF gripper state $\left\lbrack  {1,2}\right\rbrack$ . Trained on ${\mathcal{T}}^{a}$ ,the model $\mathcal{M}$ learns a policy $\pi$ to solve any ${T}_{i}^{a} \in  {\mathcal{T}}^{a}$ . Our objective is for the enhanced model $\mathcal{M} +$ DeCo to generalize zero-shot to a novel yet compositional long-horizon task ${T}^{\text{new }}$ ,decomposable into atomic steps: ${T}^{\text{new }} = {T}_{x}^{a} + {T}_{y}^{a} + \cdots  + {T}_{z}^{a}$ . At inference time, $\mathcal{M} + \mathrm{{DeCo}}$ retrieves,schedules, and executes the relevant atomic skills based on the retrieved instructions $\left\{  {{\ell }_{x}^{a},{\ell }_{y}^{a},\ldots ,{\ell }_{z}^{a}}\right\}$ ,enabling zero-shot generalization to new compositions.

<!-- Meanless: 4-->




### 3.2 Physical Interaction-based Task Decomposition and Skill Learning

To construct modular and reusable atomic skills, DeCo proposes a novel task decomposition strategy inspired by human hand-object interactions and prior work $\left\lbrack  {{13},{17}}\right\rbrack$ . This decomposition is based on the physical interactions of the robotic gripper, as described in Sec. 3.1. For the original demonstrations ${\mathcal{T}}^{o}$ used to train the multi-task IL model $\mathcal{M}$ ,DeCo decomposes tasks based on full physical interactions ${p}^{\text{full }}$ . For instance,a demonstration for the instruction "put item in a closed drawer without closing the drawer" can be divided into two atomic tasks: "open drawer" and "place item into open drawer", each aligned with a full gripper interaction. After decomposition, DeCo reformats the atomic demonstrations for skill learning. Each atomic task ${T}_{i}^{a}$ is paired with a language instruction ${\ell }_{i}^{a}$ ,forming the instruction library ${\mathcal{L}}^{a}$ . Demonstrations are processed using a keyframe discovery method [41] that identifies keyframes based on gripper state transitions or near-zero joint velocities. Each demonstration concludes with a full physical interaction ${p}^{\text{full }}$ ,and the end-effector pose in the final keyframe is marked as the goal pose. Optionally, demonstrations may include temporal data (e.g.,time steps) to support task progression modeling. Finally, $\mathcal{M} + \mathrm{{DeCo}}$ is trained with these physically consistent atomic datasets, enabling it to effectively acquire multiple atomic skills. The objective is to learn a language-conditioned policy ${\pi }_{\theta }^{a}$ that maps observation-instruction pairs to actions: ${\pi }_{\theta }^{a} = \arg \mathop{\min }\limits_{\theta }{\mathbb{E}}_{i,\left( {o,a}\right) }\left\lbrack  {{\mathcal{L}}_{\text{MT-IL }}\left( {{\pi }_{\theta }^{a}\left( {o,{\ell }_{i}^{a}}\right) ,a}\right) }\right\rbrack$ ,where ${\pi }_{\theta }^{a}\left( {o,{\ell }_{i}^{a}}\right)  = {\mathcal{M}}_{\theta }\left( {o,{\ell }_{i}^{a}}\right)$ . Unless otherwise stated, all atomic training datasets and experimental results of $\mathcal{M} + \mathrm{{DeCo}}$ presented in the main paper are based on ${p}^{\text{full }}$ . To explore the suitable granularity of physical interaction, we also implement a DeCo variant based on ${p}^{\text{half }}$ . Ablation study results are discussed in Sec. 5.3, with additional comparison experiments detailed in subsection C.2.

### 3.3 VLM-Guided Planning and Skill Scheduling

After a multi-task IL model $\mathcal{M}$ has mastered the skills of multiple atomic tasks,with the support of DeCo, $\mathcal{M}$ can further enhance its ability to tackle novel long-horizon tasks. When faced with a novel yet compositional task ${T}^{\text{new }},\mathcal{M} + \mathrm{{DeCo}}$ first utilizes the state-of-the-art vision-language model (VLM) GPT-4o [42], inputting the task's language instructions, observed RGB images, and the pre-obtained atomic instruction library. Through the powerful reasoning and planning capabilities of the VLM, it retrieves relevant instructions from the atomic instruction library and forms a planning sequence for the current new task, outlining each atomic task required to achieve the task's objectives. Subsequently, $\mathcal{M} + \mathrm{{DeCo}}$ initiates the execution of the first atomic skill based on the first atomic instruction in the planning sequence. During execution, the system continuously monitors the robot's pose to determine if it matches the goal pose specified in the corresponding skill. This real-time feedback loop is crucial for ensuring the correct and efficient execution of each skill. If the current pose matches the goal pose, it indicates that the atomic skill has been successfully completed, meaning the gripper has completed a full physical interaction cycle (from open to close and back to open), allowing the system to proceed to the next atomic skill's instruction. Conversely, if the pose does not match, the system continues executing the current skill until the desired pose is achieved. Details of this process, including the prompts input to the VLM, can be found in Appendix B.

### 3.4 Spatially-Aware Skill Chaining for Long-Horizon 3D Manipulation

Although $\mathcal{M} + \mathrm{{DeCo}}$ is capable of semantically combining atomic skills to accomplish long-horizon tasks via VLM-guided planning and skill scheduling (see Sec. 3.3), challenges remain in executing these skills sequentially. A primary issue lies in achieving smooth transitions between atomic skills in 3D space. Each atomic skill acquired by $\mathcal{M}$ is associated with distinct start and goal poses,often resulting in considerable spatial discontinuities between successive skills. Traditional motion planners, lacking spatial awareness, may trigger collisions during these transitions, ultimately causing task failure. To address this limitation, DeCo introduces a spatially-aware skill chaining module that enables seamless transitions without modifying the pose distributions. Specifically, once the current skill completes-i.e., the robot's pose matches the goal pose-the system schedules the next atomic instruction and predicts its start pose. The goal pose of the current skill, the predicted start pose of the next skill, and the scene point cloud are then passed to a spatially-aware cost map generation module, adapted from the foundation model Voxposer [12]. This module produces a set of collision-free chaining poses that bridge the gap between the current skill's goal pose and the next skill's start pose, as show in the third part of Figure 2. The robot then performs RRT-based [43] motion planning over these chaining poses to ensure safe transitions. This skill chaining module operates during the handoff between atomic skills in $\mathcal{M} + \mathrm{{DeCo}}$ ,enabling smooth composition of sequential skills and reliable execution of long-horizon 3D manipulation tasks.

<!-- Meanless: 5-->




## 4 DeCoBench : Benchmarking Multi-task IL Generalization on Compositional Long-Horizon 3D Manipulation Tasks

To better evaluate DeCo's performance in simulation, we introduce DeCoBench-a benchmark built on the physical interaction-based task decomposition described in Sec. 3.2 (overview in Figure 7, details in Appendix D). It includes 22 tabletop manipulation tasks: 10 atomic tasks (with 24 variations) for training, and 12 compositional long-horizon tasks (with 36 variations) for zero-shot evaluation. DeCoBench is designed to assess the zero-shot generalization of multi-task IL models on novel yet compositional 3D manipulation tasks. The benchmark spans three domains: Object Rearrangement with Drawer, Object Rearrangement with Cupboard, and Rubbish Cleanup. In the Drawer domain, 2 original IL tasks are decomposed into 5 atomic skills based on full physical interaction, yielding tasks with 4, 6, and 10 cycles. The Cupboard domain includes 3 atomic tasks and a 4-cycle long-horizon task, while the Cleanup domain includes 2 atomic tasks forming another 4-cycle task. DeCoBench also includes two cross-domain tasks designed to evaluate models' cross-task generalization capabilities: Transfer Box (Drawer + Cupboard) and Retrieve and Sweep Rubbish (Cupboard + Cleanup). See Appendix D for further details about the DeCoBench tasks.

## 5 Experiments

We study DeCo in both simulated and real-world environments. Specifically, we aims to answer the following research questions: 1) How well does DeCo enhance the generalization of multitask IL models on long-horizon 3D manipulation tasks (Sec. 5.2)? 2) How do heuristic settings in DeCo influence its generalization performance (Sec. 5.3)? 3) How well does DeCo enhance the generalization of multi-task IL model on real-world long-horizon manipulation tasks (Sec. 5.4)?

### 5.1 Experimental Setup

Baseline Multi-task IL Models. We apply DeCo to three representative multi-task IL models—RVT-2 [3], 3DDA [19], and ARP [5]—to validate its model-agnostic design and demonstrate its generalization benefits. RVT-2 is a multi-view robotic transformer that follows a coarse-to-fine strategy on constructed point clouds to predict the next-best action heatmap. 3DDA combines 3D scene representations with a diffusion-based policy for robotic manipulation. ARP leverages a Chunking Causal Transformer [5] to autoregressively generate heterogeneous action sequences for manipulation tasks. All three models have demonstrated competitive multi-task IL performance on the RLBench benchmark ${}^{1}$ .

Simulation Setup. We conduct simulation experiments using our proposed DeCoBench benchmark suite. Observations are collected from four RGB-D cameras positioned at the front, left shoulder, right shoulder,and wrist. RVT-2 and ARP use 128 $\times  {128}$ image inputs,while 3DDA uses ${256} \times  {256}$ , following their original settings. Each baseline and its DeCo-enhanced variant is trained on atomic tasks (50 demonstrations per task) and evaluated on compositional tasks (20 test demonstrations per task). All policies are evaluated with three random seeds, and standard deviations are reported.

---

<!-- Footnote -->

${}^{1}$ https://paperswithcode.com/sota/robot-manipulation-on-rlbench

<!-- Footnote -->

---

<!-- Meanless: 6-->


<!-- Media -->

<!-- figureText: DeCoBench for Evaluation Box in Cupboard Box in Cupboard Models Box in Cupboard Take Two Put Twe Take Two Exchange Sweep Transfe Retrieve out of Same in Diff out of Diff Boxes and Drop 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ${71.67}_{\pm {12.47}}$ $\underline{85.00}{}_{\pm {7.07}}$ 61.67 $\pm  {17.00}$ 11.67 $\pm  {6.24}$ $\underline{80.00}{}_{\pm {4.08}}$ 0.00 $\underline{10.00}{}_{\pm {4.08}}$ 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ${95.00}_{\pm {4.08}}$ 0.00 11.67 ±2.36 0.00 76.67 $\pm  {4.7}$ $\underline{63.33} \pm  {2.36}$ 0.00 0.00 1.67 $\pm  {2.36}$ Drawer Drawer Drawer Drawer <icon/> (2 cycles, for training) (4-10 cycles, for testing) Open Drawer Avg. Put in Put in Take out Take out Put Two Models w/o Close w/o Close and Close in Same RVT2 [3] 0.00 0.00 0.00 0.00 0.00 0.00 $\mathrm{{RVT}}2 + \mathrm{{DeCo}}$ 66.67 (66.67% ↑) $\underline{98.33}{}_{\pm {2.36}}$ $\underline{98.33}{}_{\pm {2.36}}$ $\underline{93.33} \pm  {6.24}$ $\underline{93.33} \pm  {6.24}$ 3DDA [19] 0.00 0.00 0.00 0.00 0.00 0.00 $3\mathrm{{DDA}} + \mathrm{{DeCo}}$ $\underline{\mathbf{{21.53}}}$ (21.53% ↑) 0.00 0.00 $\underline{83.33}{}_{\pm {9.43}}$ ${\underline{68.33}}_{\pm {4.7}}$ 0.00 $\mathrm{{ARP}} + \mathrm{{DeCo}}$ -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_6.jpg?x=311&y=205&w=1171&h=372&r=0"/>

Table 1: Generalization Performance on DeCoBench Long-horizon tasks. Above is a visualization illustrating how DeCo enables zero-shot generalization on two long-horizon tasks from DeCoBench.

<!-- Media -->

Real-robot Setup. We validate DeCo on a Franka Emika Panda robot equipped with an exocentric Intel RealSense D435i camera. We compare RVT-2 and RVT-2+DeCo on an object rearrangement task involving a drawer. Training uses 6 atomic tasks (16 variations), while evaluation covers 9 long-horizon tasks (30 variations) for zero-shot generalization. The test set includes 3 tasks with 4 cycles, 2 with 6, 2 with 12, and 2 with 16 cycles. see Appendix E for more details. Each task is executed 10 times with randomized initial object placements to compute average success rates.

### 5.2 Generalization Performance on DeCoBench

Table 1 presents the generalization performance on 12 long-horizon tasks in DeCoBench for RVT-2, 3DDA, and ARP models trained on 10 atomic tasks, along with their DeCo-enhanced counterparts.Although RVT- 2, 3DDA, and ARP perform well in atomic tasks (see subsection C. 1 for their performance on atomic tasks), they almost completely fail on 12 long-horizon tasks, indicating that base models struggle to generalize atomic skills to long-horizon scenarios. In contrast, DeCo substantially enhances their performance-yielding a 66.67% gain for RVT-2+DeCo, 21.53% for 3DDA+DeCo, and 57.92% for ARP+DeCo. These results demonstrate DeCo's model-agnostic design and its effectiveness for zero-shot generalization in multi-task IL models on novel long-horizon tasks through compositional reuse of learned atomic skills.

<!-- Media -->

<table><tr><td>Task</td><td>RVT-2+DeCo</td><td>RVT-2 (6 Long training)</td></tr><tr><td>6 Novel</td><td>83.89% (53.89% ↑)</td><td>30.00%</td></tr><tr><td>12 All</td><td>66.67% (14.31% T)</td><td>52.36%</td></tr></table>

Table 2: Impact of atomic task design.

<!-- Media -->

To further assess the impact of atomic task design, we train RVT-2 on 6 long-horizon tasks ( 6 Long) from DeCoBench, comprising 4 original IL tasks and 2 cross-domain tasks (see Appendix D for details). We then evaluate the model on the remaining 6 novel long-horizon tasks not seen during training ( 6 Novel), as well as on all 12 long-horizon tasks (12 All). As shown in Table 2, compared to RVT-2 trained directly on the 6 long-horizon tasks (RVT-2 (6 Long training)), the DeCo-based variant (RVT-2 + DeCo) achieves substantially better zero-shot generalization on the unseen 6 Novel tasks, improving the success rate by 53.89%. Even when evaluated across all 12 tasks, including those seen by RVT-2 ( 6 Long training), the DeCo-based model still yields an overall improvement of 14.31%. These results indicate that DeCo’s atomic task training set more effectively supports skill acquisition in multi-task IL models and enhances generalization to novel compositional tasks. Additional analysis on impact of atomic task design are provided in subsection C.3.

### 5.3 Ablation Studies

Figure 4 summarizes three ablation studies on DeCo's heuristic settings. Figure 4a compares the generalization performance of three models using DeCo under half and full interaction settings. The results show that DeCo improves model generalization in both settings, but different models have varying sensitivity to physical interactions. The full interaction (open $\rightarrow$ closed $\rightarrow$ open)

<!-- Meanless: 7-->




<!-- Media -->

<!-- figureText: 56.81 Half 86.80 Chaining Poses $= 0$ 3 10 train_num_10 train_num_50 Chaining Poses Num (c) Effect of Chaining Poses Num RVT-2+DeCo 66.67 Full 3DDA+DeCo 18.75 21.53 41.47 $\mathrm{{ARP}} + \mathrm{{DeCo}}$ 58.06 Atomic Tasks Av 40 Average Success Rate (%) train_num_3 train_num_5 (a) Half vs. Full Interaction (b) Effect of Training Demo Num -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_7.jpg?x=311&y=205&w=1168&h=244&r=0"/>

Figure 4: Ablation study of heuristic settings in DeCo. (b) and (c) are based on the RVT-2+DeCo model.

<!-- figureText: Multiple Atomic Skill An Execution Example of Novel Long Horizon Tasks: On and in Different N-L-H Tasks RVT2 RVT2+DeCo Put in w/o Close 0/10 7/10 Put in and Close 0/10 7/10 Take out w/o Close 0/10 6/10 Take out and Close Put 2 in Different 4/10 Take 2 out of Different 0/10 3/10 Block Exchange 0/10 9/10 On and in Different 0/10 3/10 Out of Different and off 0/10 1/10 Avg. SR on N-L-H Tasks 0% 53.33% DeCo-Enhanced Models Take out of Opened Drawer Atomic Tasks RVT2 $\mathrm{{RVT}}2 + \mathrm{{DeCo}}$ Open Drawer 8/10 8/10 Close Drawer 9/10 9/10 Put in Opened Drawer 9/10 8/10 Take out of Opened Drawer 8/10 Block on Drawer 10/10 10/10 Block off Drawer 10/10 10/10 Avg. SR on Atomic Tasks 91.67% 88.33% -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_7.jpg?x=402&y=490&w=971&h=505&r=0"/>

Table 3: Real-world results. Each entry represents the successful trials out of 10. Above is a visualization example showing how DeCo performs zero-shot generalization in one of the longest-horizon tasks (16 cycles).

<!-- Media -->

enhances DeCo's ability to provide better compositional generalization. Notably, ARP+DeCo is most affected under the half interaction, while RVT-2+DeCo and 3DDA+DeCo show only minor changes. For detailed difference of DeCo based on ${p}^{\text{half }}$ and ${p}^{\text{full }}$ ,and further analyses of the experimental results, please refer to Appendix A and subsection C.2, respectively.

We also vary the number of atomic task demonstrations for RVT-2+DeCo and report the average success rates over 10 atomic tasks and 12 long-horizon tasks. As shown in Figure 4b, more demonstrations indeed help improve the performance of IL model. As long as the RVT-2 model learns atomic skills to a certain extent, DeCo effectively composes these skills to achieve a degree of zero-shot generalization on long-horizon tasks. Moreover, the better RVT-2 performs on atomic tasks, the better generalization performance DeCo achieves. Figure 4c shows that disabling the spatially-aware skill chaining module (Chaining Poses Num = 0 ) leads to a significant drop in performance. We visualize a failure case of Put in and Close task, where the robot collides with the drawer after opening it due to a poor transition plan, preventing it from successfully picking up the item. In contrast, regardless of the number of poses, enabling our spatially-aware skill chaining module consistently enhances task success significantly. In DeCo, we heuristically set the chaining poses num to 6 .

### 5.4 Real-robot Evaluations

We conduct extensive experiments on a real-world robotic platform to further validate the practical effectiveness of the DeCo framework. As shown in Table 3 compares the success rates of RVT- 2 and RVT-2+DeCo, both trained on 6 atomic tasks, when tested on 9 novel long-horizon (N-L-H) tasks. These tasks are unseen during training but are composable via atomic skills. RVT-2 performs well on atomic tasks but fails to generalize to N-L-H tasks,achieving 0% success. In contrast, RVT-2+DeCo, though slightly less stable on atomic tasks due to vision-language-based decomposition,achieves strong zero-shot generalization,with a 53.33% average success rate on N-L-H tasks. These results confirm DeCo's practicality and generalization ability in real-world robotic settings. Experiment videos are available on the project website.

<!-- Meanless: 8-->




## 6 Conclusion

In this work, we introduce DeCo, a model-agnostic framework that enables multi-task IL models to generalize zero-shot to novel yet compositional long-horizon 3D manipulation tasks. DeCo decomposes IL demonstrations into modular, reusable atomic tasks via physical interaction analysis. At test time, it leverages VLMs to interpret high-level instructions and retrieve relevant skills, enabling flexible planning and scheduling for novel, compositional tasks. A spatially-aware chaining module ensures collision-free skill transitions, addressing temporal and spatial discontinuities in long-horizon execution. Extensive evaluations in both simulation and real-world settings demonstrate that DeCo significantly enhances the generalization of three representative multi-task IL models.

## 7 Limitations and Discussion

### 7.1 Limitations

<!-- Media -->

<!-- figureText: Sweep and Drop Retrieve and Sweep (broom out of cupboard + sweep rubbish ) Failed to grab the broom! Drop the broom! (sweep rubbish + drop rubbish) Failed to grab the broom! 3DDA+DeCo Broom falls due to collision! ARP+DeCo -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_8.jpg?x=313&y=769&w=1178&h=488&r=0"/>

Figure 5: Two visual failure cases of 3DDA+DeCo and ARP+DeCo.

<!-- Media -->

While DeCo demonstrates strong zero-shot generalization for novel yet compositional long-horizon 3D manipulation tasks across three representative multi-task imitation learning (IL) models (RVT- 2 [3], 3DDA [19] and ARP [5]), some limitations remain:

- Dependency on Base Model: The ability of the base multi-task IL model to learn atomic skills is crucial for DeCo's generalization performance in long-horizon tasks (see Figure 4b. However, the model's generalization capability relies not only on its learning of atomic skills but also on other factors. Visual robustness is a key factor: different models have varying levels of visual robustness when confronted with previously unseen combinations in scenarios. If the base multi-task IL model's visual capability cannot effectively handle these variations, it will directly impact DeCo's ability to generalize in multi-task IL models when handling combined long-horizon tasks. We provide visual failure cases of 3DDA+DeCo and ARP+DeCo in Figure 5 to further illustrate this limitation. Although 3DDA+DeCo and ARP+DeCo excel in learning atomic tasks (see Table 5), they encounter failures when facing the compositional long-horizon tasks Sweep and Drop (sweep rubbish

+ drop rubbish) and Retrieve and Sweep (broom out of cupboard + sweep rubbish). Even though DeCo can plan and schedule the corresponding atomic skills, both 3DDA and ARP struggle with visual processing in unseen combination scenarios. As a result, 3DDA+DeCo and ARP+DeCo fail to execute the atomic skills, which prevents them from completing the entire long-horizon tasks. A future direction to address this issue could involve adding a visual enhancement module $\left\lbrack  {{44},{45}}\right\rbrack$ to the base multi-task IL model in DeCo to improve its visual robustness in unseen scenarios.

<!-- Meanless: 9-->




- Dependency of Task Planning on VLM Capabilities: The effectiveness of DeCo in task planning is closely tied to the capabilities of visual language models (VLMs). Therefore, if the VLM falls short in accurately understanding the spatial relationships and contextual combinations of task language instructions in scenarios, DeCo's planning performance may be negatively impacted (see subsection C.1). This dependency could hinder DeCo's ability to generalize effectively in diverse and dynamic environments, especially when confronted with novel long-horizon tasks that require complex and advanced integration of visual information and instructions across multiple task domains. To address this limitation, a Human-in-the-Loop feedback mechanism [46] can be implemented, enabling human operators to provide corrections and insights during task execution. This interaction can refine the VLM's understanding and enhance DeCo's overall performance in complex scenarios. Additionally, DeCo currently relies on GPT-4o as its VLM; exploring alternative models such as Gemini [47], DeepSeek-VL [48], or LLaMA [49] in the future may provide deeper insights into how VLM model selection affects DeCo's performance and generalization

- Atomic Task Horizon: The atomic tasks in DeCo are currently constrained to short horizons-either 1 cycle (representing half interaction) or 2 cycles (representing a full interaction). It remains an open question how DeCo would perform with longer-horizon atomic tasks,such as those with intermediate durations $\left( {1 < \text{cycles} < 2}\right)$ or extended sequences (cycles $> 2$ ),and whether such variations would facilitate or hinder generalization when composing more complex tasks from these primitives.

- Definition of Atomic Tasks and Detection of Skill Completion: The current definition of atomic tasks in DeCo is derived from the gripper's open/close state. This simplistic criterion may be insufficient for detecting meaningful task boundaries in more complex scenarios, such as multi-stage tool manipulation, deformable object handling, or non-prehensile manipulation. Furthermore, reliable detection of atomic skill completion under such conditions remains an unresolved challenge, particularly when transitioning to more versatile or dexterous robot hardware. To address this limitation, incorporating tactile or force modalities $\left\lbrack  {{50},{51}}\right\rbrack$ in the future can provide real-time information about the state of objects and the environment, helping DeCo to flexibly divide atomic tasks and more accurately assess task completion.

### 7.2 Future Work

We plan to extend DeCo along several key directions. First, we will explore the flexibility of its physically grounded task decomposition by incorporating tactile and force-based modalities, enabling richer manipulation capabilities such as dexterous control, deformable object handling, and multi-step tool use. Second, we aim to integrate DeCo with advanced vision-language-action (VLA) models $\left\lbrack  {{52},{53},{54}}\right\rbrack$ and deploy it across a broader range of robotic platforms,including mobile manipulators and dexterous hands. A key challenge lies in ensuring robust generalization across complex tasks, which requires improving atomic skill completion detection, enhancing transition reliability, and addressing the scalability limits of a growing skill library. While expanding the skill set increases expressiveness and task coverage, it also raises demands on data, training, and inference. Balancing these trade-offs will be critical to realizing DeCo's full potential in real-world, long-horizon manipulation. References

[1] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic manipulation. In Conference on Robot Learning, pages 785-799. PMLR, 2023.

[2] A. Goyal, J. Xu, Y. Guo, V. Blukis, Y.-W. Chao, and D. Fox. Rvt: Robotic view transformer for 3d object manipulation. In Conference on Robot Learning, pages 694-710. PMLR, 2023.

[3] A. Goyal, V. Blukis, J. Xu, Y. Guo, Y.-W. Chao, and D. Fox. Rvt2: Learning precise manipulation from few demonstrations. RSS, 2024.

<!-- Meanless: 10-->




[4] J. Jiang, X. Wu, Y. He, L. Zeng, Y. Wei, D. Zhang, and W. Zheng. Rethinking bimanual robotic manipulation: Learning with decoupled interaction framework, 2025.

[5] X. Zhang, Y. Liu, H. Chang, L. Schramm, and A. Boularias. Autoregressive action sequence learning for robotic manipulation. IEEE Robotics and Automation Letters, 2025.

[6] R. Garcia, S. Chen, and C. Schmid. Towards generalizable vision-language robotic manipulation: A benchmark and llm-guided 3d policy. arXiv preprint arXiv:2410.01345, 2024.

[7] V. Myers, C. Zheng, O. Mees, K. Fang, and S. Levine. Policy adaptation via language optimization: Decomposing tasks for few-shot imitation. In 8th Annual Conference on Robot Learning, 2024.

[8] A. Curtis, N. Kumar, J. Cao, T. Lozano-Pérez, and L. P. Kaelbling. Trust the proc3s: Solving long-horizon robotics problems with llms and constraint satisfaction. In 8th Annual Conference on Robot Learning, 2024.

[9] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 9493-9500. IEEE, 2023.

[10] Z. Chen, J. Huo, Y. Chen, and Y. Gao. Robohorizon: An llm-assisted multi-view world model for long-horizon robotic manipulation. arXiv preprint arXiv:2501.06605, 2025.

[11] W. Huang, C. Wang, Y. Li, R. Zhang, and L. Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024.

[12] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023.

[13] Y. Chen, Z. Chen, J. Yin, J. Huo, P. Tian, J. Shi, and Y. Gao. Gravmad: Grounded spatial value maps guided action diffusion for generalized $3\mathrm{\;d}$ manipulation. arXiv preprint arXiv:2409.20154, 2024.

[14] Z. Chen, Z. Ji, J. Huo, and Y. Gao. Scar: Refining skill chaining for long-horizon robotic manipulation via dual regularization. Advances in Neural Information Processing Systems, 37: 111679-111714, 2024.

[15] Y. Chen, C. Wang, L. Fei-Fei, and C. K. Liu. Sequential dexterity: Chaining dexterous policies for long-horizon manipulation. arXiv preprint arXiv:2309.00987, 2023.

[16] G. Tziafas and H. Kasaei. Lifelong robot library learning: Bootstrapping composable and generalizable skills for embodied control with language models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 515-522. IEEE, 2024.

[17] N. Saito, J. Moura, T. Ogata, M. Y. Aoyama, S. Murata, S. Sugano, and S. Vijayakumar. Structured motion generation with predictive learning: Proposing subgoal for long-horizon manipulation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 9566-9572. IEEE, 2023.

[18] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019-3026, 2020.

[19] T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d scene representations. arXiv preprint arXiv:2402.10885, 2024.

[20] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. De-hghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

<!-- Meanless: 11-->




[21] J. Liang, B. Wen, K. E. Bekris, and A. Boularias. Learning sensorimotor primitives of sequential manipulation tasks from visual demonstrations. In Proceedings of the 2022 International Conference on Robotics and Automation (ICRA), 2022.

[22] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023.

[23] C. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, page 02783649241273668, 2023.

[24] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin, D. Duong, V. Sindhwani, et al. Transporter networks: Rearranging the visual world for robotic manipulation. In Conference on Robot Learning, pages 726-747. PMLR, 2021.

[25] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-man, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.

[26] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipulation. In Conference on robot learning, pages 894-906. PMLR, 2022.

[27] Z. Chen, Z. Ji, S. Liu, J. Huo, Y. Chen, and Y. Gao. Casil: Cognizing and imitating skills via a dual cognition-action architecture. arXiv preprint arXiv:2309.16299, 2023.

[28] Z. Chen, W. Li, Y. Gao, and Y. Chen. Tild: Third-person imitation learning by estimating domain cognitive differences of visual demonstrations. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, pages 2421-2423, 2023.

[29] S. James, K. Wada, T. Laidlow, and A. J. Davison. Coarse-to-fine q-attention: Efficient learning for visual robotic manipulation via discretisation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13739-13748, 2022.

[30] P.-L. Guhur, S. Chen, R. G. Pinel, M. Tapaswi, I. Laptev, and C. Schmid. Instruction-driven history-aware policies for robotic manipulations. In Conference on Robot Learning, pages 175-187. PMLR, 2023.

[31] T. Gervet, Z. Xian, N. Gkanatsios, and K. Fragkiadaki. Act3d: Infinite resolution action detection transformer for robotic manipulation. arXiv preprint arXiv:2306.17817, 2023.

[32] Z. Xian, N. Gkanatsios, T. Gervet, T.-W. Ke, and K. Fragkiadaki. Chaineddiffuser: Unifying trajectory diffusion and keypose prediction for robotic manipulation. In 7th Annual Conference on Robot Learning, 2023.

[33] T. Gao, S. Nasiriany, H. Liu, Q. Yang, and Y. Zhu. Prime: Scaffolding manipulation tasks with behavior primitives for data-efficient imitation learning. IEEE Robotics and Automation Letters, 2024.

[34] U. A. Mishra, S. Xue, Y. Chen, and D. Xu. Generative skill chaining: Long-horizon skill planning with diffusion models. In Conference on Robot Learning, pages 2905-2925. PMLR, 2023.

[35] C. Agia, T. Migimatsu, J. Wu, and J. Bohg. Stap: Sequencing task-agnostic policies. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 7951-7958. IEEE, 2023.

[36] Y. Hou, J. Ma, H. Sun, and F. Wu. Effective offline robot learning with structured task graph. IEEE Robotics and Automation Letters, 9(4):3633-3640, 2024.

<!-- Meanless: 12-->




[37] K. Zentner, R. Julian, B. Ichter, and G. S. Sukhatme. Conditionally combining robot skills using large language models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 14046-14053. IEEE, 2024.

[38] Z. Zhang, Y. Li, O. Bastani, A. Gupta, D. Jayaraman, Y. J. Ma, and L. Weihs. Universal visual decomposer: Long-horizon manipulation made easy. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 6973-6980. IEEE, 2024.

[39] C. Zhao, S. Yuan, C. Jiang, J. Cai, H. Yu, M. Y. Wang, and Q. Chen. Erra: An embodied representation and reasoning architecture for long-horizon language-conditioned manipulation tasks. IEEE Robotics and Automation Letters, 8(6):3230-3237, 2023.

[40] S. Cheng and D. Xu. League: Guided skill learning and abstraction for long-horizon manipulation. IEEE Robotics and Automation Letters, 8(10):6451-6458, 2023.

[41] S. James and A. J. Davison. Q-attention: Enabling efficient learning for vision-based robotic manipulation. IEEE Robotics and Automation Letters, 7(2):1612-1619, 2022.

[42] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.

[43] S. Karaman, M. R. Walter, A. Perez, E. Frazzoli, and S. Teller. Anytime motion planning using the rrt. In 2011 IEEE international conference on robotics and automation, pages 1478-1483. IEEE, 2011.

[44] C. Yuan, S. Joshi, S. Zhu, H. Su, H. Zhao, and Y. Gao. Roboengine: Plug-and-play robot data augmentation with semantic robot segmentation and background generation. arXiv preprint arXiv:2503.18738, 2025.

[45] B. Wen, M. Trepte, J. Aribido, J. Kautz, O. Gallo, and S. Birchfield. Foundationstereo: Zero-shot stereo matching. arXiv preprint arXiv:2501.09898, 2025.

[46] Y. Dai, J. Lee, N. Fazeli, and J. Chai. Racer: Rich language-guided failure recovery policies for imitation learning. arXiv preprint arXiv:2409.14674, 2024.

[47] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.

[48] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, H. Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024.

[49] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[50] H. Xue, J. Ren, W. Chen, G. Zhang, Y. Fang, G. Gu, H. Xu, and C. Lu. Reactive diffusion policy: Slow-fast visual-tactile policy learning for contact-rich manipulation. arXiv preprint arXiv:2503.02881, 2025.

[51] A. Agarwal, A. Ajith, C. Wen, V. Stryzheus, B. Miller, M. Chen, M. K. Johnson, J. L. S. Rincon, J. Rosca, and W. Yuan. Robotic defect inspection with visual and tactile perception for large-scale components. In 2023 IEEE/RSJ Ineternational Conference on Intelligent Robots and Systems (IROS), pages 10110-10116. IEEE, 2023.

[52] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. P. Foster, P. R. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang, and C. Finn. Openvla: An open-source vision-language-action model. In Conference on Robot Learning, volume 270, pages 2679-2713. PMLR, 2024.

<!-- Meanless: 13-->




[53] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Haus-man, B. Ichter, S. Jakubczak, T. Jones, L. Ke, S. Levine, A. Li-Bell, M. Mothukuri, S. Nair, K. Pertsch, L. X. Shi, J. Tanner, Q. Vuong, A. Walling, H. Wang, and U. Zhilinsky. ${\pi 0}$ : A vision-language-action flow model for general robot control. ArXiv, abs/2410.24164, 2024.

[54] S. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su, and J. Zhu. Rdt-1b: a diffusion foundation model for bimanual manipulation. ArXiv, abs/2410.07864, 2024.

<!-- Meanless: 14-->




<!-- Meanless: A Differences in the DeCo Framework under Half and Full Interactions-->

<!-- Media -->

<!-- figureText: Instr Pointer ++ (half) Put the yellow block into the bottom drawer (w/o close) Put yellow block in opened bottom drawer (full) Pick yellow block (half 1) Place it in bottom drawer (half 2) Update Temporal data as the task progresses Instr Pointer ++ (full) Reset Temporal Data Open bottom drawer (full) Grasp bottom handle (half 1) Pull it out (half 2) -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_14.jpg?x=315&y=339&w=1171&h=391&r=0"/>

Figure 6: Visualizing Differences in the DeCo Framework under Half and Full Interactions

<!-- Media -->

To further compare the impact of the definitions of half and full interactions from Section 3.1 on the performance of $\mathrm{{DeCo}}$ ,we detail the differences between half and full interactions in the four key stages of DeCo: task decomposition, instruction alignment, temporal modeling, and skill chaining. Figure Figure 6 visualizes these differences using the real-robot task "put the yellow block into the bottom drawer."

In the task decomposition phase, the half interaction treats each change in gripper state (e.g., open or close) as an atomic unit, leading to finer-grained subgoals and 2-cycle tasks. The full interaction mode combines an open-close pair into a single, semantically complete unit, resulting in coarser 1-cycle skills.

For instruction alignment, the instruction pointer in the half interaction setting advances after each gripper action, allowing for fine-grained alignment with low-level steps. In the full interaction setting, it updates only after completing a full 2-cycle interaction, aligning more naturally with the structure of natural language instructions.

For temporally conditioned models like RVT-2, the two modes adopt distinct temporal data scheduling strategies. In the half interaction case, temporal data are reset after each gripper event, resulting in a finer decomposition of skill phases. In the full interaction setting, the reset occurs only after an entire atomic skill (i.e., 2 cycles) is executed, ensuring that temporal dynamics align with higher-level semantic units and reducing unnecessary fluctuations.

For skill chaining, both modes adopt the same triggering condition: a new skill is activated only after completing a full open-close cycle, ensuring consistent and coherent transitions between atomic skills.

These design choices significantly influence how atomic skills are decomposed, supervised, and executed, directly affecting DeCo's temporal and semantic alignment capabilities. Ultimately, the granularity of interaction shapes both the structure of training data and the behavior of the model during inference, impacting its ability to generalize and successfully perform novel long-horizon tasks.

## B VLM Prompts in DeCo

The DeCo framework provides two prompt templates for VLM-guided planning: Full Interaction and Half Interaction. Table 4 shows the details of both prompt templates. More detailed prompts can be found at https://deco226.github.io.

<!-- Meanless: 15-->




Prompt Engineering Template: Full Interaction vs Half Interaction

<!-- Media -->

<!-- figureText: Full Half Background and Role Definition Same as Full. Scene Description and Constraints Same as Full. Robotic Arm Task Goal Same as Full. User Goal Description Same as Full. Task Decomposition Requirements 1. Break down instructions. 2. Align with environment. 3. <decomposition rule of half: Typically emphasizes outputting an even number of steps.> Domain Skills List <list-half> Skill Usage Rules <usage of skill in half> Input Format Same as Full. Output Format and Rules - High-Level Plan + Atomic-Level Commands Rules: Logical, efficient, minimal. The High-Level Plan should provide a clear overview, while the Atomic-Level Commands should detail executable actions.<example> User Input 1. High-Level Instruction <instruction> 2. Environment Image <details: Scene-related visual descriptions.>, <image> Background and Role Definition You are a robotic task planner. Translate high-level instructions and environment states into steps based on predefined skills. Scene Description and Constraints <scene: Description of the current environment, interactive objects, and their initial states.> Robotic Arm Task Goal The robotic arm interprets instructions and interacts with the environment to efficiently complete tasks. <goal: Summary of the robotic arm's main task objective.> User Goal Description Generate optimized step-by-step commands using predefined skills, eliminating redundancies. Task Decomposition Requirements 1. Break down instructions. 2. Align with environment. 3. <decomposition rule of full> Domain Skills List <list-full: Enumerate skills clearly.> Skill Usage Rules <usage of skill in full: Usage defines how skills should be correctly applied during planning.> Input Format 1. High-Level Semantic Instruction 2. Environment Image Output Format and Rules - Only Atomic-Level Commands (lowercase, numbered) Rules: Logical, efficient, minimal.<example> $\mathbf{{UserInput}}$ 1. High-Level Instruction <instruction> 2. Environment Image <details>, <image> -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_15.jpg?x=336&y=278&w=1141&h=1609&r=0"/>

Table 4: Prompt Templates for Full and Half Interaction Versions in the DeCo Framework for VLM-Guided Planning.

<!-- Meanless: 16-->


<table><tr><td/><td>Avg.</td><td>Open</td><td>Close</td><td>Put in Opened</td><td>Take Out of</td><td>Box Out of</td><td>Box in</td><td>Box Out</td><td>Broom Out</td><td>Sweep to</td><td>Rubbish in</td></tr><tr><td>Models</td><td>Success $\uparrow$</td><td>Drawer</td><td>Drawer</td><td>Drawer</td><td>Opened Drawer</td><td>Opened Drawer</td><td>Cupboard</td><td>Cupboard</td><td>Cupboard</td><td>Dustpan</td><td>Dustpan</td></tr><tr><td>RVT-2 [3]</td><td>91.83</td><td>${98.33} \pm  {2.36}$</td><td>${96.67} \pm  {2.36}$</td><td>${100.00} \pm  {0.00}$</td><td>${100.00} \pm  {0.00}$</td><td>${100.00} \pm  {0.00}$</td><td>${35.00}_{\pm {4.08}}$</td><td>${98.33} \pm  {2.36}$</td><td>98.33 $\pm  {2.36}$</td><td>${91.67} \pm  {6.24}$</td><td>${100.00} \pm  {0.00}$</td></tr><tr><td>RVT-2+DeCo</td><td>86.80</td><td>${98.33} \pm  {2.36}$</td><td>${100.00} \pm  {0.00}$</td><td>${88.33} \pm  {6.24}$</td><td>${100.00} \pm  {0.00}$</td><td>${100.00} \pm  {0.00}$</td><td>${48.33} \pm  {2.36}$</td><td>${85.00}_{\pm {7.07}}$</td><td>${65.00}_{\pm {0.00}}$</td><td>${83.00}_{\pm {6.24}}$</td><td>${100.00} \pm  {0.00}$</td></tr><tr><td>3DDA [19]</td><td>98.00</td><td>${98.33} \pm  {2.36}$</td><td>${100.00} \pm  {0.00}$</td><td>${100.00} \pm  {0.00}$</td><td>${100.00} \pm  {0.00}$</td><td>${98.33} \pm  {2.36}$</td><td>${91.67} \pm  {4.71}$</td><td>98.33 $\pm  {2.36}$</td><td>${96.67} \pm  {2.36}$</td><td>98.33 $\pm  {2.36}$</td><td>${100.00} \pm  {0.00}$</td></tr><tr><td>3DDA+DeCo</td><td>96.00</td><td>${95.00}_{\pm {0.00}}$</td><td>${100.00} \pm  {0.00}$</td><td>${100.00} \pm  {0.00}$</td><td>100.00</td><td>${100.00} \pm  {0.00}$</td><td>${88.33} \pm  {2.36}$</td><td>${100.00}_{\pm {0.00}}$</td><td>${98.33} \pm  {2.36}$</td><td>${78.33} \pm  {2.36}$</td><td>100.00$\pm  {0.00}$</td></tr><tr><td>ARP [5]</td><td>94.67</td><td>${100.00} \pm  {0.00}$</td><td>${95.00}_{\pm {0.00}}$</td><td>${100.00} \pm  {0.00}$</td><td>100.00</td><td>${100.00} \pm  {0.00}$</td><td>${65.00}_{\pm {7.07}}$</td><td>${95.00}_{\pm {0.00}}$</td><td>${100.00} \pm  {0.00}$</td><td>93.33 ±2.36</td><td>98.33$\pm  {2.36}$</td></tr><tr><td>$\mathrm{{ARP}} + \mathrm{{DeCo}}$</td><td>91.67</td><td>${100.00} \pm  {0.00}$</td><td>${100.00} \pm  {0.00}$</td><td>${100.00} \pm  {0.00}$</td><td>100.00</td><td>${100.00} \pm  {0.00}$</td><td>${30.00} \pm  {7.07}$</td><td>${95.00}_{\pm {0.00}}$</td><td>${96.67} \pm  {2.36}$</td><td>96.67 $\pm  {2.36}$</td><td>98.33$\pm  {2.36}$</td></tr></table>

Table 5: Test Performance on 10 atomic tasks in DeCoBench. Evaluations on 10 atomic tasks are conducted using 3 seeds, with 20 test episodes per task, utilizing the final checkpoints from training on 10 atomic tasks. Performance is reported for different model variants across specific task categories.

<table><tr><td rowspan="2"/><td colspan="2">train_set_50</td><td colspan="2">train_set_10</td><td colspan="2">train_set_5</td><td colspan="2">train_set_3</td></tr><tr><td>half</td><td>full</td><td>half</td><td>full</td><td>half</td><td>full</td><td>half</td><td>full</td></tr><tr><td>Average (Atomic Tasks)</td><td>81.24</td><td>86.50</td><td>77.50</td><td>81.27</td><td>70</td><td>75.28</td><td>68.50</td><td>73.48</td></tr><tr><td>Average (Compositional Tasks)</td><td>56.81</td><td>66.70</td><td>53.58</td><td>62.06</td><td>41.47</td><td>32.50</td><td>23.75</td><td>32.96</td></tr></table>

Table 6: Average success rates for RVT-2+DeCo, defined through half and full interactions, after training with varying numbers of atomic task demos, on 10 atomic tasks and 12 combinatorial long-sequence tasks in DeCoBench.

<!-- Media -->

## C More Experimental Results

### C.1 Test Performance on Atomic Tasks

Table 5 presents the performance of various models on 10 atomic tasks in the DeCoBench benchmark. Among them, the 3DDA model achieves the highest overall success rate, while ARP and RVT-2 also demonstrate stable and competitive performance. However, integrating DeCo into these models results in a noticeable drop in success rates for certain tasks. This degradation may stem from the instability of the vision-language model (VLM) used by DeCo during atomic task planning, which affects the instruction-following accuracy of multi-task imitation learning models. In contrast, the three baseline models are evaluated using ground-truth task instructions-those they have already encountered during training-which enables them to achieve strong test performance. These results further support the argument in Sec. 7: although 3DDA and ARP perform well in acquiring atomic skills, their limited robustness in visual processing constrains their ability to handle complex, long-horizon tasks that require compositional visual and semantic reasoning. While DeCo significantly improves the generalization ability of multi-task IL models on novel compositional long-horizon tasks (as shown in Table 1), the performance of 3DDA+DeCo and ARP+DeCo still falls short of RVT-2+DeCo, which adopts a coarse-to-fine visual feature extraction strategy to handle such tasks more reliably.

### C.2 Impact of Half and Full Interactions on DeCo's Generalization Performance

Experimental results in Table 6 show that using full interaction as the unit of task decomposition consistently yields higher success rates across different training set sizes compared to half interaction in RVT-2+DeCo. This indicates that using full interaction decomposition in DeCo provides a more effective granularity for structuring demonstrations in language-conditioned multi-task imitation learning.

Half interaction decomposition tends to produce overly fine-grained subtasks that often lack semantic or operational coherence. This fragmentation weakens the learning signal, increases task interference, and makes policy learning more difficult. In contrast, full interactions typically correspond to semantically meaningful subgoals, providing richer and more stable inputs that support generalization. Furthermore, full interactions align more naturally with language instructions. Since each full segment usually maps to a complete command or interpretable subgoal, it facilitates better alignment between language and perception/action modalities—an essential factor in DeCo.

<!-- Meanless: 17-->




<!-- Media -->

<table><tr><td>Task</td><td>RVT-2+DeCo</td><td>RVT-2 (6 Long training)</td></tr><tr><td>10 Atomic</td><td>86.80% (30.63% % % % % % %</td><td>56.17%</td></tr><tr><td>12 All</td><td>66.67% (14.31% ↑)</td><td>52.36%</td></tr></table>

Table 7: Additional Results on the Impact of Atomic Task Design in DeCo.

<!-- Media -->

While finer decomposition may offer more compositional flexibility, it also expands the task space and increases the risk of generating incoherent or inconsistent combinations. Full interaction decomposition strikes a more balanced trade-off between flexibility and contextual consistency, improving generalization in long-horizon settings. This level of granularity also mirrors how humans plan and describe multi-step tasks. High-level instructions such as "open drawer" or "pick up the item" naturally correspond to full interactions, making this decomposition more compatible with downstream applications such as long-term task planning and skill transfer.

Overall, full interaction decomposition not only improves empirical performance but also provides a stronger semantic structure and practical relevance, making it a more effective strategy for language-conditioned multi-task imitation learning.

### C.3 Additional Analysis on Impact of Atomic Task Design

To further investigate the necessity of atomic task decomposition, we conduct an extended evaluation of the RVT-2 model under two training strategies: (1) training directly on demonstrations from six long-horizon tasks (including four original imitation learning tasks and two cross-domain tasks), denoted as RVT-2 ( 6 Long training); and (2) training on 10 atomic tasks in DeCoBench using the $\mathrm{{DeCo}}$ framework,denoted as RVT-2 + DeCo. We evaluate the models on two subsets: the 10 atomic tasks and the full set of 12 long-horizon tasks (12 All).

The six long-horizon tasks implicitly include the 10 atomic skills. Therefore, the baseline model trained on these six tasks (RVT-2 (6 Long training)) is expected to learn and execute the atomic skills and generalize to tasks that can be completed by composing them. However, the baseline model shows severe overfitting to the six tasks. When evaluated on atomic tasks or on unseen long-horizon tasks composed of those atomic skills, the model does not demonstrate true understanding of the execution process and tends to succeed in a more "coincidental" manner.

As shown in Table 7, the baseline performs significantly worse than our model, both in executing atomic skills and in completing long-horizon tasks. These improvements are not merely due to more data, but rather to better task structure. By decomposing complex tasks into reusable and semantically meaningful sub-goals, DeCo promotes structured and transferable policy representations, reduces trajectory overfitting, and improves policy composability. Maintaining consistency in physical interaction and ensuring the reusability of skills in atomic task design are key to enhancing generalization in multi-task imitation learning models.

## D DeCoBench: Benchmark Overview

We introduce DeCoBench, illustrated in Fig. 7, a benchmark designed to evaluate the zero-shot generalization capabilities of multi-task imitation learning (IL) models on novel yet compositional long-horizon 3D manipulation tasks. DeCoBench encompasses three task domains: Object Rearrangement with Drawer, Object Rearrangement with Cupboard, and Rubbish Cleanup.

In the Object Rearrangement with Drawer domain, the original IL tasks-put item in drawer without close and take item out of drawer and close-are decomposed based on the gripper's complete physical interaction cycle (i.e.,open $\rightarrow$ closed $\rightarrow$ open),resulting in four atomic tasks: open drawer, close drawer, put item in drawer, and take item out of drawer. In addition, a variant atomic task, take box out of drawer, is constructed via object substitution. This domain includes two 4-cycle compositional tasks (put item in without close, take item out without close), five 6-cycle tasks (put item in and close, take item out and close, put two items in same, take two items out of same), and two 10-cycle tasks (put two items in two different, take two items out of two different).

<!-- Meanless: 18-->




<!-- Media -->

<!-- figureText: Open Drawer Close Drawer in Drawer out of Drawe out of Drawer 10 Atomic Tasks (2 cycles, for Training) Cupboard Cupboard Cupboard to Dustpan in Dustpan Object Rearrangement with Cupboard Rubbish Cleanup 12 Long-horizon Tasks (for Testing) Exchange Boxes (4 cycles) Sweep and Drop Rubbish (4 cycles) Cross Domain Tasks Transfer Box (6 cycles, drawer & cupboard) Overhead Task Scene Retrieve and Sweep (4 cycles, cupboard & rubbish cleanup) Overhead Task Scene Object Rearrangement with Drawer Put Item in without Close (4 cycles) Take Item out without Close (4 cycles) Put Item in and Close (6 cycles) Take Item out and Close (6 cycles) Put 2 Items in Same (6 cycles) Take 2 Items out of Same (6 cycles) Put 2 Items in 2 Different (10 cycles) Take 2 Items out of 2 Different (10 cycles) -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_18.jpg?x=312&y=206&w=1174&h=749&r=0"/>

Figure 7: DeCoBench Overview: The benchmark covers three task domains. Based on the physical interaction-based task decomposition principle proposed in Sec. 3.2, 4 original IL tasks (marked with yellow dashed lines) are decomposed into 10 atomic tasks—including 2 constructed by replacing objects within the scene-for training multi-task IL models to acquire atomic skills. By semantically composing these atomic skills, we construct 12 long-horizon 3D manipulation tasks-including both original and cross-domain tasks-for evaluating models' zero-shot generalization capabilities.

<!-- figureText: Object Rearrangement with Drawer Object Rearrangement with Cupboard Lubbish Cleanup box in box out of broom out of sweep rubbish drop rubbish in cupboard cupboard cupboard to dustpan dustpar 二人女 exchange retrieve sweep and drop sweep ${}^{1}$ _____。 Example Task Scene Task Scene long-horizon tasks (4 cycle) cross-domain long-horizon put item in take item out of take box out of drawer drawer drawer put item in drawer take item out drawer put item in drawer hake item out drawer put 2 items in take 2 items out put 2 items in 2 different take 2 items out of 2 Task Scene different -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_18.jpg?x=311&y=1268&w=1177&h=370&r=0"/>

Figure 8: The compositional relationships between the atomic tasks and the compositonal tasks as well as cross-domain tasks in DeCoBench are represented using arrows.

<!-- Media -->

In the Object Rearrangement with Cupboard domain, the original IL task exchange boxes is decomposed into two atomic tasks: box in cupboard and box out of cupboard. Another atomic task, take broom out of cupboard, is introduced by substituting the manipulated object. The exchange boxes task serves as a 4-cycle compositional long-horizon task for evaluation.

<!-- Meanless: 19-->




<!-- Media -->

<table><tr><td>Task name</td><td>Language Template</td><td>#Items</td><td>#Variations</td><td>Variation Type</td></tr><tr><td>(a) open_drawer</td><td>"Open the <top/middle/bottom> drawer."</td><td>1</td><td>3</td><td>category,placement</td></tr><tr><td>(b) close_drawer</td><td>"Close the <top/middle/bottom> drawer."</td><td>1</td><td>3</td><td>category,placement</td></tr><tr><td>(c) put_in_opened_drawer</td><td>"Put the block in the <top/middle/bottom> drawer."</td><td>1</td><td>3</td><td>category,placement</td></tr><tr><td>(d) take_out_of_opened_drawer</td><td>"Take the block out of the <top/middle/bottom> drawer and place it on the drawer's surface."</td><td>1</td><td>3</td><td>category,placement</td></tr><tr><td>(e) box_out_of_opened_drawer</td><td>"Take the strawberry jello box out of the <top/middle/bottom> drawer and place it on the drawer's surface."</td><td>1</td><td>3</td><td>category,placement</td></tr><tr><td>(f) box_in_cupboard</td><td>"Put the <strawberry jello/spam/sugar> in the cupboard."</td><td>1</td><td>3</td><td>category,placement</td></tr><tr><td>(g) box_out_of_cupboard</td><td>"Put the <strawberry jello/spam/sugar> on the table."</td><td>1</td><td>3</td><td>category,placement</td></tr><tr><td>(h) broom_out_of_cupboard</td><td>"Take the broom out of the cupboard and place it on the table."</td><td>1</td><td>1</td><td>placement</td></tr><tr><td>(i) sweep_to_dustpan</td><td>"Sweep dirt to dustpan."</td><td>1</td><td>1</td><td>placement</td></tr><tr><td>(j) rubbish_in_dustpan</td><td>"Drop the rubbish into the dustpan."</td><td>1</td><td>1</td><td>placement</td></tr></table>

Table 8: Properties of the atomic tasks, in DeCoBench. We report on language template, the number of items that the robot can interact with, the task variations and variation type.

<!-- Media -->

In the Rubbish Cleanup domain, the original IL task sweep and drop rubbish is decomposed into two atomic tasks: sweep rubbish to dustpan and drop rubbish in dustpan, with the original task used as a 4-cycle compositional long-horizon evaluation task.

Moreover, DeCoBench includes two cross-domain compositional tasks: transfer box (6 cycles), spanning the Drawer and Cupboard domains, and retrieve and sweep (4 cycles), spanning the Cupboard and Cleanup domains.

Execution videos demonstrating the performance of different models on all 12 compositional tasks in DeCoBench are available on the project website: https://deco226.github.io.The full dataset for all DeCoBench tasks is provided in the supplementary materials.

In the following we provide details of the DeCoBench tasks. A summary of the 10 atomic tasks is provided in Table 8. A summary of the 12 compositional long-horizon tasks is provided in Table 9.

### D.1 Atomic Tasks in DeCoBench

## (a) open_drawer

Task Description: The robot must open a specified drawer (top, middle, or bottom) by pulling its handle.

Success Metric: The task is considered successful when the specified drawer is opened by at least 0.15 meters.

Objects: A cabinet with three drawers (top, middle, bottom), each with its own joint and handle.

Language Instructions for Full Interaction: Open the <top/middle/bottom> drawer.

Language Instructions for Half Interactions: Grasp the <top/middle/bottom> drawer handle; Pull the <top/middle/bottom> drawer open.

Variation Number: 3

## (b) close_drawer

Task Description: The robot must close a specified drawer (top, middle, or bottom) by pushing it shut.

Success Metric: The task is considered successful when the target drawer is pushed to within 0.03 meters of its fully closed position.

Objects: A cabinet with three drawers (top, middle, bottom), each with its own joint and handle.

Language Instructions for Full Interaction: Close the <top/middle/bottom> drawer.

Language Instructions for Half Interactions: Move close to the <top/middle/bottom> drawer handle; Push the <top/middle/bottom> drawer shut.

## Variation Number: 3

<!-- Meanless: 20-->


## (c) put_in_opened_drawer

Task Description: The robot must pick up a block and place it into a specified drawer (top, middle, or bottom) that is already open.

Success Metric: The task is considered successful when the block is detected by the proximity sensor placed inside the target drawer.

Objects: A cabinet with three drawers (top, middle, bottom), and one block placed near the drawer.

Language Instructions for Full Interaction: Put the block in the <top/middle/bottom> drawer.

Language Instructions for Half Interactions: Pick up the block on the drawer's surface; Place the block in the <top/middle/bottom> drawer.

Variation Number: 3

## (d) take_out_of_opened_drawer

Task Description: The robot must take a block out of a specified drawer (top, middle, or bottom) that is already open, and place it on the surface of the drawer.

Success Metric: The task is considered successful when the block is detected by the proximity sensor placed on the drawer's surface.

Objects: A cabinet with three drawers (top, middle, bottom), one block inside the drawer, and a proximity sensor on the drawer's surface.

Language Instructions for Full Interaction: Take the block out of the $<$ top/middle/bottom $>$ drawer and place the block on the drawer's surface.

Language Instructions for Half Interactions: Pick up the block in the $<$ top/middle/bottom $>$ drawer; Place the block on the drawer's surface.

Variation Number: 3

## (e) box_out_of_opened_drawer

Task Description: The robot must take a specific object (a strawberry jello box) out of a specified drawer (top, middle, or bottom) that is already open, and place it onto the drawer's surface.

Success Metric: The task is considered successful when the jello box is detected on the drawer's surface, outside of the drawer.

Objects: A cabinet with three drawers (top, middle, bottom), and one strawberry jello box placed inside the drawer.

Language Instructions for Full Interaction: Take the strawberry jello box out of the <top/middle/bottom> drawer and place it on the drawer's surface.

Language Instructions for Half Interactions: Pick up the strawberry jello box in the <top/middle/bottom> drawer; Place it on the drawer's surface.

Variation Number: 3

## (f) box_in_cupboard

Task Description: The robot must pick up a specific grocery item (either strawberry jello, spam, or sugar) from a tabletop and place it into a cupboard.

<!-- Meanless: 21-->




Success Metric: The task is considered successful when the selected grocery item is detected inside the cupboard by a proximity sensor.

Objects: A cupboard, three grocery items (strawberry jello, spam, sugar), and a proximity sensor to detect when the items are placed in the cupboard.

Language Instructions for Full Interaction: Put the <strawberry jello/spam/sugar> in the cupboard.

Language Instructions for Half Interactions: Pick up the <strawberry jello/spam/sugar> on the table; Place the <strawberry jello/spam/sugar> in the cupboard.

Variation Number: 3

## (g) box_out_of_cupboard

Task Description: The robot must take a specific grocery item (either strawberry jello, spam, or sugar) out of a cupboard and place it onto the table.

Success Metric: The task is considered successful when the selected grocery item is detected by a proximity sensor placed on the table.

Objects: A cupboard, three grocery items (strawberry jello, spam, sugar), and a proximity sensor to detect when the items are placed on the table.

Language Instructions for Full Interaction: Put the <strawberry jello/spam/sugar> on the table.

Language Instructions for Half Interactions: Pick up the <strawberry jello/spam/sugar> in the cupboard; Place the <strawberry jello/spam/sugar> on the table.

Variation Number: 3

## (r) broom_out_of_cupboard

Task Description: The robot must take the broom out of the cupboard and place it on the table.

Success Metric: The task is considered successful when the broom is detected on the table by a proximity sensor.

Objects: A broom, a broom holder, a cupboard, a table, and a proximity sensor for detecting the broom on the table.

Language Instructions for Full Interaction: Take the broom out of the cupboard and place it on the table.

Language Instructions for Half Interactions: Pick up the broom in the cupboard; Place the broom on the table.

Variation Number: 1

## (h) sweep_to_dustpan

Task Description: The robot must use a broom to sweep dirt into a dustpan.

Success Metric: The task is considered successful when all dirt particles are detected in the dustpan by a proximity sensor.

Objects: A broom, a dustpan, and multiple dirt particles.

Language Instructions for Full Interaction: Sweep dirt to dustpan.

<!-- Meanless: 22-->




<!-- Media -->

<table><tr><td>Task name</td><td>Language Template</td><td>#Items</td><td>#Variations</td><td>Variation Type</td></tr><tr><td>(j) put_in_without_close</td><td>"Open the <top/middle/bottom> drawer; Put the block in the <top/middle/bottom> drawer."</td><td>1</td><td>3</td><td>category,placement</td></tr><tr><td>(k) take_out_without_close</td><td>"Open the <top/middle/bottom> drawer; Take the block out of the <top/middle/bottom> drawer and place the block on the drawer's surface."</td><td>1</td><td>3</td><td>category,placement</td></tr><tr><td>(l) put_in_and_close</td><td>"Open the <top/middle/bottom> drawer; Put the block in the <top/middle/bottom> drawer; Close the <top/middle/bottom> drawer."</td><td>1</td><td>3</td><td>category,placement</td></tr><tr><td>(m) take_out_and_close</td><td>"Open the <top/middle/bottom> drawer; Take the block out of the <top/middle/bottom> drawer and place the block on the drawer’s surface; Close the <top/middle/bottom> drawer."</td><td>1</td><td>3</td><td>category,placement</td></tr><tr><td>(n) put_two_in_same</td><td>"Open the <top/middle/bottom> drawer; Put the block in the <top/middle/bottom> drawer; Put the block in the $<$ top/middle/bottom $>$ drawer."</td><td>2</td><td>3</td><td>category,placement</td></tr><tr><td>(o) take_two_out_of_same</td><td>"Open the <top/middle/bottom> drawer; Take the block out of the <top/middle/bottom> drawer and place the block on the drawer's surface; Take the block out of the <top/middle/bottom> drawer and place the block on the drawer's surface."</td><td>2</td><td>3</td><td>category,placement</td></tr><tr><td>(p) put_two_in_different</td><td>"Open the <top/middle/bottom> drawer; Put one block in the <top/middle/bottom> drawer; Open the <top/middle/bottom> drawer; Put the other block in the <top/middle/bottom> drawer."</td><td>2</td><td>6</td><td>category,placement</td></tr><tr><td>(q) take_two_out_of_different</td><td>"Open the <top/middle/bottom> drawer; Take one block out of the <top/middle/bottom> drawer and place the block on the drawer's surface; Open the <top/middle/bottom> drawer; Take the other block out of the $<$ top/middle/bottom $>$ drawer and place the block on the drawer's surface."</td><td>2</td><td>6</td><td>category,placement</td></tr><tr><td>(r) box_exchange</td><td>"Put the sugar on the table and put the spam in the cupboard."</td><td>2</td><td>1</td><td>placement</td></tr><tr><td>(s) sweep_and_drop</td><td>"Drop the rubbish into the dustpan and sweep dirt into the dustpan."</td><td>2</td><td>1</td><td>placement</td></tr><tr><td>(t) transfer_box</td><td>"Open the $<$ top/middle/bottom $>$ drawer; Take the strawberry jello out of the $<$ top/middle/bottom $>$ drawer and place it in the cupboard."</td><td>2</td><td>3</td><td>category,placement</td></tr></table>

Table 9: Properties of the compositional long-horizon tasks in DeCoBench. We report on language template, the number of items that the robot can interact with, the task variations and variation type.

<!-- Media -->

Language Instructions for Half Interactions: Pick up the broom on the table; Sweep dirt to dustpan.

Variation Number: 1

## (i) rubbish_in_dustpan

Task Description: The robot must drop a piece of rubbish into a dustpan.

Success Metric: The task is considered successful when the rubbish is detected inside the dustpan by a proximity sensor.

Objects: A rubbish object and a dustpan with a proximity sensor to detect when the rubbish is placed inside.

Language Instructions for Full Interaction: Drop the rubbish into the dustpan.

Language Instructions for Half Interactions: Pick up the rubbish on the table; Drop the rubbish into the dustpan.

Variation Number: 1

### D.2 Compositional Long-horizon Tasks in DeCoBench

## (j) put_in_without_close

Task Description: The robot must place a block into a specified drawer (top, middle, or bottom) without closing the drawer afterward.

Success Metric: The task is considered successful when the block is detected inside the specified drawer by a proximity sensor.

Objects: A cabinet with three drawers (top, middle, bottom), a block, and a proximity sensor for detecting the block's presence in the drawer.

Language Instructions for Full Interaction: Open the <top/middle/bottom> drawer; Put the block in the <top/middle/bottom> drawer.

Language Instructions for Half Interactions: Grasp the <top/middle/bottom> drawer handle; Pull the <top/middle/bottom> drawer open; Pick up the block on the drawer's surface; Place the block in the <top/middle/bottom> drawer.

<!-- Meanless: 23-->




<!-- Meanless: Variation Number: 3-->

## (k) take_out_without_close

Task Description: The robot must take a block out of a specified drawer (top, middle, or bottom) and place it on the drawer's surface without closing the drawer afterward.

Success Metric: The task is considered successful when the block is detected on the drawer's surface by a proximity sensor.

Objects: A cabinet with three drawers (top, middle, bottom), a block, and a proximity sensor for detecting the block's presence on the drawer's surface.

Language Instructions for Full Interaction: Open the <top/middle/bottom> drawer; Take the block out of the <top/middle/bottom> drawer and place the block on the drawer's surface.

Language Instructions for Half Interactions: Grasp the <top/middle/bottom> drawer handle; Pull the <top/middle/bottom> drawer open; Pick up the block in the <top/middle/bottom> drawer; Place the block on the drawer's surface.

Variation Number: 3

## (l) put_in_and_close

Task Description: The robot must place a block into a specified drawer (top, middle, or bottom) and then close the drawer.

Success Metric: The task is considered successful when the block is detected inside the specified drawer and the drawer is closed, verified by a proximity sensor.

Objects: A cabinet with three drawers (top, middle, bottom), a block, and a proximity sensor for detecting the block's presence in the drawer.

Language Instructions for Full Interaction: Open the <top/middle/bottom> drawer; Put the block in the <top/middle/bottom> drawer; Close the <top/middle/bottom> drawer.

Language Instructions for Half Interactions: Grasp the <top/middle/bottom> drawer handle; Pull the <top/middle/bottom> drawer open; Pick up the block on the drawer's surface; Place the block in the <top/middle/bottom> drawer; Move close to the <top/middle/bottom> drawer handle; Push the <top/middle/bottom> drawer shut.

Variation Number: 3 (m) take_out_and_close

Task Description: The robot must take a block out of a specified drawer (top, middle, or bottom), place it on the drawer's surface, and then close the drawer.

Success Metric: The task is considered successful when the block is detected on the drawer's surface and the drawer is closed, verified by a proximity sensor.

Objects: A cabinet with three drawers (top, middle, bottom), a block, and a proximity sensor for detecting the block's presence on the drawer's surface and confirming the drawer closure.

Language Instructions for Full Interaction: Open the <top/middle/bottom> drawer; Take the block out of the $<$ top/middle/bottom $>$ drawer and place the block on the drawer's surface; Close the <top/middle/bottom> drawer.

Language Instructions for Half Interactions: Grasp the <top/middle/bottom> drawer handle; Pull the <top/middle/bottom> drawer open; Pick up the block in the <top/middle/bottom> drawer;

<!-- Meanless: 24-->




Place the block on the drawer's surface; Move close to the <top/middle/bottom> drawer handle; Push the <top/middle/bottom> drawer shut.

Variation Number: 3

## (n) put_two_in_same

Task Description: The robot must place two blocks into a specified drawer (top, middle, or bottom).

Success Metric: The task is considered successful when both blocks are detected inside the specified drawer by a proximity sensor.

Objects: A cabinet with three drawers (top, middle, bottom), two blocks, and a proximity sensor for detecting the blocks' presence in the drawer.

Language Instructions for Full Interaction: Open the <top/middle/bottom> drawer; Put the block in the <top/middle/bottom> drawer; Put the block in the <top/middle/bottom> drawer.

Language Instructions for Half Interactions: Grasp the <top/middle/bottom> drawer handle; Pull the <top/middle/bottom> drawer open; Pick up the block on the drawer's surface; Place the block in the <top/middle/bottom> drawer; Pick up the block on the drawer's surface; Place the block in the <top/middle/bottom> drawer.

Variation Number: 3

## (o) take_two_out_of_same

Task Description: The robot must take two blocks out of a specified drawer (top, middle, or bottom) and place them on the drawer's surface.

Success Metric: The task is considered successful when both blocks are detected on the drawer's surface by a proximity sensor.

Objects: A cabinet with three drawers (top, middle, bottom), two blocks, and a proximity sensor for detecting the blocks' presence on the drawer's surface.

Language Instructions for Full Interaction: Open the <top/middle/bottom> drawer; Take the block out of the $<$ top/middle/bottom $>$ drawer and place the block on the drawer's surface; Take the block out of the <top/middle/bottom> drawer and place the block on the drawer's surface.

Language Instructions for Half Interactions: Grasp the <top/middle/bottom> drawer handle; Pull the <top/middle/bottom> drawer open; Pick up the block in the <top/middle/bottom> drawer; Place the block on the drawer's surface; Pick up the block in the <top/middle/bottom> drawer; Place the block on the drawer's surface.

Variation Number: 3

## (p) put_two_in_different

Task Description: The robot must place two blocks into two different specified drawers (top, middle, or bottom).

Success Metric: The task is considered successful when each block is detected inside its corresponding drawer by a proximity sensor.

Objects: A cabinet with three drawers (top, middle, bottom), two blocks, and two proximity sensors for detecting each block's presence in its designated drawer.

<!-- Meanless: 25-->




Language Instructions for Full Interaction: Open the <top/middle/bottom> drawer; Put one block in the <top/middle/bottom> drawer; Close the <top/middle/bottom> drawer; Open the <top/middle/bottom> drawer; Put the other block in the <top/middle/bottom> drawer.

Language Instructions for Half Interactions: Grasp the <top/middle/bottom> drawer handle; Pull the <top/middle/bottom> drawer open; Pick up the block on the drawer's surface; Place the block in the <top/middle/bottom> drawer; Move close to the <top/middle/bottom> drawer handle; Push the <top/middle/bottom> drawer shut; Grasp the <top/middle/bottom> drawer handle; Pull the <top/middle/bottom> drawer open; Pick up the block on the drawer's surface; Place the block in the <top/middle/bottom> drawer.

Variation Number: 6

## (q) take_two_out_of_different

Task Description: The robot must take one block out of a specified drawer (top, middle, or bottom) and take another block out of a different specified drawer, then place both blocks on the drawer's surface.

Success Metric: The task is considered successful when both blocks are detected on the drawer's surface by a proximity sensor.

Objects: A cabinet with three drawers (top, middle, bottom), two blocks, and a proximity sensor for detecting each block's presence on the drawer's surface.

Language Instructions for Full Interaction: Open the <top/middle/bottom> drawer; Take one block out of the $<$ top/middle/bottom $>$ drawer and place the block on the drawer's surface; Close the <top/middle/bottom> drawer; Open the <top/middle/bottom> drawer; Take the other block out of the <top/middle/bottom> drawer and place the block on the drawer's surface.

Language Instructions for Half Interactions: Grasp the <top/middle/bottom> drawer handle; Pull the <top/middle/bottom> drawer open; Pick up the block in the <top/middle/bottom> drawer; Place the block on the drawer's surface; Push the <top/middle/bottom> drawer shut; Grasp the <top/middle/bottom> drawer handle; Pull the <top/middle/bottom> drawer open; Pick up the block in the <top/middle/bottom> drawer; Place the block on the drawer's surface.

Variation Number: 6

## (s) box_exchange

Task Description: The robot must exchange the positions of two grocery items: the sugar and the spam. Specifically, the robot will place the sugar on the table and put the spam in the cupboard.

Success Metric: The task is considered successful when the sugar is detected on the table and the spam is detected in the cupboard by their respective proximity sensors.

Objects: Two grocery items (sugar and spam), a table, a cupboard, and proximity sensors for detecting the placement of each item.

Language Instructions for Full Interaction: Put the sugar on the table and put the spam in the cupboard.

Language Instructions for Half Interactions: Pick up the sugar in the cupboard; Place the sugar on the table; Pick up the spam on the table; Place the spam in the cupboard.

Variation Number: 1

## (t) sweep_and_drop

Task Description: The robot must clean all dirt and rubbish into a dustpan using a broom.

<!-- Meanless: 26-->




Success Metric: The task is considered successful when all dirt pieces and the rubbish are detected in the dustpan by a proximity sensor.

Objects: A broom, a dustpan, several pieces of dirt ( 5 in total), and a piece of rubbish, along with a proximity sensor for detecting their presence in the dustpan.

Language Instructions for Full Interaction: Drop the rubbish into the dustpan and sweep dirt into the dustpan.

Language Instructions for Half Interactions: Pick up the rubbish on the table; Drop the rubbish into the dustpan; Pick up the broom on the table; Sweep dirt into the dustpan.

Variation Number: 1

## (u) transfer_box

Task Description: The robot must take the strawberry jello out of a specified drawer (top, middle, or bottom) and place it in the cupboard.

Success Metric: The task is considered successful when the strawberry jello is detected inside the cupboard by a proximity sensor.

Objects: A drawer with three compartments (top, middle, bottom), a strawberry jello item, a cupboard, and a proximity sensor for detecting the item's presence in the cupboard.

Language Instructions for Full Interaction: Open the <top/middle/bottom> drawer; Take the strawberry jello out of the $<$ top/middle/bottom $>$ drawer and place the strawberry jello on the drawer's surface; Put the strawberry jello in the cupboard.

Language Instructions for Half Interactions: Grasp the <top/middle/bottom> drawer handle; Pull the <top/middle/bottom> drawer open; Pick up the strawberry jello in the <top/middle/bottom> drawer; Place the strawberry jello in the cupboard.

Variation Number: 3

## (v) retrieve_and_sweep

Task Description: The robot must sweep dirt into a dustpan using a broom.

Success Metric: The task is considered successful when all dirt pieces are detected in the dustpan by a proximity sensor.

Objects: A broom, multiple pieces of dirt ( 5 in total), a dustpan, and a proximity sensor for detecting the presence of dirt in the dustpan.

Language Instructions for Full Interaction: Put the broom on the table and sweep dirt into the dustpan.

Language Instructions for Half Interactions: Pick up the broom in the cupboard; Sweep dirt into the dustpan.

Variation Number: 1

## E Real-world Experiments

In the following we provide details of the real-world setup and tasks. Figure 9 illustrates the real-world setup. Figure 10 visualizes the real-world atomic tasks, Figure 11 visualizes the real-world compositional long-horizon tasks. A summary of the 6 atomic tasks is provided in Table 10. A summary of the 9 compositional long-horizon tasks is provided in Table 11. Video demonstrations of the real-world tasks are provided on our website : https://deco226.github.io.

<!-- Meanless: 27-->




<!-- Media -->

<!-- figureText: Intel Real Sense D435i Franka Emika Panda Top Drawer Yellow Block Buttom Drawer Blue Block -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_27.jpg?x=455&y=223&w=881&h=632&r=0"/>

Figure 9: Real-Robot Setup with RealSense D435i and Franka Emika Panda.

<!-- figureText: Open Drawer Block on Drawer (2 cycles) Put in Opened Drawer (2 cycles) Take out of Opened Drawer (2 cycles) (2 cycles) Close Drawer (2 cycles) Block off Drawer (2 cycles) -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_27.jpg?x=423&y=976&w=953&h=485&r=0"/>

Figure 10: Real-World Atomic Tasks.

<!-- Media -->

### E.1 Real-World Atomic Tasks

(a) open_drawer

Task Description: The robot must open a specified drawer.

Success Metric: The task is successful if the target drawer is fully opened.

Objects: One drawer.

Language Instructions: Open the <top/bottom> drawer.

Variation Number: 2

## (b) close_drawer

Task Description: The robot must close a specified drawer.

<!-- Meanless: 28-->




<!-- Media -->

<table><tr><td>Task name</td><td>Language Template</td><td>#Items</td><td>#Variations</td><td>Variation Type</td></tr><tr><td>(a) open_drawer</td><td>"Open the $<$ top/bottom $>$ drawer."</td><td>1</td><td>2</td><td>placement</td></tr><tr><td>(b) close_drawer</td><td>"Close the <top/bottom> drawer."</td><td>1</td><td>2</td><td>placement</td></tr><tr><td>(c) put_in_opened_drawer</td><td>"Put the <blue/yellow> block in the <top/bottom> drawer."</td><td>2</td><td>4</td><td>color,placement</td></tr><tr><td>(d) take_out_of_opened_drawer</td><td>"Take the <blue/yellow> block out of the <top/bottom> drawer and place the <blue/yellow> block on the drawer's surface."</td><td>2</td><td>4</td><td>color,placement</td></tr><tr><td>(e) block_on_drawer</td><td>"Put the <blue/yellow> block on the drawer's surface."</td><td>2</td><td>2</td><td>color</td></tr><tr><td>(f) block_off_drawer</td><td>"Put the <blue/yellow> block on the table."</td><td>3</td><td>2</td><td>color</td></tr></table>

Table 10: Properties of atomic tasks in real-world experiments.

<!-- Media -->

Success Metric: The task is successful if the target drawer is completely closed.

Objects: One drawer.

Language Instructions: Close the <top/bottom> drawer.

Variation Number: 2

## (c) put_in_opened_drawer

Task Description: The robot places a colored block into an already opened drawer.

Success Metric: The block must be placed inside the correct opened drawer.

Objects: One colored block and one opened drawer.

Language Instructions: Put the <blue/yellow> block in the <top/bottom> drawer.

Variation Number: 4

## (d) take_out_of_opened_drawer

Task Description: The robot retrieves a block from an already opened drawer and places it on the drawer's surface.

Success Metric: The correct block is taken out and placed on the drawer surface.

Objects: One colored block and one opened drawer.

Language Instructions: Take the <blue/yellow> block out of the <top/bottom> drawer and place the <blue/yellow > block on the drawer's surface.

Variation Number: 4

## (e) block_on_drawer

Task Description: The robot places a block on top of the drawer's surface. Success Metric: The block is correctly placed on the surface of the drawer. Objects: One block and one drawer. Language Instructions: Put the <blue/yellow $>$ block on the drawer's surface. Variation Number: 2

## (f) block_off_drawer

Task Description: The robot places a block on the table, away from the drawer. Success Metric: The block is placed on the table, not on the drawer.

<!-- Meanless: 29-->




<!-- Media -->

<!-- figureText: ✓ Box Exchange (6 cycles) (4 cycles) On and in Different (6 cycles) (16 cycles) Take 2 out of Different Out of Different and of (12 cycles) (16 cycles) Put in w/o Close Put in and Close (4 cycles) Take out w/o Close Take out and Close Put 2 in Different (12 cycles) -->

<img src="https://cdn.noedgeai.com/bo_d3ve1ns601uc738lualg_29.jpg?x=311&y=200&w=1177&h=347&r=0"/>

Figure 11: Real-World Compositional Long-Horizon Tasks.

<table><tr><td>Task name</td><td>Language Template</td><td>#Items</td><td>#Variations</td><td>Variation Type</td></tr><tr><td>(g) put_in_without_close</td><td>"Put the <blue/yellow> block in the closed <top/bottom> drawer."</td><td>1</td><td>4</td><td>color,placement</td></tr><tr><td>(h) put_in_and_close</td><td>"Put the <blue/yellow> block in the closed <top/bottom> drawer and close it."</td><td>1</td><td>4</td><td>color,placement</td></tr><tr><td>(i) take_out_without_close</td><td>"Take the <blue/yellow> block out of the <top/bottom> closed drawer and place the <blue/yellow> block on the drawer's surface."</td><td>1</td><td>4</td><td>color,placement</td></tr><tr><td>(j) take_out_and_close</td><td>"Take the <blue/yellow> block out of the <top/bottom> drawer, place the <blue/yellow> block on the drawer's surface, and then close the <top/bottom> drawer."</td><td>1</td><td>4</td><td>color,placement</td></tr><tr><td>(k) put_two_in_different</td><td>"Put the <blue/yellow> block in the <top/bottom> closed drawer and put the <blue/yellow> block in the <top/bottom> closed drawer."</td><td>2</td><td>4</td><td>color,placement</td></tr><tr><td>(l) take_two_out_of_different</td><td>"Take the <blue/yellow> block out of the <top/bottom> closed drawer and take the <blue/yellow> block out of the <top/bottom> closed drawer, then place them on the drawer's surface."</td><td>2</td><td>4</td><td>color,placement</td></tr><tr><td>(m) block_exchange</td><td>"Exchange the positions of the two blocks."</td><td>2</td><td>2</td><td>color,placement</td></tr><tr><td>(n) on_and_in_different</td><td>"Place both blocks on the drawer's surface,then put the <blue/yellow> block in the $<$ top/bottom $>$ drawer and the $<$ blue/yellow $>$ block in the $<$ top/bottom $>$ drawer."</td><td>2</td><td>2</td><td>color,placement</td></tr><tr><td>(o) out_of_different_and_off</td><td>"Take the <blue/yellow> block out of the <top/bottom> drawer and take the <blue/yellow> block out of the <top/bottom> drawer, then put both blocks on the table."</td><td>2</td><td>2</td><td>color,placement</td></tr></table>

Table 11: Properties of the real-world compositional long-horizon tasks.

<!-- Media -->

Objects: One block, one drawer, and one table.

Language Instructions: Put the <blue/yellow > block on the table.

Variation Number: 2

### E.2 Real-World Compositional Long-horizon Tasks

(g) put_in_without_close

Task Description: The robot places a block into a closed drawer without closing it.

Success Metric: The block is placed in the correct drawer, which remains open.

Objects: One colored block and one drawer.

Language Instructions: Put the <blue/yellow> block in the closed <top/bottom> drawer.

Variation Number: 4

## (h) put_in_and_close

Task Description: The robot places a block into a closed drawer and then closes the drawer.

Success Metric: The block is placed correctly, and the drawer is fully closed.

Objects: One colored block and one drawer.

Language Instructions: Put the <blue/yellow> block in the closed <top/bottom> drawer and close it.

Variation Number: 4

<!-- Meanless: 30-->




## (i) take_out_without_close

Task Description: The robot takes a block out of a closed drawer and places it on the drawer's surface, leaving the drawer open.

Success Metric: The block is retrieved and placed correctly, and the drawer remains open.

Objects: One colored block and one drawer.

Language Instructions: Take the <blue/yellow> block out of the <top/bottom> closed drawer and place the <blue/yellow> block on the drawer's surface.

Variation Number: 4

## (j) take_out_and_close

Task Description: The robot retrieves a block from a closed drawer, places it on the drawer's surface, and then closes the drawer.

Success Metric: The block is placed correctly, and the drawer is closed afterward.

Objects: One colored block and one drawer.

Language Instructions: Take the <blue/yellow> block out of the <top/bottom> drawer; place the <blue/yellow> block on the drawer's surface, and then close the <top/bottom> drawer.

Variation Number: 4

## (k) put_two_in_different

Task Description: The robot places two blocks into two different closed drawers. Success Metric: Each block is correctly placed into the specified drawer. Objects: Two colored blocks and two drawers. Language Instructions: Put the <blue/yellow> block in the <top/bottom> closed drawer and put the <blue/yellow> block in the <top/bottom> closed drawer.

Variation Number: 4

## (l) take_two_out_of_different

Task Description: The robot retrieves two blocks from two different closed drawers and places them on the drawer surface.

Success Metric: Both blocks are correctly retrieved and placed.

Objects: Two colored blocks and two drawers.

Language Instructions: Take the <blue/yellow > block out of the <top/bottom> closed drawer and take the <blue/yellow > block out of the <top/bottom> closed drawer, then place them on the drawer's surface.

Variation Number: 4

(m) block_exchange

Task Description: The robot swaps the positions of two blocks.

Success Metric: The two blocks end up in each other's original positions.

<!-- Meanless: 31-->




Objects: Two blocks.

Language Instructions: Exchange the positions of the two blocks.

Variation Number: 2

(n) on_and_in_different

Task Description: The robot places both blocks on the drawer surface, then puts each block into a different drawer.

Success Metric: Both blocks are correctly placed and then inserted into the correct drawers.

Objects: Two colored blocks and two drawers.

Language Instructions: Place both blocks on the drawer’s surface,then put the <blue/yellow> block in the <top/bottom> drawer and the <blue/yellow> block in the <top/bottom> drawer.

Variation Number: 2

## (o) out_of_different_and_off

Task Description: The robot takes two blocks out of two different drawers and puts them on the table.

Success Metric: Both blocks are retrieved and placed on the table surface.

Objects: Two colored blocks and two drawers.

Language Instructions: Take the <blue/yellow> block out of the <top/bottom> drawer and take the <blue/yellow> block out of the <top/bottom> drawer, then put both blocks on the table.

Variation Number: 2

<!-- Meanless: 32-->

