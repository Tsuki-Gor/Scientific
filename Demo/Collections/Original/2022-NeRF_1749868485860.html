
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <h1>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>NeRF：将场景表示为神经辐射场以进行视图合成</h1></div><p>By Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>作者：Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi 和 Ren Ng</p></div><h2>Abstract</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>摘要</h2></div><p>We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location(x,y,z)and viewing direction \(\left( {\theta ,\phi }\right)\) ) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们提出了一种方法，通过优化基础的连续体积场函数，利用稀疏的输入视图集来实现复杂场景的新视图合成的最先进结果。我们的算法使用一个全连接（非卷积）深度网络来表示场景，其输入是一个单一的连续5D坐标（空间位置（x,y,z）和视角方向\(\left( {\theta ,\phi }\right)\)），输出是该空间位置的体积密度和视角依赖的发射辐射。我们通过沿相机光线查询5D坐标来合成视图，并使用经典的体积渲染技术将输出的颜色和密度投影到图像中。由于体积渲染自然可微分，优化我们表示所需的唯一输入是一组具有已知相机姿态的图像。我们描述了如何有效地优化神经辐射场，以渲染具有复杂几何形状和外观的照片级真实新视图，并展示了超越先前神经渲染和视图合成工作的结果。</p></div><h2>1. INTRODUCTION</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>1. 引言</h2></div><p>In this work, we address the long-standing problem of view synthesis in a new way. View synthesis is the problem of rendering new views of a scene from a given set of input images and their respective camera poses. Producing photorealistic outputs from new viewpoints requires correctly handling complex geometry and material reflectance properties. Many different scene representations and rendering methods have been proposed to attack this problem; however, so far none have been able to achieve photorealistic quality over a large camera baseline. We propose a new scene representation that can be optimized directly to reproduce a large number of high-resolution input views and is still extremely memory-efficient (see Figure 1).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在这项工作中，我们以一种新的方式解决了长期存在的视图合成问题。视图合成是从给定的一组输入图像及其各自的相机姿态渲染场景新视图的问题。从新视点生成照片级真实的输出需要正确处理复杂的几何形状和材料反射特性。已经提出了许多不同的场景表示和渲染方法来解决这个问题；然而，到目前为止，没有一种方法能够在大相机基线下实现照片级真实的质量。我们提出了一种新的场景表示，可以直接优化以重现大量高分辨率的输入视图，并且仍然极其节省内存（见图1）。</p></div><p>We represent a static scene as a continuous 5D function that outputs the radiance emitted in each direction \(\left( {\theta ,\phi }\right)\) at each point(x,y,z)in space,and a density at each point which acts like a differential opacity controlling how much radiance is accumulated by a ray passing through(x,y,z). Our method optimizes a deep fully connected neural network without any convolutional layers (often referred to as a multilayer perceptron or MLP) to represent this function by regressing from a single \(5\mathrm{D}\) coordinate \(\left( {x,y,z,\theta ,\phi }\right)\) to a single volume density and view-dependent RGB color. To render this neural radiance field (NeRF) from a particular viewpoint, we: 1) march camera rays through the scene to generate a sampled set of 3D points, 2) use those points and their corresponding \(2\mathrm{D}\) viewing directions as input to the neural network to produce an output set of colors and densities, and 3) use classical volume rendering techniques to accumulate those colors and densities into a 2D image. Because this process is naturally differentiable, we can use gradient descent to optimize this model by minimizing the error between each observed image and the corresponding views rendered from our representation. Minimizing this error across multiple views encourages the network to predict a coherent model of the scene by assigning high-volume densities and accurate colors to the locations that contain the true underlying scene content. Figure 2 visualizes this overall pipeline.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们将静态场景表示为一个连续的5D函数，该函数输出在每个方向\(\left( {\theta ,\phi }\right)\)上发射的辐射，以及在每个点（x,y,z）处的密度，该密度充当差分不透明度，控制通过（x,y,z）点的光线积累多少辐射。我们的方法优化一个没有任何卷积层的深度全连接神经网络（通常称为多层感知器或MLP），通过从单个\(5\mathrm{D}\)坐标\(\left( {x,y,z,\theta ,\phi }\right)\)回归到单个体积密度和视角依赖的RGB颜色来表示这个函数。为了从特定视点渲染这个神经辐射场（NeRF），我们：1）沿着场景中的相机光线推进，以生成一组采样的3D点，2）使用这些点及其对应的\(2\mathrm{D}\)视角方向作为输入到神经网络中，以生成一组输出颜色和密度，3）使用经典的体积渲染技术将这些颜色和密度累积到2D图像中。由于这个过程自然可微分，我们可以使用梯度下降通过最小化每个观察图像与我们表示的相应视图之间的误差来优化这个模型。在多个视图中最小化这个误差鼓励网络通过将高体积密度和准确颜色分配给包含真实基础场景内容的位置来预测场景的连贯模型。图2可视化了这个整体流程。</p></div><!-- Media --><p>Figure 1. We present a method that optimizes a continuous 5D neural radiance field representation (volume density and view-dependent color at any continuous location) of a scene from a set of input images. We use techniques from volume rendering to accumulate samples of this scene representation along rays to render the scene from any viewpoint. Here, we visualize the set of 100 input views of the synthetic Drums scene randomly captured on a surrounding hemisphere, and we show two novel views rendered from our optimized NeRF representation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图1. 我们提出了一种方法，通过一组输入图像优化场景的连续5D神经辐射场表示（在任何连续位置的体积密度和视角依赖颜色）。我们使用体积渲染技术沿光线累积这个场景表示的样本，以从任何视点渲染场景。在这里，我们可视化了合成鼓场景的100个输入视图，这些视图随机捕获在周围半球上，并展示了从我们优化的NeRF表示渲染的两个新视图。</p></div><!-- figureText: Input Images Optimize NeRF Render new views 高考学校 顺序分子 新西新城 --><img src="https://cdn.noedgeai.com/bo_d164vfn7aajc7388jth0_0.jpg?x=899&#x26;y=860&#x26;w=725&#x26;h=187&#x26;r=0"><!-- Media --><p>We find that the basic implementation of optimizing a neural radiance field representation for a complex scene does not converge to a sufficiently high-resolution representation. We address this issue by transforming input 5D coordinates with a positional encoding that enables the MLP to represent higher frequency functions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们发现，优化复杂场景的神经辐射场表示的基本实现并未收敛到足够高分辨率的表示。我们通过对输入5D坐标进行位置编码转换来解决这个问题，使得MLP能够表示更高频率的函数。</p></div><p>Our approach can represent complex real-world geometry and appearance and is well suited for gradient-based optimization using projected images. By storing a scene in the parameters of a neural network, our method overcomes the prohibitive storage costs of discretized voxel grids when modeling complex scenes at high resolutions. We demonstrate that our resulting neural radiance field method quantitatively and qualitatively outperforms state-of-the-art view synthesis methods, such as works that fit neural 3D representations to scenes as well as works that train deep convolutional networks (CNNs) to predict sampled volumetric representations. This paper presents the first continuous neural scene representation that is able to render high-resolution photorealistic novel views of real objects and scenes from RGB images captured in natural settings.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的方法能够表示复杂的现实世界几何形状和外观，并且非常适合使用投影图像进行基于梯度的优化。通过将场景存储在神经网络的参数中，我们的方法克服了在高分辨率下建模复杂场景时离散体素网格的高昂存储成本。我们证明了我们得到的神经辐射场方法在定量和定性上都优于最先进的视图合成方法，例如将神经3D表示拟合到场景的工作，以及训练深度卷积网络（CNN）以预测采样体积表示的工作。本文提出了首个连续神经场景表示，能够从自然环境中捕获的RGB图像渲染高分辨率的照片级真实新视图。</p></div><hr>
<!-- Footnote --><p>The original version of this paper was published in Proceedings of the 2020 European Conference on Computer Vision.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>本文的原始版本发表在2020年欧洲计算机视觉会议论文集中。</p></div><!-- Footnote -->
<hr><!-- Media --><p>Figure 2. An overview of our neural radiance field scene representation and differentiable rendering procedure. We synthesize images by sampling 5D coordinates (location and viewing direction) along camera rays (a), feeding those locations into an MLP to produce a color and volume density (b), and using volume rendering techniques to composite these values into an image (c). This rendering function is differentiable. so we can optimize our scene representation by minimizing the residual between synthesized and ground truth observed images (d).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2. 我们的神经辐射场场景表示和可微渲染过程的概述。我们通过沿相机光线（a）采样5D坐标（位置和视角方向），将这些位置输入到多层感知器（MLP）中以生成颜色和体积密度（b），并使用体积渲染技术将这些值合成到图像中（c）。这个渲染函数是可微的，因此我们可以通过最小化合成图像与真实观察图像之间的残差（d）来优化我们的场景表示。</p></div><!-- figureText: 5D Input Output Volume Rendering Color + Density Rendering Loss Ray 1 Ray 2 -g.t. \( {\parallel }_{2}^{2} \) Ray Distance (b) (c) (d) Position + Direction \( \left( {x,y,z,\theta ,\phi }\right) \) (RGBσ) \( {F}_{\theta } \) Ray 2 (a) --><img src="https://cdn.noedgeai.com/bo_d164vfn7aajc7388jth0_1.jpg?x=251&#x26;y=332&#x26;w=1234&#x26;h=350&#x26;r=0"><!-- Media --><h2>2. RELATED WORK</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>2. 相关工作</h2></div><p>A promising recent direction in computer vision is encoding objects and scenes in the weights of an MLP that directly maps from a 3D spatial location to an implicit representation of the shape,such as the signed distance \({}^{3}\) at that location. However, these methods have so far been unable to reproduce realistic scenes with complex geometry with the same fidelity as techniques that represent scenes using discrete representations such as triangle meshes or voxel grids. In this section, we review these two lines of work and contrast them with our approach, which enhances the capabilities of neural scene representations to produce state-of-the-art results for rendering complex realistic scenes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>计算机视觉中的一个有前景的研究方向是将物体和场景编码在一个多层感知器（MLP）的权重中，该权重直接将3D空间位置映射到形状的隐式表示，例如该位置的有符号距离\({}^{3}\)。然而，这些方法迄今为止无法以与使用离散表示（如三角网格或体素网格）相同的保真度重现具有复杂几何形状的真实场景。在本节中，我们回顾这两条研究路线，并将其与我们的方法进行对比，我们的方法增强了神经场景表示的能力，以产生最先进的复杂真实场景渲染结果。</p></div><h3>2.1. Neural 3D shape representations</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>2.1. 神经3D形状表示</h3></div><p>Recent work has investigated the implicit representation of continuous 3D shapes as level sets by optimizing deep networks that map \({xyz}\) coordinates to signed distance functions \({}^{15}\) or occupancy fields. \({}^{11}\) However,these models are limited by their requirement of access to ground truth 3D geometry, typically obtained from synthetic 3D shape datasets such as ShapeNet. \({}^{2}\) Subsequent work has relaxed this requirement of ground truth 3D shapes by formulating differentiable rendering functions that allow neural implicit shape representations to be optimized using only \(2\mathrm{D}\) images. Niemeyer et al. \({}^{14}\) represent surfaces as \(3\mathrm{D}\) occupancy fields and use a numerical method to find the surface intersection for each ray, then calculate an exact derivative using implicit differentiation. Each ray intersection location is provided as the input to a neural 3D texture field that predicts a diffuse color for that point. Sitzmann et al. \({}^{21}\) use a less direct neural 3D representation that simply outputs a feature vector and RGB color at each continuous 3D coordinate, and propose a differentiable rendering function consisting of a recurrent neural network that marches along each ray to decide where the surface is located.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最近的研究探讨了通过优化深度网络将\({xyz}\)坐标映射到有符号距离函数\({}^{15}\)或占用场的连续3D形状的隐式表示作为水平集。\({}^{11}\)然而，这些模型受到对真实3D几何形状的访问要求的限制，通常这些几何形状来自合成3D形状数据集，如ShapeNet。\({}^{2}\)后续工作通过制定可微渲染函数放宽了对真实3D形状的要求，使得神经隐式形状表示能够仅使用\(2\mathrm{D}\)图像进行优化。Niemeyer等人\({}^{14}\)将表面表示为\(3\mathrm{D}\)占用场，并使用数值方法找到每条光线的表面交点，然后使用隐式微分法计算精确导数。每个光线交点位置作为输入提供给神经3D纹理场，该场预测该点的漫反射颜色。Sitzmann等人\({}^{21}\)使用一种不太直接的神经3D表示，该表示在每个连续3D坐标处简单地输出特征向量和RGB颜色，并提出了一种可微渲染函数，由递归神经网络组成，该网络沿每条光线推进以决定表面的位置。</p></div><p>Though these techniques can potentially represent complicated and high-resolution geometry, they have so far been limited to simple shapes with low geometric complexity, resulting in oversmoothed renderings. We show that an alternate strategy of optimizing networks to encode 5D radiance fields (3D volumes with 2D view-dependent appearance) can represent higher resolution geometry and appearance to render photorealistic novel views of complex scenes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>尽管这些技术可能表示复杂和高分辨率的几何形状，但迄今为止它们仅限于低几何复杂度的简单形状，导致渲染过于平滑。我们展示了一种替代策略，通过优化网络以编码5D辐射场（具有2D视角依赖外观的3D体积），可以表示更高分辨率的几何形状和外观，从而渲染复杂场景的照片级真实新视图。</p></div><h3>2.2. View synthesis and image-based rendering</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>2.2. 视图合成和基于图像的渲染</h3></div><p>The computer vision and graphics communities have made significant progress on the task of novel view synthesis by predicting traditional geometry and appearance representations from observed images. One popular class of approaches uses mesh-based scene representations. \({}^{1,4,{23}}\) Differentiable rasterizers \({}^{9}\) or pathtracers \({}^{7}\) can directly optimize mesh representations to reproduce a set of input images using gradient descent. However, gradient-based mesh optimization based on image reprojection is often difficult, likely because of local minima or poor conditioning of the loss landscape. Furthermore, this strategy requires a template mesh with fixed topology to be provided as an initialization before optimization, \({}^{7}\) which is typically unavailable for unconstrained real-world scenes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>计算机视觉和图形学界在通过从观察到的图像预测传统几何形状和外观表示来进行新视图合成的任务上取得了显著进展。一类流行的方法使用基于网格的场景表示。\({}^{1,4,{23}}\)可微光栅化器\({}^{9}\)或路径追踪器\({}^{7}\)可以直接优化网格表示，以使用梯度下降重现一组输入图像。然而，基于图像重投影的梯度优化网格通常很困难，可能是由于局部最小值或损失景观的条件较差。此外，这种策略要求在优化之前提供具有固定拓扑的模板网格\({}^{7}\)，而这通常在不受约束的真实场景中不可用。</p></div><p>Another class of methods use volumetric representations to address the task of high-quality photorealistic view synthesis from a set of input RGB images. Volumetric approaches are able to realistically represent complex shapes and materials, are well suited for gradient-based optimization, and tend to produce less visually distracting artifacts than mesh-based methods. Early volumetric approaches used observed images to directly color voxel grids. \({}^{19}\) More recently,several methods \({}^{{12},{25}}\) have used large datasets of multiple scenes to train deep networks that predict a sampled volumetric representation from a set of input images,and then use either alpha compositing \({}^{16}\) or learned compositing along rays to render novel views at test time. Other works have optimized a combination of CNNs and sampled voxel grids for each specific scene, such that the CNN can compensate for discretization artifacts from low-resolution voxel grids \({}^{20}\) or allow the predicted voxel grids to vary based on input time or animation controls. \({}^{8}\) Although these volumetric techniques have achieved impressive results for novel view synthesis, their ability to scale to higher resolution imagery is fundamentally limited by poor time and space complexity due to their discrete sampling-rendering higher resolution images requires a finer sampling of \(3\mathrm{D}\) space. We circumvent this problem by instead encoding a continuous volume within the parameters of a deep fully connected neural network, which not only produces significantly higher quality renderings than prior volumetric approaches but also requires just a fraction of the storage cost of those sampled volumetric representations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>另一类方法使用体积表示来解决从一组输入RGB图像生成高质量照片级视图合成的任务。体积方法能够真实地表示复杂的形状和材料，适合基于梯度的优化，并且往往比基于网格的方法产生更少的视觉干扰伪影。早期的体积方法使用观察到的图像直接为体素网格上色。\({}^{19}\) 最近，几种方法\({}^{{12},{25}}\)利用多个场景的大型数据集训练深度网络，从一组输入图像中预测采样的体积表示，然后在测试时使用α合成\({}^{16}\)或沿光线学习合成来渲染新视图。其他工作优化了CNN和每个特定场景的采样体素网格的组合，使得CNN能够补偿低分辨率体素网格\({}^{20}\)的离散化伪影，或允许预测的体素网格根据输入时间或动画控制变化。\({}^{8}\) 尽管这些体积技术在新视图合成方面取得了令人印象深刻的结果，但由于其离散采样的时间和空间复杂性差，导致其在更高分辨率图像上的扩展能力受到根本限制——渲染更高分辨率图像需要更细的\(3\mathrm{D}\)空间采样。我们通过在深度全连接神经网络的参数中编码连续体积来规避这个问题，这不仅产生了比先前体积方法显著更高质量的渲染，而且只需这些采样体积表示的一小部分存储成本。</p></div><h2>3. NEURAL RADIANCE FIELD SCENE REPRESENTATION</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>3. 神经辐射场场景表示</h2></div><p>We represent a continuous scene as a 5D vector-valued function whose input is a \(3\mathrm{D}\) location \(\mathbf{x} = \left( {x,y,z}\right)\) and \(2\mathrm{D}\) viewing direction \(\left( {\theta ,\phi }\right)\) ,and whose output is an emitted color \(\mathbf{c} = (r,g\) , \(b\) ) and volume density \(\sigma\) . In practice,we express direction as a 3D Cartesian unit vector \(\mathbf{d}\) . We approximate this continuous 5D scene representation with an MLP network \({F}_{\Theta } : \left( {\mathbf{x},\mathbf{d}}\right)\) \(\rightarrow  \left( {\mathbf{c},\sigma }\right)\) and optimize its weights \(\Theta\) to map from each input 5D coordinate to its corresponding volume density and directional emitted color.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们将连续场景表示为一个5D向量值函数，其输入为\(3\mathrm{D}\)位置\(\mathbf{x} = \left( {x,y,z}\right)\)和\(2\mathrm{D}\)视角方向\(\left( {\theta ,\phi }\right)\)，输出为发射的颜色\(\mathbf{c} = (r,g\)和体积密度\(\sigma\)。在实践中，我们将方向表示为一个3D笛卡尔单位向量\(\mathbf{d}\)。我们用一个MLP网络\({F}_{\Theta } : \left( {\mathbf{x},\mathbf{d}}\right)\)\(\rightarrow  \left( {\mathbf{c},\sigma }\right)\)来近似这个连续的5D场景表示，并优化其权重\(\Theta\)，以将每个输入的5D坐标映射到其对应的体积密度和方向发射颜色。</p></div><p>We encourage the representation to be multiview consistent by restricting the network to predict the volume density \(\sigma\) as a function of only the location \(\mathbf{x}\) ,while allowing the RGB color \(\mathbf{c}\) to be predicted as a function of both location and viewing direction. To accomplish this,the MLP \({F}_{\Theta }\) first processes the input 3D coordinate \(\mathbf{x}\) with 8 fully connected layers (using ReLU activations and 256 channels per layer), and outputs \(\sigma\) and a 256-dimensional feature vector. This feature vector is then concatenated with the camera ray's viewing direction and passed to one additional fully connected layer (using a ReLU activation and 128 channels) that output the view-dependent RGB color.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们通过限制网络仅将体积密度\(\sigma\)预测为位置\(\mathbf{x}\)的函数，鼓励表示具有多视图一致性，同时允许RGB颜色\(\mathbf{c}\)作为位置和视角方向的函数进行预测。为此，MLP\({F}_{\Theta }\)首先使用8个全连接层（使用ReLU激活和每层256个通道）处理输入的3D坐标\(\mathbf{x}\)，并输出\(\sigma\)和一个256维特征向量。然后将该特征向量与相机光线的视角方向连接，并传递给一个额外的全连接层（使用ReLU激活和128个通道），输出视角依赖的RGB颜色。</p></div><p>See Figure 3 for an example of how our method uses the input viewing direction to represent non-Lambertian effects. As shown in Figure 4, a model trained without view dependence (only \(\mathbf{x}\) as input) has difficulty representing specularities.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>请参见图3，了解我们的方法如何使用输入视角方向来表示非朗伯效应。如图4所示，未考虑视角依赖性（仅\(\mathbf{x}\)作为输入）训练的模型在表示高光方面存在困难。</p></div><!-- Media --><p>Figure 3. A visualization of view-dependent emitted radiance. Our neural radiance field representation outputs RGB color as a 5D function of both spatial position \(x\) and viewing direction \(d\) . Here, we visualize example directional color distributions for two spatial locations in our neural representation of the Ship scene. In (a) and (b), we show the appearance of two fixed 3D points from two different camera positions: one on the side of the ship (orange insets) and one on the surface of the water (blue insets). Our method predicts the changing specular appearance of these two 3D points, and in (c) we show how this behavior generalizes continuously across the whole hemisphere of viewing directions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3. 视角依赖发射辐射的可视化。我们的神经辐射场表示将RGB颜色输出为空间位置\(x\)和视角方向\(d\)的5D函数。在这里，我们可视化了在我们神经表示的船场景中两个空间位置的示例方向颜色分布。在(a)和(b)中，我们展示了从两个不同相机位置观察两个固定3D点的外观：一个在船的侧面（橙色插图），一个在水面上（蓝色插图）。我们的方法预测了这两个3D点的变化高光外观，在(c)中我们展示了这种行为如何在整个视角方向半球中连续推广。</p></div><!-- figureText: (a) View 1 (b) View 2 (c) Radiance Distributions --><img src="https://cdn.noedgeai.com/bo_d164vfn7aajc7388jth0_2.jpg?x=134&#x26;y=1987&#x26;w=722&#x26;h=197&#x26;r=0"><!-- Media --><h2>4. VOLUME RENDERING WITH RADIANCE FIELDS</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>4. 使用辐射场的体积渲染</h2></div><p>Our 5D neural radiance field represents a scene as the volume density and directional emitted radiance at any point in space. We render the color of any ray passing through the scene using principles from classical volume rendering. \({}^{5}\) The volume density \(\sigma \left( \mathbf{x}\right)\) can be interpreted as the differential probability of a ray terminating at an infinitesimal particle at location \(\mathbf{x}\) . The expected color \(C\left( \mathbf{r}\right)\) of camera ray \(\mathbf{r}\left( t\right)  = \mathbf{o} + t\mathbf{d}\) with near and far bounds \({t}_{n}\) and \({t}_{f}\) is:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的5D神经辐射场将场景表示为空间中任意点的体积密度和方向发射辐射。我们使用经典体积渲染的原理渲染穿过场景的任意光线的颜色。\({}^{5}\) 体积密度 \(\sigma \left( \mathbf{x}\right)\) 可以解释为光线在位置 \(\mathbf{x}\) 处终止于一个无穷小粒子的微分概率。相机光线 \(\mathbf{r}\left( t\right)  = \mathbf{o} + t\mathbf{d}\) 的期望颜色 \(C\left( \mathbf{r}\right)\) 在近和远界限 \({t}_{n}\) 和 \({t}_{f}\) 下为：</p></div><p></p>\[C\left( \mathbf{r}\right)  = {\int }_{{t}_{n}}^{{t}_{f}}T\left( t\right) \sigma \left( {\mathbf{r}\left( t\right) }\right) \mathbf{c}\left( {\mathbf{r}\left( t\right) ,\mathbf{d}}\right) {dt}, \tag{1}\]<p></p><p></p>\[\text{where}T\left( t\right)  = \exp \left( {-{\int }_{{t}_{n}}^{t}\sigma \left( {\mathbf{r}\left( s\right) }\right) {ds}}\right) \text{.} \tag{2}\]<p></p><p>The function \(T\left( t\right)\) denotes the accumulated transmittance along the ray from \({t}_{n}\) to \(t\) ,that is,the probability that the ray travels from \({t}_{n}\) to \(t\) without hitting any other particle. Rendering a view from our continuous neural radiance field requires estimating this integral \(C\left( \mathbf{r}\right)\) for a camera ray traced through each pixel of the desired virtual camera.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>函数 \(T\left( t\right)\) 表示沿光线从 \({t}_{n}\) 到 \(t\) 的累积透射率，即光线从 \({t}_{n}\) 到 \(t\) 而不碰撞任何其他粒子的概率。从我们的连续神经辐射场渲染视图需要估计这个积分 \(C\left( \mathbf{r}\right)\)，对于每个所需虚拟相机的像素追踪的相机光线。</p></div><p>We numerically estimate this continuous integral using quadrature. Deterministic quadrature, which is typically used for rendering discretized voxel grids, would effectively limit our representation's resolution because the MLP would only be queried at a fixed discrete set of locations. Instead, we use a stratified sampling approach where we partition \(\left\lbrack  {{t}_{n},{t}_{f}}\right\rbrack\) into \(N\) evenly spaced bins and then draw one sample uniformly at random from within each bin:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们使用数值方法通过求积来估计这个连续积分。确定性求积通常用于渲染离散的体素网格，这将有效限制我们表示的分辨率，因为MLP只会在固定的离散位置集上被查询。相反，我们使用分层采样方法，将 \(\left\lbrack  {{t}_{n},{t}_{f}}\right\rbrack\) 划分为 \(N\) 个均匀间隔的区间，然后从每个区间内均匀随机抽取一个样本：</p></div><p></p>\[{t}_{i} \sim  u\left\lbrack  {{t}_{n} + \frac{i - 1}{N}\left( {{t}_{f} - {t}_{n}}\right) ,{t}_{n} + \frac{i}{N}\left( {{t}_{f} - {t}_{n}}\right) }\right\rbrack  . \tag{3}\]<p></p><p>Although we use a discrete set of samples to estimate the integral, stratified sampling enables us to represent a continuous scene representation because it results in the MLP being evaluated at continuous positions over the course of optimization. We use these samples to estimate \(C\left( \mathbf{r}\right)\) with the quadrature rule discussed in the volume rendering review by Max \({}^{10}\) :</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>尽管我们使用离散样本集来估计积分，分层采样使我们能够表示连续场景表示，因为它导致在优化过程中MLP在连续位置上被评估。我们使用这些样本通过Max \({}^{10}\) 在体积渲染回顾中讨论的求积规则来估计 \(C\left( \mathbf{r}\right)\)：</p></div><p></p>\[\widehat{C}\left( \mathbf{r}\right)  = \mathop{\sum }\limits_{{i = 1}}^{N}{T}_{i}\left( {1 - \exp \left( {-{\sigma }_{i}{\delta }_{i}}\right) }\right) {\mathbf{c}}_{i}, \tag{4}\]<p></p><p></p>\[{T}_{i} = \exp \left( {-\mathop{\sum }\limits_{{j = 1}}^{{i - 1}}{\sigma }_{j}{\delta }_{j}}\right)  \tag{5}\]<p></p><p>where \({\delta }_{i} = {t}_{i + 1} - {t}_{i}\) is the distance between adjacent samples.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \({\delta }_{i} = {t}_{i + 1} - {t}_{i}\) 是相邻样本之间的距离。</p></div><!-- Media --><p>Figure 4. Here we visualize how our full model benefits from representing view-dependent emitted radiance and from passing our input coordinates through a high-frequency positional encoding. Removing view dependence prevents the model from recreating the specular reflection on the bulldozer tread. Removing the positional encoding drastically decreases the model's ability to represent high-frequency geometry and texture, resulting in an oversmoothed appearance.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图4. 在这里，我们可视化了我们的完整模型如何受益于表示视图依赖的发射辐射以及将输入坐标通过高频位置编码。去除视图依赖性防止模型重现推土机履带上的镜面反射。去除位置编码会大幅降低模型表示高频几何和纹理的能力，导致过于平滑的外观。</p></div><!-- figureText: Ground Truth Full Model No View Dep No Pos. Enc. --><img src="https://cdn.noedgeai.com/bo_d164vfn7aajc7388jth0_2.jpg?x=909&#x26;y=1951&#x26;w=722&#x26;h=219&#x26;r=0"><!-- Media --><p>This function for calculating \(\widehat{C}\left( \mathbf{r}\right)\) from the set of \(\left( {{\mathbf{c}}_{i},{\sigma }_{i}}\right)\) values is trivially differentiable and reduces to traditional alpha compositing with alpha values \({\sigma }_{i} = 1 - \exp \left( {-{\sigma }_{i}{\delta }_{i}}\right)\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这个从一组 \(\left( {{\mathbf{c}}_{i},{\sigma }_{i}}\right)\) 值计算 \(\widehat{C}\left( \mathbf{r}\right)\) 的函数是显然可微的，并简化为传统的α合成，使用α值 \({\sigma }_{i} = 1 - \exp \left( {-{\sigma }_{i}{\delta }_{i}}\right)\)。</p></div><h2>5. OPTIMIZING A NEURAL RADIANCE FIELD</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>5. 优化神经辐射场</h2></div><p>In the previous section, we have described the core components necessary for modeling a scene as a neural radiance field and rendering novel views from this representation. However, we observe that these components are not sufficient for achieving state-of-the-art quality. We introduce two improvements to enable representing high-resolution complex scenes. The first is a positional encoding of the input coordinates that assists the MLP in representing high-frequency functions. The second is a hierarchical sampling procedure that we do not describe here; for details, see the original paper. \({}^{13}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在上一节中，我们描述了建模场景作为神经辐射场和从该表示渲染新视图所需的核心组件。然而，我们观察到这些组件不足以实现最先进的质量。我们引入了两项改进，以便表示高分辨率复杂场景。第一项是输入坐标的位置信息编码，帮助MLP表示高频函数。第二项是分层采样程序，我们在这里不做描述；有关详细信息，请参见原始论文。\({}^{13}\)</p></div><h3>5.1. Positional encoding</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.1. 位置编码</h3></div><p>Despite the fact that neural networks are universal function approximators, we found that having the network \({F}_{\Theta }\) directly operate on \({xyz\theta \phi }\) input coordinates results in renderings that perform poorly at representing high-frequency variation in color and geometry. This is consistent with recent work by Rahaman et al., \({}^{17}\) which shows that deep networks are biased toward learning lower frequency functions. They additionally show that mapping the inputs to a higher dimensional space using high-frequency functions before passing them to the network enables better fitting of data that contains high-frequency variation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>尽管神经网络是通用函数逼近器，但我们发现让网络 \({F}_{\Theta }\) 直接操作 \({xyz\theta \phi }\) 输入坐标会导致渲染在表示颜色和几何的高频变化方面表现不佳。这与Rahaman等人的最新研究一致，\({}^{17}\) 显示深度网络倾向于学习低频函数。他们还表明，在将输入传递给网络之前，使用高频函数将输入映射到更高维空间可以更好地拟合包含高频变化的数据。</p></div><p>We leverage these findings in the context of neural scene representations,and show that reformulating \({F}_{\Theta }\) as a composition of two functions \({F}_{ \ominus  } = {F}_{ \ominus  }^{\prime } \circ  \gamma\) ,one learned and one not, significantly improves performance (see Figure 4). Here \(\gamma\) is a mapping from \(\mathbb{R}\) into a higher dimensional space \({\mathbb{R}}^{2L}\) , and \({F}_{\Theta }^{\prime }\) is still simply a regular MLP. Formally,the encoding function we use is:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们在神经场景表示的背景下利用这些发现，并展示将\({F}_{\Theta }\)重新表述为两个函数\({F}_{ \ominus  } = {F}_{ \ominus  }^{\prime } \circ  \gamma\)的组合，其中一个是学习的，另一个不是，显著提高了性能（见图4）。这里\(\gamma\)是从\(\mathbb{R}\)映射到更高维空间\({\mathbb{R}}^{2L}\)，而\({F}_{\Theta }^{\prime }\)仍然只是一个常规的多层感知器（MLP）。正式地，我们使用的编码函数是：</p></div><p></p>\[\gamma \left( p\right)  = \left( {\sin \left( {{2}^{0}{\pi p}}\right) ,\cos \left( {{2}^{0}{\pi p}}\right) ,\cdots ,\sin \left( {{2}^{L - 1}{\pi p}}\right) ,\cos \left( {{2}^{L - 1}{\pi p}}\right) }\right)  \tag{6}\]<p></p><p>This function \(\gamma \left( \cdot \right)\) is applied separately to each of the three coordinate values in \(\mathbf{x}\) (which are normalized to lie in \(\left\lbrack  {-1,1}\right\rbrack  )\) and to the three components of the Cartesian viewing direction unit vector \(\mathbf{d}\) (which by construction lie in \(\left\lbrack  {-1,1}\right\rbrack\) ). In our experiments,we set \(L = {10}\) for \(\gamma \left( \mathbf{X}\right)\) and \(L = 4\) for \(\gamma \left( \mathbf{d}\right)\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这个函数\(\gamma \left( \cdot \right)\)分别应用于\(\mathbf{x}\)中的三个坐标值（这些值被归一化以位于\(\left\lbrack  {-1,1}\right\rbrack  )\)，以及笛卡尔视角单位向量\(\mathbf{d}\)的三个分量（这些分量通过构造位于\(\left\lbrack  {-1,1}\right\rbrack\)）。在我们的实验中，我们为\(\gamma \left( \mathbf{X}\right)\)设置\(L = {10}\)，为\(\gamma \left( \mathbf{d}\right)\)设置\(L = 4\)。</p></div><p>This mapping is studied in more depth in subsequent work \({}^{22}\) which shows how positional encoding enables a network to more rapidly represent higher frequency signals.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这个映射在后续工作\({}^{22}\)中进行了更深入的研究，展示了位置编码如何使网络更快速地表示高频信号。</p></div><h3>5.2. Implementation details</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.2. 实现细节</h3></div><p>We optimize a separate neural continuous volume representation network for each scene. This requires only a dataset of captured RGB images of the scene, the corresponding camera poses and intrinsic parameters, and scene bounds (we use ground truth camera poses, intrinsics, and bounds for synthetic data, and use the COLMAP structure-from-motion package \({}^{18}\) to estimate these parameters for real data). At each optimization iteration, we randomly sample a batch of camera rays from the set of all pixels in the dataset. We query the network at \(N\) random points along each ray and then use the volume rendering procedure described in Section 4 to render the color of each ray using these samples. Our loss is simply the total squared error between the rendered and true pixel colors:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们为每个场景优化一个单独的神经连续体积表示网络。这只需要一个捕获场景的RGB图像数据集、相应的相机姿态和内参，以及场景边界（我们对合成数据使用真实的相机姿态、内参和边界，并使用COLMAP运动重建包\({}^{18}\)来估计真实数据的这些参数）。在每次优化迭代中，我们从数据集中所有像素的集合中随机抽取一批相机光线。我们在每条光线的\(N\)个随机点上查询网络，然后使用第4节中描述的体积渲染过程，利用这些样本渲染每条光线的颜色。我们的损失仅仅是渲染的像素颜色与真实像素颜色之间的总平方误差：</p></div><p></p>\[\mathcal{L} = \mathop{\sum }\limits_{{\mathbf{r} \in  \mathcal{R}}}\parallel \widehat{C}\left( \mathbf{r}\right)  - C\left( \mathbf{r}\right) {\parallel }_{2}^{2} \tag{7}\]<p></p><p>where \(R\) is the set of rays in each batch,and \(C\left( \mathbf{r}\right) ,\widehat{C}\left( \mathbf{r}\right)\) are the ground truth and predicted RGB colors for ray \(\mathbf{r}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(R\)是每批中的光线集合，\(C\left( \mathbf{r}\right) ,\widehat{C}\left( \mathbf{r}\right)\)是光线\(\mathbf{r}\)的真实和预测RGB颜色。</p></div><p>In our experiments, we use a batch size of 4096 rays, each sampled at \(N = {192}\) coordinates. (These are divided between two hierarchical "coarse" and "fine" networks; for details see the original paper. \({}^{13}\) ) We use the Adam optimizer \({}^{6}\) with a learning rate that begins at \(5 \times  {10}^{-4}\) and decays exponentially to \(5 \times  {10}^{-5}\) . The optimization for a single scene typically takes about 1-2 days to converge on a single GPU.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在我们的实验中，我们使用4096条光线的批量大小，每条光线在\(N = {192}\)坐标上进行采样。（这些被分为两个层次的“粗略”和“精细”网络；有关详细信息，请参见原始论文。\({}^{13}\)）我们使用Adam优化器\({}^{6}\)，学习率从\(5 \times  {10}^{-4}\)开始，并以指数方式衰减到\(5 \times  {10}^{-5}\)。单个场景的优化通常需要大约1-2天在单个GPU上收敛。</p></div><h2>6. RESULTS</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>6. 结果</h2></div><p>We quantitatively (Table 1) and qualitatively (see Figures 5 and 6) show that our method outperforms prior work. We urge the reader to view our accompanying video to better appreciate our method's significant improvement over baseline methods when rendering smooth paths of novel views. Videos, code, and datasets can be found at <a href="https://www.matthew">https://www.matthew</a>.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们在定量（表1）和定性（见图5和6）上展示了我们的方法优于先前的工作。我们敦促读者观看我们附带的视频，以更好地欣赏我们的方法在渲染新视角的平滑路径时相较于基线方法的显著改进。视频、代码和数据集可以在<a href="https://www.matthew%E6%89%BE%E5%88%B0%E3%80%82">https://www.matthew找到。</a></p></div><h3>6.1. Datasets</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.1. 数据集</h3></div><p>Synthetic renderings of objects. We first show experimental results on two datasets of synthetic renderings of objects (Table 1,"Diffuse Synthetic \({360}^{ \circ  }\) " and "Realistic Synthetic \({360}^{ \circ  }\) "). The DeepVoxels \({}^{20}\) dataset contains four Lambertian</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>物体的合成渲染。我们首先展示了两个合成物体渲染数据集的实验结果（表1，“漫反射合成\({360}^{ \circ  }\)”和“真实感合成\({360}^{ \circ  }\)”）。DeepVoxels\({}^{20}\)数据集包含四个朗伯（Lambertian）</p></div><!-- Media --><p>Table 1. Our method quantitatively outperforms prior work on datasets of both synthetic and real images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表1. 我们的方法在合成和真实图像数据集上定量优于先前的工作。</p></div><table><tbody><tr><td rowspan="2">Method</td><td colspan="3">Diffuse Synthetic 360°20</td><td colspan="3">Realistic Synthetic 360°</td><td colspan="3">Real ForwardFacing \( {}^{12} \)</td></tr><tr><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS↓</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS↓</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS↓</td></tr><tr><td>SRN21</td><td>33.20</td><td>0.963</td><td>0.073</td><td>22.26</td><td>0.846</td><td>0.170</td><td>22.84</td><td>0.668</td><td>0.378</td></tr><tr><td>\( {\mathrm{{NV}}}^{8} \)</td><td>29.62</td><td>0.929</td><td>0.099</td><td>26.05</td><td>0.893</td><td>0.160</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LLFF12</td><td>34.38</td><td>0.985</td><td>0.048</td><td>24.88</td><td>0.911</td><td>0.114</td><td>24.13</td><td>0.798</td><td>0.212</td></tr><tr><td>Ours</td><td>40.15</td><td>0.991</td><td>0.023</td><td>31.01</td><td>0.947</td><td>0.081</td><td>26.50</td><td>0.811</td><td>0.250</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">方法</td><td colspan="3">扩散合成 360°20</td><td colspan="3">真实合成 360°</td><td colspan="3">真实前视 \( {}^{12} \)</td></tr><tr><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS↓</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS↓</td><td>PSNR↑</td><td>SSIM↑</td><td>LPIPS↓</td></tr><tr><td>SRN21</td><td>33.20</td><td>0.963</td><td>0.073</td><td>22.26</td><td>0.846</td><td>0.170</td><td>22.84</td><td>0.668</td><td>0.378</td></tr><tr><td>\( {\mathrm{{NV}}}^{8} \)</td><td>29.62</td><td>0.929</td><td>0.099</td><td>26.05</td><td>0.893</td><td>0.160</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LLFF12</td><td>34.38</td><td>0.985</td><td>0.048</td><td>24.88</td><td>0.911</td><td>0.114</td><td>24.13</td><td>0.798</td><td>0.212</td></tr><tr><td>我们的</td><td>40.15</td><td>0.991</td><td>0.023</td><td>31.01</td><td>0.947</td><td>0.081</td><td>26.50</td><td>0.811</td><td>0.250</td></tr></tbody></table></div><p>We report PSNR/SSIM (higher is better) and LPIPS \({}^{24}\) (lower is better). The DeepVoxels \({}^{20}\) dataset consists of 4 diffuse objects with simple geometry. Our realistic synthetic dataset consists of pathtraced renderings of 8 geometrically complex objects with complex non-Lambertian materials. The real dataset consists of handheld forward-facing captures of 8 real-world scenes (NV cannot be evaluated on this data because it only reconstructs objects inside a bounded volume). Bold values denote the top-performing algorithm for each of these metrics.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们报告PSNR/SSIM（越高越好）和LPIPS \({}^{24}\)（越低越好）。DeepVoxels \({}^{20}\) 数据集由4个具有简单几何形状的漫反射物体组成。我们的真实合成数据集由8个几何复杂的物体的路径追踪渲染图像组成，这些物体具有复杂的非朗伯材料。真实数据集由8个真实场景的手持前视捕捉图像组成（NV无法在此数据上进行评估，因为它仅重建位于有限体积内的物体）。粗体值表示每个指标的最佳算法。</p></div><!-- Media --><p>Figure 5. Comparisons on test-set views for scenes from our new synthetic dataset generated with a physically based renderer. Our method is able to recover fine details in both geometry and appearance, such as Ship’s rigging, Lego’s gear and treads, Microphone’s shiny stand and mesh grille, and Material’s non-Lambertian reflectance. LLFF exhibits banding artifacts on the Microphone stand and Material’s object edges and ghosting artifacts in Ship’s mast and inside the Lego object. SRN produces blurry and distorted renderings in every case. Neural Volumes cannot capture the details on the Microphone’s grille or Lego’s gears, and it completely fails to recover the geometry of Ship’s rigging. objects with simple geometry. Each object is rendered at \({512} \times  {512}\) pixels from viewpoints sampled on the upper hemisphere (479 as input and 1000 for testing). We additionally generate our own dataset containing pathtraced images of eight objects that exhibit complicated geometry and realistic non-Lambertian materials. Six are rendered from viewpoints sampled on the upper hemisphere, and two are rendered from viewpoints sampled on a full sphere. We render 100 views of each scene as input and 200 for testing, all at \({800} \times  {800}\) pixels.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图5. 我们的新合成数据集生成的测试集视图的比较，使用物理基础渲染器。我们的方法能够恢复几何和外观中的细节，例如船的索具、乐高的齿轮和履带、麦克风的光滑支架和网格格栅，以及材料的非朗伯反射。LLFF在麦克风支架和材料物体边缘上表现出条带伪影，并在船的桅杆和乐高物体内部出现重影伪影。SRN在每种情况下都产生模糊和失真的渲染。神经体积无法捕捉麦克风格栅或乐高齿轮的细节，并且完全无法恢复船的索具的几何形状。每个物体从上半球采样的视点渲染为\({512} \times  {512}\)像素（479作为输入，1000用于测试）。我们还生成了自己的数据集，包含八个具有复杂几何形状和真实非朗伯材料的物体的路径追踪图像。六个物体从上半球采样的视点渲染，两个物体从全球采样的视点渲染。我们为每个场景渲染100个视图作为输入，200个用于测试，所有图像均为\({800} \times  {800}\)像素。</p></div><!-- Media --><!-- figureText: Ship LLFF [12] SRN [21] NV [8] Lego Microphone Materials Ground Truth NeRF (ours) --><img src="https://cdn.noedgeai.com/bo_d164vfn7aajc7388jth0_4.jpg?x=150&#x26;y=392&#x26;w=1463&#x26;h=1781&#x26;r=0"><p>Figure 6. Comparisons on test-set views of real-world scenes. LLFF is specifically designed for this use case (forward-facing captures of real scenes). Our method is able to represent fine geometry more consistently across rendered views than LLFF, as shown in Fern’s leaves and the skeleton ribs and railing in T-rex. Our method also correctly reconstructs partially occluded regions that LLFF struggles to render cleanly, such as the yellow shelves behind the leaves in the bottom Fern crop and green leaves in the background of the bottom Orchid crop. Blending between multiples renderings can also cause repeated edges in LLFF, as seen in the top Orchid crop. SRN captures the low-frequency geometry and color variation in each scene but is unable to reproduce any fine detail.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6. 真实场景的测试集视图比较。LLFF专门为此用例设计（真实场景的前视捕捉）。我们的方法能够在渲染视图中更一致地表示细致的几何形状，如蕨类植物的叶子和T-rex的骨架肋骨和栏杆。我们的方法还正确重建了LLFF难以干净渲染的部分遮挡区域，例如底部蕨类植物作物中叶子后面的黄色架子和底部兰花作物背景中的绿色叶子。多个渲染之间的混合也可能导致LLFF中出现重复边缘，如顶部兰花作物所示。SRN捕捉到每个场景中的低频几何形状和颜色变化，但无法重现任何细节。</p></div><!-- figureText: Fern NeRF (ours) LLFF [12] SRN [21] T-Rex Orchid Ground Truth --><img src="https://cdn.noedgeai.com/bo_d164vfn7aajc7388jth0_5.jpg?x=129&#x26;y=384&#x26;w=1446&#x26;h=1522&#x26;r=0"><!-- Media --><p>Real images of complex scenes. We show results on complex real-world scenes captured with roughly forward-facing images (Table 1, "Real ForwardFacing"). This dataset consists of eight scenes captured with a handheld cellphone (five taken from the local light field fusion (LLFF) paper and three that we capture), captured with 20 to 62 images, and hold out \(1/8\) of these for the test set. All images are \({1008} \times  {756}\) pixels.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>复杂场景的真实图像。我们展示了在复杂真实场景上获得的结果，这些场景是通过大致前视图像捕捉的（表1，“真实前视”）。该数据集由八个场景组成，这些场景是用手持手机捕捉的（五个来自本地光场融合（LLFF）论文，三个由我们捕捉），使用20到62张图像捕捉，并将\(1/8\)张图像保留用于测试集。所有图像均为\({1008} \times  {756}\)像素。</p></div><h3>6.2. Comparisons</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.2. 比较</h3></div><p>To evaluate our model we compare against current top-performing techniques for view synthesis, detailed here. All methods use the same set of input views to train a separate network for each scene except LLFF, \({}^{12}\) which trains a single 3D CNN on a large dataset, then uses the same trained network to process input images of new scenes at test time.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了评估我们的模型，我们与当前顶尖的视图合成技术进行比较，详细信息见此处。除LLFF外，所有方法使用相同的输入视图集为每个场景训练一个单独的网络，\({}^{12}\)而LLFF则在大型数据集上训练一个单一的3D CNN，然后在测试时使用相同的训练网络处理新场景的输入图像。</p></div><p>Neural Volumes (NV) \({}^{8}\) synthesizes novel views of objects that lie entirely within a bounded volume in front of a distinct background (which must be separately captured without the object of interest). It optimizes a deep 3D CNN to predict a discretized RGB \(\alpha\) voxel grid with \({128}^{3}\) samples as well as a \(3\mathrm{D}\) warp grid with \({32}^{3}\) samples. The algorithm renders novel views by marching camera rays through the warped voxel grid.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>神经体积（NV）\({}^{8}\)合成完全位于前景中有限体积内的物体的新视图，背景必须单独捕捉且不包含感兴趣的物体。它优化一个深度3D CNN，以预测一个离散的RGB \(\alpha\)体素网格，具有\({128}^{3}\)个样本，以及一个\(3\mathrm{D}\)变形网格，具有\({32}^{3}\)个样本。该算法通过在变形体素网格中行进相机光线来渲染新视图。</p></div><p>Scene Representation Networks (SRN) \({}^{21}\) represent a continuous scene as an opaque surface, implicitly defined by an MLP that maps each(x,y,z)coordinate to a feature vector. They train a recurrent neural network to march along a ray through the scene representation by using the feature vector at any 3D coordinate to predict the next step size along the ray. The feature vector from the final step is decoded into a single color for that point on the surface. Note that SRN is a better-performing follow-up to DeepVoxels \({}^{20}\) by the same authors, which is why we do not include comparisons to DeepVoxels.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>场景表示网络（SRN）\({}^{21}\)将连续场景表示为不透明表面，隐式地由一个MLP定义，该MLP将每个(x,y,z)坐标映射到一个特征向量。它们训练一个递归神经网络，通过使用任何3D坐标的特征向量来预测沿光线的下一步大小，从而沿着场景表示的光线行进。最后一步的特征向量被解码为该表面上该点的单一颜色。请注意，SRN是同一作者对DeepVoxels \({}^{20}\)的更好后续版本，因此我们不包括与DeepVoxels的比较。</p></div><p>\({\mathrm{{LLFF}}}^{12}\) is designed for producing photorealistic novel views for well-sampled forward-facing scenes. It uses a trained 3D CNN to directly predict a discretized frustum-sampled \(\mathrm{{RGB}}\alpha\) grid (multiplane image or \({\mathrm{{MPI}}}^{25}\) ) for each input view, then renders novel views by alpha compositing and blending nearby MPIs into the novel viewpoint.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({\mathrm{{LLFF}}}^{12}\)旨在为采样良好的正面场景生成逼真的新视图。它使用训练好的3D卷积神经网络直接预测每个输入视图的离散化锥体采样\(\mathrm{{RGB}}\alpha\)网格（多平面图像或\({\mathrm{{MPI}}}^{25}\)），然后通过alpha合成和将附近的MPI混合到新视点中来渲染新视图。</p></div><h3>6.3. Discussion</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.3. 讨论</h3></div><p>We thoroughly outperform both baselines that also optimize a separate network per scene (NV and SRN) in all scenarios. Furthermore, we produce qualitatively and quantitatively superior renderings compared to LLFF (across all except one metric) while using only their input images as our entire training set.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在所有场景中，我们的表现远超同时为每个场景优化单独网络的两个基线（NV和SRN）。此外，与LLFF相比，我们在质量和数量上都产生了更优的渲染（在所有指标中只有一个例外），而只使用他们的输入图像作为我们的整个训练集。</p></div><p>The SRN method produces heavily smoothed geometry and texture, and its representational power for view synthesis is limited by selecting only a single depth and color per camera ray. The NV baseline is able to capture reasonably detailed volumetric geometry and appearance, but its use of an underlying explicit \({128}^{3}\) voxel grid prevents it from scaling to represent fine details at high resolutions. LLFF specifically provides a "sampling guideline" to not exceed 64 pixels of disparity between input views, so it frequently fails to estimate correct geometry in the synthetic datasets which contain up to 400-500 pixels of disparity between</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>SRN方法产生了严重平滑的几何形状和纹理，其视图合成的表现能力受到每个相机光线仅选择单一深度和颜色的限制。NV基线能够捕捉到相当详细的体积几何形状和外观，但其使用的基础显式\({128}^{3}\)体素网格使其无法扩展以表示高分辨率下的细节。LLFF特别提供了一个“采样指南”，以确保输入视图之间的视差不超过64像素，因此它在合成数据集中经常无法估计正确的几何形状，这些数据集的视差可达到400-500像素。</p></div><h2>References</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>参考文献</h2></div><ol>
<li>Buehler, C., Bosse, M., McMillan, L., Gortler, S., Cohen, M. Unstructured lumigraph rendering. In SIGGRAPH (2001).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol>
<li>Buehler, C., Bosse, M., McMillan, L., Gortler, S., Cohen, M. 无结构光场渲染. 在SIGGRAPH (2001).</li>
</ol></div><ol start="2">
<li>Chang, A.X., Fhnkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., et al. ShapeNet: An information-rich 3D model repository. arXiv:1512.03012 (2015).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="2">
<li>Chang, A.X., Fhnkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., 等. ShapeNet: 一个信息丰富的3D模型库. arXiv:1512.03012 (2015).</li>
</ol></div><ol start="3">
<li>Curless, B., Levoy, M. A volumetric method for building complex models from range images. In SIGGRAPH (1996).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="3">
<li>Curless, B., Levoy, M. 一种从范围图像构建复杂模型的体积方法. 在SIGGRAPH (1996).</li>
</ol></div><ol start="4">
<li>Debevec, P., Taylor, C.J., Malik,J.</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="4">
<li>Debevec, P., Taylor, C.J., Malik,J.</li>
</ol></div><p>Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach. In SIGGRAPH (1996).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>从照片建模和渲染建筑: 一种混合几何和基于图像的方法. 在SIGGRAPH (1996).</p></div><ol start="5">
<li>Kajiya, J.T., Herzen, B.P.V. Ray tracing volume densities. Comput. Graph. (SIGGRAPH) (1984).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="5">
<li>Kajiya, J.T., Herzen, B.P.V. 光线追踪体积密度. 计算机图形学 (SIGGRAPH) (1984).</li>
</ol></div><ol start="6">
<li>Kingma, D.P., Ba, J. Adam: A method for stochastic optimization. In ICLR (2015).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="6">
<li>Kingma, D.P., Ba, J. Adam: 一种随机优化方法. 在ICLR (2015).</li>
</ol></div><ol start="7">
<li>Li, T.-M., Aittala, M., Durand, F., Lehtinen, J. Differentiable monte carlo ray tracing through edge sampling. ACM Trans. Graph. (SIGGRAPH Asia) (2018). views. Additionally, LLFF blends between different scene representations for rendering different views, resulting in perceptually distracting inconsistency as is apparent in our supplementary video.</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="7">
<li>Li, T.-M., Aittala, M., Durand, F., Lehtinen, J. 通过边缘采样的可微分蒙特卡罗光线追踪. ACM图形学会 (SIGGRAPH Asia) (2018). 视图。此外，LLFF在不同场景表示之间进行混合以渲染不同视图，导致感知上令人分心的不一致性，这在我们的补充视频中显而易见。</li>
</ol></div><p>The biggest practical trade-offs between these methods are time versus space. All compared single scene methods take at least 12 hours to train per scene. In contrast, LLFF can process a small input dataset in under \({10}\mathrm{\;{min}}\) . However, LLFF produces a large 3D voxel grid for every input image, resulting in enormous storage requirements (over 15GB for one "Realistic Synthetic" scene). Our method requires only \(5\mathrm{{MB}}\) for the network weights (a relative compression of 3000 \(\times\) compared to LLFF),which is even less memory than the input images alone for a single scene from any of our datasets.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这些方法之间最大的实际权衡是时间与空间。所有比较的单场景方法每个场景至少需要12小时进行训练。相比之下，LLFF可以在不到\({10}\mathrm{\;{min}}\)的时间内处理小的输入数据集。然而，LLFF为每个输入图像生成一个大型3D体素网格，导致巨大的存储需求（一个“真实合成”场景超过15GB）。我们的方法仅需要\(5\mathrm{{MB}}\)用于网络权重（相对于LLFF的相对压缩为3000\(\times\)），这甚至比我们任何数据集中单个场景的输入图像所需的内存还要少。</p></div><h2>7. CONCLUSION</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>7. 结论</h2></div><p>Our work directly addresses deficiencies of prior work that uses MLPs to represent objects and scenes as continuous functions. We demonstrate that representing scenes as \(5\mathrm{D}\) neural radiance fields (an MLP that outputs volume density and view-dependent emitted radiance as a function of \(3\mathrm{D}\) location and 2D viewing direction) produces better renderings than the previously dominant approach of training deep CNNs to output discretized voxel representations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的工作直接解决了之前使用多层感知器（MLPs）将物体和场景表示为连续函数的不足之处。我们证明了将场景表示为\(5\mathrm{D}\)神经辐射场（一个输出体积密度和视角依赖发射辐射的多层感知器，作为\(3\mathrm{D}\)位置和2D视角方向的函数）比之前主导的方法（训练深度卷积神经网络以输出离散化体素表示）产生更好的渲染。</p></div><p>We believe that this work makes progress toward a graphics pipeline based on real-world imagery, where complex scenes could be composed of neural radiance fields optimized from images of actual objects and scenes. Indeed, many recent methods have already built upon the neural radiance field representation presented in this work and extended it to enable more functionality such as relighting, deformations, and animation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们相信这项工作在基于真实世界图像的图形管线方面取得了进展，其中复杂场景可以由从实际物体和场景的图像中优化的神经辐射场组成。实际上，许多最近的方法已经基于本工作中提出的神经辐射场表示，并扩展了其功能，例如重新照明、变形和动画。</p></div><h2>Acknowledgments</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>致谢</h2></div><p>We thank Kevin Cao, Guowei Frank Yang, and Nithin Raghavan for comments and discussions. RR acknowledges funding from ONR grants N000141712687, N000141912293 N000142012529, NSF Chase-CI and the Ronald L. Graham Chair. BM is funded by a Hertz Foundation Fellowship, and MT is funded by an NSF Graduate Fellowship. Google provided a generous donation of cloud compute credits through the BAIR Commons program. We thank the following Blend Swap users for the models used in our realistic synthetic dataset: gregzaal (ship), 1DInc (chair), bryanajones (drums), Herberhold (ficus), erickfree (hotdog), Heinzelnisse (lego), elbrujodelatribu (materials), and up3d.de (mic). ☒</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们感谢Kevin Cao、Guowei Frank Yang和Nithin Raghavan的评论和讨论。RR感谢ONR资助的N000141712687、N000141912293、N000142012529项目，NSF Chase-CI和Ronald L. Graham Chair的资助。BM由Hertz基金会奖学金资助，MT由NSF研究生奖学金资助。谷歌通过BAIR Commons项目提供了慷慨的云计算信用捐赠。我们感谢以下Blend Swap用户为我们的真实合成数据集提供的模型：gregzaal（船）、1DInc（椅子）、bryanajones（鼓）、Herberhold（榕树）、erickfree（热狗）、Heinzelnisse（乐高）、elbrujodelatribu（材料）和up3d.de（麦克风）。☒</p></div><p>research highlights</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>研究亮点</p></div><ol start="8">
<li>Lombardi, S., Simon, T., Saragih, J., Schwartz, G., Lehrmann, A., Sheikh, Y. Neural volumes: Learning dynamic renderable volumes from images. ACM</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="8">
<li>Lombardi, S., Simon, T., Saragih, J., Schwartz, G., Lehrmann, A., Sheikh, Y. 神经体积：从图像中学习动态可渲染体积。ACM</li>
</ol></div><ol start="9">
<li>Loper, M.M., Black, M.J. OpenDR: An volume rendering. IEEE Trans. Visual.</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="9">
<li>Loper, M.M., Black, M.J. OpenDR：一种体积渲染。IEEE Trans. Visual.</li>
</ol></div><ol start="11">
<li>Mescheder, L., Oechsle, M., Occupancy networks: Learning 3D In \({CVPR}\) (2019).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="11">
<li>Mescheder, L., Oechsle, M., 占用网络：学习3D In \({CVPR}\) (2019).</li>
</ol></div><ol start="12">
<li>Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi, R., Ng, R., Kar, A. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Trans. Graph. (SIGGRAPH) (2019).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="12">
<li>Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi, R., Ng, R., Kar, A. 局部光场融合：具有规范采样指南的实用视图合成。ACM Trans. Graph. (SIGGRAPH) (2019).</li>
</ol></div><ol start="13">
<li>Mildenhall, B., Srinivasan, P.P, Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R. NeRF: Representing scenes as neural radiance fields for view synthesis. In \({ECCV}\) (2020).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="13">
<li>Mildenhall, B., Srinivasan, P.P, Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R. NeRF：将场景表示为神经辐射场以进行视图合成。在 \({ECCV}\) (2020).</li>
</ol></div><ol start="14">
<li>Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A. Differentiable volumetric rendering: Learning implicit 3D representations without 3D supervision. In \({CVPR}\) (2019).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="14">
<li>Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A. 可微分体积渲染：在没有3D监督的情况下学习隐式3D表示。在 \({CVPR}\) (2019).</li>
</ol></div><ol start="15">
<li>Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S. DeepSDF: Learning continuous signed distance functions for shape representation. In \({CVPR}\) (2019).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="15">
<li>Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S. DeepSDF：学习用于形状表示的连续有符号距离函数。在 \({CVPR}\) (2019).</li>
</ol></div><ol start="16">
<li>Porter, T., Duff, T. Compositing digital images. Comput. Graph. (SIGGRAPH) (1984).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="16">
<li>Porter, T., Duff, T. 数字图像合成。计算机图形学 (SIGGRAPH) (1984).</li>
</ol></div><ol start="17">
<li>Rahaman, N., Baratin, A., Arpit, D., Dräxler, F., Lin, M., Hamprecht, F.A., Bengio, Y., Courville, A.C. On the spectral bias of neural networks. In</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="17">
<li>Rahaman, N., Baratin, A., Arpit, D., Dräxler, F., Lin, M., Hamprecht, F.A., Bengio, Y., Courville, A.C. 关于神经网络的光谱偏差。在</li>
</ol></div><ol start="18">
<li>Schönberger, J.L., Frahm, J.-M.</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="18">
<li>Schönberger, J.L., Frahm, J.-M.</li>
</ol></div><ol start="19">
<li>Seitz, S.M., Dyer, C.R. Photorealistic scene reconstruction by voxel Deepvoxels: Learning persistent 3D feature embeddings. In CVPR (2019).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="19">
<li>Seitz, S.M., Dyer, C.R. 通过体素进行照片真实场景重建 Deepvoxels：学习持久的3D特征嵌入。在CVPR (2019).</li>
</ol></div><ol start="21">
<li>Sitzmann, V., Zollhoefer, M., Wetzstein, G. Scene representation networks: Continuous 3D-structure-aware neural scene representations. In NeurIPS (2019).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="21">
<li>Sitzmann, V., Zollhoefer, M., Wetzstein, G. 场景表示网络：连续的3D结构感知神经场景表示。在NeurIPS (2019).</li>
</ol></div><ol start="22">
<li>Tancik, M., Srinivasan, P.P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J.T., Ng, R. Fourier features let networks learn high frequency functions in low dimensional domains. In NeurIPS (2020).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="22">
<li>Tancik, M., Srinivasan, P.P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J.T., Ng, R. 傅里叶特征使网络能够在低维域中学习高频函数。发表于NeurIPS（2020年）。</li>
</ol></div><ol start="23">
<li>Wood, D.N., Azuma, D.I., Aldinger, K., Curless, B., Duchamp, T., Salesin, D.H., Stuetzle, W. Surface light fields for 3D photography. In SIGGRAPH (2000).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="23">
<li>Wood, D.N., Azuma, D.I., Aldinger, K., Curless, B., Duchamp, T., Salesin, D.H., Stuetzle, W. 用于3D摄影的表面光场。发表于SIGGRAPH（2000年）。</li>
</ol></div><ol start="24">
<li>Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR (2018).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="24">
<li>Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O. 深度特征作为感知度量的非凡有效性。发表于CVPR（2018年）。</li>
</ol></div><ol start="25">
<li>Zhou, T., Tucker, R., Flynn, J., Fyffe, G., Snavely, N. Stereo magnification: Learning view synthesis using multiplane images. ACM Trans. Graph. (SIGGRAPH) (2018).</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="25">
<li>Zhou, T., Tucker, R., Flynn, J., Fyffe, G., Snavely, N. 立体放大：使用多平面图像学习视图合成。ACM Trans. Graph.（SIGGRAPH）（2018年）。</li>
</ol></div><p>Ben Mildenhall ({bmild}@cs.berkeley. edu), UC Berkeley, Berkeley, CA, USA</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Ben Mildenhall ({bmild}@cs.berkeley. edu), 加州大学伯克利分校，伯克利，加州，美国</p></div><p>Pratul P. Srinivasan, Matthew Tancik, and Ren Ng ({pratul,tancik,ren}@berkeley.edu), UC Berkeley, Berkeley, CA, USA.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Pratul P. Srinivasan, Matthew Tancik, 和 Ren Ng ({pratul,tancik,ren}@berkeley.edu), 加州大学伯克利分校，伯克利，加州，美国。</p></div><p>Jonathan T. Barron ({barron}@google. com), Google Research Mountain View, CA, USA.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Jonathan T. Barron ({barron}@google. com), 谷歌研究，山景城，加州，美国。</p></div><p>Ravi Ramamoorthi ([ravir]@cs.ucsd.edu), UC San Diego, La Jolla, CA, USA.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Ravi Ramamoorthi ([ravir]@cs.ucsd.edu), 加州大学圣地亚哥分校，拉荷亚，加州，美国。</p></div><p>Ben Mildenhall, Pratul P. Srinivasan, and Matthew Tancik contributed equally to this work.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Ben Mildenhall, Pratul P. Srinivasan, 和 Matthew Tancik 对本工作贡献相同。</p></div><!-- Media --><h2>ACM Student Research Competition</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>ACM学生研究竞赛</h2></div><p>Attention: Undergraduate and Graduate Advancing Computing as a Science &#x26; Profession Computing Students</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>注意：本科生和研究生推进计算作为科学与职业的计算学生</p></div><p>The ACM Student Research Competition (SRC), offers a unique forum for undergraduate and graduate students to present their original research before a panel of judges and attendees at well-known ACM-sponsored and cosponsored conferences. The SRC is an internationally recognized venue enabling undergraduate and graduate students to earn many tangible and intangible rewards from participating:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>ACM学生研究竞赛（SRC）为本科生和研究生提供了一个独特的论坛，让他们在知名的ACM赞助和共同赞助的会议上向评审团和与会者展示他们的原创研究。SRC是一个国际公认的场所，使本科生和研究生能够通过参与获得许多有形和无形的奖励：</p></div><ul>
<li>Awards: cash prizes, medals, and ACM student memberships</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>奖项：现金奖励、奖牌和ACM学生会员资格</li>
</ul></div><ul>
<li>Prestige: Grand Finalists and their advisors are invited to the Annual ACM Awards Banquet, where they are recognized for their accomplishments</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>声望：大决赛选手及其顾问被邀请参加年度ACM颁奖晚宴，在那里他们因其成就而受到认可</li>
</ul></div><ul>
<li>Visibility: opportunities to meet with researchers in their field of interest and make important connections</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>能见度：与其感兴趣领域的研究人员会面并建立重要联系的机会</li>
</ul></div><ul>
<li>Experience: opportunities to sharpen communication, visual, organizational, and presentation skills in preparation for the SRC experience</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>经验：在准备SRC经历中提高沟通、视觉、组织和演示技能的机会</li>
</ul></div><h2>Learn more about ACM Student Research Competitions: <a href="https://src.acm.org">https://src.acm.org</a></h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>了解更多关于ACM学生研究竞赛的信息： <a href="https://src.acm.org">https://src.acm.org</a></h2></div><!-- Media -->
      </body>
    </html>
  