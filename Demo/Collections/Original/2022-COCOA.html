
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>2022-COCOA</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      </head>
      <body>
        <div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-b8ad3474-63a8-40a3-a430-a40f126c8897" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-9cb48d12-66b8-4e1b-96f0-3b36512b62d2" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-07ab757c-9d8b-4d79-b9a9-ed05e5c241d1" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-1725f346-89dd-44c2-b68e-5766a48f8952" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-ce3c167a-c21e-4aa9-b579-8f05ed57ca8f" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-24871678-fb19-49dc-bbab-09cbddfffafd" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-7bfb87bd-2537-407b-88a6-1dbf325c8c17" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-6="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-d7b3c4b3-1658-4bca-88bb-41b75a716ff4" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-99c57337-982f-44c2-a130-0d5480dfc8ea" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div><h2><div><div>References<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">参考文献</div></div></div></h2><div><br></div><div><div><div>[1] Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for attribute-based classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 819-826, 2013.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[1] Zeynep Akata、Florent Perronnin、Zaid Harchaoui和Cordelia Schmid。基于属性分类的标签嵌入。见《IEEE计算机视觉与模式识别会议论文集》，第819 - 826页，2013年。</div></div></div></div><div><br></div><div><div><div>[2] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label embedding for image classification. TPAMI, 2016.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[2] Z. Akata、F. Perronnin、Z. Harchaoui和C. Schmid。用于图像分类的标签嵌入。《模式分析与机器智能汇刊》（TPAMI），2016年。</div></div></div></div><div><br></div><div><div><div>[3] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Evaluation of output embeddings for fine-grained image classification. CVPR, 2015.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[3] Z. Akata、S. Reed、D. Walter、H. Lee和B. Schiele。用于细粒度图像分类的输出嵌入评估。《计算机视觉与模式识别会议》（CVPR），2015年。</div></div></div></div><div><br></div><div><div><div>[4] Yashas Annadani and Soma Biswas. Preserving semantic relations for zero-shot learning. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7603- 7612, 2018.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[4] Yashas Annadani和Soma Biswas。零样本学习中保留语义关系。2018年《IEEE/CVF计算机视觉与模式识别会议》，第7603 - 7612页，2018年。</div></div></div></div><div><br></div><div><div><div>[5] Y. Balaji, S. Sankaranarayanan, and R. Chellappa. Metareg: Towards domain generalization using meta-regularization. In NeurIPS, 2018.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[5] Y. Balaji、S. Sankaranarayanan和R. Chellappa。元正则化（Metareg）：使用元正则化实现领域泛化。见《神经信息处理系统大会》（NeurIPS），2018年。</div></div></div></div><div><br></div><div><div><div>[6] Fabio Maria Carlucci, Antonio D'Innocente, Silvia Bucci, B. Caputo, and T. Tommasi. Domain generalization by solving jigsaw puzzles. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2224-2233, 2019.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[6] Fabio Maria Carlucci、Antonio D'Innocente、Silvia Bucci、B. Caputo和T. Tommasi。通过解决拼图难题实现领域泛化。2019年《IEEE/CVF计算机视觉与模式识别会议》（CVPR），第2224 - 2233页，2019年。</div></div></div></div><div><br></div><div><div><div>[7] Lampert CH, Nickisch H, and Harmeling S. Attribute-based classification for zero-shot visual object categorization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[7] Lampert CH、Nickisch H和Harmeling S。基于属性的分类用于零样本视觉对象分类。《IEEE模式分析与机器智能汇刊》，2013年。</div></div></div></div><div><br></div><div><div><div>[8] Shivam Chandhok and V. Balasubramanian. Two-level adversarial visual-semantic coupling for generalized zero-shot learning. WACV, 2021.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[8] Shivam Chandhok和V. Balasubramanian。用于广义零样本学习的两级对抗视觉 - 语义耦合。《冬季计算机视觉应用会议》（WACV），2021年。</div></div></div></div><div><br></div><div><div><div>[9] S. Changpinyo, W.-L. Chao, B.; Gong, and F. Sha. Synthesized classifiers for zero-shot learning. CVPR, 2016.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[9] S. Changpinyo、W. - L. Chao、B. Gong和F. Sha。用于零样本学习的合成分类器。《计算机视觉与模式识别会议》（CVPR），2016年。</div></div></div></div><div><br></div><div><div><div>[10] Prithvijit Chattopadhyay, Y. Balaji, and Judy Hoffman. Learning to balance specificity and invariance for in and out of domain generalization. volume abs/2008.12839, 2020.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[10] Prithvijit Chattopadhyay、Y. Balaji和Judy Hoffman。学习平衡特异性和不变性以实现领域内和领域外泛化。卷abs/2008.12839，2020年。</div></div></div></div><div><br></div><div><div><div>[11] Hisham Cholakkal, Guolei Sun, Fahad Shahbaz Khan, and Ling Shao. Object counting and instance segmentation with image-level supervision. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12389-12397, 2019.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[11] 希沙姆·乔拉卡尔（Hisham Cholakkal）、孙国磊（Guolei Sun）、法哈德·沙赫巴兹·汗（Fahad Shahbaz Khan）和邵岭（Ling Shao）。基于图像级监督的目标计数与实例分割。2019年电气与电子工程师协会/计算机视觉基金会计算机视觉与模式识别会议（IEEE/CVF Conference on Computer Vision and Pattern Recognition，CVPR），第12389 - 12397页，2019年。</div></div></div></div><div><br></div><div><div><div>[12] Rafael Felix, Vijay BG Kumar, Ian Reid, and Gustavo Carneiro. Multi-modal cycle-consistent generalized zero-shot learning. In Proceedings of the European Conference on Computer Vision, 2018.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[12] 拉斐尔·费利克斯（Rafael Felix）、维杰·BG·库马尔（Vijay BG Kumar）、伊恩·里德（Ian Reid）和古斯塔沃·卡内罗（Gustavo Carneiro）。多模态循环一致的广义零样本学习。《欧洲计算机视觉会议论文集》，2018年。</div></div></div></div><div><br></div><div><div><div>[13] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc'Aurelio Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. In Advances in neural information processing systems, pages 2121-2129, 2013.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[13] 安德里亚·弗罗姆（Andrea Frome）、格雷格·S·科拉多（Greg S Corrado）、乔恩·什伦斯（Jon Shlens）、萨米·本吉奥（Samy Bengio）、杰夫·迪恩（Jeff Dean）、马克·奥雷利奥·兰扎托（Marc'Aurelio Ranzato）和托马斯·米科洛夫（Tomas Mikolov）。DEVISE：一种深度视觉语义嵌入模型。《神经信息处理系统进展》，第2121 - 2129页，2013年。</div></div></div></div><div><br></div><div><div><div>[14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[14] 亚罗斯拉夫·加宁（Yaroslav Ganin）、叶夫根尼娅·乌斯蒂诺娃（Evgeniya Ustinova）、哈娜·阿贾坎（Hana Ajakan）、帕斯卡尔·热尔曼（Pascal Germain）、雨果·拉罗谢尔（Hugo Larochelle）、弗朗索瓦·拉维奥莱特（François Laviolette）、马里奥·马尔尚（Mario Marchand）和维克多·伦皮茨基（Victor Lempitsky）。神经网络的领域对抗训练。《机器学习研究杂志》，17(1):2096 - 2030，2016年。</div></div></div></div><div><br></div><div><div><div>[15] Muhammad Ghifary, W. Kleijn, M. Zhang, and D. Balduzzi. Domain generalization for object recognition with multi-task autoencoders. 2015 IEEE International Conference on Computer Vision (ICCV), pages 2551-2559, 2015.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[15] 穆罕默德·吉法里（Muhammad Ghifary）、W. 克莱因（W. Kleijn）、M. 张（M. Zhang）和D. 巴尔杜齐（D. Balduzzi）。基于多任务自编码器的目标识别领域泛化。2015年电气与电子工程师协会国际计算机视觉会议（IEEE International Conference on Computer Vision，ICCV），第2551 - 2559页，2015年。</div></div></div></div><div><br></div><div><div><div>[16] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rota-<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[16] 斯皮罗斯·吉达里斯（Spyros Gidaris）、普拉维尔·辛格（Praveer Singh）和尼科斯·科莫达基斯（Nikos Komodakis）。通过预测图像旋转进行无监督表征学习</div></div></div></div><div><br></div><div><div><div>tions. ArXiv, abs/1803.07728, 2018.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">。预印本平台ArXiv，编号abs/1803.07728，2018年。</div></div></div></div><div><br></div><div><div><div>[17] Boqing Gong, K. Grauman, and Fei Sha. Reshaping visual datasets for domain adaptation. In NIPS, 2013.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[17] 龚博清（Boqing Gong）、K. 格劳曼（K. Grauman）和沙飞（Fei Sha）。为领域适应重塑视觉数据集。《神经信息处理系统大会》，2013年。</div></div></div></div><div><br></div><div><div><div>[18] He Huang, Changhu Wang, Philip S Yu, and Chang-Dong Wang. Generative dual adversarial network for generalized zero-shot learning. CVPR, 2019.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[18] 黄贺（He Huang）、王长虎（Changhu Wang）、菲利普·S·于（Philip S Yu）和王长东（Chang - Dong Wang）。用于广义零样本学习的生成式对偶对抗网络。计算机视觉与模式识别会议（CVPR），2019年。</div></div></div></div><div><br></div><div><div><div>[19] Rohit Keshari, Richa Singh, and Mayank Vatsa. Generalized zero-shot learning via over-complete distribution. CVPR, 2020.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[19] 罗希特·凯沙里（Rohit Keshari）、里查·辛格（Richa Singh）和马扬克·瓦查（Mayank Vatsa）。通过过完备分布实现广义零样本学习。计算机视觉与模式识别会议（CVPR），2020年。</div></div></div></div><div><br></div><div><div><div>[20] Elyor Kodirov, Tao Xiang, Zhenyong Fu, and S. Gong. Unsupervised domain adaptation for zero-shot learning. 2015 IEEE International Conference on Computer Vision (ICCV), pages 2452-2460, 2015.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[20] 埃利奥尔·科迪罗夫（Elyor Kodirov）、陶翔（Tao Xiang）、傅振勇（Zhenyong Fu）和龚少晖（S. Gong）。用于零样本学习的无监督领域适应。2015年电气与电子工程师协会国际计算机视觉会议（IEEE International Conference on Computer Vision，ICCV），第2452 - 2460页，2015年。</div></div></div></div><div><br></div><div><div><div>[21] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain generalization. 2017 IEEE International Conference on Computer Vision (ICCV), pages 5543-5551, 2017.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[21] 李达（Da Li）、杨永新（Yongxin Yang）、宋毅哲（Yi - Zhe Song）和蒂莫西·M·霍斯佩代尔斯（Timothy M. Hospedales）。更深、更广、更具艺术性的领域泛化。2017年电气与电子工程师协会国际计算机视觉会议（IEEE International Conference on Computer Vision，ICCV），第5543 - 5551页，2017年。</div></div></div></div><div><br></div><div><div><div>[22] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for domain generalization. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17712" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>A</mi><mi>A</mi><mi>I</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2018</mn></mrow></math></mjx-assistive-mml></mjx-container> .<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[22] 李达（Da Li）、杨永新（Yongxin Yang）、宋毅哲（Yi - Zhe Song）和蒂莫西·M·霍斯佩代尔斯（Timothy M. Hospedales）。学习泛化：用于领域泛化的元学习。见<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17713" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>A</mi><mi>A</mi><mi>I</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2018</mn></mrow></math></mjx-assistive-mml></mjx-container> 。</div></div></div></div><div><br></div><div><div><div>[23] Da Li, J. Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M. Hospedales. Episodic training for domain generalization. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1446-1455, 2019.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[23] 李达（Da Li）、张健树（J. Zhang）、杨永新（Yongxin Yang）、刘聪（Cong Liu）、宋毅哲（Yi - Zhe Song）和蒂莫西·M·霍斯佩代尔斯（Timothy M. Hospedales）。用于领域泛化的情景式训练。2019年电气与电子工程师协会/计算机视觉基金会国际计算机视觉会议（IEEE/CVF International Conference on Computer Vision，ICCV），第1446 - 1455页，2019年。</div></div></div></div><div><br></div><div><div><div>[24] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic training for domain generalization. In Proceedings of the IEEE International Conference on Computer Vision, pages 1446-1455, 2019.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[24] 李达（Da Li）、张健树（Jianshu Zhang）、杨永新（Yongxin Yang）、刘聪（Cong Liu）、宋毅哲（Yi - Zhe Song）和蒂莫西·M·霍斯佩代尔斯（Timothy M Hospedales）。用于领域泛化的情景式训练。《电气与电子工程师协会国际计算机视觉会议论文集》，第1446 - 1455页，2019年。</div></div></div></div><div><br></div><div><div><div>[25] Haoliang Li, Sinno Jialin Pan, S. Wang, and A. Kot. Domain generalization with adversarial feature learning. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5400-5409, 2018.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[25] 李昊亮（Haoliang Li）、潘嘉麟（Sinno Jialin Pan）、王S.（S. Wang）和科特A.（A. Kot）。基于对抗特征学习的领域泛化。2018年电气与电子工程师协会/计算机视觉基金会计算机视觉与模式识别会议，第5400 - 5409页，2018年。</div></div></div></div><div><br></div><div><div><div>[26] Y. Li, X. Tian, Mingming Gong, Y. Liu, T. Liu, K. Zhang, and D. Tao. Deep domain generalization via conditional invariant adversarial networks. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17714" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2018</mn></mrow></math></mjx-assistive-mml></mjx-container> .<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[26] 李阳（Y. Li）、田鑫（X. Tian）、龚明明（Mingming Gong）、刘洋（Y. Liu）、刘婷（T. Liu）、张凯（K. Zhang）和陶大程（D. Tao）。通过条件不变对抗网络实现深度领域泛化。见 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17715" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2018</mn></mrow></math></mjx-assistive-mml></mjx-container>。</div></div></div></div><div><br></div><div><div><div>[27] Massimiliano Mancini, Zeynep Akata, E. Ricci, and Barbara Caputo. Towards recognizing unseen categories in unseen domains. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17716" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2020</mn></mrow></math></mjx-assistive-mml></mjx-container> .<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[27] 马西米利亚诺·曼奇尼（Massimiliano Mancini）、泽内普·阿卡塔（Zeynep Akata）、E. 里奇（E. Ricci）和芭芭拉·卡普托（Barbara Caputo）。迈向识别未见领域中的未见类别。见 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17717" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2020</mn></mrow></math></mjx-assistive-mml></mjx-container>。</div></div></div></div><div><br></div><div><div><div>[28] Udit Maniyar, K. J. Joseph, A. Deshmukh, Ü. Dogan, and V. Balasubramanian. Zero-shot domain generalization. ArXiv, abs/2008.07443, 2020.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[28] 乌迪特·马尼耶尔（Udit Maniyar）、K. J. 约瑟夫（K. J. Joseph）、A. 德什穆克（A. Deshmukh）、Ü. 多安（Ü. Dogan）和 V. 巴拉苏布拉马尼亚姆（V. Balasubramanian）。零样本领域泛化。预印本 arXiv:2008.07443，2020 年。</div></div></div></div><div><br></div><div><div><div>[29] Tomas Mikolov, Kai Chen, Greg S. Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space, 2013.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[29] 托马斯·米科洛夫（Tomas Mikolov）、陈凯（Kai Chen）、格雷格·S·科拉多（Greg S. Corrado）和杰弗里·迪恩（Jeffrey Dean）。向量空间中词表示的高效估计，2013 年。</div></div></div></div><div><br></div><div><div><div>[30] Ashish Mishra, Shiva Krishna Reddy, Anurag Mittal, and Hema A Murthy. A generative model for zero shot learning using conditional variational autoencoders. CVPRW, 2018.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[30] 阿希什·米什拉（Ashish Mishra）、希瓦·克里希纳·雷迪（Shiva Krishna Reddy）、阿努拉格·米塔尔（Anurag Mittal）和赫马·A·穆尔蒂（Hema A Murthy）。使用条件变分自编码器的零样本学习生成模型。计算机视觉与模式识别研讨会（CVPRW），2018 年。</div></div></div></div><div><br></div><div><div><div>[31] Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In International Conference on Learning Representations, 2018.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[31] 宫田健（Takeru Miyato）和小山正典（Masanori Koyama）。带投影判别器的条件生成对抗网络（cGANs）。见国际学习表征会议，2018 年。</div></div></div></div><div><br></div><div><div><div>[32] Krikamol Muandet, D. Balduzzi, and B. Schölkopf. Domain generalization via invariant feature representation. ArXiv, abs/1301.2115, 2013.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[32] 克里卡莫尔·穆安代特（Krikamol Muandet）、D. 巴尔杜齐（D. Balduzzi）和 B. 肖尔科普夫（B. Schölkopf）。通过不变特征表示实现领域泛化。预印本 arXiv:1301.2115，2013 年。</div></div></div></div><div><br></div><div><div><div></div></div></div></span></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-aaa9a812-459e-440b-bd65-a2b37c297bdf" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div><paragraphpositioning data-position-9="0,0">[33] Sanath Narayan, A. Gupta, F. Khan, Cees G. M. Snoek, and<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[33] 萨纳特·纳拉扬（Sanath Narayan）、A. 古普塔（A. Gupta）、F. 汗（F. Khan）、塞斯·G·M·斯诺克（Cees G. M. Snoek）和</div></paragraphpositioning></div></div></div><div><br></div><div><div><div>L. Shao. Latent embedding feedback and discriminative features for zero-shot classification. ArXiv, abs/2003.07833, 2020.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">邵玲（L. Shao）。用于零样本分类的潜在嵌入反馈和判别特征。预印本 arXiv:2003.07833，2020 年。</div></div></div></div><div><br></div><div><div><div>[34] Sanath Narayan, Akshita Gupta, Salman Khan, Fahad Shah-baz Khan, Ling Shao, and Mubarak Shah. Discriminative region-based multi-label zero-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 8731-8740, October 2021.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[34] 萨纳特·纳拉扬（Sanath Narayan）、阿克希塔·古普塔（Akshita Gupta）、萨尔曼·汗（Salman Khan）、法哈德·沙赫巴兹·汗（Fahad Shah-baz Khan）、邵玲（Ling Shao）和穆巴拉克·沙阿（Mubarak Shah）。基于判别区域的多标签零样本学习。见电气与电子工程师协会/计算机视觉基金会国际计算机视觉会议（IEEE/CVF International Conference on Computer Vision，ICCV）论文集，第 8731 - 8740 页，2021 年 10 月。</div></div></div></div><div><br></div><div><div><div>[35] Jian Ni, Shanghang Zhang, and Haiyong Xie. Dual adversarial semantics-consistent network for generalized zero-shot learning. NeurIPS, 2019.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[35] 倪健（Jian Ni）、张上航（Shanghang Zhang）和谢海勇（Haiyong Xie）。用于广义零样本学习的双对抗语义一致网络。神经信息处理系统大会（NeurIPS），2019 年。</div></div></div></div><div><br></div><div><div><div>[36] Jian Ni, Shanghang Zhang, and Haiyong Xie. Dual adversarial semantics-consistent network for generalized zero-shot learning. NeurIPS, , 2019.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[36] 倪健（Jian Ni）、张上航（Shanghang Zhang）和谢海勇（Haiyong Xie）。用于广义零样本学习的双对抗语义一致网络。神经信息处理系统大会（NeurIPS），，2019 年。</div></div></div></div><div><br></div><div><div><div>[37] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1406-1415, 2019.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[37] 彭兴超（Xingchao Peng）、白勤勋（Qinxun Bai）、夏西德（Xide Xia）、黄紫军（Zijun Huang）、凯特·塞内科（Kate Saenko）和王博（Bo Wang）。多源领域自适应的矩匹配。见电气与电子工程师协会国际计算机视觉会议论文集，第 1406 - 1415 页，2019 年。</div></div></div></div><div><br></div><div><div><div>[38] Jathushan Rajasegaran, Munawar Hayat, Salman Hameed Khan, Fahad Shahbaz Khan, and Ling Shao. Random path selection for continual learning. In NeurIPS, 2019.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[38] 贾图山·拉贾塞加兰（Jathushan Rajasegaran）、穆纳瓦尔·哈亚特（Munawar Hayat）、萨尔曼·哈米德·汗（Salman Hameed Khan）、法哈德·沙赫巴兹·汗（Fahad Shahbaz Khan）和邵玲（Ling Shao）。持续学习的随机路径选择。见神经信息处理系统大会（NeurIPS），2019 年。</div></div></div></div><div><br></div><div><div><div>[39] Jathushan Rajasegaran, Salman Hameed Khan, Munawar Hayat, Fahad Shahbaz Khan, and Mubarak Shah. itaml: An incremental task-agnostic meta-learning approach. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13585-13594, 2020.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[39] 贾图山·拉贾塞加兰（Jathushan Rajasegaran）、萨尔曼·哈米德·汗（Salman Hameed Khan）、穆纳瓦尔·哈亚特（Munawar Hayat）、法哈德·沙赫巴兹·汗（Fahad Shahbaz Khan）和穆巴拉克·沙阿（Mubarak Shah）。itaml：一种增量式任务无关元学习方法。2020 年电气与电子工程师协会/计算机视觉基金会计算机视觉与模式识别会议（IEEE/CVF Conference on Computer Vision and Pattern Recognition，CVPR），第 13585 - 13594 页，2020 年。</div></div></div></div><div><br></div><div><div><div>[40] Scott E. Reed, Zeynep Akata, H. Lee, and B. Schiele. Learning deep representations of fine-grained visual descriptions. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 49-58, 2016.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[40] 斯科特·E·里德（Scott E. Reed）、泽内普·阿卡塔（Zeynep Akata）、H. 李（H. Lee）和 B. 席勒（B. Schiele）。细粒度视觉描述的深度表示学习。2016 年电气与电子工程师协会计算机视觉与模式识别会议（IEEE Conference on Computer Vision and Pattern Recognition，CVPR），第 49 - 58 页，2016 年。</div></div></div></div><div><br></div><div><div><div>[41] B. Romera-Paredes and P. H. Torr. An embarrassingly simple approach to zero-shot learning. ICML, 2015.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[41] B. 罗梅拉 - 帕雷德斯（Romera - Paredes）和 P. H. 托尔（Torr）。一种极其简单的零样本学习方法。国际机器学习会议（ICML），2015 年。</div></div></div></div><div><br></div><div><div><div>[42] Edgar Schonfeld, Sayna Ebrahimi, Samarth Sinha, Trevor Darrell, and Zeynep Akata. Generalized zero-and few-shot learning via aligned variational autoencoders. CVPR, 2019.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[42] 埃德加·舍恩菲尔德（Edgar Schonfeld）、赛娜·易卜拉欣米（Sayna Ebrahimi）、萨马尔思·辛哈（Samarth Sinha）、特雷弗·达雷尔（Trevor Darrell）和泽内普·阿卡塔（Zeynep Akata）。通过对齐变分自编码器实现广义零样本和少样本学习。计算机视觉与模式识别会议（CVPR），2019 年。</div></div></div></div><div><br></div><div><div><div>[43] Mattia Segu, A. Tonioni, and Federico Tombari. Batch normalization embeddings for deep domain generalization. ArXiv, abs/2011.12672, 2020.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[43] 马蒂亚·塞古（Mattia Segu）、A. 托尼奥尼（Tonioni）和费德里科·通巴里（Federico Tombari）。用于深度领域泛化的批量归一化嵌入。预印本平台（ArXiv），编号 abs/2011.12672，2020 年。</div></div></div></div><div><br></div><div><div><div>[44] Seonguk Seo, Yumin Suh, D. Kim, Jongwoo Han, and B. Han. Learning to optimize domain specific normalization for domain generalization. 2020.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[44] 徐成旭（Seonguk Seo）、徐有民（Yumin Suh）、D. 金（Kim）、韩钟宇（Jongwoo Han）和 B. 韩（Han）。学习优化特定领域归一化以实现领域泛化。2020 年。</div></div></div></div><div><br></div><div><div><div>[45] Yuming Shen, J. Qin, and L. Huang. Invertible zero-shot recognition flows. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17704" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2020</mn></mrow></math></mjx-assistive-mml></mjx-container> .<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[45] 沈玉明、J. 秦和 L. 黄。可逆零样本识别流。见 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17705" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2020</mn></mrow></math></mjx-assistive-mml></mjx-container>。</div></div></div></div><div><br></div><div><div><div>[46] Yutaro Shigeto, Ikumi Suzuki, K. Hara, M. Shimbo, and Y. Matsumoto. Ridge regression, hubness, and zero-shot learning. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17706" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>C</mi><mi>M</mi><mi>L</mi></mrow><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mrow data-mjx-texclass="ORD"><mi>P</mi><mi>K</mi><mi>D</mi><mi>D</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2015</mn></mrow></math></mjx-assistive-mml></mjx-container> .<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[46] 重藤裕太郎（Yutaro Shigeto）、铃木郁美（Ikumi Suzuki）、K. 原（Hara）、M. 岛本（Shimbo）和 Y. 松本（Matsumoto）。岭回归、枢纽性与零样本学习。见 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17707" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>C</mi><mi>M</mi><mi>L</mi></mrow><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mrow data-mjx-texclass="ORD"><mi>P</mi><mi>K</mi><mi>D</mi><mi>D</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2015</mn></mrow></math></mjx-assistive-mml></mjx-container>。</div></div></div></div><div><br></div><div><div><div>[47] A. Torralba and Alexei A. Efros. Unbiased look at dataset bias. CVPR 2011, pages 1521-1528, 2011.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[47] A. 托拉尔巴（Torralba）和阿列克谢·A. 埃弗罗斯（Alexei A. Efros）。对数据集偏差的无偏审视。计算机视觉与模式识别会议（CVPR）2011 年，第 1521 - 1528 页，2011 年。</div></div></div></div><div><br></div><div><div><div>[48] Y.-H. H. Tsai, L.-K. Huang, and R. Salakhutdinov. Learning robust visual-semantic embeddings. ICCV, 2017.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[48] 蔡育宏（Y. - H. H. Tsai）、黄立凯（L. - K. Huang）和 R. 萨拉胡季诺夫（Salakhutdinov）。学习鲁棒的视觉 - 语义嵌入。国际计算机视觉会议（ICCV），2017 年。</div></div></div></div><div><br></div><div><div><div>[49] Ziyu Wan, Dongdong Chen, Y. Li, Xingguang Yan, Junge Zhang, Y. Yu, and Jing Liao. Transductive zero-shot learning with visual structure constraint. In NeurIPS, 2019.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[49] 万子玉、陈东冬、Y. 李、闫星光、张军格、Y. 余和廖静。具有视觉结构约束的直推式零样本学习。神经信息处理系统大会（NeurIPS），2019 年。</div></div></div></div><div><br></div><div><div><div>[50] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and B. Schiele. Latent embeddings for zero-shot classification. CVPR, 2016.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[50] Y. 西安（Xian）、Z. 阿卡塔（Akata）、G. 夏尔马（Sharma）、Q. 阮（Nguyen）、M. 海因（Hein）和 B. 席勒（Schiele）。用于零样本分类的潜在嵌入。计算机视觉与模式识别会议（CVPR），2016 年。</div></div></div></div><div><br></div><div><div><div>[51] Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt Schiele, and Zeynep Akata. Semantic projection network for zero-and few-label semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8256-8265, 2019.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[51] 西安永勤（Yongqin Xian）、苏巴布拉塔·乔杜里（Subhabrata Choudhury）、何洋、伯恩特·席勒（Bernt Schiele）和泽内普·阿卡塔（Zeynep Akata）。用于零标签和少标签语义分割的语义投影网络。见电气与电子工程师协会计算机视觉与模式识别会议论文集，第 8256 - 8265 页，2019 年。</div></div></div></div><div><br></div><div><div><div>[52] Yongqin Xian, Tobias Lorenz, Bernt Schiele, , and Zeynep Akata. Feature generating networks for zero-shot learning. CVPR, 2018.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[52] 西安永勤（Yongqin Xian）、托比亚斯·洛伦茨（Tobias Lorenz）、伯恩特·席勒（Bernt Schiele）和泽内普·阿卡塔（Zeynep Akata）。用于零样本学习的特征生成网络。计算机视觉与模式识别会议（CVPR），2018 年。</div></div></div></div><div><br></div><div><div><div>[53] Y. Xian, S. Sharma, B. Schiele, and Z. Akata. A feature generating framework for any-shot learning. CVPR, 2019.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[53] Y. 西安（Xian）、S. 夏尔马（Sharma）、B. 席勒（Schiele）和 Z. 阿卡塔（Akata）。用于任意样本学习的特征生成框架。计算机视觉与模式识别会议（CVPR），2019 年。</div></div></div></div><div><br></div><div><div><div>[54] Zheng Xu, W. Li, Li Niu, and Dong Xu. Exploiting low-rank structure from latent domains for domain generalization. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17708" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2014</mn></mrow></math></mjx-assistive-mml></mjx-container> .<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[54] 徐政、W. 李、牛莉和徐东。从潜在领域中挖掘低秩结构以实现领域泛化。见 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17709" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2014</mn></mrow></math></mjx-assistive-mml></mjx-container>。</div></div></div></div><div><br></div><div><div><div>[55] P. Yang and Wei Gao. Multi-view discriminant transfer learning. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17710" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>I</mi><mi>J</mi><mi>C</mi><mi>A</mi><mi>I</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2013</mn></mrow></math></mjx-assistive-mml></mjx-container> .<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[55] P. 杨和高伟。多视图判别式迁移学习。见 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="17711" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>I</mi><mi>J</mi><mi>C</mi><mi>A</mi><mi>I</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2013</mn></mrow></math></mjx-assistive-mml></mjx-container>。</div></div></div></div><div><br></div><div><div><div>[56] L. Zhang, Tao Xiang, and S. Gong. Learning a deep embedding model for zero-shot learning. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3010-3019, 2017.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[56] 张磊、向涛和龚少刚。学习用于零样本学习的深度嵌入模型。2017 年电气与电子工程师协会计算机视觉与模式识别会议（CVPR），第 3010 - 3019 页，2017 年。</div></div></div></div><div><br></div><div><div><div>[57] Ziming Zhang and Venkatesh Saligrama. Zero-shot learning via joint latent similarity embedding. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6034-6042, 2016.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[57] 张子铭（Ziming Zhang）和文卡特什·萨利格拉马（Venkatesh Saligrama）。通过联合潜在相似性嵌入实现零样本学习。2016 年电气与电子工程师协会计算机视觉与模式识别会议（IEEE Conference on Computer Vision and Pattern Recognition，CVPR），第 6034 - 6042 页，2016 年。</div></div></div></div><div><div><div></div></div></div></span></div></div></div></div></div></div>
      </body>
    </html>
  