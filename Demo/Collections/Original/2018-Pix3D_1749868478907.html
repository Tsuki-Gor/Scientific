
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <h1>Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>Pix3D：单图像3D形状建模的数据集和方法</h1></div><p>Xingyuan Sun \({}^{*1,2}\) Jiajun Wu \({}^{*1}\) Xiuming Zhang \({}^{1}\) Zhoutong Zhang \({}^{1}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>孙兴源 \({}^{*1,2}\) 吴佳俊 \({}^{*1}\) 张秀明 \({}^{1}\) 张周通 \({}^{1}\)</p></div><p>Chengkai Zhang \({}^{1}\;\) Tianfan \({\mathrm{{Xue}}}^{3}\;\) Joshua B. Tenenbaum \({}^{1}\;\) William T. Freeman \({}^{1,3}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>张承凯 \({}^{1}\;\) 田帆 \({\mathrm{{Xue}}}^{3}\;\) 乔舒亚·B·特嫩鲍姆 \({}^{1}\;\) 威廉·T·弗里曼 \({}^{1,3}\)</p></div><p>\({}^{1}\) Massachusetts Institute of Technology \({}^{2}\) Shanghai Jiao Tong University \({}^{3}\) Google Research</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{1}\) 麻省理工学院 \({}^{2}\) 上海交通大学 \({}^{3}\) 谷歌研究</p></div><!-- Media --><!-- figureText: Well-aligned images and shapes Mismatched 3D shapes Imprecise pose annotations Existing Datasets Our Dataset: Pix3D --><img src="https://cdn.noedgeai.com/bo_d164lj3ef24c73d1lk80_0.jpg?x=140&#x26;y=654&#x26;w=1466&#x26;h=453&#x26;r=0"><p>Figure 1: We present Pix3D, a new large-scale dataset of diverse image-shape pairs. Each 3D shape in Pix3D is associated with a rich and diverse set of images, each with an accurate 3D pose annotation to ensure precise 2D-3D alignment. In comparison, existing datasets have limitations: 3D models may not match the objects in images; pose annotations may be imprecise; or the dataset may be relatively small.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图1：我们展示了Pix3D，这是一个新的大规模多样化图像-形状对数据集。Pix3D中的每个3D形状都与一组丰富多样的图像相关联，每个图像都有准确的3D姿态注释，以确保精确的2D-3D对齐。相比之下，现有数据集存在局限性：3D模型可能与图像中的物体不匹配；姿态注释可能不准确；或者数据集可能相对较小。</p></div><!-- Media --><h2>Abstract</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>摘要</h2></div><p>We study 3D shape modeling from a single image and make contributions to it in three aspects. First, we present Pix3D, a large-scale benchmark of diverse image-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications in shape-related tasks including reconstruction, retrieval, viewpoint estimation, etc. Building such a large-scale dataset, however, is highly challenging; existing datasets either contain only synthetic data, or lack precise alignment between \({2D}\) images and \({3D}\) shapes,or only have a small number of images. Second,we calibrate the evaluation criteria for \({3D}\) shape reconstruction through behavioral studies, and use them to objectively and systematically benchmark cutting-edge reconstruction algorithms on Pix3D. Third, we design a novel model that simultaneously performs 3D reconstruction and pose estimation; our multi-task learning approach achieves state-of-the-art performance on both tasks.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们研究了从单个图像进行3D形状建模，并在三个方面做出了贡献。首先，我们提出了Pix3D，这是一个具有像素级2D-3D对齐的多样化图像-形状对的大规模基准。Pix3D在与形状相关的任务中具有广泛的应用，包括重建、检索、视角估计等。然而，构建这样一个大规模数据集是非常具有挑战性的；现有数据集要么仅包含合成数据，要么缺乏图像与形状之间的精确对齐，或者仅有少量图像。其次，我们通过行为研究校准了形状重建的评估标准，并利用这些标准对Pix3D上的前沿重建算法进行客观和系统的基准测试。第三，我们设计了一种新模型，同时执行3D重建和姿态估计；我们的多任务学习方法在这两个任务上都达到了最先进的性能。</p></div><h2>1. Introduction</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>1. 引言</h2></div><p>The computer vision community has put major efforts in building datasets. In 3D vision, there are rich 3D CAD model repositories like ShapeNet [7] and the Princeton Shape Benchmark [49], large-scale datasets associating images and shapes like Pascal 3D+ [64] and ObjectNet3D [63], and benchmarks with fine-grained pose annotations for shapes in images like IKEA [38]. Why do we need one more?</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>计算机视觉界在构建数据集方面付出了重大努力。在3D视觉领域，有丰富的3D CAD模型库，如ShapeNet [7]和普林斯顿形状基准 [49]，以及将图像和形状关联的大规模数据集，如Pascal 3D+ [64]和ObjectNet3D [63]，还有为图像中的形状提供细粒度姿态注释的基准，如IKEA [38]。我们为什么还需要一个数据集？</p></div><p>Looking into Figure 1, we realize existing datasets have limitations for the task of modeling a 3D object from a single image. ShapeNet is a large dataset for \(3\mathrm{D}\) models,but does not come with real images; Pascal 3D+ and ObjectNet3D have real images, but the image-shape alignment is rough because the 3D models do not match the objects in images; IKEA has high-quality image-3D alignment, but it only contains 90 3D models and 759 images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>从图1中，我们意识到现有数据集在从单个图像建模3D对象的任务上存在局限性。ShapeNet是一个大型的模型数据集，但没有真实图像；Pascal 3D+和ObjectNet3D有真实图像，但图像-形状对齐粗糙，因为3D模型与图像中的物体不匹配；IKEA具有高质量的图像-3D对齐，但仅包含90个3D模型和759张图像。</p></div><p>We desire a dataset that has all three merits-a large-scale dataset of real images and ground-truth shapes with precise 2D-3D alignment. Our dataset, named Pix3D, has 395 3D shapes of nine object categories. Each shape associates with a set of real images, capturing the exact object in diverse environments. Further, the 10,069 image-shape pairs have precise 3D annotations, giving pixel-level alignment between shapes and their silhouettes in the images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们希望拥有一个具备三项优点的数据集——一个具有精确2D-3D对齐的真实图像和真实形状的大规模数据集。我们的数据集名为Pix3D，包含九个物体类别的395个3D形状。每个形状都与一组真实图像相关联，捕捉到在不同环境中确切的物体。此外，10,069个图像-形状对具有精确的3D注释，提供了形状与其在图像中轮廓之间的像素级对齐。</p></div><p>Building such a dataset, however, is highly challenging. For each object, it is difficult to simultaneously collect its high-quality geometry and in-the-wild images. We can crawl many images of real-world objects, but we do not have access to their shapes; 3D CAD repositories offer object geometry, but do not come with real images. Further, for each image-shape pair, we need a precise pose annotation that aligns the shape with its projection in the image.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>然而，构建这样一个数据集是非常具有挑战性的。对于每个物体，同时收集其高质量几何形状和自然环境中的图像是困难的。我们可以抓取许多真实物体的图像，但无法获取它们的形状；3D CAD库提供物体几何形状，但没有真实图像。此外，对于每个图像-形状对，我们需要一个精确的姿态注释，以将形状与其在图像中的投影对齐。</p></div><hr>
<!-- Footnote --><ul>
<li>indicates equal contributions.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>表示贡献相等。</li>
</ul></div><!-- Footnote -->
<hr><p>We overcome these challenges by constructing Pix3D in three steps. First, we collect a large number of image-shape pairs by crawling the web and performing 3D scans ourselves. Second,we collect 2D keypoint annotations of objects in the images on Amazon Mechanical Turk, with which we optimize for 3D poses that align shapes with image silhouettes. Third, we filter out image-shape pairs with a poor alignment and, at the same time, collect attributes (i.e., truncation, occlusion) for each instance, again by crowdsourcing.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们通过三个步骤克服了这些挑战，构建了Pix3D。首先，我们通过网络抓取和自行进行3D扫描收集了大量的图像-形状对。其次，我们在亚马逊机械土耳其上收集了图像中物体的2D关键点注释，利用这些注释优化与图像轮廓对齐的3D姿态。第三，我们过滤掉对齐不良的图像-形状对，同时通过众包收集每个实例的属性（即截断、遮挡）。</p></div><p>In addition to high-quality data, we need a proper metric to objectively evaluate the reconstruction results. A well-designed metric should reflect the visual appealingness of the reconstructions. In this paper, we calibrate commonly used metrics, including intersection over union, Chamfer distance, and earth mover's distance, on how well they capture human perception of shape similarity. Based on this, we benchmark state-of-the-art algorithms for \(3\mathrm{D}\) object modeling on Pix \(3\mathrm{D}\) to demonstrate their strengths and weaknesses.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>除了高质量的数据，我们还需要一个合适的指标来客观评估重建结果。一个设计良好的指标应该反映重建的视觉吸引力。在本文中，我们校准了常用的指标，包括交并比、Chamfer距离和地球移动者距离，以评估它们在多大程度上捕捉到人类对形状相似性的感知。基于此，我们对Pix \(3\mathrm{D}\)上的最先进算法进行了基准测试，以展示它们的优缺点。</p></div><p>With its high-quality alignment, Pix3D is also suitable for object pose estimation and shape retrieval. To demonstrate that, we propose a novel model that performs shape and pose estimation simultaneously. Given a single RGB image, our model first predicts its \({2.5}\mathrm{D}\) sketches,and then regresses the \(3\mathrm{D}\) shape and the camera parameters from the estimated \({2.5}\mathrm{D}\) sketches. Experiments show that multi-task learning helps to boost the model's performance.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>凭借其高质量的对齐，Pix3D也适用于物体姿态估计和形状检索。为了证明这一点，我们提出了一种新模型，可以同时进行形状和姿态估计。给定一张单一的RGB图像，我们的模型首先预测其\({2.5}\mathrm{D}\)草图，然后从估计的\({2.5}\mathrm{D}\)草图中回归出\(3\mathrm{D}\)形状和相机参数。实验表明，多任务学习有助于提升模型的性能。</p></div><p>Our contributions are three-fold. First, we build a new dataset for single-image \(3\mathrm{D}\) object modeling; Pix \(3\mathrm{D}\) has a diverse collection of image-shape pairs with precise 2D-3D alignment. Second, we calibrate metrics for 3D shape reconstruction based on their correlations with human perception, and benchmark state-of-the-art algorithms on 3D reconstruction, pose estimation, and shape retrieval. Third, we present a novel model that simultaneously estimates object shape and pose, achieving state-of-the-art performance on both tasks.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的贡献有三方面。首先，我们建立了一个用于单图像\(3\mathrm{D}\)物体建模的新数据集；Pix \(3\mathrm{D}\)拥有多样化的图像-形状对，具有精确的2D-3D对齐。其次，我们根据与人类感知的相关性校准了3D形状重建的指标，并对3D重建、姿态估计和形状检索的最先进算法进行了基准测试。第三，我们提出了一种新模型，可以同时估计物体的形状和姿态，在这两个任务上都实现了最先进的性能。</p></div><h2>2. Related Work</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>2. 相关工作</h2></div><p>Datasets of 3D shapes and scenes. For decades, researchers have been building datasets of \(3\mathrm{D}\) objects,either as a repository of \(3\mathrm{D}\) CAD models \(\left\lbrack  {4,5,{49}}\right\rbrack\) or as images of 3D shapes with pose annotations [34, 47]. Both directions have witnessed the rapid development of web-scale databases: ShapeNet [7] was proposed as a large repository of more than \({50}\mathrm{\;K}\) models covering 55 categories,and Xiang et al. built Pascal 3D+ [64] and ObjectNet3D [63], two large-scale datasets with alignment between \(2\mathrm{D}\) images and the \(3\mathrm{D}\) shape inside. While these datasets have helped to advance the field of \(3\mathrm{D}\) shape modeling,they have their respective limitations: datasets like ShapeNet or Elastic2D3D [32] do not have real images, and recent 3D reconstruction challenges using ShapeNet have to be exclusively on synthetic images [67]; Pascal 3D+ and ObjectNet3D have only rough alignment between images and shapes, because objects in the images are matched to a pre-defined set of CAD models, not their actual shapes. This has limited their usage as a benchmark for 3D shape reconstruction [59].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D形状和场景的数据集。几十年来，研究人员一直在构建\(3\mathrm{D}\)物体的数据集，既作为\(3\mathrm{D}\)CAD模型的存储库，也作为带有姿态注释的3D形状图像[34, 47]。这两个方向都见证了网络规模数据库的快速发展：ShapeNet [7]被提出作为一个包含超过\({50}\mathrm{\;K}\)个模型的庞大存储库，覆盖55个类别，而Xiang等人构建了Pascal 3D+ [64]和ObjectNet3D [63]，这两个大型数据集在\(2\mathrm{D}\)图像和内部\(3\mathrm{D}\)形状之间具有对齐。虽然这些数据集有助于推动\(3\mathrm{D}\)形状建模领域的发展，但它们各自也存在局限性：像ShapeNet或Elastic2D3D [32]这样的数据集没有真实图像，而最近使用ShapeNet的3D重建挑战必须仅限于合成图像[67]；Pascal 3D+和ObjectNet3D在图像和形状之间只有粗略的对齐，因为图像中的物体是与预定义的CAD模型集合匹配，而不是它们的实际形状。这限制了它们作为3D形状重建基准的使用[59]。</p></div><p>With depth sensors like Kinect [24, 27], the community has built various RGB-D or depth-only datasets of objects and scenes. We refer readers to the review article from Firman [14] for a comprehensive list. Among those, many object datasets are designed for benchmarking robot manipulation \(\left\lbrack  {6,{23},{33},{51}}\right\rbrack\) . These datasets often contain a relatively small set of hand-held objects in front of clean backgrounds. Tanks and Temples [30] is an exciting new benchmark with 14 scenes, designed for high-quality, large-scale, multi-view 3D reconstruction. In comparison, our dataset, Pix3D, focuses on reconstructing a 3D object from a single image, and contains much more real-world objects and images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>借助Kinect [24, 27]等深度传感器，社区构建了各种RGB-D或仅深度的数据集，涵盖物体和场景。我们建议读者参考Firman [14]的综述文章，以获取全面的列表。在这些数据集中，许多物体数据集是为基准测试机器人操作\(\left\lbrack  {6,{23},{33},{51}}\right\rbrack\)而设计的。这些数据集通常包含相对较小的一组手持物体，背景干净。Tanks and Temples [30]是一个令人兴奋的新基准，包含14个场景，旨在进行高质量、大规模、多视角的3D重建。相比之下，我们的数据集Pix3D专注于从单一图像重建3D物体，包含更多的真实世界物体和图像。</p></div><p>Probably the dataset closest to Pix3D is the large collection of object scans from Choi et al. [8], which contains a rich and diverse set of shapes, each with an RGB-D video. Their dataset, however, is not ideal for single-image 3D shape modeling for two reasons. First, the object of interest may be truncated throughout the video; this is especially the case for large objects like sofas. Second, their dataset does not explore the various contexts that an object may appear in, as each shape is only associated with a single scan. In Pix3D, we address both problems by leveraging powerful web search engines and crowdsourcing.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>可能与Pix3D最接近的数据集是Choi等人[8]的大量物体扫描集合，包含丰富多样的形状，每个形状都有RGB-D视频。然而，他们的数据集并不理想用于单图像3D形状建模，原因有两个。首先，感兴趣的物体可能在视频中被截断；对于像沙发这样的大物体尤其如此。其次，他们的数据集没有探索物体可能出现的各种上下文，因为每个形状仅与单个扫描相关联。在Pix3D中，我们通过利用强大的网络搜索引擎和众包来解决这两个问题。</p></div><p>Another closely related benchmark is IKEA [38], which provides accurate alignment between images of IKEA objects and 3D CAD models. This dataset is therefore particularly suitable for fine pose estimation. However, it contains only 759 images and 90 shapes, relatively small for shape modeling*. In contrast, Pix3D contains 10,069 images (13.3x) and 395 shapes (4.4x) of greater variations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>另一个密切相关的基准是IKEA [38]，它提供了IKEA物体图像与3D CAD模型之间的准确对齐。因此，该数据集特别适合精确姿态估计。然而，它仅包含759张图像和90个形状，对于形状建模来说相对较小。相比之下，Pix3D包含10,069张图像（13.3倍）和395个形状（4.4倍），变化更大。</p></div><p>Researchers have also explored constructing scene datasets with 3D annotations. Notable attempts include LabelMe- 3D [46], NYU-D [50], SUN RGB-D [53], KITTI [16], and modern large-scale RGB-D scene datasets [10, 40, 54]. These datasets are either synthetic or contain only \(3\mathrm{D}\) surfaces of real scenes. Pix3D, in contrast, offers accurate alignment between 3D object shape and 2D images in the wild.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>研究人员还探索了构建带有3D注释的场景数据集。值得注意的尝试包括LabelMe-3D [46]、NYU-D [50]、SUN RGB-D [53]、KITTI [16]和现代大规模RGB-D场景数据集 [10, 40, 54]。这些数据集要么是合成的，要么仅包含真实场景的\(3\mathrm{D}\)表面。相比之下，Pix3D提供了3D物体形状与自然环境中2D图像之间的准确对齐。</p></div><p>Single-image 3D reconstruction. The problem of recovering object shape from a single image is challenging, as it requires both powerful recognition systems and prior shape knowledge. Using deep convolutional networks, researchers have made significant progress in recent years \(\lbrack 9,{17},{21},{29}\) , 41, 43, 56, 59, 60, 62, 66, 52, 61]. While most of these approaches represent objects in voxels, there have also been attempts to reconstruct objects in point clouds [12] or octave trees \(\left\lbrack  {{44},{57}}\right\rbrack\) . In this paper,we demonstrate that our newly proposed Pix3D serves as an ideal benchmark for evaluating these algorithms. We also propose a novel model that jointly estimates an object’s shape and its \(3\mathrm{D}\) pose.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>单图像3D重建。从单一图像恢复物体形状的问题具有挑战性，因为它需要强大的识别系统和先前的形状知识。利用深度卷积网络，研究人员近年来取得了显著进展\(\lbrack 9,{17},{21},{29}\)，41、43、56、59、60、62、66、52、61]。虽然大多数这些方法以体素表示物体，但也有尝试在点云[12]或八度树\(\left\lbrack  {{44},{57}}\right\rbrack\)中重建物体。在本文中，我们展示了我们新提出的Pix3D作为评估这些算法的理想基准。我们还提出了一种新模型，联合估计物体的形状及其\(3\mathrm{D}\)姿态。</p></div><hr>
<!-- Footnote --><p>*Only 90 of the 219 shapes in the IKEA dataset have associated images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>*IKEA数据集中219个形状中仅有90个形状有相关图像。</p></div><!-- Footnote -->
<hr><!-- Media --><!-- figureText: Data Source 1: Extending IKEA Image-Shape Pairs Final Pose Estimation Keypoint Levenberg- Labeling Marquardt Efficient PnP Image-Shape Pairs with Keypoints Initial Pose Estimation Data Source 2: Scanning and Taking Pictures Ourselves --><img src="https://cdn.noedgeai.com/bo_d164lj3ef24c73d1lk80_2.jpg?x=151&#x26;y=224&#x26;w=1448&#x26;h=656&#x26;r=0"><p>Figure 2: We build the dataset in two steps. First, we collect image-shape pairs by crawling web images of IKEA furniture as well as scanning objects and taking pictures ourselves. Second, we align the shapes with their 2D silhouettes by minimizing the \(2\mathrm{D}\) coordinates of the keypoints and their projected positions from 3D, using the Efficient PnP and the Levenberg-Marquardt algorithm.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2：我们分两步构建数据集。首先，我们通过爬取IKEA家具的网络图像以及扫描物体和自己拍照来收集图像-形状对。其次，我们通过最小化关键点的\(2\mathrm{D}\)坐标及其从3D投影的位置来对齐形状与其2D轮廓，使用高效的PnP和Levenberg-Marquardt算法。</p></div><!-- Media --><p>Shape retrieval. Another related research direction is retrieving similar 3D shapes given a single image, instead of reconstructing the object’s actual geometry \(\left\lbrack  {1,{15},{19},{48}}\right\rbrack\) . Pix3D contains shapes with significant inter-class and intra-class variations, and is therefore suitable for both general-purpose and fine-grained shape retrieval tasks.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>形状检索。另一个相关的研究方向是给定单一图像检索相似的3D形状，而不是重建物体的实际几何形状\(\left\lbrack  {1,{15},{19},{48}}\right\rbrack\)。Pix3D包含具有显著类间和类内变化的形状，因此适合一般用途和细粒度形状检索任务。</p></div><p>3D pose estimation. Many of the aforementioned object datasets include annotations of object poses \(\lbrack {34},{38},{47},{63}\) , 64]. Researchers have also proposed numerous methods on \(3\mathrm{D}\) pose estimation \(\left\lbrack  {{13},{42},{55},{58}}\right\rbrack\) . In this paper,we show that Pix3D is also a proper benchmark for this task.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D姿态估计。上述许多物体数据集包括物体姿态的注释\(\lbrack {34},{38},{47},{63}\)，64]。研究人员还提出了许多关于\(3\mathrm{D}\)姿态估计\(\left\lbrack  {{13},{42},{55},{58}}\right\rbrack\)的方法。在本文中，我们展示了Pix3D也是此任务的合适基准。</p></div><h2>3. Building Pix3D</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>3. 构建Pix3D</h2></div><p>Figure 2 summarizes how we build Pix3D. We collect raw images from web search engines and shapes from 3D repositories; we also take pictures and scan shapes ourselves. Finally,we use labeled keypoints on both 2D images and 3D shapes to align them.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2总结了我们如何构建Pix3D。我们从网络搜索引擎收集原始图像，从3D资源库获取形状；我们还自己拍照和扫描形状。最后，我们使用标记的关键点对齐2D图像和3D形状。</p></div><h3>3.1. Collecting Image-Shape Pairs</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.1. 收集图像-形状对</h3></div><p>We obtain raw image-shape pairs in two ways. One is to crawl images of IKEA furniture from the web and align them with CAD models provided in the IKEA dataset [38]. The other is to directly scan \(3\mathrm{D}\) shapes and take pictures.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们通过两种方式获得原始图像-形状对。一种是从网络上爬取IKEA家具的图像，并将其与IKEA数据集中提供的CAD模型对齐[38]。另一种是直接扫描\(3\mathrm{D}\)形状并拍照。</p></div><p>Extending IKEA. The IKEA dataset [38] contains 219 high-quality 3D models of IKEA furniture, but has only 759 images for 90 shapes. Therefore, we choose to keep the 3D shapes from IKEA dataset, but expand the set of 2D images using online image search engines and crowdsourcing.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>扩展IKEA。IKEA数据集[38]包含219个高质量的IKEA家具3D模型，但仅有759张90个形状的图像。因此，我们选择保留IKEA数据集中的3D形状，但使用在线图像搜索引擎和众包扩展2D图像集。</p></div><p>For each 3D shape, we first search for its corresponding 2D images through Google, Bing, and Baidu, using its IKEA model name as the keyword. We obtain 104,220 images for the 219 shapes. We then use Amazon Mechanical Turk (AMT) to remove irrelevant ones. For each image, we ask three AMT workers to label whether this image matches the 3D shape or not. For images whose three responses differ, we ask three additional workers and decide whether to keep them based on majority voting. We end up with 14,600 images for the 219 IKEA shapes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>对于每个3D形状，我们首先通过Google、Bing和百度搜索其对应的2D图像，使用其IKEA模型名称作为关键词。我们为219个形状获得了104,220张图像。然后，我们使用亚马逊机械土耳其（AMT）去除不相关的图像。对于每张图像，我们请三名AMT工人标记该图像是否与3D形状匹配。对于三名工人的回答不同的图像，我们再请三名额外的工人，并根据多数投票决定是否保留。最终，我们为219个IKEA形状获得了14,600张图像。</p></div><p>3D scan. We scan non-IKEA objects with a Structure Sen- \({\text{sor}}^{ \dagger  }\) mounted on an iPad. We choose to use the Structure Sensor because its mobility enables us to capture a wide range of shapes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D扫描。我们使用安装在iPad上的Structure Sen- \({\text{sor}}^{ \dagger  }\)扫描非宜家物体。我们选择使用Structure Sensor，因为它的移动性使我们能够捕捉到各种形状。</p></div><p>The iPad RGB camera is synchronized with the depth sensor at \({30}\mathrm{\;{Hz}}\) ,and calibrated by the Scanner App provided by Occipital, Inc. \({}^{ \ddagger  }\) The resolution of RGB frames is \({2592} \times  {1936}\) , and the resolution of depth frames is \({320} \times  {240}\) . For each object, we take a short video and fuse the depth data to get its 3D mesh by using fusion algorithm provided by Occipital, Inc. We also take 10-20 images for each scanned object in front of various backgrounds from different viewpoints, making sure the object is neither cropped nor occluded. In total, we have scanned 209 objects and taken 2,313 images. Combining these with the IKEA shapes and images, we have 418 shapes and 16,913 images altogether.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>iPad RGB相机与深度传感器在\({30}\mathrm{\;{Hz}}\)同步，并由Occipital, Inc.提供的扫描应用程序进行校准。\({}^{ \ddagger  }\) RGB帧的分辨率为\({2592} \times  {1936}\)，深度帧的分辨率为\({320} \times  {240}\)。对于每个物体，我们拍摄一段短视频，并使用Occipital, Inc.提供的融合算法将深度数据融合以获取其3D网格。我们还为每个扫描的物体在不同视角的各种背景前拍摄10-20张图像，确保物体没有被裁剪或遮挡。总共，我们扫描了209个物体，拍摄了2,313张图像。将这些与宜家的形状和图像结合，我们总共有418个形状和16,913张图像。</p></div><hr>
<!-- Footnote --><p>†<a href="https://structure.io">https://structure.io</a></p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>†<a href="https://structure.io">https://structure.io</a></p></div><p>#<a href="https://occipital.com">https://occipital.com</a></p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>#<a href="https://occipital.com">https://occipital.com</a></p></div><!-- Footnote -->
<hr><!-- Media --><!-- figureText: 3D Shape Image Alignment 3D Shape Image Alignment 3D Shape Image Alignment --><img src="https://cdn.noedgeai.com/bo_d164lj3ef24c73d1lk80_3.jpg?x=141&#x26;y=198&#x26;w=1474&#x26;h=1219&#x26;r=0"><p>Figure 3: Sample images and shapes in Pix3D. From left to right: 3D shape, image, and alignment. Rows 1-2 show some chairs we scanned, rows 3-4 show a few IKEA objects, and rows 5-6 show some objects of other categories we scanned.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3：Pix3D中的样本图像和形状。从左到右：3D形状、图像和对齐。第1-2行显示我们扫描的一些椅子，第3-4行显示一些宜家物体，第5-6行显示我们扫描的其他类别的一些物体。</p></div><!-- Media --><h3>3.2. Image-Shape Alignment</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.2. 图像-形状对齐</h3></div><p>To align a 3D CAD model with its projection in a 2D image, we need to solve for its \(3\mathrm{D}\) pose (translation and rotation), and the camera parameters used to capture the image.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了将3D CAD模型与其在2D图像中的投影对齐，我们需要求解其\(3\mathrm{D}\)姿态（平移和旋转）以及用于捕捉图像的相机参数。</p></div><p>We use a keypoint-based method inspired by Lim et al. [38]. Denote the keypoints’ 2D coordinates as \({\mathbf{X}}_{2\mathrm{D}} =\) \(\left\{  {{\mathbf{x}}_{1},{\mathbf{x}}_{2},\cdots ,{\mathbf{x}}_{n}}\right\}\) and their corresponding 3D coordinates as \({\mathbf{X}}_{3\mathrm{D}} = \left\{  {{\mathbf{X}}_{1},{\mathbf{X}}_{2},\cdots ,{\mathbf{X}}_{n}}\right\}\) . We solve for camera parameters and 3D poses that minimize the reprojection error of the key-points. Specifically,we want to find the projection matrix \(\mathbf{P}\) that minimizes</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们使用一种基于关键点的方法，灵感来自Lim等人[38]。将关键点的2D坐标表示为\({\mathbf{X}}_{2\mathrm{D}} =\) \(\left\{  {{\mathbf{x}}_{1},{\mathbf{x}}_{2},\cdots ,{\mathbf{x}}_{n}}\right\}\)，其对应的3D坐标表示为\({\mathbf{X}}_{3\mathrm{D}} = \left\{  {{\mathbf{X}}_{1},{\mathbf{X}}_{2},\cdots ,{\mathbf{X}}_{n}}\right\}\)。我们求解相机参数和3D姿态，以最小化关键点的重投影误差。具体来说，我们想要找到最小化的投影矩阵\(\mathbf{P}\)</p></div><p></p>\[\mathcal{L}\left( {\mathbf{P};{\mathbf{X}}_{3\mathrm{D}},{\mathbf{X}}_{2\mathrm{D}}}\right)  = \mathop{\sum }\limits_{i}{\begin{Vmatrix}{\operatorname{Proj}}_{\mathbf{P}}\left( {\mathbf{X}}_{i}\right)  - {\mathbf{x}}_{i}\end{Vmatrix}}_{2}^{2}, \tag{1}\]<p></p><p>where \({\operatorname{Proj}}_{P}\left( \cdot \right)\) is the projection function.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\({\operatorname{Proj}}_{P}\left( \cdot \right)\)是投影函数。</p></div><p>Under the central projection assumption (zero-skew, square pixel, and the optical center is at the center of the frame), we have \(\mathbf{P} = \mathbf{K}\left\lbrack  {\mathbf{R} \mid  \mathbf{T}}\right\rbrack\) ,where \(\mathbf{K}\) is the camera intrinsic matrix; \(\mathbf{R} \in  {\mathbb{R}}^{3 \times  3}\) and \(\mathbf{T} \in  {\mathbb{R}}^{3}\) represent the object’s \(3\mathrm{D}\) rotation and 3D translation, respectively. We know</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在中心投影假设下（零偏斜、方形像素，光学中心位于框架中心），我们有\(\mathbf{P} = \mathbf{K}\left\lbrack  {\mathbf{R} \mid  \mathbf{T}}\right\rbrack\)，其中\(\mathbf{K}\)是相机内参矩阵；\(\mathbf{R} \in  {\mathbb{R}}^{3 \times  3}\)和\(\mathbf{T} \in  {\mathbb{R}}^{3}\)分别表示物体的\(3\mathrm{D}\)旋转和3D平移。我们知道</p></div><p></p>\[\mathbf{K} = \left\lbrack  \begin{matrix} f &#x26; 0 &#x26; w/2 \\  0 &#x26; f &#x26; h/2 \\  0 &#x26; 0 &#x26; 1 \end{matrix}\right\rbrack   \tag{2}\]<p></p><p>where \(f\) is the focal length,and \(w\) and \(h\) are the width and height of the image. Therefore, there are altogether seven parameters to be estimated: rotations \(\theta ,\phi ,\psi\) ,translations \(x,y,z\) ,and focal length \(f\) (Rotation matrix \(R\) is determined by \(\theta ,\phi\) ,and \(\psi\) ).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(f\)是焦距，\(w\)和\(h\)是图像的宽度和高度。因此，总共有七个参数需要估计：旋转\(\theta ,\phi ,\psi\)、平移\(x,y,z\)和焦距\(f\)（旋转矩阵\(R\)由\(\theta ,\phi\)和\(\psi\)确定）。</p></div><p>To solve Equation 1, we first calculate a rough 3D pose using the Efficient \(\mathrm{P}n\mathrm{\;P}\) algorithm [35] and then refine it using the Levenberg-Marquardt algorithm [36, 39], as shown in Figure 2. Details of each step are described below.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了解决方程1，我们首先使用高效的\(\mathrm{P}n\mathrm{\;P}\)算法[35]计算一个粗略的3D姿态，然后使用Levenberg-Marquardt算法[36, 39]对其进行细化，如图2所示。每个步骤的详细信息如下。</p></div><!-- Media --><!-- figureText: 4000 Images From IKEA Self-taken Crawled Misc Sofa Table Tool Wardrobe 3000 2000 1000 0 Bed Chair Desk --><img src="https://cdn.noedgeai.com/bo_d164lj3ef24c73d1lk80_4.jpg?x=139&#x26;y=213&#x26;w=715&#x26;h=378&#x26;r=0"><p>Figure 4: Class distribution of the images</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图4：图像的类别分布</p></div><!-- Media --><p>Efficient \(\mathrm{{PnP}}\) . Perspective- \(n\) -Point(PnP)is the problem of estimating the pose of a calibrated camera given paired 3D points and 2D projections. The Efficient \(\mathrm{P}n\mathrm{P}\) (EPnP) algorithm solves the problem using virtual control points [36]. Because EPnP does not estimate the focal length, we enumerate the focal length \(f\) from 300 to 2,000 with a step size of 10, solve for the \(3\mathrm{D}\) pose with each \(f\) ,and choose the one with the minimum projection error.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>高效的\(\mathrm{{PnP}}\)。视角-\(n\)-点（PnP）是给定配对的3D点和2D投影来估计标定相机姿态的问题。高效的\(\mathrm{P}n\mathrm{P}\)（EPnP）算法通过虚拟控制点解决该问题[36]。由于EPnP不估计焦距，我们从300到2000枚举焦距\(f\)，步长为10，针对每个\(f\)求解\(3\mathrm{D}\)姿态，并选择具有最小投影误差的结果。</p></div><p>The Levenberg-Marquardt algorithm (LMA). We take the output of EPnP with 50 random disturbances as the initial states, and run LMA on each of them. Finally, we choose the solution with the minimum projection error.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>列文伯格-马夸特算法（LMA）。我们将EPnP的输出与50个随机扰动作为初始状态，并在每个状态上运行LMA。最后，我们选择具有最小投影误差的解。</p></div><p>Implementation details. For each 3D shape, we manually label its 3D keypoints. The number of keypoints ranges from 8 to 24. For each image, we ask three AMT workers to label if each keypoint is visible on the image, and if so, where it is. We only consider visible keypoints during the optimization.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>实现细节。对于每个3D形状，我们手动标记其3D关键点。关键点的数量范围从8到24。对于每张图像，我们请三名AMT工作者标记每个关键点在图像上是否可见，如果可见，则标记其位置。在优化过程中，我们只考虑可见的关键点。</p></div><p>The 2D keypoint annotations are noisy, which severely hurts the performance of the optimization algorithm. We try two methods to increase its robustness. The first is to use RANSAC. The second is to use only a subset of 2D keypoint annotations. For each image,denote \(C = \left\{  {{c}_{1},{c}_{2},{c}_{3}}\right\}\) as its three sets of human annotations. We then enumerate the seven nonempty subsets \({C}_{k} \subseteq  C\) ; for each keypoint,we compute the median of its \(2\mathrm{D}\) coordinates in \({C}_{k}\) . We apply our optimization algorithm on every subset \({C}_{k}\) ,and keep the output with the minimum projection error. After that, we let three AMT workers choose, for each image, which of the two methods offers better alignment, or neither performs well. At the same time, we also collect attributes (i.e., truncation, occlusion) for each image. Finally, we fine-tune the annotations ourselves using the GUI offered in ObjectNet3D [63]. Altogether there are \({3953}\mathrm{\;D}\) shapes and 10,069 images. Sample 2D-3D pairs are shown in Figure 3.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>2D关键点注释存在噪声，这严重影响了优化算法的性能。我们尝试了两种方法来提高其鲁棒性。第一种是使用RANSAC。第二种是仅使用部分2D关键点注释。对于每张图像，记\(C = \left\{  {{c}_{1},{c}_{2},{c}_{3}}\right\}\)为其三组人工注释。然后，我们枚举七个非空子集\({C}_{k} \subseteq  C\)；对于每个关键点，我们计算其在\({C}_{k}\)中的\(2\mathrm{D}\)坐标的中位数。我们在每个子集\({C}_{k}\)上应用我们的优化算法，并保留具有最小投影误差的输出。之后，我们让三名AMT工作者选择，对于每张图像，哪种方法提供了更好的对齐，或者两者都表现不佳。同时，我们还收集每张图像的属性（即截断、遮挡）。最后，我们使用ObjectNet3D [63]提供的GUI自行微调注释。总共有\({3953}\mathrm{\;D}\)个形状和10,069张图像。样本2D-3D对如图3所示。</p></div><h2>4. Exploring Pix3D</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>4. 探索Pix3D</h2></div><p>We now present some statistics of Pix3D, and contrast it with its predecessors.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们现在展示一些Pix3D的统计数据，并将其与前身进行对比。</p></div><p>Dataset statistics. Figures 4 and 5 show the category distributions of 2D images and 3D shapes in Pix3D; Figure 6 shows the distribution of the number of images each model has. Our dataset covers a large variety of shapes, each of which has a large number of in-the-wild images. Chairs cover the significant part of Pix3D, because they are common, highly diverse, and well-studied by recent literature [11, 59, 20].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>数据集统计。图4和图5显示了Pix3D中2D图像和3D形状的类别分布；图6显示了每个模型的图像数量分布。我们的数据集涵盖了各种形状，每种形状都有大量的真实场景图像。椅子占据了Pix3D的主要部分，因为它们常见、多样性高，并且在最近的文献中得到了充分研究[11, 59, 20]。</p></div><!-- Media --><!-- figureText: 240 Shapes Self-scanned From IKEA Misc Sofa Table Tool Wardrobe Figure 5: Class distribution of the shapes Number of Images 200 160 120 80 40 0 Bed Bookcase Chair Desk 140 120 Number of Shapes 100 80 60 40 20 0 8 to 15 --><img src="https://cdn.noedgeai.com/bo_d164lj3ef24c73d1lk80_4.jpg?x=888&#x26;y=208&#x26;w=718&#x26;h=777&#x26;r=0"><p>Figure 6: Number of images available for each shape.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6：每个形状可用的图像数量。</p></div><!-- Media --><p>Quantitative evaluation. As a quantitative comparison on the quality of Pix3D and other datasets, we randomly select 25 chair and 25 sofa images from PASCAL 3D+ [64], Ob-jectNet3D [63], IKEA [38], and Pix3D. For each image, we render the projected \(2\mathrm{D}\) silhouette of the shape using its pose annotation provided by the dataset. We then manually annotate the ground truth object masks in these images, and calculate Intersection over Union (IoU) between the projections and the ground truth. For each image-shape pair, we also ask 50 AMT workers whether they think the image is picturing the \(3\mathrm{D}\) ground truth shape provided by the dataset.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>定量评估。作为对Pix3D和其他数据集质量的定量比较，我们随机选择了25张椅子和25张沙发图像，来自PASCAL 3D+ [64]、ObjectNet3D [63]、IKEA [38]和Pix3D。对于每张图像，我们使用数据集提供的姿态注释渲染形状的投影\(2\mathrm{D}\)轮廓。然后，我们手动注释这些图像中的真实对象掩码，并计算投影与真实值之间的交并比（IoU）。对于每个图像-形状对，我们还询问50名AMT工作者他们是否认为图像描绘了数据集提供的\(3\mathrm{D}\)真实形状。</p></div><p>From Table 1, we see that Pix3D has much higher IoUs than PASCAL 3D+ and ObjectNet3D, and slightly higher IoUs compared with the IKEA dataset. Humans also feel IKEA and Pix3D have matched images and shapes, but not PASCAL 3D+ or ObjectNet3D. In addition, we observe that many CAD models in the IKEA dataset are of an incorrect scale, making it challenging to align the shapes with images. For example, there are only 15 unoccluded and untruncated images of sofas in IKEA, while Pix3D has 1,092.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>从表1中，我们看到Pix3D的IoU远高于PASCAL 3D+和ObjectNet3D，与IKEA数据集相比也略高。人们也认为IKEA和Pix3D有匹配的图像和形状，但PASCAL 3D+或ObjectNet3D则没有。此外，我们观察到IKEA数据集中的许多CAD模型比例不正确，这使得将形状与图像对齐变得具有挑战性。例如，IKEA中只有15张未遮挡且未截断的沙发图像，而Pix3D有1,092张。</p></div><h2>5. Metrics</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>5. 指标</h2></div><p>Designing a good evaluation metric is important to encourage researchers to design algorithms that reconstruct high-quality 3D geometry, rather than low-quality 3D reconstruction that overfits to a certain metric.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>设计一个良好的评估指标对于鼓励研究人员设计能够重建高质量3D几何形状的算法非常重要，而不是过度拟合某一指标的低质量3D重建。</p></div><!-- Media --><!-- figureText: 0.8 \( \rho  = {0.518} \) 0.1 EMD 0.2 0.3 0.4 3.5 4.5 5.5 1.5 2 2.5 3 3.5 4 4.5 5 5.5 Human Score Human Score \( \rho  = {0.371} \) \( \rho  = {0.544} \) 0.6 0.1 0.4 CD 0.2 0.3 0.2 0.4 2.5 4.5 5.5 1.5 2 2.5 3 Human Score --><img src="https://cdn.noedgeai.com/bo_d164lj3ef24c73d1lk80_5.jpg?x=135&#x26;y=201&#x26;w=1475&#x26;h=281&#x26;r=0"><p>Figure 7: Scatter plots between humans’ ratings of reconstructed shapes and their IoU, CD, and EMD. The three metrics have a Pearson's coefficient of 0.371, 0.544, and 0.518, respectively.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图7：人类对重建形状的评分与其IoU、CD和EMD之间的散点图。这三个指标的皮尔逊相关系数分别为0.371、0.544和0.518。</p></div><table><tbody><tr><td rowspan="2"></td><td colspan="2">Chairs</td><td colspan="2">Sofas</td></tr><tr><td>IoU</td><td>Match?</td><td>IoU</td><td>Match?</td></tr><tr><td>PASCAL 3D+ [64]</td><td>0.514</td><td>0.00</td><td>0.813</td><td>0.00</td></tr><tr><td>ObjectNet3D [63]</td><td>0.570</td><td>0.16</td><td>0.773</td><td>0.08</td></tr><tr><td>IKEA [38]</td><td>0.748</td><td>1.00</td><td>0.918</td><td>1.00</td></tr><tr><td>Pix3D (ours)</td><td>0.835</td><td>1.00</td><td>0.926</td><td>1.00</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2"></td><td colspan="2">椅子</td><td colspan="2">沙发</td></tr><tr><td>交并比（IoU）</td><td>匹配？</td><td>交并比（IoU）</td><td>匹配？</td></tr><tr><td>PASCAL 3D+ [64]</td><td>0.514</td><td>0.00</td><td>0.813</td><td>0.00</td></tr><tr><td>ObjectNet3D [63]</td><td>0.570</td><td>0.16</td><td>0.773</td><td>0.08</td></tr><tr><td>IKEA [38]</td><td>0.748</td><td>1.00</td><td>0.918</td><td>1.00</td></tr><tr><td>Pix3D（我们的）</td><td>0.835</td><td>1.00</td><td>0.926</td><td>1.00</td></tr></tbody></table></div><p>Table 1: We compute the Intersection over Union (IoU) between manually annotated \(2\mathrm{D}\) masks and the projections of \(3\mathrm{D}\) shapes. We also ask humans to judge whether the object in the images matches the provided shape.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表1：我们计算手动标注的\(2\mathrm{D}\)掩膜与\(3\mathrm{D}\)形状的投影之间的交并比（IoU）。我们还请人类判断图像中的物体是否与提供的形状匹配。</p></div><!-- Media --><p>Many 3D reconstruction papers use Intersection over Union (IoU) to evaluate the similarity between ground truth and reconstructed \(3\mathrm{D}\) voxels,which may significantly deviate from human perception. In contrast, metrics like shortest distance and geodesic distance are more commonly used than IoU for matching meshes in graphics [31, 25]. Here, we conduct behavioral studies to calibrate IoU, Chamfer distance (CD) [2], and Earth Mover's distance (EMD) [45] on how well they reflect human perception.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>许多3D重建论文使用交并比（IoU）来评估真实值与重建的\(3\mathrm{D}\)体素之间的相似性，这可能与人类感知有显著偏差。相比之下，最短距离和测地距离等指标在图形中匹配网格时比IoU更常用[31, 25]。在这里，我们进行行为研究，以校准IoU、Chamfer距离（CD）[2]和地球搬运工距离（EMD）[45]，以评估它们在多大程度上反映人类感知。</p></div><h3>5.1. Definitions</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.1. 定义</h3></div><p>The definition of IoU is straightforward. For Chamfer distance (CD) and Earth Mover's distance (EMD), we first convert voxels to point clouds, and then compute CD and EMD between pairs of point clouds.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>IoU的定义很简单。对于Chamfer距离（CD）和地球搬运工距离（EMD），我们首先将体素转换为点云，然后计算点云对之间的CD和EMD。</p></div><p>Voxels to a point cloud. We first extract the isosurface of each predicted voxel using the Lewiner marching cubes [37] algorithm. In practice, we use 0.1 as a universal surface value for extraction. We then uniformly sample points on the surface meshes and create the densely sampled point clouds. Finally, we randomly sample 1,024 points from each point cloud and normalize them into a unit cube for distance calculation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>体素到点云。我们首先使用Lewiner行进立方体[37]算法提取每个预测体素的等值面。在实践中，我们使用0.1作为提取的通用表面值。然后，我们在表面网格上均匀采样点，并创建密集采样的点云。最后，我们从每个点云中随机采样1,024个点，并将其归一化为单位立方体以进行距离计算。</p></div><p>Chamfer distance (CD). The Chamfer distance (CD) between \({S}_{1},{S}_{2} \subseteq  {\mathbb{R}}^{3}\) is defined as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Chamfer距离（CD）。\({S}_{1},{S}_{2} \subseteq  {\mathbb{R}}^{3}\)之间的Chamfer距离（CD）定义为</p></div><p></p>\[\mathrm{{CD}}\left( {{S}_{1},{S}_{2}}\right)  = \frac{1}{\left| {S}_{1}\right| }\mathop{\sum }\limits_{{x \in  {S}_{1}}}\mathop{\min }\limits_{{y \in  {S}_{2}}}\parallel x - y{\parallel }_{2} + \frac{1}{\left| {S}_{2}\right| }\mathop{\sum }\limits_{{y \in  {S}_{2}}}\mathop{\min }\limits_{{x \in  {S}_{1}}}\parallel x - y{\parallel }_{2}.\]<p></p><p>(3)</p><p>For each point in each cloud, CD finds the nearest point in the other point set, and sums the distances up. CD has been used in recent shape retrieval challenges [67].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>对于每个点云中的每个点，CD找到另一个点集中的最近点，并将距离相加。CD已在最近的形状检索挑战中使用[67]。</p></div><!-- Media --><table><tbody><tr><td></td><td>IoU</td><td>EMD</td><td>CD</td><td>Human</td></tr><tr><td>IoU</td><td>1</td><td>0.55</td><td>0.60</td><td>0.32</td></tr><tr><td>EMD</td><td>0.55</td><td>1</td><td>0.78</td><td>0.43</td></tr><tr><td>CD</td><td>0.60</td><td>0.78</td><td>1</td><td>0.49</td></tr><tr><td>Human</td><td>0.32</td><td>0.43</td><td>0.49</td><td>1</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td></td><td>交并比</td><td>地球移动距离</td><td>中心距离</td><td>人类</td></tr><tr><td>交并比</td><td>1</td><td>0.55</td><td>0.60</td><td>0.32</td></tr><tr><td>地球移动距离</td><td>0.55</td><td>1</td><td>0.78</td><td>0.43</td></tr><tr><td>中心距离</td><td>0.60</td><td>0.78</td><td>1</td><td>0.49</td></tr><tr><td>人类</td><td>0.32</td><td>0.43</td><td>0.49</td><td>1</td></tr></tbody></table></div><p>Table 2: Spearman's rank correlation coefficients between different metrics. IoU, EMD, and CD have a correlation coefficient of 0.32 , 0.43 , and 0.49 with human judgments, respectively.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表2：不同指标之间的斯皮尔曼等级相关系数。IoU、EMD和CD与人类判断的相关系数分别为0.32、0.43和0.49。</p></div><!-- Media --><p>Earth Mover's distance (EMD). We follow the definition of EMD in Fan et al. [12]. The Earth Mover's distance (EMD) between \({S}_{1},{S}_{2} \subseteq  {\mathbb{R}}^{3}\) (of equal size,i.e., \(\left| {S}_{1}\right|  = \left| {S}_{2}\right|\) ) is</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>地球搬运工距离（EMD）。我们遵循Fan等人[12]对EMD的定义。地球搬运工距离（EMD）在\({S}_{1},{S}_{2} \subseteq  {\mathbb{R}}^{3}\)（相同大小，即\(\left| {S}_{1}\right|  = \left| {S}_{2}\right|\)）之间是</p></div><p></p>\[\operatorname{EMD}\left( {{S}_{1},{S}_{2}}\right)  = \frac{1}{\left| {S}_{1}\right| }\mathop{\min }\limits_{{\phi  : {S}_{1} \rightarrow  {S}_{2}}}\mathop{\sum }\limits_{{x \in  {S}_{1}}}\parallel x - \phi \left( x\right) {\parallel }_{2}, \tag{4}\]<p></p><p>where \(\phi  : {S}_{1} \rightarrow  {S}_{2}\) is a bijection. We divide EMD by the size of the point cloud for normalization. In practice, calculating the exact EMD value is computationally expensive; we instead use a \(\left( {1 + \epsilon }\right)\) approximation algorithm [3].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(\phi  : {S}_{1} \rightarrow  {S}_{2}\)是一个双射。我们将EMD除以点云的大小进行归一化。在实践中，计算确切的EMD值计算量大；我们改为使用\(\left( {1 + \epsilon }\right)\)近似算法[3]。</p></div><h3>5.2. Experiments</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.2. 实验</h3></div><p>We then conduct two user studies to compare these metrics and benchmark how they capture human perception.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>然后我们进行两项用户研究，以比较这些指标并基准测试它们如何捕捉人类感知。</p></div><p>Which one looks better? We run three shape reconstructions algorithms (3D-R2N2 [9], DRC [59], and 3D-VAE-GAN [62]) on 200 randomly selected images of chairs. We then, for each image and every pair of its three constructions, ask three AMT workers to choose the one that looks closer to the object in the image. We also compute how each pair of objects rank in each metric. Finally, we calculate the Spearman's rank correlation coefficients between different metrics (i.e., IoU, EMD, CD, and human perception). Table 2 suggests that EMD and CD correlate better with human ratings.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>哪个看起来更好？我们在200张随机选择的椅子图像上运行三种形状重建算法（3D-R2N2 [9]、DRC [59]和3D-VAE-GAN [62]）。然后，对于每张图像及其三种重建的每一对，我们请三位AMT工作者选择看起来更接近图像中物体的重建。我们还计算每对物体在每个指标中的排名。最后，我们计算不同指标（即IoU、EMD、CD和人类感知）之间的斯皮尔曼等级相关系数。表2表明，EMD和CD与人类评分的相关性更好。</p></div><p>How good is it? We randomly select 400 images, and show each of them to 15 AMT workers, together with the voxel prediction by DRC [59] and the ground truth shape. We then ask them to rate the reconstruction, on a scale of 1 to 7 , based on how similar it is to the ground truth. The scatter plot in Figure 7 suggests that CD and EMD have higher Pearson's coefficients with human responses.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>它有多好？我们随机选择400张图像，并将每张图像展示给15位AMT工作者，连同DRC [59]的体素预测和真实形状。然后我们请他们根据重建与真实形状的相似程度，按1到7的评分标准进行评分。图7中的散点图表明，CD和EMD与人类反应的皮尔逊系数更高。</p></div><!-- Media --><!-- figureText: Image Predicted Voxels (2 Views) Image Predicted Voxels (2 Views) Image Predicted Voxels (2 Views) --><img src="https://cdn.noedgeai.com/bo_d164lj3ef24c73d1lk80_6.jpg?x=134&#x26;y=202&#x26;w=1444&#x26;h=425&#x26;r=0"><p>Figure 8: Our 3D reconstructions of chairs on Pix3D. For each of the images, we show two views of the predicted voxels.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图8：我们在Pix3D上的椅子3D重建。对于每张图像，我们展示预测体素的两个视图。</p></div><table><tbody><tr><td></td><td>IoU</td><td>EMD</td><td>CD</td></tr><tr><td>3D-R2N2 [9]</td><td>0.136</td><td>0.211</td><td>0.239</td></tr><tr><td>PSGN [12]</td><td>N/A</td><td>0.216</td><td>0.200</td></tr><tr><td>3D-VAE-GAN [62]</td><td>0.171</td><td>0.176</td><td>0.182</td></tr><tr><td>DRC [59]</td><td>0.265</td><td>0.144</td><td>0.160</td></tr><tr><td>MarrNet* [60]</td><td>0.231</td><td>0.136</td><td>0.144</td></tr><tr><td>AtlasNet [18]</td><td>N/A</td><td>0.128</td><td>0.125</td></tr><tr><td>Ours (w/o Pose)</td><td>0.267</td><td>0.124</td><td>0.124</td></tr><tr><td>Ours (w/ Pose)</td><td>0.282</td><td>0.118</td><td>0.119</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td></td><td>交并比</td><td>地球移动距离</td><td>中心距离</td></tr><tr><td>3D-R2N2 [9]</td><td>0.136</td><td>0.211</td><td>0.239</td></tr><tr><td>PSGN [12]</td><td>不适用</td><td>0.216</td><td>0.200</td></tr><tr><td>3D-VAE-GAN [62]</td><td>0.171</td><td>0.176</td><td>0.182</td></tr><tr><td>深度回归网络 [59]</td><td>0.265</td><td>0.144</td><td>0.160</td></tr><tr><td>MarrNet* [60]</td><td>0.231</td><td>0.136</td><td>0.144</td></tr><tr><td>AtlasNet [18]</td><td>不适用</td><td>0.128</td><td>0.125</td></tr><tr><td>我们的（不带姿态）</td><td>0.267</td><td>0.124</td><td>0.124</td></tr><tr><td>我们的（带姿态）</td><td>0.282</td><td>0.118</td><td>0.119</td></tr></tbody></table></div><p>Table 3: Results of 3D shape reconstruction. Our model gets the highest IoU, EMD, and CD. We also compare our full model with a variant that does not have the view estimator. Results show that multi-task learning helps boost its performance. As MarrNet and PSGN predict viewer-centered shapes, while the other methods are object-centered, we rotate their reconstructions into the canonical view using ground truth pose annotations before evaluation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表3：3D形状重建的结果。我们的模型获得了最高的IoU、EMD和CD。我们还将完整模型与没有视图估计器的变体进行了比较。结果表明，多任务学习有助于提升其性能。由于MarrNet和PSGN预测的是以观察者为中心的形状，而其他方法则是以物体为中心，我们在评估之前使用真实姿态注释将它们的重建旋转到标准视图。</p></div><!-- Media --><h2>6. Approach</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>6. 方法</h2></div><p>Pix3D serves as a benchmark for shape modeling tasks including reconstruction, retrieval, and pose estimation. Here, we design a new model that simultaneously performs shape reconstruction and pose estimation, and evaluate it on Pix3D.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Pix3D作为形状建模任务的基准，包括重建、检索和姿态估计。在这里，我们设计了一个新的模型，同时执行形状重建和姿态估计，并在Pix3D上进行评估。</p></div><p>Our model is an extension of MarrNet [60], both of which use 2.5D sketches (the object's depth, surface normals, and silhouette) as an intermediate representation. It contains four modules: (1) a 2.5D sketch estimator that predicts the depth, surface normals, and silhouette of the object; (2) a 2.5D sketch encoder that encodes the \({2.5}\mathrm{D}\) sketches into a low-dimensional latent vector; (3) a 3D shape decoder and (4) a view estimator that decodes a latent vector into a 3D shape and camera parameters, respectively. Different from Marr-Net [60], our model has an additional branch for pose estimation. We briefly describe them below, and please refer to the supplementary material for more details.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的模型是MarrNet [60] 的扩展，二者都使用2.5D草图（物体的深度、表面法线和轮廓）作为中间表示。它包含四个模块：（1）一个2.5D草图估计器，预测物体的深度、表面法线和轮廓；（2）一个2.5D草图编码器，将\({2.5}\mathrm{D}\)草图编码为低维潜在向量；（3）一个3D形状解码器和（4）一个视图估计器，分别将潜在向量解码为3D形状和相机参数。与Marr-Net [60]不同，我们的模型有一个额外的分支用于姿态估计。我们在下面简要描述它们，更多细节请参见补充材料。</p></div><p>2.5D sketch estimator. The first module takes an RGB image as input and predicts the object's 2.5D sketches (its depth, surface normals, and silhouette). We use an encoder-decoder network. The encoder is based on a ResNet-18 [22] and turns a \({256} \times  {256}\) image into 384 feature maps of size \({16} \times  {16}\) ; the decoder has three branches for depth, surface normals, and silhouette,respectively,each consisting of four sets of \(5 \times  5\) transposed convolutional, batch normalization and ReLU layers,followed by one \(5 \times  5\) convolutional layer. All output sketches are of size \({256} \times  {256}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>2.5D草图估计器。第一个模块以RGB图像作为输入，预测物体的2.5D草图（其深度、表面法线和轮廓）。我们使用一个编码-解码网络。编码器基于ResNet-18 [22]，将\({256} \times  {256}\)图像转换为384个大小为\({16} \times  {16}\)的特征图；解码器有三个分支，分别用于深度、表面法线和轮廓，每个分支由四组\(5 \times  5\)转置卷积、批归一化和ReLU层组成，最后是一个\(5 \times  5\)卷积层。所有输出草图的大小为\({256} \times  {256}\)。</p></div><p>2.5D sketch encoder. We use a modified ResNet-18 [22] that takes a four-channel image (three for surface normals and one for depth). Each channel is masked by the predicted silhouette. A final linear layer outputs a 200-D latent vector.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>2.5D草图编码器。我们使用一个修改过的ResNet-18 [22]，它接受一个四通道图像（三个用于表面法线，一个用于深度）。每个通道都被预测的轮廓遮罩。最后的线性层输出一个200维的潜在向量。</p></div><p>3D shape decoder. Our 3D shape decoder has five sets of \(4 \times  4 \times  4\) transposed convolutional,batch-norm,and ReLU layers,followed by a \(4 \times  4 \times  4\) transposed convolutional layer. It outputs a voxelized shape of size \({128} \times  {128} \times  {128}\) in the object's canonical view.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D形状解码器。我们的3D形状解码器有五组\(4 \times  4 \times  4\)转置卷积、批归一化和ReLU层，后面跟着一个\(4 \times  4 \times  4\)转置卷积层。它输出一个大小为\({128} \times  {128} \times  {128}\)的体素化形状，位于物体的标准视图中。</p></div><p>View estimator. The view estimator contains three sets of linear, batch normalization, and ReLU layers, followed by two parallel linear and softmax layers that predict the shape's azimuth and elevation, respectively. Here, we treat pose estimation as a classification problem, where the 360-degree azimuth angle is divided into 24 bins and the 180-degree elevation angle is divided into 12 bins.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>视图估计器。视图估计器包含三组线性、批归一化和ReLU层，后面跟着两个并行的线性和softmax层，分别预测形状的方位角和仰角。在这里，我们将姿态估计视为一个分类问题，其中360度的方位角被划分为24个区间，180度的仰角被划分为12个区间。</p></div><p>Training paradigm. For training, we use Mitsuba [26] to render each chair in ShapeNet [7] from 20 random views using three types of backgrounds: \(1/3\) on a white background, \(1/3\) on high-dynamic-range backgrounds with illumination channels,and \(1/3\) on backgrounds randomly sampled from the SUN database [65]. We augment our training data by random color and light jittering.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>训练范式。为了训练，我们使用Mitsuba [26] 从20个随机视角渲染ShapeNet [7] 中的每把椅子，使用三种类型的背景：\(1/3\)在白色背景上，\(1/3\)在具有照明通道的高动态范围背景上，以及\(1/3\)在从SUN数据库 [65] 随机采样的背景上。我们通过随机颜色和光照抖动来增强训练数据。</p></div><p>We first train the 2.5D sketch estimator. We then train the 2.5D sketch encoder and the 3D shape decoder (and the view estimator if we're predicting the pose) jointly. We finally concatenate them for prediction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们首先训练2.5D草图估计器。然后我们联合训练2.5D草图编码器和3D形状解码器（如果我们预测姿态，则包括视图估计器）。最后，我们将它们连接起来进行预测。</p></div><h2>7. Experiments</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>7. 实验</h2></div><p>We now evaluate our model and state-of-the-art algorithms on single-image 3D shape reconstruction, retrieval, and pose estimation, all using Pix3D. For all experiments, we use the 2,894 untruncated and unoccluded chair images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们现在在单图像3D形状重建、检索和姿态估计上评估我们的模型和最先进的算法，所有实验均使用Pix3D。对于所有实验，我们使用2,894张未截断和未遮挡的椅子图像。</p></div><p>3D shape reconstruction. We compare our model, with and without the pose estimation branch, with the state-of-the-art systems, including 3D-VAE-GAN [62], 3D-R2N2 [9],</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D形状重建。我们将我们的模型（有和没有姿态估计分支）与最先进的系统进行比较，包括3D-VAE-GAN [62]、3D-R2N2 [9]，</p></div><!-- Media --><!-- figureText: Jule Ours (w/ Pose) Ours (w/o Pose) Top-8 Retrieval Results Query --><img src="https://cdn.noedgeai.com/bo_d164lj3ef24c73d1lk80_7.jpg?x=140&#x26;y=204&#x26;w=1463&#x26;h=368&#x26;r=0"><p>Figure 9: Our shape retrievals of chairs on Pix3D. Here we show top-8 retrieval results from our proposed method (both with pose estimation version and without pose estimation version). The with pose estimation version tends to retrieval images with a similar pose.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图9：我们在Pix3D上对椅子的形状检索。这里展示了我们提出的方法的前8个检索结果（包括姿态估计版本和不包括姿态估计版本）。带有姿态估计的版本倾向于检索具有相似姿态的图像。</p></div><!-- figureText: Image Estimated Pose Image Estimated Pose Image Estimated Pose Image Estimated Pose (Only Azimuth and Elevation) (Only Azimuth and Elevation) (Only Azimuth and Elevation) (Only Azimuth and Elevation) --><img src="https://cdn.noedgeai.com/bo_d164lj3ef24c73d1lk80_7.jpg?x=140&#x26;y=643&#x26;w=1459&#x26;h=274&#x26;r=0"><p>Figure 10: Our pose estimations of chairs on Pix3D. For each of the images, our method only predicts azimuth and elevation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图10：我们在Pix3D上对椅子的姿态估计。对于每张图像，我们的方法仅预测方位角和仰角。</p></div><table><tbody><tr><td></td><td>R@1</td><td>R@2</td><td>R@4</td><td>R@8</td><td>R@16</td><td>R@32</td></tr><tr><td>3D-VAE-GAN [62]</td><td>0.02</td><td>0.03</td><td>0.07</td><td>0.12</td><td>0.21</td><td>0.34</td></tr><tr><td>MarrNet [60]</td><td>0.42</td><td>0.51</td><td>0.57</td><td>0.64</td><td>0.71</td><td>0.78</td></tr><tr><td>Ours (w/ Pose)</td><td>0.42</td><td>0.48</td><td>0.55</td><td>0.63</td><td>0.70</td><td>0.76</td></tr><tr><td>Ours (w/o Pose)</td><td>0.53</td><td>0.62</td><td>0.71</td><td>0.78</td><td>0.85</td><td>0.90</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td></td><td>R@1</td><td>R@2</td><td>R@4</td><td>R@8</td><td>R@16</td><td>R@32</td></tr><tr><td>3D-VAE-GAN [62]</td><td>0.02</td><td>0.03</td><td>0.07</td><td>0.12</td><td>0.21</td><td>0.34</td></tr><tr><td>MarrNet [60]</td><td>0.42</td><td>0.51</td><td>0.57</td><td>0.64</td><td>0.71</td><td>0.78</td></tr><tr><td>我们的（带姿态）</td><td>0.42</td><td>0.48</td><td>0.55</td><td>0.63</td><td>0.70</td><td>0.76</td></tr><tr><td>我们的（不带姿态）</td><td>0.53</td><td>0.62</td><td>0.71</td><td>0.78</td><td>0.85</td><td>0.90</td></tr></tbody></table></div><p>Table 4: Results for image-based shape retrieval, where R@K stands for <a href="mailto:Recall@K.Our">Recall@K.Our</a> model (without the pose estimation module) achieves the highest numbers. Our model (with the pose estimation module) does not perform as well, because it sometimes retrieves images of objects with the same pose, but not exactly the same shape.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表4：基于图像的形状检索结果，其中R@K代表Recall@K。我们的模型（不带姿态估计模块）取得了最高的结果。我们的模型（带姿态估计模块）表现不佳，因为它有时检索到的是具有相同姿态但形状不完全相同的物体图像。</p></div><!-- Media --><p>DRC [59], and MarrNet [60]. We use pre-trained models offered by the authors and we crop the input images as required by each algorithm. The results are shown in Table 3 and Figure 8. Our model outperforms the state-of-the-arts in all metrics. Our full model gets better results compared with the variant without the view estimator, suggesting multi-task learning helps to boost its performance. Also note the discrepancy among metrics: MarrNet has a lower IoU than DRC, but according to EMD and CD, it performs better.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>DRC [59]和MarrNet [60]。我们使用作者提供的预训练模型，并根据每个算法的要求裁剪输入图像。结果如表3和图8所示。我们的模型在所有指标上都优于现有技术。与不带视图估计器的变体相比，我们的完整模型获得了更好的结果，这表明多任务学习有助于提升其性能。同时注意指标之间的差异：MarrNet的IoU低于DRC，但根据EMD和CD，它的表现更好。</p></div><p>Image-based, fine-grained shape retrieval. For shape retrieval, we compare our model with 3D-VAE-GAN [62] and MarrNet [60]. We use the latent vector from each algorithm as its embedding of the input image, and use L2 distance for image retrieval. For each test image,we retrieve its \(\mathrm{K}\) nearest neighbors from the test set, and use Recall@K [28] to compute how many retrieved images are actually depicting the same shape. Here we do not consider images whose shape is not captured by any other images in the test set. The results are shown in Table 4 and Figure 9. Our model (without the pose estimation module) achieves the highest numbers; our model (with the pose estimation module) does not perform as well, because it sometimes retrieves images of objects with the same pose, but not exactly the same shape.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于图像的细粒度形状检索。对于形状检索，我们将我们的模型与3D-VAE-GAN [62]和MarrNet [60]进行比较。我们使用每个算法的潜在向量作为输入图像的嵌入，并使用L2距离进行图像检索。对于每个测试图像，我们从测试集中检索其\(\mathrm{K}\)个最近邻，并使用Recall@K [28]计算检索到的图像中有多少实际上描绘了相同的形状。在这里，我们不考虑形状未被测试集中任何其他图像捕获的图像。结果如表4和图9所示。我们的模型（不带姿态估计模块）取得了最高的结果；我们的模型（带姿态估计模块）表现不佳，因为它有时检索到的是具有相同姿态但形状不完全相同的物体图像。</p></div><!-- Media --><table><tbody><tr><td rowspan="2">#of views</td><td colspan="4">Azimuth</td><td colspan="3">Elevation</td></tr><tr><td>4</td><td>8</td><td>12</td><td>24</td><td>4</td><td>6</td><td>12</td></tr><tr><td>Render for CNN</td><td>0.71</td><td>0.63</td><td>0.56</td><td>0.40</td><td>0.57</td><td>0.56</td><td>0.37</td></tr><tr><td>Ours</td><td>0.76</td><td>0.73</td><td>0.61</td><td>0.49</td><td>0.87</td><td>0.70</td><td>0.61</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">观看次数</td><td colspan="4">方位角</td><td colspan="3">仰角</td></tr><tr><td>4</td><td>8</td><td>12</td><td>24</td><td>4</td><td>6</td><td>12</td></tr><tr><td>为CNN渲染</td><td>0.71</td><td>0.63</td><td>0.56</td><td>0.40</td><td>0.57</td><td>0.56</td><td>0.37</td></tr><tr><td>我们的</td><td>0.76</td><td>0.73</td><td>0.61</td><td>0.49</td><td>0.87</td><td>0.70</td><td>0.61</td></tr></tbody></table></div><p>Table 5: Results of 3D pose estimation. Our model outperforms Render for CNN [55] in both azimuth and elevation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表5：3D姿态估计的结果。我们的模型在方位角和仰角上均优于Render for CNN [55]。</p></div><!-- Media --><p>3D pose estimation. We compare our method with Render for CNN [55]. We calculate the classification accuracy for both azimuth and elevation, where the azimuth is divided into 24 bins and the elevation into 12 bins. Table 5 suggests that our model outperforms Render for CNN in pose estimation. Qualitative results are included in Figure 10.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D姿态估计。我们将我们的方法与Render for CNN [55]进行了比较。我们计算了方位角和仰角的分类准确率，其中方位角分为24个区间，仰角分为12个区间。表5表明我们的模型在姿态估计上优于Render for CNN。定性结果见图10。</p></div><h2>8. Conclusion</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>8. 结论</h2></div><p>We have presented Pix3D, a large-scale dataset of well-aligned 2D images and 3D shapes. We have also explored how three commonly used metrics correspond to human perception through two behavioral studies and proposed a new model that simultaneously performs shape reconstruction and pose estimation. Experiments showed that our model achieved state-of-the-art performance on \(3\mathrm{D}\) reconstruction,shape retrieval, and pose estimation. We hope our paper will inspire future research in single-image 3D shape modeling.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们提出了Pix3D，这是一个大规模的对齐良好的2D图像和3D形状数据集。我们还通过两项行为研究探讨了三种常用指标与人类感知之间的关系，并提出了一种同时进行形状重建和姿态估计的新模型。实验表明，我们的模型在\(3\mathrm{D}\)重建、形状检索和姿态估计上达到了最先进的性能。我们希望我们的论文能激励未来在单图像3D形状建模方面的研究。</p></div><p>Acknowledgements. This work is supported by NSF #1212849 and #1447476, ONR MURI N00014-16-1-2007, the Center for Brain, Minds and Machines (NSF STC award CCF-1231216), the Toyota Research Institute, and Shell Research. J. Wu is supported by a Facebook fellowship. References</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>致谢。本工作得到了NSF #1212849和#1447476、ONR MURI N00014-16-1-2007、脑、心智与机器中心（NSF STC奖 CCF-1231216）、丰田研究院和壳牌研究的支持。J. Wu获得了Facebook奖学金。参考文献</p></div><p>[1] M. Aubry, D. Maturana, A. Efros, B. Russell, and J. Sivic. Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models. In \({CVPR},{2014.3}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[1] M. Aubry, D. Maturana, A. Efros, B. Russell, 和 J. Sivic. 看到3D椅子：使用大型CAD模型数据集的示例部件基础2D-3D对齐。在\({CVPR},{2014.3}\)</p></div><p>[2] H. G. Barrow, J. M. Tenenbaum, R. C. Bolles, and H. C. Wolf. Parametric correspondence and chamfer matching: Two new techniques for image matching. In IJCAI, 1977. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[2] H. G. Barrow, J. M. Tenenbaum, R. C. Bolles, 和 H. C. Wolf. 参数对应和切线匹配：两种新的图像匹配技术。在IJCAI，1977年。6</p></div><p>[3] D. P. Bertsekas. A distributed asynchronous relaxation algorithm for the assignment problem. In \({CDC},{1985.6}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[3] D. P. Bertsekas. 一种用于分配问题的分布式异步松弛算法。在\({CDC},{1985.6}\)</p></div><p>[4] F. Bogo, J. Romero, M. Loper, and M. J. Black. Faust: Dataset and evaluation for \(3\mathrm{\;d}\) mesh registration. In \({CVPR},{2014}\) . 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[4] F. Bogo, J. Romero, M. Loper, 和 M. J. Black. Faust：\(3\mathrm{\;d}\)网格配准的数据集和评估。在\({CVPR},{2014}\)。2</p></div><p>[5] A. M. Bronstein, M. M. Bronstein, and R. Kimmel. Numerical geometry of non-rigid shapes. Springer Science &#x26; Business Media, 2008. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[5] A. M. Bronstein, M. M. Bronstein, 和 R. Kimmel. 非刚性形状的数值几何。施普林格科学与商业媒体，2008年。2</p></div><p>[6] B. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel, and A. M. Dollar. Benchmarking in manipulation research: Using the yale-cmu-berkeley object and model set. IEEE RAM, 22(3):36-52, 2015. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[6] B. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel, 和 A. M. Dollar. 操作研究中的基准测试：使用耶鲁-卡内基梅隆-伯克利物体和模型集。IEEE RAM, 22(3):36-52, 2015年。2</p></div><p>[7] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv:1512.03012, 2015.1,2,7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[7] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, 等. Shapenet：一个信息丰富的3D模型库。arXiv:1512.03012, 2015年。1,2,7</p></div><p>[8] S. Choi, Q.-Y. Zhou, S. Miller, and V. Koltun. A large dataset of object scans. arXiv:1602.02481, 2016. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[8] S. Choi, Q.-Y. Zhou, S. Miller, 和 V. Koltun. 一个大型物体扫描数据集。arXiv:1602.02481, 2016年。2</p></div><p>[9] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In \({ECCV},{2016.2},6,7\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[9] C. B. Choy, D. Xu, J. Gwak, K. Chen, 和 S. Savarese. 3D-R2N2：单视图和多视图3D物体重建的统一方法。在\({ECCV},{2016.2},6,7\)</p></div><p>[10] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In \({CVPR},{2017.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[10] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, 和 M. Nießner. ScanNet：室内场景的丰富注释3D重建。在\({CVPR},{2017.2}\)</p></div><p>[11] A. Dosovitskiy, J. Springenberg, M. Tatarchenko, and T. Brox. Learning to generate chairs, tables and cars with convolutional networks. IEEE TPAMI, 39(4):692-705, 2017. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[11] A. Dosovitskiy, J. Springenberg, M. Tatarchenko, 和 T. Brox. 学习生成椅子、桌子和汽车的卷积网络。IEEE TPAMI, 39(4):692-705, 2017年。5</p></div><p>[12] H. Fan, H. Su, and L. Guibas. A point set generation network for \(3\mathrm{\;d}\) object reconstruction from a single image. In \({CVPR}\) , 2017.3, 6, 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[12] H. Fan, H. Su, 和 L. Guibas. 一种用于从单幅图像重建\(3\mathrm{\;d}\)对象的点集生成网络. 在\({CVPR}\), 2017.3, 6, 7</p></div><p>[13] S. Fidler, S. J. Dickinson, and R. Urtasun. 3d object detection and viewpoint estimation with a deformable \(3\mathrm{\;d}\) cuboid model. In NIPS, 2012. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[13] S. Fidler, S. J. Dickinson, 和 R. Urtasun. 使用可变形\(3\mathrm{\;d}\)长方体模型进行3D对象检测和视角估计. 在NIPS, 2012. 3</p></div><p>[14] M. Firman. Rgbd datasets: Past, present and future. In CVPR Workshop, 2016. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[14] M. Firman. RGBD数据集：过去、现在和未来. 在CVPR研讨会, 2016. 2</p></div><p>[15] M. Fisher and P. Hanrahan. Context-based search for 3d models. ACM TOG, 29(6):182, 2010. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[15] M. Fisher 和 P. Hanrahan. 基于上下文的3D模型搜索. ACM TOG, 29(6):182, 2010. 3</p></div><p>[16] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR, 2012. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[16] A. Geiger, P. Lenz, 和 R. Urtasun. 我们准备好进行自动驾驶了吗？KITTI视觉基准套件. 在CVPR, 2012. 2</p></div><p>[17] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta. Learning a predictable and generative vector representation for objects. In \({ECCV},{2016.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[17] R. Girdhar, D. F. Fouhey, M. Rodriguez, 和 A. Gupta. 学习可预测和生成的对象向量表示. 在\({ECCV},{2016.2}\)</p></div><p>[18] T. Groueix, M. Fisher, V. G. Kim, B. Russell, and M. Aubry. AtlasNet: A Papier-Mâché Approach to Learning 3D Surface Generation. In \({CVPR},{2018.7}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[18] T. Groueix, M. Fisher, V. G. Kim, B. Russell, 和 M. Aubry. AtlasNet：一种学习3D表面生成的纸浆法. 在\({CVPR},{2018.7}\)</p></div><p>[19] S. Gupta, P. Arbeláez, R. Girshick, and J. Malik. Aligning 3d models to rgb-d images of cluttered scenes. In CVPR, 2015. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[19] S. Gupta, P. Arbeláez, R. Girshick, 和 J. Malik. 将3D模型与杂乱场景的RGB-D图像对齐. 在CVPR, 2015. 3</p></div><p>[20] J. Gwak, C. B. Choy, M. Chandraker, A. Garg, and S. Savarese. Weakly supervised \(3\mathrm{\;d}\) reconstruction with adversarial constraint. In \({3DV},{2017.5}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[20] J. Gwak, C. B. Choy, M. Chandraker, A. Garg, 和 S. Savarese. 具有对抗约束的弱监督\(3\mathrm{\;d}\)重建. 在\({3DV},{2017.5}\)</p></div><p>[21] C. Häne, S. Tulsiani, and J. Malik. Hierarchical surface prediction for \(3\mathrm{\;d}\) object reconstruction. In \({3DV},{2017.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[21] C. Häne, S. Tulsiani, 和 J. Malik. 用于\(3\mathrm{\;d}\)对象重建的层次表面预测. 在\({3DV},{2017.2}\)</p></div><p>[22] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In \({CVPR},{2015.7}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[22] K. He, X. Zhang, S. Ren, 和 J. Sun. 用于图像识别的深度残差学习. 在\({CVPR},{2015.7}\)</p></div><p>[23] T. Hodan, P. Haluza, Š. Obdržálek, J. Matas, M. Lourakis, and X. Zabulis. T-less: An rgb-d dataset for 6d pose estimation of texture-less objects. In \({WACV},{2017.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[23] T. Hodan, P. Haluza, Š. Obdržálek, J. Matas, M. Lourakis, 和 X. Zabulis. T-less：一个用于无纹理对象6D姿态估计的RGB-D数据集. 在\({WACV},{2017.2}\)</p></div><p>[24] S. Izadi, D. Kim, O. Hilliges, D. Molyneaux, R. A. Newcombe, P. Kohli, J. Shotton, S. Hodges, D. Freeman, A. J. Davison, and A. W. Fitzgibbon. Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera. In J. S. Pierce, M. Agrawala, and S. R. Klemmer, editors, UIST, 2011. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[24] S. Izadi, D. Kim, O. Hilliges, D. Molyneaux, R. A. Newcombe, P. Kohli, J. Shotton, S. Hodges, D. Freeman, A. J. Davison, 和 A. W. Fitzgibbon. Kinectfusion：使用移动深度相机进行实时3D重建和交互. 在J. S. Pierce, M. Agrawala, 和 S. R. Klemmer, 编辑, UIST, 2011. 2</p></div><p>[25] V. Jain and H. Zhang. Robust 3d shape correspondence in the spectral domain. In Shape Modeling and Applications, 2006. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[25] V. Jain 和 H. Zhang. 在谱域中进行鲁棒的3D形状对应. 在形状建模与应用, 2006. 6</p></div><p>[26] W. Jakob. Mitsuba renderer, 2010. <a href="http://www.mitsuba-renderer.org.7">http://www.mitsuba-renderer.org.7</a></p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[26] W. Jakob. Mitsuba渲染器, 2010. <a href="http://www.mitsuba-renderer.org.7">http://www.mitsuba-renderer.org.7</a></p></div><p>[27] A. Janoch, S. Karayev, Y. Jia, J. T. Barron, M. Fritz, K. Saenko, and T. Darrell. A category-level 3-d object dataset: Putting the kinect to work. In ICCV Workshop, 2011. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[27] A. Janoch, S. Karayev, Y. Jia, J. T. Barron, M. Fritz, K. Saenko, 和 T. Darrell. 一个类别级别的3D对象数据集：让Kinect发挥作用. 在ICCV研讨会, 2011. 2</p></div><p>[28] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE TPAMI, 33(1):117-128, 2011. 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[28] H. Jegou, M. Douze, 和 C. Schmid. 用于最近邻搜索的产品量化. IEEE TPAMI, 33(1):117-128, 2011. 8</p></div><p>[29] A. Kar, S. Tulsiani, J. Carreira, and J. Malik. Category-specific object reconstruction from a single image. In CVPR, 2015. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[29] A. Kar, S. Tulsiani, J. Carreira, 和 J. Malik. 从单张图像重建特定类别的物体. 在 CVPR, 2015. 2</p></div><p>[30] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM TOG, 36(4):78, 2017. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[30] A. Knapitsch, J. Park, Q.-Y. Zhou, 和 V. Koltun. 坦克与寺庙：大规模场景重建基准测试. ACM TOG, 36(4):78, 2017. 2</p></div><p>[31] V. Kreavoy, D. Julius, and A. Sheffer. Model composition from interchangeable components. In \({PG},{2007.6}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[31] V. Kreavoy, D. Julius, 和 A. Sheffer. 从可互换组件中组合模型. 在 \({PG},{2007.6}\)</p></div><p>[32] Z. Lahner, E. Rodola, F. R. Schmidt, M. M. Bronstein, and D. Cremers. Efficient globally optimal 2d-to-3d deformable shape matching. In \({CVPR},{2016.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[32] Z. Lahner, E. Rodola, F. R. Schmidt, M. M. Bronstein, 和 D. Cremers. 高效的全局最优2D到3D可变形形状匹配. 在 \({CVPR},{2016.2}\)</p></div><p>[33] K. Lai, L. Bo, X. Ren, and D. Fox. A large-scale hierarchical multi-view rgb-d object dataset. In ICRA, 2011. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[33] K. Lai, L. Bo, X. Ren, 和 D. Fox. 大规模分层多视角RGB-D物体数据集. 在 ICRA, 2011. 2</p></div><p>[34] B. Leibe and B. Schiele. Analyzing appearance and contour based methods for object categorization. In \({CVPR},{2003.2},3\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[34] B. Leibe 和 B. Schiele. 分析基于外观和轮廓的方法用于物体分类. 在 \({CVPR},{2003.2},3\)</p></div><p>[35] V. Lepetit, F. Moreno-Noguer, and P. Fua. Epnp: An accurate o (n) solution to the pnp problem. IJCV, 81(2):155-166, 2009. 4</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[35] V. Lepetit, F. Moreno-Noguer, 和 P. Fua. Epnp：对PNP问题的准确O(n)解. IJCV, 81(2):155-166, 2009. 4</p></div><p>[36] K. Levenberg. A method for the solution of certain non-linear problems in least squares. Quarterly of applied mathematics, 2(2):164-168, 1944. 4, 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[36] K. Levenberg. 一种解决某些非线性最小二乘问题的方法. 应用数学季刊, 2(2):164-168, 1944. 4, 5</p></div><p>[37] T. Lewiner, H. Lopes, A. W. Vieira, and G. Tavares. Efficient implementation of marching cubes' cases with topological guarantees. Journal of Graphics Tools, 8(2):1-15, 2003. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[37] T. Lewiner, H. Lopes, A. W. Vieira, 和 G. Tavares. 具有拓扑保证的高效实现行进立方体案例. 图形工具杂志, 8(2):1-15, 2003. 6</p></div><p>[38] J. J. Lim, H. Pirsiavash, and A. Torralba. Parsing ikea objects: Fine pose estimation. In \({ICCV},{2013.1},2,3,4,5,6\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[38] J. J. Lim, H. Pirsiavash, 和 A. Torralba. 解析宜家物体：精确姿态估计. 在 \({ICCV},{2013.1},2,3,4,5,6\)</p></div><p>[39] D. W. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal of the society for Industrial and Applied Mathematics, 11(2):431-441, 1963. 4</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[39] D. W. Marquardt. 一种用于非线性参数最小二乘估计的算法. 工业与应用数学学会杂志, 11(2):431-441, 1963. 4</p></div><p>[40] J. McCormac, A. Handa, S. Leutenegger, and A. J. Davison. Scenenet rgb-d: Can 5m synthetic images beat generic ima-genet pre-training on indoor segmentation? In \({ICCV},{2017}\) . 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[40] J. McCormac, A. Handa, S. Leutenegger, 和 A. J. Davison. Scenenet RGB-D：500万合成图像能否在室内分割上超越通用图像生成预训练？在 \({ICCV},{2017}\) . 2</p></div><p>[41] D. Novotny, D. Larlus, and A. Vedaldi. Learning 3d object categories by looking around them. In \({ICCV},{2017.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[41] D. Novotny, D. Larlus, 和 A. Vedaldi. 通过观察周围物体学习3D物体类别. 在 \({ICCV},{2017.2}\)</p></div><p>[42] M. Ozuysal, V. Lepetit, and P. Fua. Pose estimation for category specific multiview object localization. In CVPR, 2009. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[42] M. Ozuysal, V. Lepetit, 和 P. Fua. 针对特定类别的多视角物体定位的姿态估计. 在 CVPR, 2009. 3</p></div><p>[43] D. J. Rezende, S. Eslami, S. Mohamed, P. Battaglia, M. Jader-berg,and \(\mathrm{N}\) . Heess. Unsupervised learning of \(3\mathrm{\;d}\) structure from images. In NIPS, 2016. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[43] D. J. Rezende, S. Eslami, S. Mohamed, P. Battaglia, M. Jader-berg, 和 \(\mathrm{N}\) . Heess. 从图像中无监督学习\(3\mathrm{\;d}\)结构. 在 NIPS, 2016. 2</p></div><p>[44] G. Riegler, A. O. Ulusoys, and A. Geiger. Octnet: Learning deep 3d representations at high resolutions. In \({CVPR},{2016.3}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[44] G. Riegler, A. O. Ulusoys, 和 A. Geiger. Octnet: 在高分辨率下学习深度3D表示. 在 \({CVPR},{2016.3}\)</p></div><p>[45] Y. Rubner, C. Tomasi, and L. J. Guibas. The earth mover's distance as a metric for image retrieval. IJCV, 40(2):99-121, 2000. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[45] Y. Rubner, C. Tomasi, 和 L. J. Guibas. 地球搬运工距离作为图像检索的度量. IJCV, 40(2):99-121, 2000. 6</p></div><p>[46] B. C. Russell and A. Torralba. Building a database of 3d scenes from user annotations. In \({CVPR},{2009.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[46] B. C. Russell 和 A. Torralba. 从用户注释构建3D场景数据库. 在 \({CVPR},{2009.2}\)</p></div><p>[47] S. Savarese and L. Fei-Fei. 3d generic object categorization, localization and pose estimation. In \({ICCV},{2007.2},3\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[47] S. Savarese 和 L. Fei-Fei. 3D通用物体分类、定位和姿态估计. 在 \({ICCV},{2007.2},3\)</p></div><p>[48] M. Savva, F. Yu, H. Su, M. Aono, B. Chen, D. Cohen-Or, W. Deng, H. Su, S. Bai, X. Bai, et al. Shrec17 track: large-scale \(3\mathrm{\;d}\) shape retrieval from shapenet core55. In Eurographics Workshop on 3D Object Retrieval, 2016. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[48] M. Savva, F. Yu, H. Su, M. Aono, B. Chen, D. Cohen-Or, W. Deng, H. Su, S. Bai, X. Bai, 等. Shrec17赛道: 从shapenet core55进行大规模\(3\mathrm{\;d}\)形状检索. 在2016年欧洲图形学研讨会3D物体检索中. 3</p></div><p>[49] P. Shilane, P. Min, M. Kazhdan, and T. Funkhouser. The princeton shape benchmark. In Shape Modeling Applications, 2004. 1, 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[49] P. Shilane, P. Min, M. Kazhdan, 和 T. Funkhouser. 普林斯顿形状基准. 在形状建模应用中, 2004. 1, 2</p></div><p>[50] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and support inference from rgbd images. In \({ECCV},{2012.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[50] N. Silberman, D. Hoiem, P. Kohli, 和 R. Fergus. 从RGBD图像进行室内分割和支持推断. 在 \({ECCV},{2012.2}\)</p></div><p>[51] A. Singh, J. Sha, K. S. Narayan, T. Achim, and P. Abbeel. Bigbird: A large-scale 3d database of object instances. In ICRA, 2014. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[51] A. Singh, J. Sha, K. S. Narayan, T. Achim, 和 P. Abbeel. Bigbird: 一个大规模3D物体实例数据库. 在ICRA, 2014. 2</p></div><p>[52] A. A. Soltani, H. Huang, J. Wu, T. D. Kulkarni, and J. B. Tenenbaum. Synthesizing \(3\mathrm{\;d}\) shapes via modeling multi-view depth maps and silhouettes with deep generative networks. In CVPR, 2017. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[52] A. A. Soltani, H. Huang, J. Wu, T. D. Kulkarni, 和 J. B. Tenenbaum. 通过建模多视角深度图和轮廓合成\(3\mathrm{\;d}\)形状，使用深度生成网络. 在CVPR, 2017. 2</p></div><p>[53] S. Song, S. P. Lichtenberg, and J. Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In CVPR, 2015. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[53] S. Song, S. P. Lichtenberg, 和 J. Xiao. Sun rgb-d: 一个RGB-D场景理解基准套件. 在CVPR, 2015. 2</p></div><p>[54] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser. Semantic scene completion from a single depth image. In \({CVPR},{2017.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[54] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, 和 T. Funkhouser. 从单个深度图像进行语义场景补全. 在 \({CVPR},{2017.2}\)</p></div><p>[55] H. Su, C. R. Qi, Y. Li, and L. J. Guibas. Render for cnn: Viewpoint estimation in images using cnns trained with rendered \(3\mathrm{\;d}\) model views. In \({ICCV},{2015.3},8\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[55] H. Su, C. R. Qi, Y. Li, 和 L. J. Guibas. 为CNN渲染: 使用经过渲染的\(3\mathrm{\;d}\)模型视图进行图像中的视点估计. 在 \({ICCV},{2015.3},8\)</p></div><p>[56] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Multi-view 3d models from single images with a convolutional network. In \({ECCV},{2016.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[56] M. Tatarchenko, A. Dosovitskiy, 和 T. Brox. 从单个图像生成多视角3D模型，使用卷积网络. 在 \({ECCV},{2016.2}\)</p></div><p>[57] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs. In \({ICCV},{2017.3}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[57] M. Tatarchenko, A. Dosovitskiy, 和 T. Brox. 八叉树生成网络: 高分辨率3D输出的高效卷积架构. 在 \({ICCV},{2017.3}\)</p></div><p>[58] S. Tulsiani and J. Malik. Viewpoints and keypoints. In CVPR, 2015. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[58] S. Tulsiani 和 J. Malik. 视点和关键点. 在CVPR, 2015. 3</p></div><p>[59] S. Tulsiani, T. Zhou, A. A. Efros, and J. Malik. Multi-view supervision for single-view reconstruction via differentiable ray consistency. In \({CVPR},{2017.2},5,6,7,8\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[59] S. Tulsiani, T. Zhou, A. A. Efros, 和 J. Malik. 通过可微分光线一致性进行单视图重建的多视图监督. 在 \({CVPR},{2017.2},5,6,7,8\)</p></div><p>[60] J. Wu, Y. Wang, T. Xue, X. Sun, W. T. Freeman, and J. B. Tenenbaum. MarrNet: 3D Shape Reconstruction via 2.5D Sketches. In NIPS, 2017. 2, 7, 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[60] J. Wu, Y. Wang, T. Xue, X. Sun, W. T. Freeman, 和 J. B. Tenenbaum. MarrNet: 通过2.5D草图进行3D形状重建. 在NIPS, 2017. 2, 7, 8</p></div><p>[61] J. Wu, T. Xue, J. J. Lim, Y. Tian, J. B. Tenenbaum, A. Torralba, and W. T. Freeman. Single image 3d interpreter network. In \({ECCV},{2016.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[61] J. Wu, T. Xue, J. J. Lim, Y. Tian, J. B. Tenenbaum, A. Torralba, 和 W. T. Freeman. 单幅图像3D解释网络. 在\({ECCV},{2016.2}\)</p></div><p>[62] J. Wu, C. Zhang, T. Xue, W. T. Freeman, and J. B. Tenenbaum. Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling. In NIPS, 2016. 2, 6, 7, 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[62] J. Wu, C. Zhang, T. Xue, W. T. Freeman, 和 J. B. Tenenbaum. 通过3D生成对抗建模学习物体形状的概率潜在空间. 在NIPS, 2016. 2, 6, 7, 8</p></div><p>[63] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mot-taghi, L. Guibas, and S. Savarese. Objectnet3d: A large scale database for 3d object recognition. In \({ECCV},{2016.1},2,3,5\) , 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[63] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mottaghi, L. Guibas, 和 S. Savarese. Objectnet3d: 一个大规模的3D物体识别数据库. 在\({ECCV},{2016.1},2,3,5\), 6</p></div><p>[64] Y. Xiang, R. Mottaghi, and S. Savarese. Beyond pascal: A benchmark for 3d object detection in the wild. In WACV, 2014. 1,2,3,5,6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[64] Y. Xiang, R. Mottaghi, 和 S. Savarese. 超越Pascal: 一个野外3D物体检测基准. 在WACV, 2014. 1,2,3,5,6</p></div><p>[65] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[65] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, 和 A. Torralba. Sun数据库: 从修道院到动物园的大规模场景识别. 在CVPR, 2010. 7</p></div><p>[66] X. Yan, J. Yang, E. Yumer, Y. Guo, and H. Lee. Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision. In NIPS, 2016. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[66] X. Yan, J. Yang, E. Yumer, Y. Guo, 和 H. Lee. 透视变换网络: 学习无3D监督的单视图3D物体重建. 在NIPS, 2016. 2</p></div><p>[67] L. Yi, H. Su, L. Shao, M. Savva, H. Huang, Y. Zhou, B. Graham, M. Engelcke, R. Klokov, V. Lempitsky, et al. Large-scale \(3\mathrm{\;d}\) shape reconstruction and segmentation from shapenet core55. arXiv:1710.06104, 2017. 2, 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[67] L. Yi, H. Su, L. Shao, M. Savva, H. Huang, Y. Zhou, B. Graham, M. Engelcke, R. Klokov, V. Lempitsky, 等. 从shapenet core55进行大规模\(3\mathrm{\;d}\)形状重建和分割. arXiv:1710.06104, 2017. 2, 6</p></div>
      </body>
    </html>
  