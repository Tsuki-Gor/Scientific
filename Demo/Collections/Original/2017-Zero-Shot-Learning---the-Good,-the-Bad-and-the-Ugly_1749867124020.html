
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <p>This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the version available on IEEE Xplore.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这篇CVPR论文是开放获取版本，由计算机视觉基金会提供。除了这个水印，它与IEEE Xplore上可用的版本完全相同。</p></div><h1>Zero-Shot Learning - The Good, the Bad and the Ugly</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>零样本学习 - 好的、坏的和丑陋的</h1></div><p>Yongqin Xian \({}^{1}\;\) Bernt Schiele \({}^{1}\;\) Zeynep Akata \({}^{1,2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Yongqin Xian \({}^{1}\;\) Bernt Schiele \({}^{1}\;\) Zeynep Akata \({}^{1,2}\)</p></div><p>\({}^{1}\) Max Planck Institute for Informatics Saarland Informatics Campus</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{1}\) 马克斯·普朗克信息学研究所 萨尔兰信息学校园</p></div><p>\({}^{2}\) Amsterdam Machine Learning Lab</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{2}\) 阿姆斯特丹机器学习实验室</p></div><p>University of Amsterdam</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>阿姆斯特丹大学</p></div><h2>Abstract</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>摘要</h2></div><p>Due to the importance of zero-shot learning, the number of proposed approaches has increased steadily recently. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is threefold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss limitations of the current status of the area which can be taken as a basis for advancing it.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>由于零样本学习的重要性，最近提出的方法数量稳步增加。我们认为是时候退一步分析该领域的现状。本文的目的有三。首先，鉴于没有公认的零样本学习基准，我们首先通过统一评估协议和数据划分来定义一个新的基准。这是一个重要的贡献，因为已发布的结果往往不可比较，有时甚至由于例如在零样本测试类别上进行预训练而存在缺陷。其次，我们深入比较和分析了大量的最先进方法，既在经典的零样本设置中，也在更现实的广义零样本设置中。最后，我们讨论了该领域当前状态的局限性，这可以作为推动其发展的基础。</p></div><h2>1. Introduction</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>1. 引言</h2></div><p>Zero-shot learning aims to recognize objects whose instances may not have been seen during training [17, 22, 23, 30, 40]. The number of new zero-shot learning methods proposed every year has been increasing rapidly, i.e. the good aspects as our title suggests. Although each new method has been shown to make progress over the previous one, it is difficult to quantify this progress without an established evaluation protocol, i.e. the bad aspects. In fact, the quest for improving numbers has lead to even flawed evaluation protocols, i.e. the ugly aspects. Therefore, in this work, we propose to extensively evaluate a significant number of recent zero-shot learning methods in depth on several small to large-scale datasets using the same evaluation protocol both in zero-shot, i.e. training and test classes are disjoint, and the more realistic generalized zero-shot learning settings, i.e. training classes are present at test time.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>零样本学习旨在识别在训练期间可能未见过的对象实例[17, 22, 23, 30, 40]。每年提出的新零样本学习方法数量迅速增加，即我们的标题所暗示的好的方面。尽管每种新方法都显示出相对于前一种方法的进步，但在没有建立评估协议的情况下，很难量化这种进步，即坏的方面。事实上，追求提高数字的过程导致了甚至有缺陷的评估协议，即丑陋的方面。因此，在这项工作中，我们提议在多个小到大规模的数据集上，使用相同的评估协议对大量最近的零样本学习方法进行深入评估，既在零样本设置中，即训练和测试类别不重叠，也在更现实的广义零样本学习设置中，即训练类别在测试时存在。</p></div><p>We benchmark and systematically evaluate zero-shot learning w.r.t. three aspects, i.e. methods, datasets and evaluation protocol. The crux of the matter for all zero-shot learning methods is to associate observed and non observed classes through some form of auxiliary information which encodes visually distinguishing properties of objects. Different flavors of zero-shot learning methods that we evaluate in this work are linear [11, 2, 4, 32] and nonlinear [39, 34] compatibility learning frameworks whereas an orthogonal direction is learning independent attribute [22] classifiers and finally others [42, 7, 26] propose a hybrid model between independent classifier learning and compatibility learning frameworks.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们从三个方面对零样本学习进行基准测试和系统评估，即方法、数据集和评估协议。所有零样本学习方法的关键在于通过某种形式的辅助信息将观察到的类别与未观察到的类别关联起来，这些信息编码了对象的视觉区分特性。我们在这项工作中评估的零样本学习方法的不同类型包括线性[11, 2, 4, 32]和非线性[39, 34]兼容性学习框架，而一个正交方向是学习独立属性[22]分类器，最后其他[42, 7, 26]则提出了一种独立分类器学习与兼容性学习框架之间的混合模型。</p></div><p>We thoroughly evaluate the second aspect of zero-shot learning, by using multiple splits of several small to large-scale datasets [28, 38, 22, 10, 9]. We emphasize that it is hard to obtain labeled training data for fine-grained classes of rare objects recognizing which requires expert opinion. Therefore, we argue that zero-shot learning methods should be evaluated mainly on least populated or rare classes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们通过使用多个小到大规模数据集的多个划分，深入评估零样本学习的第二个方面[28, 38, 22, 10, 9]。我们强调，获取稀有对象的细粒度类别的标记训练数据是困难的，因为这需要专家意见。因此，我们认为零样本学习方法应主要在最少人口或稀有类别上进行评估。</p></div><p>We propose a unified evaluation protocol to address the third aspect of zero-shot learning which is arguably the most important one. We emphasize the necessity of tuning hyper-parameters of the methods on a validation class split that is disjoint from training classes as improving zero-shot learning performance via tuning parameters on test classes violates the zero-shot assumption. We argue that per-class averaged top-1 accuracy is an important evaluation metric when the dataset is not well balanced with respect to the number of images per class. We point out that extracting image features via a pre-trained deep neural network (DNN) on a large dataset that contains zero-shot test classes also violates the zero-shot learning idea as image feature extraction is a part of the training procedure. Moreover, we argue that demonstrating zero-shot performance on small-scale and coarse grained datasets, i.e. aPY [10] is not conclusive. We recommend to abstract away from the restricted nature of zero-shot evaluation and make the task more practical by including training classes in the search space, i.e. generalized zero-shot learning setting. Therefore, we argue that our work plays an important role in advancing the zero-shot learning field by analyzing the good and bad aspects of the zero-shot learning task as well as proposing ways to eliminate the ugly ones.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们提出了一个统一的评估协议，以解决零样本学习的第三个方面，这可以说是最重要的方面。我们强调在与训练类别不重叠的验证类别划分上调整方法的超参数的必要性，因为通过在测试类别上调整参数来提高零样本学习性能违反了零样本假设。我们认为，当数据集在每个类别的图像数量上不平衡时，每类平均的top-1准确率是一个重要的评估指标。我们指出，通过在包含零样本测试类别的大型数据集上使用预训练深度神经网络（DNN）提取图像特征也违反了零样本学习的理念，因为图像特征提取是训练过程的一部分。此外，我们认为在小规模和粗粒度数据集上展示零样本性能，即aPY[10]并不具有决定性。我们建议抽象出零样本评估的限制性质，通过在搜索空间中包含训练类别，使任务更具实用性，即广义零样本学习设置。因此，我们认为我们的工作在通过分析零样本学习任务的好与坏方面以及提出消除丑陋方面的方法来推动零样本学习领域的发展中发挥了重要作用。</p></div><h2>2. Related Work</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>2. 相关工作</h2></div><p>We review related work on zero-shot and generalized zero-shot learning, we present previous evaluations on the same task and describe the unique aspects of our work.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们回顾了零样本和广义零样本学习的相关工作，展示了在同一任务上的先前评估，并描述了我们工作的独特方面。</p></div><p>Zero-Shot Learning. In zero-shot learning setting test and training class sets are disjoint [17, 22, 23, 30, 40] which can be tackled by solving related sub-problems, e.g. learning intermediate attribute classifiers [22, 30, 31] and learning a mixture of seen class proportions [42, 43, 26, 7], or by a direct approach, e.g. compatibility learning frameworks [3, 4, 11, 15, 27, 32, 34, 39, 32, 12, 29, 1, 6, 24, 13, 21]. Among these methods, in our evaluation we choose to use DAP [22] for being one of the most fundamental methods in zero-shot learning research; CONSE [26] for being one of the most widely used representatives of learning a mixture of class proportions; SSE [42] for being a recent method with a public implementation; SJE [4], ALE [3], DEVISE [11] for being recent compatibility learning methods with similar loss functions; ESZSL [32] for adding a regularization term to unregularized compatibility learning methods; [39] and CMT [34] proposing non-linear extensions to bilinear compatibility learning framework and finally SYNC [7] for reporting the state-of-the-art on several benchmark datasets.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>零样本学习。在零样本学习设置中，测试和训练类别集是互不重叠的[17, 22, 23, 30, 40]，可以通过解决相关子问题来处理，例如学习中间属性分类器[22, 30, 31]和学习已见类别比例的混合[42, 43, 26, 7]，或者通过直接方法，例如兼容性学习框架[3, 4, 11, 15, 27, 32, 34, 39, 32, 12, 29, 1, 6, 24, 13, 21]。在这些方法中，在我们的评估中，我们选择使用DAP[22]，因为它是零样本学习研究中最基本的方法之一；CONSE[26]，因为它是学习类别比例混合的最广泛使用的代表之一；SSE[42]，因为它是一个具有公开实现的最新方法；SJE[4]、ALE[3]、DEVISE[11]，因为它们是具有相似损失函数的最新兼容性学习方法；ESZSL[32]，因为它为未正则化的兼容性学习方法添加了正则化项；[39]和CMT[34]，提出了对双线性兼容性学习框架的非线性扩展，最后SYNC[7]在多个基准数据集上报告了最新的研究成果。</p></div><p>Generalized Zero-shot Learning. This setting [33] generalizes the zero-shot learning task to the case with both seen and unseen classes at test time. [19] argues that although ImageNet classification challenge performance has reached beyond human performance, we do not observe similar behavior of the methods that compete at the detection challenge which involves rejecting unknown objects while detecting the position and label of a known object. [11] uses label embeddings to operate on the generalized zero-shot learning setting whereas [41] proposes to learn latent representations for images and classes through coupled linear regression of factorized joint embeddings. On the other hand, [5] introduces a new model layer to the deep net which estimates the probability of an input being from an unknown class and [34] proposes a novelty detection mechanism. We evaluate [34] and [11] for being the most widely used.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>广义零样本学习。该设置[33]将零样本学习任务推广到测试时同时包含已见和未见类别的情况。[19]认为，尽管ImageNet分类挑战的表现已超越人类表现，但我们并未观察到在检测挑战中竞争的方法表现出类似的行为，该挑战涉及在检测已知对象的位置和标签时拒绝未知对象。[11]使用标签嵌入在广义零样本学习设置中进行操作，而[41]则提出通过耦合线性回归的因子化联合嵌入来学习图像和类别的潜在表示。另一方面，[5]在深度网络中引入了一个新的模型层，该层估计输入来自未知类别的概率，[34]则提出了一种新颖性检测机制。我们评估[34]和[11]，因为它们是最广泛使用的方法。</p></div><p>Previous Evaluations of Zero-Shot Learning. In the literature some zero-shot vs generalized zero-shot learning evaluation works exist [30, 8]. Among these, [30] proposes a model to learn the similarity between images and semantic embeddings on the ImageNet \(1\mathrm{\;K}\) by using 800 classes for training and 200 for test. [8] provides a comparison between five methods evaluated on three datasets including ImageNet with three standard splits and proposes a metric to evaluate generalized zero-shot learning performance.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>零样本学习的先前评估。在文献中存在一些零样本与广义零样本学习的评估工作[30, 8]。其中，[30]提出了一种模型，通过使用800个类别进行训练和200个类别进行测试，学习图像与语义嵌入之间的相似性。[8]提供了对五种方法的比较，这些方法在包括ImageNet在内的三个数据集上进行了评估，并提出了一种评估广义零样本学习性能的指标。</p></div><p>Our work. We evaluate ten zero-shot learning methods on five datasets with several splits both for zero-shot and generalized zero-shot learning settings, provide statistical significancy and robustness tests, and present other valuable insights that emerge from our benchmark. In this sense, ours is a more extensive evaluation compared to prior work.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的工作。我们在五个数据集上评估了十种零样本学习方法，进行了多次划分，既包括零样本学习设置，也包括广义零样本学习设置，提供了统计显著性和稳健性测试，并呈现了从我们的基准中得出的其他有价值的见解。从这个意义上说，我们的评估比之前的工作更为广泛。</p></div><h2>3. Evaluated Methods</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>3. 评估的方法</h2></div><p>We start by formalizing the zero-shot learning task and then we describe the zero-shot learning methods that we evaluate in this work. Given a training set \(\mathcal{S} =\) \(\left\{  {\left( {{x}_{n},{y}_{n}}\right) ,n = 1\ldots N}\right\}\) ,with \({y}_{n} \in  {\mathcal{Y}}^{tr}\) belonging to training classes,the task is to learn \(f : \mathcal{X} \rightarrow  \mathcal{Y}\) by minimizing the regularized empirical risk:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们首先对零样本学习任务进行形式化，然后描述我们在本工作中评估的零样本学习方法。给定一个训练集\(\mathcal{S} =\) \(\left\{  {\left( {{x}_{n},{y}_{n}}\right) ,n = 1\ldots N}\right\}\)，其中\({y}_{n} \in  {\mathcal{Y}}^{tr}\)属于训练类别，任务是通过最小化正则化经验风险来学习\(f : \mathcal{X} \rightarrow  \mathcal{Y}\)：</p></div><p></p>\[\frac{1}{N}\mathop{\sum }\limits_{{n = 1}}^{N}L\left( {{y}_{n},f\left( {{x}_{n};W}\right) }\right)  + \Omega \left( W\right)  \tag{1}\]<p></p><p>with \(L\left( \text{.}\right) {beingthelossfunctionand\Omega }\left( \text{.}\right) {beingtheregu} -\) larization term. Here,the mapping \(f : \mathcal{X} \rightarrow  \mathcal{Y}\) from input to output embeddings is defined as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>带有\(L\left( \text{.}\right) {beingthelossfunctionand\Omega }\left( \text{.}\right) {beingtheregu} -\)正则化项。在这里，从输入到输出嵌入的映射\(f : \mathcal{X} \rightarrow  \mathcal{Y}\)被定义为：</p></div><p></p>\[f\left( {x;W}\right)  = \mathop{\operatorname{argmax}}\limits_{{y \in  \mathcal{Y}}}F\left( {x,y;W}\right)  \tag{2}\]<p></p><p>At test time, in zero-shot learning setting, the aim is to assign a test image to an unseen class label,i.e. \({\mathcal{Y}}^{ts} \subset  \mathcal{Y}\) and in generalized zero-shot learning setting, the test image can be assigned either to seen or unseen classes, i.e. \({\mathcal{Y}}^{{tr} + {ts}} \subset  \mathcal{Y}\) with the highest compatibility score.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在测试时，在零样本学习设置中，目标是将测试图像分配给一个未见的类别标签，即<b0>&#x3C;/b0，而在广义零样本学习设置中，测试图像可以分配给已见或未见类别，即\({\mathcal{Y}}^{{tr} + {ts}} \subset  \mathcal{Y}\)，以获得最高的兼容性得分。</b0></p></div><h3>3.1. Learning Linear Compatibility</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.1. 学习线性兼容性</h3></div><p>Attribute Label Embedding (ALE) [3], Deep Visual Semantic Embedding (DEVISE) [11] and Structured Joint Embedding (SJE) [4] use bi-linear compatibility function to associate visual and auxiliary information:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>属性标签嵌入（ALE）[3]、深度视觉语义嵌入（DEVISE）[11]和结构化联合嵌入（SJE）[4]使用双线性兼容性函数将视觉信息与辅助信息关联起来：</p></div><p></p>\[F\left( {x,y;W}\right)  = \theta {\left( x\right) }^{T}{W\phi }\left( y\right)  \tag{3}\]<p></p><p>where \(\theta \left( x\right)\) and \(\phi \left( y\right)\) ,i.e. image and class embeddings,both of which are given. \(F\left( \text{.}\right) {isparameterizedbythemapping}\) \(W\) ,to be learned. Embarassingly Simple Zero Shot Learning (ESZSL) [32] adds a regularization term to this objective. In the following, we provide a unified formulation of these four zero-shot learning methods.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(\theta \left( x\right)\)和\(\phi \left( y\right)\)，即图像和类别嵌入，均已给出。\(F\left( \text{.}\right) {isparameterizedbythemapping}\)\(W\)，待学习。令人尴尬的简单零样本学习（ESZSL）[32]在此目标中添加了一个正则化项。接下来，我们提供这四种零样本学习方法的统一公式。</p></div><p>DEVISE [11] uses pairwise ranking objective that is inspired from unregularized ranking SVM [20]:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>DEVISE [11]使用的成对排名目标受到无正则化排名支持向量机（SVM）[20]的启发：</p></div><p></p>\[\mathop{\sum }\limits_{{y \in  {\mathcal{Y}}^{tr}}}{\left\lbrack  \Delta \left( {y}_{n},y\right)  + F\left( {x}_{n},y;W\right)  - F\left( {x}_{n},{y}_{n};W\right) \right\rbrack  }_{ + } \tag{4}\]<p></p><p>ALE [3] uses weighted approximate ranking objective [37]:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>ALE [3]使用加权近似排名目标[37]：</p></div><p></p>\[\mathop{\sum }\limits_{{y \in  {\mathcal{Y}}^{tr}}}\frac{{l}_{{r}_{\Delta \left( {{x}_{n},{y}_{n}}\right) }}}{{r}_{\Delta \left( {{x}_{n},{y}_{n}}\right) }}{\left\lbrack  \Delta \left( {y}_{n},y\right)  + F\left( {x}_{n},y;W\right)  - F\left( {x}_{n},{y}_{n};W\right) \right\rbrack  }_{ + }\]<p></p><p>(5)</p><p>where \({l}_{k} = \mathop{\sum }\limits_{{i = 1}}^{k}{\alpha }_{i}\) and \({r}_{\Delta \left( {{x}_{n},{y}_{n}}\right) }\) is defined as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\({l}_{k} = \mathop{\sum }\limits_{{i = 1}}^{k}{\alpha }_{i}\)和\({r}_{\Delta \left( {{x}_{n},{y}_{n}}\right) }\)定义为：</p></div><p></p>\[\mathop{\sum }\limits_{{y \in  {\mathcal{Y}}^{tr}}}\mathbb{1}\left( {F\left( {{x}_{n},y;W}\right)  + \Delta \left( {{y}_{n},y}\right)  \geq  F\left( {{x}_{n},{y}_{n};W}\right) }\right)  \tag{6}\]<p></p><p>Following the heuristic in [18],[3] selects \({\alpha }_{i} = 1/i\) which puts high emphasis on the top of the rank list.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>根据[18]中的启发式方法，[3]选择\({\alpha }_{i} = 1/i\)，该方法对排名列表的顶部给予高度重视。</p></div><p>SJE [4] gives full weight to the top of the ranked list and is inspired from the structured SVM [36]:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>SJE [4]对排名列表的顶部给予全部权重，并受到结构化支持向量机（SVM）[36]的启发：</p></div><p></p>\[{\left\lbrack  \mathop{\max }\limits_{{y \in  {\mathcal{Y}}^{tr}}}\left( \Delta \left( {y}_{n},y\right)  + F\left( {x}_{n},y;W\right) \right)  - F\left( {x}_{n},{y}_{n};W\right) \right\rbrack  }_{ + } \tag{7}\]<p></p><p>ESZSL [32] adds the following regularization term to the unregularized risk minimization formulation:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>ESZSL [32]在无正则化风险最小化公式中添加了以下正则化项：</p></div><p></p>\[\gamma \parallel {W\phi }\left( y\right) {\parallel }_{Fro}^{2} + \lambda {\begin{Vmatrix}\theta {\left( x\right) }^{T}W\end{Vmatrix}}_{Fro}^{2} + \beta \parallel W{\parallel }_{Fro}^{2} \tag{8}\]<p></p><p>where \(\gamma ,\lambda ,\beta\) are parameters of this regularizer.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(\gamma ,\lambda ,\beta\)是该正则化项的参数。</p></div><h3>3.2. Learning Nonlinear Compatibility</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.2. 学习非线性兼容性</h3></div><p>Latent Embeddings (LATEM) [39] and Cross Modal Transfer (CMT) [34] encode an additional non-linearity in compatibility learning framework.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>潜在嵌入（LATEM）[39]和跨模态转移（CMT）[34]在兼容性学习框架中编码了额外的非线性。</p></div><p>LATEM [39] constructs a piece-wise linear compatibility:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>LATEM [39]构建了分段线性兼容性：</p></div><p></p>\[F\left( {x,y;{W}_{i}}\right)  = \mathop{\max }\limits_{{1 \leq  i \leq  K}}\theta {\left( x\right) }^{T}{W}_{i}\phi \left( y\right)  \tag{9}\]<p></p><p>where every \({W}_{i}\) models a different visual characteristic of the data and the selection of which matrix to use to do the mapping is a latent variable. LATEM uses the ranking loss formulated in Equation 4.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中每个\({W}_{i}\)模型表示数据的不同视觉特征，选择使用哪个矩阵进行映射是一个潜在变量。LATEM使用在公式4中制定的排名损失。</p></div><p>CMT [34] first maps images into a semantic space of words, i.e. class names, where a neural network with tanh nonlinearity learns the mapping:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>CMT [34]首先将图像映射到一个语义空间中，即类别名称，在该空间中，具有tanh非线性的神经网络学习映射：</p></div><p></p>\[\mathop{\sum }\limits_{{y \in  {\mathcal{Y}}^{tr}}}\mathop{\sum }\limits_{{x \in  {\mathcal{X}}_{y}}}\parallel \phi \left( y\right)  - {W}_{1}\tanh \left( {{W}_{2}\theta \left( x\right) \parallel }\right.  \tag{10}\]<p></p><p>where \(\left( {{W}_{1},{W}_{2}}\right)\) are weights of the two layer neural network. This is followed by a novelty detection mechanism that assigns images to unseen or seen classes. The novelty is detected either via thresholds learned using the embedded images of the seen classes or the outlier probabilities are obtained in an unsupervised way.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(\left( {{W}_{1},{W}_{2}}\right)\)是两层神经网络的权重。随后是一个新颖性检测机制，将图像分配给未见或已见的类别。新颖性通过使用已见类别的嵌入图像学习的阈值或以无监督方式获得的异常概率进行检测。</p></div><h3>3.3. Learning Intermediate Attribute Classifiers</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.3. 学习中间属性分类器</h3></div><p>Although Direct Attribute Prediction (DAP) [22] has been shown to perform poorly compared to compatibility learning frameworks [3], we include it to our evaluation for being historically one of the most widely used methods in the literature. DAP [22] learns probabilistic attribute classifiers and makes a class prediction by combining scores of the learned attribute classifiers. A novel image is assigned to one of the unknown classes using:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>尽管直接属性预测（DAP）[22]与兼容性学习框架[3]相比表现不佳，但我们仍将其纳入评估，因为它在文献中历史上是最广泛使用的方法之一。DAP [22]学习概率属性分类器，并通过结合学习到的属性分类器的得分进行类别预测。使用以下方法将新图像分配给未知类别之一：</p></div><p></p>\[f\left( x\right)  = \mathop{\operatorname{argmax}}\limits_{c}\mathop{\prod }\limits_{{m = 1}}^{M}\frac{p\left( {{a}_{m}^{c} \mid  x}\right) }{p\left( {a}_{m}^{c}\right) }. \tag{11}\]<p></p><p>with \(M\) being the total number of attributes. We train a one-vs-rest SVM with log loss that gives probability scores of attributes with respect to training classes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(M\)是属性的总数。我们训练一个一对多的支持向量机（SVM），使用对数损失来给出属性相对于训练类别的概率评分。</p></div><h3>3.4. Hybrid Models</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.4. 混合模型</h3></div><p>Semantic Similarity Embedding (SSE) [42], Convex Combination of Semantic Embeddings (CONSE) [26] and Synthesized Classifiers (SYNC) [7] express images and semantic class embeddings as a mixture of seen class proportions, hence we group them as hybrid models.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>语义相似性嵌入（SSE）[42]、语义嵌入的凸组合（CONSE）[26]和合成分类器（SYNC）[7]将图像和语义类别嵌入表示为已知类别比例的混合，因此我们将它们归类为混合模型。</p></div><p>SSE [42] leverages similar class relationships both in image and semantic embedding space. An image is labeled with:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>SSE [42] 在图像和语义嵌入空间中利用相似类别关系。图像被标记为：</p></div><p></p>\[\mathop{\operatorname{argmax}}\limits_{{u \in  \mathcal{U}}}\pi {\left( \theta \left( x\right) \right) }^{T}\psi \left( {\phi \left( {y}_{u}\right) }\right)  \tag{12}\]<p></p><p>where \(\pi ,\psi\) are mappings of class and image embeddings into a common space. Specifically, \(\psi\) is learned by sparse coding and \(\pi\) is by class dependent transformation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(\pi ,\psi\)是类别和图像嵌入映射到共同空间的映射。具体而言，\(\psi\)是通过稀疏编码学习的，\(\pi\)是通过类别依赖变换获得的。</p></div><p>CONSE [26] learns the probability of a training image belonging to a training class:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>CONSE [26] 学习训练图像属于训练类别的概率：</p></div><p></p>\[f\left( {x,t}\right)  = \mathop{\operatorname{argmax}}\limits_{{y \in  {\mathcal{Y}}^{tr}}}{p}_{tr}\left( {y \mid  x}\right)  \tag{13}\]<p></p><p>where \(y\) denotes the most likely training label \(\left( {t = 1}\right)\) for image \(x\) . Combination of semantic embeddings(s)is used to assign an unknown image to an unseen class:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(y\)表示图像\(x\)的最可能训练标签\(\left( {t = 1}\right)\)。语义嵌入的组合用于将未知图像分配给未见类别：</p></div><p></p>\[\frac{1}{Z}\mathop{\sum }\limits_{{i = 1}}^{T}{p}_{tr}\left( {f\left( {x,t}\right)  \mid  x}\right)  \cdot  s\left( {f\left( {x,t}\right) }\right)  \tag{14}\]<p></p><p>where \(Z = \mathop{\sum }\limits_{{i = 1}}^{T}{p}_{tr}\left( {f\left( {x,t}\right)  \mid  x}\right) ,f\left( {x,t}\right)\) denotes the \({\mathfrak{t}}^{th}\) most likely label for image \(x\) and \(T\) controls the maximum number of semantic embedding vectors.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(Z = \mathop{\sum }\limits_{{i = 1}}^{T}{p}_{tr}\left( {f\left( {x,t}\right)  \mid  x}\right) ,f\left( {x,t}\right)\)表示图像\(x\)的\({\mathfrak{t}}^{th}\)最可能标签，\(T\)控制语义嵌入向量的最大数量。</p></div><p>SYNC [7] learns a mapping between the semantic class embedding space and a model space. In the model space, training classes and a set of phantom classes form a weighted bipartite graph. The objective is to minimize distortion error:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>SYNC [7] 学习语义类别嵌入空间与模型空间之间的映射。在模型空间中，训练类别和一组虚拟类别形成一个加权二分图。目标是最小化失真误差：</p></div><p></p>\[\mathop{\min }\limits_{{{w}_{c},{v}_{r}}}{\begin{Vmatrix}{w}_{c} - \mathop{\sum }\limits_{{r = 1}}^{R}{s}_{cr}{v}_{r}\end{Vmatrix}}_{2}^{2}. \tag{15}\]<p></p><p>Semantic and model spaces are aligned by embedding real \(\left( {w}_{c}\right)\) and phantom classes \(\left( {v}_{r}\right)\) in the weighted graph \(\left( {s}_{cr}\right)\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>通过在加权图\(\left( {s}_{cr}\right)\)中嵌入真实\(\left( {w}_{c}\right)\)和虚拟类别\(\left( {v}_{r}\right)\)来对齐语义空间和模型空间。</p></div><!-- Media --><table><tbody><tr><td rowspan="4">Dataset</td><td rowspan="4">Size</td><td></td><td colspan="4" rowspan="3">Number of Classes</td><td colspan="9">Number of Images</td></tr><tr><td></td><td colspan="5">At Training Time</td><td colspan="4">At Evaluation Time</td></tr><tr><td></td><td></td><td colspan="2">SS</td><td colspan="2">\( \mathbf{{PS}} \)</td><td colspan="2">SS</td><td colspan="2">PS</td></tr><tr><td>Detail</td><td>Att</td><td>y</td><td>\( y{tr} \)</td><td>\( y{ts} \)</td><td>Total</td><td>\( y{t}^{r} \)</td><td>\( {y}^{ts} \)</td><td>\( y{tr} \)</td><td>\( y * s \)</td><td>\( y{tr} \)</td><td>\( y \) ts</td><td>\( y{t}^{r} \)</td><td>\( y + s \)</td></tr><tr><td>SUN [28]</td><td>medium</td><td>fine</td><td>102</td><td>717</td><td>\( {580} + {65} \)</td><td>72</td><td>14K</td><td>12900</td><td>0</td><td>10320</td><td>0</td><td>0</td><td>1440</td><td>2580</td><td>1440</td></tr><tr><td>CUB [38]</td><td>medium</td><td>fine</td><td>312</td><td>200</td><td>100 + 50</td><td>50</td><td>11K</td><td>8855</td><td>0</td><td>7057</td><td>0</td><td>0</td><td>2933</td><td>1764</td><td>2967</td></tr><tr><td>AWA [22]</td><td>medium</td><td>coarse</td><td>85</td><td>50</td><td>\( {27} + {13} \)</td><td>10</td><td>30K</td><td>24295</td><td>0</td><td>19832</td><td>0</td><td>0</td><td>6180</td><td>4958</td><td>5685</td></tr><tr><td>aPY [10]</td><td>small</td><td>coarse</td><td>64</td><td>32</td><td>\( {15} + 5 \)</td><td>12</td><td>15K</td><td>12695</td><td>0</td><td>5932</td><td>0</td><td>0</td><td>2644</td><td>1483</td><td>7924</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="4">数据集</td><td rowspan="4">大小</td><td></td><td colspan="4" rowspan="3">类别数量</td><td colspan="9">图像数量</td></tr><tr><td></td><td colspan="5">训练时</td><td colspan="4">评估时</td></tr><tr><td></td><td></td><td colspan="2">SS</td><td colspan="2">\( \mathbf{{PS}} \)</td><td colspan="2">SS</td><td colspan="2">PS</td></tr><tr><td>细节</td><td>注意力</td><td>y</td><td>\( y{tr} \)</td><td>\( y{ts} \)</td><td>总计</td><td>\( y{t}^{r} \)</td><td>\( {y}^{ts} \)</td><td>\( y{tr} \)</td><td>\( y * s \)</td><td>\( y{tr} \)</td><td>\( y \) ts</td><td>\( y{t}^{r} \)</td><td>\( y + s \)</td></tr><tr><td>SUN [28]</td><td>中等</td><td>精细</td><td>102</td><td>717</td><td>\( {580} + {65} \)</td><td>72</td><td>14K</td><td>12900</td><td>0</td><td>10320</td><td>0</td><td>0</td><td>1440</td><td>2580</td><td>1440</td></tr><tr><td>CUB [38]</td><td>中等</td><td>精细</td><td>312</td><td>200</td><td>100 + 50</td><td>50</td><td>11K</td><td>8855</td><td>0</td><td>7057</td><td>0</td><td>0</td><td>2933</td><td>1764</td><td>2967</td></tr><tr><td>AWA [22]</td><td>中等</td><td>粗糙</td><td>85</td><td>50</td><td>\( {27} + {13} \)</td><td>10</td><td>30K</td><td>24295</td><td>0</td><td>19832</td><td>0</td><td>0</td><td>6180</td><td>4958</td><td>5685</td></tr><tr><td>aPY [10]</td><td>小</td><td>粗糙</td><td>64</td><td>32</td><td>\( {15} + 5 \)</td><td>12</td><td>15K</td><td>12695</td><td>0</td><td>5932</td><td>0</td><td>0</td><td>2644</td><td>1483</td><td>7924</td></tr></tbody></table></div><p>Table 1: Statistics for attribute datasets: SUN [28], CUB [38], AWA [22], aPY [10] in terms of size of the datasets, fine-grained or coarse-grained,number of attributes,number of classes in training + validation \(\left( {\mathcal{Y}}^{tr}\right)\) and test classes \(\left( {\mathcal{Y}}^{ts}\right)\) ,number of images at training and test time for standard split (SS) and our proposed splits (PS).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表1：属性数据集的统计信息：SUN [28]、CUB [38]、AWA [22]、aPY [10]，包括数据集的大小、细粒度或粗粒度、属性数量、训练+验证类 \(\left( {\mathcal{Y}}^{tr}\right)\) 和测试类 \(\left( {\mathcal{Y}}^{ts}\right)\) 的数量，以及标准划分（SS）和我们提出的划分（PS）下的训练和测试时的图像数量。</p></div><!-- Media --><h2>4. Datasets and Evaluation Protocol</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>4. 数据集和评估协议</h2></div><p>In this section, we provide several components of previously used and our proposed zero-shot and generalized zero-shot learning evaluation protocols, e.g. datasets, image and class encodings and the evaluation protocol.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本节中，我们提供了之前使用的和我们提出的零样本学习和广义零样本学习评估协议的几个组成部分，例如数据集、图像和类编码以及评估协议。</p></div><h3>4.1. Dataset Statistics</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.1. 数据集统计</h3></div><p>Among the most widely used datasets for zero-shot learning, we select two coarse-grained, one small and one medium-scale, and two fine-grained, both medium-scale, datasets with attributes and one large-scale dataset without. Here,we consider between \({10}\mathrm{K}\) and \(1\mathrm{M}\) images,and,between 100 and \({1K}\) classes as medium-scale.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在最广泛使用的零样本学习数据集中，我们选择了两个粗粒度数据集、一个小规模和一个中等规模的数据集，以及两个细粒度数据集，均为中等规模，具有属性，还有一个没有属性的大规模数据集。在这里，我们认为图像数量在 \({10}\mathrm{K}\) 和 \(1\mathrm{M}\) 之间，以及类的数量在100到 \({1K}\) 之间为中等规模。</p></div><p>Attribute Datasets. Statistics of the attribute datasets are presented in Table 1. Attribute Pascal and Yahoo (aPY) [10] is a small-scale coarse-grained dataset with 64 attributes. Among the total number of 32 classes, 20 Pascal classes are used for training (we randomly select 5 for validation) and 12 Yahoo classes are used for testing. Animals with Attributes (AWA) [22] is a coarse-grained dataset that is medium-scale in terms of the number of images,i.e.30,475 and small-scale in terms of number of classes, i.e. 50. [22] introduces a standard zero-shot split with 40 classes for training (we randomly select 13 for validation) and 10 for testing. AWA has 85 attributes. Caltech-UCSD-Birds 200-2011 (CUB) [38] is a fine-grained and medium scale dataset with respect to both number of images and number of classes,i.e. 11,788 images from 200 different types of birds annotated with 312 attributes. [3] introduces the first zero-shot split of CUB with 150 training (50 validation classes) and 50 test classes. SUN [28] is a fine-grained and medium-scale dataset with respect to both number of images and number of classes,i.e. SUN contains 14340 images coming from 717 types of scenes annotated with 102 attributes. Following [22] we use 645 classes of SUN for training (we randomly select 65 for val) and 72 for testing.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>属性数据集。属性数据集的统计信息见表1。属性Pascal和Yahoo (aPY) [10] 是一个小规模粗粒度数据集，具有64个属性。在总共32个类中，20个Pascal类用于训练（我们随机选择5个用于验证），12个Yahoo类用于测试。具有属性的动物（AWA） [22] 是一个粗粒度数据集，在图像数量上为中等规模，即30,475张，在类的数量上为小规模，即50个。[22] 引入了一个标准的零样本划分，包含40个用于训练的类（我们随机选择13个用于验证）和10个用于测试。AWA有85个属性。加州理工学院-加州大学圣地亚哥分校鸟类200-2011（CUB） [38] 是一个细粒度和中等规模的数据集，图像数量和类的数量均为中等规模，即11,788张来自200种不同类型鸟类的图像，标注了312个属性。[3] 引入了CUB的第一个零样本划分，包含150个训练类（50个验证类）和50个测试类。SUN [28] 是一个细粒度和中等规模的数据集，图像数量和类的数量均为中等规模，即SUN包含14340张来自717种场景的图像，标注了102个属性。根据[22]，我们使用645个SUN类进行训练（我们随机选择65个用于验证）和72个用于测试。</p></div><p>Large-Scale ImageNet. We also evaluate the performance of methods on the large scale ImageNet [9]. Among the total of \({21}\mathrm{\;K}\) classes, \(1\mathrm{\;K}\) classes are used for training (we use 200 classes for validation) and the test split is either all the remaining \({21}\mathrm{\;K}\) classes or a subset of it,e.g. we determine these subsets based on the hierarchical distance between classes and the population of classes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>大规模ImageNet。我们还评估了方法在大规模ImageNet [9]上的性能。在总共 \({21}\mathrm{\;K}\) 个类中， \(1\mathrm{\;K}\) 个类用于训练（我们使用200个类进行验证），测试划分是所有剩余的 \({21}\mathrm{\;K}\) 个类或其子集，例如，我们根据类之间的层次距离和类的数量来确定这些子集。</p></div><h3>4.2. Proposed Evaluation Protocol</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.2. 提出的评估协议</h3></div><p>We present our proposed unified protocol for image and class embeddings, dataset splits and evaluation criteria.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们提出了统一的图像和类嵌入、数据集划分和评估标准的协议。</p></div><p>Image and Class Embedding. We extract image features from the entire image for SUN, CUB, AWA and ImageNet, with no image pre-processing. For aPY, as proposed in [10], we extract image features from bounding boxes. Our image embeddings are 2048-dim top-layer pooling units of the 101-layered ResNet [16] as we found that it performs better than 1,024-dim top-layer pooling units of GoogleNet [35]. ResNet is pre-trained on ImageNet \(1\mathrm{\;K}\) and not fine-tuned. In addition to ResNet features, we evaluate methods with their published image features. As class embeddings, for aPY, AWA, CUB and SUN, we use per-class attributes. For ImageNet we use Word2Vec [25] provided by [7] as it does not contain attribute annotation for all the classes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图像和类嵌入。我们从整个图像中提取SUN、CUB、AWA和ImageNet的图像特征，没有进行图像预处理。对于aPY，如[10]所提议，我们从边界框中提取图像特征。我们的图像嵌入是101层ResNet [16]的2048维顶层池化单元，因为我们发现其性能优于GoogleNet [35]的1024维顶层池化单元。ResNet在ImageNet \(1\mathrm{\;K}\) 上进行预训练，且未进行微调。除了ResNet特征外，我们还使用其发布的图像特征进行评估。作为类嵌入，对于aPY、AWA、CUB和SUN，我们使用每类的属性。对于ImageNet，我们使用[7]提供的Word2Vec [25]，因为它并未为所有类提供属性注释。</p></div><p>Dataset Splits. Zero-shot learning assumes disjoint training and test classes with the presence of all the images of training classes and the absence of any image from test classes during training. On the other hand, as deep neural network (DNN) training for image feature extraction is actually a part of model training, the dataset used to train DNNs, e.g. ImageNet, should not include any of the test classes. However, we notice from the standard splits (SS) of aPY and AWA datasets that 7 aPY test classes out of 12 (monkey, wolf, zebra, mug, building, bag, carriage), 6 AWA test classes out of 10 (chimpanzee, giant panda, leopard, persian cat,pig,hippopotamus),are among the \(1\mathrm{\;K}\) classes of ImageNet, i.e. are used to pre-train ResNet. On the other hand, the mostly widely used splits, i.e. we term them as standard splits (SS), for SUN from [22] and CUB from [2] shows us that 1 CUB test class out of 50 (Indigo Bunting), and 6 SUN test classes out of 72 (restaurant, supermarket, planetarium,tent,market,bridge),are also among the \(1\mathrm{\;K}\) classes of ImageNet. We noticed that the accuracy for all methods on those overlapping test classes are higher than others. Therefore, we propose new dataset splits, i.e. proposed splits (PS), insuring that none of the test classes appear in ImageNet \(1\mathrm{\;K}\) ,i.e. used to train the ResNet model. We present the differences between the standard splits (SS) and the proposed splits (PS) in Table 1 While in SS and PS no image from test classes is present at training time, at test time SS does not include any images from training classes however our PS does. We designed the PS this way as evaluating accuracy on both training and test classes is crucial to show the generalization of methods.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>数据集划分。零样本学习假设训练类和测试类是互不重叠的，训练期间存在所有训练类的图像，而测试类的任何图像都不存在。另一方面，由于深度神经网络（DNN）用于图像特征提取的训练实际上是模型训练的一部分，因此用于训练DNN的数据集，例如ImageNet，不应包含任何测试类。然而，我们注意到在aPY和AWA数据集的标准划分（SS）中，12个aPY测试类中有7个（猴子、狼、斑马、杯子、建筑、包、马车），10个AWA测试类中有6个（黑猩猩、巨型熊猫、豹子、波斯猫、猪、河马）属于ImageNet的\(1\mathrm{\;K}\)类，即用于预训练ResNet。另一方面，最广泛使用的划分，即我们称之为标准划分（SS），对于来自[22]的SUN和来自[2]的CUB显示，50个CUB测试类中有1个（靛蓝雀），72个SUN测试类中有6个（餐厅、超市、天文馆、帐篷、市场、桥）也属于ImageNet的\(1\mathrm{\;K}\)类。我们注意到，在这些重叠的测试类上，所有方法的准确性都高于其他类。因此，我们提出新的数据集划分，即提议的划分（PS），确保测试类不出现在ImageNet的\(1\mathrm{\;K}\)中，即用于训练ResNet模型。我们在表1中展示了标准划分（SS）和提议划分（PS）之间的差异。在SS和PS中，训练时没有来自测试类的图像，但在测试时，SS不包括任何来自训练类的图像，而我们的PS包括。我们设计PS的方式是，因为在训练类和测试类上评估准确性对于展示方法的泛化能力至关重要。</p></div><p>ImageNet with thousands of classes provides possibilities of constructing several zero-shot evaluation splits. Following [7], our first two standard splits consider all the classes that are 2-hops and 3-hops away from the original \(1\mathrm{\;K}\) classes according to the ImageNet label hierarchy,corresponding to 1509 and 7678 classes. This split measures the generalization ability of the models with respect to the hierarchical and semantic similarity between classes. Our proposed split considers \({500},1\mathrm{\;K}\) and \(5\mathrm{\;K}\) most populated classes among the remaining \({21}\mathrm{\;K}\) classes of ImageNet with \(\approx  {1756}\) , \(\approx  {1624}\) and \(\approx  {1335}\) images per class on average. Similarly,we consider \({500},1\mathrm{\;K}\) and \(5\mathrm{\;K}\) least-populated classes in ImageNet which correspond to most fine-grained subsets of ImageNet with \(\approx  1, \approx  3\) and \(\approx  {51}\) images per class on average. Our final split considers all the remaining \(\approx  {20}\mathrm{\;K}\) classes of ImageNet with at least 1 image per-class, \(\approx  {631}\) images per class on average.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>ImageNet拥有数千个类，提供了构建多个零样本评估划分的可能性。根据[7]，我们的前两个标准划分考虑了根据ImageNet标签层次结构与原始\(1\mathrm{\;K}\)类相距2跳和3跳的所有类，分别对应1509和7678个类。这个划分衡量了模型在类之间的层次和语义相似性方面的泛化能力。我们提议的划分考虑了在ImageNet剩余的\({21}\mathrm{\;K}\)类中，最为密集的\({500},1\mathrm{\;K}\)和\(5\mathrm{\;K}\)类，平均每类有\(\approx  {1756}\)、\(\approx  {1624}\)和\(\approx  {1335}\)张图像。同样，我们考虑了在ImageNet中最少人口的\({500},1\mathrm{\;K}\)和\(5\mathrm{\;K}\)类，这些类对应于ImageNet中最细粒度的子集，平均每类有\(\approx  1, \approx  3\)和\(\approx  {51}\)张图像。我们的最终划分考虑了ImageNet中所有剩余的\(\approx  {20}\mathrm{\;K}\)类，每类至少有1张图像，平均每类有\(\approx  {631}\)张图像。</p></div><p>Evaluation Criteria. Single label image classification accuracy has been measured with Top-1 accuracy, i.e. the prediction is accurate when the predicted class is the correct one. If the accuracy is averaged for all images, high performance on densely populated classes is encouraged. However, we are interested in having high performance also on sparsely populated classes. Therefore, we average the correct predictions independently for each class before dividing their cumulative sum w.r.t the number of classes, i.e. we measure average per-class top-1 accuracy.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>评估标准。单标签图像分类准确性通过Top-1准确性进行测量，即当预测的类是正确的时，预测是准确的。如果对所有图像的准确性进行平均，则鼓励在密集人口类上获得高性能。然而，我们也希望在稀疏人口类上获得高性能。因此，我们在对每个类的正确预测进行独立平均后，再将其累积和除以类的数量，即我们测量每类的平均Top-1准确性。</p></div><p>In generalized zero-shot learning setting, the search space at evaluation time is not restricted to only test classes, but includes also the training classes, hence this setting is more practical. As with our proposed split at test time we have access to some images from training classes, after having computed the average per-class top-1 accuracy on training and test classes, we compute the harmonic mean of training and test accuracies:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在广义零样本学习设置中，评估时的搜索空间不仅限于测试类，还包括训练类，因此这种设置更为实用。由于在测试时我们可以访问一些来自训练类的图像，在计算了训练类和测试类的平均每类Top-1准确性后，我们计算训练和测试准确性的调和平均值：</p></div><p></p>\[H = 2 * \left( {{\operatorname{accy}}_{tr} * {\operatorname{accy}}_{ts}}\right) /\left( {{\operatorname{accy}}_{tr} + {\operatorname{accy}}_{ts}}\right)  \tag{16}\]<p></p><p>where \({\operatorname{accy}}_{tr}\) and \({\operatorname{accy}}_{ts}\) represent the accuracy of images from seen \(\left( {\mathcal{Y}}^{tr}\right)\) ,and images from unseen \(\left( {\mathcal{Y}}^{ts}\right)\) classes respectively. We choose harmonic mean as our evaluation criteria and not arithmetic mean because in arithmetic mean if the seen class accuracy is much higher, it effects the overall results significantly. Instead, our aim is high accuracy on both seen and unseen classes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \({\operatorname{accy}}_{tr}\) 和 \({\operatorname{accy}}_{ts}\) 分别表示来自已知 \(\left( {\mathcal{Y}}^{tr}\right)\) 类别和未知 \(\left( {\mathcal{Y}}^{ts}\right)\) 类别的图像的准确性。我们选择调和平均数作为评估标准，而不是算术平均数，因为在算术平均数中，如果已知类别的准确性远高于其他类别，会显著影响整体结果。相反，我们的目标是在已知和未知类别上都达到高准确性。</p></div><!-- Media --><table><tbody><tr><td rowspan="2">Model</td><td colspan="2">SUN</td><td colspan="2">AWA</td></tr><tr><td>\( \mathbf{R} \)</td><td>O</td><td>\( \mathbf{R} \)</td><td>O</td></tr><tr><td>DAP [22]</td><td>22.1</td><td>22.2</td><td>41.4</td><td>41.4</td></tr><tr><td>SSE [42]</td><td>83.0</td><td>82.5</td><td>64.9</td><td>76.3</td></tr><tr><td>LATEM [39]</td><td>-</td><td>-</td><td>71.2</td><td>71.9</td></tr><tr><td>SJE [4]</td><td>-</td><td>-</td><td>67.2</td><td>66.7</td></tr><tr><td>ESZSL [32]</td><td>64.3</td><td>65.8</td><td>48.0</td><td>49.3</td></tr><tr><td>SYNC [7]</td><td>62.8</td><td>62.8</td><td>69.7</td><td>69.7</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">模型</td><td colspan="2">太阳</td><td colspan="2">AWA</td></tr><tr><td>\( \mathbf{R} \)</td><td>O</td><td>\( \mathbf{R} \)</td><td>O</td></tr><tr><td>DAP [22]</td><td>22.1</td><td>22.2</td><td>41.4</td><td>41.4</td></tr><tr><td>SSE [42]</td><td>83.0</td><td>82.5</td><td>64.9</td><td>76.3</td></tr><tr><td>LATEM [39]</td><td>-</td><td>-</td><td>71.2</td><td>71.9</td></tr><tr><td>SJE [4]</td><td>-</td><td>-</td><td>67.2</td><td>66.7</td></tr><tr><td>ESZSL [32]</td><td>64.3</td><td>65.8</td><td>48.0</td><td>49.3</td></tr><tr><td>同步 [7]</td><td>62.8</td><td>62.8</td><td>69.7</td><td>69.7</td></tr></tbody></table></div><p>Table 2: Reproducing zero-shot results: \(\mathrm{O} =\) Original results published in the paper, \(\mathrm{R} =\) Reproduced using provided image features and code. We measure top-1 accuracy in \(\%\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表2：重现零样本结果：\(\mathrm{O} =\) 论文中发布的原始结果，\(\mathrm{R} =\) 使用提供的图像特征和代码重现。我们在\(\%\)中测量top-1准确率。</p></div><!-- Media --><h2>5. Experiments</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>5. 实验</h2></div><p>We first provide zero-shot learning results on attribute datasets SUN, CUB, AWA and aPY and then on the large-scale ImageNet dataset. Finally, we present results for the generalized zero-shot learning setting.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们首先提供在属性数据集SUN、CUB、AWA和aPY上的零样本学习结果，然后是在大规模ImageNet数据集上的结果。最后，我们展示广义零样本学习设置的结果。</p></div><h3>5.1. Zero-Shot Learning Results</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.1. 零样本学习结果</h3></div><p>On attribute datasets, i.e. SUN, CUB, AWA and aPY, we first reproduce the results of each method using their evaluation protocol, then provide a unified evaluation protocol using the same train/val/test class splits, followed by our proposed train/val/test class splits. We also evaluate the robustness of the methods to parameter tuning and visualize the ranking of different methods. Finally, we evaluate the methods on the large-scale ImageNet dataset.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在属性数据集上，即SUN、CUB、AWA和aPY，我们首先使用各自的评估协议重现每种方法的结果，然后提供统一的评估协议，使用相同的训练/验证/测试类别划分，接着是我们提出的训练/验证/测试类别划分。我们还评估了方法对参数调优的鲁棒性，并可视化不同方法的排名。最后，我们在大规模ImageNet数据集上评估这些方法。</p></div><p>Reproducing Results. For sanity-check, we re-evaluate methods [22, 42, 39, 4, 32, 7] using provided features and code. We chose SUN and AWA as two representative of fine-grained and non-fine-grained datasets having been widely used in the literature. We observe from the results in Table 2 that our reproduced results and the reported results of DAP and SYNC are identical to the reported number in their original publications. For LATEM, we obtain slightly different results which can be explained by the non-convexity and thus the sensibility to initialization. Similarly for SJE random sampling in SGD might lead to slightly different results. ESZSL has some variance because its algorithm randomly picks a validation set during each run, which leads to different hyperparameters. Notable observations on SSE [42] results are as follows. The published code has hard-coded hyperparameters operational on aPY,</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>重现结果。为了进行合理性检查，我们使用提供的特征和代码重新评估方法[22, 42, 39, 4, 32, 7]。我们选择SUN和AWA作为两个代表性的数据集，分别代表细粒度和非细粒度数据集，这些数据集在文献中被广泛使用。我们从表2中的结果观察到，我们重现的结果与DAP和SYNC的报告结果与其原始出版物中的报告数字完全相同。对于LATEM，我们获得了略有不同的结果，这可以通过非凸性以及对初始化的敏感性来解释。对于SJE，SGD中的随机采样可能导致略有不同的结果。ESZSL有一些方差，因为其算法在每次运行时随机选择一个验证集，这导致不同的超参数。关于SSE [42]结果的显著观察如下。发布的代码在aPY上硬编码了超参数，</p></div><hr>
<!-- Footnote --><p>\({}^{1}\) [34] has public code available,but is not evaluated on SUN or AWA.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{1}\) [34] 有公开的代码可用，但未在SUN或AWA上进行评估。</p></div><!-- Footnote -->
<hr><!-- Media --><table><tbody><tr><td rowspan="2">\( \mathbf{{Method}} \)</td><td colspan="2">SUN</td><td colspan="2">CUB</td><td colspan="2">AWA</td><td colspan="2">\( \mathbf{{aPY}} \)</td></tr><tr><td>SS</td><td>\( \mathbf{{PS}} \)</td><td>SS</td><td>\( \mathbf{{PS}} \)</td><td>SS</td><td>\( \mathbf{{PS}} \)</td><td>SS</td><td>\( \mathbf{{PS}} \)</td></tr><tr><td>DAP [22]</td><td>38.9</td><td>39.9</td><td>37.5</td><td>40.0</td><td>57.1</td><td>44.1</td><td>35.2</td><td>33.8</td></tr><tr><td>CONSE 26</td><td>44.2</td><td>38.8</td><td>36.7</td><td>34.3</td><td>63.6</td><td>45.6</td><td>25.9</td><td>26.9</td></tr><tr><td>CMT [34]</td><td>41.9</td><td>39.9</td><td>37.3</td><td>34.6</td><td>58.9</td><td>39.5</td><td>26.9</td><td>28.0</td></tr><tr><td>SSE [42]</td><td>54.5</td><td>51.5</td><td>43.7</td><td>43.9</td><td>68.8</td><td>60.1</td><td>31.1</td><td>34.0</td></tr><tr><td>LATEM [39]</td><td>56.9</td><td>55.3</td><td>49.4</td><td>49.3</td><td>74.8</td><td>55.1</td><td>34.5</td><td>35.2</td></tr><tr><td>ALE 3</td><td>59.1</td><td>58.1</td><td>53.2</td><td>54.9</td><td>78.6</td><td>59.9</td><td>30.9</td><td>39.7</td></tr><tr><td>DEVISE [11]</td><td>57.5</td><td>56.5</td><td>53.2</td><td>52.0</td><td>72.9</td><td>54.2</td><td>35.4</td><td>39.8</td></tr><tr><td>SJE 4</td><td>57.1</td><td>53.7</td><td>55.3</td><td>53.9</td><td>76.7</td><td>65.6</td><td>32.0</td><td>32.9</td></tr><tr><td>ESZSL [32]</td><td>57.3</td><td>54.5</td><td>55.1</td><td>53.9</td><td>74.7</td><td>58.2</td><td>34.4</td><td>38.3</td></tr><tr><td>SYNC [7]</td><td>59.1</td><td>56.3</td><td>54.1</td><td>55.6</td><td>72.2</td><td>54.0</td><td>39.7</td><td>23.9</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">\( \mathbf{{Method}} \)</td><td colspan="2">太阳</td><td colspan="2">幼崽</td><td colspan="2">阿瓦</td><td colspan="2">\( \mathbf{{aPY}} \)</td></tr><tr><td>SS</td><td>\( \mathbf{{PS}} \)</td><td>SS</td><td>\( \mathbf{{PS}} \)</td><td>SS</td><td>\( \mathbf{{PS}} \)</td><td>SS</td><td>\( \mathbf{{PS}} \)</td></tr><tr><td>DAP [22]</td><td>38.9</td><td>39.9</td><td>37.5</td><td>40.0</td><td>57.1</td><td>44.1</td><td>35.2</td><td>33.8</td></tr><tr><td>CONSE 26</td><td>44.2</td><td>38.8</td><td>36.7</td><td>34.3</td><td>63.6</td><td>45.6</td><td>25.9</td><td>26.9</td></tr><tr><td>CMT [34]</td><td>41.9</td><td>39.9</td><td>37.3</td><td>34.6</td><td>58.9</td><td>39.5</td><td>26.9</td><td>28.0</td></tr><tr><td>SSE [42]</td><td>54.5</td><td>51.5</td><td>43.7</td><td>43.9</td><td>68.8</td><td>60.1</td><td>31.1</td><td>34.0</td></tr><tr><td>LATEM [39]</td><td>56.9</td><td>55.3</td><td>49.4</td><td>49.3</td><td>74.8</td><td>55.1</td><td>34.5</td><td>35.2</td></tr><tr><td>ALE 3</td><td>59.1</td><td>58.1</td><td>53.2</td><td>54.9</td><td>78.6</td><td>59.9</td><td>30.9</td><td>39.7</td></tr><tr><td>设计 [11]</td><td>57.5</td><td>56.5</td><td>53.2</td><td>52.0</td><td>72.9</td><td>54.2</td><td>35.4</td><td>39.8</td></tr><tr><td>SJE 4</td><td>57.1</td><td>53.7</td><td>55.3</td><td>53.9</td><td>76.7</td><td>65.6</td><td>32.0</td><td>32.9</td></tr><tr><td>ESZSL [32]</td><td>57.3</td><td>54.5</td><td>55.1</td><td>53.9</td><td>74.7</td><td>58.2</td><td>34.4</td><td>38.3</td></tr><tr><td>同步 [7]</td><td>59.1</td><td>56.3</td><td>54.1</td><td>55.6</td><td>72.2</td><td>54.0</td><td>39.7</td><td>23.9</td></tr></tbody></table></div><p>Table 3: Zero-shot on SS = Standard Split, PS = Proposed Split using ResNet features (top-1 accuracy in %).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表3：在SS = 标准划分，PS = 使用ResNet特征的提议划分下的零样本（top-1准确率以%表示）。</p></div><!-- figureText: SYNC [2.6] Bank ALE [2.1] DEVISE [3.2 2 SJE [4.2] 2 ESZSL [4.7] SSE [5.2] 1 SYNC [5.3] 2 2 DAP [7.7] CMT [8.8] CONSE [9.2] DEVISE [3.7] ALE [3.8] 2 LATEM [4.2] ESZSL [4.2] 2 2 SSE [6.3] 1 9 DAP [8.2] CONSE [9.0] CMT [9.0] --><img src="https://cdn.noedgeai.com/01972c4f-ca01-770c-bc22-a6f45cd79e9b_5.jpg?x=140&#x26;y=772&#x26;w=697&#x26;h=269&#x26;r=0"><p>Figure 1: Ranking 10 models by setting parameters on three validation splits on the standard (SS, left) and proposed (PS,right) setting. Element(i,j)indicates number of times model \(i\) ranks at \(j\) th over all \(4 \times  3\) observations. Models are ordered by their mean rank (displayed in brackets).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图1：通过在标准（SS，左）和提议（PS，右）设置上对三个验证划分设置参数对10个模型进行排名。元素(i,j)表示模型\(i\)在所有\(4 \times  3\)观察中排名第\(j\)的次数。模型按其平均排名（括号中显示）排序。</p></div><!-- Media --><p>i.e. number of iterations, number of data points to train SVM,and one regularizer parameter \(\gamma\) which lead to inferior results than the ones reported here, therefore we set these parameters on validation sets. On SUN, SSE uses 10 classes (instead of 72) and our results with validated parameters got an improvement of \({0.5}\%\) that may be due to random sampling of training images. On AWA, our reproduced result being \({64.9}\%\) is significantly lower than the reported result (76.3%). However, we could not reach the reported result even by tuning parameters on the test set, i.e. we obtain \({73.8}\%\) in this case.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>即迭代次数、训练SVM的数据点数量，以及一个正则化参数\(\gamma\)，这些参数导致的结果不如这里报告的结果，因此我们在验证集上设置这些参数。在SUN上，SSE使用10个类别（而不是72个），我们使用验证参数得到的结果提高了\({0.5}\%\)，这可能是由于训练图像的随机抽样。在AWA上，我们重现的结果\({64.9}\%\)显著低于报告的结果（76.3%）。然而，即使在测试集上调整参数，我们也无法达到报告的结果，即在这种情况下我们得到\({73.8}\%\)。</p></div><p>Reproduced Results vs Standard Split (SS). In addition to \(\left\lbrack  {{22},{42},{39},4,{32},7,{34}}\right\rbrack\) ,we re-implement [26,11,3] based on the original publications. We use train, validation, test splits as provided in Table 1 and report results on Table 3 with deep ResNet features. DAP [22] uses hand-crafted image features and thus reproduced results with those features are significantly lower than the results with deep features \(\left( {{22.1}\% \text{ vs }{38.9}\% }\right)\) . When we investigate the results in detail, we noticed two irregularities with reported results on SUN. First, SSE [42] and ESZSL [32] report results on a test split with 10 classes whereas the standard split of SUN contains 72 test classes (74.5% vs 54.5% with SSE [42] and \({64.3}\%\) vs \({57.3}\%\) with ESZSL [32]). Second, after careful examination and correspondence with the authors of SYNC [7], we detected that SUN features were extracted with a MIT Places [44] pre-trained model. As MIT Places dataset intersects with both training and test classes of SUN dataset, it is expected to lead to significantly better results than ImageNet pre-trained model (62.8% vs 59.1%).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>重现结果与标准划分（SS）。除了\(\left\lbrack  {{22},{42},{39},4,{32},7,{34}}\right\rbrack\)，我们还根据原始出版物重新实现了[26,11,3]。我们使用表1中提供的训练、验证、测试划分，并在表3中报告使用深度ResNet特征的结果。DAP [22]使用手工制作的图像特征，因此使用这些特征重现的结果显著低于使用深度特征的结果\(\left( {{22.1}\% \text{ vs }{38.9}\% }\right)\)。当我们详细调查结果时，我们注意到SUN上报告结果的两个不规则性。首先，SSE [42]和ESZSL [32]在一个包含10个类别的测试划分上报告结果，而SUN的标准划分包含72个测试类别（SSE [42]为74.5% vs 54.5%，ESZSL [32]为\({64.3}\%\) vs \({57.3}\%\)）。其次，在仔细检查和与SYNC [7]的作者沟通后，我们发现SUN特征是使用MIT Places [44]预训练模型提取的。由于MIT Places数据集与SUN数据集的训练和测试类别都有交集，因此预计会导致比ImageNet预训练模型（62.8% vs 59.1%）显著更好的结果。</p></div><p>Results on Standard (SS) and Proposed Splits (PS). We propose new dataset splits (see details in section 4) insuring that test classes do not belong to the ImageNet \(1\mathrm{\;K}\) that is used to pre-train ResNet. We compare these results (PS) with previously published standard split (SS) results in Table 3 Our first observation is that the results on PS is significantly lower than SS for AWA. This is expected as most of the test classes in SS is included in ImageNet 1K. On the other hand, for fine-grained datasets CUB and SUN, the results are not significantly effected. Our second observation regarding the method ranking is as follows. On SS, SYNC [7] is the best performing method on SUN (59.1%) and aPY (39.7%) datasets whereas SJE [4] performs the best on CUB (55.3%) and ALE [3] performs the best on AWA (78.6%) dataset. On PS, ALE [3] performs the best on SUN (58.1%), SYNC [7] on CUB (55.6%), SJE [4] on AWA (65.6%) and DEVISE [11] on aPY (39.8%). Note that ALE, SJE and DEVISE all use max-margin bi-linear compatibility learning framework.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>标准（SS）和提议划分（PS）上的结果。我们提出新的数据集划分（详见第4节），确保测试类别不属于用于预训练ResNet的ImageNet\(1\mathrm{\;K}\)。我们将这些结果（PS）与之前发布的标准划分（SS）结果进行比较，见表3。我们的第一观察是，AWA的PS结果显著低于SS。这是可以预期的，因为SS中的大多数测试类别包含在ImageNet 1K中。另一方面，对于细粒度数据集CUB和SUN，结果没有显著影响。我们关于方法排名的第二个观察如下。在SS上，SYNC [7]是SUN（59.1%）和aPY（39.7%）数据集上表现最好的方法，而SJE [4]在CUB（55.3%）上表现最佳，ALE [3]在AWA（78.6%）数据集上表现最佳。在PS上，ALE [3]在SUN（58.1%）上表现最佳，SYNC [7]在CUB（55.6%）上表现最佳，SJE [4]在AWA（65.6%）上表现最佳，DEVISE [11]在aPY（39.8%）上表现最佳。请注意，ALE、SJE和DEVISE都使用最大边际双线性兼容性学习框架。</p></div><p>Robustness. We evaluate robustness of 10 methods to parameters by setting them on 3 different validation splits while keeping the test split intact. We report results on SS (Figure 2, top) and PS (Figure 2, bottom). On SUN and CUB, the results are stable across methods and across splits. This is expected as these datasets have balanced number of images across classes and due to their fine-grained nature, the validation splits are similar. On the other hand, AWA and aPY being small and coarse-grained datasets have several issues. First, many of the test classes on AWA and aPY are included in ImageNet1K. Second, they are not well balanced, i.e. different validation class splits contain significantly different number of images. Third, the class embed-dings are far from each other, i.e. objects are semantically different, therefore different validation splits learn a different mapping between images and classes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>鲁棒性。我们通过在3个不同的验证集划分上设置10种方法的参数，同时保持测试集不变，来评估这些方法的鲁棒性。我们在SS（图2，上）和PS（图2，下）上报告结果。在SUN和CUB上，结果在不同方法和划分之间是稳定的。这是可以预期的，因为这些数据集在各个类别之间的图像数量是平衡的，并且由于其细粒度特性，验证划分是相似的。另一方面，AWA和aPY作为小型和粗粒度数据集存在几个问题。首先，AWA和aPY上的许多测试类别包含在ImageNet1K中。其次，它们的平衡性较差，即不同的验证类别划分包含的图像数量显著不同。第三，类别嵌入彼此相距较远，即对象在语义上是不同的，因此不同的验证划分学习到的图像与类别之间的映射不同。</p></div><p>Visualizing Method Ranking. We rank the 10 methods based on their per-class top-1 accuracy using the nonparametric Friedman test [14], which does not assume a distribution on performance but rather uses algorithm ranking. Each entry of the rank matrix on Figure 1 indicates the number of times the method is ranked at the first to tenth rank. We then compute the mean rank of each method and order them based on that. Our general observation is that the highest ranked method on the standard split (SS) is SYNC while on the proposed split (PS) it is ALE. These results indicate the importance of choosing zero-shot splits carefully. On the proposed split, the three highest ranked methods are compatibility learning methods, i.e. ALE, DEVISE and SJE whereas the three lowest ranked methods are attribute classifier learning or hybrid methods, i.e. DAP, CMT and CONSE. Therefore, max-margin compatibility learning methods lead to consistently better results in the zero-shot learning task compared to learning independent classifiers.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>可视化方法排名。我们基于每个类别的top-1准确率使用非参数Friedman检验对10种方法进行排名[14]，该检验不假设性能的分布，而是使用算法排名。图1中的排名矩阵的每个条目表示该方法被排名为第一到第十的次数。然后，我们计算每种方法的平均排名，并根据此进行排序。我们的总体观察是，在标准划分（SS）中排名最高的方法是SYNC，而在提议的划分（PS）中是ALE。这些结果表明仔细选择零样本划分的重要性。在提议的划分中，排名前三的方法是兼容性学习方法，即ALE、DEVISE和SJE，而排名最低的三种方法是属性分类器学习或混合方法，即DAP、CMT和CONSE。因此，最大边际兼容性学习方法在零样本学习任务中相比于学习独立分类器，始终能取得更好的结果。</p></div><!-- Media --><!-- figureText: SUN CUB AWA APY 80 Top-1 Acc. (in %) DAF CONSE 40 CMT SSE LATEM ALE DEVISE SJE ESZSL SYNC Models AWA APY 60 Top-1 Acc. (in %) CONSE SSE LATEN 20 DEVISE SJE ESZSI SYNC 3 6 Models Models 80 80 60 60 Top-1 Acc. (in %) Top-1 Acc. (in %) Top-1 Acc. (in %) 40 0 4 7 10 3 Models Models SUN CUE 80 60 60 Top-1 Acc. (in %) 20 Top-1 Acc. (in %) 20 Top-1 Acc. (in %) 8 Models Models --><img src="https://cdn.noedgeai.com/01972c4f-ca01-770c-bc22-a6f45cd79e9b_6.jpg?x=138&#x26;y=196&#x26;w=1469&#x26;h=597&#x26;r=0"><p>Figure 2: Robustness of 10 methods evaluated on SUN, CUB, AWA, aPY using 3 validation set splits (results are on the same test split). Top: original split, Bottom: proposed split (Image embeddings = ResNet). We measure top-1 accuracy in %.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2：在SUN、CUB、AWA、aPY上评估的10种方法的鲁棒性，使用3个验证集划分（结果基于相同的测试集）。上：原始划分，下：提议划分（图像嵌入 = ResNet）。我们以百分比测量top-1准确率。</p></div><table><tbody><tr><td rowspan="2">\( \mathbf{{Method}} \)</td><td colspan="2">Hierarchy</td><td colspan="3">\( \mathbf{{Most}} \)Populated</td><td colspan="4">LeastPopulatedAll</td></tr><tr><td>\( 2\mathrm{H} \)</td><td>\( 3\mathrm{\;H} \)</td><td>500</td><td>1K</td><td>5K</td><td>500</td><td>1K</td><td>5K</td><td>20K</td></tr><tr><td>CONSE 26</td><td>7.63</td><td>2.18</td><td>12.33</td><td>8.31</td><td>3.22</td><td>3.53</td><td>2.69</td><td>1.05</td><td>0.95</td></tr><tr><td>CMT [34]</td><td>2.88</td><td>0.67</td><td>5.10</td><td>3.04</td><td>1.04</td><td>1.87</td><td>1.08</td><td>0.33</td><td>0.29</td></tr><tr><td>LATEM [39]</td><td>5.45</td><td>1.32</td><td>10.81</td><td>6.63</td><td>1.90</td><td>4.53</td><td>2.74</td><td>0.76</td><td>0.50</td></tr><tr><td>ALE [3]</td><td>5.38</td><td>1.32</td><td>10.40</td><td>6.77</td><td>2.00</td><td>4.27</td><td>2.85</td><td>0.79</td><td>0.50</td></tr><tr><td>DEVISE [11]</td><td>5.25</td><td>1.29</td><td>10.36</td><td>6.68</td><td>1.94</td><td>4.23</td><td>2.86</td><td>0.78</td><td>0.49</td></tr><tr><td>SJE 4</td><td>5.31</td><td>1.33</td><td>9.88</td><td>6.53</td><td>1.99</td><td>4.93</td><td>2.93</td><td>0.78</td><td>0.52</td></tr><tr><td>ESZSL [32]</td><td>6.35</td><td>1.51</td><td>11.91</td><td>7.69</td><td>2.34</td><td>4.50</td><td>3.23</td><td>0.94</td><td>0.62</td></tr><tr><td>SYNC [7]</td><td>9.26</td><td>2.29</td><td>15.83</td><td>10.75</td><td>3.42</td><td>5.83</td><td>3.52</td><td>1.26</td><td>0.96</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">\( \mathbf{{Method}} \)</td><td colspan="2">层级</td><td colspan="3">\( \mathbf{{Most}} \)已有人口</td><td colspan="4">最少人口所有</td></tr><tr><td>\( 2\mathrm{H} \)</td><td>\( 3\mathrm{\;H} \)</td><td>500</td><td>1K</td><td>5K</td><td>500</td><td>1K</td><td>5K</td><td>20K</td></tr><tr><td>CONSE 26</td><td>7.63</td><td>2.18</td><td>12.33</td><td>8.31</td><td>3.22</td><td>3.53</td><td>2.69</td><td>1.05</td><td>0.95</td></tr><tr><td>CMT [34]</td><td>2.88</td><td>0.67</td><td>5.10</td><td>3.04</td><td>1.04</td><td>1.87</td><td>1.08</td><td>0.33</td><td>0.29</td></tr><tr><td>LATEM [39]</td><td>5.45</td><td>1.32</td><td>10.81</td><td>6.63</td><td>1.90</td><td>4.53</td><td>2.74</td><td>0.76</td><td>0.50</td></tr><tr><td>ALE [3]</td><td>5.38</td><td>1.32</td><td>10.40</td><td>6.77</td><td>2.00</td><td>4.27</td><td>2.85</td><td>0.79</td><td>0.50</td></tr><tr><td>DEVISE [11]</td><td>5.25</td><td>1.29</td><td>10.36</td><td>6.68</td><td>1.94</td><td>4.23</td><td>2.86</td><td>0.78</td><td>0.49</td></tr><tr><td>SJE 4</td><td>5.31</td><td>1.33</td><td>9.88</td><td>6.53</td><td>1.99</td><td>4.93</td><td>2.93</td><td>0.78</td><td>0.52</td></tr><tr><td>ESZSL [32]</td><td>6.35</td><td>1.51</td><td>11.91</td><td>7.69</td><td>2.34</td><td>4.50</td><td>3.23</td><td>0.94</td><td>0.62</td></tr><tr><td>同步 [7]</td><td>9.26</td><td>2.29</td><td>15.83</td><td>10.75</td><td>3.42</td><td>5.83</td><td>3.52</td><td>1.26</td><td>0.96</td></tr></tbody></table></div><p>Table 4: ImageNet with different splits: \(2/3\mathrm{H} =\) classes with \(2/3\) hops away from \(1\mathrm{\;K}{\mathcal{Y}}^{tr},{500}/1\mathrm{\;K}/5\mathrm{\;K}\) most populated classes, \({500}/1\mathrm{\;K}/5\mathrm{\;K}\) least populated classes, \(\mathrm{{All}} = {20}\mathrm{\;K}\) categories of ImageNet. We measure top-1 accuracy in %.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表4：不同划分的ImageNet：与\(2/3\)跳数远离\(1\mathrm{\;K}{\mathcal{Y}}^{tr},{500}/1\mathrm{\;K}/5\mathrm{\;K}\)最多类的\(2/3\mathrm{H} =\)类，\({500}/1\mathrm{\;K}/5\mathrm{\;K}\)最少类，\(\mathrm{{All}} = {20}\mathrm{\;K}\)类别的ImageNet。我们以百分比测量top-1准确率。</p></div><!-- Media --><p>Results on ImageNet. ImageNet scales the methods to a truly large-scale setting, thus these experiments provide further insights on how to tackle the zero-shot learning problem from the practical point of view. Here, we evaluate 8 methods. We exclude DAP as attributes are not available for all ImageNet classes and SSE due to scalability issues of the public implementation of the method. Table 4 shows that the best performing method is SYNC [7] which may indicate that it performs well in large-scale setting or it can learn under uncertainty due to usage of Word2Vec instead of attributes. Another possibility is Word2Vec may be tuned for SYNC as it is provided by the same authors however making a strong claim requires a full evaluation on class embeddings which is out of the scope of this paper. Our general observation from all the methods is that in the most populated classes, the results are higher than the least populated classes which indicates that fine-grained subsets are more difficult. We consistently observe a large drop in accuracy between \(1\mathrm{\;K}\) and \(5\mathrm{\;K}\) most populated classes which is expected as \(5\mathrm{\;K}\) contains \(\approx  {6.6}\mathrm{M}\) images,making the problem much more difficult than \(1\mathrm{\;K}\) ( \(\approx  {1624}\) images). On the other hand, All 20K results are poor for all methods which indicates the difficulty of this problem where there is a large room for improvement.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在ImageNet上的结果。ImageNet将方法扩展到真正的大规模设置，因此这些实验提供了从实际角度解决零样本学习问题的进一步见解。在这里，我们评估了8种方法。我们排除了DAP，因为并非所有ImageNet类都有属性可用，并且由于公共实现方法的可扩展性问题排除了SSE。表4显示，表现最佳的方法是SYNC [7]，这可能表明它在大规模设置中表现良好，或者由于使用Word2Vec而能够在不确定性下学习。另一种可能性是Word2Vec可能针对SYNC进行了调整，因为它是由同一作者提供的，但做出强有力的声明需要对类嵌入进行全面评估，这超出了本文的范围。我们对所有方法的一般观察是，在最多类中，结果高于最少类，这表明细粒度子集更难。我们始终观察到在\(1\mathrm{\;K}\)和\(5\mathrm{\;K}\)最多类之间准确率大幅下降，这是可以预期的，因为\(5\mathrm{\;K}\)包含\(\approx  {6.6}\mathrm{M}\)张图像，使得问题比\(1\mathrm{\;K}\)（\(\approx  {1624}\)张图像）更困难。另一方面，所有20K结果对于所有方法都很差，这表明这个问题的难度很大，改进的空间很大。</p></div><h3>5.2. Generalized Zero-Shot Learning Results</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.2. 泛化零样本学习结果</h3></div><p>In real world applications, image classification systems do not have access to whether a novel image belongs to a seen or unseen class in advance. Hence, generalized zero-shot learning is interesting from a practical point of view. Here, we use same models trained on zero-shot learning setting on our proposed splits (PS). We evaluate performance on both \({\mathcal{Y}}^{tr}\) and \({\mathcal{Y}}^{ts}\) ,i.e. using held-out images from \({\mathcal{Y}}^{ts}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在现实世界的应用中，图像分类系统无法提前知道新图像是否属于已见或未见的类。因此，从实际角度来看，泛化零样本学习是有趣的。在这里，我们使用在我们提出的划分（PS）上训练的相同模型进行零样本学习设置。我们在\({\mathcal{Y}}^{tr}\)和\({\mathcal{Y}}^{ts}\)上评估性能，即使用来自\({\mathcal{Y}}^{ts}\)的保留图像。</p></div><p>As shown in Table 5, generalized zero-shot results are significantly lower than zero-shot results as training classes are included in the search space. Another interesting observation is that compatibility learning frameworks, e.g. ALE, DEVISE, SJE, perform well on test classes. However, methods that learn independent attribute or object classifiers, e.g. DAP and CONSE, perform well on training classes. Due to this discrepancy, we evaluate the harmonic mean which takes a weighted average of training and test class accuracy. \(\mathrm{H}\) measure ranks \(\mathrm{{ALE}}\) as the best performing method on SUN, CUB and AWA datasets whereas on aPY dataset CMT* performs the best. Note that CMT* has an integrated novelty detection phase for which the method receives another supervision signal determining if the image belongs to a train or a test class. As a summary, generalized zero-shot learning setting provides one more level of detail on the performance of zero-shot learning methods. Our take-home message is that the accuracy of training classes is as important as the accuracy of test classes in real world scenarios. Therefore, methods should be designed in a way that they are able to predict labels well in train and test classes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>如表5所示，泛化零样本结果显著低于零样本结果，因为训练类被包含在搜索空间中。另一个有趣的观察是，兼容性学习框架，例如ALE、DEVISE、SJE，在测试类上表现良好。然而，学习独立属性或对象分类器的方法，例如DAP和CONSE，在训练类上表现良好。由于这种差异，我们评估了调和平均数，它对训练和测试类的准确率进行加权平均。\(\mathrm{H}\)测量将\(\mathrm{{ALE}}\)评为SUN、CUB和AWA数据集上表现最佳的方法，而在aPY数据集上CMT<em>表现最佳。请注意，CMT</em>具有集成的新颖性检测阶段，该方法接收另一个监督信号，以确定图像属于训练类还是测试类。总之，泛化零样本学习设置提供了零样本学习方法性能的更详细级别。我们的主要信息是，在现实世界场景中，训练类的准确率与测试类的准确率同样重要。因此，方法应设计成能够在训练和测试类中良好预测标签的方式。</p></div><!-- Media --><table><tbody><tr><td rowspan="2">\( \mathbf{{Method}} \)</td><td colspan="3">SUN</td><td colspan="3">CUB</td><td colspan="3">AWA</td><td colspan="3">\( \mathbf{{aPY}} \)</td></tr><tr><td>ts</td><td>tr</td><td>\( \mathbf{H} \)</td><td>ts</td><td>tr</td><td>H</td><td>ts</td><td>tr</td><td>H</td><td>ts</td><td>tr</td><td>\( \mathbf{H} \)</td></tr><tr><td>DAP [22]</td><td>4.2</td><td>25.1</td><td>7.2</td><td>1.7</td><td>67.9</td><td>3.3</td><td>0.0</td><td>88.7</td><td>0.0</td><td>4.8</td><td>78.3</td><td>9.0</td></tr><tr><td>CONSE [26]</td><td>6.8</td><td>39.9</td><td>11.6</td><td>1.6</td><td>72.2</td><td>3.1</td><td>0.4</td><td>88.6</td><td>0.8</td><td>0.0</td><td>91.2</td><td>0.0</td></tr><tr><td>CMT [34]</td><td>8.1</td><td>21.8</td><td>11.8</td><td>7.2</td><td>49.8</td><td>12.6</td><td>0.9</td><td>87.6</td><td>1.8</td><td>1.4</td><td>85.2</td><td>2.8</td></tr><tr><td>CMT* [34]</td><td>8.7</td><td>28.0</td><td>13.3</td><td>4.7</td><td>60.1</td><td>8.7</td><td>8.4</td><td>86.9</td><td>15.3</td><td>10.9</td><td>74.2</td><td>19.0</td></tr><tr><td>SSE [42]</td><td>2.1</td><td>36.4</td><td>4.0</td><td>8.5</td><td>46.9</td><td>14.4</td><td>7.0</td><td>80.5</td><td>12.9</td><td>0.2</td><td>78.9</td><td>0.4</td></tr><tr><td>LATEM [39]</td><td>14.7</td><td>28.8</td><td>19.5</td><td>15.2</td><td>57.3</td><td>24.0</td><td>7.3</td><td>71.7</td><td>13.3</td><td>0.1</td><td>73.0</td><td>0.2</td></tr><tr><td>ALE 3</td><td>21.8</td><td>33.1</td><td>26.3</td><td>23.7</td><td>62.8</td><td>34.4</td><td>16.8</td><td>76.1</td><td>27.5</td><td>4.6</td><td>73.7</td><td>8.7</td></tr><tr><td>DEVISE [11]</td><td>16.9</td><td>27.4</td><td>20.9</td><td>23.8</td><td>53.0</td><td>32.8</td><td>13.4</td><td>68.7</td><td>22.4</td><td>4.9</td><td>76.9</td><td>9.2</td></tr><tr><td>SJE [4]</td><td>14.7</td><td>30.5</td><td>19.8</td><td>23.5</td><td>59.2</td><td>33.6</td><td>11.3</td><td>74.6</td><td>19.6</td><td>3.7</td><td>55.7</td><td>6.9</td></tr><tr><td>ESZSL [32]</td><td>11.0</td><td>27.9</td><td>15.8</td><td>12.6</td><td>63.8</td><td>21.0</td><td>6.6</td><td>75.6</td><td>12.1</td><td>2.4</td><td>70.1</td><td>4.6</td></tr><tr><td>SYNC [7]</td><td>7.9</td><td>43.3</td><td>13.4</td><td>11.5</td><td>70.9</td><td>19.8</td><td>8.9</td><td>87.3</td><td>16.2</td><td>7.4</td><td>66.3</td><td>13.3</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">\( \mathbf{{Method}} \)</td><td colspan="3">太阳</td><td colspan="3">幼崽</td><td colspan="3">AWA</td><td colspan="3">\( \mathbf{{aPY}} \)</td></tr><tr><td>ts</td><td>tr</td><td>\( \mathbf{H} \)</td><td>ts</td><td>tr</td><td>H</td><td>ts</td><td>tr</td><td>H</td><td>ts</td><td>tr</td><td>\( \mathbf{H} \)</td></tr><tr><td>DAP [22]</td><td>4.2</td><td>25.1</td><td>7.2</td><td>1.7</td><td>67.9</td><td>3.3</td><td>0.0</td><td>88.7</td><td>0.0</td><td>4.8</td><td>78.3</td><td>9.0</td></tr><tr><td>CONSE [26]</td><td>6.8</td><td>39.9</td><td>11.6</td><td>1.6</td><td>72.2</td><td>3.1</td><td>0.4</td><td>88.6</td><td>0.8</td><td>0.0</td><td>91.2</td><td>0.0</td></tr><tr><td>CMT [34]</td><td>8.1</td><td>21.8</td><td>11.8</td><td>7.2</td><td>49.8</td><td>12.6</td><td>0.9</td><td>87.6</td><td>1.8</td><td>1.4</td><td>85.2</td><td>2.8</td></tr><tr><td>CMT* [34]</td><td>8.7</td><td>28.0</td><td>13.3</td><td>4.7</td><td>60.1</td><td>8.7</td><td>8.4</td><td>86.9</td><td>15.3</td><td>10.9</td><td>74.2</td><td>19.0</td></tr><tr><td>SSE [42]</td><td>2.1</td><td>36.4</td><td>4.0</td><td>8.5</td><td>46.9</td><td>14.4</td><td>7.0</td><td>80.5</td><td>12.9</td><td>0.2</td><td>78.9</td><td>0.4</td></tr><tr><td>LATEM [39]</td><td>14.7</td><td>28.8</td><td>19.5</td><td>15.2</td><td>57.3</td><td>24.0</td><td>7.3</td><td>71.7</td><td>13.3</td><td>0.1</td><td>73.0</td><td>0.2</td></tr><tr><td>ALE 3</td><td>21.8</td><td>33.1</td><td>26.3</td><td>23.7</td><td>62.8</td><td>34.4</td><td>16.8</td><td>76.1</td><td>27.5</td><td>4.6</td><td>73.7</td><td>8.7</td></tr><tr><td>DEVISE [11]</td><td>16.9</td><td>27.4</td><td>20.9</td><td>23.8</td><td>53.0</td><td>32.8</td><td>13.4</td><td>68.7</td><td>22.4</td><td>4.9</td><td>76.9</td><td>9.2</td></tr><tr><td>SJE [4]</td><td>14.7</td><td>30.5</td><td>19.8</td><td>23.5</td><td>59.2</td><td>33.6</td><td>11.3</td><td>74.6</td><td>19.6</td><td>3.7</td><td>55.7</td><td>6.9</td></tr><tr><td>ESZSL [32]</td><td>11.0</td><td>27.9</td><td>15.8</td><td>12.6</td><td>63.8</td><td>21.0</td><td>6.6</td><td>75.6</td><td>12.1</td><td>2.4</td><td>70.1</td><td>4.6</td></tr><tr><td>SYNC [7]</td><td>7.9</td><td>43.3</td><td>13.4</td><td>11.5</td><td>70.9</td><td>19.8</td><td>8.9</td><td>87.3</td><td>16.2</td><td>7.4</td><td>66.3</td><td>13.3</td></tr></tbody></table></div><p>Table 5: Generalized Zero-Shot Learning on Proposed Split (PS) measuring ts = Top-1 accuracy on \({\mathcal{Y}}^{ts}\) ,tr=Top-1 accuracy on \({\mathcal{Y}}^{{tr} + {ts}}\) ), \(\mathrm{H} =\) harmonic mean (CMT*: CMT with novelty detection). We measure top-1 accuracy in \(\%\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表5：在提出的划分（PS）上进行的广义零样本学习，测量ts = Top-1准确率在\({\mathcal{Y}}^{ts}\)，tr=Top-1准确率在\({\mathcal{Y}}^{{tr} + {ts}}\)），\(\mathrm{H} =\)调和平均（CMT*: 带有新颖性检测的CMT）。我们在\(\%\)中测量Top-1准确率。</p></div><!-- figureText: Rank Rank 10 11 CONSE [1.5] SYNC [4.6] 3 1 ALE [5.1] 3 CMT* [5.8] CMT [6.3] ESZSL [6.9] 2 SSE [7.0] 4 1 2 3 LATEM [7.7] DEVISE [7.8] 1 1 SJE [7.9] Rank 10 11 ALE [2.2] 2 DEVISE [2.3] SJE [4.0] ESZSL [5.2] 2 1 LATEM [5.5] 1 CMT* [5.6] 3 SYNC [6.0] 2 1 CMT [7.9] SSE [8.0] DAP [9.1] 2 1 CONSE [10.2] ALE [2.0] 2.2 SJE [3.8] 3 3 ESZSL [5.3] SYNC [5.4] CMT* [5.5] LATEM [5.5] SSE [8.1] CMT [8.3] DAP [9.1] --><img src="https://cdn.noedgeai.com/01972c4f-ca01-770c-bc22-a6f45cd79e9b_7.jpg?x=141&#x26;y=824&#x26;w=706&#x26;h=535&#x26;r=0"><p>Figure 3: Ranking 11 models on the proposed split (PS) in generalized zero-shot learning setting. Top-Left: on unseen cla sses (ts) accuracy, Top-Right: on seen classes (tr) accuracy, Bottom: on Harmonic mean (H).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3：在广义零样本学习设置中对提出的划分（PS）上的11个模型进行排名。左上：在未见类别（ts）上的准确率，右上：在已见类别（tr）上的准确率，底部：在调和平均（H）上的准确率。</p></div><!-- Media --><p>Visualizing Method Ranking. Similar to the analysis in the previous section, we rank the 11 methods based on per-class top-1 accuracy on train classes, test classes and based on Harmonic mean of the two. Looking at the rank matrix obtained by evaluating on test classes, i.e. Figure 3 top left, highest ranked 5 methods are the same as in Figure 1, i.e. ALE, DEVISE, SJE, LATEM, ESZSL while overall the absolute numbers are lower. Looking at the rank matrix obtained by evaluating the harmonic mean, i.e. Figure 3 bottom, the highest ranked 3 methods are the same as in Figure 1, i.e. ALE, DEVISE, SJE. Looking at the rank matrix obtained by evaluating on train classes, i.e. Figure 3 top right, our observations are different from Figure 1. ALE is ranked the 3rd but other highest ranked methods are at the bottom of this rank list. These results clearly suggest that we should not only optimize for test class accuracy but also for train class accuracy when evaluating zero-shot learning. Our final observation from Figure 3 is that CMT* is better than CMT in all cases which supports the argument that a simple novelty detection scheme helps to improve results.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>可视化方法排名。与前一部分的分析类似，我们根据训练类别、测试类别的每类Top-1准确率以及两者的调和平均对11种方法进行排名。查看通过在测试类别上评估获得的排名矩阵，即图3左上，排名前5的方法与图1中的相同，即ALE、DEVISE、SJE、LATEM、ESZSL，而整体绝对数值较低。查看通过评估调和平均获得的排名矩阵，即图3底部，排名前3的方法与图1中的相同，即ALE、DEVISE、SJE。查看通过在训练类别上评估获得的排名矩阵，即图3右上，我们的观察与图1不同。ALE排名第3，但其他排名最高的方法位于该排名列表的底部。这些结果清楚地表明，在评估零样本学习时，我们不仅应优化测试类别的准确率，还应优化训练类别的准确率。我们从图3的最终观察是，CMT*在所有情况下都优于CMT，这支持了简单的新颖性检测方案有助于改善结果的论点。</p></div><h2>6. Conclusion</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>6. 结论</h2></div><p>In this work, we evaluated a significant number of state-of-the-art zero-shot learning methods on several datasets within a unified evaluation protocol both in zero-shot and generalized zero-shot settings. Our evaluation showed that compatibility learning frameworks have an edge over learning independent object or attribute classifiers and also over hybrid models. We discovered that some standard zero-shot splits may treat feature learning disjoint from the training stage and accordingly proposed new dataset splits. Moreover, disjoint training and validation class split is a necessary component of parameter tuning in zero-shot learning setting. Including training classes in the search space while evaluating the methods, i.e. generalized zero-shot learning, provides an interesting playground for future research. In summary, our work extensively evaluated the good and bad aspects of zero-shot learning while sanitizing the ugly ones. References</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在这项工作中，我们在多个数据集上评估了大量最先进的零样本学习方法，采用统一的评估协议，涵盖零样本和广义零样本设置。我们的评估显示，兼容性学习框架在学习独立对象或属性分类器以及混合模型方面具有优势。我们发现一些标准的零样本划分可能将特征学习与训练阶段分开，因此提出了新的数据集划分。此外，训练和验证类别的分离划分是零样本学习设置中参数调优的必要组成部分。在评估方法时将训练类别纳入搜索空间，即广义零样本学习，为未来的研究提供了一个有趣的探索空间。总之，我们的工作广泛评估了零样本学习的优缺点，同时清理了其中的不足之处。参考文献</p></div><p>[1] Z. Akata, M. Malinowski, M. Fritz, and B. Schiele. Multi-cue zero-shot learning with strong supervision. In CVPR, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[1] Z. Akata, M. Malinowski, M. Fritz, 和 B. Schiele. 强监督下的多线索零样本学习. 在CVPR，2016。</p></div><p>[2] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label embedding for attribute-based classification. In CVPR, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[2] Z. Akata, F. Perronnin, Z. Harchaoui, 和 C. Schmid. 基于属性的分类的标签嵌入. 在CVPR，2013。</p></div><p>[3] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-embedding for image classification. TPAMI, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[3] Z. Akata, F. Perronnin, Z. Harchaoui, 和 C. Schmid. 图像分类的标签嵌入. TPAMI，2016。</p></div><p>[4] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Evaluation of output embeddings for fine-grained image classification. In \({CVPR},{2015}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[4] Z. Akata, S. Reed, D. Walter, H. Lee, 和 B. Schiele. 对细粒度图像分类的输出嵌入进行评估. 在\({CVPR},{2015}\)。</p></div><p>[5] A. Bendale and T. E. Boult. Towards open set deep networks. In \({CVPR},{2016}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[5] A. Bendale 和 T. E. Boult. 朝着开放集深度网络的方向. 在\({CVPR},{2016}\)。</p></div><p>[6] M. Bucher, S. Herbin, and F. Jurie. Improving semantic embedding consistency by metric learning for zero-shot classif-fication. In \({ECCV},{2016}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[6] M. Bucher, S. Herbin, 和 F. Jurie. 通过度量学习提高零样本分类的语义嵌入一致性. 在\({ECCV},{2016}\)。</p></div><p>[7] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Synthesized classifiers for zero-shot learning. In CVPR, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[7] S. Changpinyo, W.-L. Chao, B. Gong, 和 F. Sha. 零样本学习的合成分类器. 在CVPR，2016。</p></div><p>[8] W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha. An empirical study and analysis of generalized zero-shot learning for object recognition in the wild. In \({ECCV},{2016}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[8] W.-L. Chao, S. Changpinyo, B. Gong, 和 F. Sha. 对野外物体识别的广义零样本学习的实证研究和分析. 在\({ECCV},{2016}\)。</p></div><p>[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR, 2009.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, 和 L. Fei-Fei. ImageNet：一个大规模分层图像数据库. 在CVPR，2009。</p></div><p>[10] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. CVPR, 2009.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[10] A. Farhadi, I. Endres, D. Hoiem, 和 D. Forsyth. 通过属性描述对象. CVPR, 2009.</p></div><p>[11] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. A. Ranzato, and T. Mikolov. Devise: A deep visual-semantic embedding model. In NIPS, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[11] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. A. Ranzato, 和 T. Mikolov. Devise: 一种深度视觉-语义嵌入模型. 在 NIPS, 2013.</p></div><p>[12] Y. Fu and L. Sigal. Semi-supervised vocabulary-informed learning. In \({CVPR},{2016}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[12] Y. Fu 和 L. Sigal. 半监督词汇信息学习. 在 \({CVPR},{2016}\) .</p></div><p>[13] Z. Fu, T. Xiang, E. Kodirov, and S. Gong. Zero-shot object recognition by semantic manifold distance. In CVPR, June 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[13] Z. Fu, T. Xiang, E. Kodirov, 和 S. Gong. 通过语义流形距离进行零样本对象识别. 在 CVPR, 2015年6月.</p></div><p>[14] S. Garcia and F. Herrera. An extension on"statistical comparisons of classifiers over multiple data sets"for all pairwise comparisons. JLMR, 9:2677-2694, 2008.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[14] S. Garcia 和 F. Herrera. 关于“多个数据集上分类器的统计比较”的扩展，适用于所有成对比较. JLMR, 9:2677-2694, 2008.</p></div><p>[15] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning (2nd Ed.). Springer Series in Statistics. Springer, 2008.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[15] T. Hastie, R. Tibshirani, 和 J. Friedman. 统计学习的要素（第二版）. 施普林格统计系列. 施普林格, 2008.</p></div><p>[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In \({CVPR},{2016}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[16] K. He, X. Zhang, S. Ren, 和 J. Sun. 用于图像识别的深度残差学习. 在 \({CVPR},{2016}\) .</p></div><p>[17] S. Huang, M. Elhoseiny, A. M. Elgammal, and D. Yang. Learning hypergraph-regularized attribute predictors. In CVPR, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[17] S. Huang, M. Elhoseiny, A. M. Elgammal, 和 D. Yang. 学习超图正则化属性预测器. 在 CVPR, 2015.</p></div><p>[18] S. B. J. Weston and N. Usunier. Large scale image annotation: Learning to rank with joint word-image embeddings. In \({ECML},{2010}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[18] S. B. J. Weston 和 N. Usunier. 大规模图像注释：通过联合词-图像嵌入学习排序. 在 \({ECML},{2010}\) .</p></div><p>[19] L. Jain, W. Scheirer, and T. Boult. Multi-class open set recognition using probability of inclusion. In \({ECCV},{2014}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[19] L. Jain, W. Scheirer, 和 T. Boult. 使用包含概率的多类开放集识别. 在 \({ECCV},{2014}\) .</p></div><p>[20] T. Joachims. Optimizing search engines using clickthrough data. In \({KDD}\) . ACM,2002.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[20] T. Joachims. 使用点击数据优化搜索引擎. 在 \({KDD}\) . ACM, 2002.</p></div><p>[21] E. Kodirov, T. Xiang, Z. Fu, and S. Gong. Unsupervised domain adaptation for zero-shot learning. In \({ICCV},{2015}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[21] E. Kodirov, T. Xiang, Z. Fu, 和 S. Gong. 零样本学习的无监督领域适应. 在 \({ICCV},{2015}\) .</p></div><p>[22] C. Lampert, H. Nickisch, and S. Harmeling. Attribute-based classification for zero-shot visual object categorization. In TPAMI, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[22] C. Lampert, H. Nickisch, 和 S. Harmeling. 基于属性的分类用于零样本视觉对象分类. 在 TPAMI, 2013.</p></div><p>[23] H. Larochelle, D. Erhan, and Y. Bengio. Zero-data learning of new tasks. In \({AAAI},{2008}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[23] H. Larochelle, D. Erhan, 和 Y. Bengio. 新任务的零数据学习. 在 \({AAAI},{2008}\) .</p></div><p>[24] T. Mensink, E. Gavves, and C. G. Snoek. Costa: Co-occurrence statistics for zero-shot classification. In CVPR, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[24] T. Mensink, E. Gavves, 和 C. G. Snoek. Costa: 用于零样本分类的共现统计. 在 CVPR, 2014.</p></div><p>[25] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[25] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, 和 J. Dean. 单词和短语的分布式表示及其组合性. 在 NIPS, 2013.</p></div><p>[26] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens, A. Frome, G. Corrado, and J. Dean. Zero-shot learning by convex combination of semantic embeddings. In ICLR, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[26] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens, A. Frome, G. Corrado, 和 J. Dean. 通过语义嵌入的凸组合进行零样本学习. 在 ICLR, 2014.</p></div><p>[27] M. Palatucci, D. Pomerleau, G. E. Hinton, and T. M. Mitchell. Zero-shot learning with semantic output codes. In NIPS, 2009.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[27] M. Palatucci, D. Pomerleau, G. E. Hinton, 和 T. M. Mitchell. 使用语义输出代码的零样本学习. 在 NIPS, 2009.</p></div><p>[28] G. Patterson and J. Hays. Sun attribute database: Discovering, annotating, and recognizing scene attributes. In CVPR, 2012.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[28] G. Patterson 和 J. Hays. 太阳属性数据库：发现、注释和识别场景属性. 在 CVPR, 2012.</p></div><p>[29] R. Qiao, L. Liu, C. Shen, and A. van den Hengel. Less is more: Zero-shot learning from online textual documents with noise suppression. In \({CVPR},{2016}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[29] R. Qiao, L. Liu, C. Shen, 和 A. van den Hengel. 少即是多：从在线文本文档中进行零样本学习并抑制噪声. 在 \({CVPR},{2016}\) .</p></div><p>[30] M. Rohrbach, M. Stark, and B.Schiele. Evaluating knowledge transfer and zero-shot learning in a large-scale setting. In \({CVPR},{2011}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[30] M. Rohrbach, M. Stark, 和 B.Schiele. 在大规模环境中评估知识转移和零样本学习. 在 \({CVPR},{2011}\) .</p></div><p>[31] M. Rohrbach, M. Stark, G. Szarvas, I. Gurevych, and B. Schiele. What helps here - and why? Semantic relatedness for knowledge transfer. In CVPR, 2010.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[31] M. Rohrbach, M. Stark, G. Szarvas, I. Gurevych, 和 B. Schiele. 这里有什么帮助 - 为什么？知识转移的语义相关性. 在 CVPR, 2010.</p></div><p>[32] B. Romera-Paredes and P. H. Torr. An embarrassingly simple approach to zero-shot learning. ICML, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[32] B. Romera-Paredes 和 P. H. Torr. 一种令人尴尬的简单零样本学习方法. ICML, 2015.</p></div><p>[33] W. J. Scheirer, A. Rocha, A. Sapkota, and T. E. Boult. Towards open set recognition. TPAMI, 36, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[33] W. J. Scheirer, A. Rocha, A. Sapkota, 和 T. E. Boult. 朝着开放集识别的方向. TPAMI, 36, 2013.</p></div><p>[34] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot learning through cross-modal transfer. In NIPS. 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[34] R. Socher, M. Ganjoo, C. D. Manning, 和 A. Ng. 通过跨模态转移进行零样本学习. 在 NIPS. 2013.</p></div><p>[35] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[35] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, 和 A. Rabinovich. 通过卷积深入研究. 在 CVPR, 2015.</p></div><p>[36] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. JMLR, 2005.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[36] I. Tsochantaridis, T. Joachims, T. Hofmann, 和 Y. Altun. 结构化和相互依赖输出变量的大边距方法. JMLR, 2005.</p></div><p>[37] N. Usunier, D. Buffoni, and P. Gallinari. Ranking with ordered weighted pairwise classification. In ICML, 2009.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[37] N. Usunier, D. Buffoni, 和 P. Gallinari. 使用有序加权成对分类进行排名. 在 ICML, 2009.</p></div><p>[38] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-longie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, Caltech, 2010.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[38] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Be-longie, 和 P. Perona. 加州理工学院-加州大学圣地亚哥分校鸟类 200. 技术报告 CNS-TR-2010-001, 加州理工学院, 2010.</p></div><p>[39] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and B. Schiele. Latent embeddings for zero-shot classification. In \({CVPR},{2016}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[39] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, 和 B. Schiele. 零样本分类的潜在嵌入. 在 \({CVPR},{2016}\) .</p></div><p>[40] X. Yu and Y. Aloimonos. Attribute-based transfer learning for object categorization with zero or one training example. In \({ECCV},{2010}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[40] X. Yu 和 Y. Aloimonos. 基于属性的迁移学习用于对象分类，只有零个或一个训练示例. 在 \({ECCV},{2010}\) .</p></div><p>[41] H. Zhang, X. Shang, W. Yang, H. Xu, H. Luan, and T.-S. Chua. Online collaborative learning for open-vocabulary visual classifiers. In \({CVPR},{2016}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[41] H. Zhang, X. Shang, W. Yang, H. Xu, H. Luan, 和 T.-S. Chua. 在线协作学习用于开放词汇视觉分类器. 在 \({CVPR},{2016}\) .</p></div><p>[42] Z. Zhang and V. Saligrama. Zero-shot learning via semantic similarity embedding. In \({ICCV},{2015}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[42] Z. Zhang 和 V. Saligrama. 通过语义相似性嵌入进行零样本学习。在 \({ICCV},{2015}\)。</p></div><p>[43] Z. Zhang and V. Saligrama. Zero-shot learning via joint semantic similarity embedding. In CVPR, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[43] Z. Zhang 和 V. Saligrama. 通过联合语义相似性嵌入进行零样本学习。在 CVPR, 2016。</p></div><p>[44] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning deep features for scene recognition using places database. In NIPS, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[44] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba 和 A. Oliva. 使用场所数据库学习场景识别的深度特征。在 NIPS, 2014。</p></div>
      </body>
    </html>
  