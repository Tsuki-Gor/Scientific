
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <h1>Gaussian Mixture Variational Autoencoder with Contrastive Learning for Multi-Label Classification</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>基于对比学习的高斯混合变分自编码器用于多标签分类</h1></div><p>Junwen Bai \({}^{1}\) Shufeng Kong \({}^{1}\) Carla Gomes \({}^{1}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Junwen Bai \({}^{1}\) Shufeng Kong \({}^{1}\) Carla Gomes \({}^{1}\)</p></div><h2>Abstract</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>摘要</h2></div><p>Multi-label classification (MLC) is a prediction task where each sample can have more than one label. We propose a novel contrastive learning boosted multi-label prediction model based on a Gaussian mixture variational autoencoder (C-GMVAE), which learns a multimodal prior space and employs a contrastive loss. Many existing methods introduce extra complex neural modules like graph neural networks to capture the label correlations, in addition to the prediction modules. We find that by using contrastive learning in the supervised setting, we can exploit label information effectively in a data-driven manner, and learn meaningful feature and label embeddings which capture the label correlations and enhance the predictive power. Our method also adopts the idea of learning and aligning latent spaces for both features and labels. In contrast to previous works based on a unimodal prior, C-GMVAE imposes a Gaussian mixture structure on the latent space, to alleviate the posterior collapse and over-regularization issues. C-GMVAE outperforms existing methods on multiple public datasets and can often match other models' full performance with only \({50}\%\) of the training data. Furthermore,we show that the learnt embeddings provide insights into the interpretation of label-label interactions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>多标签分类（MLC）是一种预测任务，每个样本可以有多个标签。我们提出了一种基于高斯混合变分自编码器的新型对比学习增强的多标签预测模型（C-GMVAE），该模型学习多模态先验空间并采用对比损失。许多现有方法在预测模块之外引入额外复杂的神经模块，如图神经网络，以捕捉标签之间的相关性。我们发现，通过在监督设置中使用对比学习，我们可以以数据驱动的方式有效利用标签信息，并学习有意义的特征和标签嵌入，这些嵌入捕捉标签相关性并增强预测能力。我们的方法还采用了学习和对齐特征和标签的潜在空间的思想。与基于单模态先验的先前工作相比，C-GMVAE在潜在空间上施加了高斯混合结构，以缓解后验崩溃和过度正则化问题。C-GMVAE在多个公共数据集上优于现有方法，并且通常只需\({50}\%\)的训练数据就能匹配其他模型的完整性能。此外，我们还展示了学习到的嵌入为标签间交互的解释提供了见解。</p></div><h2>1. Introduction</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>1. 引言</h2></div><p>In many machine learning tasks, an instance can have several labels. The task of predicting multiple labels is known as multi-label classification (MLC). MLC is common in domains like computer vision (Wang et al., 2016), natural language processing (Chang et al., 2019) and biology (Yu et al., 2013). Unlike the single-label scenario, label correlations are more important in MLC. Early works capture the correlations through classifier chains (Read et al., 2009), Bayesian inference (Zhang &#x26; Zhou, 2007), and dimensionality reduction (Bhatia et al., 2015).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在许多机器学习任务中，一个实例可以有多个标签。预测多个标签的任务被称为多标签分类（MLC）。MLC在计算机视觉（Wang et al., 2016）、自然语言处理（Chang et al., 2019）和生物学（Yu et al., 2013）等领域很常见。与单标签场景不同，标签相关性在MLC中更为重要。早期的工作通过分类器链（Read et al., 2009）、贝叶斯推断（Zhang &#x26; Zhou, 2007）和降维（Bhatia et al., 2015）来捕捉相关性。</p></div><p>Thanks to the huge capacity of neural networks (NN), many previous methods can be improved by their neural extensions. For example, classifier chains can be naturally enhanced by recurrent neural networks (RNN) (Wang et al., 2016). The non-linearity of NN alleviates the complex design of feature mapping and many deep models can therefore focus on the loss function, feature-label and label-label correlation modeling.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>得益于神经网络（NN）的巨大能力，许多先前的方法可以通过其神经扩展得到改进。例如，分类器链可以通过递归神经网络（RNN）（Wang et al., 2016）自然增强。NN的非线性减轻了特征映射的复杂设计，因此许多深度模型可以专注于损失函数、特征-标签和标签-标签相关性建模。</p></div><p>One trending direction is to learn a deep latent space shared by features and labels. The encoded samples from the latent space are then decoded to targets. One typical example is C2AE (Yeh et al., 2017), which learns latent codes for both features and labels. The latent codes are passed to a decoder to derive the target labels. C2AE minimizes an \({\ell }_{2}\) distance between the feature and label codes, together with a relaxed orthogonality regularization. However, the learnt deterministic latent space lacks smoothness and structures. Small perturbations in this latent space can lead to totally different decoding results. Even if the corresponding feature and label codes are close, we cannot guarantee the decoded targets are similar. To address this concern, MPVAE (Bai et al., 2020) proposes to replace the deterministic latent space with a probabilistic space under a variational autoencoder (VAE) framework. The Gaussian latent spaces are aligned with KL-divergence, and the sampling process enforces smoothness. Similar ideas can be found in (Sundar et al., 2020). However, these methods assume a unimodal Gaussian latent space, which is known to cause over-regularization and posterior collapse (Dilokthanakul et al., 2016; Wu &#x26; Goodman, 2018). A better strategy would be to learn a multimodal latent space. It is more reasonable to assume the observed data are generated from a multimodal subspace rather than a unimodal one.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>一个流行的方向是学习一个特征和标签共享的深层潜在空间。然后，从潜在空间编码的样本被解码为目标。一个典型的例子是C2AE（Yeh et al., 2017），它为特征和标签学习潜在编码。潜在编码被传递给解码器以推导目标标签。C2AE最小化特征和标签编码之间的\({\ell }_{2}\)距离，并结合放松的正交正则化。然而，学习到的确定性潜在空间缺乏平滑性和结构。在这个潜在空间中的小扰动可能导致完全不同的解码结果。即使相应的特征和标签编码接近，我们也不能保证解码的目标是相似的。为了解决这个问题，MPVAE（Bai et al., 2020）提出在变分自编码器（VAE）框架下用概率空间替代确定性潜在空间。高斯潜在空间通过KL散度对齐，采样过程强制平滑。类似的思想可以在（Sundar et al., 2020）中找到。然而，这些方法假设单模态高斯潜在空间，这已知会导致过度正则化和后验崩溃（Dilokthanakul et al., 2016；Wu &#x26; Goodman, 2018）。更好的策略是学习多模态潜在空间。假设观察到的数据是从多模态子空间生成的，而不是单模态的，更为合理。</p></div><p>Another popular group of methods focuses on better label correlation modeling. Their idea is straightforward: some labels should be more correlated if they co-appear often while others should be less relevant. Existing methods adopt pairwise ranking loss, covariance matrices, conditional random fields (CRF) or graph neural nets (GNN) to this end (Zhang &#x26; Zhou, 2013; Bi &#x26; Kwok, 2014; Belanger &#x26; McCallum, 2016; Lanchantin et al., 2019; Chen et al., 2019b). These methods often either constrain the learning through a predefined structure (which requires a larger model size), or aren't powerful enough to capture the correlations (such as pairwise ranking loss).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>另一组流行的方法专注于更好的标签相关性建模。它们的思想很简单：如果某些标签经常共同出现，则它们之间的相关性应该更强，而其他标签则应该相关性较低。现有方法采用成对排名损失、协方差矩阵、条件随机场（CRF）或图神经网络（GNN）来实现这一目标（Zhang &#x26; Zhou, 2013；Bi &#x26; Kwok, 2014；Belanger &#x26; McCallum, 2016；Lanchantin et al., 2019；Chen et al., 2019b）。这些方法通常要么通过预定义结构约束学习（这需要更大的模型规模），要么没有足够的能力捕捉相关性（如成对排名损失）。</p></div><hr>
<!-- Footnote --><p>\({}^{1}\) Department of Computer Science,Cornell University,Ithaca, USA. Correspondence to: Shufeng Kong \(&#x3C;\) <a href="mailto:sk2299@cornell.edu">sk2299@cornell.edu</a>>.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{1}\) 计算机科学系，康奈尔大学，美国伊萨卡。通讯联系：Shufeng Kong \(&#x3C;\) <a href="mailto:sk2299@cornell.edu">sk2299@cornell.edu</a>。</p></div><p>Proceedings of the \({39}^{\text{th }}\) International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>国际机器学习会议论文集，马里兰州巴尔的摩，美国，PMLR 162，2022。版权归作者（们）所有，2022。</p></div><!-- Footnote -->
<hr><p>Our idea is simple: we learn embeddings for each label class and the inner products between embeddings should reflect the similarity. We further learn feature embeddings whose inner products with label embeddings correspond to feature-label similarity and can be used for prediction. We assume these embeddings are generated from a probabilistic multimodal latent space shared by features and labels, where we use KL-divergence to align the feature and label latent distributions. On the other hand, one might be concerned that embeddings alone won't capture both label-label and label-feature correlations, which were usually modeled by extra GNN and covariance matrices in prior works (Lan-chantin et al., 2019; Bai et al., 2020). To this end, we stress on the loss function terms rather than extra structure to capture these correlations. Intuitively, if two labels co-appear often, their embeddings should be close. Otherwise, if two labels seldom co-appear, their embeddings should be distant. A triplet-like loss could be naturally applied in this scenario. Nonetheless, its extension, contrastive loss, has shown to be even more effective than the triplet loss by introducing more samples rather than just one triplet. We show that contrastive loss can pull together correlated label embeddings, push away unrelated label embeddings (see Fig. 3), and even perform better than GNN-based or covariance-based methods.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的想法很简单：我们为每个标签类别学习嵌入，嵌入之间的内积应反映相似性。我们进一步学习特征嵌入，其与标签嵌入的内积对应于特征-标签相似性，并可用于预测。我们假设这些嵌入是从特征和标签共享的概率多模态潜在空间生成的，我们使用KL散度来对齐特征和标签的潜在分布。另一方面，人们可能会担心仅靠嵌入无法捕捉标签-标签和标签-特征的相关性，这通常在之前的工作中通过额外的GNN和协方差矩阵建模（Lan-chantin等，2019；Bai等，2020）。为此，我们强调损失函数项，而不是额外的结构来捕捉这些相关性。直观地说，如果两个标签经常共同出现，它们的嵌入应该接近。否则，如果两个标签很少共同出现，它们的嵌入应该远离。在这种情况下，可以自然地应用三元组损失。然而，它的扩展，对比损失，已被证明通过引入更多样本而不仅仅是一个三元组，比三元组损失更有效。我们展示了对比损失可以将相关的标签嵌入拉在一起，推开无关的标签嵌入（见图3），甚至表现得比基于GNN或协方差的方法更好。</p></div><p>Our new model for MLC, contrastive learning boosted Gaussian mixture variational autoencoder (C-GMVAE), alleviates the over-regularization and posterior collapse concerns, and also learns useful feature and label embeddings. C-GMVAE is applied to nine datasets and outperforms the existing methods on five metrics. Moreover, we show that using only \({50}\%\) of the data,our results can match the full performance of other state-of-the-art methods. Ablation studies and interpretability of learnt embeddings will also be illustrated in the experiments. Our contributions can be summarized in three aspects: (i) We propose to use contrastive loss instead of triplet or ranking loss to strengthen the label embedding learning. We empirically show that by using a contrastive loss, one can get rid of heavy-duty label correlation modules (e.g., covariance matrices, GNNs) while achieving even better performances. (ii) Though contrastive learning is commonly applied in self-supervised learning, our work shows that by properly defining anchor, positive and negative samples, contrastive loss can leverage label information very effectively in the supervised MLC scenario as well. (iii) Unlike prior probabilistic models, C-GMVAE learns a multimodal latent space and integrates the probabilistic modeling (VAE module) with embedding learning (contrastive module) synergistically.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们针对多标签分类（MLC）的新模型，对比学习增强的高斯混合变分自编码器（C-GMVAE），缓解了过度正则化和后验崩溃的问题，同时学习有用的特征和标签嵌入。C-GMVAE应用于九个数据集，并在五个指标上超越现有方法。此外，我们展示了仅使用\({50}\%\)的数据，我们的结果可以匹配其他最先进方法的完整性能。消融研究和学习嵌入的可解释性将在实验中说明。我们的贡献可以总结为三个方面：（i）我们建议使用对比损失而不是三元组或排名损失来增强标签嵌入学习。我们通过实验证明，使用对比损失可以摆脱繁重的标签相关模块（例如，协方差矩阵，GNN），同时实现更好的性能。（ii）尽管对比学习通常应用于自监督学习，但我们的工作表明，通过适当地定义锚点、正样本和负样本，对比损失也可以在监督的多标签分类场景中非常有效地利用标签信息。（iii）与之前的概率模型不同，C-GMVAE学习一个多模态潜在空间，并协同整合概率建模（VAE模块）与嵌入学习（对比模块）。</p></div><h2>2. Methods</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>2. 方法</h2></div><p>In MLC,given a dataset containing \(N\) labeled samples (x,y),where \(x \in  {\mathbb{R}}^{D}\) and \(y \in  \{ 0,1{\} }^{L}\) ,our goal is to find a mapping from \(x\) to \(y.N,D,L\) are the number of samples, feature length and label set size respectively. The binary coding indicates the labels associated with the sample \(x\) . Labels are correlated with each other.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在多标签分类（MLC）中，给定一个包含\(N\)个标记样本（x,y）的数据集，其中\(x \in  {\mathbb{R}}^{D}\)和\(y \in  \{ 0,1{\} }^{L}\)，我们的目标是找到从\(x\)到\(y.N,D,L\)的映射，分别是样本数量、特征长度和标签集大小。二进制编码表示与样本\(x\)相关的标签。标签之间是相关的。</p></div><h3>2.1. Preliminaries</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>2.1. 基础知识</h3></div><h4>2.1.1. Gaussian Mixture VAE</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>2.1.1. 高斯混合变分自编码器</h4></div><p>A standard VAE (Kingma &#x26; Welling, 2013) pulls together the posterior distribution and a parameter-free isotropic Gaussian prior. Two losses are optimized together in training: KL-divergence from the prior to the posterior, and the distance between the reconstructed targets and the real targets. One weakness of this formulation is the unimodality of its latent space, inhibiting the learning of more complex representations. Another concern is over-regularization: if the posterior is exactly the same as the prior, the learnt representations would be uninformative of the inputs. Numerous works extend the prior to be more complex (Chung et al., 2015; Eslami et al., 2016; Dilokthanakul et al., 2016). In our work, we adopt the Gausian mixture prior. The probability density can be depicted as \(p\left( z\right)  = \frac{1}{k}\mathop{\sum }\limits_{{i = 1}}^{k}\mathcal{N}\left( {z \mid  {\mu }_{i},{\sigma }_{i}^{2}}\right)\) where \(i\) is the cluster index of \(k\) Gaussian clusters with mean \({\mu }_{i}\) and covariance \({\sigma }_{i}^{2}\) (Shu,2016; Shi et al.,2019). Our intuition is that each label embedding could correlate to a Gaussian subspace. Given a label set, the mixture of the positive Gaussian subspaces forms a unique multimodal prior distribution. The label embeddings also receive the gradients from the contrastive loss and thus the contrastive learning is combined with latent space construction. Our formulation is also related to MVAE (Wu &#x26; Goodman, 2018; Shi et al., 2020) which adopts the idea of product-of-experts.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>标准变分自编码器（VAE）（Kingma &#x26; Welling, 2013）将后验分布与无参数的各向同性高斯先验结合在一起。在训练中优化两个损失：从先验到后验的KL散度，以及重建目标与真实目标之间的距离。这种公式的一个弱点是其潜在空间的单峰性，抑制了更复杂表示的学习。另一个问题是过度正则化：如果后验与先验完全相同，学习到的表示将对输入没有信息。许多研究扩展了先验，使其更复杂（Chung et al., 2015; Eslami et al., 2016; Dilokthanakul et al., 2016）。在我们的工作中，我们采用高斯混合先验。概率密度可以表示为\(p\left( z\right)  = \frac{1}{k}\mathop{\sum }\limits_{{i = 1}}^{k}\mathcal{N}\left( {z \mid  {\mu }_{i},{\sigma }_{i}^{2}}\right)\)，其中\(i\)是\(k\)个高斯簇的聚类索引，均值为\({\mu }_{i}\)，协方差为\({\sigma }_{i}^{2}\)（Shu,2016; Shi et al.,2019）。我们的直觉是每个标签嵌入可以与一个高斯子空间相关联。给定一个标签集，正高斯子空间的混合形成一个独特的多模态先验分布。标签嵌入还接收来自对比损失的梯度，因此对比学习与潜在空间构建相结合。我们的公式也与MVAE（Wu &#x26; Goodman, 2018; Shi et al., 2020）相关，后者采用了专家乘积的思想。</p></div><h4>2.1.2. CONTRASTIVE LEARNING</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>2.1.2. 对比学习</h4></div><p>We propose to use contrastive learning to capture the correlations (i.e., feature-label and label-label correlations). Contrastive learning (Oord et al., 2018; Chen et al., 2020; Khosla et al., 2021) is a novel learning style. The core idea is simple: given an anchor sample, it should be close to similar samples (positive) and far from dissimilar samples (negative) in some learnt embedding space. It differs from triplet loss in the number of negative samples and the loss estimation method. Contrastive loss is largely motivated by noise contrastive estimation (NCE) (Gutmann &#x26; Hyvärinen, 2010) and its form is generalizable. The raw contrastive loss formulation only considers the instance-level invariance (multiple views of one instance), but with label information, we can learn category-level invariance (multiple instances per class/category) (Wang et al., 2021).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们提议使用对比学习来捕捉相关性（即特征-标签和标签-标签相关性）。对比学习（Oord et al., 2018; Chen et al., 2020; Khosla et al., 2021）是一种新颖的学习方式。核心思想很简单：给定一个锚样本，它应该在某个学习到的嵌入空间中与相似样本（正样本）接近，与不相似样本（负样本）远离。它与三元组损失在负样本数量和损失估计方法上有所不同。对比损失主要受到噪声对比估计（NCE）（Gutmann &#x26; Hyvärinen, 2010）的启发，其形式是可推广的。原始对比损失公式仅考虑实例级不变性（一个实例的多个视图），但通过标签信息，我们可以学习类别级不变性（每个类别/类的多个实例）（Wang et al., 2021）。</p></div><!-- Media --><!-- figureText: y Contrastive Loss positive sea embedding bird embedding feature embedding car embedding align negative embedding \( \widehat{x} \) Latent Space Decoder feature 0 car car embedding \( \left( {w}_{1}^{l}\right) \) sea sea embedding \( \left( {w}_{2}^{l}\right) \) Label Encoder 1 bird bird embedding \( \left( {w}_{L}^{l}\right) \) 1 \( x \) --><img src="https://cdn.noedgeai.com/bo_d15g03ref24c73d1f6k0_2.jpg?x=165&#x26;y=194&#x26;w=1427&#x26;h=478&#x26;r=0"><p>Figure 1. The full pipeline of C-GMVAE. Every label category is mapped to a learnable embedding first. The label encoder transforms each embedding \({w}_{i}^{l}\) to a multivariate Gaussian latent space. The sample’s associated label set selects the positive latent spaces and forms a Gaussian mixture prior. Each feature is also mapped to a latent space through a feature encoder. The posterior is aligned with the prior via KL-divergence. The decoder takes in a sample from the latent space and produces a feature embedding \({w}_{x}^{f}\) . A contrastive loss is designed to pull together the feature embedding and the positive label embeddings, while separating the feature embedding from the negative label embeddings. Prediction \(\widehat{y}\) is generated by passing the inner products between the feature embedding \({w}_{x}^{f}\) and the label embeddings \({w}_{i}^{l}\) to the sigmoid functions. A sample with label set \(\{\) sea,bird \(\}\) is shown here.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图1. C-GMVAE的完整流程。每个标签类别首先映射到一个可学习的嵌入。标签编码器将每个嵌入\({w}_{i}^{l}\)转换为多元高斯潜在空间。样本的相关标签集选择正潜在空间并形成高斯混合先验。每个特征也通过特征编码器映射到潜在空间。后验通过KL散度与先验对齐。解码器从潜在空间中获取样本并生成特征嵌入\({w}_{x}^{f}\)。设计了一个对比损失，以将特征嵌入与正标签嵌入拉近，同时将特征嵌入与负标签嵌入分开。通过将特征嵌入\({w}_{x}^{f}\)与标签嵌入\({w}_{i}^{l}\)之间的内积传递给sigmoid函数生成预测\(\widehat{y}\)。这里展示了一个标签集\(\{\)海洋鸟\(\}\)的样本。</p></div><!-- Media --><p>In the multi-label scenario, one can regard the feature embedding as the anchor sample, positive label embeddings as the positive samples and negative label embeddings as the negative samples. The formulation can fit the contrastive learning framework naturally and is one of our major contributions. Compared to the pairwise ranking loss which focuses on the final logits, contrastive loss is defined on the learnt embeddings and thus becomes more expressive. Contrastive loss also includes more samples in estimating the NCE and therefore outperforms the triplet loss. In the appendix, we show triplet loss is actually a special case of our contrastive loss.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在多标签场景中，可以将特征嵌入视为锚样本，将正标签嵌入视为正样本，将负标签嵌入视为负样本。该公式可以自然地适应对比学习框架，是我们主要的贡献之一。与关注最终logits的成对排名损失相比，对比损失是在学习到的嵌入上定义的，因此变得更具表现力。对比损失还在估计NCE时包含更多样本，因此优于三元组损失。在附录中，我们展示了三元组损失实际上是我们对比损失的一个特例。</p></div><h3>2.2. C-GMVAE</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>2.2. C-GMVAE</h3></div><p>C-GMVAE inherits the general VAE framework, but with a learnable Gaussian mixture (GM) prior. During training, each sample's label set activates and mixes the positive Gaussian subspaces to derive the prior. Contrastive learning is applied to boost the embedding learning, using a contrastive loss between the feature and label embeddings. Fig. 1 provides a full illustration, and the following subsections will elaborate on the details.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>C-GMVAE 继承了通用的 VAE 框架，但具有可学习的高斯混合 (GM) 先验。在训练过程中，每个样本的标签集激活并混合正高斯子空间以推导先验。对比学习被应用于增强嵌入学习，使用特征和标签嵌入之间的对比损失。图 1 提供了完整的说明，以下小节将详细阐述细节。</p></div><h4>2.2.1. Gaussian Mixture Latent Space</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>2.2.1. 高斯混合潜在空间</h4></div><p>Given a sample(x,y)where feature \(x \in  {\mathbb{R}}^{D}\) and label \(y \in  \{ 0,1{\} }^{L}\) ,many previous works take \(y\) as the input and transform it to a dense representation through a fully-connected layer (Yeh et al., 2017; Bai et al., 2020). This layer essentially maps each label category to an embedding and sums up all the embeddings using label \(y\) as weights ( 0 or 1). The summed embedding is fed into the label encoder to produce a probabilistic latent space.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>给定一个样本 (x,y)，其中特征 \(x \in  {\mathbb{R}}^{D}\) 和标签 \(y \in  \{ 0,1{\} }^{L}\)，许多先前的工作将 \(y\) 作为输入，并通过全连接层转换为稠密表示 (Yeh et al., 2017; Bai et al., 2020)。该层本质上将每个标签类别映射到一个嵌入，并使用标签 \(y\) 作为权重 (0 或 1) 对所有嵌入进行求和。求和后的嵌入被输入到标签编码器中以生成概率潜在空间。</p></div><p>In C-GMVAE, however, we directly map each label embedding \({w}_{i}^{l} \in  {\mathbb{R}}^{E}\) of label class \(i\) to an individual latent Gaussian distribution \(\mathcal{N}\left( {{\mu }_{i},\operatorname{diag}\left( {\sigma }_{i}^{2}\right) }\right)\) ,where \({\mu }_{i} \in  {\mathbb{R}}^{d},{\sigma }_{i}^{2} \in\) \({\mathbb{R}}^{d}\) ,and \({\mu }_{i},{\sigma }_{i}^{2}\) are derived from \({w}_{i}^{l}\) through the NN-based label encoder. The randomly initialized embeddings \({w}_{i}^{l}\) are learnable during the training process, similar to (Mikolov et al., 2013), and they share the same label encoder. In Fig. 1, the label categories car, sea,..., bird are transformed to embeddings first. Embeddings are then passed directly to label encoder rather than summed up. Each label category (e.g., car) corresponds to a unimodal Gaussian in the latent space. \(y\) activates "positive" Gaussians \(\left( {{y}_{i} = 1}\right)\) and forms a Gaussian mixture subspace. Given a random variable \(z \in  {\mathbb{R}}^{d}\) ,the probability density function (PDF) in the subspace is defined as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>然而，在 C-GMVAE 中，我们直接将标签类别 \(i\) 的每个标签嵌入 \({w}_{i}^{l} \in  {\mathbb{R}}^{E}\) 映射到一个单独的潜在高斯分布 \(\mathcal{N}\left( {{\mu }_{i},\operatorname{diag}\left( {\sigma }_{i}^{2}\right) }\right)\)，其中 \({\mu }_{i} \in  {\mathbb{R}}^{d},{\sigma }_{i}^{2} \in\) \({\mathbb{R}}^{d}\) 和 \({\mu }_{i},{\sigma }_{i}^{2}\) 是通过基于神经网络的标签编码器从 \({w}_{i}^{l}\) 推导而来的。随机初始化的嵌入 \({w}_{i}^{l}\) 在训练过程中是可学习的，类似于 (Mikolov et al., 2013)，并且它们共享相同的标签编码器。在图 1 中，标签类别汽车、海洋、……、鸟类首先被转换为嵌入。然后，嵌入直接传递给标签编码器，而不是求和。每个标签类别 (例如，汽车) 对应于潜在空间中的单峰高斯。\(y\) 激活“正”高斯 \(\left( {{y}_{i} = 1}\right)\) 并形成高斯混合子空间。给定一个随机变量 \(z \in  {\mathbb{R}}^{d}\)，子空间中的概率密度函数 (PDF) 定义为</p></div><p></p>\[{p}_{\psi }\left( {z \mid  y}\right)  = \frac{1}{\mathop{\sum }\limits_{i}{y}_{i}}\mathop{\sum }\limits_{{i = 1}}^{L}\mathbb{1}\left\{  {{y}_{i} = 1}\right\}  \mathcal{N}\left( {z \mid  {\mu }_{i},\operatorname{diag}\left( {\sigma }_{i}^{2}\right) }\right)  \tag{1}\]<p></p><p>where \(\mathbb{1}\left( \cdot \right)\) is the indicator function and the label encoder is parameterized by \(\psi \left( \mathrm{{NN}}\right)\) . In Fig. \(1,y\) activates sea and bird, then we have</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(\mathbb{1}\left( \cdot \right)\) 是指示函数，标签编码器由 \(\psi \left( \mathrm{{NN}}\right)\) 参数化。在图 \(1,y\) 中激活海洋和鸟类，然后我们有</p></div><p></p>\[{p}_{\psi }\left( {z \mid  y}\right)  = \frac{1}{2}\left( {\mathcal{N}\left( {z \mid  {\mu }_{\text{sea }},\operatorname{diag}\left( {\sigma }_{\text{sea }}^{2}\right) }\right)  + }\right.  \tag{2}\]<p></p><p></p>\[\mathcal{N}\left( {z \mid  {\mu }_{\text{bird }},\operatorname{diag}\left( {\sigma }_{\text{bird }}^{2}\right) }\right) )\]<p></p><p>Most VAE-based frameworks optimize over an evidence lower bound (ELBO) (Doersch, 2016):</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>大多数基于 VAE 的框架在证据下界 (ELBO) 上进行优化 (Doersch, 2016)：</p></div><p></p>\[\mathrm{{ELBO}} = {\mathbb{E}}_{{q}_{\phi }\left( {z \mid  x}\right) }\left\lbrack  {\log {p}_{\theta }\left( {x \mid  z}\right) }\right\rbrack   -  \tag{3}\]<p></p>
<p></p>\[{D}_{KL}\left\lbrack  {{q}_{\phi }\left( {z \mid  x}\right) \parallel p\left( z\right) }\right\rbrack\]<p></p><p>The feature encoder is parameterized by \(\phi \left( \mathrm{{NN}}\right)\) . One pitfall of this objective is owing to the minimization of the KL-divergence. If the divergence between the posterior \({q}_{\phi }\left( {z \mid  x}\right)\) and the prior \({p}_{\psi }\left( z\right)\) vanishes,the learnt latent codes would become non-informative. This is called posterior collapse. Many recent works suggest learnable priors (Tomczak &#x26; Welling, 2018) or more sophisticated priors (Wang &#x26; Wang, 2019) to avoid this issue, and we adopt these ideas in our design of the prior. Compared to a standard VAE, our prior is informative, learnable and multimodal.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>特征编码器由 \(\phi \left( \mathrm{{NN}}\right)\) 参数化。这个目标的一个陷阱是由于 KL 散度的最小化。如果后验 \({q}_{\phi }\left( {z \mid  x}\right)\) 和先验 \({p}_{\psi }\left( z\right)\) 之间的散度消失，学习到的潜在编码将变得无信息。这被称为后验崩溃。许多最近的工作建议使用可学习的先验 (Tomczak &#x26; Welling, 2018) 或更复杂的先验 (Wang &#x26; Wang, 2019) 来避免这个问题，我们在先验的设计中采用了这些想法。与标准 VAE 相比，我们的先验是信息丰富的、可学习的和多模态的。</p></div><p>We form a standard posterior in our model and match it with the prior. However, unlike vanilla VAE, we cannot analytically compute the KL term. Instead, we use the following estimation:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们在模型中形成一个标准后验，并将其与先验匹配。然而，与普通 VAE 不同，我们无法解析计算 KL 项。相反，我们使用以下估计：</p></div><p></p>\[{\mathcal{L}}_{KL} \approx  \log {q}_{\phi }\left( {{z}_{0} \mid  x}\right)  - \log {p}_{\psi }\left( {{z}_{0} \mid  y}\right)\]<p></p><p></p>\[= \log \mathcal{N}\left( {{z}_{0} \mid  {\mu }_{\phi }\left( x\right) ,\operatorname{diag}\left( {{\sigma }_{\phi }^{2}\left( x\right) }\right) }\right)  -\]<p></p><p></p>\[\log \frac{1}{\mathop{\sum }\limits_{i}{y}_{i}}\mathop{\sum }\limits_{{i = 1}}^{L}\mathbb{1}\left\{  {{y}_{i} = 1}\right\}  \mathcal{N}\left( {{z}_{0} \mid  {\mu }_{i},\operatorname{diag}\left( {\sigma }_{i}^{2}\right) }\right)\]<p></p><p>(4)</p><p>where \({z}_{0} \sim  {q}_{\phi }\left( {z \mid  x}\right)\) denotes a single latent sample. Our formulation follows the design of (Shu, 2016), which has been shown to outperform the formulation in (Dilokthanakul et al., 2016).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \({z}_{0} \sim  {q}_{\phi }\left( {z \mid  x}\right)\) 表示单个潜在样本。我们的公式遵循 (Shu, 2016) 的设计，已被证明优于 (Dilokthanakul et al., 2016) 中的公式。</p></div><p>The reconstruction loss is a standard negative log-likelihood with decoder parameters \(\theta\) ,</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>重构损失是一个标准的负对数似然，解码器参数为 \(\theta\)，</p></div><p></p>\[{\mathcal{L}}_{\text{recon }} =  - {E}_{{q}_{\phi }\left( {z \mid  x}\right) }\left\lbrack  {\log {p}_{\theta }\left( {x \mid  z}\right) }\right\rbrack   \tag{5}\]<p></p><h4>2.2.2. CONTRASTIVE LEARNING MODULE</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>2.2.2. 对比学习模块</h4></div><p>The decoder function \({f}_{\theta }^{d}\left( \cdot \right)\) decodes the sample from the latent space to a feature embedding \({w}_{x}^{f} \in  {\mathbb{R}}^{E}\) . We learn \({w}_{x}^{f}\) together with label embeddings \({\left\{  {w}_{i}^{l}\right\}  }_{i = 1}^{L}\) . The objective function includes both contrastive loss and cross-entropy loss terms.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>解码器函数\({f}_{\theta }^{d}\left( \cdot \right)\)将样本从潜在空间解码为特征嵌入\({w}_{x}^{f} \in  {\mathbb{R}}^{E}\)。我们与标签嵌入\({\left\{  {w}_{i}^{l}\right\}  }_{i = 1}^{L}\)一起学习\({w}_{x}^{f}\)。目标函数包括对比损失和交叉熵损失项。</p></div><p>Prior works explicitly capture the label-label interactions with GNNs or covariance matrices, which impose the structure a priori and might not be the best modeling approach. Our contrastive module instead captures the correlations in a data-driven manner. For example, if in most of the samples, "beach" and "sunshine" appear together, the contrastive learning will implicitly pull their embeddings together (see the derivation in the appendix). In other words, if two labels do co-appear often, their label embeddings would become similar (Fig. 3). On the other hand, if they never co-occur or only co-appear occasionally, their connections are not significant and our model will not optimize for their similarity.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>先前的工作通过GNN或协方差矩阵显式捕捉标签之间的交互，这种方法事先施加了结构，可能不是最佳建模方法。我们的对比模块则以数据驱动的方式捕捉相关性。例如，如果在大多数样本中，“海滩”和“阳光”经常一起出现，对比学习将隐式地将它们的嵌入拉近（见附录中的推导）。换句话说，如果两个标签确实经常共同出现，它们的标签嵌入将变得相似（图3）。另一方面，如果它们从不共同出现或仅偶尔共同出现，它们之间的联系就不显著，我们的模型将不会优化它们的相似性。</p></div><p>Original contrastive learning (Oord et al., 2018) augments inputs and learns the instance-level invariance, but it may not generalize to the category-level invariance. In the supervised setting, however, the learning can benefit from the labels and discover the category-level invariance (Khosla et al., 2021). Let \(A \equiv  \{ 1\ldots L\}\) . We define \(P\left( y\right)  \equiv  \left\{  {i \in  A : {y}_{i} = 1}\right\}\) for sample(x,y). Suppose we have a batch of samples, \(\mathcal{B}\) ,the contrastive loss can be written as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>原始对比学习（Oord等，2018）增强输入并学习实例级不变性，但它可能无法推广到类别级不变性。然而，在监督设置中，学习可以从标签中受益并发现类别级不变性（Khosla等，2021）。设\(A \equiv  \{ 1\ldots L\}\)。我们为样本(x,y)定义\(P\left( y\right)  \equiv  \left\{  {i \in  A : {y}_{i} = 1}\right\}\)。假设我们有一批样本\(\mathcal{B}\)，对比损失可以写为</p></div><p></p>\[{\mathcal{L}}_{CL} = \frac{1}{\left| \mathcal{B}\right| }\mathop{\sum }\limits_{{\left( {x,y}\right)  \in  \mathcal{B}}}\frac{1}{\left| P\left( y\right) \right| }\mathop{\sum }\limits_{{p \in  P\left( y\right) }} - \log \frac{\operatorname{sim}\left( {{w}_{x}^{f},{w}_{p}^{l}}\right) }{\mathop{\sum }\limits_{{t \in  A}}\operatorname{sim}\left( {{w}_{x}^{f},{w}_{t}^{l}}\right) }\]<p></p><p>(6)</p><p>Here, \(\operatorname{sim}\left( \cdot \right)\) is a function measuring the similarity between two embeddings,and \({w}_{x}^{f},{w}_{i}^{l}\) denote the feature and label embeddings respectively. Eq. 6 is built on top of NCE (Gut-mann &#x26; Hyvärinen, 2010), and the equation is equivalent to a categorical cross-entropy of correctly predicting positive labels. The choice of \(\operatorname{sim}\left( \cdot \right)\) can be a log-bilinear function (Oord et al., 2018), or a more complicated neural metric function (Chen et al., 2020). In our experiments, we find it is simple and effective to take \(\operatorname{sim}\left( {{w}_{1},{w}_{2}}\right)  = \exp \left( {{w}_{1} \cdot  {w}_{2}/\tau }\right)\) where - means inner product and \(\tau\) is a temperature parameter controlling the scale of the inner product.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这里，\(\operatorname{sim}\left( \cdot \right)\)是一个测量两个嵌入之间相似性的函数，\({w}_{x}^{f},{w}_{i}^{l}\)分别表示特征和标签嵌入。公式6建立在NCE（Gutmann &#x26; Hyvärinen，2010）之上，该方程等同于正确预测正标签的分类交叉熵。\(\operatorname{sim}\left( \cdot \right)\)的选择可以是对数双线性函数（Oord等，2018），或更复杂的神经度量函数（Chen等，2020）。在我们的实验中，我们发现采用\(\operatorname{sim}\left( {{w}_{1},{w}_{2}}\right)  = \exp \left( {{w}_{1} \cdot  {w}_{2}/\tau }\right)\)是简单有效的，其中-表示内积，\(\tau\)是控制内积规模的温度参数。</p></div><p>In the single-label scenarios like SupCon (Khosla et al., 2021), if one class is positive, all other classes are contrastive to it. However, in MLC, if "beach" is positive in the label set while "sea" is not for one particular sample, we cannot say these two classes are contrastive. Their correlations should be captured implicitly by all the samples. Therefore, we do not enforce contrastive relations between labels and thus preserve the label correlations. Instead, we choose the feature embedding to be the anchor and label embeddings to be the positive and negative samples. If two label embeddings co-appear often as positive samples, they would implicitly become similar (see Fig. 3). Eq. 6 saves the effort of manually configuring the positive and negative samples, and is totally data-driven. The number of positive or negative samples could be greater than one, depending on the label set. Though \(L\) limits the max samples we can have, this formulation has already used many more samples compared to triplet loss, and we will show in experiments that this formulation is very effective.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在像SupCon（Khosla等，2021）这样的单标签场景中，如果一个类别是正类，所有其他类别都是与之对比的。然而，在多标签分类（MLC）中，如果在标签集中“海滩”是正类而“海洋”对某个特定样本不是，我们不能说这两个类别是对比的。它们的相关性应该通过所有样本隐式捕捉。因此，我们不强制标签之间的对比关系，从而保留标签相关性。相反，我们选择特征嵌入作为锚点，标签嵌入作为正样本和负样本。如果两个标签嵌入经常作为正样本共同出现，它们将隐式变得相似（见图3）。公式6节省了手动配置正样本和负样本的工作，完全依赖数据驱动。正样本或负样本的数量可以大于一个，具体取决于标签集。尽管\(L\)限制了我们可以拥有的最大样本数，但这种公式已经使用了比三元组损失更多的样本，我们将在实验中证明这种公式非常有效。</p></div><p>The triplet loss often used in multi-label learning (Seymour &#x26; Zhang, 2018) can be seen as a special case of Eq. 6 with only one positive and one negative. We illustrate this connection in the appendix. Furthermore, one desired property of embedding learning is that when a good positive embedding is already close enough to our anchor embedding, it contributes less to the gradients, while poorly learnt embed-dings contribute more to improve the model performance. In the appendix, we also show that the contrastive loss can implicitly achieve this goal and a full derivation of the gradients is provided.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在多标签学习中常用的三元组损失（Seymour &#x26; Zhang，2018）可以看作是公式6的特例，仅有一个正样本和一个负样本。我们在附录中说明了这一联系。此外，嵌入学习的一个期望特性是，当一个好的正嵌入已经足够接近我们的锚点嵌入时，它对梯度的贡献较小，而学习不佳的嵌入则对提高模型性能的贡献较大。在附录中，我们还展示了对比损失可以隐式实现这一目标，并提供了梯度的完整推导。</p></div><p>Our objective function also includes a supervised cross-entropy loss term to further facilitate the training. With the label embeddings \({w}_{i}^{l}\) and the feature embedding \({w}_{x}^{f}\) ,the cross entropy loss for each(x,y)is given by</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的目标函数还包括一个监督交叉熵损失项，以进一步促进训练。通过标签嵌入 \({w}_{i}^{l}\) 和特征嵌入 \({w}_{x}^{f}\)，每个 (x,y) 的交叉熵损失为</p></div><p></p>\[{\mathcal{L}}_{CE} = \mathop{\sum }\limits_{{i = 1}}^{L}{y}_{i}\log s\left( {{w}_{x}^{f}{w}_{i}^{l}}\right)  + \left( {1 - {y}_{i}}\right) \log \left( {1 - s\left( {{w}_{x}^{f}{w}_{i}^{l}}\right) }\right)\]<p></p><p>(7)</p><p>where \(s\left( \cdot \right)\) is the sigmoid function. In self-supervised learning, the contrastive loss typically helps the pretraining stage and the learnt representations are applied to downstream tasks. In the supervised setting, though some models (Khosla et al., 2021) stick to the two-stage training process where the model is trained with contrastive loss in the first stage and with cross-entropy loss in the second stage, we did not observe its superiority over the one-stage scheme in our MLC scenario. This is partly because we also learn a latent space that is closely connected to label embeddings. We train the model with an objective function incorporating all the losses. A joint training strategy reconciles different modules. We show in the experiments that the learnt embed-dings are semantically meaningful and can reveal the label correlations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(s\left( \cdot \right)\) 是 sigmoid 函数。在自监督学习中，对比损失通常有助于预训练阶段，学习到的表示被应用于下游任务。在监督设置中，尽管一些模型（Khosla 等，2021）坚持采用两阶段训练过程，其中模型在第一阶段使用对比损失进行训练，在第二阶段使用交叉熵损失进行训练，但我们在 MLC 场景中并未观察到其优越性。这部分是因为我们还学习了一个与标签嵌入紧密相关的潜在空间。我们使用一个包含所有损失的目标函数来训练模型。联合训练策略协调不同模块。我们在实验中表明，学习到的嵌入在语义上是有意义的，并且可以揭示标签之间的相关性。</p></div><h4>2.2.3. Objective Function</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>2.2.3. 目标函数</h4></div><p>The final objective function to minimize is simply the summation of different losses,</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最终要最小化的目标函数仅仅是不同损失的总和，</p></div><p></p>\[\mathcal{L} = {\mathcal{L}}_{KL} + {\mathcal{L}}_{\text{recon }} + \alpha {\mathcal{L}}_{CL} - \beta {\mathcal{L}}_{CE} \tag{8}\]<p></p><p>where \(\alpha ,\beta\) are trade-off weights. The model is trained with Adam (Kingma &#x26;Ba,2014). Our model is optimized with \(\mathcal{L}\) and will be tested on five different metrics. This is different from the methods that only optimize and test for specific metrics (Koyejo et al., 2015; Decubber et al., 2018).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(\alpha ,\beta\) 是权衡权重。模型使用 Adam（Kingma &#x26; Ba，2014）进行训练。我们的模型使用 \(\mathcal{L}\) 进行优化，并将在五个不同的指标上进行测试。这与仅针对特定指标进行优化和测试的方法（Koyejo 等，2015；Decubber 等，2018）不同。</p></div><h3>2.3. Prediction</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>2.3. 预测</h3></div><p>During the testing phase,the input sample \(x\) will be passed to the feature encoder and decoder to obtain its embedding \({w}_{x}^{f}\) . Label embeddings \({w}_{i}^{l}\) are fixed in testing. The inner products between \({w}_{x}^{f}\) and \({w}_{i}^{l}\) will be passed through a sigmoid function to obtain the prediction probability for each class \(i\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在测试阶段，输入样本 \(x\) 将传递给特征编码器和解码器，以获得其嵌入 \({w}_{x}^{f}\)。标签嵌入 \({w}_{i}^{l}\) 在测试中是固定的。\({w}_{x}^{f}\) 和 \({w}_{i}^{l}\) 之间的内积将通过 sigmoid 函数传递，以获得每个类别 \(i\) 的预测概率。</p></div><h3>2.4. Insights behind C-GMVAE</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>2.4. C-GMVAE 背后的见解</h3></div><p>C2AE and MPVAE have shown the importance of learning a shared latent space for both features and labels. These methods share the same high-level insight similar to a teacher-student regime: we map labels (teacher) to a latent space with some certain structure, which preserves the label information and is easier to decode back to labels. Then the features (student) are expected to be mapped to this latent space to facilitate the label prediction. Two general concerns exist for these methods: 1) the unimodal Gaussian space previously used is too restrictive to impose sophisticated structures on prior, and 2) they do not properly capture label correlations with embeddings. To address the first, we learn a modality for each label class to form a mixture latent space. For the second, we replace the commonly used ranking and triplet losses with contrastive loss since contrastive loss involves more samples than triplet loss and has a larger capacity than ranking loss.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>C2AE 和 MPVAE 显示了为特征和标签学习共享潜在空间的重要性。这些方法共享相似的高层次见解，类似于教师-学生机制：我们将标签（教师）映射到具有某种特定结构的潜在空间，这保留了标签信息，并且更容易解码回标签。然后，特征（学生）被期望映射到这个潜在空间，以促进标签预测。这些方法存在两个普遍关注的问题：1）之前使用的单模态高斯空间对先验施加复杂结构的限制过于严格，2）它们未能正确捕捉标签与嵌入之间的相关性。为了解决第一个问题，我们为每个标签类别学习一个模态，以形成混合潜在空间。对于第二个问题，我们用对比损失替代常用的排名和三元组损失，因为对比损失涉及的样本比三元组损失更多，且容量大于排名损失。</p></div><h2>3. Related Work</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>3. 相关工作</h2></div><p>Learning a shared latent space for features and labels is a common and useful idea. For single-label prediction tasks, CADA-VAE (Schönfeld et al., 2019) learns and aligns latent label and feature spaces through distribution alignment losses. Similar ideas can be seen in out-of-distribution detection as well (Sundar et al., 2020). In multi-label scenarios, methods adopting this idea typically have a similar module that directly maps the multi-hot labels to embeddings (Yeh et al., 2017; Chen et al., 2019a; Bai et al., 2020). This is a rather difficult learning task. Suppose we have 30 label categories. There could be up to \({2}^{30}\) label sets. For probabilistic models like MPVAE, that means one latent label space has to represent up to \({2}^{30}\) label combinations. In contrast,C-GMVAE learns per-category subspaces and forms a mixture prior distribution based on the observed samples' label sets.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为特征和标签学习共享潜在空间是一个常见且有用的想法。对于单标签预测任务，CADA-VAE（Schönfeld 等，2019）通过分布对齐损失学习并对齐潜在标签和特征空间。类似的想法也可以在分布外检测中看到（Sundar 等，2020）。在多标签场景中，采用这一思想的方法通常具有一个类似的模块，直接将多热标签映射到嵌入（Yeh 等，2017；Chen 等，2019a；Bai 等，2020）。这是一个相当困难的学习任务。假设我们有 30 个标签类别。可能有多达 \({2}^{30}\) 个标签集。对于像 MPVAE 这样的概率模型，这意味着一个潜在标签空间必须表示多达 \({2}^{30}\) 个标签组合。相比之下，C-GMVAE 学习每个类别的子空间，并基于观察样本的标签集形成混合先验分布。</p></div><p>Contrastive learning has become one of the most popular self-supervised learning techniques. It has also been applied to supervised learning tasks. SupCon (Khosla et al., 2021) first demonstrated the effectiveness of supervised contrastive loss in image classification tasks. It was soon generalized to other domains like visual reasoning (Malkiński &#x26; Mańdziuk, 2020; Dao et al., 2021). Nevertheless, these methods depend on vision-specific augmentation techniques and attention mechanisms. Another related work is multi-label contrastive learning (Song &#x26; Ermon, 2020). But the work does not deal with MLC. Instead, it extends contrastive learning to the identification of more than one positive sample, which resembles a multi-label scenario.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>对比学习已成为最流行的自监督学习技术之一。它也被应用于监督学习任务。监督对比损失（SupCon）（Khosla等人，2021年）首次证明了监督对比损失在图像分类任务中的有效性。它很快被推广到其他领域，如视觉推理（Malkiński和Mańdziuk，2020年；Dao等人，2021年）。然而，这些方法依赖于特定于视觉的增强技术和注意力机制。另一项相关工作是多标签对比学习（Song和Ermon，2020年）。但该工作并未处理多标签分类（MLC）问题。相反，它将对比学习扩展到识别多个正样本，这类似于多标签场景。</p></div><p>Some earlier works also attempted metric learning or triplet loss in MLC (Annarumma &#x26; Montana, 2017). Triplet loss typically only takes one pair of positive and negative samples for one anchor, while contrastive loss uses many more negative and positive samples. Recent papers found that more samples can greatly boost performance (Chen et al., 2020; Wang et al., 2021). Though our contrastive module is constrained by the maximum number of label classes, the number of used samples has already surpassed other losses</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>一些早期的工作也尝试在多标签分类（MLC）中使用度量学习或三元组损失（Annarumma和Montana，2017年）。三元组损失通常只为一个锚点选取一对正样本和负样本，而对比损失使用更多的负样本和正样本。近期的论文发现，更多的样本可以显著提升性能（Chen等人，2020年；Wang等人，2021年）。尽管我们的对比模块受到标签类别最大数量的限制，但所使用的样本数量已经超过了其他损失函数。</p></div><p>Gaussian Mixture VAE with Contrastive Learning for Multi-Label Classification (e.g., triplet loss), and our observations reinforce that more samples help with the performance.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>用于多标签分类的带对比学习的高斯混合变分自编码器（例如，三元组损失），并且我们的观察结果证实了更多的样本有助于提升性能。</p></div><!-- Media --><table><tbody><tr><td>Metric</td><td colspan="9">example-F1</td><td colspan="9">micro-F1</td></tr><tr><td>Dataset</td><td>eBird</td><td>mirflickr</td><td>nus-vec</td><td>yeast</td><td>scene</td><td>sider</td><td>reuters</td><td>\( {bkms} \)</td><td>delicious</td><td>eBird</td><td>mirflickr</td><td>nus-vec</td><td>yeast</td><td>scene</td><td>sider</td><td>reuters</td><td>\( {bkms} \)</td><td>delicious</td></tr><tr><td>BR</td><td>0.365</td><td>0.325</td><td>0.343</td><td>0.630</td><td>0.606</td><td>0.766</td><td>0.733</td><td>0.171</td><td>0.174</td><td>0.384</td><td>0.371</td><td>0.371</td><td>0.655</td><td>0.706</td><td>0.796</td><td>0.767</td><td>0.125</td><td>0.197</td></tr><tr><td>MLKNN</td><td>0.510</td><td>0.383</td><td>0.342</td><td>0.618</td><td>0.691</td><td>0.738</td><td>0.703</td><td>0.213</td><td>0.259</td><td>0.557</td><td>0.415</td><td>0.368</td><td>0.625</td><td>0.667</td><td>0.772</td><td>0.680</td><td>0.181</td><td>0.264</td></tr><tr><td>HARAM</td><td>0.510</td><td>0.432</td><td>0.396</td><td>0.629</td><td>0.717</td><td>0.722</td><td>0.711</td><td>0.216</td><td>0.267</td><td>0.573</td><td>0.447</td><td>0.415</td><td>0.635</td><td>0.693</td><td>0.754</td><td>0.695</td><td>0.230</td><td>0.273</td></tr><tr><td>SLEEC</td><td>0.258</td><td>0.416</td><td>0.431</td><td>0.643</td><td>0.718</td><td>0.581</td><td>0.885</td><td>0.363</td><td>0.308</td><td>0.412</td><td>0.413</td><td>0.428</td><td>0.653</td><td>0.699</td><td>0.697</td><td>0.845</td><td>0.300</td><td>0.333</td></tr><tr><td>C2AE</td><td>0.501</td><td>0.501</td><td>0.435</td><td>0.614</td><td>0.698</td><td>0.768</td><td>0.818</td><td>0.309</td><td>0.326</td><td>0.546</td><td>0.545</td><td>0.472</td><td>0.626</td><td>0.713</td><td>0.798</td><td>0.799</td><td>0.316</td><td>0.348</td></tr><tr><td>LaMP</td><td>0.477</td><td>0.492</td><td>0.376</td><td>0.624</td><td>0.728</td><td>0.766</td><td>0.906</td><td>0.389</td><td>0.372</td><td>0.517</td><td>0.535</td><td>0.472</td><td>0.641</td><td>0.716</td><td>0.797</td><td>0.886</td><td>0.373</td><td>0.386</td></tr><tr><td>MPVAE</td><td>0.551</td><td>0.514</td><td>0.468</td><td>0.648</td><td>0.751</td><td>0.769</td><td>0.893</td><td>0.382</td><td>0.373</td><td>0.593</td><td>0.552</td><td>0.492</td><td>0.655</td><td>0.742</td><td>0.800</td><td>0.881</td><td>0.375</td><td>0.393</td></tr><tr><td>ASL</td><td>0.528</td><td>0.477</td><td>0.468</td><td>0.613</td><td>0.770</td><td>0.752</td><td>0.880</td><td>0.373</td><td>0.359</td><td>0.580</td><td>0.525</td><td>0.495</td><td>0.637</td><td>0.753</td><td>0.795</td><td>0.869</td><td>0.354</td><td>0.387</td></tr><tr><td>RBCC</td><td>0.503</td><td>0.468</td><td>0.466</td><td>0.605</td><td>0.758</td><td>0.733</td><td>0.857</td><td>-</td><td>-</td><td>0.558</td><td>0.513</td><td>0.490</td><td>0.623</td><td>0.749</td><td>0.784</td><td>0.825</td><td>-</td><td>-</td></tr><tr><td>C-GMVAE</td><td>0.576</td><td>0.534</td><td>0.481</td><td>0.656</td><td>0.777</td><td>0.771</td><td>0.917</td><td>0.392</td><td>0.381</td><td>0.633</td><td>0.575</td><td>0.510</td><td>0.665</td><td>0.762</td><td>0.803</td><td>0.890</td><td>0.377</td><td>0.403</td></tr><tr><td>std (±)</td><td>0.001</td><td>0.002</td><td>0.000</td><td>0.001</td><td>0.002</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.002</td><td>0.001</td><td>0.001</td><td>0.000</td><td>0.002</td><td>0.002</td><td>0.000</td><td>0.001</td><td>0.001</td><td>0.002</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>度量</td><td colspan="9">示例-F1</td><td colspan="9">微观-F1</td></tr><tr><td>数据集</td><td>eBird</td><td>mirflickr</td><td>nus-vec</td><td>酵母</td><td>场景</td><td>sider</td><td>路透社</td><td>\( {bkms} \)</td><td>美味</td><td>eBird</td><td>mirflickr</td><td>nus-vec</td><td>酵母</td><td>场景</td><td>sider</td><td>路透社</td><td>\( {bkms} \)</td><td>美味</td></tr><tr><td>BR</td><td>0.365</td><td>0.325</td><td>0.343</td><td>0.630</td><td>0.606</td><td>0.766</td><td>0.733</td><td>0.171</td><td>0.174</td><td>0.384</td><td>0.371</td><td>0.371</td><td>0.655</td><td>0.706</td><td>0.796</td><td>0.767</td><td>0.125</td><td>0.197</td></tr><tr><td>MLKNN</td><td>0.510</td><td>0.383</td><td>0.342</td><td>0.618</td><td>0.691</td><td>0.738</td><td>0.703</td><td>0.213</td><td>0.259</td><td>0.557</td><td>0.415</td><td>0.368</td><td>0.625</td><td>0.667</td><td>0.772</td><td>0.680</td><td>0.181</td><td>0.264</td></tr><tr><td>HARAM</td><td>0.510</td><td>0.432</td><td>0.396</td><td>0.629</td><td>0.717</td><td>0.722</td><td>0.711</td><td>0.216</td><td>0.267</td><td>0.573</td><td>0.447</td><td>0.415</td><td>0.635</td><td>0.693</td><td>0.754</td><td>0.695</td><td>0.230</td><td>0.273</td></tr><tr><td>SLEEC</td><td>0.258</td><td>0.416</td><td>0.431</td><td>0.643</td><td>0.718</td><td>0.581</td><td>0.885</td><td>0.363</td><td>0.308</td><td>0.412</td><td>0.413</td><td>0.428</td><td>0.653</td><td>0.699</td><td>0.697</td><td>0.845</td><td>0.300</td><td>0.333</td></tr><tr><td>C2AE</td><td>0.501</td><td>0.501</td><td>0.435</td><td>0.614</td><td>0.698</td><td>0.768</td><td>0.818</td><td>0.309</td><td>0.326</td><td>0.546</td><td>0.545</td><td>0.472</td><td>0.626</td><td>0.713</td><td>0.798</td><td>0.799</td><td>0.316</td><td>0.348</td></tr><tr><td>LaMP</td><td>0.477</td><td>0.492</td><td>0.376</td><td>0.624</td><td>0.728</td><td>0.766</td><td>0.906</td><td>0.389</td><td>0.372</td><td>0.517</td><td>0.535</td><td>0.472</td><td>0.641</td><td>0.716</td><td>0.797</td><td>0.886</td><td>0.373</td><td>0.386</td></tr><tr><td>MPVAE</td><td>0.551</td><td>0.514</td><td>0.468</td><td>0.648</td><td>0.751</td><td>0.769</td><td>0.893</td><td>0.382</td><td>0.373</td><td>0.593</td><td>0.552</td><td>0.492</td><td>0.655</td><td>0.742</td><td>0.800</td><td>0.881</td><td>0.375</td><td>0.393</td></tr><tr><td>ASL</td><td>0.528</td><td>0.477</td><td>0.468</td><td>0.613</td><td>0.770</td><td>0.752</td><td>0.880</td><td>0.373</td><td>0.359</td><td>0.580</td><td>0.525</td><td>0.495</td><td>0.637</td><td>0.753</td><td>0.795</td><td>0.869</td><td>0.354</td><td>0.387</td></tr><tr><td>RBCC</td><td>0.503</td><td>0.468</td><td>0.466</td><td>0.605</td><td>0.758</td><td>0.733</td><td>0.857</td><td>-</td><td>-</td><td>0.558</td><td>0.513</td><td>0.490</td><td>0.623</td><td>0.749</td><td>0.784</td><td>0.825</td><td>-</td><td>-</td></tr><tr><td>C-GMVAE</td><td>0.576</td><td>0.534</td><td>0.481</td><td>0.656</td><td>0.777</td><td>0.771</td><td>0.917</td><td>0.392</td><td>0.381</td><td>0.633</td><td>0.575</td><td>0.510</td><td>0.665</td><td>0.762</td><td>0.803</td><td>0.890</td><td>0.377</td><td>0.403</td></tr><tr><td>标准 (±)</td><td>0.001</td><td>0.002</td><td>0.000</td><td>0.001</td><td>0.002</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.002</td><td>0.001</td><td>0.001</td><td>0.000</td><td>0.002</td><td>0.002</td><td>0.000</td><td>0.001</td><td>0.001</td><td>0.002</td></tr></tbody></table></div><p>Table 1. The example-F1 (ex-F1) and micro-F1 (mi-F1) scores of different methods on all datasets. C-GMVAE's numbers are averaged over 3 seeds. The standard deviation (std) is also shown. 0.000 means an std&#x3C; 0.0005 .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表1. 不同方法在所有数据集上的示例-F1（ex-F1）和微-F1（mi-F1）得分。C-GMVAE的数值是基于3个种子的平均值。标准差（std）也显示出来。0.000表示标准差小于0.0005。</p></div><table><tbody><tr><td>Metric</td><td colspan="9">macro-F1</td><td colspan="9">Hamming Accuracy</td></tr><tr><td>Dataset</td><td>eBird</td><td>mirflickr</td><td>nus-vec</td><td>yeast</td><td>scene</td><td>sider</td><td>reuters</td><td>bkms</td><td>delicious</td><td>eBird</td><td>mirflickr</td><td>nus-vec</td><td>yeast</td><td>scene</td><td>sider</td><td>reuters</td><td>bkms</td><td>delicious</td></tr><tr><td>BR</td><td>0.116</td><td>0.182</td><td>0.083</td><td>0.373</td><td>0.704</td><td>0.588</td><td>0.137</td><td>0.038</td><td>0.066</td><td>0.816</td><td>0.886</td><td>0.971</td><td>0.782</td><td>0.901</td><td>0.747</td><td>0.994</td><td>0.990</td><td>0.982</td></tr><tr><td>MLKNN</td><td>0.338</td><td>0.266</td><td>0.086</td><td>0.472</td><td>0.693</td><td>0.667</td><td>0.066</td><td>0.041</td><td>0.053</td><td>0.827</td><td>0.877</td><td>0.971</td><td>0.784</td><td>0.863</td><td>0.715</td><td>0.992</td><td>0.991</td><td>0.981</td></tr><tr><td>HARAM</td><td>0.474</td><td>0.284</td><td>0.157</td><td>0.448</td><td>0.713</td><td>0.649</td><td>0.100</td><td>0.140</td><td>0.074</td><td>0.819</td><td>0.634</td><td>0.971</td><td>0.744</td><td>0.902</td><td>0.650</td><td>0.905</td><td>0.990</td><td>0.981</td></tr><tr><td>SLEEC</td><td>0.363</td><td>0.364</td><td>0.135</td><td>0.425</td><td>0.699</td><td>0.592</td><td>0.403</td><td>0.195</td><td>0.142</td><td>0.816</td><td>0.870</td><td>0.971</td><td>0.782</td><td>0.894</td><td>0.675</td><td>0.996</td><td>0.989</td><td>0.982</td></tr><tr><td>C2AE</td><td>0.426</td><td>0.393</td><td>0.174</td><td>0.427</td><td>0.728</td><td>0.667</td><td>0.363</td><td>0.232</td><td>0.102</td><td>0.771</td><td>0.897</td><td>0.973</td><td>0.764</td><td>0.893</td><td>0.749</td><td>0.995</td><td>0.991</td><td>0.981</td></tr><tr><td>LaMP</td><td>0.381</td><td>0.387</td><td>0.203</td><td>0.480</td><td>0.745</td><td>0.668</td><td>0.520</td><td>0.286</td><td>0.196</td><td>0.811</td><td>0.897</td><td>0.980</td><td>0.786</td><td>0.903</td><td>0.751</td><td>0.997</td><td>0.992</td><td>0.982</td></tr><tr><td>MPVAE</td><td>0.494</td><td>0.422</td><td>0.211</td><td>0.482</td><td>0.750</td><td>0.690</td><td>0.545</td><td>0.285</td><td>0.181</td><td>0.829</td><td>0.898</td><td>0.980</td><td>0.792</td><td>0.909</td><td>0.755</td><td>0.997</td><td>0.991</td><td>0.982</td></tr><tr><td>ASL</td><td>0.467</td><td>0.410</td><td>0.208</td><td>0.484</td><td>0.765</td><td>0.668</td><td>0.563</td><td>0.264</td><td>0.183</td><td>0.831</td><td>0.893</td><td>0.975</td><td>0.796</td><td>0.912</td><td>0.759</td><td>0.997</td><td>0.991</td><td>0.982</td></tr><tr><td>RBCC</td><td>0.443</td><td>0.409</td><td>0.202</td><td>0.480</td><td>0.753</td><td>0.654</td><td>0.503</td><td>-</td><td>-</td><td>0.815</td><td>0.888</td><td>0.975</td><td>0.793</td><td>0.904</td><td>0.753</td><td>0.997</td><td>-</td><td>-</td></tr><tr><td>C-GMVAE</td><td>0.538</td><td>0.440</td><td>0.226</td><td>0.487</td><td>0.769</td><td>0.691</td><td>0.582</td><td>0.291</td><td>0.197</td><td>0.847</td><td>0.903</td><td>0.984</td><td>0.796</td><td>0.915</td><td>0.767</td><td>0.997</td><td>0.992</td><td>0.983</td></tr><tr><td>std (±)</td><td>0.000</td><td>0.001</td><td>0.001</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.000</td><td>0.000</td><td>0.002</td><td>0.001</td><td>0.003</td><td>0.000</td><td>0.000</td><td>0.000</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>度量</td><td colspan="9">宏观F1</td><td colspan="9">汉明准确率</td></tr><tr><td>数据集</td><td>eBird</td><td>mirflickr</td><td>nus-vec</td><td>酵母</td><td>场景</td><td>sider</td><td>路透社</td><td>bkms</td><td>美味</td><td>eBird</td><td>mirflickr</td><td>nus-vec</td><td>酵母</td><td>场景</td><td>sider</td><td>路透社</td><td>bkms</td><td>美味</td></tr><tr><td>BR</td><td>0.116</td><td>0.182</td><td>0.083</td><td>0.373</td><td>0.704</td><td>0.588</td><td>0.137</td><td>0.038</td><td>0.066</td><td>0.816</td><td>0.886</td><td>0.971</td><td>0.782</td><td>0.901</td><td>0.747</td><td>0.994</td><td>0.990</td><td>0.982</td></tr><tr><td>MLKNN</td><td>0.338</td><td>0.266</td><td>0.086</td><td>0.472</td><td>0.693</td><td>0.667</td><td>0.066</td><td>0.041</td><td>0.053</td><td>0.827</td><td>0.877</td><td>0.971</td><td>0.784</td><td>0.863</td><td>0.715</td><td>0.992</td><td>0.991</td><td>0.981</td></tr><tr><td>HARAM</td><td>0.474</td><td>0.284</td><td>0.157</td><td>0.448</td><td>0.713</td><td>0.649</td><td>0.100</td><td>0.140</td><td>0.074</td><td>0.819</td><td>0.634</td><td>0.971</td><td>0.744</td><td>0.902</td><td>0.650</td><td>0.905</td><td>0.990</td><td>0.981</td></tr><tr><td>SLEEC</td><td>0.363</td><td>0.364</td><td>0.135</td><td>0.425</td><td>0.699</td><td>0.592</td><td>0.403</td><td>0.195</td><td>0.142</td><td>0.816</td><td>0.870</td><td>0.971</td><td>0.782</td><td>0.894</td><td>0.675</td><td>0.996</td><td>0.989</td><td>0.982</td></tr><tr><td>C2AE</td><td>0.426</td><td>0.393</td><td>0.174</td><td>0.427</td><td>0.728</td><td>0.667</td><td>0.363</td><td>0.232</td><td>0.102</td><td>0.771</td><td>0.897</td><td>0.973</td><td>0.764</td><td>0.893</td><td>0.749</td><td>0.995</td><td>0.991</td><td>0.981</td></tr><tr><td>LaMP</td><td>0.381</td><td>0.387</td><td>0.203</td><td>0.480</td><td>0.745</td><td>0.668</td><td>0.520</td><td>0.286</td><td>0.196</td><td>0.811</td><td>0.897</td><td>0.980</td><td>0.786</td><td>0.903</td><td>0.751</td><td>0.997</td><td>0.992</td><td>0.982</td></tr><tr><td>MPVAE</td><td>0.494</td><td>0.422</td><td>0.211</td><td>0.482</td><td>0.750</td><td>0.690</td><td>0.545</td><td>0.285</td><td>0.181</td><td>0.829</td><td>0.898</td><td>0.980</td><td>0.792</td><td>0.909</td><td>0.755</td><td>0.997</td><td>0.991</td><td>0.982</td></tr><tr><td>ASL</td><td>0.467</td><td>0.410</td><td>0.208</td><td>0.484</td><td>0.765</td><td>0.668</td><td>0.563</td><td>0.264</td><td>0.183</td><td>0.831</td><td>0.893</td><td>0.975</td><td>0.796</td><td>0.912</td><td>0.759</td><td>0.997</td><td>0.991</td><td>0.982</td></tr><tr><td>RBCC</td><td>0.443</td><td>0.409</td><td>0.202</td><td>0.480</td><td>0.753</td><td>0.654</td><td>0.503</td><td>-</td><td>-</td><td>0.815</td><td>0.888</td><td>0.975</td><td>0.793</td><td>0.904</td><td>0.753</td><td>0.997</td><td>-</td><td>-</td></tr><tr><td>C-GMVAE</td><td>0.538</td><td>0.440</td><td>0.226</td><td>0.487</td><td>0.769</td><td>0.691</td><td>0.582</td><td>0.291</td><td>0.197</td><td>0.847</td><td>0.903</td><td>0.984</td><td>0.796</td><td>0.915</td><td>0.767</td><td>0.997</td><td>0.992</td><td>0.983</td></tr><tr><td>标准差 (±)</td><td>0.000</td><td>0.001</td><td>0.001</td><td>0.002</td><td>0.002</td><td>0.002</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.000</td><td>0.000</td><td>0.002</td><td>0.001</td><td>0.003</td><td>0.000</td><td>0.000</td><td>0.000</td></tr></tbody></table></div><p>Table 2. The macro-F1 (ma-F1) and Hamming accuracy (HA) scores of different methods on all datasets. C-GMVAE's numbers are averaged over 3 seeds.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表2. 不同方法在所有数据集上的宏观F1（ma-F1）和汉明准确率（HA）得分。C-GMVAE的数值是基于3个种子的平均值。</p></div><!-- Media --><h2>4. Experiments</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>4. 实验</h2></div><p>We have various setups to validate the performance of C-GMVAE. First, we compare the example-F1, micro-F1 and macro-F1 scores, Hamming accuracies and precision@1 of different methods. Second, we compare their performance when fewer training data are available. Third, an ablation study shows the importance of the proposed modules. Finally, we demonstrate the interpretability of the label embeddings on the eBird dataset. Our code is publicly available \({}^{1}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们有多种设置来验证C-GMVAE的性能。首先，我们比较不同方法的示例F1、微观F1和宏观F1得分、汉明准确率和精确度@1。其次，我们比较在训练数据较少时的性能。第三，消融研究显示了所提模块的重要性。最后，我们展示了在eBird数据集上标签嵌入的可解释性。我们的代码是公开可用的\({}^{1}\)。</p></div><h3>4.1. Setup</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.1. 设置</h3></div><p>For the main evaluation experiments, we use nine datasets, including image datasets mirflickr, nuswide, scene (Huiskes &#x26; Lew, 2008; Chua et al., 2009; Boutell et al., 2004), biology datasets sider, yeast (Kuhn et al., 2016; Nakai &#x26; Kanehisa, 1992), ecology dataset eBird (Fink et al., 2017), and text datasets reuters, bookmarks, delicious (Debole &#x26; Sebastiani, 2005; Katakis et al., 2008; Tsoumakas et al., 2008) (see Tab. 4 for dataset statistics). All features are collected in vector format (Lanchantin et al., 2019; Bai et al., 2020). The feature pre-processing is standard following previous works (Lanchantin et al., 2019; Bai et al., 2020) and the datasets are public \({}^{2}\) . Each dataset is separated into training (80%), validation (10%) and testing (10%) splits. The datasets are also preprocessed to fit the input formats of different methods. We use mini-batch training with batch size 128. Each batch is randomly sampled from the dataset.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>对于主要评估实验，我们使用九个数据集，包括图像数据集mirflickr、nuswide、scene（Huiskes &#x26; Lew, 2008; Chua et al., 2009; Boutell et al., 2004）、生物数据集sider、yeast（Kuhn et al., 2016; Nakai &#x26; Kanehisa, 1992）、生态数据集eBird（Fink et al., 2017）和文本数据集reuters、bookmarks、delicious（Debole &#x26; Sebastiani, 2005; Katakis et al., 2008; Tsoumakas et al., 2008）（见表4获取数据集统计信息）。所有特征以向量格式收集（Lanchantin et al., 2019; Bai et al., 2020）。特征预处理遵循之前的工作（Lanchantin et al., 2019; Bai et al., 2020），数据集是公开的\({}^{2}\)。每个数据集分为训练（80%）、验证（10%）和测试（10%）部分。数据集也经过预处理以适应不同方法的输入格式。我们使用小批量训练，批量大小为128。每个批次是从数据集中随机抽样的。</p></div><p>The evaluation metrics are three F1 scores, Hamming accuracy and <a href="mailto:precision@1.The">precision@1.The</a> evaluation process, model selection and preprocessing strictly follow previous works (Tu &#x26; Gimpel, 2018; Lanchantin et al., 2019; Bai et al., 2020). Most numbers are also directly quoted from the corresponding papers for comparison. Our method is compared against ASL (Ridnik et al., 2021), RBCC (Gerych et al., 2021), MPVAE (Bai et al., 2020), LaMP (Lanchantin et al., 2019), C2AE (Yeh et al., 2017), SLEEC (Bhatia et al., 2015), HARAM (Benites &#x26; Sapozhnikova, 2015), MLKNN</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>评估指标包括三个F1得分、汉明准确率和精确度@1。评估过程、模型选择和预处理严格遵循之前的工作（Tu &#x26; Gimpel, 2018; Lanchantin et al., 2019; Bai et al., 2020）。大多数数字也直接引用自相应的论文以便比较。我们的方法与ASL（Ridnik et al., 2021）、RBCC（Gerych et al., 2021）、MPVAE（Bai et al., 2020）、LaMP（Lanchantin et al., 2019）、C2AE（Yeh et al., 2017）、SLEEC（Bhatia et al., 2015）、HARAM（Benites &#x26; Sapozhnikova, 2015）、MLKNN进行比较。</p></div><hr>
<!-- Footnote --><p>\({}^{1}\) <a href="https://github.com/JunwenBai/c-gmvae">https://github.com/JunwenBai/c-gmvae</a></p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{1}\) <a href="https://github.com/JunwenBai/c-gmvae">https://github.com/JunwenBai/c-gmvae</a></p></div><p>\({}^{2}\) <a href="http://mulan.sourceforge.net/datasets-mlc.html">http://mulan.sourceforge.net/datasets-mlc.html</a></p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{2}\) <a href="http://mulan.sourceforge.net/datasets-mlc.html">http://mulan.sourceforge.net/datasets-mlc.html</a></p></div><!-- Footnote -->
<hr><!-- Media --><table><tbody><tr><td>Dataset</td><td>eBird</td><td>mir.</td><td>nus-vec</td><td>yeast</td><td>scene</td><td>sider</td><td>reuters</td><td>\( {bkms} \)</td><td>del.</td></tr><tr><td>BR</td><td>0.598</td><td>0.582</td><td>0.443</td><td>0.745</td><td>0.700</td><td>0.573</td><td>0.752</td><td>0.301</td><td>0.485</td></tr><tr><td>MLKNN</td><td>0.772</td><td>0.491</td><td>0.456</td><td>0.730</td><td>0.675</td><td>0.916</td><td>0.753</td><td>0.310</td><td>0.460</td></tr><tr><td>MLARAM</td><td>0.768</td><td>0.350</td><td>0.404</td><td>0.682</td><td>0.722</td><td>0.930</td><td>0.679</td><td>0.312</td><td>0.419</td></tr><tr><td>SLEEC</td><td>0.656</td><td>0.623</td><td>0.531</td><td>0.745</td><td>0.730</td><td>0.882</td><td>0.908</td><td>0.415</td><td>0.676</td></tr><tr><td>C2AE</td><td>0.753</td><td>0.705</td><td>0.569</td><td>0.749</td><td>0.703</td><td>0.923</td><td>0.845</td><td>0.407</td><td>0.609</td></tr><tr><td>LaMP</td><td>0.737</td><td>0.685</td><td>0.456</td><td>0.740</td><td>0.746</td><td>0.937</td><td>0.927</td><td>0.420</td><td>0.663</td></tr><tr><td>MPVAE</td><td>0.820</td><td>0.726</td><td>0.587</td><td>0.743</td><td>0.777</td><td>0.958</td><td>0.930</td><td>0.437</td><td>0.696</td></tr><tr><td>ASL</td><td>0.818</td><td>0.681</td><td>0.586</td><td>0.752</td><td>0.770</td><td>0.954</td><td>0.929</td><td>0.418</td><td>0.692</td></tr><tr><td>RBCC</td><td>0.805</td><td>0.682</td><td>0.582</td><td>0.745</td><td>0.777</td><td>0.942</td><td>0.913</td><td>-</td><td>-</td></tr><tr><td>C-GMVAE</td><td>0.825</td><td>0.732</td><td>0.595</td><td>0.751</td><td>0.788</td><td>0.962</td><td>0.939</td><td>0.465</td><td>0.707</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>数据集</td><td>eBird</td><td>镜头</td><td>nus-vec</td><td>酵母</td><td>场景</td><td>侧面</td><td>路透社</td><td>\( {bkms} \)</td><td>删除</td></tr><tr><td>BR</td><td>0.598</td><td>0.582</td><td>0.443</td><td>0.745</td><td>0.700</td><td>0.573</td><td>0.752</td><td>0.301</td><td>0.485</td></tr><tr><td>MLKNN</td><td>0.772</td><td>0.491</td><td>0.456</td><td>0.730</td><td>0.675</td><td>0.916</td><td>0.753</td><td>0.310</td><td>0.460</td></tr><tr><td>MLARAM</td><td>0.768</td><td>0.350</td><td>0.404</td><td>0.682</td><td>0.722</td><td>0.930</td><td>0.679</td><td>0.312</td><td>0.419</td></tr><tr><td>SLEEC</td><td>0.656</td><td>0.623</td><td>0.531</td><td>0.745</td><td>0.730</td><td>0.882</td><td>0.908</td><td>0.415</td><td>0.676</td></tr><tr><td>C2AE</td><td>0.753</td><td>0.705</td><td>0.569</td><td>0.749</td><td>0.703</td><td>0.923</td><td>0.845</td><td>0.407</td><td>0.609</td></tr><tr><td>LaMP</td><td>0.737</td><td>0.685</td><td>0.456</td><td>0.740</td><td>0.746</td><td>0.937</td><td>0.927</td><td>0.420</td><td>0.663</td></tr><tr><td>MPVAE</td><td>0.820</td><td>0.726</td><td>0.587</td><td>0.743</td><td>0.777</td><td>0.958</td><td>0.930</td><td>0.437</td><td>0.696</td></tr><tr><td>ASL</td><td>0.818</td><td>0.681</td><td>0.586</td><td>0.752</td><td>0.770</td><td>0.954</td><td>0.929</td><td>0.418</td><td>0.692</td></tr><tr><td>RBCC</td><td>0.805</td><td>0.682</td><td>0.582</td><td>0.745</td><td>0.777</td><td>0.942</td><td>0.913</td><td>-</td><td>-</td></tr><tr><td>C-GMVAE</td><td>0.825</td><td>0.732</td><td>0.595</td><td>0.751</td><td>0.788</td><td>0.962</td><td>0.939</td><td>0.465</td><td>0.707</td></tr></tbody></table></div><!-- Media --><p>Table 3. The precision@1 scores of different methods on all datasets. "mir." stands for mirflickr and "del." means delicious dataset.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表3. 不同方法在所有数据集上的精确度@1分数。“mir.”代表mirflickr，“del.”表示delicious数据集。</p></div><!-- Media --><table><tbody><tr><td>Dataset</td><td>#Samples</td><td>#Labels</td><td>Mean Labels /Sample</td><td>Median Labels /Sample</td><td>Max Labels /Sample</td><td>Mean Samples /Label</td></tr><tr><td>eBird</td><td>41778</td><td>100</td><td>20.69</td><td>18</td><td>96</td><td>8322.95</td></tr><tr><td>bookmarks</td><td>87856</td><td>208</td><td>2.03</td><td>1</td><td>44</td><td>584.67</td></tr><tr><td>nus-vec</td><td>269648</td><td>85</td><td>1.86</td><td>1</td><td>12</td><td>3721.7</td></tr><tr><td>mirflickr</td><td>25000</td><td>38</td><td>4.80</td><td>5</td><td>17</td><td>1247.34</td></tr><tr><td>reuters</td><td>10789</td><td>90</td><td>1.23</td><td>1</td><td>15</td><td>106.50</td></tr><tr><td>scene</td><td>2407</td><td>6</td><td>1.07</td><td>1</td><td>3</td><td>170.83</td></tr><tr><td>sider</td><td>1427</td><td>27</td><td>15.3</td><td>16</td><td>26</td><td>731.07</td></tr><tr><td>yeast</td><td>2417</td><td>14</td><td>4.24</td><td>4</td><td>11</td><td>363.14</td></tr><tr><td>delicious</td><td>16105</td><td>983</td><td>19.06</td><td>20</td><td>25</td><td>250.15</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>数据集</td><td>#样本</td><td>#标签</td><td>每个样本的平均标签</td><td>每个样本的中位数标签</td><td>每个样本的最大标签</td><td>每个标签的平均样本</td></tr><tr><td>eBird</td><td>41778</td><td>100</td><td>20.69</td><td>18</td><td>96</td><td>8322.95</td></tr><tr><td>书签</td><td>87856</td><td>208</td><td>2.03</td><td>1</td><td>44</td><td>584.67</td></tr><tr><td>nus-vec</td><td>269648</td><td>85</td><td>1.86</td><td>1</td><td>12</td><td>3721.7</td></tr><tr><td>mirflickr</td><td>25000</td><td>38</td><td>4.80</td><td>5</td><td>17</td><td>1247.34</td></tr><tr><td>路透社</td><td>10789</td><td>90</td><td>1.23</td><td>1</td><td>15</td><td>106.50</td></tr><tr><td>场景</td><td>2407</td><td>6</td><td>1.07</td><td>1</td><td>3</td><td>170.83</td></tr><tr><td>侧面</td><td>1427</td><td>27</td><td>15.3</td><td>16</td><td>26</td><td>731.07</td></tr><tr><td>酵母</td><td>2417</td><td>14</td><td>4.24</td><td>4</td><td>11</td><td>363.14</td></tr><tr><td>美味</td><td>16105</td><td>983</td><td>19.06</td><td>20</td><td>25</td><td>250.15</td></tr></tbody></table></div><p>Table 4. Dataset Statistics.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表4. 数据集统计。</p></div><!-- Media --><p>(Zhang &#x26; Zhou, 2007), and BR (Zhang et al., 2018).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(张 &#x26; 周, 2007)，以及 BR (张等, 2018)。</p></div><p>ASL introduces asymmetric loss, a variant of BCE and focal loss for MLC, and requires tuning of focusing parameters. RBCC is based on a Bayesian network, which requires structure learning to derive a directed acyclic graph (DAG) first. MPVAE is a novel method which learns and aligns the probabilistic feature and label subspaces. Label correlations are captured by a multivariate probit module. LaMP adopts attention-based neural message passing to handle the label correlations, which is a neural extension of previous CRF-based methods. C2AE was one of the first papers to use NNs to learn and align latent spaces. C2AE imposes a canonical correlation analysis (CCA) constraint on the latent space. SLEEC explores the low-rank assumption in MLC, to reduce the effective number of labels. Other deep methods make similar low-rank assumptions. HARAM was one of the first methods to introduced NNs to MLC. MLKNN is a classic MLC method using k-nearest neighbors (KNN). It finds nearest examples to a test sample and adopts Bayesian inference to select assigned labels. Lastly, binary relevance (BR) is one of the most intuitive solutions for MLC, which decomposes the multi-label scenario into independent binary prediction tasks.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>ASL 引入了不对称损失，这是 BCE 和焦点损失在多标签分类（MLC）中的变体，并需要调整聚焦参数。RBCC 基于贝叶斯网络，首先需要进行结构学习以推导出有向无环图（DAG）。MPVAE 是一种新颖的方法，它学习并对齐概率特征和标签子空间。标签相关性通过多元 probit 模块捕获。LaMP 采用基于注意力的神经消息传递来处理标签相关性，这是对以前基于条件随机场（CRF）方法的神经扩展。C2AE 是第一篇使用神经网络（NN）学习和对齐潜在空间的论文之一。C2AE 在潜在空间上施加了典型相关分析（CCA）约束。SLEEC 探索了 MLC 中的低秩假设，以减少有效标签数量。其他深度方法也做出了类似的低秩假设。HARAM 是第一批将神经网络引入 MLC 的方法之一。MLKNN 是一种经典的 MLC 方法，使用 k 最近邻（KNN）。它找到与测试样本最近的例子，并采用贝叶斯推断选择分配的标签。最后，二元相关性（BR）是 MLC 中最直观的解决方案之一，它将多标签场景分解为独立的二元预测任务。</p></div><h3>4.2. Metrics</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.2. 评估指标</h3></div><p>We evaluate our method trained with objective Eq. 8 on several commonly used multi-label metrics. Suppose the ground-truth label is \(y\) and the predicted label is \(\widehat{y}\) . We denote true positives, false positives, false negatives by \(t{p}_{j},f{p}_{j},f{n}_{j}\) respectively for the \(j\) -th of \(L\) label categories. (i) HA: \(\frac{1}{L}\mathop{\sum }\limits_{{j = 1}}^{L}\mathbb{1}\left\lbrack  {{y}_{j} = {\widehat{y}}_{j}}\right\rbrack\) (ii) example-F1: \(\frac{2\mathop{\sum }\limits_{{j = 1}}^{L}{y}_{i}{\widehat{y}}_{i}}{\mathop{\sum }\limits_{{j = 1}}^{L}{y}_{i} + \mathop{\sum }\limits_{{j = 1}}^{L}{\widehat{y}}_{i}}\) (iii) micro-F1: \(\frac{\mathop{\sum }\limits_{{j = 1}}^{L}t{p}_{j}}{\mathop{\sum }\limits_{{j = 1}}^{L}{2t}{p}_{j} + f{p}_{j} + f{n}_{j}}\) (iv) macro-F1: \(\frac{1}{L}\mathop{\sum }\limits_{{j = 1}}^{L}\frac{{2t}{p}_{j}}{{2t}{p}_{j} + f{p}_{j} + f{n}_{j}}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们在几个常用的多标签评估指标上评估了使用目标 Eq. 8 训练的方法。假设真实标签为 \(y\)，预测标签为 \(\widehat{y}\)。我们分别用 \(t{p}_{j},f{p}_{j},f{n}_{j}\) 表示第 \(j\) 类 \(L\) 标签类别的真正例、假正例和假负例。(i) HA: \(\frac{1}{L}\mathop{\sum }\limits_{{j = 1}}^{L}\mathbb{1}\left\lbrack  {{y}_{j} = {\widehat{y}}_{j}}\right\rbrack\) (ii) 示例-F1: \(\frac{2\mathop{\sum }\limits_{{j = 1}}^{L}{y}_{i}{\widehat{y}}_{i}}{\mathop{\sum }\limits_{{j = 1}}^{L}{y}_{i} + \mathop{\sum }\limits_{{j = 1}}^{L}{\widehat{y}}_{i}}\) (iii) 微-F1: \(\frac{\mathop{\sum }\limits_{{j = 1}}^{L}t{p}_{j}}{\mathop{\sum }\limits_{{j = 1}}^{L}{2t}{p}_{j} + f{p}_{j} + f{n}_{j}}\) (iv) 宏-F1: \(\frac{1}{L}\mathop{\sum }\limits_{{j = 1}}^{L}\frac{{2t}{p}_{j}}{{2t}{p}_{j} + f{p}_{j} + f{n}_{j}}\)</p></div><p>Furthermore,precision \(@1\) is the proportion of correctly predicted labels in the top-1 predictions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>此外，精确度 \(@1\) 是前 1 个预测中正确预测标签的比例。</p></div><h3>4.3. Architecture and Hyperparameters</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.3. 架构和超参数</h3></div><p>As we state in the introduction, we do not require very sophisticated neural architectures in C-GMVAE. All the neural layers are fully connected. The feature encoder is a fully connected NN with 3 hidden layers and the activation function is ReLU. The label encoder is also fully connected</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>正如我们在引言中所述，我们在 C-GMVAE 中不需要非常复杂的神经架构。所有神经层都是全连接的。特征编码器是一个具有 3 个隐藏层的全连接神经网络，激活函数为 ReLU。标签编码器也是全连接的。</p></div><!-- Media --><table><tbody><tr><td></td><td>variations</td><td>eb-F1</td><td>mi-F1</td><td>ma-F1</td></tr><tr><td rowspan="4">ebird</td><td>uni-Gaussian</td><td>0.545</td><td>0.583</td><td>0.490</td></tr><tr><td>GM only</td><td>0.561</td><td>0.603</td><td>0.511</td></tr><tr><td>contrastive only</td><td>0.558</td><td>0.594</td><td>0.515</td></tr><tr><td>GM+contrastive</td><td>0.576</td><td>0.633</td><td>0.538</td></tr><tr><td rowspan="4">mirflickr</td><td>uni-Gaussian</td><td>0.510</td><td>0.541</td><td>0.413</td></tr><tr><td>GM only</td><td>0.521</td><td>0.561</td><td>0.429</td></tr><tr><td>contrastive only</td><td>0.526</td><td>0.565</td><td>0.428</td></tr><tr><td>GM+contrastive</td><td>0.534</td><td>0.575</td><td>0.440</td></tr><tr><td rowspan="4">nus-vec</td><td>uni-Gaussian</td><td>0.461</td><td>0.479</td><td>0.203</td></tr><tr><td>GM only</td><td>0.472</td><td>0.505</td><td>0.218</td></tr><tr><td>contrastive only</td><td>0.470</td><td>0.501</td><td>0.213</td></tr><tr><td>GM+contrastive</td><td>0.481</td><td>0.510</td><td>0.226</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td></td><td>变体</td><td>eb-F1</td><td>mi-F1</td><td>ma-F1</td></tr><tr><td rowspan="4">ebird</td><td>统一高斯（uni-Gaussian）</td><td>0.545</td><td>0.583</td><td>0.490</td></tr><tr><td>仅GM</td><td>0.561</td><td>0.603</td><td>0.511</td></tr><tr><td>仅对比</td><td>0.558</td><td>0.594</td><td>0.515</td></tr><tr><td>GM+对比</td><td>0.576</td><td>0.633</td><td>0.538</td></tr><tr><td rowspan="4">mirflickr</td><td>统一高斯（uni-Gaussian）</td><td>0.510</td><td>0.541</td><td>0.413</td></tr><tr><td>仅GM</td><td>0.521</td><td>0.561</td><td>0.429</td></tr><tr><td>仅对比</td><td>0.526</td><td>0.565</td><td>0.428</td></tr><tr><td>GM+对比</td><td>0.534</td><td>0.575</td><td>0.440</td></tr><tr><td rowspan="4">nus-vec</td><td>统一高斯（uni-Gaussian）</td><td>0.461</td><td>0.479</td><td>0.203</td></tr><tr><td>仅GM</td><td>0.472</td><td>0.505</td><td>0.218</td></tr><tr><td>仅对比</td><td>0.470</td><td>0.501</td><td>0.213</td></tr><tr><td>GM+对比</td><td>0.481</td><td>0.510</td><td>0.226</td></tr></tbody></table></div><!-- Media --><p>Table 5. Ablation study on the contrastive learning module and the Gaussian mixture module. Note that both modules are contributions of this work. As shown in the table, GM consistently improves performance. The contrastive module can also further boost the performance. comprising two hidden layers and the decoder has two hidden layers as well. More details of the model can be found in the appendix. We set \(\alpha  = 1,\beta  = {0.5},E = {2048}\) by default. Grid search is applied to find the best learning rate, dropout ratio, and weight decay ratio for each dataset. We use one Nvidia V100 GPU for all experiments. More architecture, hyper-parameter tuning, and implementation details can be found in the appendix.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表5. 对比学习模块和高斯混合模块的消融研究。请注意，这两个模块都是本工作的贡献。如表中所示，高斯混合模块始终提高性能。对比模块也可以进一步提升性能。模型由两个隐藏层组成，解码器也有两个隐藏层。有关模型的更多细节，请参见附录。我们默认设置\(\alpha  = 1,\beta  = {0.5},E = {2048}\)。通过网格搜索找到每个数据集的最佳学习率、丢弃率和权重衰减率。我们使用一台Nvidia V100 GPU进行所有实验。有关更多架构、超参数调整和实现细节，请参见附录。</p></div><!-- Media --><table><tbody><tr><td></td><td>method (data %)</td><td>HA</td><td>ex-F1</td><td>mi-F1</td><td>ma-F1</td></tr><tr><td rowspan="2">ebird</td><td>MPVAE (100%)</td><td>0.829</td><td>0.551</td><td>0.593</td><td>0.494</td></tr><tr><td>C-GMVAE (50%)</td><td>0.842</td><td>0.557</td><td>0.615</td><td>0.521</td></tr><tr><td rowspan="2">mirflickr</td><td>MPVAE (100%)</td><td>0.898</td><td>0.514</td><td>0.552</td><td>0.422</td></tr><tr><td>C-GMVAE (50%)</td><td>0.899</td><td>0.512</td><td>0.553</td><td>0.412</td></tr><tr><td rowspan="2">nus-vec</td><td>MPVAE (100%)</td><td>0.980</td><td>0.468</td><td>0.492</td><td>0.211</td></tr><tr><td>C-GMVAE (50%)</td><td>0.975</td><td>0.465</td><td>0.494</td><td>0.201</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td></td><td>方法 (数据 %)</td><td>HA</td><td>ex-F1</td><td>mi-F1</td><td>ma-F1</td></tr><tr><td rowspan="2">ebird</td><td>MPVAE (100%)</td><td>0.829</td><td>0.551</td><td>0.593</td><td>0.494</td></tr><tr><td>C-GMVAE (50%)</td><td>0.842</td><td>0.557</td><td>0.615</td><td>0.521</td></tr><tr><td rowspan="2">mirflickr</td><td>MPVAE (100%)</td><td>0.898</td><td>0.514</td><td>0.552</td><td>0.422</td></tr><tr><td>C-GMVAE (50%)</td><td>0.899</td><td>0.512</td><td>0.553</td><td>0.412</td></tr><tr><td rowspan="2">nus-vec</td><td>MPVAE (100%)</td><td>0.980</td><td>0.468</td><td>0.492</td><td>0.211</td></tr><tr><td>C-GMVAE (50%)</td><td>0.975</td><td>0.465</td><td>0.494</td><td>0.201</td></tr></tbody></table></div><p>Table 6. Comparisons between MPVAE and C-GMVAE using \({100}\%\) and \({50}\%\) respectively.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表6. 使用\({100}\%\)和\({50}\%\)分别比较MPVAE和C-GMVAE。</p></div><!-- Media --><h3>4.4. Evaluations</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.4. 评估</h3></div><p>Full supervision In the full supervision scenario, which is commonly adopted by the methods we compare against, we evaluate five metrics: example-F1 (ex-F1), micro-F1 (mi-F1), macro-F1 (ma-F1), Hamming accuracy (HA) and <a href="mailto:precision@1.The">precision@1.The</a> ex-F1 score is the averaged F1-score over all the samples. The mi-F1 score measures the aggregated contributions of all classes. The ma-F1 treats each class equally and takes the class-wise average. HA counts the correctly predicted labels regardless of samples or classes. The full definitions of these metrics can be found in the appendix.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>完全监督 在完全监督场景中，这是我们比较的方法通常采用的，我们评估五个指标：示例-F1（ex-F1）、微观-F1（mi-F1）、宏观-F1（ma-F1）、汉明准确率（HA）和精确度@1。ex-F1分数是所有样本的平均F1分数。mi-F1分数衡量所有类别的聚合贡献。ma-F1平等对待每个类别并取类别平均值。HA计算正确预测的标签，无论样本或类别。所有这些指标的完整定义可以在附录中找到。</p></div><!-- Media --><!-- figureText: C-GMVAE MPVAE LaMP 0.04 0.02 1.0 0.1 0.5 1.0 data abundance \( \Delta \) ma-F1 over C2AE 0.06 0.04 0.02 0.00 0.1 0.5 data abundance --><img src="https://cdn.noedgeai.com/bo_d15g03ref24c73d1f6k0_7.jpg?x=899&#x26;y=231&#x26;w=690&#x26;h=338&#x26;r=0"><p>Figure 2. Relative improvements of C-GMVAE, MPVAE and LaMP on ma-F1 compared to C2AE. Left and right plots correspond to mirflickr and nus-vec datasets respectively. Every method (including C2AE) is trained on the same amount of data (10%, \({50}\%\) or \({100}\% )\) for comparison.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2. C-GMVAE、MPVAE和LaMP在ma-F1上相对于C2AE的相对改进。左侧和右侧图对应于mirflickr和nus-vec数据集。每种方法（包括C2AE）在相同数量的数据上训练（10%，\({50}\%\)或\({100}\% )\)进行比较。</p></div><!-- Media --><p>Tab. 1, 2 and 3 present the performance of all the methods w.r.t. the metrics. We abbreviate nuswide-vector to nus-vec, and bookmarks to bkms. C-GMVAE outperforms the existing state-of-the-art methods on all the datasets. The best numbers are marked in bold. All the numbers for C-GMVAE are averaged over 3 seeds for stability and the standard deviations are included in the table. On ex-F1, C-GMVAE improves over ASL by 5.3%, RBCC by 7.7%, MPVAE by 2.5%, and LaMP by 8.8% on average across all the datasets. Similarly, on mi-F1, C-GMVAE improves over ASL by 4.4%, RBCC by 6.7%, MPVAE by 2.4% and LaMP by 6.1% on average. On ma-F1, the improvements are as large as \({6.1}\% ,{9.4}\% ,{4.1}\%\) and \({11}\%\) ,respectively. C-GMVAE outperforms other methods consistently.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表1、2和3展示了所有方法在指标方面的表现。我们将nuswide-vector缩写为nus-vec，将书签缩写为bkms。C-GMVAE在所有数据集上优于现有的最先进方法。最佳数字以粗体标出。C-GMVAE的所有数字都是在3个种子上平均得出的，以确保稳定性，标准偏差也包含在表中。在ex-F1上，C-GMVAE在所有数据集上平均比ASL提高了5.3%，比RBCC提高了7.7%，比MPVAE提高了2.5%，比LaMP提高了8.8%。同样，在mi-F1上，C-GMVAE在所有数据集上平均比ASL提高了4.4%，比RBCC提高了6.7%，比MPVAE提高了2.4%，比LaMP提高了6.1%。在ma-F1上，改进幅度分别达到\({6.1}\% ,{9.4}\% ,{4.1}\%\)和\({11}\%\)。C-GMVAE始终优于其他方法。</p></div><p>Ablation study To demonstrate the strength of C-GMVAE, we compare it with a unimodal Gaussian latent model, a Gaussian mixture only latent model (without contrastive module), and a contrastive learning only model (without the KL divergence term) in Tab. 5. Our C-GMVAE (GM+contrastive) consistently outperforms other models by a large margin. For instance, on ma-F1, C-GMVAE improves over the unimodal Gaussian model by 7%. In Tab. 7, we show that the contrastive module outperforms both the GNN and covariance matrix modules.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>消融研究 为了展示C-GMVAE的优势，我们在表5中将其与单模态高斯潜在模型、仅高斯混合潜在模型（没有对比模块）和仅对比学习模型（没有KL散度项）进行比较。我们的C-GMVAE（GM+对比）始终以较大幅度优于其他模型。例如，在ma-F1上，C-GMVAE比单模态高斯模型提高了7%。在表7中，我们展示了对比模块优于GNN和协方差矩阵模块。</p></div><p>Training on fewer data Contrastive learning learns contrastive views and thus requires less information compared to generative learning, which demands a more complete representation for reconstruction. Contrastive learning has the potential to discover the intrinsic structure present in the data, and therefore is widely used in self-supervised learning because it generalizes well. We observe this with C-GMVAE as well. To demonstrate this, we shrink the size of training data by \({50}\%\) or \({90}\%\) and train methods on them.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在较少数据上训练 对比学习学习对比视图，因此相比于生成学习需要更少的信息，后者需要更完整的表示以进行重建。对比学习有潜力发现数据中存在的内在结构，因此在自监督学习中被广泛使用，因为它具有良好的泛化能力。我们在C-GMVAE中也观察到了这一点。为了证明这一点，我们将训练数据的大小缩小到\({50}\%\)或\({90}\%\)并在其上训练方法。</p></div><!-- Media --><table><tbody><tr><td>module in C-GMVAE</td><td>eBird</td><td>mirflickr</td><td>nus-vec</td><td>yeast</td><td>scene</td><td>sider</td></tr><tr><td>Covariance</td><td>0.601</td><td>0.556</td><td>0.488</td><td>0.650</td><td>0.751</td><td>0.787</td></tr><tr><td>GNN</td><td>0.599</td><td>0.560</td><td>0.491</td><td>0.655</td><td>0.749</td><td>0.801</td></tr><tr><td>contrastive</td><td>0.633</td><td>0.575</td><td>0.510</td><td>0.665</td><td>0.762</td><td>0.803</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>C-GMVAE中的模块</td><td>eBird</td><td>mirflickr</td><td>nus-vec</td><td>酵母</td><td>场景</td><td>侧面</td></tr><tr><td>协方差</td><td>0.601</td><td>0.556</td><td>0.488</td><td>0.650</td><td>0.751</td><td>0.787</td></tr><tr><td>GNN</td><td>0.599</td><td>0.560</td><td>0.491</td><td>0.655</td><td>0.749</td><td>0.801</td></tr><tr><td>对比的</td><td>0.633</td><td>0.575</td><td>0.510</td><td>0.665</td><td>0.762</td><td>0.803</td></tr></tbody></table></div><p>Table 7. mi-F1 performance after replacing our contrastive module with a GNN or a covariance matrix.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表7. 用GNN或协方差矩阵替换我们的对比模块后的mi-F1性能。</p></div><!-- figureText: Black-backed Gull 0.4 - 0.0 Rough-winged Swallow Great Blue Heron Tufted Titmouse Northern Flicker Northern Mockingbird Cedar Waxwing Mourning Dove House Sparrow Common Starling --><img src="https://cdn.noedgeai.com/bo_d15g03ref24c73d1f6k0_8.jpg?x=168&#x26;y=501&#x26;w=682&#x26;h=512&#x26;r=0"><p>Figure 3. Label-label inner-products from C-GMVAE. One can compare it with Fig. 4 from MPVAE in the appendix. C-GMVAE demonstrates sharper and more meaningful inner-products.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3. C-GMVAE的标签-标签内积。可以与附录中的图4（MPVAE）进行比较。C-GMVAE展示了更尖锐和更有意义的内积。</p></div><!-- Media --><p>Surprisingly, we find C-GMVAE can often match the performance of other methods with only \({50}\%\) of the training data. Tab. 6 compares MPVAE trained on all data and C-GMVAE trained on \({50}\%\) of the data. Their performance is approximately the same. We further compare several major state-of-the-art methods including ours, all trained on the same randomly selected \({10}\% ,{50}\%\) and \({100}\%\) of the data, and show their performance over C2AE. Fig. 2 shows the improvements over C2AE on ma-F1. Ours clearly outperforms the others with fewer data. More plots for other datasets and metrics are in the appendix.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>令人惊讶的是，我们发现C-GMVAE通常可以在仅使用\({50}\%\)的训练数据的情况下匹配其他方法的性能。表6比较了在所有数据上训练的MPVAE和在\({50}\%\)数据上训练的C-GMVAE。它们的性能大致相同。我们进一步比较了包括我们的方法在内的几种主要最先进的方法，所有方法均在相同随机选择的\({10}\% ,{50}\%\)和\({100}\%\)数据上训练，并展示了它们在C2AE上的性能。图2显示了在ma-F1上相对于C2AE的改进。我们的模型在使用更少数据的情况下明显优于其他模型。附录中还有其他数据集和指标的更多图表。</p></div><p>Interpretability Our work is also motivated by ecological applications (Gomes et al., 2019), where it is important to understand species interactions. Fig. 3 shows a map of inner-product weights of label embeddings on the eBird dataset. The bird species on the x-axis and the y-axis are the same. The first 3 bird species are water birds. The following 4 bird species are forest birds. The last 3 bird species are residential birds. Darker colors indicate more similar birds. We subtract the diagonal to exclude the self correlation. The heatmap matrix clearly forms three blocks on the diagonal. The first block contains Black-backed Gull, Rough-winged Swallow and Great Blue Heron. These three birds are water birds living near sea or lake. The second block has Tufted Titmouse, Northern Flicker, Northern Mockingbird, and Cedar Waxwing. These birds typically live in the forest with a lot of trees. The remaining birds are commonly seen residential birds, Mourning Dove, House Sparrow and Common Starling. They live inside or near human residences. Since human activities are wide-spread, the distribution of these birds is therefore quite broad. For example, the Mourning Dove is also correlated with forest birds in Fig. 3. But one can observe that for each group of birds, their intra-group correlations are always stronger than inter-group correlations. Therefore, the learnt embeddings do encompass semantic meanings. The derived correlations could also help the study of wildlife protection (Johnston et al., 2019).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>可解释性 我们的工作也受到生态应用的启发（Gomes等，2019），在这些应用中，理解物种间的相互作用非常重要。图3显示了eBird数据集中标签嵌入的内积权重图。x轴和y轴上的鸟类物种是相同的。前3种鸟类是水鸟。接下来的4种鸟类是森林鸟。最后3种鸟类是常见鸟。颜色越深表示鸟类越相似。我们减去对角线以排除自相关。热图矩阵在对角线上明显形成了三个块。第一个块包含黑背鸥、粗翅燕和大蓝鹭。这三种鸟是生活在海洋或湖泊附近的水鸟。第二个块包含冠雀、北方啄木鸟、北方嘲鸫和雪松蜡嘴雀。这些鸟通常生活在树木繁茂的森林中。剩下的鸟是常见的居民鸟，哀鸣鸽、家麻雀和普通椋鸟。它们生活在或靠近人类居住区。由于人类活动广泛，这些鸟的分布也相当广泛。例如，哀鸣鸽在图3中也与森林鸟相关。但可以观察到，对于每组鸟类，它们组内的相关性总是强于组间的相关性。因此，学习到的嵌入确实包含了语义意义。推导出的相关性也可以帮助野生动物保护的研究（Johnston等，2019）。</p></div><h2>5. Conclusion</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>5. 结论</h2></div><p>In this work, we propose the contrastive learning boosted Gaussian mixture variational autoencoder (C-GMVAE), a novel method for multi-label prediction tasks. C-GMVAE combines the learning of Gaussian mixture latent spaces and the contrastive learning of feature and label embed-dings. Not only does C-GMVAE achieve the state-of-the-art performance, it also provides insights into semi-supervised learning and model interpretability. Interesting future directions include the exploration of various contrastive learning mechanisms, model architecture improvements, and other latent space structures.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在这项工作中，我们提出了对比学习增强的高斯混合变分自编码器（C-GMVAE），这是一种用于多标签预测任务的新方法。C-GMVAE结合了高斯混合潜在空间的学习和特征与标签嵌入的对比学习。C-GMVAE不仅实现了最先进的性能，还为半监督学习和模型可解释性提供了见解。有趣的未来方向包括探索各种对比学习机制、模型架构改进和其他潜在空间结构。</p></div><h2>Acknowledgement</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>致谢</h2></div><p>Our work is supported by NSF Expedition CompSustNet CCF-1522054, NSF Computer and Network Systems grant CNS-1059284, and Defense University Research Instrumentation Program ARO DURIP W911NF-17-1-0187. We thank Rich Bernstein for proofreading. References</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的工作得到了NSF Expedition CompSustNet CCF-1522054、NSF计算机和网络系统资助CNS-1059284以及国防大学研究仪器计划ARO DURIP W911NF-17-1-0187的支持。我们感谢Rich Bernstein的校对。参考文献</p></div><p>Annarumma, M. and Montana, G. Deep metric learning for multi-labelled radiographs. arXiv preprint arXiv:1712.07682, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Annarumma, M. 和 Montana, G. 多标签放射线图的深度度量学习。arXiv预印本arXiv:1712.07682, 2017。</p></div><p>Bai, J., Kong, S., and Gomes, C. Disentangled variational autoencoder based multi-label classification with covariance-aware multivariate probit model. IJCAI, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Bai, J., Kong, S. 和 Gomes, C. 基于协方差感知多元Probit模型的解耦变分自编码器多标签分类。IJCAI, 2020。</p></div><p>Belanger, D. and McCallum, A. Structured prediction energy networks. In International Conference on Machine Learning, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Belanger, D. 和 McCallum, A. 结构化预测能量网络。在国际机器学习会议，2016。</p></div><p>Benites, F. and Sapozhnikova, E. Haram: a hierarchical aram neural network for large-scale text classification. In 2015 IEEE international conference on data mining workshop (ICDMW). IEEE, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Benites, F. 和 Sapozhnikova, E. Haram：一种用于大规模文本分类的层次化aram神经网络。在2015年IEEE国际数据挖掘会议研讨会（ICDMW）。IEEE, 2015。</p></div><p>Bhatia, K., Jain, H., Kar, P., Varma, M., and Jain, P. Sparse local embeddings for extreme multi-label classification. Advances in neural information processing systems, 28: 730-738, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Bhatia, K., Jain, H., Kar, P., Varma, M. 和 Jain, P. 极端多标签分类的稀疏局部嵌入。神经信息处理系统进展，28: 730-738, 2015。</p></div><p>Bi, W. and Kwok, J. Multilabel classification with label correlations and missing labels. In Proceedings of the AAAI Conference on Artificial Intelligence, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Bi, W. 和 Kwok, J. 带有标签相关性和缺失标签的多标签分类。在人工智能AAA会议论文集，2014。</p></div><p>Boutell, M. R., Luo, J., Shen, X., and Brown, C. M. Learning multi-label scene classification. Pattern recognition, 37(9):1757-1771, 2004.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Boutell, M. R., Luo, J., Shen, X., 和 Brown, C. M. 学习多标签场景分类。模式识别，37(9):1757-1771, 2004。</p></div><p>Chang, W.-C., Yu, H.-F., Zhong, K., Yang, Y., and Dhillon, I. X-bert: extreme multi-label text classification with using bidirectional encoder representations from transformers. arXiv preprint arXiv:1905.02331, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Chang, W.-C., Yu, H.-F., Zhong, K., Yang, Y., 和 Dhillon, I. X-bert：使用变换器的双向编码器表示进行极端多标签文本分类。arXiv 预印本 arXiv:1905.02331, 2019。</p></div><p>Chen, C., Wang, H., Liu, W., Zhao, X., Hu, T., and Chen, G. Two-stage label embedding via neural factorization machine for multi-label classification. In \({AAAI},{2019}\mathrm{a}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Chen, C., Wang, H., Liu, W., Zhao, X., Hu, T., 和 Chen, G. 通过神经因子分解机进行多标签分类的两阶段标签嵌入。在\({AAAI},{2019}\mathrm{a}\)。</p></div><p>Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Chen, T., Kornblith, S., Norouzi, M., 和 Hinton, G. 一种简单的视觉表示对比学习框架。arXiv 预印本 arXiv:2002.05709, 2020。</p></div><p>Chen, Z.-M., Wei, X.-S., Wang, P., and Guo, Y. Multi-label image recognition with graph convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5177-5186, 2019b.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Chen, Z.-M., Wei, X.-S., Wang, P., 和 Guo, Y. 基于图卷积网络的多标签图像识别。在IEEE/CVF计算机视觉与模式识别会议论文集中，pp. 5177-5186, 2019b。</p></div><p>Chua, T.-S., Tang, J., Hong, R., Li, H., Luo, Z., and Zheng, Y. Nus-wide: a real-world web image database from national university of singapore. In Proceedings of the ACM international conference on image and video retrieval, pp. 1-9, 2009.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Chua, T.-S., Tang, J., Hong, R., Li, H., Luo, Z., 和 Zheng, Y. Nus-wide：来自新加坡国立大学的真实世界网络图像数据库。在ACM国际图像和视频检索会议论文集中，pp. 1-9, 2009。</p></div><p>Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A. C., and Bengio, Y. A recurrent latent variable model for sequential data. Advances in neural information processing systems, 28:2980-2988, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A. C., 和 Bengio, Y. 一种用于序列数据的递归潜变量模型。神经信息处理系统进展，28:2980-2988, 2015。</p></div><p>Dao, S. D., Ethan, Z., Dinh, P., and Jianfei, C. Contrast learning visual attention for multi label classification. arXiv preprint arXiv:2107.11626, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Dao, S. D., Ethan, Z., Dinh, P., 和 Jianfei, C. 对比学习视觉注意力用于多标签分类。arXiv 预印本 arXiv:2107.11626, 2021。</p></div><p>Debole, F. and Sebastiani, F. An analysis of the relative hardness of reuters-21578 subsets. Journal of the American Society for Information Science and technology, 56 (6):584-596, 2005.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Debole, F. 和 Sebastiani, F. 对路透社21578子集相对难度的分析。美国信息科学与技术学会期刊，56 (6):584-596, 2005。</p></div><p>Decubber, S., Mortier, T., Dembczyński, K., and Waege-man, W. Deep f-measure maximization in multi-label classification: A comparative study. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 290-305. Springer, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Decubber, S., Mortier, T., Dembczyński, K., 和 Waege-man, W. 在多标签分类中深度F-measure最大化：一项比较研究。在欧洲机器学习与数据库知识发现联合会议论文集中，pp. 290-305。施普林格，2018。</p></div><p>Dilokthanakul, N., Mediano, P. A., Garnelo, M., Lee, M. C., Salimbeni, H., Arulkumaran, K., and Shanahan, M. Deep unsupervised clustering with gaussian mixture variational autoencoders. arXiv preprint arXiv:1611.02648, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Dilokthanakul, N., Mediano, P. A., Garnelo, M., Lee, M. C., Salimbeni, H., Arulkumaran, K., 和 Shanahan, M. 基于高斯混合变分自编码器的深度无监督聚类。arXiv 预印本 arXiv:1611.02648, 2016。</p></div><p>Doersch, C. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Doersch, C. 变分自编码器教程。arXiv 预印本 arXiv:1606.05908, 2016。</p></div><p>Eslami, S., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Kavukcuoglu, K., and Hinton, G. E. Attend, infer, repeat: Fast scene understanding with generative models. arXiv preprint arXiv:1603.08575, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Eslami, S., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Kavukcuoglu, K., 和 Hinton, G. E. 注意、推理、重复：使用生成模型快速理解场景。arXiv 预印本 arXiv:1603.08575, 2016。</p></div><p>Fink, D., Auer, T., Obregon, F., Hochachka, W., Iliff, M., Sullivan, B., Wood, C., Davies, I., and Kelling, S. The ebird reference dataset version 2016 (erd2016), 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Fink, D., Auer, T., Obregon, F., Hochachka, W., Iliff, M., Sullivan, B., Wood, C., Davies, I., 和 Kelling, S. ebird参考数据集版本2016 (erd2016), 2017。</p></div><p>Gerych, W., Hartvigsen, T., Buquicchio, L., Agu, E., and Rundensteiner, E. A. Recurrent bayesian classifier chains for exact multi-label classification. Advances in Neural Information Processing Systems, 34:15981-15992, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Gerych, W., Hartvigsen, T., Buquicchio, L., Agu, E., 和 Rundensteiner, E. A. 用于精确多标签分类的递归贝叶斯分类器链。神经信息处理系统进展，34:15981-15992, 2021。</p></div><p>Gomes, C., Dietterich, T., et al. Computational sustainabil-ity: Computing for a better world and a sustainable future. Communications of the ACM, 62(9):56-65, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Gomes, C., Dietterich, T., 等。计算可持续性：为更美好的世界和可持续的未来而计算。ACM通讯，62(9):56-65, 2019。</p></div><p>Gutmann, M. and Hyvärinen, A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTATS, 2010.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Gutmann, M. 和 Hyvärinen, A. 噪声对比估计：一种用于未归一化统计模型的新估计原则。在 AISTATS，2010。</p></div><p>Huiskes, M. J. and Lew, M. S. The mir flickr retrieval evaluation. In Proceedings of the 1st ACM international conference on Multimedia information retrieval, pp. 39- 43, 2008.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Huiskes, M. J. 和 Lew, M. S. mir flickr 检索评估。在第一届 ACM 国际多媒体信息检索会议论文集中，第 39-43 页，2008。</p></div><p>Johnston, A., Hochachka, W., Strimas-Mackey, M., Gutierrez, V. R., Robinson, O., Miller, E., Auer, T., Kelling, S., and Fink, D. Best practices for making reliable inferences from citizen science data: case study using ebird to estimate species distributions. BioRxiv, pp. 574392, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Johnston, A., Hochachka, W., Strimas-Mackey, M., Gutierrez, V. R., Robinson, O., Miller, E., Auer, T., Kelling, S., 和 Fink, D. 从公民科学数据中进行可靠推断的最佳实践：使用 ebird 估计物种分布的案例研究。BioRxiv，第 574392 页，2019。</p></div><p>Katakis, I., Tsoumakas, G., and Vlahavas, I. Multilabel text classification for automated tag suggestion. ECML PKDD Discovery Challenge, pp. 75, 2008.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Katakis, I., Tsoumakas, G., 和 Vlahavas, I. 自动标签建议的多标签文本分类。ECML PKDD 发现挑战，第 75 页，2008。</p></div><p>Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., and Krishnan, D. Supervised contrastive learning. arXiv preprint arXiv:2004.11362, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., 和 Krishnan, D. 监督对比学习。arXiv 预印本 arXiv:2004.11362，2021。</p></div><p>Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Kingma, D. P. 和 Ba, J. Adam：一种随机优化方法。arXiv:1412.6980，2014。</p></div><p>Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv:1312.6114, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Kingma, D. P. 和 Welling, M. 自编码变分贝叶斯。arXiv:1312.6114，2013。</p></div><p>Koyejo, O., Natarajan, N., Ravikumar, P., and Dhillon, I. S. Consistent multilabel classification. In NIPS, volume 29, pp. 3321-3329, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Koyejo, O., Natarajan, N., Ravikumar, P., 和 Dhillon, I. S. 一致的多标签分类。在 NIPS，第 29 卷，第 3321-3329 页，2015。</p></div><p>Kuhn, M., Letunic, I., Jensen, L. J., and Bork, P. The sider database of drugs and side effects. Nucleic acids research, 44(D1):D1075-D1079, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Kuhn, M., Letunic, I., Jensen, L. J., 和 Bork, P. SIDER 药物和副作用数据库。核酸研究，44(D1)：D1075-D1079，2016。</p></div><p>Lanchantin, J., Sekhon, A., and Qi, Y. Neural message passing for multi-label classification. arXiv preprint arXiv:1904.08049, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Lanchantin, J., Sekhon, A., 和 Qi, Y. 用于多标签分类的神经消息传递。arXiv 预印本 arXiv:1904.08049，2019。</p></div><p>Malkiński, M. and Mańdziuk, J. Multi-label contrastive learning for abstract visual reasoning. arXiv preprint arXiv:2012.01944, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Malkiński, M. 和 Mańdziuk, J. 用于抽象视觉推理的多标签对比学习。arXiv 预印本 arXiv:2012.01944，2020。</p></div><p>Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Mikolov, T., Chen, K., Corrado, G., 和 Dean, J. 在向量空间中有效估计词表示。arXiv 预印本 arXiv:1301.3781，2013。</p></div><p>Nakai, K. and Kanehisa, M. A knowledge base for predicting protein localization sites in eukaryotic cells. Ge-nomics, 14(4):897-911, 1992.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Nakai, K. 和 Kanehisa, M. 用于预测真核细胞中蛋白质定位位点的知识库。基因组学，14(4)：897-911，1992。</p></div><p>Oord, A. v. d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Oord, A. v. d., Li, Y., 和 Vinyals, O. 通过对比预测编码进行表示学习。arXiv 预印本 arXiv:1807.03748，2018。</p></div><p>Read, J., Pfahringer, B., Holmes, G., and Frank, E. Classifier chains for multi-label classification. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2009.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Read, J., Pfahringer, B., Holmes, G., 和 Frank, E. 多标签分类的分类器链。在机器学习与数据库知识发现的联合欧洲会议。施普林格，2009。</p></div><p>Ridnik, T., Ben-Baruch, E., Zamir, N., Noy, A., Friedman, I., Protter, M., and Zelnik-Manor, L. Asymmetric loss for multi-label classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 82-91, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Ridnik, T., Ben-Baruch, E., Zamir, N., Noy, A., Friedman, I., Protter, M., 和 Zelnik-Manor, L. 多标签分类的非对称损失。在 IEEE/CVF 国际计算机视觉会议论文集中，第 82-91 页，2021。</p></div><p>Schönfeld, E., Ebrahimi, S., Sinha, S., Darrell, T., and Akata, Z. Generalized zero-and few-shot learning via aligned variational autoencoders. In CVPR. IEEE, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Schönfeld, E., Ebrahimi, S., Sinha, S., Darrell, T., 和 Akata, Z. 通过对齐变分自编码器实现广义零样本和少样本学习。在 CVPR。IEEE，2019。</p></div><p>Seymour, Z. and Zhang, Z. Multi-label triplet embeddings for image annotation from user-generated tags. In Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, pp. 249-256, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Seymour, Z. 和 Zhang, Z. 基于用户生成标签的图像注释的多标签三元组嵌入。在2018年ACM国际多媒体检索会议论文集中，第249-256页，2018。</p></div><p>Shi, Y., Siddharth, N., Paige, B., and Torr, P. H. Variational mixture-of-experts autoencoders for multi-modal deep generative models. arXiv preprint arXiv:1911.03393. 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Shi, Y., Siddharth, N., Paige, B., 和 Torr, P. H. 用于多模态深度生成模型的变分专家混合自编码器。arXiv 预印本 arXiv:1911.03393。2019。</p></div><p>Shi, Y., Paige, B., Torr, P. H., and Siddharth, N. Relating by contrasting: A data-efficient framework for multimodal generative models. arXiv preprint arXiv:2007.01179, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Shi, Y., Paige, B., Torr, P. H., 和 Siddharth, N. 通过对比建立关系：一种数据高效的多模态生成模型框架。arXiv 预印本 arXiv:2007.01179，2020。</p></div><p>Shu, R. Gaussian mixture vae: Lessons in variational inference, generative models, and deep nets. 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Shu, R. 高斯混合变分自编码器：变分推断、生成模型和深度网络的经验教训。2016。</p></div><p>Song, J. and Ermon, S. Multi-label contrastive predictive coding. Advances in Neural Information Processing Systems, 33, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Song, J. 和 Ermon, S. 多标签对比预测编码。神经信息处理系统进展，33，2020。</p></div><p>Sundar, V. K., Ramakrishna, S., Rahiminasab, Z., Easwaran, A., and Dubey, A. Out-of-distribution detection in multi-label datasets using latent space of \(\beta\) -vae. arXiv preprint arXiv:2003.08740, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Sundar, V. K., Ramakrishna, S., Rahiminasab, Z., Easwaran, A., 和 Dubey, A. 使用\(\beta\)-变分自编码器的潜在空间进行多标签数据集的分布外检测。arXiv 预印本 arXiv:2003.08740，2020。</p></div><p>Tomczak, J. and Welling, M. Vae with a vampprior. In International Conference on Artificial Intelligence and Statistics, pp. 1214-1223. PMLR, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Tomczak, J. 和 Welling, M. 带有vampprior的变分自编码器。在人工智能与统计国际会议上，第1214-1223页。PMLR，2018。</p></div><p>Tsoumakas, G., Katakis, I., and Vlahavas, I. Effective and efficient multilabel classification in domains with large number of labels. In Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data (MMD'08), volume 21, pp. 53-59, 2008.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Tsoumakas, G., Katakis, I., 和 Vlahavas, I. 在标签数量较大的领域中有效且高效的多标签分类。在2008年ECML/PKDD多维数据挖掘研讨会（MMD'08）论文集中，第21卷，第53-59页，2008。</p></div><p>Tu, L. and Gimpel, K. Learning approximate inference networks for structured prediction. arXiv preprint arXiv:1803.03376, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Tu, L. 和 Gimpel, K. 学习结构化预测的近似推断网络。arXiv 预印本 arXiv:1803.03376，2018。</p></div><p>Wang, F., Liu, H., Guo, D., and Sun, F. Unsupervised representation learning by invariancepropagation. arXiv preprint arXiv:2010.11694, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Wang, F., Liu, H., Guo, D., 和 Sun, F. 通过不变传播进行无监督表示学习。arXiv 预印本 arXiv:2010.11694，2021。</p></div><p>Wang, J., Song, Y., Leung, T., Rosenberg, C., Wang, J., Philbin, J., Chen, B., and Wu, Y. Learning fine-grained image similarity with deep ranking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1386-1393, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Wang, J., Song, Y., Leung, T., Rosenberg, C., Wang, J., Philbin, J., Chen, B., 和 Wu, Y. 通过深度排序学习细粒度图像相似性。在IEEE计算机视觉与模式识别会议论文集中，第1386-1393页，2014。</p></div><p>Wang, J., Yang, Y., Mao, J., Huang, Z., Huang, C., and Xu, W. Cnn-rnn: A unified framework for multi-label image classification. In \({CVPR},{2016}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Wang, J., Yang, Y., Mao, J., Huang, Z., Huang, C., 和 Xu, W. CNN-RNN：用于多标签图像分类的统一框架。在\({CVPR},{2016}\)。</p></div><p>Wang, P. Z. and Wang, W. Y. Neural gaussian copula for variational autoencoder. arXiv preprint arXiv:1909.03569, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Wang, P. Z. 和 Wang, W. Y. 用于变分自编码器的神经高斯耦合。arXiv 预印本 arXiv:1909.03569，2019。</p></div><p>Wu, M. and Goodman, N. Multimodal generative models for scalable weakly-supervised learning. In Advances in Neural Information Processing Systems, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Wu, M. 和 Goodman, N. 用于可扩展弱监督学习的多模态生成模型。在神经信息处理系统进展，2018。</p></div><p>Yeh, C.-K., Wu, W.-C., Ko, W.-J., and Wang, Y.-C. F. Learning deep latent space for multi-label classification. In \({AAAI},{2017}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Yeh, C.-K., Wu, W.-C., Ko, W.-J., 和 Wang, Y.-C. F. 学习用于多标签分类的深层潜在空间。在\({AAAI},{2017}\)。</p></div><p>Yu, G., Rangwala, H., Domeniconi, C., Zhang, G., and Yu, Z. Protein function prediction using multilabel ensemble classification. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Yu, G., Rangwala, H., Domeniconi, C., Zhang, G., 和 Yu, Z. 使用多标签集成分类的蛋白质功能预测。IEEE/ACM计算生物学与生物信息学交易，2013。</p></div><p>Zhang, M.-L. and Zhou, Z.-H. Ml-knn: A lazy learning approach to multi-label learning. Pattern recognition, 40 (7):2038-2048, 2007.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>张明亮和周志华。Ml-knn：一种懒惰学习的多标签学习方法。模式识别，40 (7)：2038-2048，2007。</p></div><p>Zhang, M.-L. and Zhou, Z.-H. A review on multi-label learning algorithms. IEEE transactions on knowledge and data engineering, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>张明亮和周志华。关于多标签学习算法的综述。IEEE知识与数据工程交易，2013。</p></div><p>Zhang, M.-L., Li, Y.-K., Liu, X.-Y., and Geng, X. Binary relevance for multi-label learning: an overview. Frontiers of Computer Science, 12(2):191-202, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>张明亮，李玉凯，刘晓阳，和耿鑫。多标签学习的二元相关性：概述。计算机科学前沿，12(2)：191-202，2018。</p></div><h2>A. Contrastive Learning Module</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>A. 对比学习模块</h2></div><h3>A.1. Connection with Triplet Loss</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>A.1. 与三元组损失的联系</h3></div><p>Triplet loss (Wang et al., 2014) is one of the popular ranking losses used in multi-label learning (Seymour &#x26; Zhang, 2018).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>三元组损失（Wang et al., 2014）是多标签学习中常用的排名损失之一（Seymour &#x26; Zhang, 2018）。</p></div><p>Given an anchor embedding \({v}_{x}^{f}\) ,a positive embedding \({v}_{ + }\) and a negative embedding \({v}_{ - }\) ,they form a triplet \(\left( {{v}_{x}^{f},{v}_{ + },{v}_{ - }}\right)\) . A triplet loss is defined as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>给定一个锚点嵌入 \({v}_{x}^{f}\)，一个正嵌入 \({v}_{ + }\) 和一个负嵌入 \({v}_{ - }\)，它们形成一个三元组 \(\left( {{v}_{x}^{f},{v}_{ + },{v}_{ - }}\right)\)。三元组损失定义为</p></div><p></p>\[{\mathcal{L}}_{\text{trip }}\left( {{v}_{x}^{f},{v}_{ + },{v}_{ - }}\right)  \tag{9}\]<p></p>
<p></p>\[= \max \left\{  {0,g + \operatorname{dist}\left( {{v}_{x}^{f},{v}_{ + }}\right)  - \operatorname{dist}\left( {{v}_{x}^{f},{v}_{ - }}\right) }\right\}\]<p></p><p>where \(g\) is a gap parameter measuring the distance between \(\left( {{v}_{x}^{f},{v}_{ + }}\right)\) and \(\left( {{v}_{x}^{f},{v}_{ - }}\right)\) ,and \(\operatorname{dist}\left( {\cdot , \cdot  }\right)\) is a distance function. This hinge loss \({\mathcal{L}}_{\text{trip }}\) encourages fewer violations to "positive>negative" ranking order. Let \(\tau  = 1/2\) . With the same triplet, we can write down a contrastive loss</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(g\) 是一个间隔参数，用于测量 \(\left( {{v}_{x}^{f},{v}_{ + }}\right)\) 和 \(\left( {{v}_{x}^{f},{v}_{ - }}\right)\) 之间的距离，\(\operatorname{dist}\left( {\cdot , \cdot  }\right)\) 是一个距离函数。这个铰链损失 \({\mathcal{L}}_{\text{trip }}\) 鼓励“正>负”排名顺序的违规情况更少。设 \(\tau  = 1/2\)。使用相同的三元组，我们可以写出一个对比损失</p></div><p></p>\[{\mathcal{L}}_{CL}\left( {{v}_{x}^{f},{v}_{ + },{v}_{ - }}\right)\]<p></p><p></p>\[=  - \log \frac{\exp \left( {2 \cdot  {v}_{x}^{f} \cdot  {v}_{ + }}\right) }{\mathop{\sum }\limits_{{t \in  \{ +, - \} }}\exp \left( {2 \cdot  {v}_{x}^{f} \cdot  {v}_{t}}\right) }\]<p></p><p></p>\[= \log \left( {1 + \frac{\exp \left( {2 \cdot  {v}_{x}^{f} \cdot  {v}_{ - }}\right) }{\exp \left( {2 \cdot  {v}_{x}^{f} \cdot  {v}_{ + }}\right) }}\right)  \tag{10}\]<p></p><p></p>\[\approx  1 + \left( {2 \cdot  {v}_{x}^{f} \cdot  {v}_{ - } - 2 \cdot  {v}_{x}^{f} \cdot  {v}_{ + }}\right)\]<p></p><p></p>\[= 1 + \left( {-{v}_{x}^{f} \cdot  {v}_{x}^{f} + 2{v}_{x}^{f} \cdot  {v}_{ - } - {v}_{ - } \cdot  {v}_{ - }}\right.\]<p></p><p></p>\[\left. {+{v}_{x}^{f} \cdot  {v}_{x}^{f} - 2 \cdot  {v}_{x}^{f} \cdot  {v}_{ + } + {v}_{ + } \cdot  {v}_{ + }}\right)\]<p></p><p></p>\[= {\begin{Vmatrix}{v}_{x}^{f} - {v}_{ + }\end{Vmatrix}}^{2} + {\begin{Vmatrix}{v}_{x}^{f} - {v}_{ - }\end{Vmatrix}}^{2} + 1\]<p></p><p>Note that in the second to the last equation, \({v}_{ + }\) and \({v}_{ - }\) have the same norm due to the normalization in our contrastive learning module.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>注意，在倒数第二个方程中，由于我们对比学习模块中的归一化，\({v}_{ + }\) 和 \({v}_{ - }\) 具有相同的范数。</p></div><p>By setting \(\operatorname{dist}\left( {\cdot , \cdot  }\right)\) to commonly used \({\ell }_{2}\) distance and \(g =\) 1, Eq. 10 is a fair approximation of Eq. 9. Therefore, triplet loss can be viewed as a special case of contrastive loss. In contrastive loss, embeddings are normalized and more positives/negatives are available. As shown in (Chen et al., 2020), contrastive loss generally outperforms triplet loss.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>通过将 \(\operatorname{dist}\left( {\cdot , \cdot  }\right)\) 设置为常用的 \({\ell }_{2}\) 距离和 \(g =\) 1，方程10是方程9的一个合理近似。因此，三元组损失可以视为对比损失的一个特例。在对比损失中，嵌入被归一化，并且可用的正/负样本更多。如（Chen et al., 2020）所示，对比损失通常优于三元组损失。</p></div><h3>A.2. Gradients of Contrastive Loss</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>A.2. 对比损失的梯度</h3></div><p>Recall our contrastive loss:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>回顾我们的对比损失：</p></div><p></p>\[{\mathcal{L}}_{CL} = \mathop{\sum }\limits_{{\left( {x,y}\right)  \in  \mathcal{B}}}\frac{1}{\left| P\left( y\right) \right| }\mathop{\sum }\limits_{{p \in  P\left( y\right) }} - \log \frac{\exp \left( {{v}_{x}^{f} \cdot  {v}_{p}^{l}/\tau }\right) }{\mathop{\sum }\limits_{{t \in  A}}\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) }\]<p></p><p>(11)</p><p>For the illustration purpose, we only consider one sample (x,y)instead of one batch:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了说明，我们只考虑一个样本 (x,y) 而不是一个批次：</p></div><p></p>\[{\mathcal{L}}_{CL} = \frac{1}{\left| P\left( y\right) \right| }\mathop{\sum }\limits_{{p \in  P\left( y\right) }} - \log \frac{\exp \left( {{v}_{x}^{f} \cdot  {v}_{p}^{l}/\tau }\right) }{\mathop{\sum }\limits_{{t \in  A}}\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) } \tag{12}\]<p></p><p>Define \(N\left( y\right)  \equiv  A \smallsetminus  P\left( y\right)\) . We now derive the gradients w.r.t. \({v}_{x}^{f}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>定义 \(N\left( y\right)  \equiv  A \smallsetminus  P\left( y\right)\)。我们现在推导相对于 \({v}_{x}^{f}\) 的梯度。</p></div><p></p>\[\frac{\partial {\mathcal{L}}_{CL}}{\partial {v}_{x}^{f}} = \frac{1}{\tau \left| {P\left( y\right) }\right| }\mathop{\sum }\limits_{{p \in  P\left( y\right) }}\left( {\frac{\mathop{\sum }\limits_{{t \in  A}}{v}_{t}^{l}\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) }{\mathop{\sum }\limits_{{t \in  A}}\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) } - {v}_{p}^{l}}\right)\]<p></p><p></p>\[= \frac{1}{\tau \left| {P\left( y\right) }\right| }\mathop{\sum }\limits_{{p \in  P\left( y\right) }}\left( {\frac{\mathop{\sum }\limits_{{t \in  P\left( y\right) }}{v}_{t}^{l}\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) }{\mathop{\sum }\limits_{{t \in  A}}\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) } + }\right.\]<p></p><p></p>\[\frac{\mathop{\sum }\limits_{{t \in  N\left( y\right) }}{v}_{t}^{l}\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) }{\mathop{\sum }\limits_{{t \in  A}}\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) } - {v}_{p}^{l})\]<p></p><p></p>\[= \frac{1}{\tau }\frac{\mathop{\sum }\limits_{{t \in  P\left( y\right) }}{v}_{t}^{l}\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) }{\mathop{\sum }\limits_{{t \in  A}}\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) } +\]<p></p><p></p>\[\frac{1}{\tau }\frac{\mathop{\sum }\limits_{{t \in  N\left( y\right) }}{v}_{t}^{l}\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) }{\mathop{\sum }\limits_{{t \in  A}}\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) } -\]<p></p><p></p>\[\frac{1}{\tau \left| {P\left( y\right) }\right| }\mathop{\sum }\limits_{{p \in  P\left( y\right) }}{v}_{p}^{l}\]<p></p><p></p>\[= \frac{1}{\tau }\left\lbrack  {\mathop{\sum }\limits_{{t \in  P\left( y\right) }}{v}_{t}^{l}\left( {\frac{\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) }{\mathop{\sum }\limits_{{a \in  A}}\exp \left( {{v}_{x}^{f} \cdot  {v}_{a}^{l}/\tau }\right) } - \frac{1}{\left| P\left( y\right) \right| }}\right)  + }\right.\]<p></p><p></p>\[\left. {\mathop{\sum }\limits_{{t \in  N\left( y\right) }}{v}_{t}^{l}\frac{\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) }{\mathop{\sum }\limits_{{a \in  A}}\exp \left( {{v}_{x}^{f} \cdot  {v}_{a}^{l}/\tau }\right) }}\right\rbrack\]<p></p><p>(13)</p><p>Further,we have the unnormalized feature embedding \({w}_{x}^{f}\) , \({v}_{x}^{f} = \frac{{w}_{x}^{f}}{\begin{Vmatrix}{w}_{x}^{f}\end{Vmatrix}}.\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>此外，我们有未归一化的特征嵌入 \({w}_{x}^{f}\)， \({v}_{x}^{f} = \frac{{w}_{x}^{f}}{\begin{Vmatrix}{w}_{x}^{f}\end{Vmatrix}}.\)</p></div><p></p>\[\frac{\partial {v}_{x}^{f}}{\partial {w}_{x}^{f}} = \frac{1}{\begin{Vmatrix}{w}_{x}^{f}\end{Vmatrix}}\left( {I - \frac{{w}_{x}^{f}{w}_{x}^{{f}^{T}}}{{\begin{Vmatrix}{w}_{x}^{f}\end{Vmatrix}}^{2}}}\right)  = \frac{1}{\begin{Vmatrix}{w}_{x}^{f}\end{Vmatrix}}\left( {I - {v}_{x}^{f}{v}_{x}^{{f}^{T}}}\right)\]<p></p><p>(14)</p><p>where \(I\) is an \(E \times  E\) identity matrix. The gradient of \({\mathcal{L}}_{CL}\) w.r.t. \({w}_{x}^{f}\) can be derived with chain rule,</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(I\) 是一个 \(E \times  E\) 单位矩阵。关于 \({\mathcal{L}}_{CL}\) 相对于 \({w}_{x}^{f}\) 的梯度可以通过链式法则推导得出，</p></div><p></p>\[\frac{\partial {\mathcal{L}}_{CL}}{\partial {w}_{x}^{f}} = \frac{\partial {v}_{x}^{f}}{\partial {w}_{x}^{f}}\frac{\partial {\mathcal{L}}_{CL}}{\partial {v}_{x}^{f}}\]<p></p><p></p>\[= \frac{1}{\begin{Vmatrix}{w}_{x}^{f}\end{Vmatrix}}\left( {I - {v}_{x}^{f}{v}_{x}^{fT}}\right) \frac{1}{\tau }\left\lbrack  {\mathop{\sum }\limits_{{t \in  P\left( y\right) }}{v}_{t}^{l}\left( \frac{\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) }{\mathop{\sum }\limits_{{a \in  A}}\exp \left( {{v}_{x}^{f} \cdot  {v}_{a}^{l}/\tau }\right) }\right. }\right.\]<p></p><p></p>\[\left. {-\frac{1}{\left| P\left( y\right) \right| }}\right)  + \mathop{\sum }\limits_{{t \in  N\left( y\right) }}{v}_{t}^{l}\frac{\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) }{\mathop{\sum }\limits_{{a \in  A}}\exp \left( {{v}_{x}^{f} \cdot  {v}_{a}^{l}/\tau }\right) }\rbrack\]<p></p><p></p>\[= \frac{1}{\tau \begin{Vmatrix}{w}_{x}^{f}\end{Vmatrix}}\left\lbrack  {\mathop{\sum }\limits_{{t \in  P\left( y\right) }}\left( {{v}_{t}^{l} - \left( {{v}_{x}^{f}{v}_{t}^{l}}\right) {v}_{x}^{f}}\right) \left( \frac{\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) }{\mathop{\sum }\limits_{{a \in  A}}\exp \left( {{v}_{x}^{f} \cdot  {v}_{a}^{l}/\tau }\right) }\right. }\right.\]<p></p><p></p>\[\left. {-\frac{1}{\left| P\left( y\right) \right| }}\right)  +\]<p></p><p></p>\[\left. {\mathop{\sum }\limits_{{t \in  N\left( y\right) }}\left( {{v}_{t}^{l} - \left( {{v}_{x}^{f}{v}_{t}^{l}}\right) {v}_{x}^{f}}\right) \frac{\exp \left( {{v}_{x}^{f} \cdot  {v}_{t}^{l}/\tau }\right) }{\mathop{\sum }\limits_{{a \in  A}}\exp \left( {{v}_{x}^{f} \cdot  {v}_{a}^{l}/\tau }\right) }}\right\rbrack\]<p></p><p>(15)</p><!-- Media --><!-- figureText: Black-backed Gull Baltimore Oriole Great Blue Heron Northern Flicker Cedar Waxwing American Crow Northern Mockingbird Common Starling Brown-headed Cowbird Mourning Dove --><img src="https://cdn.noedgeai.com/bo_d15g03ref24c73d1f6k0_13.jpg?x=168&#x26;y=197&#x26;w=676&#x26;h=507&#x26;r=0"><p>Figure 4. Label-label inner-products by MPVAE.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图4. MPVAE的标签-标签内积。</p></div><!-- Media --><p>We can then observe that if \({v}_{x}^{f}\) and \({v}_{t}^{l}\) are orthogonal \(\left( {{v}_{x}^{f}{v}_{t}^{l} \rightarrow  0}\right) ,\begin{Vmatrix}{{v}_{t}^{l} - \left( {{v}_{x}^{f}{v}_{t}^{l}}\right) {v}_{x}^{f}}\end{Vmatrix}\) will be close to 1 and the gradients would be large. Otherwise, for weak positives or negatives \(\left( {\left| {{v}_{x}^{f}{v}_{t}^{l}}\right|  \rightarrow  1}\right)\) ,the gradients would be small.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们可以观察到，如果 \({v}_{x}^{f}\) 和 \({v}_{t}^{l}\) 是正交的，则 \(\left( {{v}_{x}^{f}{v}_{t}^{l} \rightarrow  0}\right) ,\begin{Vmatrix}{{v}_{t}^{l} - \left( {{v}_{x}^{f}{v}_{t}^{l}}\right) {v}_{x}^{f}}\end{Vmatrix}\) 将接近1，梯度会很大。否则，对于弱正或负的 \(\left( {\left| {{v}_{x}^{f}{v}_{t}^{l}}\right|  \rightarrow  1}\right)\)，梯度会很小。</p></div><h2>B. Supplementary Experimental Results</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>B. 补充实验结果</h2></div><h3>B.1. Implementation Details</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>B.1. 实现细节</h3></div><p>We use one Tesla V100 GPU on CentOS for every experiment. The batch size is set to 128 . The latent dimensionality is 64. The feature encoder is an MLP with 3 hidden layers of sizes [256, 512, 256]. The label encoder has 2 hidden layers of sizes \(\left\lbrack  {{512},{256}}\right\rbrack\) . The decoder contains 2 hidden layers of sizes [512, 512]. On reuters and bookmarks, we add one more hidden layer with 512 units to the decoder. The embedding size \(E\) is 2048 (tuned within the range [512, \({1024},{2048},{3072}\rbrack )\) . We set \(\alpha  = 1\) (tuned within \(\lbrack {0.1},{0.5}\) , \(1,{1.5},2\rbrack ),\beta  = {0.5}\) (tuned within \(\left\lbrack  {{0.1},{0.5},1,{1.5},{2.0}}\right\rbrack\) ) for most runs. We tune learning rates from 0.0001 to 0.004 with interval 0.0002,dropout ratio from \(\left\lbrack  {{0.3},{0.5},{0.7}}\right\rbrack\) ,and weight decay from \(\left\lbrack  {0,{0.01},{0.0001}}\right\rbrack\) . Grid search is adopted for tuning. Every batch in our experiments requires less than 16GB memory. The number of epochs is 100 by default.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们在CentOS上为每个实验使用一台Tesla V100 GPU。批量大小设置为128。潜在维度为64。特征编码器是一个具有3个隐藏层的多层感知器，层大小为[256, 512, 256]。标签编码器有2个隐藏层，层大小为 \(\left\lbrack  {{512},{256}}\right\rbrack\)。解码器包含2个隐藏层，层大小为[512, 512]。在reuters和书签数据集上，我们在解码器中添加了一个具有512个单元的隐藏层。嵌入大小 \(E\) 为2048（在范围[512, \({1024},{2048},{3072}\rbrack )\)内调整）。我们设置 \(\alpha  = 1\)（在 \(\lbrack {0.1},{0.5}\) 内调整），\(1,{1.5},2\rbrack ),\beta  = {0.5}\)（在 \(\left\lbrack  {{0.1},{0.5},1,{1.5},{2.0}}\right\rbrack\) 内调整）用于大多数运行。我们将学习率从0.0001调整到0.004，间隔为0.0002，dropout比例从 \(\left\lbrack  {{0.3},{0.5},{0.7}}\right\rbrack\)，权重衰减从 \(\left\lbrack  {0,{0.01},{0.0001}}\right\rbrack\)。采用网格搜索进行调优。我们实验中的每个批次需要少于16GB的内存。默认的训练轮数为100。</p></div><!-- Media --><table><tbody><tr><td rowspan="2">model</td><td colspan="2">eBird (s)</td><td colspan="2">mirflickr (s)</td></tr><tr><td>Train (per epoch)</td><td>Test (total)</td><td>Train (per epoch)</td><td>Test (total)</td></tr><tr><td>C2AE</td><td>27</td><td>29</td><td>18</td><td>24</td></tr><tr><td>LaMP</td><td>32</td><td>33</td><td>22</td><td>31</td></tr><tr><td>MPVAE</td><td>43</td><td>97</td><td>25</td><td>57</td></tr><tr><td>ASL</td><td>24</td><td>25</td><td>20</td><td>26</td></tr><tr><td>RBCC</td><td>3840</td><td>275</td><td>1320</td><td>337</td></tr><tr><td>C-GMVAE</td><td>22</td><td>24</td><td>14</td><td>16</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">模型</td><td colspan="2">eBird (s)</td><td colspan="2">mirflickr (s)</td></tr><tr><td>训练（每个时期）</td><td>测试（总计）</td><td>训练（每个时期）</td><td>测试（总计）</td></tr><tr><td>C2AE</td><td>27</td><td>29</td><td>18</td><td>24</td></tr><tr><td>LaMP</td><td>32</td><td>33</td><td>22</td><td>31</td></tr><tr><td>MPVAE</td><td>43</td><td>97</td><td>25</td><td>57</td></tr><tr><td>ASL</td><td>24</td><td>25</td><td>20</td><td>26</td></tr><tr><td>RBCC</td><td>3840</td><td>275</td><td>1320</td><td>337</td></tr><tr><td>C-GMVAE</td><td>22</td><td>24</td><td>14</td><td>16</td></tr></tbody></table></div><p>Table 8. Comparison of different models' time costs.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表8. 不同模型的时间成本比较。</p></div><!-- Media --><p>In Tab. 8, we show the per-epoch runtime for training, and the total time cost for testing. Our C-GMVAE is very competitive w.r.t. both training and testing time costs.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在表8中，我们展示了每个训练周期的运行时间，以及测试的总时间成本。我们的C-GMVAE在训练和测试时间成本方面都非常具有竞争力。</p></div><!-- Media --><!-- figureText: 0.002 C-GMVAE MPVAE LaMP 0.5 1.0 data abundance C-GMVAE MPVAE LaMP 0.5 1.0 data abundance C-GMVAE MPVAE LaMP 0.5 1.0 data abundance C-GMVAE MPVAE LaMP 0.5 1.0 data abundance ΔHA 0.000 -0.002 0.1 Δex-F1 0.02 0.00 0.1 Δmi-F1 0.02 0.00 0.1 0.06 Δma-F1 0.04 0.02 0.00 0.1 --><img src="https://cdn.noedgeai.com/bo_d15g03ref24c73d1f6k0_13.jpg?x=944&#x26;y=421&#x26;w=566&#x26;h=1357&#x26;r=0"><p>Figure 5. Relative performances w.r.t. HA, ex-F1, mi-F1 and ma-F1 on mirflickr dataset.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图5. 在mirflickr数据集上，HA、ex-F1、mi-F1和ma-F1的相对性能。</p></div><!-- figureText: 0.0075 C-GMVAE MPVAE LaMP 0.5 1.0 data abundance C-GMVAE MPVAE LaMP 0.5 1.0 data abundance C-GMVAE MPVAE LaMP 0.5 1.0 data abundance C-GMVAE MPVAE LaMP 0.5 1.0 data abundance ΔHA 0.0050 0.0025 0.0000 0.1 0.05 Δex-F1 0.00 -0.05 0.1 0.04 Δmi-F1 0.02 0.00 0.1 0.04 \( \Delta \mathrm{{ma} - F}1 \) 0.02 0.1 --><img src="https://cdn.noedgeai.com/bo_d15g03ref24c73d1f6k0_14.jpg?x=203&#x26;y=412&#x26;w=569&#x26;h=1383&#x26;r=0"><p>Figure 6. Relative performances w.r.t. HA, ex-F1, mi-F1 and ma-F1 on nus-vec dataset.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6. 在nus-vec数据集上，HA、ex-F1、mi-F1和ma-F1的相对性能。</p></div><!-- figureText: 0.05 C-GMVAE MPVAE LaMP 0.5 1.0 data abundance C-GMVAE MPVAE LaMP 0.5 1.0 data abundance C-GMVAE MPVAE LaMP 0.5 1.0 data abundance C-GMVAE MPVAE LaMP 0.5 1.0 data abundance ΔHA 0.00 0.1 0.05 Δex-F1 0.00 0.1 0.05 Δmi-F1 0.00 0.1 0.10 Δma-F1 0.05 0.00 -0.05 0.1 --><img src="https://cdn.noedgeai.com/bo_d15g03ref24c73d1f6k0_14.jpg?x=954&#x26;y=426&#x26;w=560&#x26;h=1352&#x26;r=0"><p>Figure 7. Relative performances w.r.t. HA, ex-F1, mi-F1 and ma-F1 on ebird dataset.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图7. 在ebird数据集上，HA、ex-F1、mi-F1和ma-F1的相对性能。</p></div><!-- Media --><h3>B.2. Training on Fewer Data</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>B.2. 在较少数据上训练</h3></div><p>We provide relative performances of several major state-of-the-art methods including ours to C2AE, on HA, ex-F1, mi-F1, ma-F1 scores. All methods are trained on 10% or \({50}\%\) of the data,including C2AE. The compared results have the same amount of data for training and thus the comparison is fair.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们提供了包括我们的方法在内的几种主要最先进方法在HA、ex-F1、mi-F1、ma-F1评分上的相对性能。所有方法均在10%或\({50}\%\)的数据上进行训练，包括C2AE。比较结果在训练数据量上相同，因此比较是公平的。</p></div><p>Fig. 5, Fig. 6, Fig. 7 show the relative performance of various state-of-the-art methods over C2AE, on mirflickr, nus-vec, eBird respectively.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图5、图6、图7分别显示了在mirflickr、nus-vec、eBird上各种最先进方法相对于C2AE的相对性能。</p></div>
      </body>
    </html>
  