
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>2024-Diffusion Self-Distillation for Zero-Shot Customized Image Generation</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      </head>
      <body>
        <div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="0,0"><div style="height: auto;"><h1><div><div>Diffusion Self-Distillation for Zero-Shot Customized Image Generation<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">扩散自蒸馏用于零-shot定制图像生成</div></div></div></h1></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="0,0"><div style="height: auto;"><div><div><div>Shengqu Cai Eric Ryan Chan Yunzhi Zhang<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">蔡胜曲 埃里克·瑞安·陈 张云志</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="0,0"><div style="height: auto;"><div><div><div>Leonidas Guibas Jiajun Wu Gordon Wetzstein<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">莱昂尼达斯·吉巴斯 佳俊·吴 戈登·维茨斯坦</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="0,0"><div style="height: auto;"><div><div><div>Stanford University<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">斯坦福大学</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_0.jpg?x=163&amp;y=605&amp;w=1469&amp;h=504"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="0,0"><div style="height: auto;"><div><div><div>Figure 1. Given an input image, Diffusion Self-Distillation is a novel diffusion-based approach that generates diverse images that maintain the input's identity across various contexts. Unlike prior approaches that require fine-tuning or are limited to specific domains, Diffusion Self-Distillation offers instant customization without any additional inference-stage training, enabling precise control and editability in text-to-image diffusion models. This ability makes Diffusion Self-Distillation a valuable tool for general AI content creation.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图1. 给定输入图像，扩散自蒸馏是一种新颖的基于扩散的方法，可以生成在各种上下文中保持输入身份的多样化图像。与之前需要微调或仅限于特定领域的方法不同，扩散自蒸馏提供了即时定制，无需任何额外的推理阶段训练，从而在文本到图像的扩散模型中实现精确控制和可编辑性。这一能力使得扩散自蒸馏成为通用人工智能内容创作的重要工具。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="0,0"><div style="height: auto;"><h2><div><div>Abstract<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">摘要</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="0,0"><div style="height: auto;"><div><div><div>Text-to-image diffusion models produce impressive results but are frustrating tools for artists who desire fine-grained control. For example, a common use case is to create images of a specific concept in novel contexts, i.e., "identity-preserving generation". This setting, along with many other tasks (e.g., relighting), is a natural fit for image+text-conditional generative models. However, there is insufficient high-quality paired data to train such a model directly. We propose Diffusion Self-Distillation, a method for using a pre-trained text-to-image model to generate its own dataset for text-conditioned image-to-image tasks. We first leverage a text-to-image diffusion model's in-context generation ability to create grids of images and curate a large paired dataset with the help of a vision-language model. We then fine-tune the text-to-image model into a text+image-to-image model using the curated paired dataset. We demonstrate that Diffusion Self-Distillation outperforms existing zero-shot methods and is competitive with per-instance tuning techniques on a wide range of identity-preserving generation tasks, without requiring test-time optimization. Project page: primecai.github.io/dsd.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">文本到图像的扩散模型产生了令人印象深刻的结果，但对于希望获得细粒度控制的艺术家来说，这些工具常常令人沮丧。例如，一个常见的用例是在新颖的上下文中创建特定概念的图像，即“身份保持生成”。这个设置以及许多其他任务（例如，重新照明）非常适合图像+文本条件生成模型。然而，直接训练这样的模型所需的高质量配对数据不足。我们提出了扩散自蒸馏，这是一种利用预训练的文本到图像模型生成自身数据集以用于文本条件的图像到图像任务的方法。我们首先利用文本到图像扩散模型的上下文生成能力创建图像网格，并借助视觉-语言模型策划一个大型配对数据集。然后，我们使用策划的配对数据集将文本到图像模型微调为文本+图像到图像模型。我们证明扩散自蒸馏在广泛的身份保持生成任务中优于现有的零-shot方法，并且在每实例调优技术上具有竞争力，而无需测试时优化。项目页面：primecai.github.io/dsd。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="0,0"><div style="height: auto;"><h2><div><div>1. Introduction<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">1. 引言</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="0,0"><div style="height: auto;"><div><div><div>In recent years, text-to-image diffusion models [24, 28, 29, 32] have set new standards in image synthesis, generating high-quality and diverse images from textual prompts. However, while their ability to generate images from text is impressive, these models often fall short in offering precise control, editability, and consistency-key features that are crucial for real-world applications. Text input alone can be insufficient to convey specific details, leading to variations that may not fully align with the user's intent, especially in scenarios that require faithful adaptation of a character or asset's identity across different contexts.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">最近几年，文本到图像的扩散模型 [24, 28, 29, 32] 在图像合成方面设定了新的标准，能够从文本提示生成高质量和多样化的图像。然而，尽管这些模型从文本生成图像的能力令人印象深刻，但它们在提供精确控制、可编辑性和一致性方面往往不足，这些特性对于现实世界的应用至关重要。仅仅依靠文本输入可能不足以传达具体细节，导致的变体可能无法完全符合用户的意图，尤其是在需要忠实适应角色或资产身份的不同上下文场景中。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-cd11c450-a854-48f6-9662-26d9a9967f04" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div>Maintaining the instance's identity is challenging, however. We distinguish structure-preserving edits, in which the target and source image share the general layout, but may differ in style, texture, or other local features, and identity-preserving edits, where assets are recognizably the same across target and source images despite potentially large-scale changes in image structure (Fig. 3). The latter task is a superset of the former and requires the model to have a significantly more profound understanding of the input image and concepts to extract and customize the desired identity. For example,image editing <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1034" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mn>2</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>22</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>43</mn></mrow></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow></math></mjx-assistive-mml></mjx-container> ,such as local content editing, re-lighting, and semantic image synthesis, etc. are all structure-preserving and identity-preserving edits, but novel-view synthesis and character-consistent generation under pose variations, are identity-preserving but not structure-preserving. We aim to address the general case, maintaining identity without constraining structure.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">然而，保持实例的身份是具有挑战性的。我们区分结构保持编辑，其中目标图像和源图像共享一般布局，但在风格、纹理或其他局部特征上可能有所不同，以及身份保持编辑，其中资产在目标和源图像中可识别地相同，尽管图像结构可能发生大规模变化（图3）。后者任务是前者的超集，要求模型对输入图像和概念有更深刻的理解，以提取和定制所需的身份。例如，图像编辑 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1035" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mn>2</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>22</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>43</mn></mrow></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow></math></mjx-assistive-mml></mjx-container> ，如局部内容编辑、重新照明和语义图像合成等，都是结构保持和身份保持的编辑，但新视角合成和在姿势变化下的一致角色生成是身份保持但不结构保持的。我们的目标是解决一般情况，在不限制结构的情况下保持身份。</div></div></div></div></span></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_1.jpg?x=168&amp;y=196&amp;w=1463&amp;h=607"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="0,0"><div style="height: auto;"><div><div><div>Figure 2. Overview of our pipeline. Left: the top shows our vanilla paired data generation wheel (Sec. 3.1). We first sample reference image captions from the LAION [33] dataset. These reference captions are parsed through an LLM to be translated into identity-preserved grid generation prompts (Sec. 3.1.2). We feed these enhanced prompts to a pretrained text-to-image diffusion model to sample potentially identity-preserved grids of images, which are then cropped and composed into vanilla image pairs (Sec. 3.1.1). On the bottom, we show our data curation pipeline (Sec. 3.1.3), where the vanilla image paired are fed into a VLM to classify whether they depict identical main subjects. This process mimics a human annotation/curation process while being fully automatic; we use the curated data as our final training data. Right: we extend the diffusion transformer model into an image-conditioned framework by treating the input image as the first frame of a two-frame sequence. The model generates both frames simultaneously-the first reconstructs the input, while the second is the edited output-allowing effective information exchange between the conditioning image and the desired output.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 2. 我们的流程概述。左侧：上方展示的是我们的基础配对数据生成流程（第 3.1 节）。我们首先从 LAION [33] 数据集中抽取参考图像的标题。这些参考标题通过大规模语言模型（LLM）解析后，转化为具有身份保留的网格生成提示（第 3.1.2 节）。我们将这些增强的提示输入到预训练的文本到图像扩散模型中，以生成可能保留身份的图像网格，接着进行裁剪和组合成基础的图像配对（第 3.1.1 节）。下方展示的是我们的数据整理流程（第 3.1.3 节），在此流程中，基础的图像配对被输入到视觉语言模型（VLM）中，分类它们是否描绘了相同的主要主体。此过程模拟了人工注释/整理过程，但完全自动化；我们将整理后的数据用作最终的训练数据。右侧：我们通过将输入图像视为两帧序列的第一帧，将扩散转换器模型扩展为图像条件框架。该模型同时生成两帧——第一帧重建输入图像，第二帧为编辑后的输出——实现了条件图像与期望输出之间的有效信息交换。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="0,0"><div style="height: auto;"><div><div><div>For structure-preserving edits, adding layers, as in Con-trolNet [43], introduces spatial conditioning controls but is limited to structure guidance and does not address consistent identity adaptation across diverse contexts. For identity-preserving edits, fine-tuning methods such as Dream-Booth [31] and LoRA [13] can improve consistency using a few reference samples but are time consuming and computationally intensive, requiring training for each reference. Zero-shot alternatives like IP-Adapter [42] and Instan-tID [37] offer faster solutions without the need for retraining but fall short in providing the desired level of consistency and customization; IP-Adapter [42] lacks full customization capabilities, and InstantID [37] is restricted to facial identity.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">对于结构保留的编辑，如在 ControlNet [43] 中所示，添加层次结构引入了空间条件控制，但仅限于结构指导，并未解决在多种上下文中一致的身份适配问题。对于身份保留的编辑，像 DreamBooth [31] 和 LoRA [13] 这样的微调方法能够通过少量参考样本提高一致性，但过程耗时且计算密集，每个参考都需要重新训练。像 IP-Adapter [42] 和 InstantID [37] 这样的零样本替代方案提供了更快的解决方案，无需重新训练，但在提供所需一致性和定制性方面仍存在不足；IP-Adapter [42] 缺乏完整的定制能力，InstantID [37] 限于面部身份。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="0,0"><div style="height: auto;"><div><div><div>In this paper, we propose a novel approach called Diffusion Self-Distillation, designed to address the core challenge of zero-shot instant customization and adaptation of any character or asset in text-to-image diffusion models. We identify the primary obstacle that hinders prior methods, such as IP-Adapter [42] and InstantID [37], from achieving better identity preservation or generalizing beyond facial contexts: the absence of large-scale paired datasets and corresponding supervised identity-preserving training pipelines. With recent advancements in foundational model capabilities, we are now positioned to exploit these strengths further. Specifically, we can generate consistent grids of identical characters or assets, opening a new pathway for customization that eliminates the need for pre-existing, handcrafted paired datasets-which are expensive and time consuming to collect. The ability to generate these consistent grids likely emerged from foundational model training on diverse datasets, including photo albums, mangas, and comics. Our approach harnesses Vision-Language Models (VLMs) to automatically curate many generated grids, producing a diverse set of grid images with consistent identity features across various contexts. This curated synthetic dataset then serves as the foundation for fine-tuning and adapting any identity, transforming the task of zero-shot customized image generation from unsupervised to supervised. Diffusion Self-Distillation offers transformative potential for applications like consistent character generation, camera control, relighting, and asset customization in fields such as comics and digital art. This flexibility allows artists to rapidly iterate and adapt their work, reducing effort and enhancing creative freedom, making Diffusion Self-Distillation a valuable tool for AI-generated content.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">在本文中，我们提出了一种新颖的方法，称为扩散自蒸馏，旨在解决文本到图像扩散模型中零-shot即时定制和适应任何角色或资产的核心挑战。我们识别出阻碍先前方法（如 IP-Adapter [42] 和 InstantID [37]）实现更好身份保留或超越面部上下文进行泛化的主要障碍：缺乏大规模配对数据集和相应的监督身份保留训练管道。随着基础模型能力的最新进展，我们现在能够进一步利用这些优势。具体而言，我们可以生成一致的相同角色或资产的网格，为定制开辟了一条新路径，消除了对现有手工配对数据集的需求，而这些数据集的收集既昂贵又耗时。生成这些一致网格的能力可能源于基础模型在多样化数据集（包括相册、漫画和漫画书）上的训练。我们的方法利用视觉-语言模型（VLMs）自动策划许多生成的网格，生成一组在各种上下文中具有一致身份特征的多样化网格图像。这个策划的合成数据集随后作为微调和适应任何身份的基础，将零-shot定制图像生成的任务从无监督转变为有监督。扩散自蒸馏为一致角色生成、相机控制、重光照和资产定制等应用提供了变革潜力，适用于漫画和数字艺术等领域。这种灵活性使艺术家能够快速迭代和调整他们的作品，减少工作量并增强创造自由，使扩散自蒸馏成为AI生成内容的宝贵工具。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="0,0"><div style="height: auto;"><div><div><div>We summarize our contributions as follows:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们将我们的贡献总结如下：</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-cca14f9b-a462-4a53-8668-f1147206f123" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="0,0"><div style="height: auto;"><div><ul><li>We propose Diffusion Self-Distillation, a zero-shot</li></ul><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><ul><li>我们提出了扩散自蒸馏，一种零-shot</li></ul></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_2.jpg?x=165&amp;y=203&amp;w=703&amp;h=192"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><div><div><div>Figure 3. Difference between structure-preserving and identity-preserving edits. In structure-preserving editing, the main structures of the image are preserved, and only local edits or stylizations are performed. In identity-preserving editing, the global structure of the image may change radically.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图3. 结构保留编辑与身份保留编辑之间的区别。在结构保留编辑中，图像的主要结构被保留，仅进行局部编辑或风格化。在身份保留编辑中，图像的全局结构可能会发生根本性的变化。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><div><div><div>identity-preserving customized image generation model that scales to any instance under any context, with performances on par with inference-stage tuning methods;<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">一种保持身份定制的图像生成模型，能够在任何实例和任何上下文下扩展，并且其性能与推理阶段调整方法相媲美；</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><div><ul><li>We provide a self-distillation pipeline to obtain identity-preserving data pairs purely from pretrained text-to-image diffusion models, LLMs, and VLMs, without any human effort involved in the entire data creation wheel;</li></ul><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><ul><li>我们提供了一种自蒸馏管道，通过该管道从预训练的文本到图像扩散模型、LLMs 和 VLMs 中获得保持身份的数据对，整个数据创建过程中不涉及任何人工工作；</li></ul></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><div><ul><li>We correspondingly design a unified architecture for image-to-image translation tasks involving both identity-and structure-preserving edits, including personalization, relighting, depth controls, and instruction following.</li></ul><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><ul><li>我们相应地设计了一个统一的架构，用于图像到图像的转换任务，涉及身份和结构保持编辑，包括个性化、光照重调、深度控制和指令跟随。</li></ul></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><h2><div><div>2. Related work<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">2. 相关工作</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><div><div><div>Recent advancements in diffusion models have underscored the need for enhanced control and customization in image-generation tasks. Various methods have been proposed to address these challenges through additional conditioning mechanisms, personalization, and rapid adaptation [26].<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">近年来，扩散模型的进展突显了在图像生成任务中对增强控制和定制的需求。为解决这些挑战，已提出多种方法，通过额外的条件机制、个性化和快速适应来应对这些问题 [26]。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><div><div><div>Control Mechanisms in Diffusion Models. To move beyond purely text-based controls, approaches like Control-Net [43] introduce spatial conditioning via inputs such as sketches, depth maps, and segmentation masks, enabling fine-grained structure control. ControlNet++ [19] refines this by enhancing the integration of spatial inputs for more nuanced control. Uni-ControlNet [44] unifies various control types within a single framework, standardizing the handling of diverse signals. T2I-Adapter [23] employs lightweight adapters to align pretrained models with external control signals without altering the core architecture. While these methods offer increased flexibility, they often focus on structural conditioning types such as depths and lack capabilities for concept extraction or identity preservation.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">扩散模型中的控制机制。为了超越纯粹基于文本的控制方法，像 Control-Net [43] 这样的方法通过草图、深度图和分割掩码等输入引入空间条件，从而实现细粒度的结构控制。ControlNet++ [19] 通过增强空间输入的集成来进一步优化这一点，实现更精细的控制。Uni-ControlNet [44] 将各种控制类型统一在一个框架内，标准化了对多种信号的处理。T2I-Adapter [23] 使用轻量级适配器，将预训练模型与外部控制信号对齐，而无需改变核心架构。尽管这些方法提供了更大的灵活性，但它们通常侧重于深度等结构条件类型，并缺乏概念提取或身份保持的能力。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><div><div><div>Personalization and Fine-Tuning. Techniques like Dream-Booth [31] and LoRA [13] enhance the consistency and relevance of generated images by fine-tuning models with small sets of reference images. DreamBooth [31] personalizes models to maintain a subject's identity across different contexts, while LoRA [13] provides an efficient approach to fine-tuning large models without extensive retraining. However, these methods require multiple images and test-time optimization for each reference, which can be computationally expensive-especially with the exponential growth in model sizes (12 billion parameters for FLUX).<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">个性化和微调。像 Dream-Booth [31] 和 LoRA [13] 这样的技术通过使用小规模的参考图像对模型进行微调，从而增强生成图像的一致性和相关性。DreamBooth [31] 使模型个性化，以保持主题在不同上下文中的身份，而 LoRA [13] 提供了一种高效的微调大型模型的方法，无需广泛的重新训练。然而，这些方法需要多个图像和每个参考的测试时优化，这在计算上可能是昂贵的，尤其是在模型规模呈指数增长的情况下（FLUX 的参数达到 120 亿）。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><div><div><div>Zero-Shot and Fast Adaptation. IP-Adapter [42] incorporates image prompts into diffusion models using image em-beddings, allowing for generations that align closely with reference visuals. InstantID [37] ensures zero-shot face preservation, maintaining a subject's key features across various contexts. While effective as zero-shot methods without user training, IP-Adapter [42] struggles to adapt specific targets like unique characters or assets, and InstantID [37] is limited to facial identity preservation. IPAdapter-Instruct [30] enhances image-based conditioning with instruct prompts but relies heavily on specific instructions and task-specific pretrained models. Other methods, such as SuTI [7] and GDT [15], handcrafted corresponding datasets, which are expensive and challenging to collect and scale. Another work along this line is Subject-Diffusion [21], which uses segmentation masks to create synthetic data for training but is bounded by achieving only simple attributes and accessories copying and editing within input images.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">零-shot 和快速适应。IP-Adapter [42] 将图像提示融入扩散模型中，使用图像嵌入，允许生成与参考视觉紧密对齐的图像。InstantID [37] 确保零-shot 面部特征保持，在各种上下文中保持主题的关键特征。虽然作为零-shot 方法在没有用户训练的情况下有效，但 IP-Adapter [42] 在适应特定目标（如独特角色或资产）方面存在困难，而 InstantID [37] 限于面部身份的保持。IPAdapter-Instruct [30] 通过指令提示增强基于图像的条件，但在很大程度上依赖于特定指令和任务特定的预训练模型。其他方法，如 SuTI [7] 和 GDT [15]，手工制作相应的数据集，这些数据集的收集和扩展既昂贵又具有挑战性。另一项相关工作是 Subject-Diffusion [21]，它使用分割掩码创建用于训练的合成数据，但仅限于实现简单属性和配件在输入图像中的复制和编辑。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><div><div><div>Existing methods contribute valuable advancements but often target specific domains or require user-stage tuning. Diffusion Self-Distillation bridges these gaps by offering a unified, zero-shot approach for consistent customization of characters and assets using minimal input. By leveraging self-distillation assisted by vision-language models, Diffusion Self-Distillation provides a comprehensive and adaptable solution for a wide range of creative applications.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">现有方法为领域提供了有价值的进展，但往往针对特定领域或需要用户阶段的调优。扩散自蒸馏通过提供统一的零-shot 方法，使用最少的输入实现角色和资产的一致定制，从而弥补了这些空白。通过利用视觉-语言模型辅助的自蒸馏，扩散自蒸馏为广泛的创意应用提供了全面且可适应的解决方案。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><h2><div><div>3. Diffusion Self-Distillation<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3. 扩散自蒸馏</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><div><div><div>We discover that recent text-to-image generation models offer the surprising ability to generate in-context, consistent image grids (see Fig. 2, left). Motivated by this insight, we develop a zero-shot adaptation network that offers fast, diverse, high-quality, and identity-preserving, i.e., consistent image generation conditioned on a reference image. For this purpose, we first generate and curate sets of images that exhibit the desired consistency using pretrained text-to-image diffusion models, large language models (LLMs), and vision-language models (VLMs) (Sec. 3.1). Then, we finetune the same pretrained diffusion model with these consistent image sets, employing our newly proposed parallel processing architecture (Sec. 3.2) to create a conditional model. By this end, Diffusion Self-Distillation finetunes a pretrained text-to-image diffusion model into a zero-shot customized image generator in a supervised manner.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们发现，最近的文本到图像生成模型提供了令人惊讶的能力，可以生成上下文一致的图像网格（见图 2，左）。基于这一洞察，我们开发了一种零-shot 适应网络，能够快速、多样、高质量且保持身份一致性，即在参考图像的条件下生成一致的图像。为此，我们首先使用预训练的文本到图像扩散模型、大型语言模型（LLMs）和视觉-语言模型（VLMs）生成和整理出展示所需一致性的图像集（第 3.1 节）。然后，我们使用这些一致的图像集对相同的预训练扩散模型进行微调，采用我们新提出的并行处理架构（第 3.2 节）来创建一个条件模型。最终，扩散自蒸馏以监督的方式将预训练的文本到图像扩散模型微调为零-shot 定制图像生成器。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><h3><div><div>3.1. Generating a Pairwise Dataset<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.1. 生成成对数据集</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-50c19fda-6020-4d0d-bc9b-9f43d9dc76b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><div><div><div>To create a pairwise dataset for supervised Diffusion Self-Distillation training, we leverage the emerging multi-image generation capabilities of pretrained text-to-image diffusion models to produce potentially consistent vanilla images (Sec. 3.1.1) created by LLM-generated prompts (Sec. 3.1.2). We then use VLMs to curate these vanilla samples, obtaining clean sets of images that share the desired identity consistency (Sec. 3.1.3). The data generation and curation pipeline is shown in Fig. 2, left.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">为了创建一个用于监督扩散自蒸馏训练的成对数据集，我们利用预训练的文本到图像扩散模型的新兴多图像生成能力，生成可能一致的原始图像（第 3.1.1 节），这些图像是通过 LLM 生成的提示（第 3.1.2 节）创建的。然后，我们使用 VLM 对这些原始样本进行整理，获得共享所需身份一致性的干净图像集（第 3.1.3 节）。数据生成和整理流程如图 2 左侧所示。</div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"><h4><div><div>3.1.1. Vanilla Data Generation via Teacher Model<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.1.1. 通过教师模型生成原始数据</div></div></div></h4></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div>To generate sets of images that fulfill the desired identity preservation, we prompt the teacher pretrained text-to-image diffusion model to create images containing multiple panels featuring the same subject with variations in expression, pose, lighting conditions, and more, for training purposes. Such prompting can be as simple as specifying the desired identity preservation in the output, such as "a grid of 4 images representing the same <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1036" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&lt;</mo></math></mjx-assistive-mml></mjx-container> object/character/scene/etc. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1037" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3E"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&gt;</mo></math></mjx-assistive-mml></mjx-container> ", "an evenly separated 4 panels,depicting identical <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1038" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&lt;</mo></math></mjx-assistive-mml></mjx-container> object/character/scene/etc. &gt;", etc. We additionally specify the expected content in each sub-image/panel. The full set of prompts is provided in our supplemental material Sec. A. Our analysis shows that current state-of-the-art text-to-image diffusion models (e.g., SD3 [8], DALL-E 3, FLUX) demonstrate this identity-preserving capability, likely emerging from their training data, which includes comics, mangas, photo albums, and video frames. Such in-context generation ability is crucial to our data generation wheel.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">为了生成能够实现所需身份保留的图像集，我们提示经过预训练的文本到图像扩散模型来创建包含多个面板的图像，面板中展示同一主题，并在表情、姿势、光照条件等方面有所变化，供训练使用。这样的提示可以简单到仅指定输出中所需的身份保留，例如“一个由4张图像组成的网格，表示相同的 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1039" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&lt;</mo></math></mjx-assistive-mml></mjx-container> 对象/角色/场景等 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1040" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3E"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&gt;</mo></math></mjx-assistive-mml></mjx-container>”，“均匀分隔的4个面板，描绘相同的 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1041" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&lt;</mo></math></mjx-assistive-mml></mjx-container> 对象/角色/场景等 &gt;”等。我们还额外指定了每个子图像/面板中预期的内容。完整的提示集请参见我们的补充材料第A节。我们的分析表明，当前最先进的文本到图像扩散模型（例如，SD3 [8]、DALL-E 3、FLUX）展示了这一身份保留能力，这可能源自它们的训练数据，其中包括漫画、动漫、相册和视频帧。这种上下文生成能力对我们的数据生成工作至关重要。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"><h4><div><div>3.1.2. Prompt Generation via LLMs<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.1.2. 通过大型语言模型生成提示</div></div></div></h4></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"><div><div><div>We rely on an LLM to "brainstorm" a large dataset of diverse prompts, from which we derive our image grid dataset. By defining a prompt structure, we prompt the LLM to produce text prompts that describe image grids. A challenge we encountered is that when prompted to create large sets of prompts, LLMs tend to produce prompts of low diversity. For example, we noticed that without additional guidance, GPT-4o has a strong preference for prompts with cars and robots, resulting in highly repetitive outputs. To address this issue, we utilize the available image captions in the LAION [33] dataset, feeding them into the LLM as content references. These references from real image captions dramatically improve the diversity of generated prompts. Optionally, we also use the LLM to filter these reference captions, ensuring they contain a clear target for identity preservation. We find that this significantly improves the hit rate of generating consistent multi-image outputs.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们依赖一个大语言模型（LLM）来“头脑风暴”生成一个包含多样化提示的大型数据集，从中我们构建了图像网格数据集。通过定义提示结构，我们要求LLM生成描述图像网格的文本提示。我们遇到的挑战是，当要求LLM生成大量提示时，LLM往往会生成低多样性的提示。例如，我们注意到，在没有额外指导的情况下，GPT-4o 强烈偏好包含汽车和机器人提示，导致输出高度重复。为了解决这个问题，我们利用LAION [33] 数据集中可用的图像说明，将它们作为内容参考输入LLM。这些来自真实图像说明的参考大大提高了生成提示的多样性。可选地，我们还使用LLM来过滤这些参考说明，确保它们包含明确的身份保留目标。我们发现，这显著提高了生成一致多图像输出的成功率。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"><h4><div><div>3.1.3. Dataset Curation and Caption with VLMs<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.1.3. 使用视觉语言模型（VLM）进行数据集整理与标注</div></div></div></h4></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"><div><div><div>While the aforementioned data generation scheme provides identity-preserving multi-image samples of decent quality and quantity, these initial "uncurated" images tend to be noisy and unsuitable for direct use. Therefore, we leverage the strong capabilities of VLMs to curate a clean dataset. We extract pairs of images from the generated samples intended to preserve the identity and ask the VLM whether the two images depict the same object, character, scene, etc. We find that employing Chain-of-Thought prompting [38] is particularly helpful in this context. Specifically, we first prompt the VLM to identify the common object, character, or scene present in both images, then have it describe each one in detail, and finally analyze whether they are identical, providing a conclusive response. This process yields pairs of images that share the same identity.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">尽管前述的数据生成方案提供了质量和数量尚可的身份保留多图像样本，但这些初始的“未整理”图像往往是嘈杂的，且不适合直接使用。因此，我们利用视觉语言模型（VLM）的强大能力来整理一个干净的数据集。我们从生成的样本中提取出一对对图像，这些图像旨在保留相同的身份，并询问VLM这两幅图像是否描绘的是相同的物体、角色、场景等。我们发现，采用“思维链”（Chain-of-Thought）提示 [38] 在这个过程中尤为有用。具体来说，我们首先提示VLM识别出两幅图像中共同出现的物体、角色或场景，然后让它详细描述每一幅图像，最后分析它们是否相同，并给出结论性回答。这个过程最终得到了具有相同身份的图像对。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"><h3><div><div>3.2. Parallel Processing Architecture<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.2. 并行处理架构</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div>We desire a conditional architecture suitable for general image-to-image tasks, including transformations in which structure is preserved, and transformations in which concepts/identities are preserved but image structure is not. This is a challenging problem because it may necessitate the transfer of fine details without guaranteeing spatial correspondences. While the ControlNet [43] architecture is excellent at structure-preserving edits, such as depth-to-image or segmentation-map-to-image, it struggles to preserve details under more complex identity-preserving edits, where the source and target images are not pixel-aligned. On the other hand, IP-Adapter [42] can extract certain concepts, such as styles, from the input image. Still, it strongly relies on a task-specific image encoder and often fails to preserve more complex concepts and identities. Drawing inspiration from the success of multi-view and video diffusion models [1, 3- <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1042" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>10</mn></mrow><mo>−</mo><mrow data-mjx-texclass="ORD"><mn>12</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>14</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>16</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>18</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>34</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>35</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>39</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>41</mn></mrow><mo fence="false" stretchy="false">]</mo></math></mjx-assistive-mml></mjx-container> ,we propose a simple yet effective method to extend the vanilla diffusion transformer model into an image-conditioned diffusion model. Specifically, we treat the input image as the first frame of a video and produce a two-frame video as output. The final loss is computed over the two-frame video, establishing an identity mapping for the first frame and a conditionally editing target for the second frame. Our architecture design allows generality for generic image-to-image translation tasks, since it enables effective information exchange between the two frames, allowing the model to capture complex semantics and perform sophisticated edits, as shown in Fig. 2, right.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们希望设计一种适用于一般图像到图像任务的条件架构，包括保持结构的变换和保持概念/身份但不保持图像结构的变换。这是一个具有挑战性的问题，因为它可能需要传递细节而不保证空间对应关系。虽然 ControlNet [43] 架构在保持结构的编辑方面表现出色，例如深度到图像或分割图到图像，但在更复杂的保持身份的编辑中，它难以保持细节，因为源图像和目标图像并未像素对齐。另一方面，IP-Adapter [42] 可以从输入图像中提取某些概念，例如风格。然而，它在很大程度上依赖于特定任务的图像编码器，通常无法保持更复杂的概念和身份。受到多视角和视频扩散模型成功的启发 [1, 3- <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1043" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>10</mn></mrow><mo>−</mo><mrow data-mjx-texclass="ORD"><mn>12</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>14</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>16</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>18</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>34</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>35</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>39</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>41</mn></mrow><mo fence="false" stretchy="false">]</mo></math></mjx-assistive-mml></mjx-container>，我们提出了一种简单而有效的方法，将普通的扩散变换器模型扩展为图像条件扩散模型。具体来说，我们将输入图像视为视频的第一帧，并生成一个两帧视频作为输出。最终损失是在两帧视频上计算的，为第一帧建立身份映射，为第二帧建立条件编辑目标。我们的架构设计允许对一般图像到图像翻译任务的通用性，因为它能够有效地在两帧之间交换信息，使模型能够捕捉复杂的语义并执行复杂的编辑，如图 2 右侧所示。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"><h2><div><div>4. Experiments<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">4. 实验</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div>Implementation details. We use FLUX1.0 DEV as both our teacher and student models, achieving self-distillation. For prompt generation, we use GPT-4o; for dataset curation and captioning, we use Gemini-1.5. We train all models on 8 NVIDIA H100 80GB GPUs with an effective batch size of 160 for 100k iterations, using AdamW optimizer [20] with a learning rate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1044" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-script style="vertical-align: 0.393em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mn>10</mn></mrow><mrow data-mjx-texclass="ORD"><mo>−</mo><mn>4</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> . Our parallel processing architecture uses LoRAs with rank 512 on the base model.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">实现细节。我们使用 FLUX1.0 DEV 作为教师模型和学生模型，实现自我蒸馏。对于提示生成，我们使用 GPT-4o；对于数据集的创建和标注，我们使用 Gemini-1.5。我们在 8 台 NVIDIA H100 80GB GPU 上训练所有模型，有效批量大小为 160，迭代次数为 100k，使用 AdamW 优化器 [20] 和学习率 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1045" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-script style="vertical-align: 0.393em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mn>10</mn></mrow><mrow data-mjx-texclass="ORD"><mo>−</mo><mn>4</mn></mrow></msup></math></mjx-assistive-mml></mjx-container>。我们的并行处理架构使用 LoRA（秩为 512）在基础模型上进行训练。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-f44c1342-3978-4737-ac1e-a13b6fba1459" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div>Datasets. Our final training dataset contains <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1046" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6B"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo><mrow data-mjx-texclass="ORD"><mn>400</mn></mrow><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">k</mi></mrow></math></mjx-assistive-mml></mjx-container> subject-consistent image pairs generated from our teacher model, FLUX1.0 DEV. Generating and curating the dataset is<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">数据集。我们最终的训练数据集包含从我们的教师模型 FLUX1.0 DEV 生成的 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1047" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6B"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo><mrow data-mjx-texclass="ORD"><mn>400</mn></mrow><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">k</mi></mrow></math></mjx-assistive-mml></mjx-container> 主体一致的图像对。生成和创建数据集是</div></div></div></div></span></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_4.jpg?x=163&amp;y=196&amp;w=1471&amp;h=954"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="0,0"><div style="height: auto;"><div><div><div>Figure 4. Qualitative comparison. Overall, our method achieves high subject identity preservation and prompt-aligned diversity while not suffering from a "copy-paste" effect, such as the results of IP-Adapter+ [42]. This is largely thanks to our supervised training pipeline, which alleviates the base model's in-context generation ability.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 4. 定性比较。总体而言，我们的方法在保持主体身份一致性和提示对齐多样性方面表现出色，并且没有出现像 IP-Adapter+ [42] 那样的“复制粘贴”效应。这主要归功于我们的监督训练流程，它缓解了基础模型在上下文生成能力方面的限制。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="0,0"><div style="height: auto;"><div><div><div>fully automated and requires no human effort, so its size could be further scaled. We use the publicly available DreamBench++ [25] dataset and follow their protocols for evaluation. DreamBench++ [25] is a comprehensive and diverse dataset for evaluating personalized image generation, consisting of 150 high-quality images and 1,350 prompts-significantly more than previous benchmarks like DreamBench [31]. The dataset covers various categories such as animals, humans, objects, etc., including photoreal-istic and non-photorealistic images, with prompts designed to span different difficulty levels (simple/imaginative). In contrast, prompts are generated using GPT-4o and refined by human annotators to ensure diversity and ethical compliance.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">完全自动化且无需人工干预，因此其规模可以进一步扩大。我们使用公开可用的 DreamBench++ [25] 数据集，并遵循其评估协议。DreamBench++ [25] 是一个全面且多样化的数据集，用于评估个性化图像生成，包含 150 张高质量图像和 1,350 个提示——显著多于之前的基准，如 DreamBench [31]。该数据集涵盖了动物、人类、物体等多种类别，包括写实和非写实图像，提示设计旨在涵盖不同难度级别（简单/富有创意）。相比之下，提示是使用 GPT-4o 生成并由人工注释员进行优化，以确保多样性和道德合规性。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="0,0"><div style="height: auto;"><div><div><div>Baselines. We follow the setups in DreamBench++ [25] and compare our model with two classes of baselines: inference-stage tuning models and zero-shot models. For inference-stage models, we compare against Textual Inversion [9], DreamBooth [31] and its LoRA [13] version. For zero-shot models, we compare with BLIP-Diffusion [17], Emu2 [36], IP-Adapter [42], IP-Adapter+ [42].<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">基准。我们遵循 DreamBench++ [25] 中的设置，并将我们的模型与两类基准进行比较：推理阶段调优模型和零-shot 模型。对于推理阶段模型，我们与 Textual Inversion [9]、DreamBooth [31] 及其 LoRA [13] 版本进行比较。对于零-shot 模型，我们与 BLIP-Diffusion [17]、Emu2 [36]、IP-Adapter [42] 和 IP-Adapter+ [42] 进行比较。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-c5f57c2c-6455-4fdf-966a-8c9a217130e7" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div>Evaluation metrics. The evaluation protocol of prior works <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1048" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mn>7</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>30</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>31</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>42</mn></mrow></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow></math></mjx-assistive-mml></mjx-container> typically involves comparing the CLIP [27] and DINO [6] feature similarities. However, we note that the metrics mentioned above capture only global semantic similarity, are extremely noisy, and are biased towards "copy-pasting" the input image. This is especially troublesome when the input image or the prompt is complex. We refer to DreamBench++ [25] for a detailed analysis of their limitations. Therefore, we follow the metrics designed in DreamBench++ [25] and report GPT-4o scores on the more diverse DreamBench++ [25] benchmark for both concept preservation (CP) with different categories of subjects and prompt following (PF) with photorealistic (Real.) and Imaginative (Imag.) prompts, then use their product as a final evaluation score. This evaluation protocol emulates a human user study using VLMs. We additionally slightly modify the GPT evaluation prompts so that penalization can be applied if the generated contents show no internal understanding and creative output but instead naively copy over components from the reference image. The modified metrics are named "de-biased concept preservation (Debiased CP)” and “de-biased prompt following (Debiased PF)”. The full set of GPT evaluation prompts will be provided in our supplementary Sec. B.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">评估指标。先前工作的评估协议 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1049" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mn>7</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>30</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>31</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>42</mn></mrow></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow></math></mjx-assistive-mml></mjx-container> 通常涉及比较 CLIP [27] 和 DINO [6] 特征相似性。然而，我们注意到上述指标仅捕捉全局语义相似性，极其嘈杂，并且偏向于“复制粘贴”输入图像。当输入图像或提示非常复杂时，这尤其成问题。我们参考 DreamBench++ [25] 进行详细分析它们的局限性。因此，我们遵循 DreamBench++ [25] 中设计的指标，并报告 GPT-4o 在更具多样性的 DreamBench++ [25] 基准上的得分，包括不同类别的主题下的概念保持（CP）和针对现实感（Real.）与富有想象力（Imag.）的提示下的提示跟随（PF），然后使用它们的乘积作为最终评估分数。该评估协议模拟了使用视觉语言模型（VLMs）的人工用户研究。我们还略微修改了 GPT 评估提示，以便如果生成的内容没有表现出内部理解和创造性输出，而是天真地复制了参考图像的组件时，可以应用惩罚。修改后的指标分别命名为“去偏概念保持（Debiased CP）”和“去偏提示跟随（Debiased PF）”。完整的 GPT 评估提示将在我们的补充材料 B 部分提供。</div></div></div></div></span></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-cf3ae439-5b61-496a-945f-3175022b5d16" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-cf3ae439-5b61-496a-945f-3175022b5d16" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-cf3ae439-5b61-496a-945f-3175022b5d16" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-cf3ae439-5b61-496a-945f-3175022b5d16" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_5.jpg?x=154&amp;y=201&amp;w=1491&amp;h=1469"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-cf3ae439-5b61-496a-945f-3175022b5d16" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-cf3ae439-5b61-496a-945f-3175022b5d16" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="0,0"><div style="height: auto;"><div><div><div>Figure 5. Qualitative result. Our Diffusion Self-Distillation is capable of various customization targets across different tasks and styles, for instance, characters or objects, photorealistic or animated. Diffusion Self-Distillation can also take instruction types of prompts as input, similar to InstructPix2Pix [2]. Further, our model exhibits relighting capabilities without significantly altering the scene's content.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 5. 质量结果。我们的扩散自蒸馏能够在不同任务和风格之间实现各种定制目标，例如角色或物体、照片真实感或动画风格。扩散自蒸馏还可以将指令类型的提示作为输入，类似于 InstructPix2Pix [2]。此外，我们的模型展现了重新照明的能力，而不会显著改变场景的内容。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-cf3ae439-5b61-496a-945f-3175022b5d16" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-cf3ae439-5b61-496a-945f-3175022b5d16" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-cf3ae439-5b61-496a-945f-3175022b5d16" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-cf3ae439-5b61-496a-945f-3175022b5d16" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="0,0"><div style="height: auto;"><div><div><div>Qualitative results. Fig. 4 presents our qualitative comparison results, demonstrating that our model significantly outperforms all baselines in subject adaptation and concept consistency while exhibiting excellent prompt alignment and diversity in the outputs. Textual Inversion [9], as an early concept extraction method, captures only vague semantics from the input image, making it unsuitable for zero-shot customization tasks that require precise subject adaptation. DreamBooth [31] and DreamBooth-LoRA [13, 31] face challenges in maintaining consistency, primarily because they perform better with multiple input images. This dependency limits their effectiveness when only a single reference image is available. In contrast, our method achieves robust results even with just one input image, highlighting its efficiency and practicality. BLIP-Diffusion [17], operating as a self-supervised representation learning framework, can extract concepts from the input in a zero-shot manner but is confined to capturing overall semantic concepts without the ability to customize specific subjects. Similarly, Emu2 [36], a multimodal foundation model, excels at extracting semantic concepts but lacks mechanisms for specific subject customization, limiting its utility in personalized image generation. IP-Adapter [42] and IP-Adapter+ [42] employ self-supervised learning schemes aimed at reconstructing the input from encoded signals. While effective in extracting global concepts, they suffer from a pronounced "copy-paste" effect, where the generated images closely resemble the input without meaningful transformation. Notably, IP-Adapter+ [42], which utilizes a stronger input image encoder, exacerbates this issue, leading to less diversity and adaptability in the outputs. In contrast, our approach effectively preserves the subject's core identity while enabling diverse and contextually appropriate transformations. As illustrated in Fig. 5, our Diffusion Self-Distillation demonstrates remarkable versatility, adeptly handling various customization targets across different targets (characters, objects, etc.) and styles (photorealistic, animated, etc.). Moreover, Diffusion Self-Distillation generalizes well to a wide range of prompts, including instructions similar to InstructPix2Pix [2], underscoring its robustness and adaptability in diverse customization tasks.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">定性结果。图 4 展示了我们的定性比较结果，表明我们的模型在主题适应性和概念一致性方面显著优于所有基线方法，同时在输出中展现出优秀的提示对齐性和多样性。文本反演 [9] 作为一种早期的概念提取方法，仅能从输入图像中捕捉到模糊的语义，因此不适用于需要精确主题适应的零样本定制任务。DreamBooth [31] 和 DreamBooth-LoRA [13, 31] 在保持一致性方面面临挑战，主要是因为它们在多个输入图像的情况下表现更好。这一依赖性限制了它们在只有单张参考图像时的有效性。相比之下，我们的方法即使仅使用一张输入图像，也能获得稳健的结果，凸显了其高效性和实用性。BLIP-Diffusion [17] 作为一种自监督表示学习框架，能够以零样本方式从输入中提取概念，但仅能捕捉整体的语义概念，无法定制特定的主题。同样，Emu2 [36] 作为一个多模态基础模型，擅长提取语义概念，但缺乏特定主题定制机制，限制了其在个性化图像生成中的应用。IP-Adapter [42] 和 IP-Adapter+ [42] 采用自监督学习方案，旨在通过编码信号重建输入。虽然在提取全局概念方面有效，但它们存在显著的“复制粘贴”效应，生成的图像与输入图像过于相似，缺乏有意义的转化。特别是，IP-Adapter+ [42] 使用更强大的输入图像编码器，进一步加剧了这一问题，导致输出的多样性和适应性降低。相比之下，我们的方法有效地保留了主题的核心特征，同时实现了多样且符合上下文的转化。如图 5 所示，我们的扩散自蒸馏方法展现了显著的多功能性，能够熟练处理不同目标（角色、物体等）和风格（写实风格、动画风格等）的各种定制任务。此外，扩散自蒸馏在多种提示上表现出良好的泛化能力，包括与 InstructPix2Pix [2] 类似的指令，突出其在多样化定制任务中的稳健性和适应性。</div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-c3f60dd0-3f27-42c6-ae52-04ba8b92e119" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-6="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-c3f60dd0-3f27-42c6-ae52-04ba8b92e119" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-c3f60dd0-3f27-42c6-ae52-04ba8b92e119" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-6="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-c3f60dd0-3f27-42c6-ae52-04ba8b92e119" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-6="0,0"><div style="height: auto;"><span style="display: inline;"><div><div class="table-container"><table class="fixed-table"><tbody><tr><td rowspan="2">Method</td><td rowspan="2">Z-S?</td><td colspan="4">Concept Preservation</td><td colspan="3">Prompt Following</td><td rowspan="2">CP.PF <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1050" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td><td colspan="4">Debiased Concept Preservation</td><td colspan="3">Debiased Prompt Following</td><td rowspan="2">Debiased CP.PF <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1051" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>Animal↑</td><td>Human↑</td><td>Object↑</td><td>Overall↑</td><td>Real. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1052" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td><td>MMAS. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1053" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td><td>Overall↑</td><td>Animal↑</td><td>Human↑</td><td>Object↑</td><td>Overall↑</td><td>Real. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1054" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td><td>Imag. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1055" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td><td>Overall↑</td></tr><tr><td>Textual Inversion</td><td>✘</td><td>0.502</td><td>0.358</td><td>0.305</td><td>0.388</td><td>0.671</td><td>0.437</td><td>0.598</td><td>0.232</td><td>0.741</td><td>0.694</td><td>0.717</td><td>0.722</td><td>0.619</td><td>0.385</td><td>0.541</td><td>0.391</td></tr><tr><td>DreamBooth</td><td>✘</td><td>0.640</td><td>0.199</td><td>0.488</td><td>0.442</td><td>0.798</td><td>0.504</td><td>0.692</td><td>0.306</td><td>0.670</td><td>0.362</td><td>0.676</td><td>0.626</td><td>0.750</td><td>0.467</td><td>0.656</td><td>0.411</td></tr><tr><td>DreamBooth LoRA</td><td>✘</td><td>0.751</td><td>0.311</td><td>0.543</td><td>0.535</td><td>0.898</td><td>0.754</td><td>0.849</td><td>0.450</td><td>0.681</td><td>0.675</td><td>0.761</td><td>0.720</td><td>0.865</td><td>0.718</td><td>0.816</td><td>0.588</td></tr><tr><td>BLIP-Diffusion</td><td>✓</td><td>0.637</td><td>0.557</td><td>0.469</td><td>0.554</td><td>0.581</td><td>0.303</td><td>0.464</td><td>0.257</td><td>0.771</td><td>0.733</td><td>0.745</td><td>0.750</td><td>0.529</td><td>0.266</td><td>0.442</td><td>0.332</td></tr><tr><td>Emu2</td><td>✓</td><td>0.670</td><td>0.546</td><td>0.447</td><td>0.554</td><td>0.732</td><td>0.560</td><td>0.670</td><td>0.371</td><td>0.652</td><td>0.683</td><td>0.701</td><td>0.681</td><td>0.686</td><td>0.494</td><td>0.622</td><td>0.424</td></tr><tr><td>IP-Adapter</td><td>✓</td><td>0.667</td><td>0.558</td><td>0.504</td><td>0.576</td><td>0.743</td><td>0.446</td><td>0.607</td><td>0.350</td><td>0.790</td><td>0.764</td><td>0.743</td><td>0.766</td><td>0.695</td><td>0.377</td><td>0.589</td><td>0.451</td></tr><tr><td>IP-Adapter+</td><td>✓</td><td>0.900</td><td>0.845</td><td>0.759</td><td>0.834</td><td>0.502</td><td>0.279</td><td>0.388</td><td>0.324</td><td>0.481</td><td>0.473</td><td>0.530</td><td>0.504</td><td>0.442</td><td>0.229</td><td>0.371</td><td>0.187</td></tr><tr><td>Ours</td><td>✓</td><td>0.647</td><td>0.567</td><td>0.640</td><td>0.631</td><td>0.777</td><td>0.625</td><td>0.726</td><td>0.458</td><td>0.852</td><td>0.774</td><td>0.750</td><td>0.789</td><td>0.808</td><td>0.681</td><td>0.757</td><td>0.597</td></tr></tbody></table></div><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><div class="table-container"><table class="fixed-table"><tbody><tr><td rowspan="2">方法</td><td rowspan="2">Z-S?</td><td colspan="4">概念保持</td><td colspan="3">提示遵循</td><td rowspan="2">CP.PF <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1056" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td><td colspan="4">去偏概念保持</td><td colspan="3">去偏提示遵循</td><td rowspan="2">去偏 CP.PF <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1057" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>动物↑</td><td>人类↑</td><td>对象↑</td><td>总体↑</td><td>实际的。 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1058" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td><td>MMAS。 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1059" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td><td>总体↑</td><td>动物↑</td><td>人类↑</td><td>对象↑</td><td>总体↑</td><td>实际的。 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1060" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td><td>想象。 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1061" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td><td>总体↑</td></tr><tr><td>文本反演</td><td>✘</td><td>0.502</td><td>0.358</td><td>0.305</td><td>0.388</td><td>0.671</td><td>0.437</td><td>0.598</td><td>0.232</td><td>0.741</td><td>0.694</td><td>0.717</td><td>0.722</td><td>0.619</td><td>0.385</td><td>0.541</td><td>0.391</td></tr><tr><td>梦境亭</td><td>✘</td><td>0.640</td><td>0.199</td><td>0.488</td><td>0.442</td><td>0.798</td><td>0.504</td><td>0.692</td><td>0.306</td><td>0.670</td><td>0.362</td><td>0.676</td><td>0.626</td><td>0.750</td><td>0.467</td><td>0.656</td><td>0.411</td></tr><tr><td>梦境亭 LoRA</td><td>✘</td><td>0.751</td><td>0.311</td><td>0.543</td><td>0.535</td><td>0.898</td><td>0.754</td><td>0.849</td><td>0.450</td><td>0.681</td><td>0.675</td><td>0.761</td><td>0.720</td><td>0.865</td><td>0.718</td><td>0.816</td><td>0.588</td></tr><tr><td>BLIP-扩散</td><td>✓</td><td>0.637</td><td>0.557</td><td>0.469</td><td>0.554</td><td>0.581</td><td>0.303</td><td>0.464</td><td>0.257</td><td>0.771</td><td>0.733</td><td>0.745</td><td>0.750</td><td>0.529</td><td>0.266</td><td>0.442</td><td>0.332</td></tr><tr><td>Emu2</td><td>✓</td><td>0.670</td><td>0.546</td><td>0.447</td><td>0.554</td><td>0.732</td><td>0.560</td><td>0.670</td><td>0.371</td><td>0.652</td><td>0.683</td><td>0.701</td><td>0.681</td><td>0.686</td><td>0.494</td><td>0.622</td><td>0.424</td></tr><tr><td>IP-适配器</td><td>✓</td><td>0.667</td><td>0.558</td><td>0.504</td><td>0.576</td><td>0.743</td><td>0.446</td><td>0.607</td><td>0.350</td><td>0.790</td><td>0.764</td><td>0.743</td><td>0.766</td><td>0.695</td><td>0.377</td><td>0.589</td><td>0.451</td></tr><tr><td>IP-Adapter+</td><td>✓</td><td>0.900</td><td>0.845</td><td>0.759</td><td>0.834</td><td>0.502</td><td>0.279</td><td>0.388</td><td>0.324</td><td>0.481</td><td>0.473</td><td>0.530</td><td>0.504</td><td>0.442</td><td>0.229</td><td>0.371</td><td>0.187</td></tr><tr><td>我们的方法</td><td>✓</td><td>0.647</td><td>0.567</td><td>0.640</td><td>0.631</td><td>0.777</td><td>0.625</td><td>0.726</td><td>0.458</td><td>0.852</td><td>0.774</td><td>0.750</td><td>0.789</td><td>0.808</td><td>0.681</td><td>0.757</td><td>0.597</td></tr></tbody></table></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-c3f60dd0-3f27-42c6-ae52-04ba8b92e119" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-6="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-c3f60dd0-3f27-42c6-ae52-04ba8b92e119" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-6="0,0"><div style="height: auto;"><div><div><div>Table 1. Quantitative result. On the human-aligned GPT score metrics, our method is only inferior to IP-Adapter+ [42] for concept preservation (largely because of IP-Adapter families' "copy-pasting" effect) and the tuning-base DreamBooth-LoRA [13,31] for prompt following, but outperforms every other baseline, achieving the best overall performance considering both concept preservation and prompt following. We also note that on the de-biased GPT evaluation, which penalizes "copy-pasting" the reference image without significant creative interpretation or transformation, the advantages of IP-Adaper+ [42] no longer hold. This can also be partly observed by their bad prompt following scores, meaning they are biased towards the reference input and are not accommodating the input prompt. The first, second, and third values are highlighted, where Diffusion Self-Distillation is the best overall performing model.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">表1. 定量结果。在与人类对齐的GPT评分指标上，我们的方法在概念保留方面仅次于IP-Adapter+ [42]（主要是因为IP-Adapter系列的“复制粘贴”效应），在提示跟随方面仅次于基于调优的DreamBooth-LoRA [13,31]，但优于其他所有基线，综合考虑概念保留和提示跟随，取得了最佳的整体表现。我们还注意到，在去偏GPT评估中，该评估惩罚了未经重大创意解释或转化的“复制粘贴”参考图像，IP-Adapter+ [42] 的优势不再成立。这一点也可以部分通过它们较差的提示跟随评分得到体现，意味着它们偏向参考输入，未能充分响应输入提示。第一、第二和第三个数值被突出显示，其中扩散自我蒸馏是最佳的整体表现模型。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-c3f60dd0-3f27-42c6-ae52-04ba8b92e119" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-6="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-c3f60dd0-3f27-42c6-ae52-04ba8b92e119" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-c3f60dd0-3f27-42c6-ae52-04ba8b92e119" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-6="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-c3f60dd0-3f27-42c6-ae52-04ba8b92e119" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-6="0,0"><div style="height: auto;"><div><div><div>Quantitative results. Quantitative comparison with the baselines are shown in Tab. 1, where we report GPT evaluation following DreamBench++ [25]. Such an evaluation protocol is similar to human score but uses automatic multimodal LLMs. Our method achieves the best overall performances accommodating both concept preservation and prompt following, while only being inferior to IP-Adapter+ [42] for the former (mainly because of the "copy-paste" effect again), and the per-instance tuning DreamBooth-LoRA [13, 31] for the latter. We note that the concept preservation evaluation of DreamBench++ [25] is still biased towards favoring a "copy-paste" effect, especially on more challenging and diverse prompts. For instance, the outstanding concept preservation performances of the IP-Adapter family [42] are primarily because of their strong "copy-paste" effect, which copies over the input image without considering relevant essential changes in the prompts. This can also be partly observed by their underperforming prompt following scores, which means they are biased towards the reference input and do not accommodate the input prompt. Therefore, we also present our "de-biased" version of GPT scores, which are as simple as telling GPT to penalize if the generated image resembles a direct copy of the reference image. We observe that the advantages of IP-Adaper+ [42] no longer hold. Overall, Diffusion Self-Distillation is the best-performing model.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">定量结果。与基准方法的定量比较如表1所示，我们根据DreamBench++ [25]进行GPT评估。该评估协议类似于人工评分，但使用了自动化的多模态LLM。我们的方法在概念保持和提示跟随方面取得了最佳的总体表现，尽管在概念保持方面略逊于IP-Adapter+ [42]（主要是由于“复制粘贴”效应），在提示跟随方面仅略逊于按实例调优的DreamBooth-LoRA [13, 31]。我们注意到，DreamBench++ [25]的概念保持评估仍然偏向于支持“复制粘贴”效应，尤其是在更具挑战性和多样化的提示上。例如，IP-Adapter系列 [42] 在概念保持方面的卓越表现，主要是由于其强大的“复制粘贴”效应，这种效应直接复制输入图像，而没有考虑提示中的相关变化。这一点也部分体现在其较差的提示跟随得分上，这表明它们偏向于参考输入，而不考虑输入提示。因此，我们还展示了我们的“去偏”版本的GPT得分，方法简单明了，即告诉GPT，如果生成的图像与参考图像直接相似则进行惩罚。我们观察到，IP-Adapter+ [42] 的优势不再成立。总体而言，扩散自蒸馏是表现最好的模型。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-c3f60dd0-3f27-42c6-ae52-04ba8b92e119" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-6="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-c3f60dd0-3f27-42c6-ae52-04ba8b92e119" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-6="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div>Ablation studies. (1) Data curation: During dataset generation, we first synthesize grids using a frozen pre-trained FLUX model and then filter the images via VLM curation. Why not fine-tune the FLUX model on image grids to improve the hit rate? To study this, we fit a LoRA [13] using <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1062" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3E"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&gt;</mo><mrow data-mjx-texclass="ORD"><mn>7000</mn></mrow></math></mjx-assistive-mml></mjx-container> consistent grids (Fig. 6,left). Though a more significant proportion of samples are consistent grids, we find that the teacher model loses diversity in its output. Therefore, we choose to rely entirely on VLMs to help us curate from large numbers of diverse but potentially noisy grids. (2) Parallel processing architecture: We compare the parallel processing architecture to three alternative image-to-image architectures: 1) concatenating the source image to the noise image ("concatenation"); 2) a ControlNet [43]- based design, and 3) an IP-Adapter [42]-based design. We train each architecture using the same data as our parallel processing model (Fig. 6, middle). For ControlNet [43], we draw the same conclusion as prior work [14], in that it works best for structure-aligned edits, but generally struggles to preserve details when the source image and target image differ in camera pose. IP-Adapter [42] struggles to effectively transfer details and styles from the source image due to the limited capacity of its image encoder. (3) Other image-to-image tasks: Although not "self-distillation", since<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">消融实验。 (1) 数据整理：在数据集生成过程中，我们首先使用冻结的预训练 FLUX 模型合成网格，然后通过 VLM 整理过滤图像。为什么不在图像网格上微调 FLUX 模型以提高命中率？为了研究这一点，我们使用 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1063" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3E"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&gt;</mo><mrow data-mjx-texclass="ORD"><mn>7000</mn></mrow></math></mjx-assistive-mml></mjx-container> 一致网格（图 6，左）拟合 LoRA [13]。尽管更多样本是一致网格，但我们发现教师模型在输出中失去了多样性。因此，我们选择完全依赖 VLM 来帮助我们从大量多样化但可能存在噪声的网格中整理数据。 (2) 并行处理架构：我们将并行处理架构与三种替代的图像到图像架构进行比较：1）将源图像与噪声图像拼接（“拼接”）；2）基于 ControlNet [43] 的设计；3）基于 IP-Adapter [42] 的设计。我们使用与并行处理模型相同的数据训练每个架构（图 6，中）。对于 ControlNet [43]，我们得出与先前工作 [14] 相同的结论，即它在结构对齐编辑中效果最佳，但当源图像和目标图像在摄像机姿势上存在差异时，通常难以保持细节。IP-Adapter [42] 由于其图像编码器的有限容量，难以有效地从源图像中传递细节和风格。 (3) 其他图像到图像任务：尽管不是“自蒸馏”，因为</div></div></div></div></span></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_7.jpg?x=166&amp;y=203&amp;w=1461&amp;h=368"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"><div><div><div>Figure 6. Ablation study. Left: We compare the base model's in-context sampling ability with a consistent grid LoRA-overfitted model. We observe that although applying LoRA to the base model can increase the likelihood of outputs being consistent grids, it may adversely affect output diversity. Therefore, we rely on vision-language models (VLMs) to curate from a large number of diverse but potentially noisy grids. Right: We compare our architectural design with a vanilla conditional model (by adding a few input channels), ControlNet [43], and IP-Adapter [42]. Our architecture learns the input concepts and identities significantly better. We also demonstrate that our architecture can effectively scale to depth-conditioned image generation similar to ControlNet [43].<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 6. 消融研究。左：我们比较了基础模型的上下文采样能力与一致性网格 LoRA 过拟合模型。我们观察到，虽然将 LoRA 应用到基础模型可以增加输出结果一致性网格的可能性，但它可能会对输出的多样性产生不利影响。因此，我们依赖于视觉-语言模型（VLMs）从大量多样化但可能含有噪声的网格中进行筛选。右：我们将我们的架构设计与普通条件模型（通过添加几个输入通道）、ControlNet [43] 和 IP-Adapter [42] 进行了比较。我们的架构在学习输入概念和身份方面表现得明显更好。我们还展示了我们的架构能够有效扩展到类似于 ControlNet [43] 的深度条件图像生成。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"><span style="display: inline;"><div><div class="table-container"><table class="fixed-table"><tbody><tr><td>Method</td><td>CP↑</td><td>PF <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1064" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td><td>Creativity <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1065" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>Textual Inversion [9]</td><td>1.693</td><td>1.924</td><td>2.850</td></tr><tr><td>DreamBooth [31]</td><td>2.329</td><td>2.883</td><td>3.597</td></tr><tr><td>DreamBooth LoRA [13, 31]</td><td>2.576</td><td>3.386</td><td>4.247</td></tr><tr><td>BLIP-Diffusion [17]</td><td>1.854</td><td>2.281</td><td>0.286</td></tr><tr><td>Emu2 [36]</td><td>1.843</td><td>2.096</td><td>2.965</td></tr><tr><td>IP-Adapter [42]</td><td>2.274</td><td>2.307</td><td>3.481</td></tr><tr><td>IP-Adapter+ [42]</td><td>3.733</td><td>1.959</td><td>2.428</td></tr><tr><td>Ours</td><td>3.661</td><td>3.328</td><td>4.453</td></tr></tbody></table></div><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><div class="table-container"><table class="fixed-table"><tbody><tr><td>方法</td><td>CP↑</td><td>PF <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1066" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td><td>创造力 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1067" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>文本反转 [9]</td><td>1.693</td><td>1.924</td><td>2.850</td></tr><tr><td>DreamBooth [31]</td><td>2.329</td><td>2.883</td><td>3.597</td></tr><tr><td>DreamBooth LoRA [13, 31]</td><td>2.576</td><td>3.386</td><td>4.247</td></tr><tr><td>BLIP-Diffusion [17]</td><td>1.854</td><td>2.281</td><td>0.286</td></tr><tr><td>Emu2 [36]</td><td>1.843</td><td>2.096</td><td>2.965</td></tr><tr><td>IP-Adapter [42]</td><td>2.274</td><td>2.307</td><td>3.481</td></tr><tr><td>IP-Adapter+ [42]</td><td>3.733</td><td>1.959</td><td>2.428</td></tr><tr><td>我们的方法</td><td>3.661</td><td>3.328</td><td>4.453</td></tr></tbody></table></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"><div><div><div>Table 2. User study. "CP" refers to concept preservation scores and "PF" refers to prompt following scores. The first, second, and third values are highlighted. Our user study results mostly align with our GPT evaluation, where our Diffusion Self-Distillation is the best overall performing model.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">表 2. 用户研究。“CP”指的是概念保持分数，“PF”指的是提示遵循分数。第一、第二和第三个值被突出显示。我们的用户研究结果大致与 GPT 评估结果一致，其中我们的扩散自蒸馏模型表现最佳。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"><div><div><div>it requires an externally-sourced paired dataset (generated with Depth Anything [40]), we additionally train our architecture on depth-to-image to demonstrate its utility for more general image-to-image tasks (Fig. 6, right).<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">它需要一个外部来源的配对数据集（通过 Depth Anything [40] 生成），我们还在深度到图像任务上训练了我们的架构，以展示它在更通用的图像到图像任务中的应用（图 6，右）。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"><div><div><div>User study. To evaluate the fidelity and prompt consistency of our generated images, we conducted a user study on a random subset of the DreamBench++ [25] test cases, selecting 20 samples. A total of 25 female and 29 male annotators, aged from 22 to 78 (average 34), independently scored each image from 1 to 5 based on three criteria: (1) concept preservation-the consistency with the reference image, (2) prompt alignment-the consistency with the given prompt, and (3) creativity-the level of internal understanding and transformation. The average scores are presented in Tab. 2. Our human annotations closely align with the GPT evaluation, demonstrating that our Diffusion Self-Distillation is slightly behind IP-Adapter+[42] in concept preservation and the inference-stage tuning method DreamBooth-LoRA [13, 31] in prompt alignment. Notably, our model achieved the highest creativity score, while IP-Adapter+ [42] scored lower in this metric due to its "copy-paste" effect. These results further confirm that our Diffusion Self-Distillation offers the<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">用户研究。为了评估我们生成图像的保真度和提示一致性，我们对DreamBench++ [25] 测试集中的一个随机子集进行了用户研究，选择了20个样本。共有25名女性和29名男性标注者，年龄从22岁到78岁不等（平均34岁），每个标注者根据三个标准独立评分每张图像，评分范围为1到5：（1）概念保持——与参考图像的一致性，（2）提示对齐——与给定提示的一致性，（3）创造力——内部理解和转化的水平。平均分数见表2。我们的人工标注与GPT评估结果高度一致，表明我们的扩散自蒸馏方法在概念保持方面略逊色于IP-Adapter+ [42]，在推理阶段调整方法DreamBooth-LoRA [13, 31]上则稍有落后于其提示对齐表现。值得注意的是，我们的模型在创造力评分上取得了最高分，而IP-Adapter+ [42]由于其“复制粘贴”效应，在这一指标上得分较低。这些结果进一步确认了我们的扩散自蒸馏方法提供了最平衡且卓越的整体表现。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"><div><div><div>most balanced and superior overall performance.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">最具平衡性和卓越性的整体表现。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"><h2><div><div>5. Discussion<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">5. 讨论</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"><div><div><div>We present Diffusion Self-Distillation, a zero-shot approach designed to achieve identity adaptation across a wide range of contexts using text-to-image diffusion models without any human effort. Our method effectively transforms zero-shot customized image generation into a supervised task, substantially reducing its difficulty. Empirical evaluations demonstrate that Diffusion Self-Distillation performs comparably to inference-stage tuning techniques while retaining the efficiency of zero-shot methods.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们提出了扩散自蒸馏（Diffusion Self-Distillation）方法，这是一种零-shot方法，旨在使用文本到图像的扩散模型实现跨多种情境的身份适配，无需任何人工努力。我们的方法有效地将零-shot定制化图像生成转化为监督任务，显著降低了其难度。实证评估表明，扩散自蒸馏在性能上与推理阶段调整技术相当，同时保持了零-shot方法的效率。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"><div><div><div>Limitations and future work. Our work focuses on identity-preserving edits of characters, objects, and scene relighting. Future directions could explore additional tasks and use cases. Integration with ControlNet [43], for example, could provide fine-grained and independent control of identity and structure. Additionally, extending our approach from image to video generation is a promising avenue of future work.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">局限性与未来工作。我们的研究重点是角色、物体和场景重光照的身份保持编辑。未来的研究方向可以探索更多任务和使用案例。例如，与ControlNet [43] 的集成可以提供更细粒度和独立的身份与结构控制。此外，将我们的方法从图像扩展到视频生成也是未来研究的一个有前景的方向。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"><div><div><div>Ethics. We are mindful of the potential misuse, particularly in deepfakes. We oppose exploiting our work for purposes that infringe upon ethical standards or privacy.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">伦理。我们意识到潜在的误用，特别是在深度伪造方面。我们反对将我们的工作用于侵犯伦理标准或隐私的目的。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"><div><div><div>Conclusion. Our Diffusion Self-Distillation democratizes content creation, enabling identity-preserving, high-quality, and fast customized image generation that adapts seamlessly to evolving foundational models, significantly expanding the creative boundaries of art, design, and digital storytelling.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">结论。我们的扩散自我蒸馏技术使内容创作民主化，能够实现身份保留、高质量和快速定制的图像生成，能够无缝适应不断发展的基础模型，显著扩展了艺术、设计和数字叙事的创造边界。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-7="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-bd2f88c1-6cdf-47f9-8640-80262b6bfd1e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"><div><div><div>Acknowledgements. This project was in part supported by Google, Kaiber AI, 3bodylabs, ONR N00014-23-1-2355, and NSF RI #2211258. ERC was supported by the Nvidia Graduate Fellowship and the Snap Research Fellowship. YZ was in part supported by the Stanford Interdisciplinary Graduate Fellowship.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">致谢。本项目部分得到了谷歌、Kaiber AI、3bodylabs、ONR N00014-23-1-2355 和 NSF RI #2211258 的支持。ERC 得到了英伟达研究生奖学金和 Snap 研究奖学金的支持。YZ 部分得到了斯坦福跨学科研究生奖学金的支持。</div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><h2><div><div>References<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">参考文献</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. In arXiv, 2023. 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, 和 Robin Rombach. 稳定视频扩散：将潜在视频扩散模型扩展到大规模数据集。在 arXiv，2023。4</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div>[2] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-structpix2pix: Learning to follow image editing instructions. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1068" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>C</mi><mi>V</mi><mi>P</mi><mi>R</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2023</mn></mrow><mn>.2</mn><mo>,</mo><mn>6</mn><mo>,</mo><mn>7</mn><mo>,</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[2] Tim Brooks, Aleksander Holynski, 和 Alexei A. Efros. In-structpix2pix：学习遵循图像编辑指令。在 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1069" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>C</mi><mi>V</mi><mi>P</mi><mi>R</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2023</mn></mrow><mn>.2</mn><mo>,</mo><mn>6</mn><mo>,</mo><mn>7</mn><mo>,</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container></div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. https://openai.com/research/video-generation-models-as-world-simulators, 2024. 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, 和 Aditya Ramesh. 视频生成模型作为世界模拟器。https://openai.com/research/video-generation-models-as-world-simulators, 2024。4</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[4] Shengqu Cai, Eric Ryan Chan, Songyou Peng, Mohamad Shahbazi, Anton Obukhov, Luc Van Gool, and Gordon Wet-zstein. Diffdreamer: Towards consistent unsupervised single-view scene extrapolation with conditional diffusion models. In ICCV, 2023.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[4] Shengqu Cai, Eric Ryan Chan, Songyou Peng, Mohamad Shahbazi, Anton Obukhov, Luc Van Gool, 和 Gordon Wet-zstein。Diffdreamer：利用条件扩散模型进行一致的无监督单视角场景外推。在 ICCV, 2023。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[5] Shengqu Cai, Duygu Ceylan, Matheus Gadelha, Chun-Hao Huang, Tuanfeng Wang, and Gordon. Wetzstein. Generative rendering: Controllable 4d-guided video generation with 2d diffusion models. In CVPR, 2024. 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[5] Shengqu Cai, Duygu Ceylan, Matheus Gadelha, Chun-Hao Huang, Tuanfeng Wang, 和 Gordon Wetzstein。生成渲染：使用二维扩散模型进行可控的 4D 引导视频生成。在 CVPR, 2024。4</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 5<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, 和 Armand Joulin。自监督视觉变换器中的新兴特性。在 ICCV, 2021。5</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[7] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William W Cohen. Subject-driven text-to-image generation via apprenticeship learning. In NeurIPS, 2023. 3, 5<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[7] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, 和 William W Cohen。通过学徒学习进行以主题为驱动的文本到图像生成。在 NeurIPS, 2023。3, 5</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[8] Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[8] Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, 和 Robin Rombach。扩展整流流变换器以进行高分辨率图像合成。在 ICML, 2024。4</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div>[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1070" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>I</mi><mi>C</mi><mi>L</mi><mi>R</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2023.5</mn></mrow><mo>,</mo><mn>6</mn><mo>,</mo><mn>8</mn></math></mjx-assistive-mml></mjx-container><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, 和 Daniel Cohen-or. 一幅图胜过千言万语：使用文本反演个性化文本到图像生成。发表于 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1071" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>I</mi><mi>C</mi><mi>L</mi><mi>R</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2023.5</mn></mrow><mo>,</mo><mn>6</mn><mo>,</mo><mn>8</mn></math></mjx-assistive-mml></mjx-container></div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In arXiv, 2023. 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, 和 Bo Dai. Animatediff: 无需特定调整即可动画化您的个性化文本到图像扩散模型。发表于 arXiv，2023年。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[11] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. In arXiv, 2022.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[11] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, 和 Qifeng Chen. 潜在视频扩散模型用于高保真长视频生成。发表于 arXiv，2022年。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[12] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In arXiv, 2022. 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[12] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, 和 Jie Tang. Cogvideo: 通过变换器的大规模预训练用于文本到视频生成。发表于 arXiv，2022年。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[13] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 2, 3, 5, 6, 7, 8<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[13] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. Lora: 大型语言模型的低秩适配。发表于 ICLR，2022年。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[14] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In arXiv, 2023. 4, 7<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[14] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, 和 Liefeng Bo. Animate anyone: 一致且可控的人物动画图像到视频合成。发表于 arXiv，2023年。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[15] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Huanzhang Dou, Yupeng Shi, Yutong Feng, Chen Liang, Yu Liu, and Jingren Zhou. Group diffusion transformers are unsupervised multitask learners. In arXiv, 2024. 3<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[15] 黄亮华, 王伟, 吴志凡, 窦焕章, 史宇鹏, 冯宇彤, 梁晨, 刘宇, 周景仁. 群体扩散变换器是无监督的多任务学习者. 发表在 arXiv, 2024. 3</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[16] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hong-sheng Li, Leonidas Guibas, and Gordon. Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. In NeurIPS, 2024. 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[16] 匡正飞, 蔡胜曲, 何浩, 许英豪, 李洪生, 古巴斯, 和戈登·韦茨坦. 协作视频扩散：具有相机控制的一致多视频生成. 发表在 NeurIPS, 2024. 4</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[17] Dongxu Li, Junnan Li, and Steven Hoi. BLIP-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. In NeurIPS, 2023. 5, 6, 8<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[17] 李东旭, 李俊南, 和 史蒂文·霍伊. BLIP-扩散：用于可控文本到图像生成和编辑的预训练主题表示. 发表在 NeurIPS, 2023. 5, 6, 8</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[18] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. In arXiv, 2023. 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[18] 李佳豪, 谷浩, 张凯, 许泽翔, 阮富军, 许英豪, 洪怡聪, 卡利扬·孙卡瓦利, 格雷格·沙赫纳罗维奇, 和 蔡比. Instant3d：快速文本到3D生成，具有稀疏视图生成和大型重建模型. 发表在 arXiv, 2023. 4</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div>[19] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen. Controlnet++: Improving conditional controls with efficient consistency feedback. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1072" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2024.3</mn></mrow></math></mjx-assistive-mml></mjx-container><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[19] 李明, 杨涛建南, 况华峰, 吴杰, 王兆宁, 肖雪峰, 和 陈. Controlnet++：通过高效一致性反馈改善条件控制. 发表在 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1073" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2024.3</mn></mrow></math></mjx-assistive-mml></mjx-container></div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[20] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[20] 伊利亚·洛希奇洛夫 和 弗兰克·胡特. 解耦权重衰减正则化. 发表在 ICLR, 2019. 4</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[21] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In SIGGRAPH, 2024. 3<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[21] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: 开放领域个性化文本到图像生成，无需在测试时微调。发表于 SIGGRAPH, 2024. 3</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[22] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. 2<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[22] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: 基于随机微分方程的引导图像合成与编辑。发表于 ICLR, 2022. 2</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[23] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In AAAI, 2024. 3<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[23] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: 学习适配器以挖掘更多可控能力用于文本到图像的扩散模型。发表于 AAAI, 2024. 3</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In arXiv, 2021. 1<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: 基于文本引导的扩散模型，朝着摄影级图像生成与编辑的目标迈进。发表于 arXiv, 2021. 1</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[25] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: A human-aligned benchmark for personalized image generation. In arXiv, 2023. 5, 7, 8, 1, 2<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[25] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: 一个与人类对齐的个性化图像生成基准。发表于 arXiv, 2023. 5, 7, 8, 1, 2</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[26] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T Barron, Amit H Bermano, Eric Ryan Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, et al. State of the art on diffusion models for visual computing. In arXiv, 2023. 3<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[26] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T Barron, Amit H Bermano, Eric Ryan Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, 等人。关于视觉计算的扩散模型的最新进展。发表于arXiv，2023年。3</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-8="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-4c622730-34ad-4d6e-9c26-c198ce8ec3f9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"><div><div><div>[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In CoRR, 2021. 5<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, 和 Ilya Sutskever。通过自然语言监督学习可迁移的视觉模型。发表于CoRR，2021年。5</div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. In arXiv, 2022. 1<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, 和 Mark Chen。基于CLIP潜变量的分层文本条件图像生成。发表于arXiv，2022年。1</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, 和 Björn Ommer。使用潜扩散模型进行高分辨率图像合成。发表于CVPR，2022年。1</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[30] Ciara Rowles, Shimon Vainer, Dante De Nigris, Slava Elizarov, Konstantin Kutsy, and Simon Donné. Ipadapter-instruct: Resolving ambiguity in image-based conditioning using instruct prompts. In arXiv, 2024. 3, 5<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[30] Ciara Rowles, Shimon Vainer, Dante De Nigris, Slava Elizarov, Konstantin Kutsy, 和 Simon Donné。Ipadapter-instruct：利用指令提示解决基于图像的条件设定中的歧义问题。发表于arXiv，2024年。3, 5</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div>[31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1074" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>C</mi><mi>V</mi><mi>P</mi><mi>R</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2023</mn></mrow><mn>.2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>6</mn><mo>,</mo><mn>7</mn><mo>,</mo><mn>8</mn></math></mjx-assistive-mml></mjx-container><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, 和 Kfir Aberman。Dreambooth：针对主题驱动生成的文本到图像扩散模型微调。发表于<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1075" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>C</mi><mi>V</mi><mi>P</mi><mi>R</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>2023</mn></mrow><mn>.2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>6</mn><mo>,</mo><mn>7</mn><mo>,</mo><mn>8</mn></math></mjx-assistive-mml></mjx-container></div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. 1<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, 和 Mohammad Norouzi. 具有深度语言理解的逼真文本到图像扩散模型. 发表在 NeurIPS, 2022. 1</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[33] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-man, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. 2, 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[33] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-man, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, 和 Jenia Jitsev. LAION-5b: 一个用于训练下一代图像-文本模型的开放大规模数据集. 发表在 NeurIPS, 2022. 2, 4</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[34] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun, and Yebin Liu. Human4dit: 360-degree human video generation with 4d diffusion transformer. In SIGGRAPH Asia, 2024. 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[34] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun, 和 Yebin Liu. Human4dit: 基于 4D 扩散变换器的360度人类视频生成. 发表在 SIGGRAPH Asia, 2024. 4</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[35] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In arXiv, 2023. 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[35] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, 和 Xiao Yang. Mvdream: 用于 3D 生成的多视角扩散方法. 发表在 arXiv, 2023. 4</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[36] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In CVPR, 2024. 5, 7, 8<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[36] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, 和 Xinlong Wang. 生成多模态模型是上下文学习者。发表于 CVPR, 2024. 5, 7, 8</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[37] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. In arXiv, 2024. 2, 3<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[37] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, 和 Anthony Chen. Instantid: 零样本身份保留生成，秒级完成。发表于 arXiv, 2024. 2, 3</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In NeurIPS, 2022. 4, 1<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V Le, 和 Denny Zhou. 思维链提示引发大型语言模型的推理。发表于 NeurIPS, 2022. 4, 1</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[39] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, and Kai Zhang. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. In arXiv, 2023. 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[39] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, 和 Kai Zhang. DMV3D: 使用 3D 大规模重建模型的去噪多视角扩散。发表于 arXiv, 2023. 4</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[40] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. 8<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[40] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, 和 Hengshuang Zhao. 深度万物: 释放大规模无标签数据的力量。发表于 CVPR, 2024. 8</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[41] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-han Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In arXiv, 2024. 4<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[41] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-han Zhang, Guanyu Feng 等人。Cogvideox：带有专家转换器的文本到视频扩散模型。发表于 arXiv，2024年。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[42] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. In arXiv, 2023. 2, 3, 4, 5, 7, 8<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[42] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, 和 Wei Yang。Ip-adapter：适用于文本到图像扩散模型的文本兼容图像提示适配器。发表于 arXiv，2023年。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[43] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 2, 3, 4, 7, 8<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[43] Lvmin Zhang, Anyi Rao, 和 Maneesh Agrawala。为文本到图像扩散模型添加条件控制。发表于 ICCV，2023年。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-9="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7126a612-c218-40a7-a674-e2c8614b3c06" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"><div><div><div>[44] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K. Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. In NeurIPS, 2023. 3<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[44] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, 和 Kwan-Yee K. Wong。Uni-controlnet：一体化控制文本到图像扩散模型。发表于 NeurIPS，2023年。</div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h1><div><div>Diffusion Self-Distillation for Zero-Shot Customized Image Generation<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">扩散自蒸馏：用于零-shot定制图像生成</div></div></div></h1></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>Supplementary Material<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">附加材料</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h2><div><div>A. Data Pipeline Prompts<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">A. 数据管道提示</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>In this section, we list out the detailed prompts used in our data generation (Sec. A.1), curation (Sec. A.2) and caption (Sec. A.3) pipelines.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">在本节中，我们列出了在数据生成（A.1节）、数据策划（A.2节）和标题生成（A.3节）管道中使用的详细提示。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h3><div><div>A.1. Data Generation Prompts<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">A.1. 数据生成提示</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>To generate grid prompts, we employ GPT-4o as our language model (LLM) engine We instruct the LLM to focus on specific aspects during the grid generation process: preserving the identity of the subject, providing detailed content within each grid quadrant, and maintaining appropriate text length. However, we observed that not all sampled reference captions inherently include a clear instance suitable for identity preservation. To address this issue, we introduce an initial filtering stage to ensure that each sampled reference caption contains an identity-preserving target. This filtering enhances the quality and consistency of the generated grids.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">为了生成网格提示，我们采用了 GPT-4o 作为语言模型 (LLM) 引擎。我们指示 LLM 在网格生成过程中专注于特定方面：保持主题的身份，提供每个网格象限内的详细内容，并保持适当的文本长度。然而，我们发现并非所有采样的参考标题都能固有地包含适合身份保持的明确实例。为了解决这一问题，我们引入了一个初步筛选阶段，确保每个采样的参考标题都包含一个保持身份的目标。此筛选过程提高了生成网格的质量和一致性。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h2><div><div>Grid Generation Prompts<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">网格生成提示</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h2><div><div>User Prompt:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">用户提示：</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>Please be very creative and generate a prompt for text-to-image generation using flux, the prompt should create an evenly seperated grid of four. The four quadrants depict an identical item/asset/character under different environments/camera views/lighting conditions, etc (please be very very creative here). Every prompt should specify what the top-left, top-right, bottom-left, bottom-right quadrant depicts. Extract the asset from the following caption: <sampled_reference_caption><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">请发挥创意，生成一个用于文本到图像生成的提示，使用 flux，提示应创建一个均匀分隔的四分网格。四个象限展示相同的物品/资产/角色，在不同的环境/镜头视角/光照条件下等（请在这里尽量发挥创意）。每个提示应指定左上角、右上角、左下角和右下角各自展示的内容。从以下标题中提取资产：<sampled_reference_caption></sampled_reference_caption></div></sampled_reference_caption></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h2><div><div>System Prompt:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">系统提示：</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>Response only the required prompt. Keep the fomat as one line and be as short and precise as possible, do not exceed 77 tokens. Be very creative! It could be a four-panel comic strip, a four-panel manga, real images, etc. The prompt should start with 'a grid of ...'<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">仅响应所需的提示。保持格式为单行，尽可能简短且精准，不超过 77 个词。请非常富有创意！可以是四格漫画、一部四格漫画、真实图像等。提示应以“a grid of ...”开始</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h3><div><div>A.2. Data Curation Prompts<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">A.2. 数据策划提示</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>For data curation, we employ Gemini-1.5. To guide the vision-language model (VLM) in focusing on identity preservation, we utilize Chain-of-Thought (CoT) prompting [38]. Specifically, we first instruct the VLM to identify the common object or character present in both images. Next, we prompt it to describe each one in detail. Finally, we ask the VLM to analyze whether they are identical and to provide a conclusive response. We find that this CoT prompting significantly enhances the model's ability to concentrate on the identity and intricate details of the target object or character.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">对于数据整理，我们使用了 Gemini-1.5。为了引导视觉-语言模型（VLM）专注于身份保持，我们采用了连锁思维（CoT）提示 [38]。具体来说，我们首先指示 VLM 确定两张图片中共同存在的物体或角色。接着，我们提示它详细描述每一张图片。最后，我们要求 VLM 分析它们是否完全相同，并给出最终的回答。我们发现，这种 CoT 提示显著提高了模型专注于目标物体或角色的身份和细节的能力。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h2><div><div>Data Curation Prompts<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">数据整理提示</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>CoT Step 1:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">CoT 步骤 1:</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>Can you identity a common character/asset/item in the two images?<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">你能识别出两张图片中共同的角色/物品/资产吗？</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>CoT Step 2:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">CoT 步骤 2:</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>Could you describe to me what the character/asset/item looks like in detail in the two images?<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">你能详细描述两张图片中角色/物品/资产的外观吗？</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>CoT Step 3:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">CoT 步骤 3:</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>Do the two images depict identical character/asset/item presented under different poses/lighting conditions/camera views/environment/etc.? Please consider this in terms of character/asset/item identity and be extremely critical. Could you describe to me what the common character/asset/item looks like in detail if it is indeed the same? End the response with a single 'yes' or 'no',<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">这两张图片是否呈现相同的角色/物品/资产，只是姿势/光照条件/拍摄角度/环境等有所不同？请从角色/物品/资产的身份角度进行严格评估。如果它们确实是同一个对象，你能详细描述一下它的外观吗？请用“是”或“不是”来结束你的回答。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h3><div><div>A.3. Image Caption Prompts<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">A.3. 图片说明提示</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>We provide two methods for prompting our model: using the description of the expected output (Target Description) or InstructPix2Pix [2]-type instructions (Instruction).<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们为模型提供了两种提示方法：使用期望输出的描述（目标描述）或 InstructPix2Pix [2] 类型的指令（指导）。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h2><div><div>Image Caption Prompts<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图片说明提示</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h2><div><div>Target Description:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">目标描述：</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>Please provide a prompt for the image for Diffusion Model text-to-image generative model training, i.e. for FLUX or StableDiffusion 3. The prompt should be a detailed description of the image, including the character/asset/item, the environment, the pose, the lighting, the camera view, etc. The prompt should be detailed enough to generate the image. The prompt should be as short and precise as possible, in one-line format, and do not exceed 77 tokens.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">请为 Diffusion Model 文本生成图像模型训练提供一个提示词，即 FLUX 或 StableDiffusion 3。提示词应是对图像的详细描述，包括角色/资产/物品、环境、姿势、光照、相机视角等。提示词应足够详细，以生成图像。提示词应尽可能简洁精确，采用一行格式，并且不超过 77 个 token。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h2><div><div>Instruction:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">指令：</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>Please provide a caption/prompt for the purpose of image-to-image editing, so that the prompt edits the first image into the second image. Do not include terms such as 'transform', 'image', etc.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">请提供一个用于图像到图像编辑的标题/提示词，以便该提示词将第一张图像编辑成第二张图像。请勿包含如“变换”、“图像”等术语。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h3><div><div>B.GPT Evaluation Prompts<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">B.GPT 评估提示</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>We closely follow DreamBench++ [25] in terms of our GPT evaluation. In Fig. 7, we demonstrate the prompts we use for evaluation, including our "de-biased" evaluation that penalizes "copy-pasting" effect.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们的 GPT 评估紧密遵循 DreamBench++ [25]。在图 7 中，我们展示了用于评估的提示词，包括我们“去偏差”评估，该评估惩罚“复制粘贴”效应。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h2><div><div>C. Additional Results<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">C. 额外结果</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h3><div><div>C.1. Additional Qualitative Comparisons<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">C.1. 额外的定性比较</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>In Fig. 8, we demonstrate more of the qualitative evaluation cases from the DreamBench++ [25] benchmark.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">在图 8 中，我们展示了来自 DreamBench++ [25] 基准测试的更多定性评估案例。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><h3><div><div>C.2. Additional Qualitative Results<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">C.2. 额外的定性结果</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-10="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-0c1646a7-7026-465b-ac04-348a83f295ff" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"><div><div><div>Due to space constraints in the main paper, we presented shortened prompts. Here, we provide additional qualitative results in Fig. 9, Fig. 10, Fig. 11, Fig. 12, Fig. 13 and Fig. 14, including the full prompts used for their generation. These detailed captions capture various aspects of the images and offer deeper insights into how our model operates.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">由于主文中的篇幅限制，我们展示了缩短的提示词。在此，我们提供了图 9、图 10、图 11、图 12、图 13 和图 14 中的额外定性结果，包括用于生成这些结果的完整提示词。这些详细的标题捕捉了图像的各个方面，并提供了更深入的见解，帮助理解我们的模型是如何运作的。</div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-527fe261-c498-4521-871a-84f448bd918d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-11="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-527fe261-c498-4521-871a-84f448bd918d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-527fe261-c498-4521-871a-84f448bd918d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-11="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-527fe261-c498-4521-871a-84f448bd918d" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-11="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_11.jpg?x=165&amp;y=204&amp;w=1470&amp;h=1474"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-527fe261-c498-4521-871a-84f448bd918d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-11="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-527fe261-c498-4521-871a-84f448bd918d" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-11="0,0"><div style="height: auto;"><div><div><div>Figure 7. GPT evaluation prompts used across our evaluation, where the left shows the vanilla prompts from DreamBench++ [25] and the right shows our modified "de-biased" prompts, which strongly penalizes "copy-pasting" effects without sufficient creative inputs. We highlight our modified sentences in red.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 7. 我们评估中使用的 GPT 评估提示，左侧展示了来自 DreamBench++ [25] 的原始提示，右侧展示了我们修改后的“去偏见”提示，该提示对“复制粘贴”效应进行强烈惩罚，要求有足够的创意输入。我们将修改后的句子用红色突出显示。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-527fe261-c498-4521-871a-84f448bd918d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-11="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-527fe261-c498-4521-871a-84f448bd918d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-527fe261-c498-4521-871a-84f448bd918d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-11="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-527fe261-c498-4521-871a-84f448bd918d" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-11="0,0"><div style="height: auto;"><h3><div><div>C.3. Story Telling<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">C.3. 故事讲述</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-527fe261-c498-4521-871a-84f448bd918d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-11="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-527fe261-c498-4521-871a-84f448bd918d" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-11="0,0"><div style="height: auto;"><div><div><div>Our model exhibits the capability to generate simple comics and manga narratives, as demonstrated in Fig. 15 and Fig. 16, where the conditioning image acts as the first panel. To create these storytelling sequences, we input the initial panel into GPT-4o, which generates a series of prompts centered around the main character from the input image. These prompts are crafted to form a coherent story spanning 8-10 panels, with each prompt being contextually meaningful on its own. Utilizing these prompts alongside the conditioning image, we generate the subsequent panels and finally align them to reconstruct a cohesive narrative.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们的模型展示了生成简单漫画和漫画叙事的能力，如图 15 和图 16 所示，其中条件图像作为第一面板。为了创建这些故事讲述序列，我们将初始面板输入到 GPT-4o 中，它会围绕输入图像中的主要角色生成一系列提示。这些提示旨在形成一个贯穿 8-10 个面板的连贯故事，每个提示都在上下文中具有独立的意义。通过将这些提示与条件图像结合使用，我们生成后续的面板，并最终将它们对齐，以重建一个连贯的叙事。</div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-12="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-12="0,0"><div style="height: auto;"><h2><div><div>Story Telling Prompts<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">故事讲述提示</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-12="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-12="0,0"><div style="height: auto;"><h2><div><div>Step 1: Identify Main Character<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">步骤 1：识别主要角色</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-12="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-12="0,0"><div style="height: auto;"><div><div><div>Please provide a prompt for the image for Diffusion Model text-to-image generative model training, i.e. for FLUX or StableDiffusion 3. The prompt should be a detailed description of the image, including the character/asset/item, the environment, the pose, the lighting, the camera view, etc. The prompt should be detailed enough to generate the image. The prompt should be as short and precise as possible, in one-line format, and does not exceed 77 tokens.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">请提供一个提示，用于 Diffusion 模型文本到图像生成模型的训练，即 FLUX 或 StableDiffusion 3。提示应详细描述图像的内容，包括角色/资产/物品、环境、姿势、光线、相机视角等。提示应足够详细，以生成图像。提示应尽可能简短精确，采用单行格式，并且不超过 77 个 token。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-12="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-12="0,0"><div style="height: auto;"><h2><div><div>Step 2: Coherent Story Generation<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">步骤 2：连贯的故事生成</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-12="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-12="0,0"><div style="height: auto;"><div><div><div>Can you generate a series of prompts using the main character? The series of prompts should form a coherent story of 8-10 panels.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">你能生成一系列使用主要角色的提示吗？这些提示应形成一个贯穿 8-10 个面板的连贯故事。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-12="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-12="0,0"><div style="height: auto;"><h2><div><div>Step 3: Prompts Generation<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">步骤 3：提示生成</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-12="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-12="0,0"><div style="height: auto;"><div><div><div>Can you transfer the prompts so that each of them is individually sound?<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">你能将提示转化，使每个提示都在上下文中合理吗？</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-12="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-12="0,0"><div style="height: auto;"><h2><div><div>D. Discussion on Scalability<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">D. 关于可扩展性的讨论</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-12="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-63e284c2-4cb6-4eed-bcc2-533219230f6d" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-12="0,0"><div style="height: auto;"><div><div><div>We acknowledge that the scalability of Diffusion Self-Distillation is not fully explored within the scope of this paper. However, we posit that Diffusion Self-Distillation is inherently scalable along three key dimensions. First, Diffusion Self-Distillation can scale with advancements in the teacher model's grid generation capabilities and its in-context understanding of identity preservation. Second, the scalability extends to the range of tasks we leverage; while this paper focuses on general adaptation tasks, a broader spectrum of applications remains open for exploration. Third, Diffusion Self-Distillation scales with the extent to which we harness foundation models. Increased diversity and more meticulously curated data contribute to improved generalization of our model. As foundation models-including base text-to-image generation models, language models (LLMs), and vision-language models (VLMs)-continue to evolve, Diffusion Self-Distillation naturally benefits from these advancements without necessitating any modifications to the existing workflow. A direct next step involves scaling the method to incorporate a significantly larger dataset and integrating forthcoming, more advanced foundation models.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们承认，在本文的范围内，扩展性问题尚未完全探讨。然而，我们认为扩散自蒸馏（Diffusion Self-Distillation）本质上在三个关键维度上是可扩展的。首先，扩散自蒸馏可以随着教师模型的网格生成能力和其在上下文中对身份保持的理解进展而扩展。第二，扩展性还体现在我们所利用的任务范围；虽然本文主要集中在通用适应任务上，但更广泛的应用领域仍待探索。第三，扩散自蒸馏随着我们利用基础模型的程度而扩展。增加的数据多样性和更精心策划的数据有助于模型的泛化。随着基础模型的不断发展，包括基础的文本到图像生成模型、语言模型（LLMs）和视觉语言模型（VLMs），扩散自蒸馏自然会从这些进展中受益，而无需对现有工作流进行任何修改。下一步直接的工作是将该方法扩展到包含一个更大数据集，并整合即将发布的、更先进的基础模型。</div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-cb9b71be-f332-4188-b531-43afec6ed0ec" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-13="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-cb9b71be-f332-4188-b531-43afec6ed0ec" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-cb9b71be-f332-4188-b531-43afec6ed0ec" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-13="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-cb9b71be-f332-4188-b531-43afec6ed0ec" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-13="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_13.jpg?x=159&amp;y=297&amp;w=1477&amp;h=1625"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-cb9b71be-f332-4188-b531-43afec6ed0ec" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-13="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-cb9b71be-f332-4188-b531-43afec6ed0ec" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-13="0,0"><div style="height: auto;"><div><div><div>Figure 8. Additional qualitative comparison.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 8. 额外的定性比较。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-cb9b71be-f332-4188-b531-43afec6ed0ec" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-13="0,0"></paragraphpositioning></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-6d6fb40d-a4ca-4ea1-a2c6-b9d1be014919" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-14="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_14.jpg?x=155&amp;y=284&amp;w=1493&amp;h=1647"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-6d6fb40d-a4ca-4ea1-a2c6-b9d1be014919" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-14="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-6d6fb40d-a4ca-4ea1-a2c6-b9d1be014919" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-14="0,0"><div style="height: auto;"><div><div><div>Figure 9. Additional character identity preserving results.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 9. 额外的角色身份保持结果。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-6d6fb40d-a4ca-4ea1-a2c6-b9d1be014919" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-14="0,0"></paragraphpositioning></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f1429240-9f39-4789-9be9-976e0cc75026" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-15="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_15.jpg?x=161&amp;y=284&amp;w=1480&amp;h=1645"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f1429240-9f39-4789-9be9-976e0cc75026" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-15="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f1429240-9f39-4789-9be9-976e0cc75026" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-15="0,0"><div style="height: auto;"><div><div><div>Figure 10. Additional character identity preserving results.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 10. 额外的角色身份保持结果。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f1429240-9f39-4789-9be9-976e0cc75026" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-15="0,0"></paragraphpositioning></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="17" id="mark-dfc0c099-af30-4acd-bd3f-f2e2ea73cb5f" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-16="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_16.jpg?x=161&amp;y=291&amp;w=1481&amp;h=1631"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="17" id="mark-dfc0c099-af30-4acd-bd3f-f2e2ea73cb5f" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-16="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="17" id="mark-dfc0c099-af30-4acd-bd3f-f2e2ea73cb5f" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-16="0,0"><div style="height: auto;"><div><div><div>Figure 11. Additional object/item identity preserving results.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 11. 额外的物体/项目身份保持结果。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="17" id="mark-dfc0c099-af30-4acd-bd3f-f2e2ea73cb5f" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-16="0,0"></paragraphpositioning></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="18" id="mark-42b45926-2e41-4823-9dc8-b6d6be257d88" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-17="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_17.jpg?x=159&amp;y=302&amp;w=1486&amp;h=1615"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="18" id="mark-42b45926-2e41-4823-9dc8-b6d6be257d88" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-17="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="18" id="mark-42b45926-2e41-4823-9dc8-b6d6be257d88" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-17="0,0"><div style="height: auto;"><div><div><div>Figure 12. Additional object/item identity preserving results.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 12. 额外的物体/项目身份保持结果。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="18" id="mark-42b45926-2e41-4823-9dc8-b6d6be257d88" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-17="0,0"></paragraphpositioning></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="19" id="mark-c4eafc2e-e31f-4c78-8520-e706c62df0b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-18="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_18.jpg?x=165&amp;y=421&amp;w=1466&amp;h=1374"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="19" id="mark-c4eafc2e-e31f-4c78-8520-e706c62df0b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-18="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="19" id="mark-c4eafc2e-e31f-4c78-8520-e706c62df0b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-18="0,0"><div style="height: auto;"><div><div><div>Figure 13. Additional instruction prompting results.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 13. 额外的指令提示结果。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="19" id="mark-c4eafc2e-e31f-4c78-8520-e706c62df0b3" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-18="0,0"></paragraphpositioning></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="20" id="mark-693760d6-9016-4d0a-85db-73d9d9c49000" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-19="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_19.jpg?x=166&amp;y=456&amp;w=1468&amp;h=1307"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="20" id="mark-693760d6-9016-4d0a-85db-73d9d9c49000" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-19="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="20" id="mark-693760d6-9016-4d0a-85db-73d9d9c49000" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-19="0,0"><div style="height: auto;"><div><div><div>Figure 14. Additional relighting results.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 14. 额外的重光照结果。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="20" id="mark-693760d6-9016-4d0a-85db-73d9d9c49000" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-19="0,0"></paragraphpositioning></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-4363eeeb-aca2-4fd3-9e42-a86440d5d74e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-20="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_20.jpg?x=152&amp;y=197&amp;w=1518&amp;h=1868"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-4363eeeb-aca2-4fd3-9e42-a86440d5d74e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-20="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-4363eeeb-aca2-4fd3-9e42-a86440d5d74e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-4363eeeb-aca2-4fd3-9e42-a86440d5d74e" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-20="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-4363eeeb-aca2-4fd3-9e42-a86440d5d74e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-20="0,0"><div style="height: auto;"><div><div><div>Figure 15. Comic generation example 1. The conditioned image is the first panel.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 15. 漫画生成示例 1。条件图像为第一面板。</div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-0282bbc0-bf27-4e23-a5e7-656f2fb6b8c4" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-21="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-0282bbc0-bf27-4e23-a5e7-656f2fb6b8c4" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-0282bbc0-bf27-4e23-a5e7-656f2fb6b8c4" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-21="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-0282bbc0-bf27-4e23-a5e7-656f2fb6b8c4" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-21="0,0"><div style="height: auto;"><div><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01937d56-a182-7714-8c80-77ff2e641f51_21.jpg?x=140&amp;y=211&amp;w=1502&amp;h=1849"></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-0282bbc0-bf27-4e23-a5e7-656f2fb6b8c4" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-21="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-0282bbc0-bf27-4e23-a5e7-656f2fb6b8c4" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-0282bbc0-bf27-4e23-a5e7-656f2fb6b8c4" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-21="0,0"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-0282bbc0-bf27-4e23-a5e7-656f2fb6b8c4" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-21="0,0"><div style="height: auto;"><div><div><div>Figure 16. Comic generation example 2. The conditioned image is the first panel.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 16. 漫画生成示例 2。条件图像为第一面板。</div></div></div></div></div></div></div></div></div></div>
      </body>
    </html>
  