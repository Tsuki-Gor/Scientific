
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <h1>PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>PromptStyler: 基于提示的风格生成用于无源领域泛化</h1></div><p>\({}^{2}\) POSTECH</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{2}\) POSTECH</p></div><p><a href="https://PromptStyler.github.io">https://PromptStyler.github.io</a></p><h2>Abstract</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>摘要</h2></div><p>In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Also, a recent study has demonstrated the cross-modal transferability phenomenon of this joint space. From these observations, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. The proposed method learns to generate a variety of style features (from "a \({S}_{ * }\) style of a") via learnable style word vectors for pseudo-words \({\mathbf{S}}_{ * }\) . To ensure that learned styles do not distort content information, we force style-content features (from "a \({S}_{ * }\) style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, even though it does not require any images for training.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在一个联合视觉-语言空间中，文本特征（例如，“一张狗的照片”）可以有效地表示其相关的图像特征（例如，来自狗照片）。此外，最近的研究已经证明了这个联合空间的跨模态可转移性现象。基于这些观察，我们提出了PromptStyler，它通过合成多样的风格来模拟联合空间中的各种分布变化，而无需使用任何图像来处理无源领域泛化。所提出的方法通过可学习的风格词向量为伪词\({\mathbf{S}}_{ * }\)生成多种风格特征（来自“一种\({S}_{ * }\)风格的”）。为了确保学习到的风格不会扭曲内容信息，我们强制风格-内容特征（来自“一种\({S}_{ * }\)风格的[class]”）在联合视觉-语言空间中靠近其对应的内容特征（来自“[class]”）。在学习风格词向量后，我们使用合成的风格-内容特征训练线性分类器。尽管不需要任何图像进行训练，PromptStyler在PACS、VLCS、OfficeHome和DomainNet上达到了最先进的水平。</p></div><h2>1. Introduction</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>1. 引言</h2></div><p>Deep neural networks are usually trained with the assumption that training and test data are independent and identically distributed, which makes them vulnerable to substantial distribution shifts between training and test data \(\left\lbrack  {{23},{52}}\right\rbrack\) . This susceptibility is considered as one of the major obstacles to their deployment in real-world applications. To enhance their robustness to such distribution shifts, Domain Adaptation (DA) \(\left\lbrack  {2,{24},{32},{33},{54},{56},{57},{68}}\right\rbrack\) has been studied; it aims at adapting neural networks to a target domain using target domain data available in training. However, such a target domain is often latent in common training scenarios, which considerably limits the application of DA. Recently, a body of research has addressed this limitation by Domain Generalization (DG) \(\left\lbrack  {3,5,{21},{29},{35},{37},{74}}\right\rbrack\) that aims to improve model's generalization capability to any unseen domains. It has been a common practice in DG to utilize multiple source domains for learning domain-invariant features [61,69], but it is unclear which source domains are ideal for DG, since arbitrary unseen domains should be addressed. Furthermore, it is costly and sometimes even infeasible to collect and annotate large-scale multi-source domain data for training.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>深度神经网络通常是在假设训练数据和测试数据是独立同分布的情况下进行训练的，这使得它们在训练数据和测试数据之间存在显著的分布变化时变得脆弱\(\left\lbrack  {{23},{52}}\right\rbrack\)。这种脆弱性被认为是它们在实际应用中部署的主要障碍之一。为了增强它们对这种分布变化的鲁棒性，领域适应（DA）\(\left\lbrack  {2,{24},{32},{33},{54},{56},{57},{68}}\right\rbrack\)已被研究；其目的是使用在训练中可用的目标领域数据来适应神经网络到目标领域。然而，这样的目标领域在常见的训练场景中往往是潜在的，这大大限制了DA的应用。最近，一系列研究通过领域泛化（DG）\(\left\lbrack  {3,5,{21},{29},{35},{37},{74}}\right\rbrack\)解决了这一限制，旨在提高模型对任何未见领域的泛化能力。在DG中，利用多个源领域学习领域不变特征已成为一种常见做法[61,69]，但尚不清楚哪些源领域是DG的理想选择，因为任意未见领域都应被考虑。此外，收集和标注大规模多源领域数据进行训练的成本高昂，有时甚至不可行。</p></div><!-- Media --><!-- figureText: "a photo of a cat" Joint Vision-Language Space “a cartoon of a cat 0 0 "a cartoon of a dog ☆ Text Feature — Decision Boundary Joint Vision-Language Space “a \( {S}_{2} \) style of a cat "a \( {S}_{3} \) style of a cat" "a \( {S}_{3} \) style of a dog" “a \( {S}_{2} \) style of a dog, (a) "a photo of a dog" ○ Image Feature "a \( {S}_{1} \) style of a cat" (b) "a \( {S}_{1} \) style of a dog" --><img src="https://cdn.noedgeai.com/01972c4e-e368-7ba0-a0ce-4063bebda408_0.jpg?x=899&#x26;y=671&#x26;w=712&#x26;h=801&#x26;r=0"><p>Figure 1: Motivation of our method. (a) Text features could effectively represent various image styles in a joint vision-language space. (b) PromptStyler synthesizes diverse styles in a joint vision-language space via learnable style word vectors for pseudo-words \({\mathbf{S}}_{ * }\) without using any images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图1：我们方法的动机。(a) 文本特征可以有效地表示联合视觉-语言空间中的各种图像风格。(b) PromptStyler通过可学习的风格词向量为伪词\({\mathbf{S}}_{ * }\)合成多样的风格，而无需使用任何图像。</p></div><!-- Media --><p>We notice that a large-scale pre-trained model might have already observed a great variety of domains and thus can be used as an efficient proxy of actual multiple source domains. From this perspective, we raised a question "Could we further improve model's generalization capability by simulating various distribution shifts in the latent space of such a large-scale model without using any source domain data?" If this is possible, DG will become immensely practical by effectively and efficiently exploiting such a large-scale model. However, this approach is much more challenging since any actual data of source and target domains are not accessible but only the target task definition (e.g., class names) is given.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们注意到，大规模预训练模型可能已经观察到了各种领域，因此可以作为实际多个源领域的有效代理。从这个角度来看，我们提出了一个问题：“我们能否通过在这样的大规模模型的潜在空间中模拟各种分布变化来进一步提高模型的泛化能力，而无需使用任何源领域数据？”如果这是可能的，DG将通过有效和高效地利用这样的大规模模型变得极为实用。然而，这种方法要困难得多，因为源领域和目标领域的任何实际数据都不可用，只有目标任务定义（例如，类名）是给定的。</p></div><!-- Media --><!-- figureText: Style Diversity Content Consistency content "a \( {S}_{1} \) style of a fox" “a \( {S}_{1} \) style of a dog ‘dog’ \( {\mathbf{S}}_{1} \) style of a cat" dog’ “cat” “cat” “a \( {S}_{1} \) style of a cat” "a \( {S}_{1} \) style of a dog" "a \( {\mathbf{S}}_{1} \) style of a fox" ☆ Text Feature style “a \( {S}_{5} \) style of a” as \( {S}_{4} \) style of a’ style of a" \( {S}_{3} \) style of a” “a \( {S}_{2} \) style of a” a \( {S}_{1} \) style of a” ☆ Text Feature --><img src="https://cdn.noedgeai.com/01972c4e-e368-7ba0-a0ce-4063bebda408_1.jpg?x=138&#x26;y=200&#x26;w=1472&#x26;h=514&#x26;r=0"><p>Figure 2: Important factors in the proposed method. PromptStyler learns style word vectors for pseudo-words \({\mathbf{S}}_{ * }\) which lead to diverse style features (from "a \({S}_{ * }\) style of a") while preserving content information encoded in style-content features (from "a \({S}_{ * }\) style of a [class]"). \({\mathcal{L}}_{\text{style }}\) and \({\mathcal{L}}_{\text{content }}\) are the loss functions used for maximizing style diversity and content consistency in a hyperspherical joint vision-language space (e.g., CLIP [50] latent space).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2：所提方法中的重要因素。PromptStyler为伪词\({\mathbf{S}}_{ * }\)学习风格词向量，这导致多样的风格特征（来自“一种\({S}_{ * }\)风格的”），同时保留编码在风格-内容特征中的内容信息（来自“一种\({S}_{ * }\)风格的[class]”）。\({\mathcal{L}}_{\text{style }}\)和\({\mathcal{L}}_{\text{content }}\)是用于最大化风格多样性和内容一致性的损失函数，在超球面联合视觉-语言空间中（例如，CLIP [50] 潜在空间）。</p></div><!-- Media --><p>In this paper, we argue that large-scale vision-language models \(\left\lbrack  {{26},{50},{64}}\right\rbrack\) could shed light on this challenging source-free domain generalization. As conceptually illustrated in Figure 1(a), text features could effectively represent their relevant image features in a joint vision-language space. Despite the modality gap between two modalities in the joint space [39], a recent study has demonstrated the cross-modal transferability phenomenon [67]; we could train a classifier using text features while running an inference with the classifier using image features. This training procedure meets the necessary condition for the source-free domain generalization, i.e., source domain images are not required. Using such a joint vision-language space, we could simulate various distribution shifts via prompts without any images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本文中，我们认为大规模视觉-语言模型 \(\left\lbrack  {{26},{50},{64}}\right\rbrack\) 可以为这一具有挑战性的无源领域泛化提供启示。如图1(a)所示，文本特征可以有效地在联合视觉-语言空间中表示其相关的图像特征。尽管在联合空间中两种模态之间存在模态差距 [39]，但最近的研究已经证明了跨模态可转移性现象 [67]；我们可以使用文本特征训练分类器，同时使用图像特征进行推理。这一训练过程满足无源领域泛化的必要条件，即不需要源领域图像。利用这样的联合视觉-语言空间，我们可以通过提示模拟各种分布变化，而无需任何图像。</p></div><p>We propose a prompt-driven style generation method, dubbed PromptStyler, which synthesizes diverse styles via learnable word vectors to simulate distribution shifts in a hyperspherical joint vision-language space. PromptStyler is motivated by the observation that a shared style of images could characterize a domain \(\left\lbrack  {{27},{74}}\right\rbrack\) and such a shared style could be captured by a learnable word vector for a pseudo-word \({S}_{ * }\) using CLIP [50] with a prompt ("a painting in the style of \({S}_{ * }\) ") [17]. As shown in Figure 1(b),our method learns a style word vector for \({\mathbf{S}}_{ * }\) to represent each style.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们提出了一种基于提示的风格生成方法，称为PromptStyler，它通过可学习的词向量合成多样的风格，以模拟超球面联合视觉-语言空间中的分布变化。PromptStyler的灵感来源于观察到共享的图像风格可以表征一个领域 \(\left\lbrack  {{27},{74}}\right\rbrack\)，而这种共享风格可以通过使用CLIP [50] 的伪词 \({S}_{ * }\) 的可学习词向量捕捉，提示为（“以\({S}_{ * }\)风格的画”）[17]。如图1(b)所示，我们的方法为\({\mathbf{S}}_{ * }\)学习一个风格词向量，以表示每种风格。</p></div><p>To effectively simulate various distribution shifts, we try to maximize style diversity as illustrated in Figure 2. Specifically, our method encourages learnable style word vectors to result in orthogonal style features in the hyperspherical space, where each style feature is obtained from a style prompt ("a \({S}_{ * }\) style of a") via a pre-trained text encoder. To prevent learned styles from distorting content information, we also consider content consistency as illustrated in Figure 2. Each style-content feature obtained from a style-content prompt ("a \({S}_{ * }\) style of a [class]") is forced to be located closer to its corresponding content feature obtained from a content prompt ("[class]") than the other content features.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了有效地模拟各种分布变化，我们尝试最大化风格多样性，如图2所示。具体而言，我们的方法鼓励可学习的风格词向量在超球面空间中产生正交的风格特征，其中每个风格特征是通过预训练的文本编码器从风格提示（“一种\({S}_{ * }\)风格的a”）获得的。为了防止学习到的风格扭曲内容信息，我们还考虑了内容一致性，如图2所示。每个从风格-内容提示（“一种\({S}_{ * }\)风格的a [类别]”）获得的风格-内容特征被迫比其他内容特征更靠近其对应的从内容提示（“[类别]”）获得的内容特征。</p></div><p>Learned style word vectors are used to synthesize style-content features for training a classifier; these synthesized features could simulate images of known contents with diverse unknown styles in the joint space. These style-content features are fed as input to a linear classifier which is trained by a classification loss using contents ("[class]") as their class labels. At inference time, an image encoder extracts image features from input images, which are fed as input to the trained classifier. Note that the text and image encoders are derived from the same pre-trained vision-language model (e.g., CLIP [50]); the text encoder is only involved in training and the image encoder is only involved at inference time.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>学习到的风格词向量用于合成风格-内容特征，以训练分类器；这些合成的特征可以在联合空间中模拟已知内容的图像，具有多样的未知风格。这些风格-内容特征作为输入提供给线性分类器，该分类器通过使用内容（“[类别]”）作为其类别标签的分类损失进行训练。在推理时，图像编码器从输入图像中提取图像特征，这些特征作为输入提供给训练好的分类器。请注意，文本和图像编码器源自同一预训练的视觉-语言模型（例如，CLIP [50]）；文本编码器仅参与训练，而图像编码器仅在推理时参与。</p></div><p>The proposed method achieves state-of-the-art results on PACS [34], VLCS [15], OfficeHome [60] and Domain-Net [48] without using any actual data of source and target domains. It takes just \(\sim  {30}\) minutes for the entire training using a single RTX 3090 GPU,and our model is \(\sim  {2.6} \times\) smaller and \(\sim  {243} \times\) faster at inference compared with CLIP [50].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>所提出的方法在PACS [34]、VLCS [15]、OfficeHome [60]和Domain-Net [48]上取得了最先进的结果，而无需使用任何源领域和目标领域的实际数据。整个训练过程仅需 \(\sim  {30}\) 分钟，使用单个RTX 3090 GPU，并且我们的模型在推理时比CLIP [50] \(\sim  {2.6} \times\) 更小且 \(\sim  {243} \times\) 更快。</p></div><p>Our contributions are summarized as follows:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的贡献总结如下：</p></div><ul>
<li>This work is the first attempt to synthesize a variety of styles in a joint vision-language space via prompts to effectively tackle source-free domain generalization.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>这项工作是首次尝试通过提示在联合视觉-语言空间中合成多种风格，以有效应对无源领域泛化。</li>
</ul></div><ul>
<li>This paper proposes a novel method that effectively simulates images of known contents with diverse unknown styles in a joint vision-language space.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>本文提出了一种新颖的方法，能够有效地在联合视觉-语言空间中模拟已知内容的图像，具有多样的未知风格。</li>
</ul></div><ul>
<li>PromptStyler achieves the state of the art on domain generalization benchmarks without using any images.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>PromptStyler在领域泛化基准测试中实现了最先进的成果，而无需使用任何图像。</li>
</ul></div><!-- Media --><table><tbody><tr><td>Setup</td><td>Source</td><td>Target</td><td>Task Definition</td></tr><tr><td>DA</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>DG</td><td>✓</td><td>-</td><td>✓</td></tr><tr><td>Source-free DA</td><td>-</td><td>✓</td><td>✓</td></tr><tr><td>Source-free DG</td><td>-</td><td>-</td><td>✓</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>设置</td><td>源</td><td>目标</td><td>任务定义</td></tr><tr><td>DA</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>DG</td><td>✓</td><td>-</td><td>✓</td></tr><tr><td>无源DA</td><td>-</td><td>✓</td><td>✓</td></tr><tr><td>无源DG</td><td>-</td><td>-</td><td>✓</td></tr></tbody></table></div><p>Table 1: Different requirements in each setup. Source-free DG only assumes the task definition (i.e., what should be predicted) without requiring source and target domain data.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表1：每种设置中的不同要求。无源领域泛化仅假设任务定义（即，应该预测什么），而不需要源域和目标域数据。</p></div><!-- Media --><h2>2. Related Work</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>2. 相关工作</h2></div><p>Domain Generalization. Model's generalization capability to arbitrary unseen domains is the key factor to successful deployment of neural networks in real-world applications, since substantial distribution shifts between source and target domains could significantly degrade their performance [23, 52]. To this end, Domain Generalization (DG) [4, 5, 10, 16, \({21},{29},{35},{37},{44},{45},{61},{69}\rbrack\) has been studied. It assumes target domain data are not accessible while using data from source domains. Generally speaking, existing DG methods could be divided into two categories: multi-source DG [3, 12, 36, 42, 43, 51, 55, 63, 73, 74] and single-source DG [14, \({38},{49},{62}\rbrack\) . Mostly,multi-source DG methods aim to learn domain-invariant features by exploiting available multiple source domains, and single-source DG methods also aim to learn such features by generating diverse domains based on a single domain and then exploiting the synthesized domains.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>领域泛化。模型对任意未见领域的泛化能力是神经网络在实际应用中成功部署的关键因素，因为源域和目标域之间的显著分布变化可能会显著降低其性能[23, 52]。为此，领域泛化（DG）[4, 5, 10, 16, \({21},{29},{35},{37},{44},{45},{61},{69}\rbrack\) 已被研究。它假设目标域数据不可访问，同时使用源域的数据。一般来说，现有的DG方法可以分为两类：多源DG [3, 12, 36, 42, 43, 51, 55, 63, 73, 74] 和单源DG [14, \({38},{49},{62}\rbrack\)。大多数情况下，多源DG方法旨在通过利用可用的多个源域来学习领域不变特征，而单源DG方法也旨在通过基于单一领域生成多样化领域，然后利用合成的领域来学习这些特征。</p></div><p>Source-free Domain Generalization. In this setup, we are not able to access any source and target domains as summarized in Table 1. Thus, source-free DG is much more challenging than multi-source and single-source DG. From the observation that synthesizing new domains from the given source domain could effectively improve model's generalization capability \(\left\lbrack  {{27},{38},{62},{72},{73}}\right\rbrack\) ,we also try to generate diverse domains but without using any source domains to deal with source-free DG. By leveraging a large-scale pre-trained model which has already seen a great variety of domains, our method could simulate various distribution shifts in the latent space of the large-scale model. This approach has several advantages compared with existing DG methods; source domain images are not required and there is no concern for catastrophic forgetting which might impede model's generalization capability. Also, it would be immensely practical to exploit such a large-scale model for downstream visual recognition tasks, since we only need the task definition.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>无源领域泛化。在这种设置中，我们无法访问任何源域和目标域，如表1所总结。因此，无源DG比多源和单源DG更具挑战性。根据从给定源域合成新领域可以有效提高模型的泛化能力\(\left\lbrack  {{27},{38},{62},{72},{73}}\right\rbrack\)的观察，我们也尝试生成多样化领域，但不使用任何源域来处理无源DG。通过利用一个已经见过多种领域的大规模预训练模型，我们的方法可以在大规模模型的潜在空间中模拟各种分布变化。与现有的DG方法相比，这种方法有几个优点；不需要源域图像，也不必担心可能妨碍模型泛化能力的灾难性遗忘。此外，利用这样一个大规模模型进行下游视觉识别任务将极具实用性，因为我们只需要任务定义。</p></div><p>Large-scale model in Domain Generalization. Recently, several DG methods \(\left\lbrack  {5,{53}}\right\rbrack\) exploit a large-scale pre-trained model (e.g., CLIP [50]) to leverage its great generalization capability. While training neural networks on available data, CAD [53] and MIRO [5] try to learn robust features using such a large-scale model. Compared with them, the proposed method could learn domain-invariant features using a large-scale pre-trained model without requiring any actual data.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>领域泛化中的大规模模型。最近，一些DG方法\(\left\lbrack  {5,{53}}\right\rbrack\)利用大规模预训练模型（例如，CLIP [50]）来利用其出色的泛化能力。在可用数据上训练神经网络时，CAD [53]和MIRO [5]尝试使用这样的一个大规模模型学习鲁棒特征。与它们相比，所提出的方法可以在不需要任何实际数据的情况下，使用大规模预训练模型学习领域不变特征。</p></div><p>Joint vision-language space. Large-scale vision-language models \(\left\lbrack  {{26},{50},{64}}\right\rbrack\) are trained with a great amount of image-text pairs, and achieve state-of-the-art results on downstream visual recognition tasks \(\left\lbrack  {{20},{41},{66},{70},{71}}\right\rbrack\) . By leveraging their joint vision-language spaces, we could also effectively manipulate visual features via prompts \(\left\lbrack  {{13},{18},{31},{47}}\right\rbrack\) . Interestingly, Textual Inversion [17] shows that a learnable style word vector for a pseudo-word \({\mathbf{S}}_{ * }\) could capture a shared style of images using CLIP [50] with a prompt ("a painting in the style of \({S}_{ * }\) "). From this observation,we argue that learnable style word vectors would be able to seek a variety of styles for simulating various distribution shifts in a joint vision-language space without using any images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>联合视觉-语言空间。大规模视觉-语言模型\(\left\lbrack  {{26},{50},{64}}\right\rbrack\)使用大量图像-文本对进行训练，并在下游视觉识别任务中取得了最先进的结果\(\left\lbrack  {{20},{41},{66},{70},{71}}\right\rbrack\)。通过利用它们的联合视觉-语言空间，我们也可以通过提示有效地操控视觉特征\(\left\lbrack  {{13},{18},{31},{47}}\right\rbrack\)。有趣的是，文本反演[17]表明，伪词\({\mathbf{S}}_{ * }\)的可学习风格词向量可以使用CLIP [50]捕捉图像的共享风格，提示为（“以\({S}_{ * }\)风格的画”）。基于这一观察，我们认为可学习的风格词向量能够在不使用任何图像的情况下，寻求多种风格以模拟联合视觉-语言空间中的各种分布变化。</p></div><h2>3. Method</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>3. 方法</h2></div><p>The overall framework of the proposed method is shown in Figure 3, and pseudo-code of PromptStyler is described in Algorithm 1. Our method learns style word vectors to represent a variety of styles in a hyperspherical joint vision-language space (e.g., CLIP [50] latent space). After learning those style word vectors, we train a linear classifier using synthesized style-content features produced by a pre-trained text encoder \(T\left( \cdot \right)\) . At inference time,a pre-trained image encoder \(I\left( \cdot \right)\) extracts image features from input images,which are fed as input to the trained linear classifier. Thanks to the cross-modal transferability phenomenon of the joint vision-language space [67], this classifier could produce class scores using the image features. Note that we exploit CLIP as our large-scale vision-language model; its image encoder and text encoder are frozen in our entire framework.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>所提出方法的整体框架如图3所示，PromptStyler的伪代码在算法1中描述。我们的方法学习风格词向量，以表示超球面联合视觉-语言空间（例如，CLIP [50]潜在空间）中的多种风格。在学习这些风格词向量后，我们使用预训练文本编码器\(T\left( \cdot \right)\)生成的合成风格-内容特征训练线性分类器。在推理时，预训练图像编码器\(I\left( \cdot \right)\)从输入图像中提取图像特征，这些特征作为输入提供给训练好的线性分类器。得益于联合视觉-语言空间的跨模态可迁移现象[67]，该分类器可以使用图像特征生成类别分数。请注意，我们将CLIP作为我们的大规模视觉-语言模型；在整个框架中，其图像编码器和文本编码器都是冻结的。</p></div><h3>3.1. Prompt-driven style generation</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.1. 基于提示的风格生成</h3></div><p>An input text prompt is converted to several tokens via a tokenization process, and then such tokens are replaced by their corresponding word vectors via a word lookup process. In PromptStyler,a pseudo-word \({\mathbf{S}}_{i}\) in a prompt is a placeholder which is replaced by a style word vector \({\mathbf{s}}_{i} \in  {\mathbb{R}}^{D}\) during the word lookup process. Note that three kinds of prompts are used in the proposed method: a style prompt \({\mathcal{P}}_{i}^{\text{style }}\) ("a \({\mathbf{S}}_{i}\) style of a"),a content prompt \({\mathcal{P}}_{m}^{\text{content }}\) (" \({\left\lbrack  \text{class}\right\rbrack  }_{m}\) "),and a style-content prompt \({\mathcal{P}}_{i}^{\text{style }} \circ  {\mathcal{P}}_{m}^{\text{content }}\) ("a \({\mathbf{S}}_{i}\) style of a \({\left\lbrack  \text{class}\right\rbrack  }_{m}\) "). \({\mathbf{S}}_{i}\) indicates the placeholder for \(i\) -th style word vector and [class] \({}_{m}\) denotes \(m\) -th class name.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>输入文本提示通过分词过程转换为多个标记，然后这些标记通过词查找过程被其对应的词向量替换。在PromptStyler中，提示中的伪词\({\mathbf{S}}_{i}\)是一个占位符，在词查找过程中被样式词向量\({\mathbf{s}}_{i} \in  {\mathbb{R}}^{D}\)替换。请注意，所提议的方法中使用了三种类型的提示：样式提示\({\mathcal{P}}_{i}^{\text{style }}\)（“一种\({\mathbf{S}}_{i}\)的样式”）、内容提示\({\mathcal{P}}_{m}^{\text{content }}\)（“\({\left\lbrack  \text{class}\right\rbrack  }_{m}\)”）和样式-内容提示\({\mathcal{P}}_{i}^{\text{style }} \circ  {\mathcal{P}}_{m}^{\text{content }}\)（“一种\({\mathbf{S}}_{i}\)的样式的\({\left\lbrack  \text{class}\right\rbrack  }_{m}\)”）。\({\mathbf{S}}_{i}\)表示第\(i\)个样式词向量的占位符，而[class] \({}_{m}\)表示第\(m\)个类名。</p></div><p>Suppose we want to generate \(K\) different styles in a joint vision-language space. In this case, the proposed method needs to learn \(K\) style word vectors \({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\) ,where each \({\mathbf{s}}_{i}\) is randomly initialized at the beginning. To effectively simulate various distribution shifts in the joint vision-language space, those style word vectors need to be diverse while not distorting content information when they are exploited in style-content prompts. There are two possible design choices for learning such word vectors: (1) learning each style word vector \({\mathbf{s}}_{i}\) in a sequential manner,or (2) learning all style word vectors \({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\) in a parallel manner. We choose the former, since it takes much less memory during training. Please refer to the supplementary material (Section A.2) for the empirical justification of our design choice.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>假设我们想在一个联合视觉-语言空间中生成\(K\)种不同的样式。在这种情况下，所提议的方法需要学习\(K\)个样式词向量\({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\)，其中每个\({\mathbf{s}}_{i}\)在开始时是随机初始化的。为了有效模拟联合视觉-语言空间中的各种分布变化，这些样式词向量需要多样化，同时在样式-内容提示中使用时不扭曲内容信息。学习这些词向量有两种可能的设计选择：（1）以顺序方式学习每个样式词向量\({\mathbf{s}}_{i}\)，或（2）以并行方式学习所有样式词向量\({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\)。我们选择前者，因为在训练过程中所需的内存要少得多。有关我们设计选择的实证依据，请参见补充材料（第A.2节）。</p></div><!-- Media --><!-- figureText: (i) Prompt-driven style generation (ii) Training a linear classifier using diverse styles 。 dog “a \( {S}_{1} \) style of a dog” Text Linear Encoder Layer cat fox Style Content \( T\left( \cdot \right) \) (class label) Class Scores - Frozen weights \( \square {\ell }_{2} \) normalization layer (iii) Inference using the trained classifier 。 dog Image Linear Encoder Layer \( I\left( \cdot \right) \) cat fox Class Scores - Frozen weights \( \square {\ell }_{2} \) normalization layer \( {\mathcal{L}}_{\text{style }} + {\mathcal{L}}_{\text{content }} \) a \( {S}_{1} \) style of a dog" "a \( {S}_{1} \) style of a cat" “cat” "a \( {S}_{2} \) style of a dog" C. "a \( {S}_{3} \) style of a dog" “a \( {S}_{2} \) style of a cat” .. \( {S}_{3} \) style of a cat "a \( {S}_{3} \) style of a fox" “a \( {S}_{2} \) style of a fox” “a \( {S}_{1} \) style of a fox” “fox” --><img src="https://cdn.noedgeai.com/01972c4e-e368-7ba0-a0ce-4063bebda408_3.jpg?x=135&#x26;y=203&#x26;w=1483&#x26;h=591&#x26;r=0"><p>Figure 3: PromptStyler learns diverse style word vectors which do not distort content information of style-content prompts. After learning style word vectors,we synthesize style-content features (e.g.,from "a \({S}_{1}\) style of a dog") via a pre-trained text encoder for training a linear classifier. The classifier is trained by a classification loss using those synthesized features and their corresponding class labels (e.g., "dog"). At inference time, a pre-trained image encoder extracts image features, which are fed as input to the trained classifier. Note that the encoders are derived from the same vision-language model (e.g., CLIP [50]).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3：PromptStyler学习多样化的样式词向量，这些词向量不会扭曲样式-内容提示的内容信息。在学习样式词向量后，我们通过预训练的文本编码器合成样式-内容特征（例如，从“一种\({S}_{1}\)的狗的样式”）以训练线性分类器。分类器通过使用这些合成特征及其对应的类标签（例如，“狗”）的分类损失进行训练。在推理时，预训练的图像编码器提取图像特征，这些特征作为输入提供给训练好的分类器。请注意，编码器源自同一视觉-语言模型（例如，CLIP [50]）。</p></div><!-- Media --><p>Style diversity loss. To maximize the diversity of \(K\) styles in a hyperspherical joint vision-language space, we sequentially learn style word vectors \({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\) in such a way that \(i\) -th style feature \(T\left( {\mathcal{P}}_{i}^{\text{style }}\right)  \in  {\mathbb{R}}^{C}\) produced by \(i\) -th style word vector \({\mathbf{s}}_{i}\) is orthogonal to \({\left\{  T\left( {\mathcal{P}}_{j}^{\text{style }}\right) \right\}  }_{j = 1}^{i - 1}\) produced by previously learned style word vectors \({\left\{  {\mathbf{s}}_{j}\right\}  }_{j = 1}^{i - 1}\) . Regarding this,the style diversity loss \({\mathcal{L}}_{\text{style }}\) for learning \(i\) -th style word vector \({\mathbf{s}}_{i}\) is computed by</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>样式多样性损失。为了最大化在超球形联合视觉-语言空间中\(K\)种样式的多样性，我们以顺序方式学习样式词向量\({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\)，使得第\(i\)个样式特征\(T\left( {\mathcal{P}}_{i}^{\text{style }}\right)  \in  {\mathbb{R}}^{C}\)由第\(i\)个样式词向量\({\mathbf{s}}_{i}\)产生，与由之前学习的样式词向量\({\left\{  {\mathbf{s}}_{j}\right\}  }_{j = 1}^{i - 1}\)产生的\({\left\{  T\left( {\mathcal{P}}_{j}^{\text{style }}\right) \right\}  }_{j = 1}^{i - 1}\)正交。关于这一点，学习第\(i\)个样式词向量\({\mathbf{s}}_{i}\)的样式多样性损失\({\mathcal{L}}_{\text{style }}\)是通过计算得出的。</p></div><p></p>\[{\mathcal{L}}_{\text{style }} = \frac{1}{i - 1}\mathop{\sum }\limits_{{j = 1}}^{{i - 1}}\left| {\frac{T\left( {\mathcal{P}}_{i}^{\text{style }}\right) }{{\begin{Vmatrix}T\left( {\mathcal{P}}_{i}^{\text{style }}\right) \end{Vmatrix}}_{2}} \cdot  \frac{T\left( {\mathcal{P}}_{j}^{\text{style }}\right) }{{\begin{Vmatrix}T\left( {\mathcal{P}}_{j}^{\text{style }}\right) \end{Vmatrix}}_{2}}}\right|  \tag{1}\]<p></p><p>This style loss \({\mathcal{L}}_{\text{style }}\) aims to minimize the absolute value of the cosine similarity between \(i\) -th style feature and each of the existing style features. When the value of this loss becomes zero,it satisfies the orthogonality between \(i\) -th style feature and all the existing style features.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这种风格损失 \({\mathcal{L}}_{\text{style }}\) 旨在最小化 \(i\) -th 风格特征与所有现有风格特征之间余弦相似度的绝对值。当该损失值为零时，它满足 \(i\) -th 风格特征与所有现有风格特征之间的正交性。</p></div><p>Content consistency loss. Learning the style word vectors \({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\) only using the style diversity loss sometimes leads to undesirable outcome,since a learned style \({\mathbf{s}}_{i}\) could substantially distort content information when used to generate a style-content feature \(T\left( {{\mathcal{P}}_{i}^{\text{style }} \circ  {\mathcal{P}}_{m}^{\text{content }}) \in  {\mathbb{R}}^{C}}\right.\) . To alleviate this problem, we encourage the content information in the style-content feature to be consistent with its corresponding content feature \(T\left( {\mathcal{P}}_{m}^{\text{content }}\right)  \in  {\mathbb{R}}^{C}\) while learning each \(i\) -th style word vector \({\mathbf{s}}_{i}\) . Specifically,each style-content feature synthesized via \(i\) -th style word vector \({\mathbf{s}}_{i}\) should have the highest cosine similarity score with its corresponding content feature. For \(i\) -th style word vector \({\mathbf{s}}_{i}\) ,a cosine similarity score \({z}_{imn}\) between a style-content feature with \(m\) -th class name and a content feature with \(n\) -th class name is computed by</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>内容一致性损失。仅使用风格多样性损失学习风格词向量 \({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\) 有时会导致不理想的结果，因为学习到的风格 \({\mathbf{s}}_{i}\) 在用于生成风格-内容特征 \(T\left( {{\mathcal{P}}_{i}^{\text{style }} \circ  {\mathcal{P}}_{m}^{\text{content }}) \in  {\mathbb{R}}^{C}}\right.\) 时可能会显著扭曲内容信息。为了解决这个问题，我们鼓励风格-内容特征中的内容信息与其对应的内容特征 \(T\left( {\mathcal{P}}_{m}^{\text{content }}\right)  \in  {\mathbb{R}}^{C}\) 一致，同时学习每个 \(i\) -th 风格词向量 \({\mathbf{s}}_{i}\)。具体来说，通过 \(i\) -th 风格词向量 \({\mathbf{s}}_{i}\) 合成的每个风格-内容特征应与其对应的内容特征具有最高的余弦相似度分数。对于 \(i\) -th 风格词向量 \({\mathbf{s}}_{i}\)，计算具有 \(m\) -th 类名的风格-内容特征与具有 \(n\) -th 类名的内容特征之间的余弦相似度分数 \({z}_{imn}\)。</p></div><p></p>\[{z}_{imn} = \frac{T\left( {{\mathcal{P}}_{i}^{\text{style }} \circ  {\mathcal{P}}_{m}^{\text{content }}}\right) }{{\begin{Vmatrix}T\left( {\mathcal{P}}_{i}^{\text{style }} \circ  {\mathcal{P}}_{m}^{\text{content }}\right) \end{Vmatrix}}_{2}} \cdot  \frac{T\left( {\mathcal{P}}_{n}^{\text{content }}\right) }{{\begin{Vmatrix}T\left( {\mathcal{P}}_{n}^{\text{content }}\right) \end{Vmatrix}}_{2}} \tag{2}\]<p></p><p>Using cosine similarity scores between style-content features and content features,the content consistency loss \({\mathcal{L}}_{\text{content }}\) for learning \(i\) -th style word vector \({\mathbf{s}}_{i}\) is computed by</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>使用风格-内容特征与内容特征之间的余弦相似度分数，学习 \(i\) -th 风格词向量 \({\mathbf{s}}_{i}\) 的内容一致性损失 \({\mathcal{L}}_{\text{content }}\) 计算为</p></div><p></p>\[{\mathcal{L}}_{\text{content }} =  - \frac{1}{N}\mathop{\sum }\limits_{{m = 1}}^{N}\log \left( \frac{\exp \left( {z}_{imm}\right) }{\mathop{\sum }\limits_{{n = 1}}^{N}\exp \left( {z}_{imn}\right) }\right) , \tag{3}\]<p></p><p>where \(N\) denotes the number of classes pre-defined in the target task. This content loss \({\mathcal{L}}_{\text{content }}\) is a contrastive loss which encourages each style-content feature to be located closer to its corresponding content feature so that it forces each \(i\) -th style word vector \({\mathbf{s}}_{i}\) to preserve content information when used to synthesize style-content features.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(N\) 表示在目标任务中预定义的类的数量。该内容损失 \({\mathcal{L}}_{\text{content }}\) 是一种对比损失，鼓励每个风格-内容特征靠近其对应的内容特征，从而迫使每个 \(i\) -th 风格词向量 \({\mathbf{s}}_{i}\) 在用于合成风格-内容特征时保留内容信息。</p></div><p>Total prompt loss. PromptStyler learns \(K\) style word vectors \({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\) in a sequential manner,where each \(i\) -th style word vector \({\mathbf{s}}_{i}\) is learned using both \({\mathcal{L}}_{\text{style }}\left( {\mathrm{{Eq}}.\left( 1\right) }\right)\) and \({\mathcal{L}}_{\text{content }}\) (Eq. (3)). In the proposed method,the total loss \({\mathcal{L}}_{\text{prompt }}\) for learning \(i\) -th style word vector is computed by</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>总提示损失。PromptStyler 以顺序方式学习 \(K\) 风格词向量 \({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\)，其中每个 \(i\) -th 风格词向量 \({\mathbf{s}}_{i}\) 是同时使用 \({\mathcal{L}}_{\text{style }}\left( {\mathrm{{Eq}}.\left( 1\right) }\right)\) 和 \({\mathcal{L}}_{\text{content }}\) 学习的（公式 (3)）。在所提出的方法中，学习 \(i\) -th 风格词向量的总损失 \({\mathcal{L}}_{\text{prompt }}\) 计算为</p></div><p></p>\[{\mathcal{L}}_{\text{prompt }} = {\mathcal{L}}_{\text{style }} + {\mathcal{L}}_{\text{content }}. \tag{4}\]<p></p><p>Using this prompt loss \({\mathcal{L}}_{\text{prompt }}\) ,we train \(i\) -th style word vector \({\mathbf{s}}_{i}\) for \(L\) training iterations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>使用该提示损失 \({\mathcal{L}}_{\text{prompt }}\)，我们训练 \(i\) -th 风格词向量 \({\mathbf{s}}_{i}\) 进行 \(L\) 次训练迭代。</p></div><!-- Media --><p>Algorithm 1 PromptStyler</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>算法 1 PromptStyler</p></div><hr><p>Requirement: pre-trained text encoder \(T\left( \cdot \right)\) ,pre-defined \(N\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>要求：预训练文本编码器 \(T\left( \cdot \right)\)，预定义 \(N\)</p></div><pre><code>				class names in the target task
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>				目标任务中的类名
</code></pre></div><p>Input: number of style word vectors \(K\) ,number of training</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>输入：风格词向量的数量 \(K\)，训练的次数</p></div><pre><code>	iterations \( L \)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	迭代次数 \( L \)
</code></pre></div><p>Output: \({KN}\) style-content features</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>输出：\({KN}\) 风格-内容特征</p></div><pre><code>#randomly initialize style word vectors
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>#随机初始化风格词向量
</code></pre></div><pre><code>\( {\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K} \leftarrow  {\operatorname{random}}_{ - }\operatorname{initialize}\left( {\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\right) \)
</code></pre><pre><code>#sequentially learn \( K \) style word vectors
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>#顺序学习 \( K \) 风格词向量
</code></pre></div><pre><code>for \( i = 1,2,\ldots ,K \) do
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>for \( i = 1,2,\ldots ,K \) do
</code></pre></div><pre><code>		#\( L \) training iterations for learning each word vector
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>		#\( L \) 次训练迭代以学习每个词向量
</code></pre></div><pre><code>	for iteration \( = 1,2,\ldots ,L \) do
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	对于迭代 \( = 1,2,\ldots ,L \) 执行
</code></pre></div><pre><code>			#compute \( {\mathcal{L}}_{\text{style }} \) using \( T\left( \cdot \right) \) and word vectors
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>			#使用 \( T\left( \cdot \right) \) 和词向量计算 \( {\mathcal{L}}_{\text{style }} \)
</code></pre></div><pre><code>			\( {\mathcal{L}}_{\text{style }} \leftarrow \) style_diversity_loss \( \left( {{\mathbf{s}}_{i},{\left\{  {\mathbf{s}}_{j}\right\}  }_{j = 1}^{i - 1}}\right) \)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>			\( {\mathcal{L}}_{\text{style }} \leftarrow \) 风格多样性损失 \( \left( {{\mathbf{s}}_{i},{\left\{  {\mathbf{s}}_{j}\right\}  }_{j = 1}^{i - 1}}\right) \)
</code></pre></div><pre><code>			#compute \( {\mathcal{L}}_{\text{content }} \) using \( T\left( \cdot \right) \) and a word vector
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>			#使用 \( T\left( \cdot \right) \) 和一个词向量计算 \( {\mathcal{L}}_{\text{content }} \)
</code></pre></div><pre><code>			\( {\mathcal{L}}_{\text{content }} \leftarrow \) content_consistency_loss \( \left( {\mathbf{s}}_{i}\right) \)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>			\( {\mathcal{L}}_{\text{content }} \leftarrow \) 内容一致性损失 \( \left( {\mathbf{s}}_{i}\right) \)
</code></pre></div><pre><code>			\( {\mathcal{L}}_{\text{prompt }} \leftarrow  {\mathcal{L}}_{\text{style }} + {\mathcal{L}}_{\text{content }} \)
</code></pre><pre><code>			Update \( {\mathbf{s}}_{i} \) using \( {\mathcal{L}}_{\text{prompt }} \) by gradient descent
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>			通过梯度下降更新 \( {\mathbf{s}}_{i} \) 使用 \( {\mathcal{L}}_{\text{prompt }} \)
</code></pre></div><pre><code>		end for
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>		结束循环
</code></pre></div><pre><code>end for
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>结束循环
</code></pre></div><pre><code>Synthesize \( {KN} \) style-content features using the learned
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>使用学习到的 \( {KN} \) 风格-内容特征进行合成
</code></pre></div><pre><code>\( K \) style word vectors and \( N \) class names via \( T\left( \cdot \right) \)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>\( K \) 风格词向量和 \( N \) 类别名称通过 \( T\left( \cdot \right) \)
</code></pre></div><hr><!-- Media --><h3>3.2. Training a linear classifier using diverse styles</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.2. 使用多样化风格训练线性分类器</h3></div><p>After learning \(K\) style word vectors \({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\) ,we generate \({KN}\) style-content features for training a linear classifier. To be specific,we synthesize those features using the learned \(K\) styles and pre-defined \(N\) classes via the text encoder \(T\left( \cdot \right)\) . The linear classifier is trained by a classification loss using \({\ell }_{2}\) -normalized style-content features and their class labels; each class label is the class name used to generate each style-content feature. To effectively leverage the hyperspherical joint vision-language space, we adopt ArcFace [8] loss as our classification loss \({\mathcal{L}}_{\text{class }}\) . Note that ArcFace loss is an angular Softmax loss which computes the cosine similarities between classifier input features and classifier weights with an additive angular margin penalty between classes. This angular margin penalty allows for more discriminative predictions by pushing features from different classes further apart. Thanks to the property, this angular Softmax loss has been widely used in visual recognition tasks [7, 9, 30, 40, 65].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在学习了 \(K\) 风格词向量 \({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\) 后，我们生成 \({KN}\) 风格-内容特征以训练线性分类器。具体来说，我们使用学习到的 \(K\) 风格和预定义的 \(N\) 类别通过文本编码器 \(T\left( \cdot \right)\) 合成这些特征。线性分类器通过使用 \({\ell }_{2}\) 归一化的风格-内容特征及其类别标签来训练分类损失；每个类别标签是用于生成每个风格-内容特征的类别名称。为了有效利用超球面联合视觉-语言空间，我们采用 ArcFace [8] 损失作为我们的分类损失 \({\mathcal{L}}_{\text{class }}\)。请注意，ArcFace 损失是一种角度 Softmax 损失，它计算分类器输入特征与分类器权重之间的余弦相似度，并在类别之间施加附加的角度边际惩罚。这个角度边际惩罚通过将不同类别的特征推得更远，从而允许更具辨别力的预测。得益于这一特性，这种角度 Softmax 损失在视觉识别任务中得到了广泛应用 [7, 9, 30, 40, 65]。</p></div><h3>3.3. Inference using the trained classifier</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.3. 使用训练好的分类器进行推理</h3></div><p>The trained classifier is used with a pre-trained image encoder \(I\left( \cdot \right)\) at inference time. Given an input image \(\mathbf{x}\) ,the image encoder extracts its image feature \(I\left( \mathbf{x}\right)  \in  {\mathbb{R}}^{C}\) ,which is mapped to the hyperspherical joint vision-language space by \({\ell }_{2}\) normalization. Then,the trained classifier produces class scores using the \({\ell }_{2}\) -normalized image feature. Note that the text encoder \(T\left( \cdot \right)\) is not used at inference time,while the image encoder \(I\left( \cdot \right)\) is only exploited at inference time.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>训练好的分类器在推理时与预训练的图像编码器 \(I\left( \cdot \right)\) 一起使用。给定输入图像 \(\mathbf{x}\)，图像编码器提取其图像特征 \(I\left( \mathbf{x}\right)  \in  {\mathbb{R}}^{C}\)，该特征通过 \({\ell }_{2}\) 归一化映射到超球面联合视觉-语言空间。然后，训练好的分类器使用 \({\ell }_{2}\) 归一化的图像特征生成类别分数。请注意，文本编码器 \(T\left( \cdot \right)\) 在推理时未被使用，而图像编码器 \(I\left( \cdot \right)\) 仅在推理时被利用。</p></div><h2>4. Experiments</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>4. 实验</h2></div><p>For more comprehensive understanding, please refer to the supplementary material (Section B and D).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>有关更全面的理解，请参阅补充材料（第 B 和 D 节）。</p></div><h3>4.1. Evaluation datasets</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.1. 评估数据集</h3></div><p>The proposed method does not require any actual data for training. To analyze its generalization capability, four domain generalization benchmarks are used for evaluation: PACS [34] (4 domains and 7 classes), VLCS [15] (4 domains and 5 classes), OfficeHome [60] ( 4 domains and 65 classes) and DomainNet [48] (6 domains and 345 classes). On these benchmarks, we repeat each experiment three times using different random seeds and report average top-1 classification accuracies with standard errors. Unlike the leave-one-domain-out cross-validation evaluation protocol [21], we do not exploit any source domain data for training.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>所提出的方法不需要任何实际数据进行训练。为了分析其泛化能力，使用四个领域泛化基准进行评估：PACS [34]（4个领域和7个类别）、VLCS [15]（4个领域和5个类别）、OfficeHome [60]（4个领域和65个类别）和DomainNet [48]（6个领域和345个类别）。在这些基准上，我们使用不同的随机种子重复每个实验三次，并报告平均的top-1分类准确率及标准误差。与留一领域交叉验证评估协议 [21] 不同，我们不利用任何源领域数据进行训练。</p></div><h3>4.2. Implementation details</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.2. 实现细节</h3></div><p>PromptStyler is implemented and trained with the same configuration regardless of the evaluation datasets. Training takes about 30 minutes using a single RTX 3090 GPU.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>PromptStyler的实现和训练使用相同的配置，无论评估数据集如何。使用单个RTX 3090 GPU进行训练大约需要30分钟。</p></div><p>Architecture. We choose CLIP [50] as our large-scale pre-trained vision-language model, and use the publicly available pre-trained model. \({}^{1}\) The text encoder \(T\left( \cdot \right)\) used in training is Transformer [59] and the image encoder \(I\left( \cdot \right)\) used at inference is ResNet-50 [22] as default setting in experiments; our method is also implemented with ViT-B/16 [11] or ViT-L/14 [11] for further evaluations as shown in Table 2. Note that text and image encoders are derived from the same CLIP model and frozen in the entire pipeline. The dimension of each text feature or image feature is \(C = {1024}\) when our method is implemented with ResNet-50,while \(C = {512}\) in the case of ViT-B/16 and \(C = {768}\) in the case of ViT-L/14.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>架构。我们选择CLIP [50]作为我们的大规模预训练视觉-语言模型，并使用公开可用的预训练模型。\({}^{1}\) 训练中使用的文本编码器\(T\left( \cdot \right)\)是Transformer [59]，推理中使用的图像编码器\(I\left( \cdot \right)\)是ResNet-50 [22]，作为实验中的默认设置；我们的方法也使用ViT-B/16 [11]或ViT-L/14 [11]进行进一步评估，如表2所示。请注意，文本和图像编码器来自同一CLIP模型，并在整个流程中保持不变。当我们的方法与ResNet-50实现时，每个文本特征或图像特征的维度为\(C = {1024}\)，而在ViT-B/16的情况下为\(C = {512}\)，在ViT-L/14的情况下为\(C = {768}\)。</p></div><p>Learning style word vectors. We follow prompt learning methods \(\left\lbrack  {{70},{71}}\right\rbrack\) when learning the word vectors. Using a zero-mean Gaussian distribution with 0.02 standard deviation,we randomly initialize \(K\) style word vectors \({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\) , where \(K = {80}\) . The dimension of each style word vector is \(D = {512}\) when the proposed method is implemented with ResNet-50 [22] or ViT-B/16 [11],while \(D = {768}\) in the case of ViT-L/14 [11]. Each \(i\) -th style word vector \({\mathbf{s}}_{i}\) is trained by the prompt loss \({\mathcal{L}}_{\text{prompt }}\) for \(L = {100}\) training iterations using the SGD optimizer with 0.002 learning rate and 0.9 momentum. The number of classes \(N\) is pre-defined by each target task definition,e.g., \(N = {345}\) for DomainNet [48].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>学习风格词向量。我们在学习词向量时遵循提示学习方法\(\left\lbrack  {{70},{71}}\right\rbrack\)。使用标准差为0.02的零均值高斯分布，我们随机初始化\(K\)风格词向量\({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{K}\)，其中\(K = {80}\)。当所提出的方法与ResNet-50 [22]或ViT-B/16 [11]实现时，每个风格词向量的维度为\(D = {512}\)，而在ViT-L/14 [11]的情况下为\(D = {768}\)。每个\(i\) -th风格词向量\({\mathbf{s}}_{i}\)通过提示损失\({\mathcal{L}}_{\text{prompt }}\)进行训练，训练迭代次数为\(L = {100}\)，使用学习率为0.002和动量为0.9的SGD优化器。类别数量\(N\)由每个目标任务定义预先定义，例如，\(N = {345}\)用于DomainNet [48]。</p></div><p>Training a linear classifier. The classifier is trained for 50 epochs using the SGD optimizer with 0.005 learning rate, 0.9 momentum, and a batch size of 128. In ArcFace [8] loss, its scaling factor is set to 5 with 0.5 angular margin.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>训练线性分类器。分类器使用学习率为0.005、动量为0.9和批量大小为128的SGD优化器训练50个周期。在ArcFace [8]损失中，其缩放因子设置为5，角度边距为0.5。</p></div><p>Inference. Input images are pre-processed in the same way with the CLIP model; resized to \({224} \times  {224}\) and normalized.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>推理。输入图像以与CLIP模型相同的方式进行预处理；调整大小为\({224} \times  {224}\)并进行归一化。</p></div><hr>
<!-- Footnote --><p><a href="https://github.com/openai/CLIP">https://github.com/openai/CLIP</a></p><!-- Footnote -->
<hr><!-- Media --><table><tbody><tr><td rowspan="2">Method</td><td colspan="2">Configuration</td><td colspan="5">Accuracy (%)</td></tr><tr><td>Source Domain</td><td>Domain Description</td><td>PACS</td><td>VLCS</td><td>OfficeHome</td><td>DomainNet</td><td>Avg.</td></tr><tr><td colspan="8">ResNet-50 [22] with pre-trained weights on ImageNet [6]</td></tr><tr><td>DANN [19]</td><td>✓</td><td>-</td><td>83.6±0.4</td><td>\( {78.6} \pm  {0.4} \)</td><td>\( {65.9} \pm  {0.6} \)</td><td>\( {38.3} \pm  {0.1} \)</td><td>66.6</td></tr><tr><td>RSC [25]</td><td>✓</td><td>-</td><td>\( {85.2} \pm  {0.9} \)</td><td>77.1±0.5</td><td>\( {65.5} \pm  {0.9} \)</td><td>\( {38.9} \pm  {0.5} \)</td><td>66.7</td></tr><tr><td>MLDG [35]</td><td>✓</td><td>-</td><td>\( {84.9} \pm  {1.0} \)</td><td>77.2±0.4</td><td>\( {66.8} \pm  {0.6} \)</td><td>41.2±0.1</td><td>67.5</td></tr><tr><td>SagNet [46]</td><td>✓</td><td>-</td><td>\( \mathbf{{86.3}} \pm  {0.2} \)</td><td>77.8±0.5</td><td>68.1±0.1</td><td>40.3±0.1</td><td>68.1</td></tr><tr><td>SelfReg [28]</td><td>✓</td><td>-</td><td>85.6±0.4</td><td>77.8±0.9</td><td>67.9±0.7</td><td>42.8±0.0</td><td>68.5</td></tr><tr><td>GVRT [44]</td><td>✓</td><td>-</td><td>85.1±0.3</td><td>79.0±0.2</td><td>70.1±0.1</td><td>44.1±0.1</td><td>69.6</td></tr><tr><td>MIRO [5]</td><td>✓</td><td>-</td><td>\( {85.4} \pm  {0.4} \)</td><td>79.0±0.0</td><td>\( {70.5} \pm  {0.4} \)</td><td>\( {44.3} \pm  {0.2} \)</td><td>69.8</td></tr><tr><td colspan="8">ResNet-50 [22] with pre-trained weights from CLIP [50]</td></tr><tr><td>ZS-CLIP (C) [50]</td><td>-</td><td>-</td><td>90.6±0.0</td><td>76.0±0.0</td><td>\( {68.6} \pm  {0.0} \)</td><td>45.6±0.0</td><td>70.2</td></tr><tr><td>CAD [53]</td><td>✓</td><td>-</td><td>90.0±0.6</td><td>\( {81.2} \pm  {0.6} \)</td><td>70.5±0.3</td><td>\( {45.5} \pm  {2.1} \)</td><td>71.8</td></tr><tr><td>ZS-CLIP (PC) [50]</td><td>-</td><td>✓</td><td>90.7±0.0</td><td>80.1±0.0</td><td>\( {72.0} \pm  {0.0} \)</td><td>46.2±0.0</td><td>72.3</td></tr><tr><td>PromptStyler</td><td>-</td><td>-</td><td>93.2±0.0</td><td>\( \mathbf{{82.3}} \pm  {0.1} \)</td><td>73.6±0.1</td><td>49.5±0.0</td><td>74.7</td></tr><tr><td colspan="8">ViT-B/16 [11] with pre-trained weights from CLIP [50]</td></tr><tr><td>ZS-CLIP (C) [50]</td><td>-</td><td>-</td><td>95.7±0.0</td><td>76.4±0.0</td><td>79.9±0.0</td><td>57.8±0.0</td><td>77.5</td></tr><tr><td>MIRO [5]</td><td>✓</td><td>-</td><td>95.6</td><td>82.2</td><td>82.5</td><td>54.0</td><td>78.6</td></tr><tr><td>ZS-CLIP (PC) [50]</td><td>-</td><td>✓</td><td>96.1±0.0</td><td>\( {82.4} \pm  {0.0} \)</td><td>\( {82.3} \pm  {0.0} \)</td><td>\( {57.7} \pm  {0.0} \)</td><td>79.6</td></tr><tr><td>PromptStyler</td><td>-</td><td>-</td><td>\( {97.2} \pm  {0.1} \)</td><td>\( \mathbf{{82.9}} \pm  {0.0} \)</td><td>\( \mathbf{{83.6}} \pm  {0.0} \)</td><td>59.4±0.0</td><td>80.8</td></tr><tr><td colspan="8">ViT-L/14 [11] with pre-trained weights from CLIP [50]</td></tr><tr><td>ZS-CLIP (C) [50]</td><td>-</td><td>-</td><td>97.6±0.0</td><td>77.5±0.0</td><td>\( {85.9} \pm  {0.0} \)</td><td>63.3±0.0</td><td>81.1</td></tr><tr><td>ZS-CLIP (PC) [50]</td><td>-</td><td>✓</td><td>\( {98.5} \pm  {0.0} \)</td><td>\( \mathbf{{82.4}} \pm  {0.0} \)</td><td>\( {86.9} \pm  {0.0} \)</td><td>\( {64.0} \pm  {0.0} \)</td><td>83.0</td></tr><tr><td>PromptStyler</td><td>-</td><td>-</td><td>98.6±0.0</td><td>\( \mathbf{{82.4}} \pm  {0.2} \)</td><td>89.1±0.0</td><td>65.5±0.0</td><td>83.9</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">方法</td><td colspan="2">配置</td><td colspan="5">准确率 (%)</td></tr><tr><td>源领域</td><td>领域描述</td><td>PACS</td><td>VLCS</td><td>OfficeHome</td><td>DomainNet</td><td>平均</td></tr><tr><td colspan="8">ResNet-50 [22] 在 ImageNet [6] 上的预训练权重</td></tr><tr><td>DANN [19]</td><td>✓</td><td>-</td><td>83.6±0.4</td><td>\( {78.6} \pm  {0.4} \)</td><td>\( {65.9} \pm  {0.6} \)</td><td>\( {38.3} \pm  {0.1} \)</td><td>66.6</td></tr><tr><td>RSC [25]</td><td>✓</td><td>-</td><td>\( {85.2} \pm  {0.9} \)</td><td>77.1±0.5</td><td>\( {65.5} \pm  {0.9} \)</td><td>\( {38.9} \pm  {0.5} \)</td><td>66.7</td></tr><tr><td>MLDG [35]</td><td>✓</td><td>-</td><td>\( {84.9} \pm  {1.0} \)</td><td>77.2±0.4</td><td>\( {66.8} \pm  {0.6} \)</td><td>41.2±0.1</td><td>67.5</td></tr><tr><td>SagNet [46]</td><td>✓</td><td>-</td><td>\( \mathbf{{86.3}} \pm  {0.2} \)</td><td>77.8±0.5</td><td>68.1±0.1</td><td>40.3±0.1</td><td>68.1</td></tr><tr><td>SelfReg [28]</td><td>✓</td><td>-</td><td>85.6±0.4</td><td>77.8±0.9</td><td>67.9±0.7</td><td>42.8±0.0</td><td>68.5</td></tr><tr><td>GVRT [44]</td><td>✓</td><td>-</td><td>85.1±0.3</td><td>79.0±0.2</td><td>70.1±0.1</td><td>44.1±0.1</td><td>69.6</td></tr><tr><td>MIRO [5]</td><td>✓</td><td>-</td><td>\( {85.4} \pm  {0.4} \)</td><td>79.0±0.0</td><td>\( {70.5} \pm  {0.4} \)</td><td>\( {44.3} \pm  {0.2} \)</td><td>69.8</td></tr><tr><td colspan="8">ResNet-50 [22] 从 CLIP [50] 获取的预训练权重</td></tr><tr><td>ZS-CLIP (C) [50]</td><td>-</td><td>-</td><td>90.6±0.0</td><td>76.0±0.0</td><td>\( {68.6} \pm  {0.0} \)</td><td>45.6±0.0</td><td>70.2</td></tr><tr><td>CAD [53]</td><td>✓</td><td>-</td><td>90.0±0.6</td><td>\( {81.2} \pm  {0.6} \)</td><td>70.5±0.3</td><td>\( {45.5} \pm  {2.1} \)</td><td>71.8</td></tr><tr><td>ZS-CLIP (PC) [50]</td><td>-</td><td>✓</td><td>90.7±0.0</td><td>80.1±0.0</td><td>\( {72.0} \pm  {0.0} \)</td><td>46.2±0.0</td><td>72.3</td></tr><tr><td>PromptStyler</td><td>-</td><td>-</td><td>93.2±0.0</td><td>\( \mathbf{{82.3}} \pm  {0.1} \)</td><td>73.6±0.1</td><td>49.5±0.0</td><td>74.7</td></tr><tr><td colspan="8">ViT-B/16 [11] 从 CLIP [50] 获取的预训练权重</td></tr><tr><td>ZS-CLIP (C) [50]</td><td>-</td><td>-</td><td>95.7±0.0</td><td>76.4±0.0</td><td>79.9±0.0</td><td>57.8±0.0</td><td>77.5</td></tr><tr><td>MIRO [5]</td><td>✓</td><td>-</td><td>95.6</td><td>82.2</td><td>82.5</td><td>54.0</td><td>78.6</td></tr><tr><td>ZS-CLIP (PC) [50]</td><td>-</td><td>✓</td><td>96.1±0.0</td><td>\( {82.4} \pm  {0.0} \)</td><td>\( {82.3} \pm  {0.0} \)</td><td>\( {57.7} \pm  {0.0} \)</td><td>79.6</td></tr><tr><td>PromptStyler</td><td>-</td><td>-</td><td>\( {97.2} \pm  {0.1} \)</td><td>\( \mathbf{{82.9}} \pm  {0.0} \)</td><td>\( \mathbf{{83.6}} \pm  {0.0} \)</td><td>59.4±0.0</td><td>80.8</td></tr><tr><td colspan="8">ViT-L/14 [11] 从 CLIP [50] 获取的预训练权重</td></tr><tr><td>ZS-CLIP (C) [50]</td><td>-</td><td>-</td><td>97.6±0.0</td><td>77.5±0.0</td><td>\( {85.9} \pm  {0.0} \)</td><td>63.3±0.0</td><td>81.1</td></tr><tr><td>ZS-CLIP (PC) [50]</td><td>-</td><td>✓</td><td>\( {98.5} \pm  {0.0} \)</td><td>\( \mathbf{{82.4}} \pm  {0.0} \)</td><td>\( {86.9} \pm  {0.0} \)</td><td>\( {64.0} \pm  {0.0} \)</td><td>83.0</td></tr><tr><td>PromptStyler</td><td>-</td><td>-</td><td>98.6±0.0</td><td>\( \mathbf{{82.4}} \pm  {0.2} \)</td><td>89.1±0.0</td><td>65.5±0.0</td><td>83.9</td></tr></tbody></table></div><p>Table 2: Comparison with the state-of-the-art domain generalization methods. ZS-CLIP (C) denotes zero-shot CLIP using "[class]" as its text prompt, and ZS-CLIP (PC) indicates zero-shot CLIP using "a photo of a [class]" as its text prompt. Note that PromptStyler does not exploit any source domain data and domain descriptions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表2：与最先进的领域泛化方法的比较。ZS-CLIP (C) 表示使用 "[class]" 作为文本提示的零-shot CLIP，而 ZS-CLIP (PC) 表示使用 "a photo of a [class]" 作为文本提示的零-shot CLIP。请注意，PromptStyler 不利用任何源领域数据和领域描述。</p></div><!-- Media --><h3>4.3. Evaluations</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.3. 评估</h3></div><p>Main results. PromptStyler achieves the state of the art in every evaluation on PACS [34], VLCS [15], OfficeHome [60] and DomainNet [48] as shown in Table 2. Note that all existing methods utilize source domain data except for zero-shot CLIP [50] in Table 2. Compared with zero-shot CLIP which generates each text feature using a domain-agnostic prompt ("[class]"), PromptStyler largely outperforms its records in all evaluations. Our method also shows higher accuracy compared with zero-shot CLIP which produces each text feature using a domain-specific prompt ("a photo of a [class]"), even though we do not exploit any domain descriptions. These results confirm that the proposed method effectively improves the generalization capability of the chosen pre-trained model, i.e., CLIP, without using any images by simulating various distribution shifts via prompts in its latent space.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>主要结果。PromptStyler 在 PACS [34]、VLCS [15]、OfficeHome [60] 和 DomainNet [48] 的每项评估中都达到了最先进的水平，如表2所示。请注意，除了表2中的零-shot CLIP [50] 外，所有现有方法都利用了源领域数据。与使用领域无关提示 ("[class]") 生成每个文本特征的零-shot CLIP 相比，PromptStyler 在所有评估中大幅超越了其记录。我们的方法在与使用领域特定提示 ("a photo of a [class]") 生成每个文本特征的零-shot CLIP 的比较中也显示出更高的准确性，尽管我们没有利用任何领域描述。这些结果确认了所提出的方法有效提高了所选预训练模型（即 CLIP）的泛化能力，而无需使用任何图像，通过在其潜在空间中通过提示模拟各种分布变化。</p></div><p>Computational evaluations. In Table 3, we compare our PromptStyler and zero-shot CLIP [50] in terms of the number of parameters and inference speed; the inference speed was measured using a single RTX 3090 GPU with a batch size</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>计算评估。在表3中，我们比较了我们的 PromptStyler 和零-shot CLIP [50] 在参数数量和推理速度方面；推理速度是使用单个 RTX 3090 GPU 和一个批量大小测量的。</p></div><!-- Media --><table><tbody><tr><td rowspan="2">Method</td><td colspan="2">Inference Module</td><td rowspan="2">#Params</td><td rowspan="2">FPS</td></tr><tr><td>Image Encoder</td><td>Text Encoder</td></tr><tr><td colspan="5">OfficeHome (65 classes)</td></tr><tr><td>ZS-CLIP [50]</td><td>✓</td><td>✓</td><td>102.0M</td><td>1.6</td></tr><tr><td>PromptStyler</td><td>✓</td><td>-</td><td>\( \mathbf{{38.4M}} \)</td><td>72.9</td></tr><tr><td colspan="5">DomainNet (345 classes)</td></tr><tr><td>ZS-CLIP [50]</td><td>✓</td><td>✓</td><td>102.0M</td><td>0.3</td></tr><tr><td>PromptStyler</td><td>✓</td><td>-</td><td>\( \mathbf{{38.7M}} \)</td><td>72.9</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">方法</td><td colspan="2">推理模块</td><td rowspan="2">#参数</td><td rowspan="2">帧率</td></tr><tr><td>图像编码器</td><td>文本编码器</td></tr><tr><td colspan="5">OfficeHome（65类）</td></tr><tr><td>ZS-CLIP [50]</td><td>✓</td><td>✓</td><td>102.0M</td><td>1.6</td></tr><tr><td>提示样式器</td><td>✓</td><td>-</td><td>\( \mathbf{{38.4M}} \)</td><td>72.9</td></tr><tr><td colspan="5">DomainNet（345类）</td></tr><tr><td>ZS-CLIP [50]</td><td>✓</td><td>✓</td><td>102.0M</td><td>0.3</td></tr><tr><td>提示样式器</td><td>✓</td><td>-</td><td>\( \mathbf{{38.7M}} \)</td><td>72.9</td></tr></tbody></table></div><p>Table 3: The number of parameters and inference speed on OfficeHome [60] and DomainNet [48] using ResNet-50 [22] as an image encoder. Note that CLIP [50] text encoder needs to generate text features as many as the number of classes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表3：在使用ResNet-50（ResNet-50）作为图像编码器的情况下，OfficeHome（OfficeHome）[60]和DomainNet（DomainNet）[48]上的参数数量和推理速度。请注意，CLIP（CLIP）文本编码器需要生成与类别数量相同的文本特征。</p></div><!-- Media --><p>of 1 . Note that we do not exploit a text encoder at inference time,which makes our model \(\sim  {2.6} \times\) smaller and \(\sim  {243} \times\) faster compared with CLIP. Regarding the inference speed, the proposed model is about \({45} \times\) faster for the target task OfficeHome [60] (65 classes) and it is about \({243} \times\) faster for the target task DomainNet [48] (345 classes).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>注意，我们在推理时并未利用文本编码器，这使得我们的模型\(\sim  {2.6} \times\)更小且\(\sim  {243} \times\)更快，与CLIP相比。关于推理速度，所提议的模型在目标任务OfficeHome（OfficeHome）[60]（65个类别）上快约\({45} \times\)，在目标任务DomainNet（DomainNet）[48]（345个类别）上快约\({243} \times\)。</p></div><!-- Media --><!-- figureText: (a) \( {\mathcal{L}}_{\text{style }} \) (b) \( {\mathcal{L}}_{\text{content }} \) (c) \( {\mathcal{L}}_{\text{style }} + {\mathcal{L}}_{\text{content }} \) --><img src="https://cdn.noedgeai.com/01972c4e-e368-7ba0-a0ce-4063bebda408_6.jpg?x=142&#x26;y=208&#x26;w=1464&#x26;h=429&#x26;r=0"><p>Figure 4: t-SNE [58] visualization results for the target task VLCS [15] (5 classes) using synthesized style-content features. We visualize such features obtained from the learned 80 style word vectors \({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{80}\) and all the 5 classes (bird,car,chair,dog, person). Different colors denote features obtained from different style word vectors, and different shapes indicate features obtained from different class names. We only colorize features from the first 10 styles \({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{10}\) . Combining the style diversity loss \({\mathcal{L}}_{\text{style }}\) and content consistency loss \({\mathcal{L}}_{\text{content }}\) leads to diverse styles while preserving content information.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图4：使用合成的风格-内容特征对目标任务VLCS（VLCS）[15]（5个类别）的t-SNE（t-SNE）[58]可视化结果。我们可视化了从学习到的80个风格词向量\({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{80}\)和所有5个类别（鸟、车、椅子、狗、人物）中获得的特征。不同的颜色表示从不同的风格词向量获得的特征，不同的形状表示从不同类别名称获得的特征。我们仅对前10种风格的特征进行了着色\({\left\{  {\mathbf{s}}_{i}\right\}  }_{i = 1}^{10}\)。结合风格多样性损失\({\mathcal{L}}_{\text{style }}\)和内容一致性损失\({\mathcal{L}}_{\text{content }}\)可以在保留内容信息的同时实现多样化的风格。</p></div><img src="https://cdn.noedgeai.com/01972c4e-e368-7ba0-a0ce-4063bebda408_6.jpg?x=141&#x26;y=836&#x26;w=1467&#x26;h=244&#x26;r=0"><p>Figure 5: Text-to-Image synthesis results using style-content features (from "a \({\mathbf{S}}_{ * }\) style of a cat") with 6 different style word vectors. By leveraging the proposed method, we could learn a variety of styles while not distorting content information.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图5：使用风格-内容特征（来自“猫的\({\mathbf{S}}_{ * }\)风格”）和6个不同的风格词向量的文本到图像合成结果。通过利用所提议的方法，我们可以学习多种风格，同时不扭曲内容信息。</p></div><table><tbody><tr><td rowspan="2">\( {\mathcal{L}}_{\text{style }} \)</td><td rowspan="2">\( {\mathcal{L}}_{\text{content }} \)</td><td colspan="5">Accuracy (%)</td></tr><tr><td>Pacs</td><td>VLCS</td><td>OfficeHome</td><td>DomainNet</td><td>Avg.</td></tr><tr><td>-</td><td>-</td><td>92.6</td><td>78.3</td><td>72.2</td><td>48.0</td><td>72.8</td></tr><tr><td>✓</td><td>-</td><td>92.3</td><td>80.9</td><td>71.5</td><td>48.2</td><td>73.2</td></tr><tr><td>-</td><td>✓</td><td>92.8</td><td>80.5</td><td>72.4</td><td>48.6</td><td>73.6</td></tr><tr><td>✓</td><td>✓</td><td>93.2</td><td>82.3</td><td>73.6</td><td>49.5</td><td>74.7</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">\( {\mathcal{L}}_{\text{style }} \)</td><td rowspan="2">\( {\mathcal{L}}_{\text{content }} \)</td><td colspan="5">准确率 (%)</td></tr><tr><td>医学影像存档与通信系统</td><td>视觉语言与计算系统</td><td>办公室与家庭</td><td>领域网络</td><td>平均值</td></tr><tr><td>-</td><td>-</td><td>92.6</td><td>78.3</td><td>72.2</td><td>48.0</td><td>72.8</td></tr><tr><td>✓</td><td>-</td><td>92.3</td><td>80.9</td><td>71.5</td><td>48.2</td><td>73.2</td></tr><tr><td>-</td><td>✓</td><td>92.8</td><td>80.5</td><td>72.4</td><td>48.6</td><td>73.6</td></tr><tr><td>✓</td><td>✓</td><td>93.2</td><td>82.3</td><td>73.6</td><td>49.5</td><td>74.7</td></tr></tbody></table></div><p>Table 4: Ablation study on the style diversity loss \({\mathcal{L}}_{\text{style }}\) and content consistency loss \({\mathcal{L}}_{\text{content }}\) used in the prompt loss.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表4：关于在提示损失中使用的风格多样性损失\({\mathcal{L}}_{\text{style }}\)和内容一致性损失\({\mathcal{L}}_{\text{content }}\)的消融研究。</p></div><!-- Media --><p>t-SNE visualization results. In Figure 4, we qualitatively evaluate style-content features synthesized for the target task VLCS [15] (5 classes) using t-SNE [58] visualization. As shown in Figure 4(c), PromptStyler generates a variety of styles while not distorting content information; style-content features obtained from the same class name share similar semantics with diverse variations. This result confirms that we could effectively simulate various distribution shifts in the latent space of a large-scale vision-language model by synthesizing diverse styles via learnable style word vectors. Text-to-Image synthesis results. In Figure 5, we visualize style-content features (from "a \({S}_{ * }\) style of a cat") via diffusers library. \({}^{2}\) These results are obtained with 6 different style word vectors, where the word vectors are learned for the target task DomainNet [48] using ViT-L/14 [11] model.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>t-SNE可视化结果。在图4中，我们对为目标任务VLCS [15]（5个类别）合成的风格-内容特征进行了定性评估，使用了t-SNE [58]可视化。如图4(c)所示，PromptStyler生成了多种风格，同时没有扭曲内容信息；来自同一类别名称的风格-内容特征在语义上具有相似性，并且有多样的变化。这个结果确认了我们可以通过合成多样的风格来有效模拟大规模视觉-语言模型潜在空间中的各种分布变化，方法是使用可学习的风格词向量。文本到图像合成结果。在图5中，我们通过diffusers库可视化风格-内容特征（来自“猫的\({S}_{ * }\)风格”）。\({}^{2}\)这些结果是通过6个不同的风格词向量获得的，这些词向量是针对目标任务DomainNet [48]使用ViT-L/14 [11]模型学习的。</p></div><!-- Media --><table><tbody><tr><td rowspan="2">\( {\mathcal{L}}_{\text{class }} \)</td><td colspan="5">Accuracy (%)</td></tr><tr><td>Pacs</td><td>VLCS</td><td>OfficeHome</td><td>DomainNet</td><td>Avg.</td></tr><tr><td>Softmax</td><td>92.5</td><td>81.2</td><td>72.3</td><td>48.6</td><td>73.7</td></tr><tr><td>ArcFace</td><td>93.2</td><td>82.3</td><td>73.6</td><td>49.5</td><td>74.7</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">\( {\mathcal{L}}_{\text{class }} \)</td><td colspan="5">准确率 (%)</td></tr><tr><td>医学影像存档与通信系统</td><td>视觉语言与计算机系统</td><td>办公室家庭</td><td>领域网络</td><td>平均值</td></tr><tr><td>软最大值</td><td>92.5</td><td>81.2</td><td>72.3</td><td>48.6</td><td>73.7</td></tr><tr><td>弧面脸</td><td>93.2</td><td>82.3</td><td>73.6</td><td>49.5</td><td>74.7</td></tr></tbody></table></div><p>Table 5: Ablation study on the classification loss \({\mathcal{L}}_{\text{class }}\) used for training a linear classifier in the proposed framework.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表5：在所提框架中用于训练线性分类器的分类损失的消融研究。</p></div><!-- Media --><h3>4.4. More analyses</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.4. 更多分析</h3></div><p>Ablation study on the prompt loss. In Table 4, we evaluate the effects of \({\mathcal{L}}_{\text{style }}\) and \({\mathcal{L}}_{\text{content }}\) in \({\mathcal{L}}_{\text{prompt }}\) used for learning style words. Interestingly, our method also achieves state-of-the-art results even without using these losses, i.e., the proposed framework (Fig. 3) is substantially effective by itself. Note that randomly initialized style word vectors are already diverse, and CLIP [50] is already good at extracting correct content information from a style-content prompt even without training the word vectors using \({\mathcal{L}}_{\text{content }}\) . When we learn style word vectors using \({\mathcal{L}}_{\text{style }}\) without \({\mathcal{L}}_{\text{content }}\) , style-content features obtained from different class names share more similar features than those from the same class name (Fig. 4(a)). On the other hand,using \({\mathcal{L}}_{\text{content }}\) without \({\mathcal{L}}_{\text{style }}\) leads to less diverse style-content features (Fig. 4(b)). When incorporating both losses, we could generate diverse styles while not distorting content information (Fig. 4(c)).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>对提示损失的消融研究。在表4中，我们评估了用于学习风格词的\({\mathcal{L}}_{\text{style }}\)和\({\mathcal{L}}_{\text{content }}\)在\({\mathcal{L}}_{\text{prompt }}\)中的效果。有趣的是，即使不使用这些损失，我们的方法也能取得最先进的结果，即所提框架（图3）本身就非常有效。请注意，随机初始化的风格词向量已经具有多样性，而CLIP [50]即使在不使用\({\mathcal{L}}_{\text{content }}\)训练词向量的情况下，也能很好地从风格-内容提示中提取正确的内容信息。当我们使用\({\mathcal{L}}_{\text{style }}\)学习风格词向量而不使用\({\mathcal{L}}_{\text{content }}\)时，来自不同类名的风格-内容特征共享的相似特征比来自同一类名的特征更多（图4(a)）。另一方面，使用\({\mathcal{L}}_{\text{content }}\)而不使用\({\mathcal{L}}_{\text{style }}\)会导致风格-内容特征的多样性降低（图4(b)）。当结合两种损失时，我们可以生成多样的风格，同时不扭曲内容信息（图4(c)）。</p></div><hr>
<!-- Footnote --><p>\({}^{2}\) <a href="https://github.com/huggingface/diffusers">https://github.com/huggingface/diffusers</a></p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{2}\) <a href="https://github.com/huggingface/diffusers">https://github.com/huggingface/diffusers</a></p></div><!-- Footnote -->
<hr><!-- Media --><!-- figureText: PACS VLCS OfficeHome DomainNet 81 54 78 Accuracy (%) 52 50 48 46 44 Accuracy (%) 75 72 69 66 42 63 1510 20 40 60 80 100 1510 20 40 60 80 100 Number of style word vectors Number of style word vectors 88 Accuracy (%) 96 Accuracy (%) 85 82 79 73 92 90 86 1510 20 40 60 80 100 1510 20 40 60 80 100 Number of style word vectors Number of style word vectors --><img src="https://cdn.noedgeai.com/01972c4e-e368-7ba0-a0ce-4063bebda408_7.jpg?x=135&#x26;y=205&#x26;w=1479&#x26;h=337&#x26;r=0"><p>Figure 6: Top-1 classification accuracy on the PACS [34], VLCS [15], OfficeHome [60] and DomainNet [48] datasets with regard to the number of learnable style word vectors \(K\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6：关于可学习风格词向量数量\(K\)在PACS [34]、VLCS [15]、OfficeHome [60]和DomainNet [48]数据集上的Top-1分类准确率。</p></div><!-- figureText: PACS VLCS OfficeHome DomainNet 78 54 Accuracy (%) 76 Accuracy (%) 50 48 46 74 72 70 68 44 66 1510 20 40 60 100 1510 2 40 100 Number of training iterations Number of training iterations 96 90 Accuracy (%) 92 Accuracy (%) 87 84 78 72 1510 20 60 80 1510 20 40 60 80 Number of training iterations Number of training iteration --><img src="https://cdn.noedgeai.com/01972c4e-e368-7ba0-a0ce-4063bebda408_7.jpg?x=137&#x26;y=622&#x26;w=1474&#x26;h=333&#x26;r=0"><p>Figure 7: Top-1 classification accuracy on the PACS [34], VLCS [15], OfficeHome [60] and DomainNet [48] datasets with regard to the number of training iterations \(L\) for learning each style word vector \({\mathbf{s}}_{i}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图7：关于学习每个风格词向量\({\mathbf{s}}_{i}\)的训练迭代次数\(L\)在PACS [34]、VLCS [15]、OfficeHome [60]和DomainNet [48]数据集上的Top-1分类准确率。</p></div><table><tbody><tr><td rowspan="2">Method</td><td colspan="2">Configuration</td><td>Accuracy (%)</td></tr><tr><td>Source Domain</td><td>Domain Description</td><td>Terra Incognita</td></tr><tr><td colspan="4">ResNet-50 [22] with pre-trained weights on ImageNet [6]</td></tr><tr><td>SelfReg [28]</td><td>✓</td><td>-</td><td>\( {47.0} \pm  {0.3} \)</td></tr><tr><td>GVRT [44]</td><td>✓</td><td>-</td><td>\( {48.0} \pm  {0.2} \)</td></tr><tr><td colspan="4">ResNet-50 [22] with pre-trained weights from CLIP [50]</td></tr><tr><td>ZS-CLIP (C) [50]</td><td>-</td><td>-</td><td>19.5±0.0</td></tr><tr><td>ZS-CLIP (PC) [50]</td><td>-</td><td>✓</td><td>23.8±0.0</td></tr><tr><td>PromptStyler</td><td>-</td><td>-</td><td>\( {30.5} \pm  {0.8} \)</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">方法</td><td colspan="2">配置</td><td>准确率 (%)</td></tr><tr><td>源领域</td><td>领域描述</td><td>未知领域</td></tr><tr><td colspan="4">在ImageNet [6]上预训练的ResNet-50 [22]</td></tr><tr><td>自我调节 [28]</td><td>✓</td><td>-</td><td>\( {47.0} \pm  {0.3} \)</td></tr><tr><td>GVRT [44]</td><td>✓</td><td>-</td><td>\( {48.0} \pm  {0.2} \)</td></tr><tr><td colspan="4">从CLIP [50]获取预训练权重的ResNet-50 [22]</td></tr><tr><td>ZS-CLIP (C) [50]</td><td>-</td><td>-</td><td>19.5±0.0</td></tr><tr><td>ZS-CLIP (PC) [50]</td><td>-</td><td>✓</td><td>23.8±0.0</td></tr><tr><td>提示样式器</td><td>-</td><td>-</td><td>\( {30.5} \pm  {0.8} \)</td></tr></tbody></table></div><p>Table 6: Unsatisfactory results obtained from CLIP [50] without using source domain data from Terra Incognita [1].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表6：未使用Terra Incognita [1] 的源域数据而从CLIP [50]获得的不满意结果。</p></div><!-- Media --><p>Ablation study on the classification loss. In Table 5, we evaluate the effects of the original Softmax loss and the angular Softmax loss (i.e., ArcFace [8]). PromptStyler also achieves the state of the art using the original one, which validates that the performance improvement of our method mainly comes from the proposed framework (Fig. 3). Note that the angular Softmax loss further improves its accuracy by leveraging the hyperspherical joint vision-language space. Effect of the number of styles. We evaluate our method with regard to the number of style word vectors \(K\) as shown in Figure 6. Interestingly, our PromptStyler outperforms CLIP [50] using just 5 styles. This evaluation shows that 20 style word vectors are enough to achieve decent results.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>分类损失的消融研究。在表5中，我们评估了原始Softmax损失和角度Softmax损失（即ArcFace [8]）的效果。PromptStyler使用原始损失也达到了最先进的水平，这验证了我们的方法性能提升主要来自于所提出的框架（图3）。值得注意的是，角度Softmax损失通过利用超球面联合视觉-语言空间进一步提高了其准确性。风格数量的影响。我们评估了我们的方法与风格词向量数量\(K\)的关系，如图6所示。有趣的是，我们的PromptStyler仅使用5种风格就超越了CLIP [50]。这一评估表明，20个风格词向量足以获得良好的结果。</p></div><p>Effect of the number of iterations. We evaluate our method with regard to the number of training iterations \(L\) for learning each style word vector as shown in Figure 7. This evaluation shows that 20 iterations are enough to achieve decent results.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>迭代次数的影响。我们评估了我们的方法与每个风格词向量学习的训练迭代次数\(L\)的关系，如图7所示。这一评估表明，20次迭代足以获得良好的结果。</p></div><h2>5. Limitation</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>5. 限制</h2></div><p>The performance of our method depends on the quality of the joint vision-language space constructed by the chosen vision-language model. For example, although PromptStyler largely outperforms its base model (i.e., CLIP [50]) in all evaluations, our method shows lower accuracy on the Terra Incognita dataset [1] compared with other methods which utilize several images from the dataset as shown in Table 6. The main reason for this might be due to the low accuracy of CLIP on the dataset. Nevertheless, given that our method consistently outperforms its base model in every evaluation, this limitation could be alleviated with the development of large-scale vision-language models.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的方法性能依赖于所选视觉-语言模型构建的联合视觉-语言空间的质量。例如，尽管PromptStyler在所有评估中大幅超越其基础模型（即CLIP [50]），但与其他利用数据集中的多张图像的方法相比，我们的方法在Terra Incognita数据集[1]上的准确性较低，如表6所示。这主要可能是由于CLIP在该数据集上的低准确性。然而，考虑到我们的方法在每次评估中始终超越其基础模型，这一限制可能会随着大规模视觉-语言模型的发展而得到缓解。</p></div><h2>6. Conclusion</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>6. 结论</h2></div><p>We have presented a novel method that synthesizes a variety of styles in a joint vision-language space via learnable style words without exploiting any images to deal with source-free domain generalization. PromptStyler simulates various distribution shifts in the latent space of a large-scale pre-trained model, which could effectively improve its generalization capability. The proposed method achieves state-of-the-art results without using any source domain data on multiple domain generalization benchmarks. We hope that future work could apply our method to other tasks using different large-scale vision-language models.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们提出了一种新颖的方法，通过可学习的风格词在联合视觉-语言空间中合成多种风格，而无需利用任何图像来处理无源域泛化。PromptStyler在大规模预训练模型的潜在空间中模拟各种分布变化，这可以有效提高其泛化能力。所提出的方法在多个领域泛化基准上实现了最先进的结果，而无需使用任何源域数据。我们希望未来的工作能够将我们的方法应用于使用不同大规模视觉-语言模型的其他任务。</p></div><p>Acknowledgment. This work was supported by the Agency for Defense Development grant funded by the Korean government. References</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>致谢。本工作得到了韩国政府资助的国防开发局的支持。参考文献</p></div><p>[1] Sara Beery, Grant van Horn, and Pietro Perona. Recognition in Terra Incognita. In Proceedings of the European Conference on Computer Vision (ECCV), 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[1] Sara Beery, Grant van Horn, 和 Pietro Perona. 在Terra Incognita中的识别. 见于2018年欧洲计算机视觉会议（ECCV）论文集。</p></div><p>[2] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of Representations for Domain Adaptation. In Advances in Neural Information Processing Systems (NIPS), 2006.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[2] Shai Ben-David, John Blitzer, Koby Crammer, 和 Fernando Pereira. 域适应的表示分析. 见于2006年神经信息处理系统进展（NIPS）论文集。</p></div><p>[3] Fabio Maria Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain Generalization by Solving Jigsaw Puzzles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[3] Fabio Maria Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, 和 Tatiana Tommasi. 通过解决拼图来进行域泛化. 见于2019年IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集。</p></div><p>[4] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. SWAD: Domain Generalization by Seeking Flat Minima. In Advances in Neural Information Processing Systems (NeurIPS), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[4] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, 和 Sungrae Park. SWAD：通过寻找平坦最小值进行域泛化. 见于2021年神经信息处理系统进展（NeurIPS）论文集。</p></div><p>[5] Junbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk Chun. Domain Generalization by Mutual-Information Regularization with Pre-trained Models. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[5] Junbum Cha, Kyungjae Lee, Sungrae Park, 和 Sanghyuk Chun. 通过与预训练模型的互信息正则化进行域泛化. 见于2022年欧洲计算机视觉会议（ECCV）论文集。</p></div><p>[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248-255, 2009.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, 和 Li Fei-Fei. ImageNet：一个大规模分层图像数据库. 见于2009年IEEE计算机视觉与模式识别会议，页248-255，2009。</p></div><p>[7] Jiankang Deng, Jia Guo, Tongliang Liu, Mingming Gong, and Stefanos Zafeiriou. Sub-center ArcFace: Boosting Face Recognition by Large-Scale Noisy Web Faces. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[7] Jiankang Deng, Jia Guo, Tongliang Liu, Mingming Gong, 和 Stefanos Zafeiriou. 子中心ArcFace：通过大规模噪声网络人脸提升人脸识别. 见于2020年欧洲计算机视觉会议（ECCV）论文集。</p></div><p>[8] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. ArcFace: Additive Angular Margin Loss for Deep Face Recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[8] Jiankang Deng, Jia Guo, Niannan Xue, 和 Stefanos Zafeiriou. ArcFace：用于深度人脸识别的加性角度边际损失. 见于2019年IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集。</p></div><p>[9] Jiankang Deng and Stefanos Zafeririou. ArcFace for Disguised Face Recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[9] Jiankang Deng 和 Stefanos Zafeririou. ArcFace 用于伪装人脸识别. 载于 IEEE/CVF 国际计算机视觉会议 (ICCV) 论文集, 2019.</p></div><p>[10] Yu Ding, Lei Wang, Bin Liang, Shuming Liang, Yang Wang, and Fang Chen. Domain Generalization by Learning and Removing Domain-specific Features. In Advances in Neural Information Processing Systems (NeurIPS), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[10] Yu Ding, Lei Wang, Bin Liang, Shuming Liang, Yang Wang 和 Fang Chen. 通过学习和去除领域特定特征实现领域泛化. 载于神经信息处理系统进展 (NeurIPS), 2022.</p></div><p>[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations (ICLR), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit 和 Neil Houlsby. 一张图像价值 16x16 个词: 用于大规模图像识别的变换器. 载于学习表征国际会议 (ICLR), 2021.</p></div><p>[12] Qi Dou, Daniel C. Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain Generalization via Model-Agnostic Learning of Semantic Features. In Advances in Neural Information Processing Systems (NeurIPS), 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[12] Qi Dou, Daniel C. Castro, Konstantinos Kamnitsas 和 Ben Glocker. 通过模型无关的语义特征学习实现领域泛化. 载于神经信息处理系统进展 (NeurIPS), 2019.</p></div><p>[13] Lisa Dunlap, Clara Mohri, Devin Guillory, Han Zhang, Trevor Darrell, Joseph E. Gonzalez, Aditi Raghunathan, and Anja Rohrbach. Using Language to Extend to Unseen Domains. In International Conference on Learning Representations (ICLR), 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[13] Lisa Dunlap, Clara Mohri, Devin Guillory, Han Zhang, Trevor Darrell, Joseph E. Gonzalez, Aditi Raghunathan 和 Anja Rohrbach. 使用语言扩展到未见领域. 载于学习表征国际会议 (ICLR), 2023.</p></div><p>[14] Xinjie Fan, Qifei Wang, Junjie Ke, Feng Yang, Boqing Gong, and Mingyuan Zhou. Adversarially Adaptive Normalization for Single Domain Generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[14] Xinjie Fan, Qifei Wang, Junjie Ke, Feng Yang, Boqing Gong 和 Mingyuan Zhou. 针对单一领域泛化的对抗自适应归一化. 载于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集, 2021.</p></div><p>[15] Chen Fang, Ye Xu, and Daniel N. Rockmore. Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[15] Chen Fang, Ye Xu 和 Daniel N. Rockmore. 无偏度度量学习: 利用多个数据集和网络图像来减轻偏差. 载于 IEEE 国际计算机视觉会议 (ICCV) 论文集, 2013.</p></div><p>[16] Ahmed Frikha, Haokun Chen, Denis Krompaß, Thomas Run-kler, and Volker Tresp. Towards Data-Free Domain Generalization. In Asian Conference on Machine Learning (ACML), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[16] Ahmed Frikha, Haokun Chen, Denis Krompaß, Thomas Runkler 和 Volker Tresp. 朝着无数据领域泛化的方向. 载于亚洲机器学习会议 (ACML), 2022.</p></div><p>[17] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. In International Conference on Learning Representations (ICLR), 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[17] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik 和 Daniel Cohen-Or. 一张图像价值一个词: 使用文本反转个性化文本到图像生成. 载于学习表征国际会议 (ICLR), 2023.</p></div><p>[18] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators. ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[18] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik 和 Daniel Cohen-Or. StyleGAN-NADA: 基于 CLIP 的图像生成器领域适应. ACM 图形学会会刊 (Proc. SIGGRAPH Asia), 2022.</p></div><p>[19] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-Adversarial Training of Neural Networks. In Journal of Machine Learning Research (JMLR), 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[19] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand 和 Victor Lempitsky. 神经网络的领域对抗训练. 载于机器学习研究期刊 (JMLR), 2016.</p></div><p>[20] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. CLIP-Adapter: Better Vision-Language Models with Feature Adapters. arXiv preprint arXiv:2110.04544, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[20] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li 和 Yu Qiao. CLIP-Adapter: 通过特征适配器提升视觉-语言模型. arXiv 预印本 arXiv:2110.04544, 2021.</p></div><p>[21] Ishaan Gulrajani and David Lopez-Paz. In Search of Lost Domain Generalization. In International Conference on Learning Representations (ICLR), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[21] Ishaan Gulrajani 和 David Lopez-Paz. 寻找失落的领域泛化. 载于学习表征国际会议 (ICLR), 2021.</p></div><p>[22] He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[22] He, Kaiming 和 Zhang, Xiangyu 和 Ren, Shaoqing 和 Sun, Jian. 深度残差学习用于图像识别. 载于 IEEE 计算机视觉与模式识别会议 (CVPR) 论文集, 页码 770-778, 2016.</p></div><p>[23] Dan Hendrycks and Thomas Dietterich. Benchmarking Neural Network Robustness to Common Corruptions and Perturbations. In International Conference on Learning Representations (ICLR), 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[23] Dan Hendrycks 和 Thomas Dietterich. 基准测试神经网络对常见干扰和扰动的鲁棒性. 载于学习表征国际会议 (ICLR), 2019.</p></div><p>[24] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A. Efros, and Trevor Darrell. CyCADA: Cycle-Consistent Adversarial Domain Adaptation. In International Conference on Machine Learning (ICML), 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[24] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A. Efros 和 Trevor Darrell. CyCADA: 循环一致的对抗领域适应. 载于机器学习国际会议 (ICML), 2018.</p></div><p>[25] Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. Self-Challenging Improves Cross-Domain Generalization. In Proceedings of the European Conference on Computer Vision \(\left( {ECCV}\right) ,{2020}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[25] Zeyi Huang, Haohan Wang, Eric P. Xing 和 Dong Huang. 自我挑战提高跨域泛化能力. 在欧洲计算机视觉会议论文集中 \(\left( {ECCV}\right) ,{2020}\) .</p></div><p>[26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. In International Conference on Machine Learning (ICML), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li 和 Tom Duerig. 利用噪声文本监督扩大视觉和视觉-语言表示学习. 在国际机器学习会议 (ICML), 2021.</p></div><p>[27] Juwon Kang, Sohyun Lee, Namyup Kim, and Suha Kwak. Style Neophile: Constantly Seeking Novel Styles for Domain Generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[27] Juwon Kang, Sohyun Lee, Namyup Kim 和 Suha Kwak. 风格新手：不断寻求新颖风格以实现领域泛化. 在IEEE/CVF计算机视觉与模式识别会议论文集中 (CVPR), 2022.</p></div><p>[28] Daehee Kim, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. SelfReg: Self-supervised Contrastive Regularization for Domain Generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[28] Daehee Kim, Seunghyun Park, Jinkyu Kim 和 Jaekoo Lee. SelfReg：用于领域泛化的自监督对比正则化. 在IEEE/CVF国际计算机视觉会议论文集中 (ICCV), 2021.</p></div><p>[29] Donghyun Kim, Kaihong Wang, Stan Sclaroff, and Kate Saenko. A Broad Study of Pre-training for Domain Generalization and Adaptation. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[29] Donghyun Kim, Kaihong Wang, Stan Sclaroff 和 Kate Saenko. 领域泛化和适应的预训练广泛研究. 在欧洲计算机视觉会议论文集中 (ECCV), 2022.</p></div><p>[30] Dimitrios Kollias and Stefanos Zafeiriou. Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace. In Proceedings of the British Machine Vision Conference (BMVC), 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[30] Dimitrios Kollias 和 Stefanos Zafeiriou. 表情、情感、动作单元识别：Aff-Wild2、多任务学习和ArcFace. 在英国机器视觉会议论文集中 (BMVC), 2019.</p></div><p>[31] Gihyun Kwon and Jong Chul Ye. CLIPstyler: Image Style Transfer with a Single Text Condition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[31] Gihyun Kwon 和 Jong Chul Ye. CLIPstyler：基于单一文本条件的图像风格迁移. 在IEEE/CVF计算机视觉与模式识别会议论文集中 (CVPR), 2022.</p></div><p>[32] Sohyun Lee, Taeyoung Son, and Suha Kwak. FIFO: Learning Fog-invariant Features for Foggy Scene Segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[32] Sohyun Lee, Taeyoung Son 和 Suha Kwak. FIFO：学习雾不变特征以进行雾霾场景分割. 在IEEE/CVF计算机视觉与模式识别会议论文集中 (CVPR), 2022.</p></div><p>[33] Yoonho Lee, Annie S. Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgical Fine-Tuning Improves Adaptation to Distribution Shifts. In International Conference on Learning Representations (ICLR), 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[33] Yoonho Lee, Annie S. Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang 和 Chelsea Finn. 外科微调改善对分布变化的适应. 在国际学习表征会议 (ICLR), 2023.</p></div><p>[34] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, Broader and Artier Domain Generalization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[34] Da Li, Yongxin Yang, Yi-Zhe Song 和 Timothy M. Hospedales. 更深、更广和更艺术的领域泛化. 在IEEE国际计算机视觉会议论文集中 (ICCV), 2017.</p></div><p>[35] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to Generalize: Meta-Learning for Domain Generalization. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[35] Da Li, Yongxin Yang, Yi-Zhe Song 和 Timothy M. Hospedales. 学习泛化：用于领域泛化的元学习. 在AAAI人工智能会议论文集中 (AAAI), 2018.</p></div><p>[36] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M. Hospedales. Episodic Training for Domain Generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[36] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song 和 Timothy M. Hospedales. 用于领域泛化的情节训练. 在IEEE/CVF国际计算机视觉会议论文集中 (ICCV), 2019.</p></div><p>[37] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain Generalization With Adversarial Feature Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[37] Haoliang Li, Sinno Jialin Pan, Shiqi Wang 和 Alex C. Kot. 通过对抗特征学习实现领域泛化. 在IEEE/CVF计算机视觉与模式识别会议论文集中 (CVPR), 2018.</p></div><p>[38] Lei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xi-aoyue Mi, Zhengze Yu, Xiaoya Li, and Boyang xia. Progressive Domain Expansion Network for Single Domain Generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[38] Lei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xi-aoyue Mi, Zhengze Yu, Xiaoya Li 和 Boyang xia. 单领域泛化的渐进领域扩展网络. 在IEEE/CVF计算机视觉与模式识别会议论文集中 (CVPR), 2021.</p></div><p>[39] Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[39] Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung 和 James Zou. 注意差距：理解多模态对比表示学习中的模态差距. 在神经信息处理系统进展 (NeurIPS), 2022.</p></div><p>[40] Boxiao Liu, Guanglu Song, Manyuan Zhang, Haihang You, and Yu Liu. Switchable K-Class Hyperplanes for Noise-Robust Representation Learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[40] Boxiao Liu, Guanglu Song, Manyuan Zhang, Haihang You 和 Yu Liu. 可切换的K类超平面用于抗噪声表示学习. 在IEEE/CVF国际计算机视觉会议论文集中 (ICCV), 2021.</p></div><p>[41] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian. Prompt Distribution Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[41] 吕宇宁，刘建壮，张永刚，刘雅静，田欣梅。提示分布学习。在2022年IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集中。</p></div><p>[42] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain Generalization using Causal Matching. In International Conference on Machine Learning (ICML), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[42] Divyat Mahajan，Shruti Tople和Amit Sharma。使用因果匹配的领域泛化。在2021年国际机器学习会议（ICML）上。</p></div><p>[43] Toshihiko Matsuura and Tatsuya Harada. Domain Generalization Using a Mixture of Multiple Latent Domains. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[43] 松浦俊彦和原田达也。使用多重潜在领域混合的领域泛化。在2020年AAAI人工智能会议（AAAI）论文集中。</p></div><p>[44] Seonwoo Min, Nokyung Park, Siwon Kim, Seunghyun Park, and Jinkyu Kim. Grounding Visual Representations with Texts for Domain Generalization. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[44] 闵善宇，朴诺京，金时源，朴胜贤和金振奎。通过文本为领域泛化奠定视觉表示基础。在2022年欧洲计算机视觉会议（ECCV）论文集中。</p></div><p>[45] Krikamol Muandet, David Balduzzi, and Bernhard Schölkopf. Domain Generalization via Invariant Feature Representation. In International Conference on Machine Learning (ICML), 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[45] Muandet Krikamol，David Balduzzi和Bernhard Schölkopf。通过不变特征表示实现领域泛化。在2013年国际机器学习会议（ICML）上。</p></div><p>[46] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing Domain Gap by Reducing Style Bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[46] 南贤燮，李贤宰，朴钟灿，尹元俊和柳东根。通过减少风格偏差来缩小领域差距。在2021年IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集中。</p></div><p>[47] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[47] Or Patashnik，吴宗泽，Eli Shechtman，Daniel Cohen-Or和Dani Lischinski。StyleCLIP：基于文本的StyleGAN图像操控。在2021年IEEE/CVF国际计算机视觉会议（ICCV）论文集中。</p></div><p>[48] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment Matching for Multi-Source Domain Adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[48] 彭兴超，白沁勋，夏希德，黄子俊，凯特·萨恩科和王博。多源领域适应的时刻匹配。在2019年IEEE/CVF国际计算机视觉会议（ICCV）论文集中。</p></div><p>[49] Fengchun Qiao, Long Zhao, and Xi Peng. Learning to Learn Single Domain Generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[49] 乔丰春，赵龙和彭希。学习单领域泛化。在2020年IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集中。</p></div><p>[50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In International Conference on Machine Learning (ICML), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[50] Alec Radford，金钟旭，Chris Hallacy，Aditya Ramesh，Gabriel Goh，Sandhini Agarwal，Girish Sastry，Amanda Askell，Pamela Mishkin，Jack Clark，Gretchen Krueger和Ilya Sutskever。从自然语言监督中学习可转移的视觉模型。在2021年国际机器学习会议（ICML）上。</p></div><p>[51] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization. In International Conference on Machine Learning (ICML), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[51] Alexandre Rame，Corentin Dancette和Matthieu Cord。Fishr：用于分布外泛化的不变梯度方差。在2022年国际机器学习会议（ICML）上。</p></div><p>[52] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet Classifiers Generalize to ImageNet? In International Conference on Machine Learning (ICML), 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[52] Benjamin Recht，Rebecca Roelofs，Ludwig Schmidt和Vaishaal Shankar。ImageNet分类器能否泛化到ImageNet？在2019年国际机器学习会议（ICML）上。</p></div><p>[53] Yangjun Ruan, Yann Dubois, and Chris J. Maddison. Optimal Representations for Covariate Shift. In International Conference on Learning Representations (ICLR), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[53] 阮阳君，Yann Dubois和Chris J. Maddison。协变量偏移的最优表示。在2022年学习表示国际会议（ICLR）上。</p></div><p>[54] Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-Supervised Domain Adaptation via Minimax Entropy. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[54] 齐藤邦明，金东贤，Stan Sclaroff，Trevor Darrell和凯特·萨恩科。通过极小极大熵进行半监督领域适应。在2019年IEEE/CVF国际计算机视觉会议（ICCV）论文集中。</p></div><p>[55] Seonguk Seo, Yumin Suh, Dongwan Kim, Geeho Kim, Jong-woo Han, and Bohyung Han. Learning to Optimize Domain Specific Normalization for Domain Generalization. In Proceedings of the European Conference on Computer Vision \(\left( {ECCV}\right) ,{2020}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[55] 徐成旭，徐裕敏，金东完，金基浩，韩钟宇和韩博亨。学习优化领域特定归一化以实现领域泛化。在欧洲计算机视觉会议论文集中\(\left( {ECCV}\right) ,{2020}\)。</p></div><p>[56] Baochen Sun, Jiashi Feng, and Kate Saenko. Return of Frustratingly Easy Domain Adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[56] 孙宝晨，冯佳士和凯特·萨恩科。令人沮丧的简单领域适应的回归。在2016年AAAI人工智能会议（AAAI）论文集中。</p></div><p>[57] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial Discriminative Domain Adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[57] Eric Tzeng, Judy Hoffman, Kate Saenko 和 Trevor Darrell. 对抗性判别领域适应. 载于2017年IEEE计算机视觉与模式识别会议（CVPR）论文集.</p></div><p>[58] Laurens van der Maaten and Geoffrey Hinton. Visualizing Data using t-SNE. In Journal of Machine Learning Research (JMLR), 2008.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[58] Laurens van der Maaten 和 Geoffrey Hinton. 使用t-SNE可视化数据. 载于2008年机器学习研究期刊（JMLR）.</p></div><p>[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems (NIPS), 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser 和 Illia Polosukhin. 注意力机制是你所需要的一切. 载于2017年神经信息处理系统进展（NIPS）.</p></div><p>[60] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep Hashing Network for Unsupervised Domain Adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[60] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty 和 Sethuraman Panchanathan. 用于无监督领域适应的深度哈希网络. 载于2017年IEEE计算机视觉与模式识别会议（CVPR）论文集.</p></div><p>[61] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip S. Yu. Generalizing to Unseen Domains: A Survey on Domain Generalization. In IEEE Transactions on Knowledge and Data Engineering (TKDE), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[61] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng 和 Philip S. Yu. 泛化到未见领域：领域泛化的调查. 载于2021年IEEE知识与数据工程汇刊（TKDE）.</p></div><p>[62] Zijian Wang, Yadan Luo, Ruihong Qiu, Zi Huang, and Mahsa Baktashmotlagh. Learning to Diversify for Single Domain Generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[62] Zijian Wang, Yadan Luo, Ruihong Qiu, Zi Huang 和 Mahsa Baktashmotlagh. 学习多样化以实现单领域泛化. 载于2021年IEEE/CVF国际计算机视觉会议（ICCV）论文集.</p></div><p>[63] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A Fourier-based Framework for Domain Generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[63] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang 和 Qi Tian. 基于傅里叶的领域泛化框架. 载于2021年IEEE计算机视觉与模式识别会议（CVPR）论文集.</p></div><p>[64] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. Vision-Language Pre-Training with Triple Contrastive Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[64] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi 和 Junzhou Huang. 通过三重对比学习进行视觉-语言预训练. 载于2022年IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集.</p></div><p>[65] Dingyi Zhang, Yingming Li, and Zhongfei Zhang. Deep metric learning with spherical embedding. In Advances in Neural Information Processing Systems (NeurIPS), 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[65] Dingyi Zhang, Yingming Li 和 Zhongfei Zhang. 具有球面嵌入的深度度量学习. 载于2020年神经信息处理系统进展（NeurIPS）.</p></div><p>[66] Renrui Zhang, Zhang Wei, Rongyao Fang, Peng Gao, Kun-chang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[66] Renrui Zhang, Zhang Wei, Rongyao Fang, Peng Gao, Kun-chang Li, Jifeng Dai, Yu Qiao 和 Hongsheng Li. Tip-Adapter：无训练的CLIP少样本分类适应. 载于2022年欧洲计算机视觉会议（ECCV）论文集.</p></div><p>[67] Yuhui Zhang, Jeff Z. HaoChen, Shih-Cheng Huang, Kuan-Chieh Wang, James Zou, and Serena Yeung. Diagnosing and Rectifying Vision Models using Language. In International Conference on Learning Representations (ICLR), 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[67] Yuhui Zhang, Jeff Z. HaoChen, Shih-Cheng Huang, Kuan-Chieh Wang, James Zou 和 Serena Yeung. 使用语言诊断和修正视觉模型. 载于2023年国际学习表征会议（ICLR）.</p></div><p>[68] Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J. Gordon. On Learning Invariant Representation for Domain Adaptation. In International Conference on Machine Learning (ICML), 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[68] Han Zhao, Remi Tachet des Combes, Kun Zhang 和 Geoffrey J. Gordon. 关于学习不变表示以进行领域适应. 载于2019年国际机器学习会议（ICML）.</p></div><p>[69] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain Generalization: A Survey. In IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[69] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang 和 Chen Change Loy. 领域泛化：一项调查. 载于2022年IEEE模式分析与机器智能汇刊（TPAMI）.</p></div><p>[70] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Zi-wei Liu. Conditional Prompt Learning for Vision-Language Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[70] Kaiyang Zhou, Jingkang Yang, Chen Change Loy 和 Zi-wei Liu. 面向视觉-语言模型的条件提示学习. 载于2022年IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集.</p></div><p>[71] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to Prompt for Vision-Language Models. In International Journal of Computer Vision (IJCV), 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[71] Kaiyang Zhou, Jingkang Yang, Chen Change Loy 和 Ziwei Liu. 学习为视觉-语言模型提供提示. 载于2022年国际计算机视觉期刊（IJCV）.</p></div><p>[72] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Deep Domain-Adversarial Image Generation for Domain Generalisation. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[72] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales 和 Tao Xiang. 用于领域泛化的深度领域对抗图像生成. 载于2020年AAAI人工智能会议论文集（AAAI）.</p></div><p>[73] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to Generate Novel Domains for Domain Generalization. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[73] 周开扬、杨永鑫、蒂莫西·霍斯佩达莱斯和向涛。学习生成新领域以实现领域泛化。在2020年欧洲计算机视觉会议（ECCV）论文集中。</p></div><p>[74] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain Generalization with MixStyle. In International Conference on Learning Representations (ICLR), 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[74] 周开扬、杨永鑫、乔宇和向涛。使用MixStyle进行领域泛化。在2021年国际学习表征会议（ICLR）上。</p></div>
      </body>
    </html>
  