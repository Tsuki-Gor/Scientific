
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>2017-Select-additive learning</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      </head>
      <body>
        <div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="255,268"><div style="height: auto;"><h1><div><div xt-marked="ok">SELECT-ADDITIVE LEARNING: IMPROVING GENERALIZATION IN MULTIMODAL <xt-mark w="sentiment" style="color: #ff8861 !important">SENTIMENT</xt-mark> ANALYSIS<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">选择性加法学习：提高多模态情感分析的泛化能力</div></div></div></h1></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="255,268"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="358,402"><div style="height: auto;"><div><div><div>Haohan Wang, Aaksha Meghawat, Louis-Philippe Morency and Eric P. Xing<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">Haohan Wang, Aaksha Meghawat, Louis-Philippe Morency 和 Eric P. Xing</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="666,486"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="666,486"><div style="height: auto;"><div><div><div>Language Technologies Institute<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">语言技术研究所</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="694,530"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="694,530"><div style="height: auto;"><div><div><div>School of Computer Science<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">计算机科学学院</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="698,570"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="698,570"><div style="height: auto;"><div><div><div>Carnegie Mellon University<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">卡内基梅隆大学</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="529,611"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="529,611"><div style="height: auto;"><div><div><div>{haohanw, aaksham, morency, epxing}@cs.cmu.edu</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="428,724"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="428,724"><div style="height: auto;"><h2><div><div>ABSTRACT<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">摘要</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="428,724"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="154,771"><div style="height: auto;"><div><div><div>Multimodal sentiment analysis is drawing an increasing amount of attention these days. It enables mining of opinions in video reviews which are now available aplenty on online platforms. However, multimodal sentiment analysis has only a few high-quality data sets annotated for training machine learning algorithms. These limited resources restrict the generalizability of models, where, for example, the unique characteristics of a few speakers (e.g., wearing glasses) may become a confounding factor for the sentiment classification task. In this paper, we propose a Select-Additive Learning (SAL) procedure that improves the generalizability of trained neural networks for multimodal sentiment analysis. In our experiments, we show that our SAL approach improves prediction accuracy significantly in all three modalities (verbal, acoustic, visual), as well as in their fusion. Our results show that SAL, even when trained on one dataset, achieves good generalization across two new test datasets.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">多模态情感分析近年来引起了越来越多的关注。它使得在如今在线平台上大量存在的视频评论中挖掘意见成为可能。然而，多模态情感分析仅有少量高质量的数据集被标注用于训练机器学习算法。这些有限的资源限制了模型的泛化能力，例如，少数说话者的独特特征（例如，佩戴眼镜）可能成为情感分类任务的混淆因素。本文提出了一种选择性加法学习（SAL）程序，旨在提高训练神经网络在多模态情感分析中的泛化能力。在我们的实验中，我们展示了SAL方法在所有三种模态（语言、声学、视觉）及其融合中显著提高了预测准确性。我们的结果表明，即使在一个数据集上训练，SAL也能在两个新的测试数据集上实现良好的泛化。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="195,1384"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="195,1384"><div style="height: auto;"><div><div><div>Index Terms- multimodal, sentiment analysis, cross-datasets, generalization, cross-individual<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">关键词- 多模态，情感分析，跨数据集，泛化，跨个体</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="370,1491"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="370,1491"><div style="height: auto;"><h2><div><div>1. INTRODUCTION<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">1. 引言</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="370,1491"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="153,1565"><div style="height: auto;"><div><div><div>Sentiment analysis is the automatic identification of the private state of a human mind with a focus on determining whether this state is positive, negative or neutral [1]. It has been extensively studied in the last few decades [2], primarily based on textual data. With the recent proliferation of online avenues for sharing multimedia content, people are posting more and more videos with opinions. The opinions are expressed through the spoken word (verbal modality), how these words are spoken (acoustic modality) and what gestures and facial expressions accompany the spoken words (visual modality). Multimodal sentiment analysis extends traditional textual sentiment analysis by analyzing all three modalities present in online videos, including acoustic and visual modalities [3, 4].<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">情感分析是自动识别人类心理状态的过程，重点在于确定该状态是积极的、消极的还是中性的 [1]。在过去几十年中，这一领域得到了广泛研究 [2]，主要基于文本数据。随着在线多媒体内容分享渠道的迅速增加，人们发布的意见视频也越来越多。这些意见通过口头表达（语言模态）、这些话语的表达方式（声学模态）以及伴随口语的手势和面部表情（视觉模态）来表达。多模态情感分析通过分析在线视频中存在的所有三种模态，包括声学和视觉模态，扩展了传统的文本情感分析 [3, 4]。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="196,2060"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="196,2060"><div style="height: auto;"><div><div><div>To foster research in this area, a few datasets have been created with quality annotations for sentiment [1, 5, 6], but unfortunately the total number of annotations is still in the order of thousand samples. These limited-size resources make it challenging for conventional machine learning algorithms to generalize well across datasets. In these limited data scenarios, a unique characteristic of a few speakers in the training dataset (e.g., wearing glasses) can end up creating a confounding effect with the sentiment classification task. Fig. 1(a) shows one illustrative example where limited data can bring in learning and generalization challenges. Since in this example all individuals with glasses happen to be expressing negative sentiments, the classifier ends up learning an association between visual appearance of wearing glasses and negative sentiment (see Fig. 1(b)) .<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">为了促进该领域的研究，已经创建了一些具有高质量情感标注的数据集 [1, 5, 6]，但不幸的是，标注的总数量仍然在千个样本的范围内。这些有限规模的资源使得传统机器学习算法在跨数据集的泛化上面临挑战。在这些有限数据的场景中，训练数据集中少数说话者的独特特征（例如，佩戴眼镜）可能会对情感分类任务产生混淆效应。图1(a)展示了一个说明性的例子，其中有限的数据可能带来学习和泛化的挑战。由于在这个例子中，所有佩戴眼镜的个体恰好都表达了消极情感，因此分类器最终学习到了佩戴眼镜的视觉外观与消极情感之间的关联（见图1(b)）。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="923,717"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="923,717"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="923,717"><div style="height: auto;"><div><div class="text-center"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/0192e32f-c235-7adb-8c25-056c00c78188_0.jpg?x=923&amp;y=717&amp;w=718&amp;h=443"></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="923,717"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="920,1178"><div style="height: auto;"><div><div><div>Fig. 1: An illustrative data set demonstrating the "wearing glass" as a confounding factor. Due to the limited amount of data, the model learns that wearing glasses means negative sentiment, which is only applicable to this training data set. (Orange denotes negative sentiment; green denotes positive sentiment; blue denotes correct rules &amp; red denotes incorrect rules).<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图1：一个说明性数据集，展示了“佩戴眼镜”作为混淆因素。由于数据量有限，模型学习到佩戴眼镜意味着消极情感，这仅适用于该训练数据集。（橙色表示消极情感；绿色表示积极情感；蓝色表示正确规则；红色表示错误规则）。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="909,716"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-0="909,716"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-694c5f2d-bcb1-4075-a158-4ca9bf712e52" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="963,1956"><div style="height: auto;"><div><div><div>The role that the visual appearance plays here is statistically known as a confounding factor [7, 8]. To generalize across datasets and individuals, a robust multimodal sentiment classifier should not include features from a confounding factor. In other words, the prediction of the sentiment polarity should not be dependent on the speaker's unique characteristic, namely, identity of the speaker.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">视觉外观在这里所扮演的角色在统计上被称为混杂因素 [7, 8]。为了在数据集和个体之间进行概括，一个稳健的多模态情感分类器不应包含来自混杂因素的特征。换句话说，情感极性的预测不应依赖于说话者的独特特征，即说话者的身份。</div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="197,285"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="197,285"><div style="height: auto;"><span style="display: inline;"><div><div><div>Before going further in this research agenda, we studied if the confounding factor also exists in the real-world multimodal sentiment analysis datasets. In the MOSI multimodal sentiment analysis data set [6], we tested for the null hypothesis that sentiment is independent of an individual's identity. Chi-square test obtains a p-value of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="134" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-script style="vertical-align: 0.393em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>1.202</mn></mrow><mo>×</mo><msup><mrow data-mjx-texclass="ORD"><mn>10</mn></mrow><mrow data-mjx-texclass="ORD"><mo>−</mo><mrow data-mjx-texclass="ORD"><mn>19</mn></mrow></mrow></msup></math></mjx-assistive-mml></mjx-container> ,which strongly suggests the dependence between individual identities and the expressed sentiment. Consequently, naively applying machine learning algorithms on this dataset will most likely result in a suboptimal model that misinterprets an individual's identity as prescient information for sentiment.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">在进一步推进这一研究议程之前，我们研究了混杂因素是否也存在于真实世界的多模态情感分析数据集中。在MOSI多模态情感分析数据集 [6] 中，我们检验了情感与个体身份独立的零假设。卡方检验得到了 p 值为 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="135" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-script style="vertical-align: 0.393em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>1.202</mn></mrow><mo>×</mo><msup><mrow data-mjx-texclass="ORD"><mn>10</mn></mrow><mrow data-mjx-texclass="ORD"><mo>−</mo><mrow data-mjx-texclass="ORD"><mn>19</mn></mrow></mrow></msup></math></mjx-assistive-mml></mjx-container>，这强烈表明个体身份与表达的情感之间存在依赖关系。因此，天真地在该数据集上应用机器学习算法很可能会导致一个次优模型，将个体身份误解为情感的先验信息。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="197,673"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="197,673"><div style="height: auto;"><div><div><div>In this paper, we propose a Select-Additive Learning (SAL) procedure that addresses the confounding factor problem, specifically for neural architectures such as convolutional neural networks. Our proposed SAL approach is a two-phase procedure with the (Selection phase and Addition phase. During the Selection phase, SAL identifies the confounding factors from the latent representation learned by neural networks. During the Addition phase, SAL forces the original model to discard (or rather, give less importance to) the confounding elements by adding Gaussian noises to these representations. We conduct extensive experiments to test the performances of state-of-art neural-based models enhanced by SAL. All our experiments are performed in a person-independent setting, where subjects in the test set are different from the training and validation sets. We test the generalization with both, within-data and across-datasets experiments.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">在本文中，我们提出了一种选择-加法学习（SAL）程序，专门解决混杂因素问题，特别是针对卷积神经网络等神经架构。我们提出的SAL方法是一个两阶段程序，包括选择阶段和加法阶段。在选择阶段，SAL从神经网络学习的潜在表示中识别混杂因素。在加法阶段，SAL通过向这些表示添加高斯噪声，迫使原始模型丢弃（或更确切地说，降低对）混杂元素的重要性。我们进行了广泛的实验，以测试通过SAL增强的最先进的基于神经网络的模型的性能。我们所有的实验都是在一个与人无关的设置中进行的，其中测试集中的受试者与训练集和验证集中的受试者不同。我们通过数据内和跨数据集的实验来测试泛化能力。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="363,1275"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="363,1275"><div style="height: auto;"><h2><div><div>2. RELATED WORK<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">2. 相关工作</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="363,1275"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="152,1352"><div style="height: auto;"><div><div><div>Multimodal data has been studied for a variety of applications to analyze human behaviors, including person detection and identification [9, 10], human action recognition [11, 12], face recognition [13, 14], as well as sentiment analysis.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">多模态数据已被研究用于分析人类行为的各种应用，包括人脸检测和识别 [9, 10]、人类动作识别 [11, 12]、面部识别 [13, 14] 以及情感分析。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="196,1497"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="196,1497"><div style="height: auto;"><div><div><div>Originating from analysis of the textual modality, sentiment analysis has been carried out at the word level [15], phrase level [16] and sentence level [17]. [18] performed sentiment analysis on audio data by first transcribing the spoken words and then performing sentiment analysis. Related to audio-based sentiment analysis is the task of estimating emotional state of the speaker from audio input [19]. For the visual modality, the Facial Action Coding System [20] laid the groundwork for analyzing facial expressions and emotions. Recently, convolutional neural networks were used to discover the affective regions for sentiment on still images<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">情感分析起源于对文本模态的分析，已在词级 [15]、短语级 [16] 和句子级 [17] 进行。 [18] 通过首先转录口语并随后进行情感分析，对音频数据进行了情感分析。与基于音频的情感分析相关的任务是从音频输入中估计说话者的情感状态 [19]。对于视觉模态，面部动作编码系统 [20] 为分析面部表情和情感奠定了基础。最近，卷积神经网络被用于发现静态图像中情感的影响区域。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="153,1884"><div style="height: auto;"><div><div><div>[21].</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="198,1919"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="198,1919"><div style="height: auto;"><div><div><div>The fusion of textual, acoustic and visual modalities for sentiment analysis has drawn increasing attention lately [1]. A variety of methods have been proposed and extensively discussed in recent years [22, 23, 24]. The state-of-the-art performance is achieved with a Convolutional Neural Network<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">文本、声学和视觉模态在情感分析中的融合最近引起了越来越多的关注 [1]。近年来提出并广泛讨论了多种方法 [22, 23, 24]。最先进的性能是通过卷积神经网络实现的。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="154,2096"><div style="height: auto;"><div><div><div>[25].</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="1015,204"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="1015,204"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="1015,204"><div style="height: auto;"><div><div class="text-center"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/0192e32f-c235-7adb-8c25-056c00c78188_1.jpg?x=1015&amp;y=204&amp;w=535&amp;h=354"></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="1015,204"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="921,579"><div style="height: auto;"><div><div><div>Fig. 2: The SAL architecture is achieved by a simple extension of a general deep learning discriminative classifier. The purple part is the original deep learning model. The red part is the extension SAL introduces. The extension network is connected to the original network via a Gaussian Sampling Layer.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 2：SAL架构是通过对一般深度学习判别分类器的简单扩展实现的。紫色部分是原始深度学习模型。红色部分是SAL引入的扩展。扩展网络通过高斯采样层与原始网络连接。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="915,194"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="915,194"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="964,854"><div style="height: auto;"><div><div><div>Our proposed Select-Additive Learning (SAL) procedure improves the generalizability of neural networks. Our experiments show improved prediction accuracy for all three modalities (verbal, acoustic and visual) as well as for multimodal fusion. The following section introduces our proposed Select-Additive Learning procedure.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们提出的选择-加性学习（SAL）过程提高了神经网络的泛化能力。我们的实验显示，在所有三种模态（语言、声学和视觉）以及多模态融合方面，预测准确性都有所提高。以下部分介绍我们提出的选择-加性学习过程。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="1033,1103"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="1033,1103"><div style="height: auto;"><h2><div><div>3. SELECT-ADDITIVE LEARNING<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3. 选择-加性学习</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="1033,1103"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="919,1181"><div style="height: auto;"><div><div><div>The main goal of our work is to increase the generalizability of multimodal sentiment prediction models by encouraging the model to consider sentiment-associated features (i.e. people are smiling while expressing positive sentiment) more than the identity-related features (i.e. wearing glasses).<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们工作的主要目标是通过鼓励模型更多地考虑与情感相关的特征（即人们在表达积极情感时微笑）而非与身份相关的特征（即佩戴眼镜），来提高多模态情感预测模型的泛化能力。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="963,1361"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="963,1361"><div style="height: auto;"><span style="display: inline;"><div><div><div>We formalize the problem by defining an input feature matrix <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="136" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>X</mi></math></mjx-assistive-mml></mjx-container> of size <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="137" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>×</mo><mi>p</mi></math></mjx-assistive-mml></mjx-container> that encodes the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="138" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></mjx-assistive-mml></mjx-container> features for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="139" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> utterances. In the multimodal scenario, p will be the total number of verbal, acoustic and visual features. We also define a vector <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="140" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math></mjx-assistive-mml></mjx-container> of size <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="141" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>×</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> which represents the sentiment of each utterance. Finally,we define a new matrix <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="142" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Z</mi></math></mjx-assistive-mml></mjx-container> which encodes for each utterance the speaker identity in a one-hot matrix of size <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="143" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>×</mo><mi>m</mi></math></mjx-assistive-mml></mjx-container> where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="144" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi></math></mjx-assistive-mml></mjx-container> represents the total number of unique individuals in the dataset.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们通过定义一个输入特征矩阵 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="145" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>X</mi></math></mjx-assistive-mml></mjx-container>，其大小为 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="146" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>×</mo><mi>p</mi></math></mjx-assistive-mml></mjx-container>，来形式化这个问题，该矩阵编码了 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="147" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></mjx-assistive-mml></mjx-container> 特征用于 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="148" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> 发言。在多模态场景中，p 将是口头、声学和视觉特征的总数。我们还定义了一个大小为 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="149" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>×</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> 的向量 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="150" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math></mjx-assistive-mml></mjx-container>，它表示每个发言的情感。最后，我们定义了一个新的矩阵 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="151" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Z</mi></math></mjx-assistive-mml></mjx-container>，该矩阵为每个发言编码说话者身份，采用大小为 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="152" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>×</mo><mi>m</mi></math></mjx-assistive-mml></mjx-container> 的独热矩阵，其中 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="153" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi></math></mjx-assistive-mml></mjx-container> 表示数据集中独特个体的总数。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="920,1707"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="920,1707"><div style="height: auto;"><h3><div><div>3.1. Select-Additive Learning Architecture<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.1. 选择-加性学习架构</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="920,1707"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-eefc8d0f-6bcc-44ae-9e0c-25fee5ea2d93" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="920,1777"><div style="height: auto;"><span style="display: inline;"><div><div><div>Our proposed SAL procedure is designed to enhance a preexisting (i.e. pre-trained model) discriminative neural network to be more robust against confounding factors. To formally introduce our SAL approach, we define two main components present in most discriminative neural network classifiers (e.g., Convolutional Neural Network, CNN): a representation learner component and a classification component. To simplify the notation,we use <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="154" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> denotes the representation learner component and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="155" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>θ</mi></math></mjx-assistive-mml></mjx-container> stands for its parameters. Our hypothesis is that confounding factors will be constrained to a subset of dimensions present in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="156" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> . Similarly,we use <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="157" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>ϕ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> to denote the classification component and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="158" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϕ</mi></math></mjx-assistive-mml></mjx-container> denotes the parameters. Therefore, a full neural network classifier is denoted as <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="159" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>;</mo><mi>ϕ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> . In our SAL approach, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="160" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> from identity-related features as identity related confounding dimensions. Our SAL approach can be summarized as first identifying these dimensions and then reduce the impact of these dimensions by adding noise to them.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们提出的 SAL 程序旨在增强一个预先存在的（即预训练模型）判别神经网络，使其对混淆因素更具鲁棒性。为了正式介绍我们的 SAL 方法，我们定义了大多数判别神经网络分类器中存在的两个主要组件（例如，卷积神经网络 CNN）：一个表示学习组件和一个分类组件。为了简化符号表示，我们用 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="161" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 表示表示学习组件，用 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="162" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>θ</mi></math></mjx-assistive-mml></mjx-container> 表示其参数。我们的假设是，混淆因素将被限制在 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="163" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 中存在的一个维度子集内。同样，我们用 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="164" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>ϕ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 表示分类组件，用 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="165" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϕ</mi></math></mjx-assistive-mml></mjx-container> 表示其参数。因此，一个完整的神经网络分类器表示为 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="166" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>;</mo><mi>ϕ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container>。在我们的 SAL 方法中， <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="167" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 来自与身份相关的特征，作为与身份相关的混淆维度。我们的 SAL 方法可以总结为首先识别这些维度，然后通过向它们添加噪声来减少这些维度的影响。</div></div></div></div></span></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="195,496"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="195,496"><div style="height: auto;"><span style="display: inline;"><div><div><div>To select identity-related confounding dimensions, SAL introduces a simple neural network (denoted by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="168" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> ,where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="169" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>δ</mi></math></mjx-assistive-mml></mjx-container> stands for its parameters). This is to predict identity-related confounding dimensions from individual identities <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="170" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Z</mi></math></mjx-assistive-mml></mjx-container> ,by minimizing the difference between <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="171" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="172" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>X</mi><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> . Therefore, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="173" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> will effectively pinpoint the identity-related confounding dimensions in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="174" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>X</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> . Figure 3a shows an overview of this Selection Phase.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">为了选择与身份相关的混杂维度，SAL 引入了一个简单的神经网络（用 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="175" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 表示，其中 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="176" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>δ</mi></math></mjx-assistive-mml></mjx-container> 代表其参数）。这是为了通过最小化 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="177" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="178" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>X</mi><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 之间的差异，从个体身份 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="179" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Z</mi></math></mjx-assistive-mml></mjx-container> 中预测与身份相关的混杂维度。因此，<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="180" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 将有效地确定 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="181" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>X</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 中与身份相关的混杂维度。图 3a 显示了这一选择阶段的概述。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="197,777"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="197,777"><div style="height: auto;"><span style="display: inline;"><div><div><div>To force the model to discard identity-related confounding dimensions, SAL introduces Gaussian noise to these dimensions while minimizing prediction error,so that <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="182" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>ϕ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> learns to neglect noised representation. The noise is added through a Gaussian Sampling Layer [26]. Figure 3b shows an overview of this addition phase.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">为了迫使模型舍弃与身份相关的混杂维度，SAL 在最小化预测误差的同时向这些维度引入高斯噪声，以便 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="183" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>ϕ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 学会忽略噪声表示。噪声是通过高斯采样层 [26] 添加的。图 3b 显示了这一添加阶段的概述。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="195,988"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="195,988"><div style="height: auto;"><span style="display: inline;"><div><div><div>Figure 2 shows how SAL assembles <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="184" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>ϕ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="185" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> together via a Gaussian Sampling Layer.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 2 显示了 SAL 如何通过高斯采样层将 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="186" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>ϕ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="187" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 组合在一起。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="150,1088"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="150,1088"><div style="height: auto;"><h3><div><div>3.2. Select-Additive Learning Algorithm<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.2. 选择-加法学习算法</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="150,1088"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="152,1156"><div style="height: auto;"><div><div><div>A pre-requisite to our Select-Additive Learning (SAL) approach is first learn a discriminative neural classifier. On our experiments, we achieve this goal by minimizing the following lost function:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们的选择-加法学习（SAL）方法的前提是首先学习一个区分性的神经分类器。在我们的实验中，我们通过最小化以下损失函数来实现这一目标：</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="323,1317"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="323,1317"><div style="height: auto;"><span style="display: inline;"><div><div><div><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="188" style="font-size: 122.8%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-n"><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-munder space="2"><mjx-row><mjx-base><mjx-texatom texclass="OP"><mjx-mo class="mjx-n"><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mo></mjx-texatom></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.359em;"><mjx-texatom size="s" texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom></mjx-under></mjx-row></mjx-munder><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mfrac space="2"><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-msup><mjx-texatom texclass="ORD"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-script style="vertical-align: 0.477em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>arg</mi><mo data-mjx-texclass="NONE">⁡</mo><munder><mrow data-mjx-texclass="OP"><mo data-mjx-texclass="OP">min</mo></mrow><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi>ϕ</mi><mo>,</mo><mi>θ</mi></mrow></mrow></munder><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>y</mi><mo>−</mo><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>X</mi><mo>;</mo><mi>θ</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>;</mo><mi>ϕ</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="323,1317"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="151,1417"><div style="height: auto;"><div><div><div>The same loss function is often used in discriminative neural networks [27].<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">相同的损失函数通常用于区分性神经网络 [27]。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="150,1525"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="150,1525"><div style="height: auto;"><h4><div><div>3.2.1. Selection Phase<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.2.1. 选择阶段</div></div></div></h4></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="150,1525"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="153,1588"><div style="height: auto;"><span style="display: inline;"><div><div><div>Once the original representation <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="189" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c67"></mjx-c></mjx-mi></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c58"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">g</mi></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">X</mi></mrow><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> is learned,the selection phase optimizes a new loss function to discover the identity related confounding dimensions. This selection phase is operationalized by tuning the parameters <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="190" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D706 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>λ</mi></math></mjx-assistive-mml></mjx-container> using the following loss function (as illustrated in Figure 3(a)):<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">一旦学习了原始表示 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="191" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c67"></mjx-c></mjx-mi></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c58"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">g</mi></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">X</mi></mrow><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container>，选择阶段就会优化一个新的损失函数，以发现与身份相关的混杂维度。此选择阶段通过使用以下损失函数调整参数 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="192" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D706 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>λ</mi></math></mjx-assistive-mml></mjx-container> 来实现（如图 3(a) 所示）：</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="262,1785"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="262,1785"><div style="height: auto;"><span style="display: inline;"><div><div><div><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" width="full" tabindex="0" ctxtmenu_counter="193" style="font-size: 122.8%; min-width: 21.487em; position: relative;"><mjx-math width="full" display="true" class="MJX-TEX" aria-hidden="true"><mjx-mtable width="full" side="right" style="min-width: 21.487em;"><mjx-table style="width: auto; min-width: 17.331em; margin: 0px 2.078em;"><mjx-itable width="full"><mjx-mlabeledtr><mjx-mtd><mjx-mi class="mjx-n"><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-munder space="2"><mjx-row><mjx-base><mjx-texatom texclass="OP"><mjx-mo class="mjx-n"><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mo></mjx-texatom></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.677em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-under></mjx-row></mjx-munder><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mfrac space="2"><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-msup><mjx-texatom texclass="ORD"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-script style="vertical-align: 0.477em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D706 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi><mjx-msub><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mlabeledtr></mjx-itable></mjx-table><mjx-labels style="width: 21.487em;"><mjx-itable align="right" style="right: 0px;"><mjx-mtr style="height: 2.123em;"><mjx-mtd id="mjx-eqn:1"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c28"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c29"></mjx-c></mjx-mtext><mjx-tstrut style="height: 2.123em; vertical-align: -0.781em;"></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-labels></mjx-mtable></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd><mtext>(1)</mtext></mtd><mtd><mi>arg</mi><mo data-mjx-texclass="NONE">⁡</mo><munder><mrow data-mjx-texclass="OP"><mo data-mjx-texclass="OP">min</mo></mrow><mrow data-mjx-texclass="ORD"><mi>δ</mi></mrow></munder><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>X</mi><mo>;</mo><mi>θ</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>−</mo><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>Z</mi><mo>;</mo><mi>δ</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msup><mo>+</mo><mi>λ</mi><mo>∥</mo><mi>δ</mi><msub><mrow data-mjx-texclass="ORD"><mo>∥</mo></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub></mtd></mlabeledtr></mtable></math></mjx-assistive-mml></mjx-container></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="262,1785"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="153,1881"><div style="height: auto;"><span style="display: inline;"><div><div><div>where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="194" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D706 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>λ</mi></math></mjx-assistive-mml></mjx-container> is a scalar that controls the weight of the sparsity regularizer. In this phase,both <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="195" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>X</mi></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="196" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Z</mi></math></mjx-assistive-mml></mjx-container> are available,but only <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="197" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>δ</mi></math></mjx-assistive-mml></mjx-container> is tuned,as shown in Fig. 3(a).<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">其中 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="198" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D706 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>λ</mi></math></mjx-assistive-mml></mjx-container> 是一个标量，用于控制稀疏正则化器的权重。在此阶段， <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="199" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>X</mi></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="200" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Z</mi></math></mjx-assistive-mml></mjx-container> 都可用，但仅调整 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="201" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>δ</mi></math></mjx-assistive-mml></mjx-container>，如图 3(a) 所示。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="195,1992"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="195,1992"><div style="height: auto;"><span style="display: inline;"><div><div><div>The goal of this phase is to select identity-related confounding dimensions from the original representation. To achieve this,we tune <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="202" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>δ</mi></math></mjx-assistive-mml></mjx-container> to minimize the difference between <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="203" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>X</mi><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="204" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> . As <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="205" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Z</mi></math></mjx-assistive-mml></mjx-container> only encodes identity information,<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">本阶段的目标是从原始表示中选择与身份相关的混杂维度。为此，我们调整 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="206" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>δ</mi></math></mjx-assistive-mml></mjx-container> 以最小化 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="207" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>X</mi><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="208" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 之间的差异。由于 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="209" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>Z</mi></math></mjx-assistive-mml></mjx-container> 仅编码身份信息，</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="993,203"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="993,203"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="993,203"><div style="height: auto;"><div><div class="text-center"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/0192e32f-c235-7adb-8c25-056c00c78188_2.jpg?x=993&amp;y=203&amp;w=587&amp;h=277"></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="993,203"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="987,485"><div style="height: auto;"><span style="display: inline;"><div><div><div>(a) Selection Phase: SAL forces <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="210" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> to identify identity-related confounding dimensions. Right side figure shows that <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="211" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> selects these dimensions.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">(a) 选择阶段：SAL 强制 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="212" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 识别与身份相关的混杂维度。右侧图形显示 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="213" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 选择了这些维度。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="967,190"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="996,600"><div style="height: auto;"><div><div class="text-center"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/0192e32f-c235-7adb-8c25-056c00c78188_2.jpg?x=996&amp;y=600&amp;w=582&amp;h=387"></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="996,600"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="986,992"><div style="height: auto;"><div><div><div>(b) Addition Phase: SAL forces the model to focus on other dimensions by adding Gaussian noise to identity-related confounding dimensions. Right side shows that the model shifts focus because these dimensions are contaminated/noisy.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">(b) 添加阶段：SAL 强制模型通过向与身份相关的混杂维度添加高斯噪声来关注其他维度。右侧显示模型的关注点发生了变化，因为这些维度受到污染/噪声影响。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="958,593"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="958,593"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="920,1134"><div style="height: auto;"><span style="display: inline;"><div><div><div>Fig. 3: Illustration of SAL. On the left, network structure and training objective is presented. On the right, circles denote neurons. Squares denote dimensions of representation. the minimum of difference will be achieved when <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="214" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> is matched to the identity-related confounding dimensions of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="215" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>X</mi><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> . L1 regularization of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="216" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>δ</mi></math></mjx-assistive-mml></mjx-container> is necessary to avoid overfit-ting as output dimension of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="217" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> is typically significantly higher than input dimension.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 3：SAL 的示意图。左侧展示了网络结构和训练目标。右侧，圆圈表示神经元，方块表示表示的维度。当 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="218" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 与 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="219" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>X</mi><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 的与身份相关的混杂维度匹配时，将实现差异的最小值。对 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="220" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>δ</mi></math></mjx-assistive-mml></mjx-container> 进行 L1 正则化是必要的，以避免过拟合，因为 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="221" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 的输出维度通常显著高于输入维度。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="962,1474"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="962,1474"><div style="height: auto;"><span style="display: inline;"><div><div><div>The result of this selection phase is shown on the righthand-side of Fig. 3(a). All the weights of original model (purple circle) are active and connected to every dimension while only some weights of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="222" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>,</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> (red circle) are active and connected to the identity-related confounding dimensions<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">选择阶段的结果显示在图 3(a) 的右侧。原始模型的所有权重（紫色圆圈）都是活跃的并连接到每个维度，而只有一些 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="223" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>,</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 的权重（红色圆圈）是活跃的并连接到与身份相关的混杂维度。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="918,1681"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="918,1681"><div style="height: auto;"><h4><div><div>3.2.2. Addition Phase<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.2.2. 添加阶段</div></div></div></h4></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="918,1681"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="921,1744"><div style="height: auto;"><span style="display: inline;"><div><div><div>After the selection phase <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="224" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>,</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> should be pointing at the identity-related confounding dimensions. Our remaining step is to learn a new neural network classifier where the confounding dimensions have "masked". We achieve this by adding Gaussian noise. Our addition phase defines the following loss function to achieve this goal:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">在选择阶段之后，<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="225" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>,</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 应该指向与身份相关的混杂维度。我们剩下的步骤是学习一个新的神经网络分类器，其中混杂维度已被“屏蔽”。我们通过添加高斯噪声来实现这一点。我们的添加阶段定义了以下损失函数以实现这一目标：</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="1008,1964"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="1008,1964"><div style="height: auto;"><span style="display: inline;"><div><div><div><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" width="full" tabindex="0" ctxtmenu_counter="226" style="font-size: 122.8%; min-width: 22.843em; position: relative;"><mjx-math width="full" display="true" class="MJX-TEX" aria-hidden="true"><mjx-mtable width="full" side="right" style="min-width: 22.843em;"><mjx-table style="width: auto; min-width: 18.687em; margin: 0px 2.078em;"><mjx-itable width="full"><mjx-mlabeledtr><mjx-mtd><mjx-mi class="mjx-n"><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-munder space="2"><mjx-row><mjx-base><mjx-texatom texclass="OP"><mjx-mo class="mjx-n"><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mo></mjx-texatom></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.623em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-under></mjx-row></mjx-munder><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mfrac space="2"><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-msup><mjx-texatom texclass="ORD"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2218"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-script style="vertical-align: 0.477em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mlabeledtr></mjx-itable></mjx-table><mjx-labels style="width: 22.843em;"><mjx-itable align="right" style="right: 0px;"><mjx-mtr style="height: 2.245em;"><mjx-mtd id="mjx-eqn:2"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c28"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c29"></mjx-c></mjx-mtext><mjx-tstrut style="height: 2.245em; vertical-align: -0.903em;"></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-labels></mjx-mtable></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd><mtext>(2)</mtext></mtd><mtd><mi>arg</mi><mo data-mjx-texclass="NONE">⁡</mo><munder><mrow data-mjx-texclass="OP"><mo data-mjx-texclass="OP">min</mo></mrow><mrow data-mjx-texclass="ORD"><mi>ϕ</mi></mrow></munder><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>y</mi><mo>−</mo><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>X</mi><mo>;</mo><mi>θ</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>+</mo><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>Z</mi><mo>;</mo><mi>δ</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>∘</mo><mi>ϵ</mi><mo>;</mo><mi>ϕ</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msup></mtd></mlabeledtr></mtable></math></mjx-assistive-mml></mjx-container></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="1008,1964"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-9ce8ce7e-dbec-4d11-bb14-9ea74eb64634" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="920,2057"><div style="height: auto;"><span style="display: inline;"><div><div><div>where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="227" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi><mo>∼</mo><mi>N</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mn>0</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mi>σ</mi><mi>I</mi></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="228" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2218"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∘</mo></math></mjx-assistive-mml></mjx-container> stands for element-wise product, as showed in Fig. 3(b).<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">其中 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="229" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi><mo>∼</mo><mi>N</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mn>0</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mi>σ</mi><mi>I</mi></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="230" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2218"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∘</mo></math></mjx-assistive-mml></mjx-container> 表示逐元素乘积，如图 3(b) 所示。</div></div></div></div></span></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="197,212"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="197,212"><div style="height: auto;"><span style="display: inline;"><div><div><div>In this phase,parameter <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="231" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϕ</mi></math></mjx-assistive-mml></mjx-container> is tuned. The input representation of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="232" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>ϕ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> consists of the representation learned from <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="233" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>X</mi><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> and the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="234" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> -selected identity-related confounding dimensions with Gaussian noise added. The noise ensures that identity-related confounding dimensions are no longer informative so that <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="235" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>ϕ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> can be trained to ignore them.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">在此阶段，参数 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="236" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϕ</mi></math></mjx-assistive-mml></mjx-container> 被调整。<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="237" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>ϕ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 的输入表示由从 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="238" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>X</mi><mo>;</mo><mi>θ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 学习到的表示和添加了高斯噪声的 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="239" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>;</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 选择的与身份相关的混淆维度组成。噪声确保与身份相关的混淆维度不再具有信息性，从而使得 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="240" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c3B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D719 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>;</mo><mi>ϕ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 可以被训练以忽略它们。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="196,426"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="196,426"><div style="height: auto;"><div><div><div>As illustrated on the right side of Fig. 3(b), identity-related confounding dimensions are contaminated with addition of noise. Therefore, the model learns to discard these non-informative dimensions, and its weights get optimized to focus on the rest of the dimensions.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">如图 3(b) 右侧所示，与身份相关的混淆维度在添加噪声后受到污染。因此，模型学习丢弃这些非信息性维度，其权重被优化以关注其余维度。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="376,640"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="376,640"><div style="height: auto;"><h2><div><div>4. EXPERIMENTS<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">4. 实验</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="376,640"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="153,718"><div style="height: auto;"><div><div><div>In this section, we perform an extensive set of experiments on three different data sets to see whether SAL can help improve the generalizability of a discriminative neural classifier. Generalizability is tested by performing across-dataset experiments where two of the dataset are kept exclusively for testing. All our experiments follow a person independent methodology where none of the subject from the training data are present in the test datasets.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">在本节中，我们对三个不同的数据集进行了广泛的实验，以观察 SAL 是否能帮助提高判别神经分类器的可泛化性。可泛化性通过进行跨数据集实验来测试，其中两个数据集专门用于测试。我们所有的实验遵循一种与人无关的方法论，训练数据中的任何受试者均不出现在测试数据集中。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="150,1039"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="150,1039"><div style="height: auto;"><h3><div><div>4.1. Models<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">4.1. 模型</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="150,1039"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="152,1103"><div style="height: auto;"><div><div><div>We compare the following models:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们比较以下模型：</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="199,1141"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="199,1141"><div style="height: auto;"><span style="display: inline;"><div><div><div><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="241" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C6"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D402 TEX-B"></mjx-c><mjx-c class="mjx-c1D40D TEX-B"></mjx-c><mjx-c class="mjx-c1D40D TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⋆</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">CNN</mi></mrow></mrow></math></mjx-assistive-mml></mjx-container> : The state-of-the-art seven layer convolutional neural network architecture used previously for multimodal sentiment analysis [25].<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="242" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C6"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D402 TEX-B"></mjx-c><mjx-c class="mjx-c1D40D TEX-B"></mjx-c><mjx-c class="mjx-c1D40D TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⋆</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">CNN</mi></mrow></mrow></math></mjx-assistive-mml></mjx-container> : 之前用于多模态情感分析的最先进的七层卷积神经网络架构 [25]。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="197,1248"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="197,1248"><div style="height: auto;"><span style="display: inline;"><div><div><div>*SAL-CNN: After the state-of-the-art CNN is fully trained, we use SAL to increase its generalizability and predict sentiment. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="243" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>,</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> is a neural perceptron [27].<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">*SAL-CNN: 在最先进的 CNN 完全训练后，我们使用 SAL 来提高其可泛化性并预测情感。<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="244" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mo>⋅</mo><mo>,</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 是一个神经感知器 [27]。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="152,1392"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="152,1392"><div style="height: auto;"><h3><div><div>4.2. Datasets<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">4.2. 数据集</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="152,1392"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="153,1456"><div style="height: auto;"><div><div><div>We performed our experiment on three multimodal sentiment analysis data sets:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们在三个多模态情感分析数据集上进行了实验：</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="198,1529"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="198,1529"><div style="height: auto;"><div><div><div>⋆MOSI: This dataset consists of 93 videos obtained from YouTube channels. Each video contains the opinions from one unique individual. The dataset has 2199 utterances manually segmented from online videos of movie reviews. Each utterance was also manually annotated for sentiment label [6].<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">⋆MOSI: 该数据集由 93 个从 YouTube 频道获得的视频组成。每个视频包含来自一个独特个体的观点。该数据集包含 2199 个从电影评论的在线视频中手动分割的发言。每个发言也经过手动标注以获取情感标签 [6]。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="197,1707"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="197,1707"><div style="height: auto;"><span style="display: inline;"><div><div><div><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="245" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C6"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⋆</mo></math></mjx-assistive-mml></mjx-container> YouTube: This dataset consists of 47 opinion videos with 280 utterances with manually annotated sentiment labels [1]. Each video contains the opinions from one unique individual.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="246" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C6"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⋆</mo></math></mjx-assistive-mml></mjx-container> YouTube：该数据集由47个意见视频组成，包含280个带有手动注释情感标签的发言 [1]。每个视频包含一个独特个体的观点。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="198,1846"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="198,1846"><div style="height: auto;"><div><div><div>*MOUD: This dataset consists of 498 Spanish opinion utterances from 55 unique individuals [5].<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">*MOUD：该数据集由来自55个独特个体的498个西班牙语意见发言组成 [5]。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="195,1919"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="195,1919"><div style="height: auto;"><div><div><div>Although, majority of the data originate from YouTube, they differ in recording quality and the processing done after curation. The verbal features in the MOUD dataset need one extra step of translation from Spanish to English. These three datasets are good candidates to evaluate across-dataset generalization.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">尽管大多数数据来自YouTube，但它们在录制质量和后期处理上有所不同。MOUD数据集中的语言特征需要额外一步将西班牙语翻译为英语。这三个数据集是评估跨数据集泛化的良好候选者。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="932,194"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="932,194"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="1060,207"><div style="height: auto;"><div><div><div>Table 1: Within data set experiments<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">表1：数据集内实验</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="949,264"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="949,264"><div style="height: auto;"><div><div class="table-container"><div class="table-container"><table class="fixed-table"><tbody><tr><td></td><td></td><td>CNN</td><td>SAL-CNN</td></tr><tr><td rowspan="3">Unimodal</td><td>Verbal</td><td>0.678</td><td>0.732</td></tr><tr><td>Acoustic</td><td>0.588</td><td>0.618</td></tr><tr><td>Visual</td><td>0.572</td><td>0.636</td></tr><tr><td rowspan="3">Bimodal</td><td>Verbal+Acoustic</td><td>0.687</td><td>0.725</td></tr><tr><td>Verbal+Visual</td><td>0.706</td><td>0.73</td></tr><tr><td>Acoustic+Visual</td><td>0.661</td><td>0.621</td></tr><tr><td colspan="2">All Modalities</td><td>0.715</td><td>0.73</td></tr></tbody></table></div></div><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><div class="table-container"><div class="table-container"><table class="fixed-table"><tbody><tr><td></td><td></td><td>卷积神经网络</td><td>SAL-CNN</td></tr><tr><td rowspan="3">单模态</td><td>语言</td><td>0.678</td><td>0.732</td></tr><tr><td>声音</td><td>0.588</td><td>0.618</td></tr><tr><td>视觉</td><td>0.572</td><td>0.636</td></tr><tr><td rowspan="3">双模态</td><td>语言+声音</td><td>0.687</td><td>0.725</td></tr><tr><td>语言+视觉</td><td>0.706</td><td>0.73</td></tr><tr><td>声音+视觉</td><td>0.661</td><td>0.621</td></tr><tr><td colspan="2">所有模态</td><td>0.715</td><td>0.73</td></tr></tbody></table></div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="949,264"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="932,194"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="917,616"><div style="height: auto;"><h3><div><div>4.3. Feature Extraction<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">4.3. 特征提取</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="917,616"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="920,688"><div style="height: auto;"><div><div><div>We extracted an embedding for each word using a word2vec dictionary pre-trained on a Google News corpus [28]. The text feature of each utterance was formed by concatenating the word embeddings for all the words in the sentence and padding them with the appropriate zeros to have the same dimension. We set the maximum length as 60 and discarded additional words'. For YouTube dataset, we extracted the transcripts using the IBM Bluemix’s speech2text API For MOUD dataset, we translated Spanish transcripts into English transcripts. We used openSMILE [29] to extract the low-level audio descriptors for each spoken utterance. These audio descriptors included the Mel-frequency cepstral coefficients, pitch and voice quality. We processed every frame in each video and used the audio-visual synchrony to identify which frames happen during a specific utterance. We used the CLM-Z library [30] for extracting facial characteristic points.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们使用在Google News语料库上预训练的word2vec字典为每个单词提取了嵌入 [28]。每个发言的文本特征是通过连接句子中所有单词的嵌入并用适当的零填充以保持相同维度来形成的。我们将最大长度设置为60，并丢弃额外的单词。对于YouTube数据集，我们使用IBM Bluemix的speech2text API提取了转录文本。对于MOUD数据集，我们将西班牙语转录文本翻译为英语转录文本。我们使用openSMILE [29]提取了每个口语发言的低级音频描述符。这些音频描述符包括梅尔频率倒谱系数、音高和音质。我们处理了每个视频中的每一帧，并利用音视频同步来识别特定发言发生时的帧。我们使用CLM-Z库 [30] 提取面部特征点。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="919,1299"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="919,1299"><div style="height: auto;"><h3><div><div>4.4. Experiment Setup<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">4.4. 实验设置</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="919,1299"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="920,1370"><div style="height: auto;"><span style="display: inline;"><div><div><div>We remove the netural utterances out of the data set. The first 62 individuals in the MOSI data set are selected as training/validation set. There are around 1250 utterances in total. These utterances are shuffled and then <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="247" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c38"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>80</mn></mrow><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> are used for training and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="248" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>20</mn></mrow><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> used for validation. We have three test datasets. 1) MOSI: 546 utterances from the remaining 31 individuals. 2) YouTube: 195 utterances from 47 individuals and 3) MOUD: 450 utterances from 55 individuals. We use MOSI as training set because it is the largest and most recent dataset among all three.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们从数据集中移除了中立的发言。MOSI 数据集中的前 62 个人被选为训练/验证集。总共有大约 1250 个发言。这些发言被打乱，然后 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="249" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c38"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>80</mn></mrow><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> 用于训练， <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="250" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>20</mn></mrow><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> 用于验证。我们有三个测试数据集。1) MOSI：来自剩余 31 个人的 546 个发言。2) YouTube：来自 47 个人的 195 个发言，3) MOUD：来自 55 个人的 450 个发言。我们使用 MOSI 作为训练集，因为它是三者中最大和最新的数据集。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="920,1770"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="920,1770"><div style="height: auto;"><h3><div><div>4.5. Experiment Results<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">4.5. 实验结果</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="920,1770"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="917,1836"><div style="height: auto;"><h4><div><div>4.5.1. Within data set<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">4.5.1. 数据集内</div></div></div></h4></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="917,1836"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="920,1899"><div style="height: auto;"><div><div><div>Table 1 shows the results for CNN and SAL-CNN tested on the remaining 31 individuals' data of MOSI. The results indicate that SAL could help to increase the generalizability of the trained model.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">表 1 显示了在剩余 31 个人的 MOSI 数据上测试的 CNN 和 SAL-CNN 的结果。结果表明，SAL 可以帮助提高训练模型的泛化能力。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="952,2067"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><hr><paragraphpositioning data-position-3="952,2067"></paragraphpositioning></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="952,2067"><div style="height: auto;"><span style="display: inline;"><div><div><div><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="251" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> only around <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="252" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>0.5</mn></mrow><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> utterances in our datasets have more than 60 words<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="253" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> 我们的数据集中只有大约 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="254" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>0.5</mn></mrow><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> 个发言超过 60 个单词</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="952,2101"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="952,2101"><div style="height: auto;"><span style="display: inline;"><div><div><div><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="255" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> https://www.ibm.com/watson/developercloud/speech-to-text.html</div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-3="952,2101"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b16ea835-9e6d-4703-a1ce-ce3310793604" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><hr><paragraphpositioning data-position-3="952,2101"></paragraphpositioning></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="149,198"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="293,206"><div style="height: auto;"><div><div><div>Table 2: Across data set experiments<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">表 2：跨数据集实验</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="163,262"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="163,262"><div style="height: auto;"><div><div class="table-container"><div class="table-container"><table class="fixed-table"><tbody><tr><td rowspan="2"></td><td colspan="2">Youtube</td><td colspan="2">MOUD</td></tr><tr><td>CNN</td><td>SAL-CNN</td><td>CNN</td><td>SAL-CNN</td></tr><tr><td>Verbal</td><td>0.605</td><td>0.657</td><td>0.522</td><td>0.569</td></tr><tr><td>Acoustic</td><td>0.441</td><td>0.564</td><td>0.455</td><td>0.549</td></tr><tr><td>Visual</td><td>0.492</td><td>0.549</td><td>0.555</td><td>0.548</td></tr><tr><td>Ver+Acou</td><td>0.642</td><td>0.652</td><td>0.515</td><td>0.574</td></tr><tr><td>Ver+Vis</td><td>0.642</td><td>0.667</td><td>0.542</td><td>0.574</td></tr><tr><td>Acou+Vis</td><td>0.452</td><td>0.559</td><td>0.533</td><td>0.554</td></tr><tr><td>All</td><td>0.611</td><td>0.667</td><td>0.531</td><td>0.574</td></tr></tbody></table></div></div><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><div class="table-container"><div class="table-container"><table class="fixed-table"><tbody><tr><td rowspan="2"></td><td colspan="2">YouTube</td><td colspan="2">MOUD</td></tr><tr><td>CNN</td><td>SAL-CNN</td><td>CNN</td><td>SAL-CNN</td></tr><tr><td>语言</td><td>0.605</td><td>0.657</td><td>0.522</td><td>0.569</td></tr><tr><td>声学</td><td>0.441</td><td>0.564</td><td>0.455</td><td>0.549</td></tr><tr><td>视觉</td><td>0.492</td><td>0.549</td><td>0.555</td><td>0.548</td></tr><tr><td>语言+声学</td><td>0.642</td><td>0.652</td><td>0.515</td><td>0.574</td></tr><tr><td>语言+视觉</td><td>0.642</td><td>0.667</td><td>0.542</td><td>0.574</td></tr><tr><td>声学+视觉</td><td>0.452</td><td>0.559</td><td>0.533</td><td>0.554</td></tr><tr><td>所有</td><td>0.611</td><td>0.667</td><td>0.531</td><td>0.574</td></tr></tbody></table></div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="163,262"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="236,647"><div style="height: auto;"><div><div class="text-center"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/0192e32f-c235-7adb-8c25-056c00c78188_4.jpg?x=236&amp;y=647&amp;w=554&amp;h=311"></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="236,647"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="152,982"><div style="height: auto;"><div><div><div>Fig. 4: Confounding factors identified in the Selection phase for first 50 utterances (rows), first 100 representation values (columns) in the training set.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 4：在训练集中识别的前 50 个发言（行）、前 100 个表示值（列）选择阶段的混淆因素。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="144,640"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><!-- Media --></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="144,640"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="150,1143"><div style="height: auto;"><h4><div><div>4.5.2. Across data sets<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">4.5.2. 跨数据集</div></div></div></h4></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="150,1143"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="152,1206"><div style="height: auto;"><div><div><div>Table 2 shows the results for CNN and SAL-CNN tested on YouTube and MOUD dataset. First, it is noteworthy that in some cases the performance of the CNN is worse than mere chance. This inferior performance substantiates the existence of the non-generalization problems we are targeting.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">表 2 显示了在 YouTube 和 MOUD 数据集上测试的 CNN 和 SAL-CNN 的结果。首先，值得注意的是，在某些情况下，CNN 的表现甚至不如随机猜测。这种劣质表现证实了我们所针对的非泛化问题的存在。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="197,1387"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="197,1387"><div style="height: auto;"><div><div><div>Overall, Select-Additive Learning increases the robustness and performance of the previous models consistently (except only two cases: Video modality in MOUD and fusion of acoustic &amp; visual in MOSI). Permutation Test rejects the null hypothesis (no improvement) with p-values 0.037,0.0003,0.0023respectively for MOSI,YouTube,and MOUD, indicating significant improvement. 3<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">总体而言，选择性加性学习持续提高了先前模型的鲁棒性和性能（仅有两个例外：MOUD中的视频模态和MOSI中的声学与视觉融合）。置换检验以p值0.037、0.0003、0.0023分别拒绝了零假设（无改善），表明显著改善。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="149,1665"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="149,1665"><div style="height: auto;"><h3><div><div>4.6. Discussion<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">4.6. 讨论</div></div></div></h3></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="149,1665"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="152,1730"><div style="height: auto;"><div><div><div>To substantiate our proposed model and algorithm, we examine the learning process and verify that representation of confounding factors exists and our method can mitigate its effects. We demonstrate this with the visual modality as it intuitively contributes the most to confounding.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">为了证实我们提出的模型和算法，我们检查学习过程并验证混杂因素的表示存在，我们的方法可以减轻其影响。我们通过视觉模态展示这一点，因为它直观上对混杂因素的贡献最大。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="195,1909"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="195,1909"><div style="height: auto;"><span style="display: inline;"><div><div><div>Figure 4 shows a plot of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="256" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>,</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> during the Selection phase. It is a zoomed-in figure for the first 50 utterances (rows) and first 100 values of the representation vector (columns). Blue indicates lowest values and red indicates highest values and other colors are linearly interpolated.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图4显示了选择阶段期间<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="257" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D6FF TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>Z</mi><mo>,</mo><mi>δ</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container>的图。它是前50个发言（行）和表示向量前100个值（列）的放大图。蓝色表示最低值，红色表示最高值，其他颜色为线性插值。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="961,250"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="961,250"><div style="height: auto;"><div><div><div>The representation of utterances forms clear clusters and each cluster belongs to one person. Despite each individual having their own pattern, some dimensions have generalized well across individuals. Our model learns to assign more weights to these dimensions after noise is introduced.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">发言的表示形成了清晰的聚类，每个聚类属于一个人。尽管每个个体都有自己的模式，但某些维度在个体之间有很好的概括。我们的模型在引入噪声后学会对这些维度赋予更多权重。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="961,426"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="961,426"><div style="height: auto;"><div><div><div>In addition to these results, we calculated the inter-cluster distance over intra-cluster distance ratio for the representation learned under two situations: 1) clustered by category of sentiment and 2) clustered by individual's identity. We compared the ratios for CNN and SAL-CNN. The higher ratio indicates a clearer clustering structure.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">除了这些结果外，我们计算了在两种情况下学习到的表示的聚类间距离与聚类内距离的比率：1）按情感类别聚类，2）按个体身份聚类。我们比较了CNN和SAL-CNN的比率。更高的比率表明更清晰的聚类结构。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="961,636"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="961,636"><div style="height: auto;"><span style="display: inline;"><div><div><div>After SAL, for representation clustered by category of sentiment,the ratio increased by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="258" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>44</mn></mrow><mi mathvariant="normal">%</mi><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>15</mn></mrow><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="259" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>72</mn></mrow><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> respectively for verbal, acoustic, and visual modality, while for representation clustered by individual's identity, the ratio increased by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="260" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c39"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>9</mn><mi mathvariant="normal">%</mi><mo>,</mo><mn>3</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="261" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>13</mn></mrow><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> ,respectively. These numbers indicate SAL almost maintains the clustering structure of identity, but greatly improves the clustering structure of category of sentiment. This shows the effectiveness of SAL.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">在 SAL 之后，对于按情感类别聚类的表示，口头、声学和视觉模态的比例分别增加了 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="262" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>44</mn></mrow><mi mathvariant="normal">%</mi><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>15</mn></mrow><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="263" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>72</mn></mrow><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container>，而对于按个体身份聚类的表示，比例分别增加了 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="264" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c39"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>9</mn><mi mathvariant="normal">%</mi><mo>,</mo><mn>3</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="265" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>13</mn></mrow><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container>。这些数字表明 SAL 几乎保持了身份的聚类结构，但大大改善了情感类别的聚类结构。这显示了 SAL 的有效性。</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="1154,953"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="1154,953"><div style="height: auto;"><h2><div><div>5. CONCLUSION<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">5. 结论</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="1154,953"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="920,1028"><div style="height: auto;"><div><div><div>High-quality datasets required to train machine learning models for automatic multimodal sentiment analysis are only of the order of a few thousand samples. These limited resources restrict models' generalizability, leading to the issue of confounding factors. We proposed a Select-Additive Learning (SAL) procedure that can mitigate this problem. With extensive experiments, we have shown how SAL improves the generalizability of state-of-the-art models. We increased prediction accuracy significantly in all three modalities (verbal, acoustic, visual), as well as in their fusion. We also showed that SAL could achieve good prediction accuracy even when tested across data sets.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">训练用于自动多模态情感分析的机器学习模型所需的高质量数据集仅在几千个样本的数量级。这些有限的资源限制了模型的泛化能力，导致混杂因素的问题。我们提出了一种选择性加法学习（SAL）程序，可以缓解这个问题。通过大量实验，我们展示了 SAL 如何提高最先进模型的泛化能力。我们在所有三种模态（口头、声学、视觉）及其融合中显著提高了预测准确性。我们还展示了 SAL 即使在跨数据集测试时也能达到良好的预测准确性。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="1151,1489"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="1151,1489"><div style="height: auto;"><h2><div><div>6. REFERENCES<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">6. 参考文献</div></div></div></h2></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="1151,1489"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="934,1563"><div style="height: auto;"><div><div><div>[1] Louis-Philippe Morency, Rada Mihalcea, and Payal Doshi, "Towards multimodal sentiment analysis: Harvesting opinions from the web," in Proceedings of the 13th international conference on multimodal interfaces. ACM, 2011.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[1] Louis-Philippe Morency, Rada Mihalcea, 和 Payal Doshi, "朝向多模态情感分析：从网络中收集意见," 载于第十三届国际多模态接口会议论文集。ACM, 2011。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="935,1763"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="935,1763"><div style="height: auto;"><div><div><div>[2] Bo Pang and Lillian Lee, "Opinion mining and sentiment analysis," Foundations and trends in information retrieval, 2008.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[2] Bo Pang 和 Lillian Lee, "意见挖掘与情感分析," 信息检索的基础与趋势, 2008。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="935,1893"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="935,1893"><div style="height: auto;"><div><div><div>[3] Akshi Kumar and Mary Sebastian Teeja, "Sentiment analysis: A perspective on its past, present and future," International Journal of Intelligent Systems and Applications, 2012.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[3] Akshi Kumar 和 Mary Sebastian Teeja, "情感分析：对其过去、现在和未来的看法," 国际智能系统与应用期刊, 2012。</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="936,2057"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="936,2057"><div style="height: auto;"><div><div><div>[4] Martin Wollmer, Felix Weninger, Timo Knaup, Bjorn Schuller, Congkai Sun, Kenji Sagae, and Louis-Philippe<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[4] Martin Wollmer, Felix Weninger, Timo Knaup, Bjorn Schuller, Congkai Sun, Kenji Sagae, 和 Louis-Philippe</div></div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="182,2067"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><hr><paragraphpositioning data-position-4="182,2067"></paragraphpositioning></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="182,2067"><div style="height: auto;"><span style="display: inline;"><div><div><div><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="266" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>3</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> Select-additive Learning implementation is available at<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="267" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>3</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> 选择性加法学习的实现可在</div></div></div></div></span></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="152,2101"><div style="height: auto;"><div><div><div>https://github.com/HaohanWang/SelectAdditiveLearning</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-4="152,2101"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-78a3945a-54f5-4f5d-86bd-5a1e7264eb26" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><hr><paragraphpositioning data-position-4="152,2101"></paragraphpositioning></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="216,211"><div style="height: auto;"><div><div><div>Morency, "Youtube movie reviews: Sentiment analysis in an audio-visual context," Intelligent Systems, IEEE, 2013.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="167,338"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="167,338"><div style="height: auto;"><div><div><div>[5] Verónica Pérez Rosas, Rada Mihalcea, and Louis-Philippe Morency, "Multimodal sentiment analysis of spanish online videos," IEEE Intelligent Systems, , no. 3, 2013.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="167,501"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="167,501"><div style="height: auto;"><div><div><div>[6] Amir Zadeh, "Micro-opinion sentiment intensity analysis and summarization in online videos," in ICMI. ACM, 2015.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="167,627"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="167,627"><div style="height: auto;"><div><div><div>[7] Robert M Ewers and Raphael K Didham, "Confounding factors in the detection of species responses to habitat fragmentation," Biological Reviews, 2006.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="168,756"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="168,756"><div style="height: auto;"><div><div><div>[8] Haohan Wang and Jingkang Yang, "Multiple confounders correction with regularized linear mixed effect models, with application in biological processes," in BIBM. IEEE, 2016.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="167,918"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="167,918"><div style="height: auto;"><div><div><div>[9] Lingxiang Wu, Jinqiao Wang, Guibo Zhu, Min Xu, and Hanqing Lu, "Person re-identification via rich color-gradient feature," in ICME. IEEE, 2016.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="153,1045"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="153,1045"><div style="height: auto;"><div><div><div>[10] Xiaoke Zhu, Xiao-Yuan Jing, Fei Wu, Weishi Zheng, Ruimin Hu, Chunxia Xiao, and Chao Liang, "Distance learning by treating negative samples differently and exploiting impostors with symmetric triplet constraint for person re-identification," in ICME. IEEE, 2016.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="151,1244"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="151,1244"><div style="height: auto;"><div><div><div>[11] Antonio Tejero-de Pablos, Yuta Nakashima, Tomokazu Sato, and Naokazu Yokoya, "Human action recognition-based video summarization for rgb-d personal sports video," in ICME. IEEE, 2016.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="151,1406"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="151,1406"><div style="height: auto;"><div><div><div>[12] Ying Zhao, Huijun Di, Jian Zhang, Yao Lu, and Feng Lv, "Recognizing human actions from low-resolution videos by region-based mixture models," in ICME. IEEE, 2016.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="154,1569"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="154,1569"><div style="height: auto;"><div><div><div>[13] Zhongjun Wu and Weihong Deng, "One-shot deep neural network for pose and illumination normalization face recognition," in ICME. IEEE, 2016.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="151,1696"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="151,1696"><div style="height: auto;"><div><div><div>[14] Binghui Chen and Weihong Deng, "Weakly-supervised deep self-learning for face recognition," in ICME. IEEE, 2016.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="153,1823"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="153,1823"><div style="height: auto;"><div><div><div>[15] Erik Cambria, Daniel Olsher, and Dheeraj Rajagopal, "Senticnet 3: a common and common-sense knowledge base for cognition-driven sentiment analysis," in AAAI. AAAI Press, 2014.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="152,1986"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="152,1986"><div style="height: auto;"><div><div><div>[16] Theresa Wilson, Janyce Wiebe, and Paul Hoffmann, "Recognizing contextual polarity in phrase-level sentiment analysis," in EMNLP. Association for Computational Linguistics, 2005.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="137,194"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="917,211"><div style="height: auto;"><div><div><div>[17] Ellen Riloff and Janyce Wiebe, "Learning extraction patterns for subjective expressions," in EMNLP. Association for Computational Linguistics, 2003.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="920,335"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="920,335"><div style="height: auto;"><div><div><div>[18] Lakshmish Kaushik, Abhijeet Sangwan, and John HL Hansen, "Sentiment extraction from natural audio streams," in ICASSP. IEEE, 2013.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="919,459"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="919,459"><div style="height: auto;"><div><div><div>[19] Boya Wu, Jia Jia, Tao He, Juan Du, Xiaoyuan Yi, and Yishuang Ning, "Inferring usersemotions for human-mobile voice dialogue applications,".</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="920,583"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="920,583"><div style="height: auto;"><div><div><div>[20] Paul Ekman and Wallace V Friesen, "Facial action coding system," 1977.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="919,672"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="919,672"><div style="height: auto;"><div><div><div>[21] Ming Sun, Jufeng Yang, Kai Wang, and Hui Shen, "Discovering affective regions in deep convolutional neural networks for visual sentiment prediction," in ICME. IEEE, 2016.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="920,833"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="920,833"><div style="height: auto;"><div><div><div>[22] Verónica Pérez-Rosas, Rada Mihalcea, and Louis-Philippe Morency, "Utterance-level multimodal sentiment analysis.," in ACL, 2013.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="920,957"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="920,957"><div style="height: auto;"><div><div><div>[23] Luca Casaburi, Francesco Colace, Massimo De Santo, and Luca Greco, "magic mirror in my hand, what is the sentiment in the lens?: An action unit based approach for mining sentiments from multimedia contents," Journal of Visual Languages &amp; Computing.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="920,1151"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="920,1151"><div style="height: auto;"><div><div><div>[24] Soujanya Poria, Erik Cambria, Newton Howard, Guang-Bin Huang, and Amir Hussain, "Fusing audio, visual and textual clues for sentiment analysis from multimodal content," Neurocomputing.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="920,1311"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="920,1311"><div style="height: auto;"><div><div><div>[25] Soujanya Poria, Erik Cambria, and Alexander Gelbukh, "Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis," in EMNLP, 2015, pp. 2539-2544.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="920,1470"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="920,1470"><div style="height: auto;"><div><div><div>[26] Diederik P Kingma and Max Welling, "Auto-encoding variational bayes," arXiv preprint arXiv:1312.6114, 2013.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="919,1594"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="919,1594"><div style="height: auto;"><div><div><div>[27] Haohan Wang and Bhiksha Raj, "On the origin of deep learning,” arXiv preprint arXiv:1702.07800, 2017.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="922,1684"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="922,1684"><div style="height: auto;"><div><div><div>[28] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean, "Efficient estimation of word representations in vector space," arXiv preprint arXiv:1301.3781, 2013.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="919,1808"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="919,1808"><div style="height: auto;"><div><div><div>[29] Florian Eyben, Martin Wöllmer, and Björn Schuller, "Opensmile: the munich versatile and fast open-source audio feature extractor," in International conference on Multimedia. ACM, 2010.</div></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="919,1969"></paragraphpositioning></div></div></div></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-87fee491-713d-49c6-8909-538b168875a8" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="919,1969"><div style="height: auto;"><div><div><div>[30] Tadas Baltrušaitis, Peter Robinson, and Louis-Philippe Morency, "3d constrained local model for rigid and nonrigid facial tracking," in CVPR. IEEE, 2012, pp. 2610- 2617.</div></div></div></div></div></div></div></div></div>
      </body>
    </html>
  