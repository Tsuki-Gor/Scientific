
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <h1>Surface Reconstruction from Point Clouds: A Survey and a Benchmark</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>从点云进行表面重建：综述与基准</h1></div><p>Zhangjin Huang*, Yuxin Wen*, Zihao Wang, Jinjuan Ren, and Kui Jia</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>黄张金*, 温宇欣*, 王子豪, 任金娟, 及贾奎</p></div><p>Abstract-Reconstruction of a continuous surface of two-dimensional manifold from its raw, discrete point cloud observation is a long-standing problem in computer vision and graphics research. The problem is technically ill-posed, and becomes more difficult considering that various sensing imperfections would appear in the point clouds obtained by practical depth scanning. In literature, a rich set of methods has been proposed, and reviews of existing methods are also provided. However, existing reviews are short of thorough investigations on a common benchmark. The present paper aims to review and benchmark existing methods in the new era of deep learning surface reconstruction. To this end, we contribute a large-scale benchmarking dataset consisting of both synthetic and real-scanned data; the benchmark includes object- and scene-level surfaces and takes into account various sensing imperfections that are commonly encountered in practical depth scanning. We conduct thorough empirical studies by comparing existing methods on the constructed benchmark, and pay special attention on robustness of existing methods against various scanning imperfections; we also study how different methods generalize in terms of reconstructing complex surface shapes. Our studies help identify the best conditions under which different methods work, and suggest some empirical findings. For example, while deep learning methods are increasingly popular in the research community, our systematic studies suggest that, surprisingly, a few classical methods perform even better in terms of both robustness and generalization; our studies also suggest that the practical challenges of misalignment of point sets from multi-view scanning, missing of surface points, and point outliers remain unsolved by all the existing surface reconstruction methods. We expect that the benchmark and our studies would be valuable both for practitioners and as a guidance for new innovations in future research. We make the benchmark publicly accessible at <a href="https://Gorilla-Lab-SCUT.github.io/SurfaceReconstructionBenchmark">https://Gorilla-Lab-SCUT.github.io/SurfaceReconstructionBenchmark</a>.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>摘要-从原始的离散点云观测中重建二维流形的连续表面是计算机视觉和图形学研究中的一个长期问题。该问题在技术上是病态的，考虑到在实际深度扫描中会出现各种传感缺陷，使得问题更加复杂。文献中提出了一系列丰富的方法，并提供了现有方法的综述。然而，现有的综述缺乏对共同基准的深入研究。本文旨在回顾和基准现有方法在深度学习表面重建的新纪元。为此，我们贡献了一个大规模的基准数据集，包括合成数据和真实扫描数据；该基准涵盖了对象和场景级别的表面，并考虑了在实际深度扫描中常见的各种传感缺陷。我们通过在构建的基准上比较现有方法进行全面的实证研究，特别关注现有方法在各种扫描缺陷下的鲁棒性；我们还研究了不同方法在重建复杂表面形状方面的泛化能力。我们的研究有助于识别不同方法有效工作的最佳条件，并提出一些实证发现。例如，尽管深度学习方法在研究界越来越受欢迎，但我们的系统研究表明，令人惊讶的是，一些经典方法在鲁棒性和泛化能力方面表现得更好；我们的研究还表明，来自多视角扫描的点集错位、表面点缺失和点异常值等实际挑战仍未被所有现有的表面重建方法解决。我们期望该基准和我们的研究对从业者和未来研究中的新创新提供指导都具有价值。我们在 <a href="https://Gorilla-Lab-SCUT.github.io/SurfaceReconstructionBenchmark">https://Gorilla-Lab-SCUT.github.io/SurfaceReconstructionBenchmark</a> 上公开访问该基准。</p></div><p>Index Terms-Surface reconstruction, surface modeling, point cloud, benchmarking dataset, literature survey, deep learning.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>索引词-表面重建，表面建模，点云，基准数据集，文献综述，深度学习。</p></div><h2>1 Introduction</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>1 引言</h2></div><p>Modeling and reconstruction of object or scene surfaces is a fundamental problem in computer vision and graphics research. Its applications range from virtual/augmented reality, computer animation, to computer-aided design and robotics. Given that the mathematical nature of a surface shape is a continuous 2D manifold embedded in the 3D Euclidean space, different approximations are usually adopted when capturing, transmitting, and storing surface shapes, where the prominent examples include point clouds, polygon meshes, and quantized volumes. In this work, we are particularly interested in reconstructing a continuous surface from its discrete approximation of point cloud, since many depth sensors (e.g., those based on multi-view stereo, structured light, or time-of-flight measurements) produce point clouds (or equivalently, the depth maps) as their original forms of data acquisition, and surface reconstructions from the obtained point clouds are subsequently demanded for various downstream applications.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>对象或场景表面的建模和重建是计算机视觉和图形学研究中的一个基本问题。其应用范围从虚拟/增强现实、计算机动画到计算机辅助设计和机器人技术。考虑到表面形状的数学性质是嵌入在三维欧几里得空间中的连续二维流形，在捕获、传输和存储表面形状时通常采用不同的近似方法，显著的例子包括点云、多边形网格和量化体积。在这项工作中，我们特别关注从点云的离散近似中重建连续表面，因为许多深度传感器（例如，基于多视角立体、结构光或飞行时间测量的传感器）以点云（或等效的深度图）作为其原始数据采集形式，随后需要从获得的点云中进行表面重建以满足各种下游应用。</p></div><p>The problem is technically ill-posed - infinitely many solutions of the underlying, continuous surface may exist given an observed set of discrete points. The challenges become even severer considering that various sensing imperfections would appear during the data acquisition process; the captured point clouds could be noisy and distributed in a non-uniform manner, and they could contain outliers and/or cover less on some surface areas; when point clouds are captured at multiple views, they could be subject to less accurate alignments. All these issues pose the classical problem of surface reconstruction from point clouds as a long-standing challenge that draws continuous efforts from the research community.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>该问题在技术上是病态的——给定一组观察到的离散点，可能存在无限多的潜在连续表面的解。考虑到在数据采集过程中会出现各种传感缺陷，挑战变得更加严峻；捕获的点云可能是嘈杂的，并且以非均匀的方式分布，可能包含异常值和/或在某些表面区域覆盖较少；当点云在多个视角下捕获时，可能会受到较不准确的对齐影响。所有这些问题使得从点云进行表面重建的经典问题成为一个长期挑战，持续吸引着研究界的努力。</p></div><p>In literature, a rich set of methods has been proposed for the focused studies; depending on the types of data imperfection they assume, these methods leverage various priors of surface geometry to combat the otherwise ill-posed problem.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在文献中，针对重点研究提出了一系列丰富的方法；根据它们假设的数据缺陷类型，这些方法利用各种表面几何的先验知识来应对本来病态的问题。</p></div><p>While comprehensive reviews of these methods are given in [1], [2], [3], [4], these reviews are short of investigations and analyses on a common benchmark that could distinguish existing methods when they cope with the aforementioned data imperfections. In the meanwhile, the field has witnessed a recent surge of deep learning surface reconstruction, where models of deep networks are learned and employed to either decode surface shapes from point clouds explicitly [5], [6], [7], or generate implicit fields whose zero-level iso-surfaces can be extracted as the results of surface reconstruction [8], [9], [10]. It is thus desirable to benchmark both the classical and the more recent, deep learning solutions in order to understand their respective strengths and limitations; such investigations would be no doubt valuable for use of the appropriate methods by practitioners, and also as a guidance to new innovations in future research.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>虽然在[1]、[2]、[3]、[4]中对这些方法进行了全面的综述，但这些综述缺乏对共同基准的调查和分析，以区分现有方法在应对上述数据缺陷时的表现。同时，该领域见证了深度学习表面重建的近期激增，其中深度网络模型被学习并用于显式地从点云解码表面形状[5]、[6]、[7]，或生成可以提取为表面重建结果的零水平等值面隐式场[8]、[9]、[10]。因此，基准测试经典和更近期的深度学习解决方案是可取的，以理解它们各自的优缺点；这样的调查无疑对从业者使用适当的方法具有价值，也为未来研究中的新创新提供指导。</p></div><p>The present paper aims to provide a comprehensive review and benchmark existing methods in the new era of deep learning surface reconstruction. We organize our review by categorizing existing methods according to what priors of surface geometry they have used to regularize their reconstructions, where we include the more recent priors of deep models and deep learning, in addition to the classical, optimization-based ones. One of our key contributions is a large-scale benchmarking dataset consisting of both synthetic and real-scanned data (cf. Fig. 2, Fig. 3, and Fig. 5 for illustrations on how the benchmark is constructed). The benchmark includes object- and scene-level surfaces and takes into account various sensing imperfections that are commonly contained in the point clouds obtained by practical 3D scanning, such as point-wise noise, non-uniform distribution of surface points, point outliers, missing of surface points, and misalignment of point sets from multi-view scanning; Fig. 1 gives an illustration of these scanning imperfections. We conduct thorough empirical studies on the constructed benchmark, by comparing existing methods in terms of their capabilities to reconstruct surfaces from observed point clouds. We pay special attention on robustness of existing methods against various scanning imperfections; we also study how different methods generalize in terms of reconstructing complex surface shapes. Our thorough studies help identify the strengths and limitations of existing methods from multiple perspectives. We summarize a few important findings as follows.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>本文旨在提供对深度学习表面重建新纪元中现有方法的全面回顾和基准测试。我们通过根据现有方法所使用的表面几何先验来组织我们的回顾，以规范它们的重建，其中我们包括了更近期的深度模型和深度学习的先验，除了经典的基于优化的方法。我们的一项关键贡献是一个大规模基准数据集，包含合成数据和真实扫描数据（参见图2、图3和图5，了解基准的构建方式）。该基准包括对象和场景级表面，并考虑了在实际3D扫描中获得的点云中常见的各种传感缺陷，如逐点噪声、表面点的非均匀分布、点异常值、表面点的缺失以及多视角扫描中点集的错位；图1展示了这些扫描缺陷的示例。我们对构建的基准进行了全面的实证研究，通过比较现有方法在从观察到的点云重建表面方面的能力。我们特别关注现有方法在面对各种扫描缺陷时的鲁棒性；我们还研究了不同方法在重建复杂表面形状方面的泛化能力。我们的全面研究有助于从多个角度识别现有方法的优缺点。我们总结了一些重要发现如下。</p></div><hr>
<!-- Footnote --><ul>
<li>Z. Huang, Y. Wen, Z. Wang and K. Jia are with the School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>Z. Huang、Y. Wen、Z. Wang 和 K. Jia 现任职于中国广州南方科技大学电子与信息工程学院。</li>
</ul></div><p>E-mail: {eehuangzhangjin, wen.yuxin, eezihaowang}@mail.scut.edu.cn, <a href="mailto:kuijia@scut.edu.cn">kuijia@scut.edu.cn</a></p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>电子邮件：{eehuangzhangjin, wen.yuxin, eezihaowang}@mail.scut.edu.cn, <a href="mailto:kuijia@scut.edu.cn">kuijia@scut.edu.cn</a></p></div><ul>
<li>J. Ren is with the University of Macau, Macau, China.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>J. Ren 现任职于中国澳门大学。</li>
</ul></div><p>E-mails: <a href="mailto:jinjuanren@um.edu.mo">jinjuanren@um.edu.mo</a></p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>电子邮件：<a href="mailto:jinjuanren@um.edu.mo">jinjuanren@um.edu.mo</a></p></div><ul>
<li>Corresponding author: Kui Jia.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>通讯作者：Kui Jia。</li>
</ul></div><ul>
<li>indicates equal contribution.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>表示贡献相等。</li>
</ul></div><!-- Footnote -->
<hr><!-- Media --><!-- figureText: Perfect scanning Non-uniform distribution Point-wise noise Missing points Misalignment Point outliers --><img src="https://cdn.noedgeai.com/bo_d163ugjef24c73d1leb0_1.jpg?x=162&#x26;y=123&#x26;w=1466&#x26;h=467&#x26;r=0"><p>Fig. 1: An illustration of different surface complexities and scanning challenges included in our contributed benchmark. From left to right: example object surfaces of low, middle, and high complexities, the five challenges possibly encountered in practical surface scanning, and examples of different severity levels for the challenge of noisy scanning.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图1：我们贡献的基准中不同表面复杂性和扫描挑战的示例。从左到右：低、中、高复杂度的示例对象表面，实际表面扫描中可能遇到的五个挑战，以及噪声扫描挑战的不同严重程度示例。</p></div><!-- Media --><ul>
<li>While many challenges of surface reconstruction from point clouds can be more or less tackled by existing methods, those of misalignment, missing points, and outliers have been less addressed and remain unsolved.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>尽管现有方法在一定程度上可以解决点云表面重建的许多挑战，但错位、缺失点和异常值的挑战仍然较少被解决，尚未得到解决。</li>
</ul></div><ul>
<li>Deep learning solutions have shown great promise recently for surface modeling and reconstruction; however, our systematic studies suggest that they struggle in generalizing to reconstruction of complex shapes. It is surprising that some classical methods perform even better in terms of both generalization and robustness.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>深度学习解决方案最近在表面建模和重建方面显示出巨大的潜力；然而，我们的系统研究表明，它们在重建复杂形状方面的泛化能力较弱。令人惊讶的是，一些经典方法在泛化和鲁棒性方面的表现甚至更好。</li>
</ul></div><ul>
<li>The use of surface normals is a key to the success of surface reconstruction from raw, observed point clouds, even when the surface normals are estimated less accurately; in many cases, the reconstruction result improves as long as the interior/exterior of the surface can be identified in the 3D space.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>使用表面法线是从原始观察点云中成功进行表面重建的关键，即使表面法线的估计不够准确；在许多情况下，只要能够在3D空间中识别表面的内外部，重建结果就会改善。</li>
</ul></div><ul>
<li>There exist inconsistencies between different evaluation metrics, and in many cases, good quantitative results are not always concordant with the visually pleasant ones. This suggests that more foundation studies are demanded to better benchmark different methods and advance the field.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>不同评估指标之间存在不一致性，在许多情况下，良好的定量结果并不总是与视觉上令人愉悦的结果一致。这表明需要更多基础研究来更好地基准不同方法并推动该领域的发展。</li>
</ul></div><h3>1.1 Related Works</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>1.1 相关工作</h3></div><p>In this section, we give a summary of existing literatures on surface reconstruction from either point clouds or other observations. We also summarize existing datasets and benchmarks that have served for advancing the field.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本节中，我们总结了现有文献中关于从点云或其他观察中进行表面重建的研究。我们还总结了现有的数据集和基准，这些数据集和基准为推动该领域的发展提供了支持。</p></div><p>Surface Reconstruction from Point Clouds There exists a rich set of existing methods studying surface reconstruction from point clouds. These methods are reviewed in [1], [2], [3], [4]. Earlier reviews of [1] and [2] organize existing methods according to what functions of surface representation they use, e.g., implicit or explicit functions. More recently, existing methods are categorized in [4] based on the difference of used techniques, including interpolation and approximation techniques, learning-based techniques, and soft computing techniques. Our organization of review in the present paper is more similar to that in [3], which also organizes existing methods based on the used geometry priors. Compared with [3], our review is more comprehensive and includes the recent methods of deep learning surface reconstruction. In addition, we also compare existing methods empirically on a common benchmark, which helps identify the respective strengths and limitations of existing methods.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>从点云进行表面重建 现有方法中有丰富的研究点云表面重建的文献。这些方法在[1]、[2]、[3]、[4]中进行了回顾。早期的[1]和[2]的回顾根据它们使用的表面表示函数（例如隐式或显式函数）组织现有方法。最近，现有方法在[4]中根据所使用技术的不同进行分类，包括插值和逼近技术、基于学习的技术和软计算技术。我们在本文中的回顾组织方式更类似于[3]，也根据所使用的几何先验组织现有方法。与[3]相比，我们的回顾更为全面，并包括了最近的深度学习表面重建方法。此外，我们还在一个共同的基准上对现有方法进行了实证比较，这有助于识别现有方法的各自优缺点。</p></div><p>More General Surface Modeling and Reconstruction Surface reconstruction can be achieved from other raw observations as well, such as single- or multi-view images, motion, and/or illumination and shading. Literature reviews on the traditional methods of multi-view image reconstruction are provided in [11], [12], and [13]. A more recent survey is given in [14] on multi-view image reconstruction with deep learning. Fahim et al. [15] focus on a more challenging setting of deep learning surface reconstruction from as few as a single image. Zhu et al. [16] provide a more comprehensive survey of 3D modeling methods, including multiview 3D reconstruction, structure from motion, and shape from shading. Different from the above reviews, the present work focuses on surface reconstruction from raw, observed point clouds, considering that depth sensors are increasingly popularly deployed in either portable or fixed devices.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>更通用的曲面建模与重建 曲面重建也可以从其他原始观测数据中实现，例如单视图或多视图图像、运动以及/或者光照和明暗信息。文献[11]、[12]和[13]对多视图图像重建的传统方法进行了综述。文献[14]对基于深度学习的多视图图像重建进行了较新的调研。法希姆（Fahim）等人[15]专注于一个更具挑战性的场景，即仅从单张图像进行深度学习曲面重建。朱（Zhu）等人[16]对三维建模方法进行了更全面的调研，包括多视图三维重建、运动恢复结构和明暗恢复形状。与上述综述不同，本文主要关注从原始观测点云进行曲面重建，考虑到深度传感器在便携式或固定设备中的应用越来越广泛。</p></div><p>Datasets and Benchmarks Existing datasets that support surface reconstruction studies are based on synthetic or real-scanned data; they may include object- and/or scene-level surfaces. For synthetic datasets, surface meshes are usually provided from which point clouds can be sampled. For example, the ShapeNet [17] and ModelNet [18] are two commonly used synthetic datasets consisting of simple, object-level shapes. More complex synthetic object surfaces are provided in the datasets of 3DNet [19], ABC [20], Thingi10k [21], and Three D Scans [22]. The datasets of SceneNet [23] and 3D-FRONT [24] provides synthetic, scene-level surfaces. In the meanwhile, there exist datasets of real-scanned, object-level surfaces [25], [26], [27] and those of real-scanned, scene-level surfaces [28], [29]; however, due to the lack of high-precision scanning, their reconstruction ground truths are usually obtained by appropriate surface reconstruction algorithms, which jeopardizes their roles for benchmarking different methods. Most of the above datasets do not consider sensing imperfections that may appear in practically scanned point clouds, except for [30] that uses virtual scanning to simulate point cloud imperfections; however, the dataset [30] is relatively small, with only eight instances of object surfaces. In contrast, our contributed benchmarking dataset is more comprehensive, including both synthetic and real-scanned data, and covering both object- and scene-level surfaces; we intentionally inject various sensing imperfections into point cloud data of the dataset, including point-wise noise, nonuniform distribution of surface points, point outliers, missing of surface points, and misalignment among point sets from multiview scanning. We expect our benchmark would facilitate more thorough studies in future research.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>数据集与基准 现有的支持曲面重建研究的数据集基于合成数据或真实扫描数据；这些数据集可能包含物体级和/或场景级曲面。对于合成数据集，通常会提供曲面网格，从中可以采样得到点云。例如，ShapeNet [17]和ModelNet [18]是两个常用的合成数据集，包含简单的物体级形状。3DNet [19]、ABC [20]、Thingi10k [21]和Three D Scans [22]等数据集提供了更复杂的合成物体曲面。SceneNet [23]和3D - FRONT [24]数据集提供了合成的场景级曲面。同时，存在真实扫描的物体级曲面数据集[25]、[26]、[27]和真实扫描的场景级曲面数据集[28]、[29]；然而，由于缺乏高精度扫描，它们的重建真值通常是通过适当的曲面重建算法获得的，这影响了它们作为不同方法基准的作用。除了文献[30]使用虚拟扫描来模拟点云缺陷外，上述大多数数据集都没有考虑实际扫描点云中可能出现的传感缺陷；然而，文献[30]中的数据集相对较小，只有八个物体曲面实例。相比之下，我们贡献的基准数据集更全面，包括合成数据和真实扫描数据，涵盖物体级和场景级曲面；我们有意在数据集中的点云数据中引入各种传感缺陷，包括逐点噪声、曲面上点的不均匀分布、点离群值、曲面点缺失以及多视图扫描点集之间的不对齐。我们期望我们的基准能够促进未来研究中更深入的研究。</p></div><h3>1.2 Contributions</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>1.2 贡献</h3></div><p>As stated in Section 1, with the surge of deep learning surface reconstruction, the present paper aims to provide a comprehensive review of exiting methods in the new era, and study their respective advantages and disadvantages when reconstructing object- or scene-level surfaces from raw, observed point clouds. To this end, we contribute a large-scale benchmark consisting of both synthetic and real-scanned data. We use the constructed benchmark for systematic studies of existing methods, focusing on the robustness of these methods against various data imperfections, and also on how existing methods generalize in terms of reconstructing complex surface shapes. We summarize our key contributions as follows.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>如第1节所述，随着深度学习曲面重建的兴起，本文旨在对新时代现有的方法进行全面综述，并研究它们在从原始观测点云重建物体级或场景级曲面时各自的优缺点。为此，我们贡献了一个包含合成数据和真实扫描数据的大规模基准。我们使用构建的基准对现有方法进行系统研究，重点关注这些方法对各种数据缺陷的鲁棒性，以及现有方法在重建复杂曲面形状方面的泛化能力。我们总结了我们的主要贡献如下。</p></div><ul>
<li>We provide a comprehensive review of existing surface reconstruction methods, by bridging together the classical, optimization-based methods with the more recent, deep learning-based ones, where we categorize these methods according to what priors of surface geometry they have used to regularize their solutions.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>我们对现有的曲面重建方法进行了全面综述，将经典的基于优化的方法与较新的基于深度学习的方法联系起来，我们根据这些方法在正则化其解时所使用的曲面几何先验对它们进行分类。</li>
</ul></div><ul>
<li>We contribute a large-scale benchmarking dataset consisting of both synthetic and real-scanned data. The point cloud data in the benchmark have various sensing imperfections that are commonly encountered in practical \(3\mathrm{D}\) scanning processes; these imperfections are intentionally included to benchmark the robustness of existing methods.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>我们贡献了一个包含合成数据和真实扫描数据的大规模基准数据集。基准中的点云数据具有实际 \(3\mathrm{D}\) 扫描过程中常见的各种传感缺陷；有意包含这些缺陷是为了评估现有方法的鲁棒性。</li>
</ul></div><ul>
<li>We compare existing methods by conducting thorough empirical studies on the constructed benchmark. Our studies help identify the strengths and limitations of existing methods, which are valuable both for choices of appropriate methods by practitioners and for guiding the directions of new innovations in future research.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>我们通过在构建的基准上进行全面的实证研究来比较现有方法。我们的研究有助于识别现有方法的优势和局限性，这对于从业者选择合适的方法以及指导未来研究中的新创新方向都很有价值。</li>
</ul></div><h3>1.3 Paper Organization</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>1.3 论文结构</h3></div><p>The paper is organized as follows. Section 2 gives the formal definition of our studied problem. Section 3 organizes and reviews existing methods based on priors of surface geometry that they have used to regularize the reconstructions. We present our contributed large-scale benchmark in Section 4, where we give details about how we construct the benchmark and also the benchmark statistics; we make the benchmark, our construction manner of the benchmark, and also implementation codes of representative methods publicly accessible at <a href="https://Gorilla-Lab-SCUT.github.io/SurfaceReconstructionBenchmark.Experimental">https://Gorilla-Lab-SCUT.github.io/SurfaceReconstructionBenchmark.Experimental</a> setups of our empirical studies are given in Section 5, before results, analyses, and important insights are presented in Section 6. We finally draw the paper conclusion in Section 7.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>论文结构如下。第2节给出了我们所研究问题的正式定义。第3节根据现有方法在正则化重建时所使用的曲面几何先验对它们进行组织和综述。我们在第4节介绍我们贡献的大规模基准，详细说明我们如何构建基准以及基准的统计信息；我们将基准、我们构建基准的方式以及代表性方法的实现代码公开在<a href="https://Gorilla">https://Gorilla</a> - Lab - SCUT.github.io/SurfaceReconstructionBenchmark。第5节给出我们实证研究的实验设置，然后在第6节展示结果、分析和重要见解。最后，我们在第7节得出论文结论。</p></div><h2>2 Problem Statement</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>2 问题陈述</h2></div><p>Consider a discrete point set \(\mathcal{P}\) that may be obtained by scanning an object or scene surface using some 3D sensing devices; each \(\mathbf{p} \in  {\mathbb{R}}^{3}\) of its contained points collects the coordinates in the Euclidean space. Our goal of interest is to recover its underlying, continuous surface \({\mathcal{S}}^{ * }\) from which the points \(\{ \mathbf{p} \in  \mathcal{P}\}\) are practically observed. Given that recovering the continuous \({\mathcal{S}}^{ * }\) from the discrete \(\mathcal{P}\) is an ill-posed problem,an appropriate regularization must be imposed in order to recover a geometry-aware approximation \(\mathcal{S}\) ,e.g.,a smooth and/or fair surface [31]. This formally amounts to solving the following regularized optimization</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>考虑一个离散点集 \(\mathcal{P}\)，该点集可以通过使用某些3D传感设备扫描物体或场景表面获得；其包含的每个 \(\mathbf{p} \in  {\mathbb{R}}^{3}\) 点收集在欧几里得空间中的坐标。我们感兴趣的目标是从中恢复其潜在的连续表面 \({\mathcal{S}}^{ * }\)，从而实际观察到这些点 \(\{ \mathbf{p} \in  \mathcal{P}\}\)。鉴于从离散 \(\mathcal{P}\) 中恢复连续 \({\mathcal{S}}^{ * }\) 是一个病态问题，必须施加适当的正则化，以便恢复一个几何感知的近似 \(\mathcal{S}\)，例如，一个光滑和/或公平的表面 [31]。这在形式上等同于解决以下正则化优化</p></div><p></p>\[\mathop{\min }\limits_{\mathcal{S}}L\left( {\mathcal{S};\mathcal{P}}\right)  + {\lambda R}\left( \mathcal{S}\right)  \tag{1}\]<p></p><p>where \(L\) is a loss term for data fidelity to the observed \(\mathcal{P},R\) is a regularizer that constrains the solution with a certain prior of surface geometry,and \(\lambda\) is a scalar penalty. Note that the objective (1) is only in an abstract form, since it is difficult to define both \(L\) and \(R\) directly on \(\mathcal{S}\) . In practice,one may represent a surface either explicitly as a parametric mapping \(\mathbf{f} : {\Omega }^{2} \rightarrow  \mathcal{S}\) ,where \({\Omega }^{2} \subset  {\mathbb{R}}^{2}\) denotes the \(2\mathrm{D}\) domain and \(\mathcal{S} = \left\{  {\mathbf{f}\left( \mathbf{x}\right)  \in  {\mathbb{R}}^{3}}\right\}\) with \(\mathbf{x} \in  {\Omega }^{2}\) ,or implicitly as the zero-level set of an implicit function \(F : {\mathbb{R}}^{3} \rightarrow  \mathbb{R}\) ,i.e., \(\mathcal{S} = \left\{  {\mathbf{q} \in  {\mathbb{R}}^{3} \mid  F\left( \mathbf{q}\right)  = 0}\right\}\) . Correspondingly, one can instantiate the objective (1) as the following one that pursues \(\mathcal{S}\) by optimizing an explicit mapping</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(L\) 是对观察到的 \(\mathcal{P},R\) 的数据保真度的损失项，\(\mathcal{P},R\) 是约束解的正则化项，具有某种表面几何的先验，\(\lambda\) 是一个标量惩罚。请注意，目标 (1) 仅以抽象形式存在，因为很难直接在 \(\mathcal{S}\) 上定义 \(L\) 和 \(R\)。在实践中，可以将表面显式表示为参数映射 \(\mathbf{f} : {\Omega }^{2} \rightarrow  \mathcal{S}\)，其中 \({\Omega }^{2} \subset  {\mathbb{R}}^{2}\) 表示 \(2\mathrm{D}\) 域，\(\mathcal{S} = \left\{  {\mathbf{f}\left( \mathbf{x}\right)  \in  {\mathbb{R}}^{3}}\right\}\) 与 \(\mathbf{x} \in  {\Omega }^{2}\)，或隐式表示为隐函数 \(F : {\mathbb{R}}^{3} \rightarrow  \mathbb{R}\) 的零水平集，即 \(\mathcal{S} = \left\{  {\mathbf{q} \in  {\mathbb{R}}^{3} \mid  F\left( \mathbf{q}\right)  = 0}\right\}\)。相应地，可以将目标 (1) 实例化为以下追求 \(\mathcal{S}\) 的形式，通过优化显式映射</p></div><p></p>\[\mathop{\min }\limits_{{\mathbf{f} \in  {\mathcal{H}}_{\mathbf{f}}}}{L}^{\exp }\left( {\mathbf{f};\mathcal{P}}\right)  + \lambda {R}^{\exp }\left( \mathbf{f}\right) , \tag{2}\]<p></p><p>where \({L}^{\exp }\) and \({R}^{\exp }\) are respectively the instantiated loss function and regularizer,and \(\mathbf{f}\) is optionally constrained in a hypothesis space \({\mathcal{H}}_{\mathbf{f}}\) (e.g.,by choosing \(\mathbf{f}\) as a neural network). Alternatively, one may instantiate the objective (1) as the following one that pursues an implicit representation of \(\mathcal{S}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \({L}^{\exp }\) 和 \({R}^{\exp }\) 分别是实例化的损失函数和正则化项，\(\mathbf{f}\) 可选地约束在假设空间 \({\mathcal{H}}_{\mathbf{f}}\) 中（例如，通过选择 \(\mathbf{f}\) 作为神经网络）。或者，可以将目标 (1) 实例化为以下追求 \(\mathcal{S}\) 的隐式表示的形式</p></div><p></p>\[\mathop{\min }\limits_{{F \in  {\mathcal{H}}_{F}}}{L}^{\mathrm{{imp}}}\left( {F;\mathcal{P}}\right)  + \lambda {R}^{\mathrm{{imp}}}\left( F\right) \text{ s.t. }F\left( \mathbf{q}\right)  = 0\forall \mathbf{q} \in  \mathcal{S}, \tag{3}\]<p></p><p>where \({L}^{\text{imp }}\) and \({R}^{\text{imp }}\) are again the instantiated functions,and \({\mathcal{H}}_{F}\) denotes a hypothesis space that optionally constrains the implicit function \(F\) . We present the subsequent sections based on the notations defined in Table 1, unless specified otherwise.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \({L}^{\text{imp }}\) 和 \({R}^{\text{imp }}\) 再次是实例化的函数，\({\mathcal{H}}_{F}\) 表示一个假设空间，可选地约束隐函数 \(F\)。除非另有说明，我们将根据表1中定义的符号呈现后续部分。</p></div><h2>3 Surface Reconstruction with a Categorization of Geometric Priors</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>3 表面重建与几何先验的分类</h2></div><p>Given an observed point set \(\mathcal{P}\) ,a reconstructed surface \(\mathcal{S}\) should be close to \(\mathcal{P}\) under some distance metric; this is guaranteed by the first term \(L\left( {\mathcal{S};\mathcal{P}}\right)\) of data fidelity in the abstract objective (1). Section 2 also suggests that \(L\left( {\mathcal{S};\mathcal{P}}\right)\) can be instantiated either explicitly or implicitly. The explicit form is generally written as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>给定一个观察到的点集 \(\mathcal{P}\)，重建的表面 \(\mathcal{S}\) 应该在某种距离度量下接近 \(\mathcal{P}\)；这由抽象目标 (1) 中的数据保真度的第一项 \(L\left( {\mathcal{S};\mathcal{P}}\right)\) 保证。第2节还建议 \(L\left( {\mathcal{S};\mathcal{P}}\right)\) 可以显式或隐式实例化。显式形式通常写为</p></div><p></p>\[{L}^{\exp }\left( {\mathbf{f};\mathcal{P}}\right)  = \frac{1}{{n}_{\mathcal{P}}}\mathop{\sum }\limits_{{\mathbf{p} \in  \mathcal{P}}}\mathop{\min }\limits_{{\mathbf{x} \in  {\Omega }^{2}}}\parallel \mathbf{f}\left( \mathbf{x}\right)  - \mathbf{p}{\parallel }_{\ell }, \tag{4}\]<p></p><p>where \(\parallel  \cdot  {\parallel }_{\ell }\) denotes a proper norm of distance,with \(\ell\) typically set as 1 or 2; the above term (4) constrains the learning of mapping function \(\mathbf{f} \in  {\mathcal{H}}_{\mathbf{f}}\) . In practice,one may sample a fixed set \(\{ \mathbf{x} \in\) \(\left. {\Omega }^{2}\right\}\) instead of optimizing over the whole domain \({\Omega }^{2}\) ,which gives variants of Eq. (4) based on point-set distances, such as Chamfer or Hausdorff distances. An implicit form of \(L\left( {\mathcal{S};\mathcal{P}}\right)\) is generally written as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(\parallel  \cdot  {\parallel }_{\ell }\) 表示距离的适当范数，\(\ell\) 通常设为 1 或 2；上述公式 (4) 限制了映射函数 \(\mathbf{f} \in  {\mathcal{H}}_{\mathbf{f}}\) 的学习。在实践中，可以对一个固定的集合 \(\{ \mathbf{x} \in\) \(\left. {\Omega }^{2}\right\}\) 进行采样，而不是在整个领域 \({\Omega }^{2}\) 上进行优化，这会基于点集距离给出公式 (4) 的变体，例如 Chamfer 距离或 Hausdorff 距离。\(L\left( {\mathcal{S};\mathcal{P}}\right)\) 的隐式形式通常写作</p></div><p></p>\[{L}^{\mathrm{{imp}}}\left( {F;\mathcal{P}}\right)  = \frac{1}{{n}_{\mathcal{P}}}\mathop{\sum }\limits_{{\mathbf{p} \in  \mathcal{P}}}\parallel F\left( \mathbf{p}\right) {\parallel }_{\ell }, \tag{5}\]<p></p><!-- Media --><p>TABLE 1: Math notations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表 1：数学符号。</p></div><table><tbody><tr><td>Notation</td><td>Description</td></tr><tr><td>\( {\mathcal{S}}^{ * } \)</td><td>An underlying surface to be recovered; \( {\mathcal{S}}^{ * } \subset  {\mathbb{R}}^{3} \) .</td></tr><tr><td>\( \mathcal{S} \)</td><td>A reconstructed surface; \( \mathcal{S} \subset  {\mathbb{R}}^{3} \) .</td></tr><tr><td>\( {\mathcal{G}}_{S} \)</td><td>A triangular mesh representation of surface \( \mathcal{S} \) . A mesh with \( {n}_{\mathcal{G}} \) faces is collectively written as \( {\mathcal{G}}_{\mathcal{S}} \triangleq  {\left\{  {\mathcal{T}}_{i}\right\}  }_{i = 1}^{{n}_{\mathcal{G}}} \) ,where each face \( \mathcal{T} \) is specified by \( \left\{  {{\mathbf{v}}_{1},{\mathbf{v}}_{2},{\mathbf{v}}_{3}}\right\} \) containing three vertices; we also write as \( {\mathbf{e}}_{ij} \) for the edge connecting vertices \( {\mathbf{v}}_{i} \) and \( {\mathbf{v}}_{j} \) .</td></tr><tr><td>\( \mathcal{P} \)</td><td>A set of \( {n}_{\mathcal{P}} \) discrete points \( {\left\{  {\mathbf{p}}_{i} \in  {\mathbb{R}}^{3}\right\}  }_{i = 1}^{{n}_{\mathcal{P}}} \) ,repre- senting the practical sampling of an underlying surface \( {S}^{ * } \) .</td></tr><tr><td>\( {n}_{p} \)</td><td>An estimated, oriented surface normal defined at a surface point \( \mathbf{p};{\mathbf{n}}_{\mathbf{p}} \in  {\mathbb{R}}^{3} \) .</td></tr><tr><td>\( \mathcal{N}\left( \mathbf{p}\right) \)</td><td>A local neighborhood of points centered at \( \mathbf{p} \) .</td></tr><tr><td>\( \Omega \)</td><td>A domain of subset space,e.g., \( {\Omega }^{k} \subset  {\mathbb{R}}^{k} \) .</td></tr><tr><td>\( {\mathcal{C}}^{k} \)</td><td>The smoothness of a function,where \( k \) is the number of continuous derivatives the function has over some domain.</td></tr><tr><td>\( d\left( \mathbf{p}\right) \)</td><td>A signed or unsigned distance field value at a point \( \mathbf{p} \) .</td></tr><tr><td>\( {f}_{\theta } \) or \( {F}_{\theta } \)</td><td>An explicit or implicit model of surface reconstruction parameterized by \( \mathbf{\theta } \) (e.g.,a neural network); the param- eters are also denoted as \( {\mathbf{\theta }}_{f} \) or \( {\mathbf{\theta }}_{F} \) .</td></tr><tr><td>\( {\nabla }_{x}{f}_{\mathbf{\theta }} \) or \( {\nabla }_{x}{F}_{\mathbf{\theta }} \)</td><td>Model derivative with respect to \( x \) .</td></tr><tr><td>\( L\left( {\mathcal{S};\mathcal{P}}\right) \)</td><td>Loss function of a reconstructed surface \( \mathcal{S} \) for data fidelity to an observed \( \mathcal{P} \) .</td></tr><tr><td>\( R\left( \mathcal{S}\right) \)</td><td>Regularizer imposed on a reconstructed surface \( \mathcal{S} \) .</td></tr><tr><td>\( K \)</td><td>Extrinsic matrix of a camera.</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>符号</td><td>描述</td></tr><tr><td>\( {\mathcal{S}}^{ * } \)</td><td>待恢复的基础表面；\( {\mathcal{S}}^{ * } \subset  {\mathbb{R}}^{3} \)。</td></tr><tr><td>\( \mathcal{S} \)</td><td>重建的表面；\( \mathcal{S} \subset  {\mathbb{R}}^{3} \)。</td></tr><tr><td>\( {\mathcal{G}}_{S} \)</td><td>表面\( \mathcal{S} \)的三角网格表示。具有\( {n}_{\mathcal{G}} \)个面的网格统称为\( {\mathcal{G}}_{\mathcal{S}} \triangleq  {\left\{  {\mathcal{T}}_{i}\right\}  }_{i = 1}^{{n}_{\mathcal{G}}} \)，其中每个面\( \mathcal{T} \)由\( \left\{  {{\mathbf{v}}_{1},{\mathbf{v}}_{2},{\mathbf{v}}_{3}}\right\} \)指定，包含三个顶点；我们也将连接顶点\( {\mathbf{v}}_{i} \)和\( {\mathbf{v}}_{j} \)的边写作\( {\mathbf{e}}_{ij} \)。</td></tr><tr><td>\( \mathcal{P} \)</td><td>一组\( {n}_{\mathcal{P}} \)离散点\( {\left\{  {\mathbf{p}}_{i} \in  {\mathbb{R}}^{3}\right\}  }_{i = 1}^{{n}_{\mathcal{P}}} \)，表示对基础表面\( {S}^{ * } \)的实际采样。</td></tr><tr><td>\( {n}_{p} \)</td><td>在表面点\( \mathbf{p};{\mathbf{n}}_{\mathbf{p}} \in  {\mathbb{R}}^{3} \)处定义的估计的、有向的表面法线。</td></tr><tr><td>\( \mathcal{N}\left( \mathbf{p}\right) \)</td><td>以\( \mathbf{p} \)为中心的局部点邻域。</td></tr><tr><td>\( \Omega \)</td><td>子集空间的一个域，例如，\( {\Omega }^{k} \subset  {\mathbb{R}}^{k} \)。</td></tr><tr><td>\( {\mathcal{C}}^{k} \)</td><td>函数的光滑度，其中\( k \)是函数在某个域上具有的连续导数的数量。</td></tr><tr><td>\( d\left( \mathbf{p}\right) \)</td><td>在点\( \mathbf{p} \)处的有符号或无符号距离场值。</td></tr><tr><td>\( {f}_{\theta } \)或\( {F}_{\theta } \)</td><td>由\( \mathbf{\theta } \)参数化的表面重建的显式或隐式模型（例如，神经网络）；参数也表示为\( {\mathbf{\theta }}_{f} \)或\( {\mathbf{\theta }}_{F} \)。</td></tr><tr><td>\( {\nabla }_{x}{f}_{\mathbf{\theta }} \)或\( {\nabla }_{x}{F}_{\mathbf{\theta }} \)</td><td>关于\( x \)的模型导数。</td></tr><tr><td>\( L\left( {\mathcal{S};\mathcal{P}}\right) \)</td><td>重建表面\( \mathcal{S} \)的损失函数，用于数据对观察到的\( \mathcal{P} \)的保真度。</td></tr><tr><td>\( R\left( \mathcal{S}\right) \)</td><td>施加在重建表面\( \mathcal{S} \)上的正则化器。</td></tr><tr><td>\( K \)</td><td>相机的外部矩阵。</td></tr></tbody></table></div><!-- Media --><p>which learns the implicit function \(F \in  {\mathcal{H}}_{F}\) by minimizing a proper norm of \(F\left( \mathbf{p}\right)\) for any \(\mathbf{p} \in  \mathcal{P}\) . Advanced versions of the implicit data fidelity loss exist, e.g.,</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>通过最小化任何\(\mathbf{p} \in  \mathcal{P}\)的适当范数来学习隐式函数\(F \in  {\mathcal{H}}_{F}\)。隐式数据保真损失的高级版本存在，例如，</p></div><p></p>\[{L}^{\mathrm{{imp}} +  + }\left( {F;\mathcal{P}}\right)  = {\alpha }_{1}{\mathbb{E}}_{\mathbf{q} \in  {\mathbb{R}}^{3}}{\begin{Vmatrix}F\left( \mathbf{q}\right)  - d\left( \mathbf{q};\mathcal{P}\right) \end{Vmatrix}}_{{\ell }_{1}} +  \tag{6}\]<p></p>
<p></p>\[{\alpha }_{2}{\mathbb{E}}_{\mathbf{q} \in  {\mathbb{R}}^{3}}{\begin{Vmatrix}{\nabla }_{\mathbf{q}}F\left( \mathbf{q}\right)  - \mathbf{n}\left( \mathbf{q};\mathcal{P}\right) \end{Vmatrix}}_{{\ell }_{2}} + \cdots ,\]<p></p><p>where when \(F\) models a Signed Distance Function (SDF) [8], \(d\left( {\mathbf{q};\mathcal{P}}\right)\) denotes the signed distance between any space point \(\mathbf{q} \in\) \({\mathbb{R}}^{3}\) and the observed point set \(\mathcal{P}\) ,which vanishes when \(\mathbf{q}\) hits any \(\mathbf{p} \in  \mathcal{P}\) ,and when \(F\) models an Occupancy Field (OF) [9], \(d\left( {\mathbf{q};\mathcal{P}}\right)  \in  \{ 0,1\}\) depending on whether \(\mathbf{q}\) is inside or outside the surface \(\mathcal{S}\) ,which is practically estimated by comparing \(\mathbf{q}\) with the observed \(\mathcal{P};\mathbf{n}\left( {\mathbf{q};\mathcal{P}}\right)\) denotes the normal at point \(\mathbf{q}\) ,which, when \(\mathbf{q}\) hits some \(\mathbf{p} \in  \mathcal{P}\) ,can be estimated by computing the local tangent plane of \(\mathcal{P}\) at \(\mathbf{p}\) ,and \(\parallel \mathbf{n}\left( {\mathbf{q};\mathcal{P}}\right) {\parallel }_{2} = 1\) otherwise (when \(F\) models an SDF,the second term in Eq. (6) is correspondingly written as \({\mathbb{E}}_{\mathbf{q} \in  {\mathbb{R}}^{3}}\left| {{\begin{Vmatrix}{\nabla }_{\mathbf{q}}F\left( \mathbf{q}\right) \end{Vmatrix}}_{2} - 1}\right|\) ); one may use \(\left\{  {{\alpha }_{1},{\alpha }_{2},\cdots }\right\}\) to weight or switch on/off different terms in Eq. (6).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>当\(F\)建模为有符号距离函数（SDF）[8]时，\(d\left( {\mathbf{q};\mathcal{P}}\right)\)表示任何空间点\(\mathbf{q} \in\) \({\mathbb{R}}^{3}\)与观察点集\(\mathcal{P}\)之间的有符号距离，当\(\mathbf{q}\)碰到任何\(\mathbf{p} \in  \mathcal{P}\)时，该距离消失；而当\(F\)建模为占用场（OF）[9]时，\(d\left( {\mathbf{q};\mathcal{P}}\right)  \in  \{ 0,1\}\)取决于\(\mathbf{q}\)是在表面\(\mathcal{S}\)内部还是外部，该值通过将\(\mathbf{q}\)与观察到的\(\mathcal{P};\mathbf{n}\left( {\mathbf{q};\mathcal{P}}\right)\)进行比较来实际估计，表示在点\(\mathbf{q}\)的法线，当\(\mathbf{q}\)碰到某些\(\mathbf{p} \in  \mathcal{P}\)时，可以通过计算在\(\mathbf{p}\)处的\(\mathcal{P}\)的局部切平面来估计，而在其他情况下\(\parallel \mathbf{n}\left( {\mathbf{q};\mathcal{P}}\right) {\parallel }_{2} = 1\)（当\(F\)建模为SDF时，公式（6）中的第二项相应地写为\({\mathbb{E}}_{\mathbf{q} \in  {\mathbb{R}}^{3}}\left| {{\begin{Vmatrix}{\nabla }_{\mathbf{q}}F\left( \mathbf{q}\right) \end{Vmatrix}}_{2} - 1}\right|\)）；可以使用\(\left\{  {{\alpha }_{1},{\alpha }_{2},\cdots }\right\}\)来加权或开关公式（6）中的不同项。</p></div><p>Due to the ill-posed nature of surface reconstruction from \(\mathcal{P}\) , neither of the data fidelity loss terms (4) or (5) is sufficient to reconstruct a geometry-plausible \(\mathcal{S}\) . In literature,various instantiations of the regularization \(R\left( \mathcal{S}\right)\) in Eq. (1) have been proposed,in order to make the problem be better posed. In the remainder of this section, we discuss the essence of existing surface reconstruction methods by categorizing their adopted regularization of geometric priors, including triangulation-based prior, smoothness prior, template-based prior, modeling prior, learning-based prior, and hybrid prior, where we include both classical and the recent, deep learning ones. We expect our categorization and discussion would foster new innovations by bridging classical surface reconstruction methods with the more recent deep learning solutions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>由于从\(\mathcal{P}\)进行表面重建的病态性质，数据保真损失项（4）或（5）都不足以重建几何上合理的\(\mathcal{S}\)。在文献中，已经提出了公式（1）中正则化\(R\left( \mathcal{S}\right)\)的各种实例，以使问题更好地被提出。在本节的其余部分，我们通过对现有表面重建方法所采用的几何先验的正则化进行分类，讨论其本质，包括基于三角剖分的先验、平滑性先验、基于模板的先验、建模先验、基于学习的先验和混合先验，其中我们包括经典的和最近的深度学习方法。我们期望我们的分类和讨论能够通过将经典表面重建方法与更近期的深度学习解决方案相结合，促进新的创新。</p></div><h3>3.1 Triangulation-based Prior</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.1 基于三角剖分的先验</h3></div><p>A closed surface is continuous and could be locally differentiable up to different orders. When the \(\mathcal{S}\) to be recovered from \(\mathcal{P}\) is locally differentiable up to the first order,i.e., \(f\) is locally of \({\mathcal{C}}^{1}\) , a piecewise linear assumption stands as a good prior for modeling the surface, which gives a mesh representation of the surface. Among various meshing schemes, Delaunay triangulation [32] is a classical one that approximates \(\mathcal{S}\) as a mesh that satisfies</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>一个封闭表面是连续的，并且可以局部可微到不同的阶数。当要从\(\mathcal{P}\)恢复的\(\mathcal{S}\)局部可微到一阶，即\(f\)局部为\({\mathcal{C}}^{1}\)时，分段线性假设作为建模表面的良好先验，这为表面提供了网格表示。在各种网格化方案中，Delaunay三角剖分[32]是一个经典的方案，它将\(\mathcal{S}\)近似为满足</p></div><p></p>\[{\mathcal{G}}_{\mathcal{S}} = {\left\{  {\mathcal{T}}_{i}\right\}  }_{i = 1}^{{n}_{\mathcal{G}}}\]<p></p><p></p>\[\text{s.t.}{\mathbf{v}}_{1}^{\mathcal{T}} \in  \mathcal{P},{\mathbf{v}}_{2}^{\mathcal{T}} \in  \mathcal{P},{\mathbf{v}}_{3}^{\mathcal{T}} \in  \mathcal{P}\forall \mathcal{T} \in  {\mathcal{G}}_{\mathcal{S}}\text{,}\]<p></p><p></p>\[\mathbf{p} \notin  \mathrm{{CC}}\left( \mathcal{T}\right) \forall \mathbf{p} \in  \mathcal{P}/\left\{  {{\mathbf{v}}_{1}^{\mathcal{T}},{\mathbf{v}}_{2}^{\mathcal{T}},{\mathbf{v}}_{3}^{\mathcal{T}}}\right\}   \land  \forall \mathcal{T} \in  {\mathcal{G}}_{\mathcal{S}}, \tag{7}\]<p></p><p>where \(\mathcal{T}\) is a triangular face with its three vertices denoted as \(\left\{  {{\mathbf{v}}_{1}^{\mathcal{T}},{\mathbf{v}}_{2}^{\mathcal{T}},{\mathbf{v}}_{3}^{\mathcal{T}}}\right\}  ,{n}_{\mathcal{G}}\) is the number of faces,and \(\mathrm{{CC}}\left( \mathcal{T}\right)\) denotes the space enclosed by the circumcircle of \(\mathcal{T}\) passing through its three vertices; note that by Delaunay triangulation, the explicit data fidelity Eq. (4) is satisfied simultaneously.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(\mathcal{T}\) 是一个三角形面，其三个顶点表示为 \(\left\{  {{\mathbf{v}}_{1}^{\mathcal{T}},{\mathbf{v}}_{2}^{\mathcal{T}},{\mathbf{v}}_{3}^{\mathcal{T}}}\right\}  ,{n}_{\mathcal{G}}\) ，\(\left\{  {{\mathbf{v}}_{1}^{\mathcal{T}},{\mathbf{v}}_{2}^{\mathcal{T}},{\mathbf{v}}_{3}^{\mathcal{T}}}\right\}  ,{n}_{\mathcal{G}}\) 是面的数量，\(\mathrm{{CC}}\left( \mathcal{T}\right)\) 表示通过 \(\mathcal{T}\) 三个顶点的外接圆所包围的空间；请注意，通过德劳内三角剖分（Delaunay triangulation），显式数据保真度方程（4）会同时得到满足。</p></div><p>Representative methods [33], [34] of Delaunay triangulation first generate a set of triangular faces directly from the observed \(\mathcal{P}\) ,and then select the optimal subset from them to generate the final triangular mesh. Greedy Delaunay (GD) [33] proposes greedy algorithm based on topological constraints to select valid triangles sequentially, where the initial triangles are generated by Delaunay triangulation; Ball-Pivoting Algorithm (BPA) [34] uses balls with various radii rolling over the points in \(\mathcal{P}\) to generate triangles, where every three points touched by a rolling ball will construct a new triangle if the triangle does not encompass any other points, which can also be regarded as an approximation of Delaunay triangulation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>德劳内三角剖分（Delaunay triangulation）的代表性方法 [33]、[34] 首先直接从观测到的 \(\mathcal{P}\) 生成一组三角形面，然后从这些面中选择最优子集以生成最终的三角网格。贪婪德劳内（Greedy Delaunay，GD）[33] 提出了基于拓扑约束的贪婪算法，用于依次选择有效的三角形，其中初始三角形是通过德劳内三角剖分生成的；球旋转算法（Ball - Pivoting Algorithm，BPA）[34] 使用不同半径的球在 \(\mathcal{P}\) 中的点上滚动来生成三角形，如果一个滚动的球接触到的三个点所构成的三角形不包含任何其他点，那么这三个点就会构成一个新的三角形，这也可以看作是德劳内三角剖分的一种近似。</p></div><h3>3.2 Surface Smoothness Priors</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.2 表面平滑先验</h3></div><p>In more general cases,the surface \(\mathcal{S}\) to be recovered is expected to be smooth or continuously differentiable up to a certain order [31]. Surface smoothness is usually enforced by the following two manners.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在更一般的情况下，待重建的表面 \(\mathcal{S}\) 期望是平滑的，或者在一定阶数上是连续可微的 [31]。表面平滑通常通过以下两种方式来实现。</p></div><p>Given that the observed points in \(\mathcal{P}\) could be noisy,the first manner smoothes out \(\{ \mathbf{p} \in  \mathcal{P}\}\) via local weighted combination when fitting the explicit mapping function \(\mathbf{f}\) ,resulting in an regularized version of Eq. (4) as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>鉴于 \(\mathcal{P}\) 中的观测点可能存在噪声，第一种方式是在拟合显式映射函数 \(\mathbf{f}\) 时，通过局部加权组合对 \(\{ \mathbf{p} \in  \mathcal{P}\}\) 进行平滑处理，从而得到方程（4）的正则化版本为</p></div><p></p>\[\mathop{\min }\limits_{\mathbf{f}}{L}^{\exp }\left( {\mathbf{f};\mathcal{P}}\right)  = \frac{1}{{n}_{\mathcal{P}}}\mathop{\sum }\limits_{{\mathbf{p} \in  \mathcal{P}}}\mathop{\min }\limits_{{\mathbf{x} \in  {\Omega }^{2}}}\parallel \mathbf{f}\left( \mathbf{x}\right)  - \widehat{\mathbf{p}}\left( \mathbf{p}\right) {\parallel }_{2}^{2} \tag{8}\]<p></p>
<p></p>\[\text{s.t.}\widehat{\mathbf{p}}\left( \mathbf{p}\right)  = \mathop{\sum }\limits_{{{\mathbf{p}}^{\prime } \in  \mathcal{N}\left( \mathbf{p}\right) }}{\mathbf{p}}^{\prime }/g\left( {\begin{Vmatrix}\mathbf{p} - {\mathbf{p}}^{\prime }\end{Vmatrix}}_{2}\right) \text{,}\]<p></p><p>where \(\mathcal{N}\left( \mathbf{p}\right)\) denotes a local neighborhood of the observed \(\mathbf{p}\) , containing \(\left\{  {{\mathbf{p}}^{\prime } \in  \mathcal{P}}\right\}\) ,and \(g\left( {\begin{Vmatrix}\mathbf{p} - {\mathbf{p}}^{\prime }\end{Vmatrix}}_{2}\right)\) denotes a function (e.g., a Radial Basis Function or RBF) whose value is proportional to the distance \({\begin{Vmatrix}\mathbf{p} - {\mathbf{p}}^{\prime }\end{Vmatrix}}_{2}\) . A similar implicit objective exists by regularizing the second-order term in Eq. (6), giving rise to the method of Poisson Surface Reconstruction (PSR) [35]</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(\mathcal{N}\left( \mathbf{p}\right)\) 表示观测到的 \(\mathbf{p}\) 的一个局部邻域，包含 \(\left\{  {{\mathbf{p}}^{\prime } \in  \mathcal{P}}\right\}\) ，\(g\left( {\begin{Vmatrix}\mathbf{p} - {\mathbf{p}}^{\prime }\end{Vmatrix}}_{2}\right)\) 表示一个函数（例如，径向基函数或 RBF），其值与距离 \({\begin{Vmatrix}\mathbf{p} - {\mathbf{p}}^{\prime }\end{Vmatrix}}_{2}\) 成正比。通过对方程（6）中的二阶项进行正则化，存在一个类似的隐式目标，从而产生了泊松表面重建（Poisson Surface Reconstruction，PSR）方法 [35]</p></div><p></p>\[\mathop{\min }\limits_{F}{L}^{\mathrm{{PSR}}}\left( {F;\mathcal{P}}\right)  = {\mathbb{E}}_{\mathbf{q} \in  {\mathbb{R}}^{3}}{\begin{Vmatrix}{\nabla }_{\mathbf{q}}F\left( \mathbf{q}\right)  - \widehat{\mathbf{n}}\left( \mathbf{q};\mathcal{P}\right) \end{Vmatrix}}_{2}^{2}\]<p></p><p></p>\[\text{s.t.}\widehat{\mathbf{n}}\left( {\mathbf{q};\mathcal{P}}\right)  = \mathop{\sum }\limits_{{{\mathbf{p}}^{\prime } \in  \mathcal{N}\left( \mathbf{q}\right) }}\mathbf{n}\left( {{\mathbf{p}}^{\prime };\mathcal{P}}\right) /g\left( {\begin{Vmatrix}\mathbf{q} - {\mathbf{p}}^{\prime }\end{Vmatrix}}_{2}\right) \text{,} \tag{9}\]<p></p><p>where \(\mathcal{N}\left( \mathbf{q}\right)\) denotes a local neighborhood of a space point \(\mathbf{q} \in  {\mathbb{R}}^{3}\) ,containing observed points \(\left\{  {{\mathbf{p}}^{\prime } \in  \mathcal{P}}\right\}\) . PSR constrains the normal field only and usually produces over-smooth results. As a remedy, Screened Poisson Surface Reconstruction (SPSR) [36] improves over PSR by incorporating regularized versions of both the first- and second-order terms in Eq. (6). More recently, Shape As Points [37] develops a differentiable Poisson solver in a spectral manner, which enables an end-to-end optimization and thus could be integrated into the learning of deep neural networks.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(\mathcal{N}\left( \mathbf{q}\right)\) 表示空间点 \(\mathbf{q} \in  {\mathbb{R}}^{3}\) 的一个局部邻域，包含观测点 \(\left\{  {{\mathbf{p}}^{\prime } \in  \mathcal{P}}\right\}\) 。PSR 仅对法向场进行约束，通常会产生过度平滑的结果。作为一种改进，筛选泊松表面重建（Screened Poisson Surface Reconstruction，SPSR）[36] 通过将方程（6）中的一阶和二阶项的正则化版本结合起来，对 PSR 进行了改进。最近，“形状即点”（Shape As Points）[37] 以谱方法开发了一种可微的泊松求解器，它实现了端到端的优化，因此可以集成到深度神经网络的学习中。</p></div><p>The second manner achieves surface smoothness by constraining the function complexities of \(\mathbf{f}\) or \(F\) . This can be equivalently achieved by constraining the hypothesis space of \({\mathcal{H}}_{f}\) or \({\mathcal{H}}_{F}\) , e.g.,by constraining \({\mathcal{H}}_{\mathbf{f}}\) as B-spline functions [38],[39],[40] or NURBS [41], [42]. Intuitively, a more complex function is able to fit a surface of complex geometry; but it also tends to be overfitted to the observed \(\mathcal{P}\) ,producing a less smooth surface. For example, using RBFs in [43] means that the approximate function is in the form of low-degree polynomials with an interpolation of many basic functions centered at the observed points. The works [44], [45] are similar to [43] but approximate their respective implicit field functions using different basis functions. More specifically, Kazhdan [44] firstly computes the Fourier coefficients of its implicit field function with the help of Monte-Carlo approximation of Divergence Theorem, and then uses inverse Fourier transform to obtain the implicit function for extraction of iso-surface; a regular grid is needed to perform fast Fourier transform in [44], and instead Manson et al. [45] use wavelets, which provide a localized, multi-resolution representation of the implicit function.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>第二种方式通过限制 \(\mathbf{f}\) 或 \(F\) 的函数复杂度来实现表面平滑。这可以等效地通过限制 \({\mathcal{H}}_{f}\) 或 \({\mathcal{H}}_{F}\) 的假设空间来实现，例如，将 \({\mathcal{H}}_{\mathbf{f}}\) 限制为 B 样条函数 [38]、[39]、[40] 或非均匀有理 B 样条（NURBS）[41]、[42]。直观地说，更复杂的函数能够拟合复杂几何形状的表面；但它也容易对观测到的 \(\mathcal{P}\) 产生过拟合，从而生成不太平滑的表面。例如，文献 [43] 中使用径向基函数（RBFs）意味着近似函数是低次多项式的形式，并对以观测点为中心的许多基函数进行插值。文献 [44]、[45] 与 [43] 类似，但使用不同的基函数来近似各自的隐式场函数。更具体地说，Kazhdan [44] 首先借助散度定理的蒙特卡罗近似计算其隐式场函数的傅里叶系数，然后使用傅里叶逆变换来获得用于提取等值面的隐式函数；文献 [44] 中需要一个规则网格来执行快速傅里叶变换，而 Manson 等人 [45] 则使用小波，小波为隐式函数提供了局部化、多分辨率的表示。</p></div><p>Methods such as Point Set Surfaces (PSS) [46], [47], [48], [49] combine both of the above smoothing strategies. PSS is derived from Moving Least Squares (MLS) [50], [51], [52], [53], whose surface can be defined either by an explicit function with stationary projection operator [46], [47], [50], [51], [48] or by an implicit function [52], [53], [49]. In either case, a weighted combination of spatially-varying low-degree polynomials acts as the most important ingredient to locally approximate the observed points and construct the surfaces. In Simple Point Set Surfaces (SPSS) [48], the authors iteratively project all the given points along the normal directions onto the local reference planes, which are defined by a weighted average of the points to be projected and their neighborhood points; then the local reference plane at each evaluation point would give a local orthogonal coordinate system to compute a local bivariate polynomial approximation to the surface. However, the local reference plane can hardly be a good approximation and sometimes even becomes unstable when the observed points are sparse. Algebraic Point Set Surface (APSS) [49] overcomes this issue by using an algebraic sphere to fit the observed points, which forms an implicit function to represent the algebraic distance between the evaluation point and the fitted sphere; the fitting problem is then solved by a least squares problem between the gradient field of the algebraic sphere and the normals of the observed points. In Robust Implicit MLS (RIMLS) [53], the authors combine kernel regression and statistical robustness with MLS to cope with the limitation that MLS can only reconstruct smooth surfaces. Other PSS methods share the same principle; more details can be found in [54] and [3].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>诸如点集表面（PSS）[46]、[47]、[48]、[49] 等方法结合了上述两种平滑策略。点集表面（PSS）源自移动最小二乘法（MLS）[50]、[51]、[52]、[53]，其表面可以通过具有固定投影算子的显式函数 [46]、[47]、[50]、[51]、[48] 来定义，也可以通过隐式函数 [52]、[53]、[49] 来定义。在这两种情况下，空间变化的低次多项式的加权组合是局部近似观测点并构建表面的最重要因素。在简单点集表面（SPSS）[48] 中，作者将所有给定的点沿着法线方向迭代投影到局部参考平面上，这些局部参考平面由待投影点及其邻域点的加权平均值定义；然后，每个评估点处的局部参考平面将给出一个局部正交坐标系，以计算表面的局部二元多项式近似。然而，当观测点稀疏时，局部参考平面很难成为一个好的近似，有时甚至会变得不稳定。代数点集表面（APSS）[49] 通过使用代数球面来拟合观测点解决了这个问题，该代数球面形成一个隐式函数来表示评估点与拟合球面之间的代数距离；然后通过代数球面的梯度场与观测点法线之间的最小二乘问题来解决拟合问题。在鲁棒隐式移动最小二乘法（RIMLS）[53] 中，作者将核回归和统计鲁棒性与移动最小二乘法（MLS）相结合，以应对移动最小二乘法（MLS）只能重建平滑表面的局限性。其他点集表面（PSS）方法遵循相同的原理；更多细节可参考 [54] 和 [3]。</p></div><h3>3.3 Template-based Priors</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.3 基于模板的先验</h3></div><p>Template-based priors assume that a surface could be represented by combination of a group of templates, where the templates could be geometric primitives such as spheres or cubes, or complex ones from an auxiliary dataset. As such, surface reconstruction boils down as the problem of estimating and fitting the correct templates to the observed \(\mathcal{P}\) . This can be formally written as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于模板的先验假设一个表面可以由一组模板组合表示，其中模板可以是诸如球体或立方体之类的几何基元，也可以是来自辅助数据集的复杂模板。因此，表面重建归结为估计并将正确的模板拟合到观测到的 \(\mathcal{P}\) 的问题。这可以正式表示为</p></div><p></p>\[\mathop{\min }\limits_{{\{ w\} ,\{ \mathbf{\theta }\} }}D\left( {\mathop{\sum }\limits_{{i = 1}}^{\left| \{ \mathcal{M}\} \right| }{w}_{i}{\mathcal{M}}_{i}\left( {\mathbf{\theta }}_{i}\right) ,\mathcal{P}}\right) , \tag{10}\]<p></p><p>where \(\{ \mathcal{M}\}\) denotes the set of predefined templates,and each template \(\mathcal{M}\) is parameterized by (the possibly learnable) \(\mathbf{\theta }\) and weighted by \(w;D\left( {\cdot , \cdot  }\right)\) denotes a proper distance between the formed surface and the observed \(\mathcal{P}\) . By minimizing the fitting error (10),a surface is reconstructed by the determined \(\{ w\}\) and/or \(\{ \mathbf{\theta }\}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(\{ \mathcal{M}\}\) 表示预定义模板的集合，每个模板 \(\mathcal{M}\) 由（可能可学习的）\(\mathbf{\theta }\) 参数化，并由 \(w;D\left( {\cdot , \cdot  }\right)\) 加权，\(w;D\left( {\cdot , \cdot  }\right)\) 表示形成的表面与观测到的 \(\mathcal{P}\) 之间的适当距离。通过最小化拟合误差 (10)，由确定的 \(\{ w\}\) 和/或 \(\{ \mathbf{\theta }\}\) 重建一个表面。</p></div><h4>3.3.1 Geometric Primitives</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.3.1 几何基元</h4></div><p>Templates of geometric primitives are simple shapes that can be analytically represented by a certain number of parameters, e.g. , cuboids, spheres, cylinders, cones, etc. Random sample consensus (RANSAC) [55], [56] is the most commonly used method to solve Eq. (10) that fits the right templates to the observed \(\mathcal{P}\) ,where \(D\left( {\cdot , \cdot  }\right)\) sums up the element-wise (Euclidean) distances between randomly sampled \(\{ \mathbf{p} \in  \mathcal{P}\}\) and their projections onto the fitted templates. More specifically, Schnabel et al. [55] introduce an efficient RANSAC-based algorithm with a novel sampling strategy and an efficient score evaluation scheme, where the primitive with maximum score is extracted iteratively. Nan and Wonka [56] propose to only extract planar primitives based on RANSAC, obtaining the final surface by minimizing a weighted sum of several energy terms to select the optimal set of faces.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>几何原始体的模板是可以通过一定数量的参数进行解析表示的简单形状，例如，长方体、球体、圆柱体、锥体等。随机样本一致性（RANSAC）[55]，[56] 是解决方程（10）以将正确的模板拟合到观察到的 \(\mathcal{P}\) 的最常用方法，其中 \(D\left( {\cdot , \cdot  }\right)\) 汇总了随机采样的 \(\{ \mathbf{p} \in  \mathcal{P}\}\) 与其在拟合模板上的投影之间的逐元素（欧几里得）距离。更具体地说，Schnabel 等人 [55] 引入了一种基于 RANSAC 的高效算法，采用新颖的采样策略和高效的评分评估方案，其中具有最大评分的原始体被迭代提取。Nan 和 Wonka [56] 提出仅基于 RANSAC 提取平面原始体，通过最小化多个能量项的加权和来选择最佳面，从而获得最终表面。</p></div><h4>3.3.2 Retrieval-based Templates</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.3.2 基于检索的模板</h4></div><p>Given an auxiliary set \(\{ \mathcal{M}\}\) of shape models,retrieving-and-deforming methods solve Eq. (10) to find the closest shapes and deform them,via optimization of model parameters \(\{ \mathbf{\theta }\}\) ,to fit the observed \(\mathcal{P}\) . For example,in scene reconstruction [57],[58], [59], [60], the observed scene points are segmented into semantic classes, each of which is then fit with a retrieved shape model followed by rigid or non-rigid deformation; in object reconstruction, Pauly et al. [61] warp and blend multiple retrieved object shapes to conform with the observed points, and Shen et al. [62] retrieve individual object parts to form a surface by part assembly.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>给定一组辅助形状模型 \(\{ \mathcal{M}\}\)，检索和变形方法通过优化模型参数 \(\{ \mathbf{\theta }\}\) 来解决方程（10），以找到最接近的形状并对其进行变形，以适应观察到的 \(\mathcal{P}\)。例如，在场景重建 [57]，[58]，[59]，[60] 中，观察到的场景点被分割成语义类别，每个类别随后与检索到的形状模型进行拟合，然后进行刚性或非刚性变形；在物体重建中，Pauly 等人 [61] 扭曲并混合多个检索到的物体形状以符合观察到的点，而 Shen 等人 [62] 则检索单个物体部件，通过部件组装形成表面。</p></div><h3>3.4 Modeling Priors</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.4 建模先验</h3></div><p>While constraining the complexity of hypothesis space of \({\mathcal{H}}_{\mathbf{f}}\) or \({\mathcal{H}}_{F}\) would promote reconstruction of smoother surfaces,as discussed in Section 3.2,the choice of \({\mathcal{H}}_{f}\) or \({\mathcal{H}}_{F}\) itself regularizes the reconstruction given that only specific types of surface can be modeled by the choice. A prominent example is the recent trend of using deep neural networks for geometric modeling and surface reconstruction. We term the geometric priors provided by the respective designs of models themselves as modeling priors.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>虽然限制 \({\mathcal{H}}_{\mathbf{f}}\) 或 \({\mathcal{H}}_{F}\) 的假设空间复杂性会促进更平滑表面的重建，如第 3.2 节所讨论的，但 \({\mathcal{H}}_{f}\) 或 \({\mathcal{H}}_{F}\) 的选择本身会对重建进行正则化，因为只有特定类型的表面可以通过该选择进行建模。一个显著的例子是最近使用深度神经网络进行几何建模和表面重建的趋势。我们将模型自身设计所提供的几何先验称为建模先验。</p></div><p>Motivated from deep image prior [63], Deep Geometric Prior (DGP) [64] verifies the efficacy of deep networks as a prior for geometric surface modeling, even when the networks are not trained. Latter on, Point2Mesh [65] and SAIL-S3 [66] extend the global modeling adopted in [64] as local ones, where the former constructs its local, implicit functions as a weight-shared MeshCNN [67] and the later method constructs them as a weight-shared Multi-Layer Perceptron (MLP). Deep Manifold Prior [68] delivers mathematical analyses on such modeling properties for MLP as well as convolutional networks. There have also been a few works making use of modeling priors while not explicitly mentioning it. Atzmon et al. [69] theoretically prove that MLP with Rectified Linear Units (ReLUs) generate piecewise linear surfaces, and a meshing algorithm of Analytic Marching is also proposed in [70] that is able to analytically compute the piecewise linear surface mesh from such a network, both of which deliver mathematical analyses on how the structure of MLP itself acts as a regularizer. Later on, Deep Manifold Prior [68] extends such modeling properties for both MLP and convolutional networks. Implicit Geometric Regularization (IGR) [71] shows that additional regularization on the gradient of neural network would further encourage the generated surfaces to be smooth. Sign Agnostic Learning (SAL) [72] and its variants [73], [74] study reconstructing a surface from an un-oriented point cloud via a specially initialized neural network, and Davies et al. [75] adopt the same strategy for surface reconstruction from an oriented point cloud. Neural Splines [76] performs reconstruction based on random feature kernels arising from infinitely-wide shallow ReLU networks and shows that such solutions bias toward reconstruction of smooth surface.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>受深度图像先验 [63] 的启发，深度几何先验（DGP）[64] 验证了深度网络作为几何表面建模先验的有效性，即使这些网络没有经过训练。随后，Point2Mesh [65] 和 SAIL-S3 [66] 将 [64] 中采用的全局建模扩展为局部建模，其中前者将其局部隐式函数构建为权重共享的 MeshCNN [67]，而后者则将其构建为权重共享的多层感知器（MLP）。深度流形先验 [68] 对 MLP 以及卷积网络的这种建模特性进行了数学分析。还有一些工作利用建模先验，但没有明确提及。Atzmon 等人 [69] 理论证明了具有修正线性单元（ReLUs）的 MLP 生成分段线性表面，并且在 [70] 中还提出了一种解析行进算法，能够从这样的网络中解析计算分段线性表面网格，这两者都对 MLP 本身的结构如何作为正则化器进行了数学分析。随后，深度流形先验 [68] 扩展了 MLP 和卷积网络的这种建模特性。隐式几何正则化（IGR）[71] 表明对神经网络梯度的额外正则化将进一步鼓励生成的表面光滑。无符号学习（SAL）[72] 及其变体 [73]，[74] 研究通过特别初始化的神经网络从无定向点云重建表面，Davies 等人 [75] 采用相同策略从有定向点云进行表面重建。神经样条 [76] 基于来自无限宽浅层 ReLU 网络的随机特征核进行重建，并表明这样的解决方案偏向于重建光滑表面。</p></div><h3>3.5 Learning-based Priors</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.5 基于学习的先验</h3></div><p>Given the parametrization of \({\mathbf{\theta }}_{\mathbf{f}}\) for \(\mathbf{f} \in  {\mathcal{H}}_{\mathbf{f}}\) and \({\mathbf{\theta }}_{F}\) for \(F \in  {\mathcal{H}}_{F}\) , priors can be learned from an auxiliary set of training shapes as optimized model parameters. Note that such learning-based priors are different from modeling priors presented in the preceding section, where priors are provided by the models themselves. Let \({\left\{  {\mathcal{P}}_{i},{\mathcal{S}}_{i}^{ * }\right\}  }_{i = 1}^{N}\) be the training set containing \(N\) pairs of observed point sets and their corresponding ground-truth surfaces. By adapting the objective (2), an explicit surface reconstruction based on a learned prior can be generally written as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>给定 \({\mathbf{\theta }}_{\mathbf{f}}\) 关于 \(\mathbf{f} \in  {\mathcal{H}}_{\mathbf{f}}\) 的参数化以及 \({\mathbf{\theta }}_{F}\) 关于 \(F \in  {\mathcal{H}}_{F}\) 的参数化，可以从一组辅助训练形状中学习先验信息，并将其作为优化后的模型参数。请注意，这种基于学习的先验信息与上一节中介绍的建模先验信息不同，上一节中的先验信息由模型本身提供。设 \({\left\{  {\mathcal{P}}_{i},{\mathcal{S}}_{i}^{ * }\right\}  }_{i = 1}^{N}\) 为包含 \(N\) 对观测点集及其对应真实表面的训练集。通过调整目标函数 (2)，基于学习到的先验信息进行的显式表面重建通常可以写成如下形式</p></div><p></p>\[\mathop{\min }\limits_{\mathbf{z}}{L}^{\exp }\left( {\mathbf{f}\left( {\mathbf{z};{\widetilde{\mathbf{\theta }}}_{\mathbf{f}}}\right) ;\mathcal{P}}\right)  + \lambda {R}^{\exp }\left( \mathbf{z}\right)  \tag{11}\]<p></p><p></p>\[\text{s.t.}{\widetilde{\mathbf{\theta }}}_{\mathbf{f}} = \arg \mathop{\min }\limits_{{\mathbf{\theta }}_{\mathbf{f}}}\frac{1}{N}\mathop{\sum }\limits_{{i = 1}}^{N}{\widetilde{L}}^{\exp }\left( {{\mathbf{\theta }}_{\mathbf{f}};\left\{  {{\mathcal{P}}_{i},{\mathcal{S}}_{i}^{ * }}\right\}  }\right)  + \widetilde{\lambda }{\widetilde{R}}^{\exp }\left( {\mathbf{\theta }}_{\mathbf{f}}\right) \text{,}\]<p></p><p>(12)</p><p>where the constraint (12) learns the prior as the optimized model parameter \({\widetilde{\mathbf{\theta }}}_{\mathbf{f}},{\widetilde{L}}^{\exp }\) could be different from \({L}^{\exp }\) ,and an objective for smooth and/or fair surface may also be incorporated into \({\widetilde{R}}^{\exp }\) in addition to a simple norm constraint of \({\mathbf{\theta }}_{\mathbf{f}}\) ; given the learned and then fixed \({\widetilde{\mathbf{\theta }}}_{\mathbf{f}}\) ,the objective (11) fits the model prediction to any observed \(\mathcal{P}\) by optimizing a latent code \(\mathbf{z}\) ,where norm of \(z\) is usually penalized to prevent overfitting. Learning a prior for implicit surface reconstruction can be similarly written as follows, by adapting the objective (3)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中，约束条件 (12) 将先验信息作为优化后的模型参数 \({\widetilde{\mathbf{\theta }}}_{\mathbf{f}},{\widetilde{L}}^{\exp }\) 进行学习，\({\widetilde{\mathbf{\theta }}}_{\mathbf{f}},{\widetilde{L}}^{\exp }\) 可能与 \({L}^{\exp }\) 不同，并且除了 \({\mathbf{\theta }}_{\mathbf{f}}\) 的简单范数约束外，还可以将平滑和/或美观表面的目标纳入 \({\widetilde{R}}^{\exp }\) 中；给定学习到并固定的 \({\widetilde{\mathbf{\theta }}}_{\mathbf{f}}\)，目标函数 (11) 通过优化潜在代码 \(\mathbf{z}\) 使模型预测与任何观测到的 \(\mathcal{P}\) 相匹配，其中 \(z\) 的范数通常会受到惩罚以防止过拟合。通过调整目标函数 (3)，隐式表面重建的先验信息学习可以类似地写成如下形式</p></div><p></p>\[\mathop{\min }\limits_{\mathbf{z}}{L}^{\mathrm{{imp}}}\left( {F\left( {\mathbf{z};{\widetilde{\mathbf{\theta }}}_{F}}\right) ;\mathcal{P}}\right)  + \lambda {R}^{\mathrm{{imp}}}\left( \mathbf{z}\right)  \tag{13}\]<p></p><p></p>\[\text{s.t.}F\left( {\mathbf{q},\mathbf{z};{\widetilde{\mathbf{\theta }}}_{F}}\right)  = 0\forall \mathbf{q} \in  \mathcal{S}\;\text{and}\]<p></p><p></p>\[{\widetilde{\mathbf{\theta }}}_{F} = \arg \mathop{\min }\limits_{{\mathbf{\theta }}_{F}}\frac{1}{N}\mathop{\sum }\limits_{{i = 1}}^{N}{\widetilde{L}}^{\mathrm{{imp}}}\left( {{\mathbf{\theta }}_{F};\left\{  {{\mathcal{P}}_{i},{\mathcal{S}}_{i}^{ * }}\right\}  }\right)  + \widetilde{\lambda }{\widetilde{R}}^{\mathrm{{imp}}}\left( {\mathbf{\theta }}_{F}\right) . \tag{14}\]<p></p><p>The model \(\mathbf{f}\) can also be constructed as an auto-encoder architecture [9],[77],[78],i.e., \(\mathbf{f} = {\mathbf{f}}_{\text{encoder }} \circ  {\mathbf{f}}_{\text{decoder }}\) . In such a case,given the prior \({\widetilde{\mathbf{\theta }}}_{\mathbf{f}} = \left\{  {{\widetilde{\mathbf{\theta }}}_{{\mathbf{f}}_{\text{encoder }}},{\widetilde{\mathbf{\theta }}}_{{\mathbf{f}}_{\text{decoder }}}}\right\}\) already learned by Eq. (12),a latent code can be directly obtained as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>模型 \(\mathbf{f}\) 也可以构建为自编码器架构 [9]、[77]、[78]，即 \(\mathbf{f} = {\mathbf{f}}_{\text{encoder }} \circ  {\mathbf{f}}_{\text{decoder }}\)。在这种情况下，给定已经通过公式 (12) 学习到的先验信息 \({\widetilde{\mathbf{\theta }}}_{\mathbf{f}} = \left\{  {{\widetilde{\mathbf{\theta }}}_{{\mathbf{f}}_{\text{encoder }}},{\widetilde{\mathbf{\theta }}}_{{\mathbf{f}}_{\text{decoder }}}}\right\}\)，可以直接得到潜在代码为</p></div><p></p>\[\mathbf{z} = {\mathbf{f}}_{\text{encoder }}\left( {\mathcal{P};{\widetilde{\mathbf{\theta }}}_{{\mathbf{f}}_{\text{encoder }}}}\right) , \tag{15}\]<p></p><p>instead of optimizing the objective (11). The above applies to an auto-encoder based implicit function \(F = {F}_{\text{encoder }} \circ  {F}_{\text{decoder }}\) as well, and given the learned prior \({\widetilde{\mathbf{\theta }}}_{F} = \left\{  {{\widetilde{\mathbf{\theta }}}_{{F}_{\text{encoder }}},{\widetilde{\mathbf{\theta }}}_{{F}_{\text{decoder }}}}\right\}\) ,one can compute its latent code directly as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>而不是优化目标函数 (11)。上述内容同样适用于基于自编码器的隐式函数 \(F = {F}_{\text{encoder }} \circ  {F}_{\text{decoder }}\)，并且给定学习到的先验信息 \({\widetilde{\mathbf{\theta }}}_{F} = \left\{  {{\widetilde{\mathbf{\theta }}}_{{F}_{\text{encoder }}},{\widetilde{\mathbf{\theta }}}_{{F}_{\text{decoder }}}}\right\}\)，可以直接计算其潜在代码为</p></div><p></p>\[\mathbf{z} = {F}_{\text{encoder }}\left( {\mathcal{P};{\widetilde{\mathbf{\theta }}}_{{F}_{\text{encoder }}}}\right) . \tag{16}\]<p></p><p>Note that the most recent implicit methods [79], [80] allow the priors of model parameters to be further optimized when fitting to the observed points, i.e., optimizing over both the latent code and model parameters in Eq. (11) or Eq. (13), and thus potentially bridge the gap between the learning-based priors and classical ones.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>请注意，最新的隐式方法 [79]、[80] 允许在拟合观测点时进一步优化模型参数的先验信息，即在公式 (11) 或公式 (13) 中同时对潜在代码和模型参数进行优化，从而有可能缩小基于学习的先验信息与经典先验信息之间的差距。</p></div><p>Depending on how the training shapes in \({\left\{  {\mathcal{P}}_{i},{\mathcal{S}}_{i}^{ * }\right\}  }_{i = 1}^{N}\) are organized, the priors learned via Eq. (12) or Eq. (14) may be either at a global, semantic level or as local, shape primitives. For example,when pairs in \({\left\{  {\mathcal{P}}_{i},{\mathcal{S}}_{i}^{ * }\right\}  }_{i = 1}^{N}\) capture surfaces of object instances belonging to a semantic category, the learned priors would encode shape patterns common to this object category. To learn priors for reconstruction of arbitrary surface shapes, one may have to prepare \({\left\{  {\mathcal{P}}_{i},{\mathcal{S}}_{i}^{ * }\right\}  }_{i = 1}^{N}\) as those encoding surface patches, and expect a global surface of arbitrary shape can be better reconstructed by providing priors on its local shape primitives.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>根据\({\left\{  {\mathcal{P}}_{i},{\mathcal{S}}_{i}^{ * }\right\}  }_{i = 1}^{N}\)中训练形状的组织方式，通过公式(12)或公式(14)学习到的先验可以是全局的、语义层面的，也可以是局部的、形状原语。例如，当\({\left\{  {\mathcal{P}}_{i},{\mathcal{S}}_{i}^{ * }\right\}  }_{i = 1}^{N}\)中的配对捕捉到属于某一语义类别的物体实例的表面时，学习到的先验将编码该物体类别的形状模式。为了学习任意表面形状重建的先验，可能需要将\({\left\{  {\mathcal{P}}_{i},{\mathcal{S}}_{i}^{ * }\right\}  }_{i = 1}^{N}\)准备为编码表面补丁的形式，并期望通过提供其局部形状原语的先验，可以更好地重建任意形状的全局表面。</p></div><h4>3.5.1 Learning Semantic Priors</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.5.1 学习语义先验</h4></div><p>The recent trend of deep learning surface reconstruction starts from learning deep priors at the semantic, object-level. For example, an explicit method of Deep Marching Cubes [81] proposes a novel, differentiable layer of marching cubes, which connects mesh surface generation with the learning of semantic prior via a shape encoding network that generates displaced voxel grids. The seminal deep learning-based implicit methods [9], [82], [8] model either SDF or OF via deep neural networks. Specifically, OccNet [9] and IM-Net [82] adopt an auto-encoder structure; after training, the encoder generates the latent code representing the shape via a single forward propagation in Eq. (16) for any observed point set, and the decoder predicts the probability of occupancy according to the given space point along with the latent code. Different from OccNet and IM-Net, DeepSDF [8] adopts the structure of decoder-only model, which optimizes the latent code of the given point clouds via maximum a posteriori in Eq. (13), and predicts signed distances according to the given space point along with the latent code. Curriculum DeepSDF [83] improves DeepSDF by introducing a progressive learning strategy to learn local details; in the meanwhile, Yao et al. [84] implement such learning using Graph Neural Networks, which converts the global, semantic latent code into more sophisticated local ones, before feeding into the decoder. MeshUDF [85] further extends DeepSDF to reconstruct open surfaces, which predicts unsigned distances of any given space point and generates the surface using their customized marching cubes. More recently, a few methods [86], [87] adopt networks based on transformer [88], [89] to help reconstruction. Note also that learning semantic priors enables reconstruction of surfaces from raw observations that are originally of no or less 3D shape information (e.g., as few as a single RGB image [5], [90], [91]), by training encoders that learn latent shape spaces from such observations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>深度学习表面重建的最新趋势始于在语义、物体层面学习深度先验。例如，Deep Marching Cubes [81] 的一种显式方法提出了一种新颖的、可微分的marching cubes层，将网格表面生成与通过形状编码网络学习语义先验连接起来，该网络生成位移的体素网格。开创性的基于深度学习的隐式方法 [9]、[82]、[8] 通过深度神经网络建模SDF或OF。具体而言，OccNet [9] 和IM-Net [82] 采用自编码器结构；经过训练后，编码器通过公式(16)对任何观察到的点集生成表示形状的潜在编码，解码器根据给定空间点及潜在编码预测占用概率。与OccNet和IM-Net不同，DeepSDF [8] 采用仅解码器模型的结构，通过公式(13)最大后验优化给定点云的潜在编码，并根据给定空间点及潜在编码预测有符号距离。课程DeepSDF [83] 通过引入渐进学习策略来学习局部细节，从而改进DeepSDF；与此同时，Yao等 [84] 使用图神经网络实现这种学习，将全局语义潜在编码转换为更复杂的局部编码，然后输入解码器。MeshUDF [85] 进一步扩展DeepSDF以重建开放表面，预测任何给定空间点的无符号距离，并使用其定制的marching cubes生成表面。最近，一些方法 [86]、[87] 采用基于变换器的网络 [88]、[89] 来帮助重建。还要注意，学习语义先验使得能够从原本没有或较少3D形状信息的原始观察中重建表面（例如，只有一张RGB图像 [5]、[90]、[91]），通过训练编码器从这些观察中学习潜在形状空间。</p></div><h4>3.5.2 Learning Priors as Local, Shape Primitives</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.5.2 将先验学习为局部形状原语</h4></div><p>To reconstruct a scene surface or surface of an object that cannot be semantically categorized, existing methods resort to modeling and learning local priors of shape primitives either at regular grids that partition the 3D space [92], [77], [93], [94], [10], [95], [96], or on local patches along the surface manifold [97], [98], [78]. Among the former methods, Implicit Feature Networks (IF-Net) [92] and Convolutional OccNet [77] (ConvOccNet) obtain latent codes for local grids via auto-encoder structure, where IF-Net [92] adopts 3D convolution that convolves each input point with its surrounding points to get the latent code at each local grid, and ConvOccNet [77] uses PointNet [99] as its encoder to get the latent code for each point and then encapsulates all the latent codes into a volumetric feature via average pooling. Later on, Chibane et al. [93] extend IF-Net [92] to support sign-agnostic learning. Deep Local Shape (DeepLS) [94] divides the whole 3D space into regular voxels, and trains an implicit function whose parameters are shared among different local voxels. Local Implicit Grid (LIG) [10] trains an auto-encoder to extract the latent codes of local voxels during training (via Eq. (16)), while retaining the voxel-shared decoder with fixed parameters during inference only. To improve the efficiency of local encoding, Scalable Surface Reconstruction Network (SSRNet) [95] makes use of octree to partition the whole 3D space, and uses fully-convolutional U-shaped network with skip connections, which is based on modified tangent convolution [100] to get the latent code for each local grid. Neural Geometric Level of Detail (LOD) [96] adopts the structure of sparse octree to further improve the efficiency, where latent codes are computed only for local voxels that intersect with the surface. Ummenhofer et al. [101] aggregate latent codes for local grids via adaptive grid convolution on multiple levels of the octree input space. As for the methods based on local surface patches, Badki et al. [97] introduce the concept of meshlets, and train a variational auto-encoder to learn the latent space of pose-disentangled meshlets; by back-propagating the error with respect to the given points and the meshlets, the method updates the meshlets' latent codes and deforms the meshlets to fit the given points at inference time. PatchNets [98] leverages the structure of implicit auto-decoder to learn across different local surface patches, and the decoder is also trained with an elaborate loss function to ensure the smoothness of the reconstructed patches. Points2Surf [78] learns features from both local patches and the global surface, and reconstructs the surface with an implicit decoder, where the former takes a local encoder to learn the absolute distance of a queried point from the local surfaces, and the latter learns the interior/exterior of the surface with a global encoder. POCO [102] and [103] encodes each input point into a single latent code, and then perform weighted interpolation among a point and its neighboring ones to get the local latent code.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了重建一个无法语义分类的场景表面或物体表面，现有方法依赖于在划分3D空间的规则网格[92]、[77]、[93]、[94]、[10]、[95]、[96]或沿表面流形的局部补丁[97]、[98]、[78]上建模和学习形状原语的局部先验。在前者方法中，隐式特征网络（IF-Net）[92]和卷积OccNet [77]（ConvOccNet）通过自编码器结构为局部网格获取潜在编码，其中IF-Net [92]采用3D卷积，将每个输入点与其周围点进行卷积，以获取每个局部网格的潜在编码，而ConvOccNet [77]使用PointNet [99]作为其编码器，为每个点获取潜在编码，然后通过平均池化将所有潜在编码封装成体积特征。随后，Chibane等[93]扩展IF-Net [92]以支持符号无关学习。深局部形状（DeepLS）[94]将整个3D空间划分为规则体素，并训练一个隐式函数，其参数在不同局部体素之间共享。局部隐式网格（LIG）[10]训练一个自编码器，在训练过程中提取局部体素的潜在编码（通过公式（16）），同时在推理时仅保留具有固定参数的体素共享解码器。为了提高局部编码的效率，可扩展表面重建网络（SSRNet）[95]利用八叉树划分整个3D空间，并使用基于修改的切线卷积[100]的全卷积U型网络与跳跃连接来获取每个局部网格的潜在编码。神经几何细节层次（LOD）[96]采用稀疏八叉树的结构进一步提高效率，其中仅为与表面相交的局部体素计算潜在编码。Ummenhofer等[101]通过在八叉树输入空间的多个层次上进行自适应网格卷积来聚合局部网格的潜在编码。至于基于局部表面补丁的方法，Badki等[97]引入了网格块的概念，并训练一个变分自编码器来学习姿态解耦网格块的潜在空间；通过对给定点和网格块的误差进行反向传播，该方法更新网格块的潜在编码，并在推理时变形网格块以适应给定点。PatchNets [98]利用隐式自解码器的结构在不同局部表面补丁之间进行学习，解码器也通过精心设计的损失函数进行训练，以确保重建补丁的平滑性。Points2Surf [78]从局部补丁和全局表面学习特征，并通过隐式解码器重建表面，其中前者使用局部编码器学习查询点与局部表面的绝对距离，后者使用全局编码器学习表面的内部/外部。POCO [102]和[103]将每个输入点编码为一个单一的潜在编码，然后在一个点及其邻近点之间执行加权插值以获取局部潜在编码。</p></div><h3>3.6 Hybrid Priors</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.6 混合先验</h3></div><p>Methods discussed in the preceding sections are organized according to their respectively used, main priors of surface geometry. To improve the plausibility of reconstructed surfaces, existing methods usually combine multiple priors, e.g., by combining smoothness priors with triangulation or template-based ones [104], [105], [106], by imposing additional priors on top of modeling ones [107], [108], [109], [110], [111], [112], [113], or by learning priors to improve over regularization provided by pre-defined ones [6], [7], [114], [115], [116], [40]. In this section, we focus our discussion on the last case, considering that such hybrid priors are popularly used in the new era of deep learning surface reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>前面讨论的方法根据其各自使用的主要表面几何先验进行组织。为了提高重建表面的可信度，现有方法通常结合多个先验，例如，通过将平滑性先验与三角剖分或基于模板的先验结合[104]、[105]、[106]，通过在建模先验之上施加额外的先验[107]、[108]、[109]、[110]、[111]、[112]、[113]，或通过学习先验以改善预定义先验提供的正则化[6]、[7]、[114]、[115]、[116]、[40]。在本节中，我们将讨论最后一种情况，考虑到这种混合先验在深度学习表面重建的新时期被广泛使用。</p></div><p>Given auxiliary sets of training shapes, learning-based priors are combined with triangulation-based prior in [7], [117], [114] that encourage deep networks to learn particular properties for triangulation from the observed point clouds. More specifically, PointTriNet [7] introduces a framework to generate triangles from observed point clouds directly, where a proposal network suggests candidates of triangles and a classification network predicts whether a proposed candidate should appear in the reconstructed surface or not, and two networks iteratively take effects until the final surface of triangular mesh is generated. Liu et al. [117] show that connecting those pairs of vertices whose geodesic distance approaches to their Euclidean distance can approximately reconstruct the underlying surface; they construct a network to select candidate triangles satisfying this property. Delaunay Surface Elements (DSE) [114] introduces small triangulated patches generated by combining Delaunay triangulation with learned logarithmic maps, from which candidate triangles of the output surface are then iteratively selected via their proposed adaptive voting algorithm. DeepDT [6] learns deep networks to extract geometric features from observed point clouds, and then integrates the learned features together with graph structural information to vote for the inside/outside labels of Delaunay tetrahedrons; the surface is reconstructed by extracting triangular faces between tetrahedrons of different labels. Gao et al. [118] learn a network to predict the offsets of vertices of tetrahedrons and a second network to predict the occupancy of each tetrahedron, and object surface is generated on the facet that belongs to two tetrahedrons with different occupancies.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>给定辅助的训练形状集，基于学习的先验与基于三角剖分的先验在[7]、[117]、[114]中结合，鼓励深度网络从观察到的点云中学习三角剖分的特定属性。更具体地说，PointTriNet [7] 引入了一个框架，直接从观察到的点云生成三角形，其中提议网络建议三角形候选，分类网络预测提议的候选是否应出现在重建的表面上，两个网络迭代作用，直到生成最终的三角网格表面。Liu等人[117]表明，连接那些测地距离接近其欧几里得距离的顶点对可以近似重建基础表面；他们构建了一个网络来选择满足该属性的候选三角形。Delaunay表面元素（DSE）[114]引入了通过将Delaunay三角剖分与学习的对数映射结合生成的小三角剖分补丁，从中通过他们提出的自适应投票算法迭代选择输出表面的候选三角形。DeepDT [6]学习深度网络从观察到的点云中提取几何特征，然后将学习到的特征与图结构信息结合起来，为Delaunay四面体的内外标签投票；通过提取不同标签的四面体之间的三角面来重建表面。Gao等人[118]学习一个网络来预测四面体顶点的偏移量，第二个网络预测每个四面体的占用情况，物体表面在属于两个具有不同占用情况的四面体的面上生成。</p></div><p>Learning-based priors are commonly combined with smoothness priors (e.g., the Laplacian regularization in [119] and the cosine smoothness in [120]). IMLSNet [115] trains an implicit network that evaluates signed distances at grids of an octree-based 3D space, where smoothness is regularized by defining the signed distances in a way similar to implicit moving least-squares [53]. Xiao et al. [121] learn a network to implicitly model an indicator function derived from Gauss lemma, which is smooth and linearly approximates the surface; similar to Points2Surf [78], they learn both local and global features.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于学习的先验通常与平滑性先验结合（例如，[119]中的拉普拉斯正则化和[120]中的余弦平滑性）。IMLSNet [115]训练一个隐式网络，在基于八叉树的3D空间的网格上评估有符号距离，其中通过以类似于隐式移动最小二乘法[53]的方式定义有符号距离来正则化平滑性。Xiao等人[121]学习一个网络，隐式建模从高斯引理导出的指示函数，该函数是平滑的并线性近似表面；类似于Points2Surf [78]，他们学习局部和全局特征。</p></div><p>There have also been plenty of works [116], [40], [122] combining learning-based priors with template-based ones. Li et al. [116] propose to train a deep network to fit geometric primitives according to the observed point clouds; the trained network predicts point-wise features that are fed into a differentiable estimator for algebraic computation of primitive parameters. ParseNet [40] uses a neural decomposition module to partition an observed point cloud into multiple subsets, each of which is assumed to be a primitive type modeled as an open or close B-spline by a deep network. Local Deep Implicit Function (LDIF) [122] represents a surface shape as a set of shape elements, each of which is parameterized by analytic shape variables and latent shape codes; the method learns such variables and latent codes by its SIF encoder and PointNet encoder respectively. For reconstruction of a large-scale scene surface, RetrievalFuse [123] retrieves a set of object templates from an auxiliary scene dataset, and learns a network with attention-based refinement to produce the reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>还有许多工作[116]、[40]、[122]将基于学习的先验与基于模板的先验结合。Li等人[116]提出训练一个深度网络，根据观察到的点云拟合几何原语；训练后的网络预测逐点特征，这些特征被输入到一个可微分估计器中，用于原语参数的代数计算。ParseNet [40]使用神经分解模块将观察到的点云划分为多个子集，每个子集被假定为由深度网络建模的开或闭B样条的原语类型。局部深度隐式函数（LDIF）[122]将表面形状表示为一组形状元素，每个元素由解析形状变量和潜在形状编码参数化；该方法通过其SIF编码器和PointNet编码器分别学习这些变量和潜在编码。为了重建大规模场景表面，RetrievalFuse [123]从辅助场景数据集中检索一组对象模板，并学习一个基于注意力的精炼网络以生成重建。</p></div><h2>4 A Surface Reconstruction Benchmark</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>4 表面重建基准</h2></div><p>As discussed in Section 3, a rich set of methods exist that aim to address the ill-posed problem of surface reconstruction from point observations. These methods have their respective merits, yet it is less clear on their advatanges/disandvatanges under different working conditions, due to the lack of a comprehensive surface reconstruction benchmark that identifies and includes the main challenges faced by the studied problem. In this work, we contribute such a benchmark by both synthesizing and practically scanning point clouds of object and scene surfaces. We identify the main challenges of surface reconstruction from point clouds obtained by imperfect surface scanning, including point-wise noise, point outliers, non-uniform distribution of points, misalignment among point sets obtained by scanning different but overlapped, partial surfaces of an object or scene, and missing points of one or several surface patches. An illustration of such challenges is given in Fig. 1. We include all the five challenges in synthetic data of the benchmark, and expect that an arbitrary combination of these challenges may appear in any sample of the real-scanned data. The benchmark is organized as the synthetic data of object surfaces, the synthetic data of scene surfaces, and the data of real-scanned surfaces. Tables 3 and 4 summarize the statistics. We make the benchmark publicly accessible at <a href="https://Gorilla-Lab-SCUT.github.io/">https://Gorilla-Lab-SCUT.github.io/</a> SurfaceReconstructionBenchmark, where we also release the code implementation of our synthetic scanning pipeline to facilitate future research in the community.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>如第3节所讨论，存在丰富的方法旨在解决从点观测中重建表面的病态问题。这些方法各有其优点，但在不同工作条件下的优缺点尚不清楚，因为缺乏一个全面的表面重建基准，该基准识别并包含所研究问题面临的主要挑战。在这项工作中，我们通过合成和实际扫描对象和场景表面的点云贡献了这样一个基准。我们识别出从通过不完美表面扫描获得的点云进行表面重建的主要挑战，包括逐点噪声、点异常值、点的非均匀分布、通过扫描不同但重叠的部分表面获得的点集之间的错位，以及一个或多个表面补丁的缺失点。图1给出了这些挑战的示例。我们在基准的合成数据中包含所有五个挑战，并期望这些挑战的任意组合可能出现在任何真实扫描数据的样本中。基准组织为对象表面的合成数据、场景表面的合成数据和真实扫描表面数据。表3和表4总结了统计数据。我们在<a href="https://Gorilla-Lab-SCUT.github.io/SurfaceReconstructionBenchmark%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E8%AF%A5%E5%9F%BA%E5%87%86%EF%BC%8C%E5%90%8C%E6%97%B6%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E5%90%88%E6%88%90%E6%89%AB%E6%8F%8F%E7%AE%A1%E9%81%93%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E7%A4%BE%E5%8C%BA%E6%9C%AA%E6%9D%A5%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://Gorilla-Lab-SCUT.github.io/SurfaceReconstructionBenchmark上公开访问该基准，同时发布我们的合成扫描管道的代码实现，以促进社区未来的研究。</a></p></div><h3>4.1 The Synthetic Data of Object Surfaces</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.1 对象表面的合成数据</h3></div><p>To prepare synthetic data of object instances in the benchmark, we collect CAD models from existing repositories [21], [19], [20], [22], and pre-process them before synthetically scanning their point clouds. For each CAD model, we simulate the aforementioned five ways of imperfect scanning, in addition to a perfect scanning that gives a clean object point cloud; we in total have six kinds of scanned point clouds for each instance. We organize the collected object instances into three levels of (algebraic) surface complexity [124], in order to study how different methods perform on surface reconstruction of varying complexities.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了准备基准中的物体实例合成数据，我们从现有的库中收集CAD模型[21]，[19]，[20]，[22]，并在合成扫描其点云之前对其进行预处理。对于每个CAD模型，我们模拟上述五种不完美扫描方式，此外还有一种完美扫描，生成干净的物体点云；我们总共有六种扫描点云用于每个实例。我们将收集到的物体实例组织为三种（代数）表面复杂度[124]，以研究不同方法在不同复杂度的表面重建中的表现。</p></div><h4>4.1.1 Data Collection</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.1.1 数据收集</h4></div><p>We collect CAD models of object instances from the existing repositories of Thingi10k [21], ABC [20], 3DNet Cat200 subset [19], and Three D Scans [22]. More specifically, we randomly select 3,000 objects from Thingi10k [21] (a dataset collected from online-shared 3D printing models), 3,000 industrial components from the set of Chunk 0080 in ABC [20], 3, 400 commodities from 3DNet Cat200 subset [19], and 106 art sculptures from Three D Scans [22]. All these instances are represented in the form of triangular mesh. As illustrated in Fig. 2, some meshes of the collected instances could be non-watertight, with self-occlusion, and/or topologically too complex, and are thus less convenient to be synthetically scanned to simulate practical sensing conditions; we filter out these instances by determining their states of being watertight and 2D-manifold [125], checking self-occlusion [126], and calculating their surface genus [31]. We finally normalize each mesh of the remaining instances by centering it at the origin and scaling it isotropically to fit into the unit sphere. We obtain a total of 1,620 object surface meshes in the benchmark.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们从Thingi10k [21]、ABC [20]、3DNet Cat200子集[19]和Three D Scans [22]的现有库中收集物体实例的CAD模型。更具体地说，我们随机选择了来自Thingi10k [21]（一个从在线共享3D打印模型中收集的数据集）的3,000个物体，来自ABC [20]的Chunk 0080集合中的3,000个工业组件，来自3DNet Cat200子集[19]的3,400个商品，以及来自Three D Scans [22]的106个艺术雕塑。所有这些实例以三角网格的形式表示。如图2所示，收集到的某些实例的网格可能不是水密的，存在自遮挡和/或拓扑过于复杂，因此不便于合成扫描以模拟实际传感条件；我们通过确定它们是否水密和2D流形[125]，检查自遮挡[126]，以及计算它们的表面属[31]来过滤这些实例。我们最终将剩余实例的每个网格进行归一化，使其中心位于原点，并等比缩放以适应单位球体。我们在基准中获得了总共1,620个物体表面网格。</p></div><h4>4.1.2 Groups of Varying Surface Complexities</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.1.2 不同表面复杂度的组</h4></div><p>It is possible that performance of different methods depends on the complexities of object surfaces to be reconstructed. To identify better working conditions for different methods, we intend to divide the above processed object surface meshes into groups of varying complexities. Surface complexities can be measured under different metrics, among which algebraic complexity and topological complexity are the measures more relevant to surface reconstruction from point clouds [124]; simply put, the former measures the degree of polynomials needed to represent a surface, and the latter can be measured as the surface genus (e.g., the number of holes on the surface). Given that our instances of surface meshes have been processed to satisfy the conditions of being watertight, 2D manifold, and topologically simpler (equal to or smaller than genus 5 ), we divide all the 1,620 instances into three groups of low-, middle-, and high-complexity based on the measure of algebraic complexity.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>不同方法的性能可能取决于待重建物体表面的复杂度。为了识别不同方法的更好工作条件，我们打算将上述处理过的物体表面网格分为不同复杂度的组。表面复杂度可以在不同的度量标准下进行测量，其中代数复杂度和拓扑复杂度是与从点云进行表面重建更相关的度量[124]；简单来说，前者测量表示表面所需的多项式的次数，后者可以通过表面属（例如，表面上的孔的数量）来测量。鉴于我们的表面网格实例已处理以满足水密、2D流形和拓扑更简单（属等于或小于5）的条件，我们根据代数复杂度的度量将所有1,620个实例分为低、中和高复杂度的三组。</p></div><p>The algebraic complexity of a surface is usually computed as the highest degree of polynomial functions that approximate/fit local patches on the surface [124]. However, given fixed budget of approximation errors, it is usually unstable to fit local surface patches with polynomials of high degrees [50], causing inaccurate prediction of polynomial degrees and thus that of algebraic complexity. Instead, we take the strategy of fixing the highest function degree and measuring the averaged approximation errors of local surface patches; technical details are given in Appendix A. We finally obtain the three groups respectively of 972 instances, 486 instances,and 162 instances (at the ratio of around \(6 : 3 : 1\) for low-, middle-, and high-complexity groups). Table 2 summarizes the group statistics.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表面的代数复杂度通常计算为近似/拟合表面上局部补丁的多项式函数的最高次数[124]。然而，考虑到固定的近似误差预算，通常用高次多项式拟合局部表面补丁是不稳定的[50]，导致多项式次数的预测不准确，从而导致代数复杂度的预测不准确。相反，我们采取固定最高函数次数的策略，并测量局部表面补丁的平均近似误差；技术细节见附录A。我们最终分别获得了972个实例、486个实例和162个实例的三组（低、中和高复杂度组的比例约为\(6 : 3 : 1\)）。表2总结了组统计信息。</p></div><!-- Media --><p>TABLE 2: The benchmark collection of synthetic object instances from existing repositories and their distributions in the three groups of low, middle, and high complexities.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表2：来自现有库的合成物体实例的基准集合及其在低、中和高复杂度三组中的分布。</p></div><table><tbody><tr><td>Surface complexity</td><td>Low</td><td>Middle</td><td>High</td><td>Total</td></tr><tr><td>Thingi10k [21]</td><td>516</td><td>230</td><td>97</td><td>845</td></tr><tr><td>3DNet Cat200 subset [19]</td><td>144</td><td>89</td><td>33</td><td>266</td></tr><tr><td>ABC [20]</td><td>312</td><td>160</td><td>6</td><td>478</td></tr><tr><td>Three D Scans [22]</td><td>0</td><td>7</td><td>26</td><td>31</td></tr><tr><td>Our benchmark</td><td>972</td><td>486</td><td>162</td><td>1,620</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>表面复杂性</td><td>低</td><td>中</td><td>高</td><td>总计</td></tr><tr><td>Thingi10k [21]</td><td>516</td><td>230</td><td>97</td><td>845</td></tr><tr><td>3DNet Cat200 子集 [19]</td><td>144</td><td>89</td><td>33</td><td>266</td></tr><tr><td>ABC [20]</td><td>312</td><td>160</td><td>6</td><td>478</td></tr><tr><td>三维扫描 [22]</td><td>0</td><td>7</td><td>26</td><td>31</td></tr><tr><td>我们的基准</td><td>972</td><td>486</td><td>162</td><td>1,620</td></tr></tbody></table></div><!-- Media --><h4>4.1.3 Synthetic Point Cloud Scanning</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.1.3 合成点云扫描</h4></div><p>We use Blender Sensor Simulation Toolbox (BlenSor) [127] to synthetically scan our collected surface meshes of object instances. We describe in this section our scanning pipeline, including how we implement different ways of imperfect scanning that simulate point cloud sensing happening in practical conditions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们使用Blender传感器仿真工具箱（BlenSor）[127]对收集到的物体实例表面网格进行合成扫描。本节描述了我们的扫描流程，包括如何实现不同方式的不完美扫描，以模拟在实际条件下发生的点云感知。</p></div><p>Perfect scanning - We first show our way of perfect scanning to give a whole picture of how we conduct our virtual scanning pipline as shown in Fig. 2. As described in Section 4.1.1, our instances of surface meshes have been normalized at the center of a unit sphere in the simulator. To scan an instance, we place a virtual time-of-flight (TOF) camera [128] on viewing spheres whose radii range from \({r}_{\min } = {2.5}\) to \({r}_{\max } = {3.5}\) ; a viewpoint on any of the spheres can be specified as the camera extrinsic \(\mathbf{K} = \left\lbrack  {\mathbf{R} \mid  \mathbf{t}}\right\rbrack   \in  {\mathbb{R}}^{4 \times  4}\) ,where the rotation \(\mathbf{R} \in  {\mathbb{R}}^{4 \times  3}\) and translation \(\mathbf{t} \in  {\mathbb{R}}^{4 \times  1}\) together specify how the camera is positioned. Denote as \({\mathcal{P}}_{\mathbf{K}}\) the point cloud obtained by scanning from the viewpoint \(\mathbf{K}\) ; one may transform it into the world coordinate system as \(\mathbf{K} \circ  {\mathcal{P}}_{\mathbf{K}}\) ,where \(\circ\) denotes an operator \({}^{1}\) . In our setting,we sample1,000viewpoints on the spheres of different radii, resulting in a collection of scanned point clouds \(\left\{  {{\mathbf{K}}_{1} \circ  {\mathcal{P}}_{{\mathbf{K}}_{1}},\cdots ,{\mathbf{K}}_{1000} \circ  {\mathcal{P}}_{{\mathbf{K}}_{1000}}}\right\}\) ,each of which partially covers the surface. Then we can register and fuse different partial point clouds to cover the complete surface \(\mathop{\bigcup }\limits_{{i = 1}}^{{1000}}{\mathbf{K}}_{i} \circ  {\mathcal{P}}_{{\mathbf{K}}_{i}}\) . We obtain the final,uniformly distributed point cloud scanning \(\mathcal{P}\) by applying Farthest Point Sampling (FPS) [99] to \(\mathop{\bigcup }\limits_{{i = 1}}^{{1000}}{\mathbf{K}}_{i} \circ  {\mathcal{P}}_{{\mathbf{K}}_{i}}\) ; \(\mathcal{P}\) is set to contain \({80}\mathrm{k}\) points for low-complexity surfaces, \({120}\mathrm{k}\) points for middle-complexity surfaces,and \({160}\mathrm{k}\) points for high-complexity surfaces.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>完美扫描 - 我们首先展示完美扫描的方法，以便全面了解我们如何进行虚拟扫描流程，如图2所示。如第4.1.1节所述，我们的表面网格实例已在仿真器的单位球体中心进行了归一化。为了扫描一个实例，我们在半径范围从\({r}_{\min } = {2.5}\)到\({r}_{\max } = {3.5}\)的视图球上放置一个虚拟的飞行时间（TOF）相机[128]；可以将视图球上的任何一个视点指定为相机的外部参数\(\mathbf{K} = \left\lbrack  {\mathbf{R} \mid  \mathbf{t}}\right\rbrack   \in  {\mathbb{R}}^{4 \times  4}\)，其中旋转\(\mathbf{R} \in  {\mathbb{R}}^{4 \times  3}\)和位移\(\mathbf{t} \in  {\mathbb{R}}^{4 \times  1}\)共同指定相机的位置。将从视点\(\mathbf{K}\)扫描得到的点云记作\({\mathcal{P}}_{\mathbf{K}}\)；可以将其转换为世界坐标系，记作\(\mathbf{K} \circ  {\mathcal{P}}_{\mathbf{K}}\)，其中\(\circ\)表示一个算子\({}^{1}\)。在我们的设置中，我们在不同半径的球体上采样1,000个视点，得到一组扫描的点云\(\left\{  {{\mathbf{K}}_{1} \circ  {\mathcal{P}}_{{\mathbf{K}}_{1}},\cdots ,{\mathbf{K}}_{1000} \circ  {\mathcal{P}}_{{\mathbf{K}}_{1000}}}\right\}\)，每个点云部分覆盖表面。然后，我们可以注册并融合不同的部分点云，以覆盖完整的表面\(\mathop{\bigcup }\limits_{{i = 1}}^{{1000}}{\mathbf{K}}_{i} \circ  {\mathcal{P}}_{{\mathbf{K}}_{i}}\)。我们通过对\(\mathop{\bigcup }\limits_{{i = 1}}^{{1000}}{\mathbf{K}}_{i} \circ  {\mathcal{P}}_{{\mathbf{K}}_{i}}\)应用最远点采样（FPS）[99]获得最终均匀分布的点云扫描\(\mathcal{P}\)；\(\mathcal{P}\)设置为包含低复杂度表面\({80}\mathrm{k}\)个点，中复杂度表面\({120}\mathrm{k}\)个点，以及高复杂度表面\({160}\mathrm{k}\)个点。</p></div><hr>
<!-- Footnote --><p>\({}^{1}\) Note that a point \(\mathbf{p} \in  {\mathcal{P}}_{\mathbf{K}}\) in the camera coordinate system can be transformed to point \({\mathbf{p}}^{\text{world }} \in  {\mathcal{P}}_{\mathbf{K}}^{\text{world }}\) in the world coordinate system in terms of a homogeneous equation as \({\left\lbrack  {\mathbf{p}}^{\text{world }};1\right\rbrack  }^{\top } = {\left\lbrack  \mathbf{p};1\right\rbrack  }^{\top }{\mathbf{K}}^{-1}\) ; here we transform \({\mathcal{P}}_{\mathbf{K}}\) to \({\mathcal{P}}_{\mathbf{K}}^{\text{world }}\) and write collectively as \({\mathcal{P}}_{\mathbf{K}}^{\text{world }} = \mathbf{K} \circ  {\mathcal{P}}_{\mathbf{K}}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{1}\) 请注意，相机坐标系中的一个点\(\mathbf{p} \in  {\mathcal{P}}_{\mathbf{K}}\)可以通过齐次方程转换为世界坐标系中的点\({\mathbf{p}}^{\text{world }} \in  {\mathcal{P}}_{\mathbf{K}}^{\text{world }}\)，表示为\({\left\lbrack  {\mathbf{p}}^{\text{world }};1\right\rbrack  }^{\top } = {\left\lbrack  \mathbf{p};1\right\rbrack  }^{\top }{\mathbf{K}}^{-1}\)；在这里，我们将\({\mathcal{P}}_{\mathbf{K}}\)转换为\({\mathcal{P}}_{\mathbf{K}}^{\text{world }}\)，并统称为\({\mathcal{P}}_{\mathbf{K}}^{\text{world }} = \mathbf{K} \circ  {\mathcal{P}}_{\mathbf{K}}\)。</p></div><!-- Footnote -->
<hr><!-- Media --><!-- figureText: Collecting Filtering Grouping Synthetic scanning Missing point Perfect scanning Outlier Misalig- nment [RS] \( {\left\{  \Delta {K}_{i}\right\}  }_{i = 0}^{1000} \) Noise Non-uniform Thingi10k A Self intersection A Self-occlusion ABC database A Non-manifold ∄ Genus too large 3DNet Cat200 \( \forall \) Edge has A Non-manifold exactly two vertices incident faces Three D Scans Normalizing Centering & Scaling --><img src="https://cdn.noedgeai.com/bo_d163ugjef24c73d1leb0_8.jpg?x=168&#x26;y=125&#x26;w=1454&#x26;h=562&#x26;r=0"><p>Fig. 2: The pipeline of constructing our synthetic object-level dataset (cf. Section 4.1 for the details). We firstly collect CAD models of object surface from four repositories, as shown in the leftmost of the figure. We then filter the collected surfaces by dropping those failing to meet our requirements, and normalize the remaining ones, as shown in the middle of the figure. We further organize the normalized object surfaces into three groups of low,middle,and high complexities at a ratio of around \(6 : 3 : 1\) ,based on the criterion of (algebraic) surface complexity. We finally perform synthetic scanning as shown in the right of the figure, where different challenges possibly encountered in practical scanning are simulated. Blue spots indicate the scanning positions on viewing spheres; after estimating oriented normals (shown as \(\mathrm{N}\) in the figure),we make registration between different viewpoints of scanning and get the final scanned point cloud, where farthest point sampling (FPS) is used for sampling a fixed number of points per point cloud (except otherwise mentioned cases in which random sampling (RS) is used).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2：构建我们合成物体级数据集的流程（详见第4.1节）。我们首先从四个库中收集物体表面的CAD模型，如图左侧所示。然后，我们通过剔除不符合要求的表面来过滤收集到的表面，并对剩余的表面进行归一化，如图中所示。我们进一步根据（代数）表面复杂度的标准，将归一化的物体表面组织成低、中、高三组，比例约为\(6 : 3 : 1\)。最后，我们执行合成扫描，如图右侧所示，模拟在实际扫描中可能遇到的不同挑战。蓝点表示在观察球上的扫描位置；在估计定向法线（如图中\(\mathrm{N}\)所示）后，我们在不同的扫描视点之间进行配准，得到最终的扫描点云，其中使用最远点采样（FPS）对每个点云进行固定数量的点采样（除非另有说明的情况使用随机采样（RS））。</p></div><!-- Media --><p>Since some of existing methods require surface normals to preform reconstruction, we compute the oriented surface normals as follows. For any point \(p \in  \mathcal{P}\) ,we first compute its unoriented normal \({\overline{\mathbf{n}}}_{\mathbf{p}}\) by performing PCA on the local neighborhood constructed by \(k = {40}\) nearest neighbors of the point; orientation of \({\overline{\mathbf{n}}}_{\mathbf{p}}\) can be simply determined by comparing \(\mathbf{p}\) with the camera position,giving rise to the oriented normal \({\mathbf{n}}_{\mathbf{p}}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>由于一些现有方法需要表面法线来进行重建，我们按如下方式计算定向表面法线。对于任何点\(p \in  \mathcal{P}\)，我们首先通过对该点的\(k = {40}\)个最近邻构建的局部邻域进行主成分分析（PCA）来计算其无定向法线\({\overline{\mathbf{n}}}_{\mathbf{p}}\)；通过将\(\mathbf{p}\)与相机位置进行比较，可以简单地确定\({\overline{\mathbf{n}}}_{\mathbf{p}}\)的方向，从而得到定向法线\({\mathbf{n}}_{\mathbf{p}}\)。</p></div><p>Point-wise noise - Due to sensor noise, ambient noise, reflective nature of the surface, and the incapable precision of the scanning devices, point clouds from practical scanning are inevitably noisy. In this case, each scanned point is not exactly on the underlying surface, deviating away from the surface in a pointwise, independent manner; and to simulate such noise, we add point-wise perturbations to the points obtained by the aforementioned perfect scanning. Specifically,for any surface point \(\mathbf{q}\) ,we generate \(\Delta {\mathbf{q}}_{\text{noise }} \in  {\mathbb{R}}^{3}\) by randomly sampling its element values from a Gaussian distribution \(\mathcal{N}\left( {0,{\sigma }_{\text{noise }}^{2}}\right)\) with a truncated values \(\left\lbrack  {-2{\sigma }_{\text{noise }},2{\sigma }_{\text{noise }}}\right\rbrack\) . Given a point \(\mathbf{p} \in  \mathcal{P}\) ,the corresponding noisy point from noisy scanning is obtained as \(\mathbf{p} = \mathbf{q} + \Delta {\mathbf{q}}_{\text{noise }}\) . We set \({\sigma }_{\text{noise }}\) respectively as0.001,0.003,and 0.006 in our benchmark to simulate different severity levels of point-wise noise. Note that the truncation above is to prevent individual \(\{ \mathbf{p} \in  \mathcal{P}\}\) from deviating too far away from the surface, which would become point outliers to be discussed shortly.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>逐点噪声 - 由于传感器噪声、环境噪声、表面的反射特性以及扫描设备的精度不足，实际扫描得到的点云不可避免地存在噪声。在这种情况下，每个扫描点并不完全位于基础表面上，而是以逐点、独立的方式偏离表面；为了模拟这种噪声，我们对通过上述完美扫描获得的点添加逐点扰动。具体而言，对于任何表面点\(\mathbf{q}\)，我们通过从具有截断值\(\left\lbrack  {-2{\sigma }_{\text{noise }},2{\sigma }_{\text{noise }}}\right\rbrack\)的高斯分布\(\mathcal{N}\left( {0,{\sigma }_{\text{noise }}^{2}}\right)\)中随机采样其元素值来生成\(\Delta {\mathbf{q}}_{\text{noise }} \in  {\mathbb{R}}^{3}\)。给定一个点\(\mathbf{p} \in  \mathcal{P}\)，从噪声扫描中获得的相应噪声点为\(\mathbf{p} = \mathbf{q} + \Delta {\mathbf{q}}_{\text{noise }}\)。我们在基准测试中将\({\sigma }_{\text{noise }}\)分别设置为0.001、0.003和0.006，以模拟不同严重程度的逐点噪声。请注意，上述截断是为了防止个别\(\{ \mathbf{p} \in  \mathcal{P}\}\)偏离表面过远，这将成为稍后讨论的点异常值。</p></div><p>Non-uniform distribution of points - Practical scanning often produces a point cloud whose points are not uniformly distributed over the surface. For example, the surface patches that are scanned for multiple times (possibly from different viewpoints) would have more points, and a closer scanning position would produce denser points as well. To simulate such phenomena, we replace the final step of uniformity-promoting FPS in perfect scanning with Random Sampling (RS), and local point densities of the resulting \(\mathcal{P}\) would be less uniform over the surface.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>点的非均匀分布 - 实际扫描通常产生的点云，其点在表面上并不是均匀分布的。例如，被多次扫描的表面块（可能来自不同的视点）会有更多的点，而更近的扫描位置也会产生更密集的点。为了模拟这种现象，我们用随机采样（RS）替代完美扫描中促进均匀性的最远点采样（FPS）的最后一步，结果的\(\mathcal{P}\)的局部点密度在表面上会更不均匀。</p></div><p>Point outliers - As mentioned above, outliers of a surface point cloud are defined as those deviating far away from the surface. They are often caused by impulsive noise of practical scanning. We simulate such outliers as follows. Given a point cloud obtained by perfect scanning,we first randomly sample a ratio \({r}_{\text{outlier }}\) of its points,and for any sampled point \(\mathbf{q}\) that is on the surface,we generate \(\Delta {\mathbf{q}}_{\text{outlier }} \in  {\mathbb{R}}^{3}\) by randomly sampling its element values from a uniform distribution \(\mathcal{U}\left\lbrack  {{a}_{\text{outlier }},{b}_{\text{outlier }}}\right\rbrack\) ; the corresponding point outlier \({\mathbf{p}}_{\text{outlier }} \in  \mathcal{P}\) is then obtained as \(\mathbf{p} = \mathbf{q} \pm  \Delta {\mathbf{q}}_{\text{outlier }}\) . We set \({a}_{\text{outlier }} = {0.01}\) to distinguish point outliers from noisy points and set \({b}_{\text{outlier }} = {0.1}\) to prevent the outliers from being less relevantly distancing. The ratio \({r}_{\text{outlier }}\) is respectively set as \({0.1}\% ,{0.3}\%\) ,and \({0.6}\%\) for varying numbers of outliers in each obtained \(\mathcal{P}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>指出离群点 - 如上所述，表面点云的离群点定义为那些远离表面的点。它们通常是由实际扫描中的冲击噪声引起的。我们模拟这些离群点如下。给定一个通过完美扫描获得的点云，我们首先随机抽样其点的比例 \({r}_{\text{outlier }}\)，对于任何位于表面的抽样点 \(\mathbf{q}\)，我们通过从均匀分布 \(\mathcal{U}\left\lbrack  {{a}_{\text{outlier }},{b}_{\text{outlier }}}\right\rbrack\) 中随机抽样其元素值来生成 \(\Delta {\mathbf{q}}_{\text{outlier }} \in  {\mathbb{R}}^{3}\)；然后相应的点离群点 \({\mathbf{p}}_{\text{outlier }} \in  \mathcal{P}\) 被获得为 \(\mathbf{p} = \mathbf{q} \pm  \Delta {\mathbf{q}}_{\text{outlier }}\)。我们设置 \({a}_{\text{outlier }} = {0.01}\) 以区分点离群点和噪声点，并设置 \({b}_{\text{outlier }} = {0.1}\) 以防止离群点的相关距离过小。比例 \({r}_{\text{outlier }}\) 分别设置为 \({0.1}\% ,{0.3}\%\) 和 \({0.6}\%\)，以适应每个获得的 \(\mathcal{P}\) 中不同数量的离群点。</p></div><p>Misalignment - As described for perfect scanning, a scanning viewpoint is specified by the camera extrinsic \(\mathbf{K} = \left\lbrack  {\mathbf{R} \mid  \mathbf{t}}\right\rbrack\) ; scanning from the viewpoint \(\mathbf{K}\) would produce a point cloud \({\mathcal{P}}_{\mathbf{K}}\) that covers the surface partially; a complete point cloud is obtained by scanning from 1,000 viewpoints and then registering and fusing the obtained point clouds as \(\mathop{\bigcup }\limits_{{i = 1}}^{{1000}}{\mathbf{K}}_{i} \circ  {\mathcal{P}}_{{\mathbf{K}}_{i}}\) . However, misalignment would happen when the camera extrinsics are less accurate. To simulate such a misalignment, for each viewpoint \(\mathbf{K}\) ,we generate the perturbation \(\Delta \mathbf{K} = \left\lbrack  {\Delta \mathbf{R} \mid  \Delta \mathbf{t}}\right\rbrack   \in  {\mathbb{R}}^{4 \times  4}\) where \(\Delta \mathbf{R} \in  {\mathbb{R}}^{4 \times  3}\) is obtained by the XYZ Euler angle convention [129],i.e., \(\Delta \mathbf{R} = \left\lbrack  {{\mathbf{R}}_{x}\left( \alpha \right) {\mathbf{R}}_{y}\left( \beta \right) {\mathbf{R}}_{z}\left( \gamma \right) ;{\mathbf{0}}_{3}^{\top }}\right\rbrack\) with \(\alpha\) , \(\beta\) ,and \(\gamma\) uniformly sampled from \(\mathcal{U}\left\lbrack  {{a}_{\text{rotation }},{b}_{\text{rotation }}}\right\rbrack\) and those of the translation perturbation \(\Delta \mathbf{t} \in  {\mathbb{R}}^{4 \times  1}\) uniformly sampled from \(\mathcal{U}\left\lbrack  {{a}_{\text{translation }},{b}_{\text{translation }}}\right\rbrack\) . The final point cloud \(\mathcal{P}\) with misalignment is obtained by applying FPS to \(\mathop{\bigcup }\limits_{{i = 1}}^{{1000}}\left( {{\mathbf{K}}_{i} + \Delta {\mathbf{K}}_{i}}\right)  \circ  {\mathcal{P}}_{{\mathbf{K}}_{i}}\) . We set \(\left\lbrack  {{a}_{\text{rotation }},{b}_{\text{rotation }}}\right\rbrack\) as \(\left\lbrack  {-{0.5}^{ \circ  },{0.5}^{ \circ  }}\right\rbrack  ,\left\lbrack  {-{1}^{ \circ  },{1}^{ \circ  }}\right\rbrack\) ,and \(\left\lbrack  {-{2}^{ \circ  },{2}^{ \circ  }}\right\rbrack\) , and set \(\left\lbrack  {{a}_{\text{translation }},{b}_{\text{translation }}}\right\rbrack\) as \(\left\lbrack  {-{0.005},{0.005}}\right\rbrack  ,\left\lbrack  {-{0.01},{0.01}}\right\rbrack\) ,and \(\left\lbrack  {-{0.02},{0.02}}\right\rbrack\) ,which are respectively for different severities of misalignment.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>错位 - 如完美扫描所描述，扫描视点由相机外部参数 \(\mathbf{K} = \left\lbrack  {\mathbf{R} \mid  \mathbf{t}}\right\rbrack\) 指定；从视点 \(\mathbf{K}\) 扫描将产生一个部分覆盖表面的点云 \({\mathcal{P}}_{\mathbf{K}}\)；通过从1,000个视点扫描并注册和融合获得的点云，可以得到完整的点云 \(\mathop{\bigcup }\limits_{{i = 1}}^{{1000}}{\mathbf{K}}_{i} \circ  {\mathcal{P}}_{{\mathbf{K}}_{i}}\)。然而，当相机外部参数不够准确时，会发生错位。为了模拟这种错位，对于每个视点 \(\mathbf{K}\)，我们生成扰动 \(\Delta \mathbf{K} = \left\lbrack  {\Delta \mathbf{R} \mid  \Delta \mathbf{t}}\right\rbrack   \in  {\mathbb{R}}^{4 \times  4}\)，其中 \(\Delta \mathbf{R} \in  {\mathbb{R}}^{4 \times  3}\) 是通过XYZ欧拉角约定获得的 [129]，即 \(\Delta \mathbf{R} = \left\lbrack  {{\mathbf{R}}_{x}\left( \alpha \right) {\mathbf{R}}_{y}\left( \beta \right) {\mathbf{R}}_{z}\left( \gamma \right) ;{\mathbf{0}}_{3}^{\top }}\right\rbrack\)，其中 \(\alpha\)、\(\beta\) 和 \(\gamma\) 从 \(\mathcal{U}\left\lbrack  {{a}_{\text{rotation }},{b}_{\text{rotation }}}\right\rbrack\) 中均匀采样，平移扰动 \(\Delta \mathbf{t} \in  {\mathbb{R}}^{4 \times  1}\) 从 \(\mathcal{U}\left\lbrack  {{a}_{\text{translation }},{b}_{\text{translation }}}\right\rbrack\) 中均匀采样。通过对 \(\mathop{\bigcup }\limits_{{i = 1}}^{{1000}}\left( {{\mathbf{K}}_{i} + \Delta {\mathbf{K}}_{i}}\right)  \circ  {\mathcal{P}}_{{\mathbf{K}}_{i}}\) 应用FPS，获得带有错位的最终点云 \(\mathcal{P}\)。我们将 \(\left\lbrack  {{a}_{\text{rotation }},{b}_{\text{rotation }}}\right\rbrack\) 设置为 \(\left\lbrack  {-{0.5}^{ \circ  },{0.5}^{ \circ  }}\right\rbrack  ,\left\lbrack  {-{1}^{ \circ  },{1}^{ \circ  }}\right\rbrack\)，将 \(\left\lbrack  {-{2}^{ \circ  },{2}^{ \circ  }}\right\rbrack\) 设置为 \(\left\lbrack  {{a}_{\text{translation }},{b}_{\text{translation }}}\right\rbrack\)，将 \(\left\lbrack  {-{0.005},{0.005}}\right\rbrack  ,\left\lbrack  {-{0.01},{0.01}}\right\rbrack\) 和 \(\left\lbrack  {-{0.02},{0.02}}\right\rbrack\) 设置为不同程度的错位。</p></div><p>Missing points - Due to surface reflection, self-occlusion, and/or simply insufficient covering of the surface, practical scanning often produces a point cloud that does not cover the whole object surface of interest. Surface reflection depends on a mixed effect of lighting and surface material, and the latter is not included in our collected CAD models. Instead, we take the following simple approach in our benchmark to simulate missing surface points. Rather than allowing the scanning viewpoint \({\mathbf{K}}_{i}\) to locate on the whole viewing spheres as in perfect scanning (shown as the blue points in Fig. 2), we only allow it to locate on a limited number of viewing positions to simulate missing points. Specifically, we define the viewing positions in a few narrow bands of trajectories with the polar angle of \(\varphi  \pm  {\varphi }_{\Delta }\) (shown as the yellow points in Fig. 2). For different severities, we respectively set the number of trajectories to be 3,2,and 1,with the polar angle \(\varphi\) set to be \(\left\lbrack  {{20}^{ \circ  },{40}^{ \circ  },{60}^{ \circ  }}\right\rbrack  ,\left\lbrack  {{20}^{ \circ  },{40}^{ \circ  }}\right\rbrack\) ,and \(\left\lbrack  {20}^{ \circ  }\right\rbrack\) respectively,and with \({\varphi }_{\Delta } = {3}^{ \circ  }\) . The scanned surface areas approximately cover \({99}\%\) , \({94}\%\) ,and \({86}\%\) of the whole surface for different severities.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>缺失点 - 由于表面反射、自我遮挡和/或表面覆盖不足，实际扫描通常会产生一个未能覆盖整个目标物体表面的点云。表面反射取决于光照和表面材料的混合效应，而后者未包含在我们收集的CAD模型中。相反，我们在基准测试中采取以下简单方法来模拟缺失的表面点。我们不允许扫描视点\({\mathbf{K}}_{i}\)位于整个视球上，如完美扫描中所示（如图2中的蓝点），而仅允许其位于有限数量的视点位置以模拟缺失点。具体而言，我们在几个狭窄的轨迹带中定义视点位置，极角为\(\varphi  \pm  {\varphi }_{\Delta }\)（如图2中的黄点所示）。对于不同的严重程度，我们分别将轨迹数量设置为3、2和1，极角\(\varphi\)分别设置为\(\left\lbrack  {{20}^{ \circ  },{40}^{ \circ  },{60}^{ \circ  }}\right\rbrack  ,\left\lbrack  {{20}^{ \circ  },{40}^{ \circ  }}\right\rbrack\)和\(\left\lbrack  {20}^{ \circ  }\right\rbrack\)，并且有\({\varphi }_{\Delta } = {3}^{ \circ  }\)。扫描的表面区域大约覆盖了整个表面的\({99}\%\)、\({94}\%\)和\({86}\%\)，对应不同的严重程度。</p></div><h3>4.2 The Synthetic Data of Scene Surfaces</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.2 场景表面的合成数据</h3></div><p>We adopt a pipeline similar to that presented in Section 4.1 for synthetic scanning of the scene-level data. Since the scale of a scene surface is larger and practical scanning usually produces a point cloud that includes multiple types of imperfections. As such, we include all the five challenges of point-wise noise, point outliers, non-uniform distribution of points, misalignment, and missing points into our synthetic scanning of a single scene. Fig. 3 gives the illustration.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们采用与第4.1节中提出的类似的管道来合成场景级数据的扫描。由于场景表面的规模较大，实际扫描通常会产生包含多种缺陷的点云。因此，我们将点噪声、点异常值、点的非均匀分布、错位和缺失点这五个挑战纳入到单个场景的合成扫描中。图3给出了说明。</p></div><p>Data collection - We choose to collect CAD surface models of indoor scenes in the benchmark. To form the collection, we randomly select 4 indoor scenes from SceneNet [23], 4 from 3D-FRONT [24], and 2 from Replica [28]. The scenes from the three datasets are diverse in terms of varying room types, varying room sizes, and the contained different furniture and furnishings. The collected surfaces are represented in the form of triangular mesh as well. We normalize each scene mesh by centering it at the origin but keeping its original size.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>数据收集 - 我们选择在基准测试中收集室内场景的CAD表面模型。为了形成该集合，我们随机选择了4个来自SceneNet [23]的室内场景、4个来自3D-FRONT [24]的场景和2个来自Replica [28]的场景。这三个数据集中的场景在房间类型、房间大小以及包含的不同家具和装饰方面各具多样性。收集的表面也以三角网格的形式表示。我们通过将每个场景网格居中于原点但保持其原始大小来对其进行归一化。</p></div><p>Synthetic scanning - We still use BlenSor [127] to perform our synthetical scene scanning. In practical scanning of indoor scenes, the scanner is usually placed at a medium distancing from the scene surface; we thus choose the sensor of Kinect V2 [130] whose working distance ranges from \({0.75}\mathrm{\;m}\) to \({2.1}\mathrm{\;m}\) . We prepare the scanning by placing each surface mesh of indoor scene in a bounded 3D space, where the scene center has been aligned at the origin and the space size is set to be just enclosing the scene surface (cf. Fig. 3). We firstly partition the 3D space into the volume of \(1{\mathrm{\;m}}^{3}\) -sized, \({0.5}{\mathrm{\;m}}^{3}\) -overlapped cubes; some of the cubes would be empty while others contain certain patches of the scene surface. We choose the centers of those empty cubes as the positions from which the virtual camera views the scene, and abandon others to ensure that the working distance between the camera and scene surface satisfy the aforementioned Kinect V2 requirements. From each position, the camera could view a certain patch of the scene surface towards arbitrary directions on a viewing sphere originated at the cube center. More specifically, for a center of empty cube positioned at \(\mathbf{x} \in  {\mathbb{R}}^{3}\) ,we make it homogeneous in the form of \({\mathbf{t}}_{i} = \left\lbrack  {\mathbf{x};1}\right\rbrack   \in  {\mathbb{R}}^{4}\) and randomly sample 100 directions to get the camera extrinsics \({\left\{  {\mathbf{K}}_{ij} = \left\lbrack  {\mathbf{R}}_{j} \mid  {\mathbf{t}}_{i}\right\rbrack  \right\}  }_{j = 1}^{100}\) ,each of which would be used to generate a point cloud \({\mathcal{P}}_{{\mathbf{K}}_{ij}}\) covering a certain patch of the scene surface. Similar to object scanning, a final point cloud \(\mathcal{P}\) is obtained by registering and fusing all the point clouds obtained by the preceding scanning \({\left\{  {\mathbf{K}}_{ij} \circ  {\mathcal{P}}_{{\mathbf{K}}_{ij}}\right\}  }_{i,j}\) . Note that we do not conduct FPS as object-level synthetic scanning does; and consequently, the challenge of non-uniform distribution of points naturally appears here. Since some methods require surface normals,we compute the oriented normal \({\mathbf{n}}_{\mathbf{p}}\) for all the point \(\mathbf{p} \in  \mathcal{P}\) in the same way as object scanning does. Apart from the natural challenges of non-uniform distribution and missing points (due to self-occlusion), we also simulate the other challenges of point-wise noise, point outliers, and misalignment in the same way as described in Section 4.1.3. Specifically, following the settings of Kinect V2 camera,we set \({\sigma }_{\text{noise }} = {0.005}\mathrm{\;m}\) to control the level of point-wise noise,set \({a}_{\text{outlier }} = {0.01}\mathrm{\;m},{b}_{\text{outlier }} = {0.1}\mathrm{\;m}\) ,and the ratio \({r}_{\text{outlier }} = {0.4}\%\) to control the level of outliers,and set \(\left\lbrack  {{a}_{\text{rotation }},{b}_{\text{rotation }}}\right\rbrack\) as \(\left\lbrack  {-{1.5}^{ \circ  },{1.5}^{ \circ  }}\right\rbrack\) and \(\left\lbrack  {{a}_{\text{translation }},{b}_{\text{translation }}}\right\rbrack\) as \(\left\lbrack  {-{0.015}\mathrm{\;m},{0.015}\mathrm{\;m}}\right\rbrack\) to control the level of misalignment. Such a point cloud \(\mathcal{P}\) contains around one million points containing all the five challenges.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>合成扫描 - 我们仍然使用BlenSor [127]来进行合成场景扫描。在实际的室内场景扫描中，扫描仪通常放置在距离场景表面一定距离的位置；因此，我们选择Kinect V2 [130]传感器，其工作距离范围从 \({0.75}\mathrm{\;m}\) 到 \({2.1}\mathrm{\;m}\)。我们通过将室内场景的每个表面网格放置在一个有界的三维空间中来准备扫描，其中场景中心已对齐到原点，并且空间大小设置为刚好包围场景表面（参见图3）。我们首先将三维空间划分为大小为 \(1{\mathrm{\;m}}^{3}\)、重叠度为 \({0.5}{\mathrm{\;m}}^{3}\) 的立方体；其中一些立方体为空，而另一些则包含场景表面的特定部分。我们选择那些空立方体的中心作为虚拟相机观察场景的位置，并舍弃其他位置，以确保相机与场景表面之间的工作距离满足上述Kinect V2的要求。从每个位置，相机可以在以立方体中心为原点的观察球上向任意方向观察场景表面的特定部分。更具体地说，对于位于 \(\mathbf{x} \in  {\mathbb{R}}^{3}\) 的空立方体中心，我们将其表示为 \({\mathbf{t}}_{i} = \left\lbrack  {\mathbf{x};1}\right\rbrack   \in  {\mathbb{R}}^{4}\) 的齐次形式，并随机采样100个方向以获得相机外参 \({\left\{  {\mathbf{K}}_{ij} = \left\lbrack  {\mathbf{R}}_{j} \mid  {\mathbf{t}}_{i}\right\rbrack  \right\}  }_{j = 1}^{100}\)，每个外参将用于生成覆盖场景表面特定部分的点云 \({\mathcal{P}}_{{\mathbf{K}}_{ij}}\)。与物体扫描类似，通过对前面扫描 \({\left\{  {\mathbf{K}}_{ij} \circ  {\mathcal{P}}_{{\mathbf{K}}_{ij}}\right\}  }_{i,j}\) 获得的所有点云进行配准和融合，得到最终的点云 \(\mathcal{P}\)。请注意，我们不像物体级合成扫描那样进行最远点采样（FPS）；因此，这里自然会出现点分布不均匀的问题。由于一些方法需要表面法线，我们以与物体扫描相同的方式为所有点 \(\mathbf{p} \in  \mathcal{P}\) 计算有向法线 \({\mathbf{n}}_{\mathbf{p}}\)。除了点分布不均匀和点缺失（由于自遮挡）这些自然问题外，我们还以第4.1.3节中描述的相同方式模拟了逐点噪声、离群点和未对齐等其他问题。具体来说，根据Kinect V2相机的设置，我们设置 \({\sigma }_{\text{noise }} = {0.005}\mathrm{\;m}\) 来控制逐点噪声的水平，设置 \({a}_{\text{outlier }} = {0.01}\mathrm{\;m},{b}_{\text{outlier }} = {0.1}\mathrm{\;m}\) 以及比例 \({r}_{\text{outlier }} = {0.4}\%\) 来控制离群点的水平，并将 \(\left\lbrack  {{a}_{\text{rotation }},{b}_{\text{rotation }}}\right\rbrack\) 设置为 \(\left\lbrack  {-{1.5}^{ \circ  },{1.5}^{ \circ  }}\right\rbrack\)，将 \(\left\lbrack  {{a}_{\text{translation }},{b}_{\text{translation }}}\right\rbrack\) 设置为 \(\left\lbrack  {-{0.015}\mathrm{\;m},{0.015}\mathrm{\;m}}\right\rbrack\) 来控制未对齐的水平。这样的点云 \(\mathcal{P}\) 包含大约一百万个点，包含所有这五个问题。</p></div><!-- Media --><p>TABLE 3: Statistics of synthetic data in the benchmark. Elements in each triple \(\cdot  / \cdot  / \cdot\) represent the hyper-parameters that control the scanning with three levels of severity; # denotes the number. (cf. Sections 4.1 and 4.2 for specific meanings of the math notations.)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表3：基准测试中合成数据的统计信息。每个三元组 \(\cdot  / \cdot  / \cdot\) 中的元素表示控制扫描的超参数，具有三个严重程度级别；# 表示数量。（有关数学符号的具体含义，请参阅第4.1节和第4.2节。）</p></div><table><tbody><tr><td colspan="2"></td><td colspan="2">Object level</td><td>Scene level</td></tr><tr><td colspan="2">normalization \( \left\lbrack  {{r}_{\min },{r}_{\max }}\right\rbrack \) (camera distancing)</td><td colspan="2">centering+scaling \( \left\lbrack  {{2.5},{3.5}}\right\rbrack \)</td><td>centering \( \left\lbrack  {{0.75}\mathrm{\;m},{2.1}\mathrm{\;m}}\right\rbrack \)</td></tr><tr><td>point-wise noise</td><td>\( {\sigma }_{\text{noise }} \)</td><td colspan="2">0.001/0.003/0.006</td><td>0.005m</td></tr><tr><td>point outliers</td><td>\( \left\lbrack  {{a}_{\text{outlier }},{b}_{\text{outlier }}}\right\rbrack \) \( {r}_{\text{outlier }} \)</td><td colspan="2">\( \left\lbrack  {{0.01},{0.1}}\right\rbrack \) 0.1%/0.3%/0.6%</td><td>\( \left\lbrack  {{0.01}\mathrm{\;m},{0.1}\mathrm{\;m}}\right\rbrack \) 0.4%</td></tr><tr><td>missing points</td><td>\( \varphi \) \( {\Delta \varphi } \)</td><td colspan="2">\( \left\lbrack  {{20}^{ \circ  },{40}^{ \circ  },{60}^{ \circ  }}\right\rbrack  /\left\lbrack  {{20}^{ \circ  },{40}^{ \circ  }}\right\rbrack  /\left\lbrack  {20}^{ \circ  }\right\rbrack \) \( {3}^{ \circ  } \)</td><td>-</td></tr><tr><td rowspan="2">misalignment</td><td>\( \left\lbrack  {{a}_{\text{rotation }},{b}_{\text{rotation }}}\right\rbrack \)</td><td colspan="2">\( \lbrack  - {0.5}^{ \circ  },{0.5}^{ \circ  }\rbrack /\lbrack  - {1}^{ \circ  },{1}^{ \circ  }\rbrack /\lbrack  - {2}^{ \circ  },{2}^{ \circ  }\rbrack \)</td><td>\( \left\lbrack  {-{1.5}^{ \circ  },{1.5}^{ \circ  }}\right\rbrack \)</td></tr><tr><td>\( \left\lbrack  {{a}_{\text{translation }},{b}_{\text{translation }}}\right\rbrack \)</td><td colspan="2">\( \left\lbrack  {-{0.005},{0.005}}\right\rbrack  /\left\lbrack  {-{0.01},{0.01}}\right\rbrack  /\left\lbrack  {-{0.02},{0.02}}\right\rbrack \)</td><td>\( \left\lbrack  {-{0.015}\mathrm{\;m},{0.015}\mathrm{\;m}}\right\rbrack \)</td></tr><tr><td colspan="2">#viewpoints</td><td colspan="2">1000</td><td>\( {100}/{\mathrm{m}}^{3} \)</td></tr><tr><td colspan="2" rowspan="3">#scanned points</td><td>low complexity</td><td>80k</td><td rowspan="3">1000k</td></tr><tr><td>middle complexity</td><td>120k</td></tr><tr><td>high complexity</td><td>160k</td></tr><tr><td colspan="2">#surfaces</td><td colspan="2">1620</td><td>10</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td colspan="2"></td><td colspan="2">对象级别</td><td>场景级别</td></tr><tr><td colspan="2">归一化 \( \left\lbrack  {{r}_{\min },{r}_{\max }}\right\rbrack \) (相机距离)</td><td colspan="2">居中+缩放 \( \left\lbrack  {{2.5},{3.5}}\right\rbrack \)</td><td>居中 \( \left\lbrack  {{0.75}\mathrm{\;m},{2.1}\mathrm{\;m}}\right\rbrack \)</td></tr><tr><td>点噪声</td><td>\( {\sigma }_{\text{noise }} \)</td><td colspan="2">0.001/0.003/0.006</td><td>0.005m</td></tr><tr><td>点异常值</td><td>\( \left\lbrack  {{a}_{\text{outlier }},{b}_{\text{outlier }}}\right\rbrack \) \( {r}_{\text{outlier }} \)</td><td colspan="2">\( \left\lbrack  {{0.01},{0.1}}\right\rbrack \) 0.1%/0.3%/0.6%</td><td>\( \left\lbrack  {{0.01}\mathrm{\;m},{0.1}\mathrm{\;m}}\right\rbrack \) 0.4%</td></tr><tr><td>缺失点</td><td>\( \varphi \) \( {\Delta \varphi } \)</td><td colspan="2">\( \left\lbrack  {{20}^{ \circ  },{40}^{ \circ  },{60}^{ \circ  }}\right\rbrack  /\left\lbrack  {{20}^{ \circ  },{40}^{ \circ  }}\right\rbrack  /\left\lbrack  {20}^{ \circ  }\right\rbrack \) \( {3}^{ \circ  } \)</td><td>-</td></tr><tr><td rowspan="2">错位</td><td>\( \left\lbrack  {{a}_{\text{rotation }},{b}_{\text{rotation }}}\right\rbrack \)</td><td colspan="2">\( \lbrack  - {0.5}^{ \circ  },{0.5}^{ \circ  }\rbrack /\lbrack  - {1}^{ \circ  },{1}^{ \circ  }\rbrack /\lbrack  - {2}^{ \circ  },{2}^{ \circ  }\rbrack \)</td><td>\( \left\lbrack  {-{1.5}^{ \circ  },{1.5}^{ \circ  }}\right\rbrack \)</td></tr><tr><td>\( \left\lbrack  {{a}_{\text{translation }},{b}_{\text{translation }}}\right\rbrack \)</td><td colspan="2">\( \left\lbrack  {-{0.005},{0.005}}\right\rbrack  /\left\lbrack  {-{0.01},{0.01}}\right\rbrack  /\left\lbrack  {-{0.02},{0.02}}\right\rbrack \)</td><td>\( \left\lbrack  {-{0.015}\mathrm{\;m},{0.015}\mathrm{\;m}}\right\rbrack \)</td></tr><tr><td colspan="2">#视点</td><td colspan="2">1000</td><td>\( {100}/{\mathrm{m}}^{3} \)</td></tr><tr><td colspan="2" rowspan="3">#扫描点</td><td>低复杂度</td><td>80k</td><td rowspan="3">1000k</td></tr><tr><td>中等复杂度</td><td>120k</td></tr><tr><td>高复杂度</td><td>160k</td></tr><tr><td colspan="2">#表面</td><td colspan="2">1620</td><td>10</td></tr></tbody></table></div><!-- Media --><h3>4.3 The Real-scanned Data</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.3 实际扫描数据</h3></div><p>We provide real-scanned data of the benchmark by scanning real object instances via two depth cameras of varying precisions. To scan an object,we use SHINING 3D Einscan SE \({}^{2}\) whose precision is of 100 micrometers to get the input point cloud, and use SHINING 3D OKIO \(5{\mathrm{M}}^{3}\) whose precision is of 5 micrometers to get the approximate ground-truth. \({}^{4}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们通过两台不同精度的深度相机扫描真实物体实例，提供基准的实际扫描数据。为了扫描一个物体，我们使用精度为100微米的SHINING 3D Einscan SE \({}^{2}\)获取输入点云，并使用精度为5微米的SHINING 3D OKIO \(5{\mathrm{M}}^{3}\)获取近似的真实值。\({}^{4}\)</p></div><p>Data collection - We collect 20 object instances of varying surface complexities, including commodities, instruments, and artwares, and also of varying materials, including metal, plastic, ceramic, and cloth; this is to ensure that various sensing imperfections would appear in the obtained point clouds. Fig. 5 shows these objects.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>数据收集 - 我们收集了20个表面复杂度不同的物体实例，包括商品、工具和艺术品，以及不同材料的物体，包括金属、塑料、陶瓷和布料；这确保了在获得的点云中会出现各种传感器缺陷。图5展示了这些物体。</p></div><hr>
<!-- Footnote --><p>\({}^{2}\) <a href="https://www.einscan.com/desktop-3d-scanners/einscan-se/">https://www.einscan.com/desktop-3d-scanners/einscan-se/</a> \({}^{3}\) <a href="https://www.shining3d.com/solutions/optimscan-5m">https://www.shining3d.com/solutions/optimscan-5m</a></p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{2}\) <a href="https://www.einscan.com/desktop-3d-scanners/einscan-se/">https://www.einscan.com/desktop-3d-scanners/einscan-se/</a> \({}^{3}\) <a href="https://www.shining3d.com/solutions/optimscan-5m">https://www.shining3d.com/solutions/optimscan-5m</a></p></div><p>\({}^{4}\) Our surface reconstruction evaluation is based on metrics of point set distances (cf. Section 5.3), for which we obtain the ground-truth point clouds by sampling from the corresponding surface meshes. We thus choose to directly use the raw point clouds scanned by SHINING 3D OKIO 5M, instead of converting the scanned point clouds as surface meshes using its in-built software.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{4}\) 我们的表面重建评估基于点集距离的度量（参见第5.3节），为此我们通过从相应的表面网格中采样获得真实值点云。因此，我们选择直接使用SHINING 3D OKIO 5M扫描的原始点云，而不是使用其内置软件将扫描的点云转换为表面网格。</p></div><!-- Footnote -->
<hr><!-- Media --><!-- figureText: Collecting Synthetic scanning \( \pm  \Delta {\mathbf{q}}_{\mathrm{{outlie}}} \) RS \( \{ \Delta {K}_{i}{\} }_{i = 0}^{100} \) 3D-FRONT Centering Replica --><img src="https://cdn.noedgeai.com/bo_d163ugjef24c73d1leb0_10.jpg?x=166&#x26;y=124&#x26;w=1454&#x26;h=582&#x26;r=0"><p>Fig. 3: The pipeline of constructing our synthetic scene-level dataset (cf. Section 4.2 for the details). We collect CAD surface models of indoor scenes from three datasets, as shown in the left of the figure, and normalize them by centering at the origin in the 3D space. We partition the 3D space into overlapped cubes, as shown in the right of the figure; some of the cubes contain certain patches of the scene surface, while others are empty. The camera is positioned at centers of those empty cubes (e.g., the purple and blue cubes); for each positioned camera, we randomly sample 100 viewpoints for the scanning. We include all the five challenges of practical scanning into each scanning of the scene surface, whose individual implementations are similar to those for synthetic object scanning.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3：构建我们的合成场景级数据集的流程（详细信息见第4.2节）。我们从三个数据集中收集室内场景的CAD表面模型，如图左所示，并通过在3D空间中以原点为中心进行归一化。我们将3D空间划分为重叠的立方体，如图右所示；其中一些立方体包含场景表面的某些区域，而其他则为空。相机位于这些空立方体的中心（例如，紫色和蓝色立方体）；对于每个放置的相机，我们随机采样100个视点进行扫描。我们在每次场景表面的扫描中包含了实际扫描的所有五个挑战，其各自的实现与合成物体扫描的实现类似。</p></div><!-- Media --><p>Real scanning - In general, better scanning results could be obtained by increasing the numbers of scanning shots from multiple viewpoints, as empirically verified in Fig. 4. Since qualities of the scanned point clouds start to saturate at around 40 shots for Ein-scan SE and 20 shots for OKIO \(5\mathrm{M}\) ,we conduct 40 shots for Ein-scan SE and 20 shots for OKIO \(5\mathrm{M}\) shots when scanning an object. After scanning an object, we use CloudCompare [131] to align different point clouds obtained from different shots. Table 4 gives the statistics of our real-scanned data in the benchmark. Some scanned pairs from the two scanners are visualized in Fig. 6.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>实际扫描 - 一般来说，通过从多个视点增加扫描次数可以获得更好的扫描结果，这在图4中得到了经验验证。由于Einscan SE的扫描点云质量在大约40次扫描和OKIO \(5\mathrm{M}\)的20次扫描时开始饱和，因此在扫描物体时，我们对Einscan SE进行40次扫描，对OKIO \(5\mathrm{M}\)进行20次扫描。扫描物体后，我们使用CloudCompare [131]对从不同拍摄中获得的不同点云进行对齐。表4给出了我们基准中实际扫描数据的统计信息。图6展示了来自两台扫描仪的一些扫描对。</p></div><!-- Media --><!-- figureText: 1.0 Einscan SE OKIO 5M 60 0.8 0.6 0.4 0.2 20 --><img src="https://cdn.noedgeai.com/bo_d163ugjef24c73d1leb0_10.jpg?x=573&#x26;y=938&#x26;w=286&#x26;h=270&#x26;r=0"><p>Fig. 4: The relationship between scan quality and scan shots.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图4：扫描质量与扫描次数之间的关系。</p></div><!-- figureText: a BUC --><img src="https://cdn.noedgeai.com/bo_d163ugjef24c73d1leb0_10.jpg?x=231&#x26;y=1526&#x26;w=552&#x26;h=542&#x26;r=0"><p>Fig. 5: An illustration of the objects used in our real-scanned dataset. The objects are organized by the types of material, where "a" shows the objects made of plastic, "b" for ceramic, "c" for cloth, and "d" for metal.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图5：我们实际扫描数据集中使用的物体的示例。物体按材料类型组织，其中“a”表示塑料制成的物体，“b”表示陶瓷，“c”表示布料，“d”表示金属。</p></div><p>TABLE 4: Statistics of real-scanned data. # denotes the number.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表4：实际扫描数据的统计信息。#表示数量。</p></div><table><tbody><tr><td colspan="2"></td><td>Einscan SE</td><td>OKIO 5M</td></tr><tr><td colspan="2">precision</td><td>100 micrometers</td><td>5 micrometers</td></tr><tr><td colspan="2">professional operators</td><td>✘</td><td>✓</td></tr><tr><td colspan="2">#resolution</td><td>\( {1300k} \)</td><td>\( {5000k} \)</td></tr><tr><td colspan="2">#viewpoints</td><td>30-50</td><td>13-35</td></tr><tr><td colspan="2">#scanned points</td><td>210k-3000k</td><td>330k-2000k</td></tr><tr><td colspan="2">#surfaces</td><td colspan="2">20</td></tr><tr><td rowspan="4">#material</td><td rowspan="4">metal plastic ceramic cloth</td><td colspan="2">4</td></tr><tr><td colspan="2">9</td></tr><tr><td colspan="2">5</td></tr><tr><td colspan="2">2</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td colspan="2"></td><td>爱因斯坎 SE</td><td>OKIO 5M</td></tr><tr><td colspan="2">精度</td><td>100 微米</td><td>5 微米</td></tr><tr><td colspan="2">专业操作员</td><td>✘</td><td>✓</td></tr><tr><td colspan="2">#分辨率</td><td>\( {1300k} \)</td><td>\( {5000k} \)</td></tr><tr><td colspan="2">#视角</td><td>30-50</td><td>13-35</td></tr><tr><td colspan="2">#扫描点</td><td>210k-3000k</td><td>330k-2000k</td></tr><tr><td colspan="2">#表面</td><td colspan="2">20</td></tr><tr><td rowspan="4">#材料</td><td rowspan="4">金属 塑料 陶瓷 布料</td><td colspan="2">4</td></tr><tr><td colspan="2">9</td></tr><tr><td colspan="2">5</td></tr><tr><td colspan="2">2</td></tr></tbody></table></div><!-- figureText: Einscan SE OKIO 5M --><img src="https://cdn.noedgeai.com/bo_d163ugjef24c73d1leb0_10.jpg?x=975&#x26;y=1286&#x26;w=614&#x26;h=479&#x26;r=0"><p>Fig. 6: Examples of real-scanned object pairs.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6：真实扫描物体对的示例。</p></div><!-- Media --><h2>5 Experimental Set-up for Benchmarking Exist- ing Surface Reconstruction Methods</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>5 用于基准测试现有表面重建方法的实验设置</h2></div><p>With the benchmark prepared in Section 4, we aim to empirically compare existing surface reconstruction methods, and identify their advantages and disadvantages in different working conditions (e.g., various scanning imperfections considered in our benchmark). We expect such studies would both provide insights for future research in this area, and guide the use of appropriate methods for practical surface reconstruction from point observations. Such a comprehensive investigation could be timely for the community, given that a plethora of new methods have been proposed recently, in particular those based on deep learning.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在第4节准备的基准测试下，我们旨在实证比较现有的表面重建方法，并识别它们在不同工作条件下的优缺点（例如，我们基准测试中考虑的各种扫描缺陷）。我们期望这样的研究不仅能为该领域未来的研究提供见解，还能指导在实际表面重建中使用适当的方法。考虑到最近提出了大量新方法，特别是基于深度学习的方法，这样的全面调查对社区来说是及时的。</p></div><h3>5.1 Data</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.1 数据</h3></div><p>We conduct the empirical studies using a subset of our collected benchmark, while publicly releasing the whole benchmark to facilitate the community research. More specifically, we randomly sample 22 synthetic surfaces of object instances, whose distribution in the three groups of low-, middle-, and high-complexity is \({12} : 6 : 4\) ; as described in Section 4.1,for each instance we conduct six ways of synthetic scanning, which produce either a clean point cloud or point clouds with various imperfections, giving rise to a total of 308 input-output pairs for benchmarking algorithms. We also use all the 10 synthetic scene surfaces and all the 20 real-scanned surfaces for the studies. For some of existing methods using learning-based priors, we prepare an auxiliary set of training data consisting of surfaces from ShapeNet [17] and ABC [20], and also the remaining synthetic data of object instances from our benchmark.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们使用收集的基准测试的一个子集进行实证研究，同时公开发布整个基准测试以促进社区研究。更具体地说，我们随机抽取22个合成表面对象实例，其在低、中、高复杂度三组中的分布为\({12} : 6 : 4\)；如第4.1节所述，对于每个实例，我们进行六种合成扫描方式，产生干净的点云或具有各种缺陷的点云，从而产生总共308个输入输出对用于基准算法。我们还使用所有10个合成场景表面和所有20个真实扫描表面进行研究。对于一些使用基于学习的先验的现有方法，我们准备了一组辅助训练数据，包括来自ShapeNet [17]和ABC [20]的表面，以及我们基准测试中剩余的合成对象实例数据。</p></div><h3>5.2 Pre-processing</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.2 预处理</h3></div><p>While existing surface reconstruction methods can be compared using the data prepared in Section 5.1, for most of them, their performance could be greatly improved via some standard pipeline of point cloud pre-processing. In this work, we compare existing methods both without and with such a pre-processing pipeline. For synthetic data, the pipeline is in the order of outlier removal, de-noising, and point re-sampling; details are given as follows. For real-scanned data, we use the inbuilt pre-processing of different scanners,and use a final step of FPS to re-sample 200,000 points for each point cloud.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>虽然可以使用第5.1节准备的数据比较现有的表面重建方法，但对于大多数方法，通过一些标准的点云预处理流程可以大大提高其性能。在这项工作中，我们比较了现有方法在没有和有这样的预处理流程下的表现。对于合成数据，预处理流程的顺序为去除离群点、去噪和点重采样；具体细节如下。对于真实扫描数据，我们使用不同扫描仪的内置预处理，并使用最终步骤FPS对每个点云重新采样200,000个点。</p></div><p>Outlier removal - Performance of surface reconstruction degrades severely when extreme outliers exist in a point cloud; fortunately, these outliers are easy to be removed. We use a statistical method [132] to remove extreme point outliers. For a point cloud \(\mathcal{P}\) ,it regards any \(\mathbf{p} \in  \mathcal{P}\) as an outlier and remove it when \(p\) is very far away from its local neighborhood. More precisely,for any \(\mathbf{p} \in  \mathcal{P}\) ,we first compute the averaged distance \({\bar{d}}_{\mathbf{p}}\) between \(\mathbf{p}\) and its \(k\) nearest neighbors in \(\mathcal{P}\) ; we then compute the mean \({m}_{\bar{d}}\) and standard deviation \({\sigma }_{\bar{d}}\) of such distances for all \(\{ \mathbf{p} \in  \mathcal{P}\}\) ; a point \(\mathbf{p}\) is regarded as an outlier when its corresponding \({\bar{d}}_{\mathbf{p}} > 5 \cdot  {\sigma }_{\bar{d}}\) . We set \(k = {35}\) in this work for outlier removal.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>离群点去除 - 当点云中存在极端离群点时，表面重建的性能会严重下降；幸运的是，这些离群点很容易被去除。我们使用统计方法[132]去除极端点离群点。对于点云\(\mathcal{P}\)，当\(p\)远离其局部邻域时，任何\(\mathbf{p} \in  \mathcal{P}\)都被视为离群点并被去除。更准确地说，对于任何\(\mathbf{p} \in  \mathcal{P}\)，我们首先计算\(\mathbf{p}\)与其在\(\mathcal{P}\)中的\(k\)个最近邻之间的平均距离\({\bar{d}}_{\mathbf{p}}\)；然后计算所有\(\{ \mathbf{p} \in  \mathcal{P}\}\)的这些距离的均值\({m}_{\bar{d}}\)和标准差\({\sigma }_{\bar{d}}\)；当其对应的\({\bar{d}}_{\mathbf{p}} > 5 \cdot  {\sigma }_{\bar{d}}\)时，点\(\mathbf{p}\)被视为离群点。我们在这项工作中设置\(k = {35}\)用于离群点去除。</p></div><p>De-noising - The inevitable existence of point-wise noise influences surface reconstruction as well. For an input point cloud \(\mathcal{P}\) ,we choose to suppress such noise using Jets smoothing [133], which smoothes out the point cloud without sacrificing its surface curvatures. It works by first fitting a parametric surface patch to a local neighborhood \(\mathcal{N}\) of \(k\) points in \(\mathcal{P}\) ,and then projecting \(\{ \mathbf{p} \in  \mathcal{N}\}\) onto the fitted surface patch. We set \(k = {18}\) in this work for point-wise de-noising.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>去噪 - 点噪声的不可避免存在也会影响表面重建。对于输入点云\(\mathcal{P}\)，我们选择使用Jets平滑[133]来抑制这种噪声，该方法在不牺牲表面曲率的情况下平滑点云。它的工作原理是首先为点云\(\mathcal{P}\)中的局部邻域\(\mathcal{N}\)的\(k\)个点拟合一个参数化的表面补丁，然后将\(\{ \mathbf{p} \in  \mathcal{N}\}\)投影到拟合的表面补丁上。我们在这项工作中设置\(k = {18}\)用于点去噪。</p></div><p>Point re-sampling - Empirical results show that surface reconstruction benefits from more uniform distribution of points, even when reducing the number of points contained in \(\mathcal{P}\) [134]. For the synthetic data of object or scene surfaces, we simply use farthest point sampling [99] as the method to re-sample a more uniform distribution of points; we preserve \({40}\%\) of original points during re-sampling.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>点重采样 - 实证结果表明，表面重建受益于点的更均匀分布，即使在减少点云\(\mathcal{P}\)中包含的点数时[134]。对于对象或场景表面的合成数据，我们简单地使用最远点采样[99]作为重采样更均匀分布点的方法；在重采样过程中，我们保留原始点的\({40}\%\)。</p></div><h3>5.3 Evaluation Metrics</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.3 评估指标</h3></div><p>We quantitatively compare reconstruction results from different methods using the popular metrics of Chamfer Distance (CD) [135], F-score [136], and Normal Consistency Score (NCS) [9]; Appendix B specifies their computations. We also propose a neural metric, termed Neural Feature Similarity (NFS), focusing on perceptual consistency between each reconstruction and the ground-truth; intuitively speaking, NFS compares the similarity of two shapes in the deep feature space, and thus depends more on the high-level semantic information consistent with human perception [137]; details are given in Appendix B as well.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们定量比较了不同方法的重建结果，使用了流行的度量标准，包括Chamfer距离（CD）[135]、F-score [136]和法线一致性得分（NCS）[9]；附录B详细说明了它们的计算方法。我们还提出了一种神经度量，称为神经特征相似性（NFS），重点关注每个重建与真实值之间的感知一致性；直观地说，NFS比较了深度特征空间中两个形状的相似性，因此更依赖于与人类感知一致的高层语义信息[137]；详细信息也在附录B中给出。</p></div><p>CD and F-score are used for measuring the overall similarity between two shapes; NCS is more useful for measuring the nuance of two similar shapes by measuring their consistency of surface normals; NFS measures semantic difference related to human perception.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>CD和F-score用于测量两个形状之间的整体相似性；NCS更适合通过测量表面法线的一致性来衡量两个相似形状的细微差别；NFS测量与人类感知相关的语义差异。</p></div><h3>5.4 Methods and Implementation Details</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.4 方法和实施细节</h3></div><p>In Section 3.5, we have categorized existing methods according to what geometric priors they have respectively used to regularize the reconstruction. It is less feasible to study and empirically compare all the existing methods; instead, we take the strategy of selecting representative ones from each method group of geometric priors, assuming that our studies and conclusions would generalize in the same groups of existing methods. More specifically, we adopt the most representative Greedy Delaunay (GD) [33] and BPA [34] as the methods to be studied for triangulation-based prior; for priors of surface smoothness, we adopt SPSR [36] using the first manner of surface smoothness (cf. Eq. (9)) and RIMLS [53] using both two manners of surface smoothness (cf. Eq. (8) and constraining \(\left. {\mathcal{H}}_{f}\right)\) ; for modeling priors,we adopt SALD [73] that is able to reconstruct surfaces from un-oriented point clouds, and IGR [71] that can do so from oriented ones; for learning-based priors, we adopt the global, semantic learning methods of OccNet [9] and DeepSDF [8], and also the local learning methods of LIG [10] and Points2Surf [78]; we consider three methods that use hybrid priors, including Delaunay Surface Elements (DSE) [114] that combines triangulation-based prior with learning-based prior, IMLSNet [115] that combines surface smoothness prior with learning-based prior, and ParseNet [40] that combines template-based prior with learning-based prior; we do not consider methods using template-based priors only, given that their performance largely depends on whether there would exist a good match between a surface to be reconstructed and the assumed templates (e.g., the assumed geometric primitives or the templates that can be retrieved in an auxiliary dataset), and that even the one with learning-based templates [40] fail to reconstruct surfaces of certain complexities.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在第3.5节中，我们根据现有方法所使用的几何先验对其进行了分类，以规范重建。研究和经验比较所有现有方法的可行性较低；相反，我们采取从每个几何先验方法组中选择代表性方法的策略，假设我们的研究和结论可以在同一组现有方法中推广。更具体地说，我们采用最具代表性的贪婪德劳内（GD）[33]和BPA [34]作为基于三角剖分的先验研究方法；对于表面光滑度的先验，我们采用SPSR [36]，使用第一种表面光滑度方法（参见公式（9））和RIMLS [53]，使用两种表面光滑度方法（参见公式（8）并约束\(\left. {\mathcal{H}}_{f}\right)\)）；对于建模先验，我们采用SALD [73]，能够从无定向点云重建表面，以及IGR [71]，能够从有定向点云重建；对于基于学习的先验，我们采用OccNet [9]和DeepSDF [8]的全局语义学习方法，以及LIG [10]和Points2Surf [78]的局部学习方法；我们考虑三种使用混合先验的方法，包括将基于三角剖分的先验与基于学习的先验相结合的德劳内表面元素（DSE）[114]，将表面光滑度先验与基于学习的先验相结合的IMLSNet [115]，以及将基于模板的先验与基于学习的先验相结合的ParseNet [40]；我们不考虑仅使用基于模板的先验的方法，因为它们的性能在很大程度上取决于待重建表面与假定模板（例如，假定的几何原语或可以在辅助数据集中检索的模板）之间是否存在良好的匹配，即使是使用基于学习的模板[40]的方法也无法重建某些复杂性的表面。</p></div><p>Our implementations of the more classical, learning-free methods are based on established libraries; for example, we implement GD using the CGAL library [138], and directly execute BPA [34], SPSR [36], and RIMLS [53] with proper parameter tunings in MeshLab [139]. For those learning-based methods, we use the implementation codes publicly released by the authors when they are available, again with proper tuning of hyper-parameters for individual surfaces to be reconstructed. For those without code releasing, we re-implement their algorithms and tune the respective hyper-parameters as the optimal ones.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们对更经典的无学习方法的实现基于已建立的库；例如，我们使用CGAL库[138]实现GD，并在MeshLab [139]中直接执行BPA [34]、SPSR [36]和RIMLS [53]，并进行适当的参数调优。对于那些基于学习的方法，我们在可用时使用作者公开发布的实现代码，同样对待重建的各个表面进行超参数的适当调优。对于那些没有代码发布的方法，我们重新实现其算法并调优相应的超参数，以达到最佳效果。</p></div><p>In Section 6, we report and discuss the results that are obtained with the pre-processing mentioned in Section 5.2. We note that those without pre-processing are of similar comparative qualities, and we put them in Appendix E.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在第6节中，我们报告并讨论了在第5.2节中提到的预处理所获得的结果。我们注意到那些没有预处理的结果具有相似的比较质量，我们将它们放在附录E中。</p></div><h2>6 Main Results</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>6 主要结果</h2></div><p>We first summarize our key empirical findings, before presenting details of our series of experiments; insights are drawn subsequently.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们首先总结我们的关键实证发现，然后呈现我们一系列实验的细节；随后得出见解。</p></div><ul>
<li>While many challenges of surface reconstruction from point clouds can be more or less tackled by using different regularization/priors of surface geometry, the challenges of misalignment, missing points, and outliers have been less addressed and remain unsolved.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>尽管使用不同的表面几何正则化/先验可以或多或少地解决点云表面重建的许多挑战，但对错位、缺失点和离群点的挑战则较少被解决，仍然未得到解决。</li>
</ul></div><ul>
<li>Data-driven solutions using deep learning have recently shown great promise for surface modeling and reconstruction, including their potential to deal with various data imperfections, however, our systematic experiments suggest that they struggle in generalizing to reconstruction of complex shapes; it is surprising that some classical methods such as SPSR [36] perform even better in terms of generalization and robustness.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>最近，基于数据驱动的深度学习解决方案在表面建模和重建方面显示出巨大的潜力，包括处理各种数据缺陷的潜力，然而，我们的系统实验表明，它们在推广复杂形状的重建方面存在困难；令人惊讶的是，一些经典方法如SPSR [36]在推广性和鲁棒性方面表现得更好。</li>
</ul></div><ul>
<li>Use of surface normals is a key to success of surface reconstruction from raw, observed point clouds, even when the surface normals are estimated less accurately; in many cases, the reconstruction result improves as long as the interior and exterior of a surface can be identified in the 3D space.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>使用表面法线是从原始观察点云中成功进行表面重建的关键，即使表面法线的估计不够准确；在许多情况下，只要能够在3D空间中识别表面的内部和外部，重建结果就会有所改善。</li>
</ul></div><ul>
<li>There exist inconsistencies between different evaluation metrics, and in many cases, good quantitative results do not translate as visually pleasant ones. For example, quantitative results measured by CD and F-score are not much affected by the challenge of misalignment; however, the reduced scores of NCS and NFS suggest that the recovered surfaces might be less pleasant to human perception.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>不同评估指标之间存在不一致，在许多情况下，良好的定量结果并不转化为视觉上令人愉悦的结果。例如，通过CD和F-score测量的定量结果并未受到错位挑战的太大影响；然而，NCS和NFS的降低分数表明，恢复的表面可能对人类感知不太愉悦。</li>
</ul></div><p>Quantitative results of comparative methods are given in Tables 5, 6, and 7, which are respectively for synthetic data of object surfaces, synthetic data of scene surfaces, and real-scanned data. Qualitative results are presented in the following sections accompanying our discussions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>比较方法的定量结果见表5、6和7，分别对应物体表面的合成数据、场景表面的合成数据和真实扫描数据。定性结果将在随后的章节中与我们的讨论一起呈现。</p></div><h3>6.1 The Remaining Challenges</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.1 剩余挑战</h3></div><p>For ease of analysis, we plot in Fig. 7 the quantitative results in Table 5 under the metrics of CD, F-score, NCS, and NFS. As presented in Section 5.3, the four metrics focus on different measure perspectives. By diagnosing the comparative methods using these measures and investigating their capabilities to cope with different challenges of imperfect scanning, Fig. 7 helps in identifying the remaining challenges.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了便于分析，我们在图7中绘制了表5中CD、F-score、NCS和NFS指标下的定量结果。如第5.3节所述，这四个指标关注不同的测量视角。通过使用这些指标诊断比较方法并调查它们应对不完美扫描的不同挑战的能力，图7有助于识别剩余的挑战。</p></div><p>Fig. 7 shows that, under all the metrics, the challenge of nonuniform distribution of points is relatively easy to be tackled by almost all the methods, except those learning semantics or geometric primitives (i.e., DeepSDF [8], OccNet [9], and ParseNet [40]), achieving similar results as those on data of perfect scanning. The discussion on why some learning-based methods fail to generalize is given in Section 6.2. For the challenge of point-wise noise, though the overall shape structures (measured by CD and F-Score) can be roughly recovered by most of the methods (again, except some learning-based ones), the reconstructions might be short of surface details, as verified by the reduced scores of NCS and NFS, especially for triangulation-based methods such as GD [33], BPA [34], and the hybrid one of DSE [114] that combines the triangulation-based prior. This is intuitive since methods based on triangulation of points rely heavily on cleanness of input points.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图7显示，在所有指标下，点的非均匀分布挑战几乎被所有方法相对容易地解决，除了那些学习语义或几何原语的方法（即DeepSDF [8]、OccNet [9]和ParseNet [40]），其结果与完美扫描数据的结果相似。关于一些基于学习的方法为何无法泛化的讨论见第6.2节。对于逐点噪声的挑战，尽管大多数方法（再次，除了某些基于学习的方法）可以大致恢复整体形状结构（通过CD和F-Score测量），但重建可能缺乏表面细节，正如NCS和NFS的降低分数所验证的，特别是对于基于三角剖分的方法，如GD [33]、BPA [34]和结合了基于三角剖分先验的混合方法DSE [114]。这是直观的，因为基于点的三角剖分的方法在很大程度上依赖于输入点的干净程度。</p></div><!-- Media --><!-- figureText: BPA GE BPA SALD IGR DeepSDF 20 DSE IMLSNet ParseNet Perfec Non-uniforn Msalignmen (b) F-score 90 80 BPA 70 SPSF SALD DeepSDF LIG Points2Sur 30 20 ParseNet Perfec Non-uniform Noist Misalignment Missing Outlier: (d) NFS 400 RIMLS IGR 300 DeepSDI LiG 200 DSE MLSNet 100 Perfect Non-uniform Misalignment Missing Outliers (a) CD 95 SPSR SALD DeepSDF Points2Sur ParseNel Perfect Non-uniform Noise Misalignment Missing Outliers (c) NCS --><img src="https://cdn.noedgeai.com/bo_d163ugjef24c73d1leb0_12.jpg?x=920&#x26;y=126&#x26;w=733&#x26;h=623&#x26;r=0"><p>Fig. 7: Plotting of quantitative results in Table 5.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图7：表5中定量结果的绘制。</p></div><!-- Media --><p>There exists a similar but severer phenomenon for the challenge of misalignment. As indicated by Fig. 7, when measured by CD and F-score, most of the methods give reasonably good results, suggesting that the overall shape structures have been recovered. However, the reduced scores of NCS and NFS suggest that some of the recovered surfaces might be less pleasant to human perception. In fact, as shown by the example in Fig. 8, most methods fail in reconstructing the surface on the misaligned areas, generating thickened or even multiple layers of the local surface. Misalignment is a practical issue in 3D scanning, especially when using hand-held, consumer scanners. Fig. 7 and Fig. 8 show that methods using smoothness and/or modeling priors have the advantage in handling misalignment.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>对于错位挑战，存在类似但更严重的现象。如图7所示，当通过CD和F-score测量时，大多数方法给出了合理的好结果，表明整体形状结构已被恢复。然而，NCS和NFS的降低分数表明，某些恢复的表面可能对人类感知不太愉悦。实际上，如图8中的例子所示，大多数方法在重建错位区域的表面时失败，生成了加厚或甚至多层的局部表面。错位是3D扫描中的一个实际问题，尤其是在使用手持消费级扫描仪时。图7和图8显示，使用平滑性和/或建模先验的方法在处理错位方面具有优势。</p></div><p>Fig. 7 also shows that the two challenges of missing points and point outliers are much more difficult to be handled. When an input point cloud has missing points, as shown in Fig. 9, most of the methods ignore reconstruction of the missing surface areas, resulting in incomplete surfaces. The implicit methods (e.g., IGR [71], OccNet [9], and Points2Surf [78]) tend to generate watertight surfaces by filling the holes with concave/convex hulls; however, such envelopes may not represent the surface correctly, possibly making the reconstruction even poorer under all the evaluation metrics (cf. Fig. 7).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图7还显示，缺失点和点异常值这两个挑战更难以处理。当输入点云存在缺失点时，如图9所示，大多数方法忽略了对缺失表面区域的重建，导致表面不完整。隐式方法（例如IGR [71]、OccNet [9]和Points2Surf [78]）倾向于通过用凹/凸包填补孔洞来生成密闭表面；然而，这种包络可能无法正确表示表面，可能使重建在所有评估指标下变得更差（参见图7）。</p></div><p>As for point outliers, although a pre-processing step of outlier removal has already been adopted for all the comparative methods (cf. Section 5.2), performance of different methods still varies drastically. Fig. 10 gives an example; the results depend on whether the respective methods have their inbuilt mechanisms of outlier removal. For example, BPA [34] requires the sizes of its triangular faces to satisfy certain conditions, making it naturally suitable for handling outliers; methods using a global implicit field (e.g., SPSR [48] and IGR [71]) ignore the outliers implicitly, and are thus capable of handling point outliers as well.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>至于点异常值，尽管所有比较方法已经采用了异常值移除的预处理步骤（参见第5.2节），但不同方法的性能仍然差异很大。图10给出了一个例子；结果取决于各自方法是否具有内置的异常值移除机制。例如，BPA [34]要求其三角面片的大小满足某些条件，使其自然适合处理异常值；使用全局隐式场的方法（例如SPSR [48]和IGR [71]）隐式地忽略异常值，因此也能够处理点异常值。</p></div><p>Summarizing the above analyses gives us the following empirical findings: (1) the challenge of missing points remains unsolved by all the comparative methods; (2) for the challenges of misalignment and point outliers, most of the methods (except few ones such as SPSR [48]) give unsatisfactory results; (3) there exist inconsistencies between different evaluation metrics, and in many cases, good quantitative results do not translate as visually pleasant ones; (4) methods that learn semantics or pre-defined shape patterns may fail to generalize even on clean data of perfect scanning, when the testing point clouds do not fall in the learned data domains; we will discuss more on this issue shortly.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>总结上述分析，我们得出以下经验发现：（1）缺失点的挑战仍未被所有比较方法解决；（2）对于错位和点异常值的挑战，大多数方法（除了少数如SPSR [48]）给出了不满意的结果；（3）不同评估指标之间存在不一致，在许多情况下，良好的定量结果并不转化为视觉上令人愉悦的结果；（4）学习语义或预定义形状模式的方法可能在干净的完美扫描数据上也无法泛化，当测试点云不落在学习的数据域时；我们将很快对此问题进行更多讨论。</p></div><!-- Media --><p>TABLE 5: Quantitative results on the testing synthetic data of object surfaces. Comparisons are made on data of perfect scanning and those of all the five challenges of imperfect scanning specified in Section 4.1.3; for those challenges with varying levels of severity, we use the data of middle-level severity for the comparison (cf. Appendix C for the overall results). Results of the best and second best methods are highlighted in each column. Comparative methods are also grouped according to what priors of surface geometry they have used (cf. Section 3 for the grouping and Section 5.4 for how these representative methods are selected).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表5：对象表面测试合成数据的定量结果。比较了完美扫描的数据和第4.1.3节中指定的五个不完美扫描挑战的数据；对于那些具有不同严重程度的挑战，我们使用中等严重程度的数据进行比较（参见附录C以获取总体结果）。每一列中最佳和第二最佳方法的结果被突出显示。比较方法也根据它们使用的表面几何先验进行分组（参见第3节以获取分组信息，第5.4节以了解这些代表性方法的选择方式）。</p></div><table><tbody><tr><td rowspan="2">Prior</td><td rowspan="2">Method</td><td colspan="6">CD \( \left( {\times {10}^{-4}}\right)  \downarrow \)</td><td colspan="6">F-score (%) ↑</td></tr><tr><td>Perfect scanning</td><td>Non-uniform distribution</td><td>Point-wise noise</td><td>Point outliers</td><td>Missing points</td><td>Mis- alignment</td><td>Perfect scanning</td><td>Non-uniform distribution</td><td>Point-wise noise</td><td>Point outliers</td><td>Missing points</td><td>Mis- alignment</td></tr><tr><td rowspan="2">Triangulation- based</td><td>GD [33]</td><td>14.24</td><td>14.87</td><td>18.20</td><td>123.19</td><td>64.15</td><td>21.15</td><td>99.66</td><td>99.07</td><td>98.91</td><td>54.59</td><td>87.72</td><td>97.41</td></tr><tr><td>BPA [34]</td><td>14.89</td><td>17.13</td><td>18.58</td><td>14.66</td><td>62.55</td><td>20.88</td><td>98.70</td><td>97.02</td><td>98.51</td><td>99.31</td><td>87.85</td><td>97.24</td></tr><tr><td rowspan="2">Smoothness</td><td>SPSR [36]</td><td>14.47</td><td>15.36</td><td>16.05</td><td>14.71</td><td>225.66</td><td>17.24</td><td>99.59</td><td>99.02</td><td>99.46</td><td>99.65</td><td>76.91</td><td>99.27</td></tr><tr><td>RIMLS [53]</td><td>15.73</td><td>16.74</td><td>17.17</td><td>126.40</td><td>65.36</td><td>21.12</td><td>99.27</td><td>98.76</td><td>99.24</td><td>57.78</td><td>87.92</td><td>97.53</td></tr><tr><td rowspan="2">Modeling</td><td>SALD [73]</td><td>15.10</td><td>14.96</td><td>18.77</td><td>53.65</td><td>55.63</td><td>20.09</td><td>99.45</td><td>99.10</td><td>98.94</td><td>88.05</td><td>85.78</td><td>98.09</td></tr><tr><td>IGR [71]</td><td>18.40</td><td>18.33</td><td>18.57</td><td>43.60</td><td>151.53</td><td>20.72</td><td>97.54</td><td>97.79</td><td>97.75</td><td>91.11</td><td>70.49</td><td>96.46</td></tr><tr><td rowspan="2">Learning Semantics</td><td>OccNet [9]</td><td>201.96</td><td>210.80</td><td>205.21</td><td>225.85</td><td>231.65</td><td>212.51</td><td>31.03</td><td>29.90</td><td>29.77</td><td>23.75</td><td>24.52</td><td>29.19</td></tr><tr><td>DeepSDF [8]</td><td>229.18</td><td>227.42</td><td>230.40</td><td>511.36</td><td>378.58</td><td>232.16</td><td>17.79</td><td>18.81</td><td>17.08</td><td>4.15</td><td>14.06</td><td>17.05</td></tr><tr><td>Local</td><td>LIG [10]</td><td>23.09</td><td>24.03</td><td>22.05</td><td>115.38</td><td>70.98</td><td>24.30</td><td>96.20</td><td>95.32</td><td>96.99</td><td>76.69</td><td>82.43</td><td>95.01</td></tr><tr><td>Learning</td><td>Points2Surf [78]</td><td>17.18</td><td>18.81</td><td>18.48</td><td>83.91</td><td>102.18</td><td>20.36</td><td>98.14</td><td>97.72</td><td>97.39</td><td>72.94</td><td>76.46</td><td>96.49</td></tr><tr><td rowspan="3">Hybird</td><td>DSE [114]</td><td>14.26</td><td>15.34</td><td>17.89</td><td>100.37</td><td>68.88</td><td>20.06</td><td>99.64</td><td>98.84</td><td>99.17</td><td>52.21</td><td>87.71</td><td>98.20</td></tr><tr><td>IMLSNet [115]</td><td>22.56</td><td>23.17</td><td>22.67</td><td>99.95</td><td>74.35</td><td>23.77</td><td>94.82</td><td>94.51</td><td>94.36</td><td>64.55</td><td>80.01</td><td>94.14</td></tr><tr><td>ParseNet [40]</td><td>162.94</td><td>161.14</td><td>135.84</td><td>176.38</td><td>195.98</td><td>136.86</td><td>40.52</td><td>38.82</td><td>44.13</td><td>37.21</td><td>41.28</td><td>46.60</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">先前</td><td rowspan="2">方法</td><td colspan="6">CD \( \left( {\times {10}^{-4}}\right)  \downarrow \)</td><td colspan="6">F-score (%) ↑</td></tr><tr><td>完美扫描</td><td>非均匀分布</td><td>逐点噪声</td><td>点异常值</td><td>缺失点</td><td>错位</td><td>完美扫描</td><td>非均匀分布</td><td>逐点噪声</td><td>点异常值</td><td>缺失点</td><td>错位</td></tr><tr><td rowspan="2">基于三角测量</td><td>GD [33]</td><td>14.24</td><td>14.87</td><td>18.20</td><td>123.19</td><td>64.15</td><td>21.15</td><td>99.66</td><td>99.07</td><td>98.91</td><td>54.59</td><td>87.72</td><td>97.41</td></tr><tr><td>BPA [34]</td><td>14.89</td><td>17.13</td><td>18.58</td><td>14.66</td><td>62.55</td><td>20.88</td><td>98.70</td><td>97.02</td><td>98.51</td><td>99.31</td><td>87.85</td><td>97.24</td></tr><tr><td rowspan="2">平滑性</td><td>SPSR [36]</td><td>14.47</td><td>15.36</td><td>16.05</td><td>14.71</td><td>225.66</td><td>17.24</td><td>99.59</td><td>99.02</td><td>99.46</td><td>99.65</td><td>76.91</td><td>99.27</td></tr><tr><td>RIMLS [53]</td><td>15.73</td><td>16.74</td><td>17.17</td><td>126.40</td><td>65.36</td><td>21.12</td><td>99.27</td><td>98.76</td><td>99.24</td><td>57.78</td><td>87.92</td><td>97.53</td></tr><tr><td rowspan="2">建模</td><td>SALD [73]</td><td>15.10</td><td>14.96</td><td>18.77</td><td>53.65</td><td>55.63</td><td>20.09</td><td>99.45</td><td>99.10</td><td>98.94</td><td>88.05</td><td>85.78</td><td>98.09</td></tr><tr><td>IGR [71]</td><td>18.40</td><td>18.33</td><td>18.57</td><td>43.60</td><td>151.53</td><td>20.72</td><td>97.54</td><td>97.79</td><td>97.75</td><td>91.11</td><td>70.49</td><td>96.46</td></tr><tr><td rowspan="2">学习语义</td><td>OccNet [9]</td><td>201.96</td><td>210.80</td><td>205.21</td><td>225.85</td><td>231.65</td><td>212.51</td><td>31.03</td><td>29.90</td><td>29.77</td><td>23.75</td><td>24.52</td><td>29.19</td></tr><tr><td>DeepSDF [8]</td><td>229.18</td><td>227.42</td><td>230.40</td><td>511.36</td><td>378.58</td><td>232.16</td><td>17.79</td><td>18.81</td><td>17.08</td><td>4.15</td><td>14.06</td><td>17.05</td></tr><tr><td>局部</td><td>LIG [10]</td><td>23.09</td><td>24.03</td><td>22.05</td><td>115.38</td><td>70.98</td><td>24.30</td><td>96.20</td><td>95.32</td><td>96.99</td><td>76.69</td><td>82.43</td><td>95.01</td></tr><tr><td>学习</td><td>Points2Surf [78]</td><td>17.18</td><td>18.81</td><td>18.48</td><td>83.91</td><td>102.18</td><td>20.36</td><td>98.14</td><td>97.72</td><td>97.39</td><td>72.94</td><td>76.46</td><td>96.49</td></tr><tr><td rowspan="3">混合</td><td>DSE [114]</td><td>14.26</td><td>15.34</td><td>17.89</td><td>100.37</td><td>68.88</td><td>20.06</td><td>99.64</td><td>98.84</td><td>99.17</td><td>52.21</td><td>87.71</td><td>98.20</td></tr><tr><td>IMLSNet [115]</td><td>22.56</td><td>23.17</td><td>22.67</td><td>99.95</td><td>74.35</td><td>23.77</td><td>94.82</td><td>94.51</td><td>94.36</td><td>64.55</td><td>80.01</td><td>94.14</td></tr><tr><td>ParseNet [40]</td><td>162.94</td><td>161.14</td><td>135.84</td><td>176.38</td><td>195.98</td><td>136.86</td><td>40.52</td><td>38.82</td><td>44.13</td><td>37.21</td><td>41.28</td><td>46.60</td></tr></tbody></table></div><table><tbody><tr><td rowspan="2">Prior</td><td rowspan="2">Method</td><td colspan="6">NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td colspan="6">NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>Perfect scanning</td><td>Non-uniform distribution</td><td>Point-wise noise</td><td>Point outliers</td><td>Missing points</td><td>Mis- alignment</td><td>Perfect scanning</td><td>Non-uniform distribution</td><td>Point-wise noise</td><td>Point outliers</td><td>Missing points</td><td>Mis- alignment</td></tr><tr><td rowspan="2">Triangulation- based</td><td>GD [33]</td><td>98.57</td><td>98.05</td><td>87.58</td><td>92.17</td><td>95.17</td><td>84.96</td><td>95.22</td><td>95.19</td><td>84.63</td><td>37.47</td><td>82.93</td><td>80.69</td></tr><tr><td>BPA [34]</td><td>98.37</td><td>97.89</td><td>91.68</td><td>98.07</td><td>94.42</td><td>90.12</td><td>94.10</td><td>93.60</td><td>87.49</td><td>93.24</td><td>80.36</td><td>81.96</td></tr><tr><td rowspan="2">Smoothness</td><td>SPSR [36]</td><td>98.58</td><td>98.38</td><td>97.03</td><td>98.56</td><td>89.99</td><td>96.24</td><td>96.38</td><td>96.22</td><td>94.98</td><td>96.31</td><td>69.34</td><td>94.28</td></tr><tr><td>RIMLS [53]</td><td>98.19</td><td>97.77</td><td>95.23</td><td>83.42</td><td>93.27</td><td>92.48</td><td>95.01</td><td>94.02</td><td>92.67</td><td>62.01</td><td>79.23</td><td>87.12</td></tr><tr><td rowspan="2">Modeling</td><td>SALD [73]</td><td>98.67</td><td>98.52</td><td>96.42</td><td>95.32</td><td>95.19</td><td>94.73</td><td>96.11</td><td>96.65</td><td>89.72</td><td>51.74</td><td>84.01</td><td>88.26</td></tr><tr><td>IGR [71]</td><td>97.62</td><td>97.59</td><td>97.52</td><td>96.47</td><td>90.61</td><td>97.04</td><td>94.71</td><td>94.22</td><td>94.52</td><td>84.63</td><td>68.14</td><td>92.54</td></tr><tr><td rowspan="2">Learning Semantics</td><td>OccNet [9]</td><td>79.55</td><td>79.30</td><td>79.64</td><td>78.55</td><td>78.43</td><td>79.58</td><td>47.33</td><td>46.55</td><td>46.03</td><td>42.11</td><td>42.46</td><td>45.80</td></tr><tr><td>DeepSDF [8]</td><td>78.65</td><td>79.11</td><td>77.78</td><td>73.40</td><td>74.52</td><td>78.12</td><td>39.94</td><td>40.91</td><td>39.29</td><td>16.65</td><td>31.59</td><td>39.26</td></tr><tr><td rowspan="2">Local Learning</td><td>LIG [10]</td><td>96.49</td><td>95.79</td><td>94.22</td><td>91.66</td><td>89.56</td><td>92.70</td><td>91.58</td><td>90.66</td><td>89.55</td><td>66.21</td><td>74.98</td><td>84.34</td></tr><tr><td>Points2Surf [78]</td><td>95.24</td><td>95.09</td><td>94.62</td><td>87.87</td><td>86.36</td><td>94.48</td><td>93.45</td><td>93.23</td><td>92.59</td><td>63.30</td><td>68.53</td><td>91.59</td></tr><tr><td rowspan="3">Hybird</td><td>DSE [114]</td><td>98.60</td><td>97.86</td><td>87.79</td><td>77.34</td><td>94.40</td><td>86.20</td><td>94.50</td><td>94.75</td><td>83.53</td><td>42.32</td><td>79.62</td><td>76.63</td></tr><tr><td>IMLSNet [115]</td><td>96.13</td><td>95.98</td><td>96.02</td><td>87.45</td><td>90.48</td><td>95.87</td><td>90.61</td><td>90.20</td><td>89.97</td><td>52.82</td><td>74.59</td><td>89.19</td></tr><tr><td>ParseNet [40]</td><td>77.71</td><td>76.89</td><td>80.31</td><td>75.46</td><td>75.83</td><td>80.48</td><td>38.54</td><td>37.71</td><td>49.30</td><td>35.98</td><td>38.40</td><td>45.73</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">先前</td><td rowspan="2">方法</td><td colspan="6">NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td colspan="6">NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>完美扫描</td><td>非均匀分布</td><td>逐点噪声</td><td>点异常值</td><td>缺失点</td><td>错位</td><td>完美扫描</td><td>非均匀分布</td><td>逐点噪声</td><td>点异常值</td><td>缺失点</td><td>错位</td></tr><tr><td rowspan="2">基于三角测量</td><td>GD [33]</td><td>98.57</td><td>98.05</td><td>87.58</td><td>92.17</td><td>95.17</td><td>84.96</td><td>95.22</td><td>95.19</td><td>84.63</td><td>37.47</td><td>82.93</td><td>80.69</td></tr><tr><td>BPA [34]</td><td>98.37</td><td>97.89</td><td>91.68</td><td>98.07</td><td>94.42</td><td>90.12</td><td>94.10</td><td>93.60</td><td>87.49</td><td>93.24</td><td>80.36</td><td>81.96</td></tr><tr><td rowspan="2">平滑性</td><td>SPSR [36]</td><td>98.58</td><td>98.38</td><td>97.03</td><td>98.56</td><td>89.99</td><td>96.24</td><td>96.38</td><td>96.22</td><td>94.98</td><td>96.31</td><td>69.34</td><td>94.28</td></tr><tr><td>RIMLS [53]</td><td>98.19</td><td>97.77</td><td>95.23</td><td>83.42</td><td>93.27</td><td>92.48</td><td>95.01</td><td>94.02</td><td>92.67</td><td>62.01</td><td>79.23</td><td>87.12</td></tr><tr><td rowspan="2">建模</td><td>SALD [73]</td><td>98.67</td><td>98.52</td><td>96.42</td><td>95.32</td><td>95.19</td><td>94.73</td><td>96.11</td><td>96.65</td><td>89.72</td><td>51.74</td><td>84.01</td><td>88.26</td></tr><tr><td>IGR [71]</td><td>97.62</td><td>97.59</td><td>97.52</td><td>96.47</td><td>90.61</td><td>97.04</td><td>94.71</td><td>94.22</td><td>94.52</td><td>84.63</td><td>68.14</td><td>92.54</td></tr><tr><td rowspan="2">学习语义</td><td>OccNet [9]</td><td>79.55</td><td>79.30</td><td>79.64</td><td>78.55</td><td>78.43</td><td>79.58</td><td>47.33</td><td>46.55</td><td>46.03</td><td>42.11</td><td>42.46</td><td>45.80</td></tr><tr><td>DeepSDF [8]</td><td>78.65</td><td>79.11</td><td>77.78</td><td>73.40</td><td>74.52</td><td>78.12</td><td>39.94</td><td>40.91</td><td>39.29</td><td>16.65</td><td>31.59</td><td>39.26</td></tr><tr><td rowspan="2">局部学习</td><td>LIG [10]</td><td>96.49</td><td>95.79</td><td>94.22</td><td>91.66</td><td>89.56</td><td>92.70</td><td>91.58</td><td>90.66</td><td>89.55</td><td>66.21</td><td>74.98</td><td>84.34</td></tr><tr><td>Points2Surf [78]</td><td>95.24</td><td>95.09</td><td>94.62</td><td>87.87</td><td>86.36</td><td>94.48</td><td>93.45</td><td>93.23</td><td>92.59</td><td>63.30</td><td>68.53</td><td>91.59</td></tr><tr><td rowspan="3">混合</td><td>DSE [114]</td><td>98.60</td><td>97.86</td><td>87.79</td><td>77.34</td><td>94.40</td><td>86.20</td><td>94.50</td><td>94.75</td><td>83.53</td><td>42.32</td><td>79.62</td><td>76.63</td></tr><tr><td>IMLSNet [115]</td><td>96.13</td><td>95.98</td><td>96.02</td><td>87.45</td><td>90.48</td><td>95.87</td><td>90.61</td><td>90.20</td><td>89.97</td><td>52.82</td><td>74.59</td><td>89.19</td></tr><tr><td>ParseNet [40]</td><td>77.71</td><td>76.89</td><td>80.31</td><td>75.46</td><td>75.83</td><td>80.48</td><td>38.54</td><td>37.71</td><td>49.30</td><td>35.98</td><td>38.40</td><td>45.73</td></tr></tbody></table></div><!-- figureText: Input PC GT GD [33] BPA [34] SPSR [36] RIMLS [53] SALD [73] IGR [71] DSE [114] IMLSNet [115] ParseNet [40] View Direction SPSR [36] RIMLS [53] SALD [73] IGR [71] DSE [114] IMLSNet [115] ParseNet [40] GT Fig. 9: An example of qualitative results from different methods when dealing with the challenge of missing points. SPSR [36] RIMLS [53] SALD [73] IGR [71] DSE [114] IMLSNet [115] ParseNet [40] GT OccNet [9] DeepSDF [8] LIG [10] Points2Surf [78] Fig. 8: An example of qualitative results from different methods when dealing with the challenge of misalignme Input PC GT GD [33] BPA [34] OccNet [9] DeepSDF [8] LIG [10] Points2Surf [78] Input PC GT GD [33] BPA [34] OccNet [9] DeepSDF [8] LIG [10] Points2Surf [78] --><img src="https://cdn.noedgeai.com/bo_d163ugjef24c73d1leb0_13.jpg?x=205&#x26;y=938&#x26;w=1388&#x26;h=1223&#x26;r=0"><p>Fig. 10: An example of qualitative results from different methods when dealing with the challenge of point outliers.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图10：不同方法在处理点异常值挑战时的定性结果示例。</p></div><p>TABLE 6: Quantitative results on the testing synthetic data of scene surfaces. Results of the best and second best methods are highlighted in each column. Comparative methods are grouped according to what priors of surface geometry they have used (cf. Section 3 for the grouping and Section 5.4 for how these representative methods are selected). "-" indicates the method cannot produce reasonable results due to their limited generalization.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表6：场景表面的测试合成数据的定量结果。每列中突出显示了最佳和第二最佳方法的结果。比较方法根据它们使用的表面几何先验进行分组（参见第3节的分组和第5.4节关于如何选择这些代表性方法）。“-”表示该方法由于其有限的泛化能力无法产生合理的结果。</p></div><table><tbody><tr><td>Prior</td><td>Method</td><td>CD \( \left( {\times {10}^{-3}}\right)  \downarrow \)</td><td>F-score (%) ↑</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td rowspan="2">Triangulation- based</td><td>GD [33]</td><td>33.85</td><td>75.95</td><td>62.08</td><td>41.92</td></tr><tr><td>BPA [34]</td><td>45.82</td><td>53.46</td><td>58.25</td><td>44.19</td></tr><tr><td rowspan="2">Smoothness</td><td>SPSR [36]</td><td>30.47</td><td>83.22</td><td>83.74</td><td>63.03</td></tr><tr><td>RIMLS [53]</td><td>41.45</td><td>74.56</td><td>69.93</td><td>38.13</td></tr><tr><td rowspan="2">Modeling</td><td>SALD [73]</td><td>32.43</td><td>79.03</td><td>91.58</td><td>52.79</td></tr><tr><td>IGR [71]</td><td>31.41</td><td>81.63</td><td>91.26</td><td>67.58</td></tr><tr><td>Learning</td><td>OccNet [9]</td><td>93.12</td><td>37.75</td><td>85.98</td><td>50.34</td></tr><tr><td>Semantics</td><td>DeepSDF [8]</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Local</td><td>LIG [10]</td><td>41.40</td><td>78.03</td><td>88.12</td><td>59.97</td></tr><tr><td>Learning</td><td>Points2Surf [78]</td><td>36.24</td><td>76.14</td><td>83.60</td><td>61.82</td></tr><tr><td rowspan="3">Hybird</td><td>DSE [114]</td><td>32.97</td><td>77.53</td><td>57.99</td><td>41.56</td></tr><tr><td>IMLSNet [115]</td><td>35.52</td><td>78.05</td><td>87.17</td><td>61.98</td></tr><tr><td>ParseNet [40]</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>先前</td><td>方法</td><td>CD \( \left( {\times {10}^{-3}}\right)  \downarrow \)</td><td>F-score (%) ↑</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td rowspan="2">基于三角测量</td><td>GD [33]</td><td>33.85</td><td>75.95</td><td>62.08</td><td>41.92</td></tr><tr><td>BPA [34]</td><td>45.82</td><td>53.46</td><td>58.25</td><td>44.19</td></tr><tr><td rowspan="2">平滑性</td><td>SPSR [36]</td><td>30.47</td><td>83.22</td><td>83.74</td><td>63.03</td></tr><tr><td>RIMLS [53]</td><td>41.45</td><td>74.56</td><td>69.93</td><td>38.13</td></tr><tr><td rowspan="2">建模</td><td>SALD [73]</td><td>32.43</td><td>79.03</td><td>91.58</td><td>52.79</td></tr><tr><td>IGR [71]</td><td>31.41</td><td>81.63</td><td>91.26</td><td>67.58</td></tr><tr><td>学习</td><td>OccNet [9]</td><td>93.12</td><td>37.75</td><td>85.98</td><td>50.34</td></tr><tr><td>语义</td><td>DeepSDF [8]</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>局部</td><td>LIG [10]</td><td>41.40</td><td>78.03</td><td>88.12</td><td>59.97</td></tr><tr><td>学习</td><td>Points2Surf [78]</td><td>36.24</td><td>76.14</td><td>83.60</td><td>61.82</td></tr><tr><td rowspan="3">混合</td><td>DSE [114]</td><td>32.97</td><td>77.53</td><td>57.99</td><td>41.56</td></tr><tr><td>IMLSNet [115]</td><td>35.52</td><td>78.05</td><td>87.17</td><td>61.98</td></tr><tr><td>ParseNet [40]</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></div><p>TABLE 7: Quantitative results on the real-scanned data. Results of the best and second best methods are highlighted in each column. Comparative methods are grouped according to what priors of surface geometry they have used (cf. Section 3 for the grouping and Section 5.4 for how these representative methods are selected).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表7：在真实扫描数据上的定量结果。每列中突出显示了最佳和第二最佳方法。比较方法根据它们使用的表面几何先验进行分组（参见第3节的分组和第5.4节关于如何选择这些代表性方法）。</p></div><table><tbody><tr><td>Prior</td><td>Method</td><td>CD \( \left( {\times {10}^{-2}}\right)  \downarrow \)</td><td>F-score (%) ↑</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td rowspan="2">Triangulation- based</td><td>GD [33]</td><td>31.72</td><td>87.51</td><td>88.86</td><td>82.20</td></tr><tr><td>BPA [34]</td><td>40.37</td><td>80.95</td><td>87.56</td><td>68.69</td></tr><tr><td rowspan="2">Smoothness</td><td>SPSR [36]</td><td>31.05</td><td>87.74</td><td>94.94</td><td>89.38</td></tr><tr><td>RIMLS [53]</td><td>32.80</td><td>87.05</td><td>91.97</td><td>85.19</td></tr><tr><td rowspan="2">Modeling</td><td>SALD [73]</td><td>31.13</td><td>87.72</td><td>94.68</td><td>86.86</td></tr><tr><td>IGR [71]</td><td>32.70</td><td>87.18</td><td>95.99</td><td>89.10</td></tr><tr><td rowspan="2">Learning Semantics</td><td>OccNet [9]</td><td>232.71</td><td>17.11</td><td>80.96</td><td>39.70</td></tr><tr><td>DeepSDF [8]</td><td>263.92</td><td>19.83</td><td>77.95</td><td>40.95</td></tr><tr><td rowspan="2">Local Learning</td><td>LIG [10]</td><td>48.75</td><td>83.76</td><td>92.57</td><td>81.48</td></tr><tr><td>Points2Surf [78]</td><td>48.93</td><td>80.89</td><td>89.52</td><td>81.83</td></tr><tr><td rowspan="3">Hybird</td><td>DSE [114]</td><td>32.16</td><td>86.88</td><td>87.20</td><td>76.81</td></tr><tr><td>IMLSNet [115]</td><td>38.46</td><td>82.44</td><td>93.31</td><td>85.30</td></tr><tr><td>ParseNet [40]</td><td>149.96</td><td>38.92</td><td>81.51</td><td>45.67</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>先前</td><td>方法</td><td>CD \( \left( {\times {10}^{-2}}\right)  \downarrow \)</td><td>F-score (%) ↑</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td rowspan="2">基于三角测量</td><td>GD [33]</td><td>31.72</td><td>87.51</td><td>88.86</td><td>82.20</td></tr><tr><td>BPA [34]</td><td>40.37</td><td>80.95</td><td>87.56</td><td>68.69</td></tr><tr><td rowspan="2">平滑性</td><td>SPSR [36]</td><td>31.05</td><td>87.74</td><td>94.94</td><td>89.38</td></tr><tr><td>RIMLS [53]</td><td>32.80</td><td>87.05</td><td>91.97</td><td>85.19</td></tr><tr><td rowspan="2">建模</td><td>SALD [73]</td><td>31.13</td><td>87.72</td><td>94.68</td><td>86.86</td></tr><tr><td>IGR [71]</td><td>32.70</td><td>87.18</td><td>95.99</td><td>89.10</td></tr><tr><td rowspan="2">学习语义</td><td>OccNet [9]</td><td>232.71</td><td>17.11</td><td>80.96</td><td>39.70</td></tr><tr><td>DeepSDF [8]</td><td>263.92</td><td>19.83</td><td>77.95</td><td>40.95</td></tr><tr><td rowspan="2">局部学习</td><td>LIG [10]</td><td>48.75</td><td>83.76</td><td>92.57</td><td>81.48</td></tr><tr><td>Points2Surf [78]</td><td>48.93</td><td>80.89</td><td>89.52</td><td>81.83</td></tr><tr><td rowspan="3">混合</td><td>DSE [114]</td><td>32.16</td><td>86.88</td><td>87.20</td><td>76.81</td></tr><tr><td>IMLSNet [115]</td><td>38.46</td><td>82.44</td><td>93.31</td><td>85.30</td></tr><tr><td>ParseNet [40]</td><td>149.96</td><td>38.92</td><td>81.51</td><td>45.67</td></tr></tbody></table></div><!-- Media --><h3>6.2 Optimization-based, Learning-free Methods Versus Learning-based, Data-driven Ones</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.2 基于优化的无学习方法与基于学习的数据驱动方法的比较</h3></div><p>In this section, we investigate the behaviors of comparative methods by organizing them into two groups of optimization-based, learning-free methods and learning-based, data-driven ones. The former group includes SPSR [36], RIMLS [53], SALD [73], IGR [71], and we also include GD [33] and BPA [34] into the group for a complete coverage of the studied methods; the latter group includes OccNet [9], DeepSDF [8], LIG [10], Points2Surf [78], DSE [114], IMLSNet [115], and ParseNet [40]. By doing so, we aim to investigate how the two groups perform in terms of generalizing to complex shapes, where we pay special attention to methods of global, semantic learning (i.e., OccNet [9] and DeepSDF [8]) whose advantages may only be manifested when the categories of object surfaces exist in the auxiliary training set. Note that our testing data of synthetic object surfaces include 22 randomly selected objects, as described in Section 5.1, which contain both those belonging to popular semantic categories (e.g., chair) and those without clearly defined semantics; for the former case, our auxiliary training set contains rich shape instances of same categories from ShapeNet [17]. In this section, we also study robustness of the two groups of methods against imperfect scanning at varying levels of severity.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本节中，我们通过将比较方法组织为两组：基于优化的无学习方法和基于学习的数据驱动方法，来研究它们的行为。前一组包括 SPSR [36]、RIMLS [53]、SALD [73]、IGR [71]，我们还将 GD [33] 和 BPA [34] 纳入该组，以全面覆盖所研究的方法；后一组包括 OccNet [9]、DeepSDF [8]、LIG [10]、Points2Surf [78]、DSE [114]、IMLSNet [115] 和 ParseNet [40]。通过这样做，我们旨在研究这两组方法在推广复杂形状方面的表现，特别关注全局语义学习方法（即 OccNet [9] 和 DeepSDF [8]），其优势可能仅在对象表面类别存在于辅助训练集中时显现。请注意，我们的合成对象表面测试数据包括 22 个随机选择的对象，如第 5.1 节所述，其中包含属于流行语义类别（例如，椅子）和没有明确定义语义的对象；对于前一种情况，我们的辅助训练集包含来自 ShapeNet [17] 的同类丰富形状实例。在本节中，我们还研究了这两组方法在不同严重程度的扫描不完美情况下的鲁棒性。</p></div><p>Quantitative results in Table 8 show that on reconstruction of synthetic object surfaces, optimization-based, learning-free methods generalize better under different evaluation metrics, since our testing shapes contain both semantic ones and non-semantic, complex ones; we have consistent observations from examples in Fig. 11 - while learning-based, data-driven methods are good at reconstructing an chair surface, they fail in generalizing to nonsemantic shapes. Table 8 also show that learning-based methods are good in terms of robustness against higher levels of data imperfections. We observe similar phenomena on reconstruction of real-scanned data (cf. Appendix F for more details),by reorganizing Table 7 according to the two method groups.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表 8 中的定量结果显示，在合成对象表面的重建中，基于优化的无学习方法在不同评估指标下的推广能力更强，因为我们的测试形状包含语义形状和非语义复杂形状；我们从图 11 中的例子中得到了相似的观察——虽然基于学习的数据驱动方法在重建椅子表面方面表现良好，但它们在推广到非语义形状时失败。表 8 还显示，基于学习的方法在面对更高水平的数据不完美时表现出良好的鲁棒性。我们在重建真实扫描数据时观察到了类似现象（详见附录 F），通过根据两组方法重新组织表 7。</p></div><p>To further investigate the behaviors of learning from auxiliary data, we conduct experiments of synthetic scene surface reconstruction. Results in Table 6 (after re-organization of method groups) and Fig. 12 show that among the learning-based methods, LIG [10], Points2Surf [78] and IMLSNet [115] are capable of handling reconstruction of scene-level surfaces via local modeling and aggregation, while semantic learning methods of OccNet and DeepSDF fail, as expected.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了进一步研究从辅助数据中学习的行为，我们进行了合成场景表面重建实验。表 6 中的结果（在方法组重新组织后）和图 12 显示，在基于学习的方法中，LIG [10]、Points2Surf [78] 和 IMLSNet [115] 能够通过局部建模和聚合处理场景级表面的重建，而语义学习方法 OccNet 和 DeepSDF 则未能如预期那样成功。</p></div><p>We summarize the above analyses as follows: (1) learning-based, data-driven methods are able to reconstruct object surfaces when the training set contains object instances of the same semantic categories, and they show a certain degree of robustness against data imperfections; however, these methods fail to generalize when the condition is not satisfied; (2) for reconstruction of scene-level surfaces, local learning methods succeed by local modeling and aggregation, and in contrast, global, semantic learning methods fail to do so; (3) some optimization-based, learning-free methods (e.g., SPSR [36]) perform surprisingly well in robustness and generalization for both object-level and scene-level surfaces.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们将上述分析总结如下：（1）基于学习的数据驱动方法能够在训练集包含相同语义类别的对象实例时重建对象表面，并且在面对数据不完美时表现出一定程度的鲁棒性；然而，当条件不满足时，这些方法无法推广；（2）对于场景级表面的重建，局部学习方法通过局部建模和聚合成功，而全局语义学习方法则未能做到；（3）一些基于优化的无学习方法（例如，SPSR [36]）在对象级和场景级表面的鲁棒性和推广能力方面表现出惊人的良好。</p></div><h3>6.3 The Importance of Orientations of Surface Normals</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.3 表面法线方向的重要性</h3></div><p>Some of our studied methods compute surface normals from observed point clouds, and use the computed normals for surface reconstruction, including BPA [34], SPSR [36], RIMLS [53], IGR [71], OccNet [9], DeepSDF [8], LIG [10], and ParseNet [40]; a few other methods (e.g., Points2Surf [78] and IMLSNet [115]) compute surface normals on training data, and then train models to estimate surface normals when reconstructing testing surfaces. To investigate how these methods benefit from surface normal computation/estimation, we conduct experiments on our testing data with scanning imperfections, since robustness of these methods would be tested when less accurate surface normals are computed from imperfectly scanned data. Assume that the relative pose of a camera w.r.t. an observed point cloud is given; for any observed point, we compute its oriented surface normal by performing PCA on its local neighborhood of points (cf. Section 4.1.3 for the details); as such, the computed surface normals may not be precise but their inward or outward orientations must be correct.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们研究的一些方法从观察到的点云中计算表面法线，并使用计算得到的法线进行表面重建，包括 BPA [34]、SPSR [36]、RIMLS [53]、IGR [71]、OccNet [9]、DeepSDF [8]、LIG [10] 和 ParseNet [40]；还有一些其他方法（例如，Points2Surf [78] 和 IMLSNet [115]）在训练数据上计算表面法线，然后训练模型在重建测试表面时估计表面法线。为了研究这些方法如何从表面法线计算/估计中受益，我们在具有扫描不完美的测试数据上进行了实验，因为当从不完美扫描数据中计算出不太准确的表面法线时，这些方法的鲁棒性将受到考验。假设相机相对于观察到的点云的相对姿态已知；对于任何观察到的点，我们通过对其局部邻域的点进行主成分分析（PCA）来计算其定向表面法线（详见第 4.1.3 节）；因此，计算得到的表面法线可能不精确，但其内向或外向的方向必须是正确的。</p></div><p>Table 9 gives the quantitative results for synthetic data of object surfaces; under different evaluation metrics, the additional computation or estimation of surface normals does help in improving surface reconstruction. When the relative pose of camera w.r.t. observed surface points is not available, the computed surface normals would be wrongly oriented at some local surface neighborhoods, since principal directions of PCA on local neighborhoods of noisy points are less reliable. To investigate the</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表 9 给出了合成对象表面数据的定量结果；在不同评估指标下，额外的表面法线计算或估计确实有助于改善表面重建。当相机相对于观察到的表面点的相对姿态不可用时，计算得到的表面法线在某些局部表面邻域可能会方向错误，因为对噪声点的局部邻域进行 PCA 的主方向不太可靠。为了研究</p></div><!-- Media --><p>TABLE 8: Comparison between optimization-based, learning-free methods and learning-based, data-driven methods on the testing synthetic data of object surfaces. We report quantitative results for the imperfect scanning of point-wise noise at three levels of severity; results are in the format of ". / . / . ", where the most left one is the absolute value under each evaluation metric, and the right two ones are those relative to the most left one. The best and second best methods are highlighted in each column.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表8：基于优化的无学习方法与基于学习的数据驱动方法在物体表面测试合成数据上的比较。我们报告了在三种严重程度下点状噪声的不完美扫描的定量结果；结果格式为“././.”，最左侧的是每个评估指标下的绝对值，右侧两个是相对于最左侧的值。每列中最佳和第二最佳方法已被突出显示。</p></div><table><tbody><tr><td rowspan="2">Algorithms</td><td rowspan="2">Priors</td><td colspan="3">CD \( \left( {\times {10}^{-4}}\right)  \downarrow \)</td><td colspan="2">F-score (%) ↑</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td></td><td>low- Imiddle- I herel</td><td></td><td></td><td>low- Imiddle- I herel</td><td>low- Imiddle- I hevel</td><td>level</td></tr><tr><td>GD [33]</td><td rowspan="6">learning- free</td><td>16.52 /</td><td>1.68 / 13.21</td><td></td><td></td><td>/ -0.18 / -7.44</td><td>)0 / -6.42 / -25.68</td><td>91.57 /-6.94 / -35.27</td></tr><tr><td>BPA [34]</td><td>16.59 /</td><td>1.99 / 12.71</td><td></td><td></td><td>98.63 / -0.12 / -9.04</td><td>95.36 / -3.68 / -17.39</td><td>90.71 / -3.22 / -25.56</td></tr><tr><td>SPSR [36]</td><td>15.50 /</td><td>0.55 / 2.66</td><td></td><td></td><td>99.51 / -0.05 / -0.35</td><td>97.84 / -0.81 / -3.95</td><td>95.60 / -0.62 / -3.61</td></tr><tr><td>RIMLS [53]</td><td>16.13 /</td><td></td><td></td><td></td><td>99.36 / -0.12 / -4.67</td><td>97.36 / -2.13 / -11.23</td><td>94.19 /-1.52 / -14.43</td></tr><tr><td>SALD [73]</td><td>15.33 /</td><td>3.44 / 12.26</td><td></td><td></td><td>99.54 / -0.60 / -7.06</td><td>98.07 /\( - {1.65}/ - {9.16} \)</td><td>95.66 /\( - {5.94}/ - {28.48} \)</td></tr><tr><td>IGR [71]</td><td>18.21 /</td><td>0.36 / 0.99</td><td></td><td></td><td>97.87 / -0.12 / -0.35</td><td>97.64 /-0.12 / -0.54</td><td>94.37 /0.15 / -0.70</td></tr><tr><td>OccNet [9]</td><td rowspan="7">learning- based</td><td>209.04 /</td><td>-3.83 / 0.97</td><td></td><td></td><td>29.85 / -0.08 / -1.93</td><td>79.43 /0.21 / -0.38</td><td>46.17 /-0.14 / -1.13</td></tr><tr><td>DeepSDF [8]</td><td>241.28 /</td><td>-10.88 / -1.65</td><td></td><td></td><td>16.70 / 0.38 / -0.71</td><td>78.01 /-0.23 / 0.51</td><td>38.55 /0.74 / 0.15</td></tr><tr><td>LIG [10]</td><td>23.96 /</td><td>\( - {1.91}/{2.31} \)</td><td></td><td>94.50 /</td><td></td><td>93.96 /0.26 / -5.70</td><td>86.71 /2.84 / -5.68</td></tr><tr><td>Points2Surf [78]</td><td>17.74 /</td><td></td><td></td><td>98.02 /</td><td>-0.63 / -3.81</td><td>94.97 /\( - {0.35}/ - {1.53} \)</td><td>92.83 /-0.24 / -2.62</td></tr><tr><td>DSE [114]</td><td>16.07 /</td><td></td><td></td><td>99.40 /</td><td>-0.23 / -6.65</td><td>94.43 /\( - {6.64}/ - {23.04} \)</td><td>90.05 /-6.52 / -32.97</td></tr><tr><td>IMLSNet [115]</td><td>22.64 /</td><td>0.03 / 1.09</td><td></td><td>94.41 /</td><td>-0.05 / -0.01</td><td>96.08 /-0.06 / -0.46</td><td>90.13 /-0.16 / -0.47</td></tr><tr><td>ParseNet [40]</td><td>154.11 /</td><td>-18.27 / -14.14</td><td></td><td>44.80 /</td><td>-0.67 / -2.14</td><td>78.55 /1.76 / 2.60</td><td>46.21 /3.09 / 2.79</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">算法</td><td rowspan="2">先验</td><td colspan="3">CD \( \left( {\times {10}^{-4}}\right)  \downarrow \)</td><td colspan="2">F-score (%) ↑</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td></td><td>低-中-我在这里</td><td></td><td></td><td>低-中-我在这里</td><td>低-中-我水平</td><td>水平</td></tr><tr><td>GD [33]</td><td rowspan="6">无学习</td><td>16.52 /</td><td>1.68 / 13.21</td><td></td><td></td><td>/ -0.18 / -7.44</td><td>)0 / -6.42 / -25.68</td><td>91.57 /-6.94 / -35.27</td></tr><tr><td>BPA [34]</td><td>16.59 /</td><td>1.99 / 12.71</td><td></td><td></td><td>98.63 / -0.12 / -9.04</td><td>95.36 / -3.68 / -17.39</td><td>90.71 / -3.22 / -25.56</td></tr><tr><td>SPSR [36]</td><td>15.50 /</td><td>0.55 / 2.66</td><td></td><td></td><td>99.51 / -0.05 / -0.35</td><td>97.84 / -0.81 / -3.95</td><td>95.60 / -0.62 / -3.61</td></tr><tr><td>RIMLS [53]</td><td>16.13 /</td><td></td><td></td><td></td><td>99.36 / -0.12 / -4.67</td><td>97.36 / -2.13 / -11.23</td><td>94.19 /-1.52 / -14.43</td></tr><tr><td>SALD [73]</td><td>15.33 /</td><td>3.44 / 12.26</td><td></td><td></td><td>99.54 / -0.60 / -7.06</td><td>98.07 /\( - {1.65}/ - {9.16} \)</td><td>95.66 /\( - {5.94}/ - {28.48} \)</td></tr><tr><td>IGR [71]</td><td>18.21 /</td><td>0.36 / 0.99</td><td></td><td></td><td>97.87 / -0.12 / -0.35</td><td>97.64 /-0.12 / -0.54</td><td>94.37 /0.15 / -0.70</td></tr><tr><td>OccNet [9]</td><td rowspan="7">基于学习</td><td>209.04 /</td><td>-3.83 / 0.97</td><td></td><td></td><td>29.85 / -0.08 / -1.93</td><td>79.43 /0.21 / -0.38</td><td>46.17 /-0.14 / -1.13</td></tr><tr><td>DeepSDF [8]</td><td>241.28 /</td><td>-10.88 / -1.65</td><td></td><td></td><td>16.70 / 0.38 / -0.71</td><td>78.01 /-0.23 / 0.51</td><td>38.55 /0.74 / 0.15</td></tr><tr><td>LIG [10]</td><td>23.96 /</td><td>\( - {1.91}/{2.31} \)</td><td></td><td>94.50 /</td><td></td><td>93.96 /0.26 / -5.70</td><td>86.71 /2.84 / -5.68</td></tr><tr><td>Points2Surf [78]</td><td>17.74 /</td><td></td><td></td><td>98.02 /</td><td>-0.63 / -3.81</td><td>94.97 /\( - {0.35}/ - {1.53} \)</td><td>92.83 /-0.24 / -2.62</td></tr><tr><td>DSE [114]</td><td>16.07 /</td><td></td><td></td><td>99.40 /</td><td>-0.23 / -6.65</td><td>94.43 /\( - {6.64}/ - {23.04} \)</td><td>90.05 /-6.52 / -32.97</td></tr><tr><td>IMLSNet [115]</td><td>22.64 /</td><td>0.03 / 1.09</td><td></td><td>94.41 /</td><td>-0.05 / -0.01</td><td>96.08 /-0.06 / -0.46</td><td>90.13 /-0.16 / -0.47</td></tr><tr><td>ParseNet [40]</td><td>154.11 /</td><td>-18.27 / -14.14</td><td></td><td>44.80 /</td><td>-0.67 / -2.14</td><td>78.55 /1.76 / 2.60</td><td>46.21 /3.09 / 2.79</td></tr></tbody></table></div><!-- figureText: learning-free SPSR [36] RIMLS [53] SALD [73] IGR [71] DSE [114] IMLSNet [115] ParseNet [40] GT Input PC GT GD [33] BPA [34] learning-based OccNet [9] DeepSDF [8] LIG [10] Points2Surf [78] --><img src="https://cdn.noedgeai.com/bo_d163ugjef24c73d1leb0_15.jpg?x=138&#x26;y=723&#x26;w=1510&#x26;h=878&#x26;r=0"><!-- Media --><p>Fig. 11: Qualitative results respectively from optimization-based, learning-free methods and learning-based, data-driven methods on the testing synthetic data of object surfaces. The input point clouds have imperfect scanning of point-wise noise. The example of chair is of popular semantic categories, for which our auxiliary training set contains rich instances of the same category from ShapeNet [17]; the other one is non-semantic. TABLE 9: Comparison on testing synthetic data of object surfaces among methods without using surface normals \(\left( \times \right)\) ,methods using surfaces normals \(\left( \sqrt{}\right)\) ,and methods using surface normals only during learning \(\left( *\right)\) . We report quantitative results for the imperfect scanning of point-wise noise at the middle level of severity; results are in the format of ". \(1 \cdot  "\) ,where the left one is obtained assuming the availability of ground-truth camera poses, and the right one importance of inward or outward orientations of surface normals, we also report experiments in Table 9 where each result on the right of the "/" symbol is obtained without knowing the relative camera poses; compared with results on the left side of "/" that are obtained assuming the ground-truth camera poses, results on the right side drops drastically. We also conduct experiments on our synthetic data of scene surfaces and real-scanned data, and observe similar phenomena; these results are presented in Appendix D.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图11：基于优化的无学习方法和基于学习的数据驱动方法在物体表面的测试合成数据上的定性结果。输入的点云存在点状噪声的不完美扫描。椅子的例子属于流行的语义类别，我们的辅助训练集包含来自ShapeNet（原词）[17]的同一类别的丰富实例；另一个则是非语义的。表9：在不使用表面法线的方法\(\left( \times \right)\)、使用表面法线的方法\(\left( \sqrt{}\right)\)和仅在学习期间使用表面法线的方法\(\left( *\right)\)之间对物体表面的测试合成数据的比较。我们报告了在中等严重程度的点状噪声不完美扫描下的定量结果；结果格式为“.\(1 \cdot  "\)，左侧结果是在假设有真实相机姿态的情况下获得的，右侧结果则是表面法线的内向或外向方向的重要性，我们还在表9中报告了实验，其中“/”符号右侧的每个结果是在不知道相对相机姿态的情况下获得的；与假设真实相机姿态的“/”左侧结果相比，右侧结果大幅下降。我们还在我们的场景表面合成数据和真实扫描数据上进行实验，并观察到类似现象；这些结果在附录D中呈现。</p></div><!-- Media --><!-- figureText: learning-free ✓ SPSR [36] RIMLS [53] SALD [73] IGR [71] DSE [114] IMLSNet [115] ParseNet [40] GT Input PC GT GD [33] BPA [34] learning-based OccNet [9] DeepSDF [8] LIG [10] Points2Surf [78] --><img src="https://cdn.noedgeai.com/bo_d163ugjef24c73d1leb0_15.jpg?x=137&#x26;y=1737&#x26;w=1526&#x26;h=376&#x26;r=0"><p>Fig. 12: Qualitative results respectively from optimization-based, learning-free methods and learning-based, data-driven methods on the testing synthetic data of scene surfaces. The scene is a bedroom instance from the 3D-FRONT [24].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图12：基于优化的无学习方法和基于学习的数据驱动方法在场景表面的测试合成数据上的定性结果。场景是来自3D-FRONT（原词）[24]的一个卧室实例。</p></div><p>is obtained without knowing the camera poses. The best and second best methods are highlighted in each column.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>是在不知道相机姿态的情况下获得的。每一列中最佳和第二最佳方法被突出显示。</p></div><table><tbody><tr><td>Algorithms</td><td>Normals</td><td>\( \mathrm{{CD}}\left( {\times {10}^{-4}}\right)  \downarrow \)</td><td>F-score \( \left( \% \right)  \uparrow \)</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>GD [33]</td><td></td><td>18.20 /</td><td>98.91 /</td><td>87.58 /</td><td>84.63 /</td></tr><tr><td>SALD [73]</td><td>✘</td><td>18.77 /</td><td>98.94 /</td><td>96.42 /</td><td>89.72 /</td></tr><tr><td>DSE [114]</td><td></td><td>17.89 /</td><td>99.17 /</td><td>87.79 /</td><td>83.53 /</td></tr><tr><td>BPA [34]</td><td></td><td>18.58 / 18.61</td><td>98.51 / 98.56</td><td>91.68 / 91.79</td><td>87.49 / 85.08</td></tr><tr><td>SPSR [36]</td><td></td><td>\( \mathbf{{16.05}}/{60.42} \)</td><td>99.46 / 91.45</td><td>\( \mathbf{{97.03}}/{94.68} \)</td><td>\( \mathbf{{94.98}}/{84.34} \)</td></tr><tr><td>RIMLS [53]</td><td></td><td>\( {17.17}/{18.58} \)</td><td>99.24 / 97.47</td><td>95.23 / 94.06</td><td>92.67 / 86.63</td></tr><tr><td>IGR [71]</td><td>✓</td><td>18.57 / 262.40</td><td>97.75 / 85.98</td><td>\( \mathbf{{97.52}}/{95.28} \)</td><td>94.52 / 79.61</td></tr><tr><td>OccNet [9]</td><td></td><td>205.21 / 214.12</td><td>29.77 / 28.83</td><td>79.64 / 79.01</td><td>46.03 / 45.12</td></tr><tr><td>DeepSDF [8]</td><td></td><td>230.40 / 569.41</td><td>\( {17.08}/{15.64} \)</td><td>77.78 / 77.09</td><td>\( {39.29}/{33.78} \)</td></tr><tr><td>LIG [10]</td><td></td><td>\( {22.05}/{34.78} \)</td><td>96.99 / 89.60</td><td>94.22 / 91.36</td><td>\( {89.55}/{80.02} \)</td></tr><tr><td>ParseNet [40]</td><td></td><td>135.84 / 197.28</td><td>44.13 / 35.67</td><td>80.31 / 76.19</td><td>49.30 / 41.65</td></tr><tr><td>Points2Surf [78]</td><td rowspan="2">*</td><td>18.48 /</td><td>97.39 /</td><td>94.62 /</td><td>92.59 /</td></tr><tr><td>IMLSNet [115]</td><td>22.67 /</td><td>94.36 /</td><td>96.02 /</td><td>89.97 /</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>算法</td><td>法线</td><td>\( \mathrm{{CD}}\left( {\times {10}^{-4}}\right)  \downarrow \)</td><td>F-score \( \left( \% \right)  \uparrow \)</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>GD [33]</td><td></td><td>18.20 /</td><td>98.91 /</td><td>87.58 /</td><td>84.63 /</td></tr><tr><td>SALD [73]</td><td>✘</td><td>18.77 /</td><td>98.94 /</td><td>96.42 /</td><td>89.72 /</td></tr><tr><td>DSE [114]</td><td></td><td>17.89 /</td><td>99.17 /</td><td>87.79 /</td><td>83.53 /</td></tr><tr><td>BPA [34]</td><td></td><td>18.58 / 18.61</td><td>98.51 / 98.56</td><td>91.68 / 91.79</td><td>87.49 / 85.08</td></tr><tr><td>SPSR [36]</td><td></td><td>\( \mathbf{{16.05}}/{60.42} \)</td><td>99.46 / 91.45</td><td>\( \mathbf{{97.03}}/{94.68} \)</td><td>\( \mathbf{{94.98}}/{84.34} \)</td></tr><tr><td>RIMLS [53]</td><td></td><td>\( {17.17}/{18.58} \)</td><td>99.24 / 97.47</td><td>95.23 / 94.06</td><td>92.67 / 86.63</td></tr><tr><td>IGR [71]</td><td>✓</td><td>18.57 / 262.40</td><td>97.75 / 85.98</td><td>\( \mathbf{{97.52}}/{95.28} \)</td><td>94.52 / 79.61</td></tr><tr><td>OccNet [9]</td><td></td><td>205.21 / 214.12</td><td>29.77 / 28.83</td><td>79.64 / 79.01</td><td>46.03 / 45.12</td></tr><tr><td>DeepSDF [8]</td><td></td><td>230.40 / 569.41</td><td>\( {17.08}/{15.64} \)</td><td>77.78 / 77.09</td><td>\( {39.29}/{33.78} \)</td></tr><tr><td>LIG [10]</td><td></td><td>\( {22.05}/{34.78} \)</td><td>96.99 / 89.60</td><td>94.22 / 91.36</td><td>\( {89.55}/{80.02} \)</td></tr><tr><td>ParseNet [40]</td><td></td><td>135.84 / 197.28</td><td>44.13 / 35.67</td><td>80.31 / 76.19</td><td>49.30 / 41.65</td></tr><tr><td>Points2Surf [78]</td><td rowspan="2">*</td><td>18.48 /</td><td>97.39 /</td><td>94.62 /</td><td>92.59 /</td></tr><tr><td>IMLSNet [115]</td><td>22.67 /</td><td>94.36 /</td><td>96.02 /</td><td>89.97 /</td></tr></tbody></table></div><!-- Media --><p>We have following empirical findings based on the above analyses: (1) computation or estimation of oriented surface normals help in surface reconstruction from point clouds; (2) compared with precisions of surface normals, it is more important to have the correct inward or outward orientations of surface normals.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于上述分析，我们得出了以下实证发现：(1) 计算或估计定向表面法线有助于从点云中重建表面；(2) 与表面法线的精度相比，正确的内向或外向法线方向更为重要。</p></div><h2>7 Conclusion</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>7 结论</h2></div><p>In this paper, we have reviewed both the classical and the more recent deep learning-based methods for surface reconstruction from point clouds; we have organized our reviews by categorizing these methods according to what priors of surface geometry they had used to regularize their solutions. To better understand the respective strengths and limitations of existing methods, we contribute a large-scale benchmarking dataset consisting of both synthetic and real-scanned data, which provides various sensing imperfections that are commonly encountered in practical \(3\mathrm{D}\) scanning. We conduct thorough empirical studies on the constructed benchmark, evaluating the robustness and generalization of different methods. Our studies help identify the remaining challenges faced by existing methods, and we expect that our studies would be useful for guiding the directions in future research. References</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本文中，我们回顾了经典的和最近基于深度学习的点云表面重建方法；我们通过根据这些方法所使用的表面几何先验对其解决方案进行规范化来组织我们的回顾。为了更好地理解现有方法的各自优缺点，我们贡献了一个大规模基准数据集，包括合成数据和真实扫描数据，提供了在实际\(3\mathrm{D}\)扫描中常见的各种传感器缺陷。我们对构建的基准进行了全面的实证研究，评估不同方法的鲁棒性和泛化能力。我们的研究有助于识别现有方法面临的挑战，我们期望我们的研究对未来研究方向的指导有所帮助。参考文献</p></div><p>[1] R. Bolle and B. Vemuri, "On three-dimensional surface reconstruction methods," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 13, no. 1, pp. 1-13, 1991. 1, 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[1] R. Bolle 和 B. Vemuri, "关于三维表面重建方法," IEEE模式分析与机器智能汇刊, 第13卷，第1期, 第1-13页, 1991. 1, 2</p></div><p>基于上述分析，我们得出了以下实证发现：(1) 计算或估计定向表面法线有助于从点云中重建表面；(2) 与表面法线的精度相比，正确的内向或外向法线方向更为重要。</p><p>[2] S. P. Lim and H. Haron, "Surface reconstruction techniques: a review," Artificial Intelligence Review, vol. 42, no. 1, pp. 59-78, 2014. 1, 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[2] S. P. Lim 和 H. Haron, "表面重建技术：综述," 人工智能评论, 第42卷，第1期, 第59-78页, 2014. 1, 2</p></div><p>[3] M. Berger, A. Tagliasacchi, L. M. Seversky, P. Alliez, G. Guennebaud, J. A. Levine, A. Sharf, and C. T. Silva, "A survey of surface reconstruction from point clouds," in Computer Graphics Forum, vol. 36, no. 1. Wiley Online Library, 2017, pp. 301-329. 1, 2, 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[3] M. Berger, A. Tagliasacchi, L. M. Seversky, P. Alliez, G. Guennebaud, J. A. Levine, A. Sharf, 和 C. T. Silva, "从点云中进行表面重建的调查," 在计算机图形论坛, 第36卷，第1期. Wiley在线图书馆, 2017, 第301-329页. 1, 2, 5</p></div><p>[4] C. C. You, S. P. Lim, S. C. Lim, J. San Tan, C. K. Lee, and Y. M. J. Khaw, "A survey on surface reconstruction techniques for structured and unstructured data," in 2020 IEEE Conference on Open Systems (ICOS). IEEE, 2020, pp. 37-42. 1, 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[4] C. C. You, S. P. Lim, S. C. Lim, J. San Tan, C. K. Lee, 和 Y. M. J. Khaw, "针对结构化和非结构化数据的表面重建技术调查," 在2020年IEEE开放系统会议（ICOS）. IEEE, 2020, 第37-42页. 1, 2</p></div><p>[5] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry, "A papier-mâché approach to learning \(3\mathrm{\;d}\) surface generation," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 216-224. 1, 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[5] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, 和 M. Aubry, "一种纸浆法学习\(3\mathrm{\;d}\)表面生成," 在2018年IEEE计算机视觉与模式识别会议论文集, 第216-224页. 1, 6</p></div><p>[6] Y. Luo, Z. Mi, and W. Tao, "Deepdt: Learning geometry from delaunay triangulation for surface reconstruction," in Thirty-Fifth AAAI Conference on Artificial Intelligence, 2021, pp. 2277-2285. 1, 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[6] Y. Luo, Z. Mi, 和 W. Tao, "Deepdt：从德劳内三角剖分中学习几何以进行表面重建," 在第三十五届AAAI人工智能会议, 2021, 第2277-2285页. 1, 7</p></div><p>[7] N. Sharp and M. Ovsjanikov, "Pointtrinet: Learned triangulation of 3d point sets," in European Conference on Computer Vision. Springer, 2020, pp. 762-778. 1, 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[7] N. Sharp 和 M. Ovsjanikov, "Pointtrinet：学习三维点集的三角剖分," 在欧洲计算机视觉会议. Springer, 2020, 第762-778页. 1, 7</p></div><p>[8] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove, "Deepsdf: Learning continuous signed distance functions for shape representation," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 165-174. 1, 4, 6, 12, 13, 14, 15,16,17,23,24,25,26,27</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[8] J. J. Park, P. Florence, J. Straub, R. Newcombe, 和 S. Lovegrove, "Deepsdf：学习用于形状表示的连续有符号距离函数," 在2019年IEEE计算机视觉与模式识别会议论文集, 第165-174页. 1, 4, 6, 12, 13, 14, 15,16,17,23,24,25,26,27</p></div><p>[9] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger, "Occupancy networks: Learning 3d reconstruction in function space," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4460-4470. 1, 4, 6, 12, 13, 14, 15, 16, 17, 21, 23, 24, 25, 26, 27</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[9] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, 和 A. Geiger, "占用网络：在函数空间中学习三维重建," 在2019年IEEE计算机视觉与模式识别会议论文集, 第4460-4470页. 1, 4, 6, 12, 13, 14, 15, 16, 17, 21, 23, 24, 25, 26, 27</p></div><p>[10] C. Jiang, A. Sud, A. Makadia, J. Huang, M. Nießner, and T. Funkhouser, "Local implicit grid representations for 3d scenes," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 6001-6010. 1, 7, 12, 14, 15, 16, 17, 23, 24, 25, 26, 27</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[10] C. Jiang, A. Sud, A. Makadia, J. Huang, M. Nießner, 和 T. Funkhouser, "3D场景的局部隐式网格表示," 载于2020年IEEE/CVF计算机视觉与模式识别会议论文集, 第6001-6010页。1, 7, 12, 14, 15, 16, 17, 23, 24, 25, 26, 27</p></div><p>[11] G. Slabaugh, R. Schafer, T. Malzbender, and B. Culbertson, "A survey of methods for volumetric scene reconstruction from photographs," in Volume Graphics 2001. Springer, 2001, pp. 81-100. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[11] G. Slabaugh, R. Schafer, T. Malzbender, 和 B. Culbertson, "从照片中进行体积场景重建的方法综述," 载于2001年体积图形会议。施普林格, 2001, 第81-100页。2</p></div><p>[12] F. Remondino and S. El-Hakim, "Image-based 3d modelling: a review," The photogrammetric record, vol. 21, no. 115, pp. 269-291, 2006. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[12] F. Remondino 和 S. El-Hakim, "基于图像的3D建模：综述," 摄影测量记录, 第21卷, 第115期, 第269-291页, 2006。2</p></div><p>[13] S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and R. Szeliski, "A comparison and evaluation of multi-view stereo reconstruction algorithms," in 2006 IEEE computer society conference on computer vision and pattern recognition, vol. 1. IEEE, 2006, pp. 519-528. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[13] S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, 和 R. Szeliski, "多视图立体重建算法的比较与评估," 载于2006年IEEE计算机学会计算机视觉与模式识别会议, 第1卷。IEEE, 2006, 第519-528页。2</p></div><p>[14] X. Han, H. Laga, and M. Bennamoun, "Image-based 3d object reconstruction: State-of-the-art and trends in the deep learning era," IEEE transactions on pattern analysis and machine intelligence, 2019. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[14] X. Han, H. Laga, 和 M. Bennamoun, "基于图像的3D物体重建：深度学习时代的最新进展与趋势," IEEE模式分析与机器智能汇刊, 2019。2</p></div><p>[15] G. Fahim, K. Amin, and S. Zarif, "Single-view 3d reconstruction: A survey of deep learning methods," Computers &#x26; Graphics, vol. 94, pp. 164-190, 2021. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[15] G. Fahim, K. Amin, 和 S. Zarif, "单视图3D重建：深度学习方法的综述," 计算机与图形, 第94卷, 第164-190页, 2021。2</p></div><p>[16] H. Zhu, Y. Nie, T. Yue, and X. Cao, "The role of prior in image based 3d modeling: a survey," Frontiers of Computer Science, vol. 11, no. 2, pp. 175-191, 2017. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[16] H. Zhu, Y. Nie, T. Yue, 和 X. Cao, "先验在基于图像的3D建模中的作用：综述," 计算机科学前沿, 第11卷, 第2期, 第175-191页, 2017。2</p></div><p>[17] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu, "Shapenet: An information-rich 3d model repository," arXiv preprint arXiv:1512.03012, 2015. 2, 12, 15, 16</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[17] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, 和 F. Yu, "Shapenet：一个信息丰富的3D模型库," arXiv预印本 arXiv:1512.03012, 2015。2, 12, 15, 16</p></div><p>[18] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, "3d shapenets: A deep representation for volumetric shapes," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 1912-1920. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[18] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, 和 J. Xiao, "3D Shapenets：体积形状的深度表示," 载于2015年IEEE计算机视觉与模式识别会议, 第1912-1920页。2</p></div><p>[19] W. Wohlkinger, A. Aldoma, R. B. Rusu, and M. Vincze, "3dnet: Large-scale object class recognition from cad models," in 2012 IEEE international conference on robotics and automation. IEEE, 2012, pp. 5384-5391. 2, 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[19] W. Wohlkinger, A. Aldoma, R. B. Rusu, 和 M. Vincze, "3DNet：从CAD模型进行大规模物体类别识别," 载于2012年IEEE国际机器人与自动化会议。IEEE, 2012, 第5384-5391页。2, 8</p></div><p>[20] S. Koch, A. Matveev, Z. Jiang, F. Williams, A. Artemov, E. Burnaev, M. Alexa, D. Zorin, and D. Panozzo, "Abc: A big cad model dataset for geometric deep learning," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 9601-9611. 2, 8, 12</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[20] S. Koch, A. Matveev, Z. Jiang, F. Williams, A. Artemov, E. Burnaev, M. Alexa, D. Zorin, 和 D. Panozzo, "ABC：一个用于几何深度学习的大型CAD模型数据集," 载于2019年IEEE计算机视觉与模式识别会议, 第9601-9611页。2, 8, 12</p></div><p>[21] Q. Zhou and A. Jacobson, "Thingi10k: A dataset of 10,000 3d-printing models," CoRR, 2016. 2, 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[21] Q. Zhou 和 A. Jacobson, "Thingi10k：一个包含10,000个3D打印模型的数据集," CoRR, 2016。2, 8</p></div><p>[22] V. Albertina, V. Kunsthistorisches Museum, V. Theater Museum, P. Musée Guimet, P. Musée des Monuments français, Cité de l'architecture et du patrimoine, D. des sculptures de la Ville de Paris, P. Musée Carnavalet, L. The Collection, L. Usher Gallery, M. A. N. di Firenze, and B. KODE Artmuseums, "Three d scans," <a href="https://threedscans.com/.2">https://threedscans.com/.2</a>, 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[22] V. Albertina, V. Kunsthistorisches Museum, V. Theater Museum, P. Musée Guimet, P. Musée des Monuments français, Cité de l'architecture et du patrimoine, D. des sculptures de la Ville de Paris, P. Musée Carnavalet, L. The Collection, L. Usher Gallery, M. A. N. di Firenze, 和 B. KODE Artmuseums, "三维扫描," <a href="https://threedscans.com/.2">https://threedscans.com/.2</a>, 8</p></div><p>[23] A. Handa, V. Pătrăucean, S. Stent, and R. Cipolla, "Scenenet: An annotated model generator for indoor scene understanding," in 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2016, pp. 5737-5743. 2, 10</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[23] A. Handa, V. Pătrăucean, S. Stent, 和 R. Cipolla, "Scenenet: 一个用于室内场景理解的注释模型生成器," 2016年IEEE国际机器人与自动化会议（ICRA）上发表。IEEE, 2016, 第5737-5743页。 2, 10</p></div><p>[24] H. Fu, B. Cai, L. Gao, L.-X. Zhang, J. Wang, C. Li, Q. Zeng, C. Sun, R. Jia, B. Zhao et al., "3d-front: 3d furnished rooms with layouts and semantics," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 10933-10942. 2, 10, 16</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[24] H. Fu, B. Cai, L. Gao, L.-X. Zhang, J. Wang, C. Li, Q. Zeng, C. Sun, R. Jia, B. Zhao 等, "3d-front: 布局和语义的3D家具房间," 在2021年IEEE/CVF国际计算机视觉会议论文集中， 第10933-10942页。 2, 10, 16</p></div><p>[25] A. Singh, J. Sha, K. S. Narayan, T. Achim, and P. Abbeel, "Bigbird: A large-scale 3d database of object instances," in 2014 IEEE international conference on robotics and automation (ICRA). IEEE, 2014, pp. 509- 516.3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[25] A. Singh, J. Sha, K. S. Narayan, T. Achim, 和 P. Abbeel, "Bigbird: 一个大规模的3D物体实例数据库," 在2014年IEEE国际机器人与自动化会议（ICRA）上发表。IEEE, 2014, 第509-516页。3</p></div><p>[26] B. Calli, A. Singh, J. Bruce, A. Walsman, K. Konolige, S. Srinivasa, P. Abbeel, and A. M. Dollar, "Yale-cmu-berkeley dataset for robotic manipulation research," The International Journal of Robotics Research, vol. 36, no. 3, pp. 261-268, 2017. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[26] B. Calli, A. Singh, J. Bruce, A. Walsman, K. Konolige, S. Srinivasa, P. Abbeel, 和 A. M. Dollar, "耶鲁-卡内基梅隆-伯克利数据集用于机器人操作研究," 国际机器人研究杂志, 第36卷，第3期, 第261-268页, 2017。3</p></div><p>[27] A. Kasper, Z. Xue, and R. Dillmann, "The kit object models database: An object model database for object recognition, localization and manipulation in service robotics," The International Journal of Robotics Research, vol. 31, no. 8, pp. 927-934, 2012. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[27] A. Kasper, Z. Xue, 和 R. Dillmann, "工具对象模型数据库: 一个用于服务机器人中的对象识别、定位和操作的对象模型数据库," 国际机器人研究杂志, 第31卷，第8期, 第927-934页, 2012。3</p></div><p>[28] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Ren, S. Verma, A. Clarkson, M. Yan, B. Budge, Y. Yan, X. Pan, J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira, M. Savva, D. Batra, H. M. Strasdat, R. D. Nardi, M. Goesele, S. Lovegrove, and R. Newcombe, "The replica dataset: A digital replica of indoor spaces," arXiv preprint arXiv:1906.05797, 2019. 3, 10</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[28] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Ren, S. Verma, A. Clarkson, M. Yan, B. Budge, Y. Yan, X. Pan, J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira, M. Savva, D. Batra, H. M. Strasdat, R. D. Nardi, M. Goesele, S. Lovegrove, 和 R. Newcombe, "复制数据集: 室内空间的数字复制品," arXiv预印本 arXiv:1906.05797, 2019。3, 10</p></div><p>[29] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang, "Matterport3D: Learning from RGB-D data in indoor environments," International Conference on 3D Vision \(\left( {3DV}\right) ,{2017.3}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[29] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, 和 Y. Zhang, "Matterport3D: 从室内环境中的RGB-D数据学习," 3D视觉国际会议 \(\left( {3DV}\right) ,{2017.3}\)</p></div><p>[30] M. Berger, J. A. Levine, L. G. Nonato, G. Taubin, and C. T. Silva, "A benchmark for surface reconstruction," ACM Transactions on Graphics (TOG), vol. 32, no. 2, pp. 1-17, 2013. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[30] M. Berger, J. A. Levine, L. G. Nonato, G. Taubin, 和 C. T. Silva, "一个表面重建基准," ACM图形学交易（TOG）, 第32卷，第2期, 第1-17页, 2013。3</p></div><p>[31] M. Botsch, L. Kobbelt, M. Pauly, P. Alliez, and B. Lévy, Polygon mesh processing. AK Peters / CRC Press, Sep. 2010. 3, 4, 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[31] M. Botsch, L. Kobbelt, M. Pauly, P. Alliez, 和 B. Lévy, 多边形网格处理。AK Peters / CRC Press, 2010年9月。3, 4, 8</p></div><p>[32] H. Edelsbrunner and N. R. Shah, "Triangulating topological spaces," in Proceedings of the tenth annual symposium on Computational geometry, 1994, pp. 285-292. 4</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[32] H. Edelsbrunner 和 N. R. Shah, "三角剖分拓扑空间," 在第十届计算几何年会论文集中, 1994, 第285-292页。4</p></div><p>[33] D. Cohen-Steiner and F. Da, "A greedy delaunay-based surface reconstruction algorithm," The visual computer, vol. 20, no. 1, pp. 4-16, 2004.4,12,13,14,15,16,17,23,24,25,26,27</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[33] D. Cohen-Steiner 和 F. Da, "一种贪婪的基于德劳内的表面重建算法," 视觉计算, 第20卷，第1期, 第4-16页, 2004。4,12,13,14,15,16,17,23,24,25,26,27</p></div><p>[34] F. Bernardini, J. Mittleman, H. Rushmeier, C. Silva, and G. Taubin, "The ball-pivoting algorithm for surface reconstruction," IEEE transactions on visualization and computer graphics, vol. 5, no. 4, pp. 349-359, 1999. 4, 12, 13, 14, 15, 16, 17, 23, 24, 25, 26, 27</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[34] F. 贝尔纳迪尼（F. Bernardini）、J. 米特尔曼（J. Mittleman）、H. 拉什迈尔（H. Rushmeier）、C. 席尔瓦（C. Silva）和 G. 陶宾（G. Taubin），《用于曲面重建的球枢算法》，《电气与电子工程师协会可视化与计算机图形汇刊》，第 5 卷，第 4 期，第 349 - 359 页，1999 年。4, 12, 13, 14, 15, 16, 17, 23, 24, 25, 26, 27</p></div><p>[35] M. Kazhdan, M. Bolitho, and H. Hoppe, "Poisson surface reconstruction," in Proceedings of the fourth Eurographics symposium on Geometry processing, vol. 7, 2006. 4</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[35] M. 卡日丹（M. Kazhdan）、M. 博利托（M. Bolitho）和 H. 霍普（H. Hoppe），《泊松曲面重建》，载于《第四届欧洲图形学几何处理研讨会论文集》，第 7 卷，2006 年。4</p></div><p>[36] M. Kazhdan and H. Hoppe, "Screened poisson surface reconstruction," ACM Transactions on Graphics (ToG), vol. 32, no. 3, pp. 1-13, 2013. 5,12,13,14,15,16,17,23,24,25,26,27</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[36] M. 卡日丹（M. Kazhdan）和 H. 霍普（H. Hoppe），《筛选泊松曲面重建》，《美国计算机协会图形学汇刊》，第 32 卷，第 3 期，第 1 - 13 页，2013 年。5,12,13,14,15,16,17,23,24,25,26,27</p></div><p>[37] S. Peng, C. M. Jiang, Y. Liao, M. Niemeyer, M. Pollefeys, and A. Geiger, "Shape as points: A differentiable poisson solver," in Advances in Neural Information Processing Systems (NeurIPS), 2021. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[37] S. 彭（S. Peng）、C. M. 江（C. M. Jiang）、Y. 廖（Y. Liao）、M. 尼迈耶（M. Niemeyer）、M. 波勒费斯（M. Pollefeys）和 A. 盖格（A. Geiger），《以点表示形状：一种可微泊松求解器》，载于《神经信息处理系统进展》，2021 年。5</p></div><p>[38] M. Eck and H. Hoppe, "Automatic reconstruction of b-spline surfaces of arbitrary topological type," in Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, 1996, pp. 325-334. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[38] M. 埃克（M. Eck）和 H. 霍普（H. Hoppe），《任意拓扑类型 B 样条曲面的自动重建》，载于《第 23 届计算机图形与交互技术年度会议论文集》，1996 年，第 325 - 334 页。5</p></div><p>[39] Y. He and H. Qin, "Surface reconstruction with triangular b-splines," in Geometric Modeling and Processing, 2004. Proceedings. IEEE, 2004, pp. 279-287. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[39] Y. 何（Y. He）和 H. 秦（H. Qin），《基于三角 B 样条的曲面重建》，载于《几何建模与处理会议论文集》，电气与电子工程师协会，2004 年，第 279 - 287 页。5</p></div><p>[40] G. Sharma, D. Liu, S. Maji, E. Kalogerakis, S. Chaudhuri, and R. Měch, "Parsenet: A parametric surface fitting network for \(3\mathrm{\;d}\) point clouds," in European Conference on Computer Vision. Springer, 2020, pp. 261- 276.5,7,12,13,14,15,16,17,23,24,25,26,27</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[40] G. 夏尔马（G. Sharma）、D. 刘（D. Liu）、S. 马吉（S. Maji）、E. 卡洛格拉基斯（E. Kalogerakis）、S. 乔杜里（S. Chaudhuri）和 R. 梅赫（R. Měch），《Parsenet：用于 \(3\mathrm{\;d}\) 点云的参数化曲面拟合网络》，载于《欧洲计算机视觉会议》，施普林格出版社，2020 年，第 261 - 276 页。5,7,12,13,14,15,16,17,23,24,25,26,27</p></div><p>[41] N. Leal, E. Leal, and J. W. Branch, "Simple method for constructing nurbs surfaces from unorganized points," in Proceedings of the 19th international meshing roundtable. Springer, 2010, pp. 161-175. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[41] N. 利尔（N. Leal）、E. 利尔（E. Leal）和 J. W. 布兰奇（J. W. Branch），《从无组织点构建非均匀有理 B 样条（NURBS）曲面的简单方法》，载于《第 19 届国际网格划分圆桌会议论文集》，施普林格出版社，2010 年，第 161 - 175 页。5</p></div><p>[42] A. Hashemian and S. F. Hosseini, "An integrated fitting and fairing approach for object reconstruction using smooth nurbs curves and surfaces," Computers &#x26; Mathematics with Applications, vol. 76, no. 7, pp. 1555-1575, 2018. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[42] A. 哈希米安（A. Hashemian）和 S. F. 侯赛尼（S. F. Hosseini），《使用光滑非均匀有理 B 样条（NURBS）曲线和曲面进行对象重建的集成拟合与光顺方法》，《计算机与数学应用》，第 76 卷，第 7 期，第 1555 - 1575 页，2018 年。5</p></div><p>[43] J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell, W. R. Fright, B. C. McCallum, and T. R. Evans, "Reconstruction and representation of 3d objects with radial basis functions," in Proceedings of the 28th annual conference on Computer graphics and interactive techniques, 2001, pp. 67-76. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[43] J. C. 卡尔（J. C. Carr）、R. K. 比森（R. K. Beatson）、J. B. 切里（J. B. Cherrie）、T. J. 米切尔（T. J. Mitchell）、W. R. 弗赖特（W. R. Fright）、B. C. 麦卡勒姆（B. C. McCallum）和 T. R. 埃文斯（T. R. Evans），《使用径向基函数进行三维对象的重建与表示》，载于《第 28 届计算机图形与交互技术年度会议论文集》，2001 年，第 67 - 76 页。5</p></div><p>[44] M. Kazhdan, "Reconstruction of solid models from oriented point sets," in Proceedings of the Third Eurographics Symposium on Geometry Processing, ser. SGP '05. Goslar, DEU: Eurographics Association, 2005, p. 73-es. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[44] M. 卡日丹（M. Kazhdan），《从有向点集重建实体模型》，载于《第三届欧洲图形学几何处理研讨会论文集》，系列编号 SGP '05。德国戈斯拉尔：欧洲图形学协会，2005 年，第 73 - es 页。5</p></div><p>[45] J. Manson, G. Petrova, and S. Schaefer, "Streaming surface reconstruction using wavelets," in Computer Graphics Forum, vol. 27, no. 5. Wiley Online Library, 2008, pp. 1411-1420. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[45] J. 曼森（J. Manson）、G. 彼得罗娃（G. Petrova）和 S. 舍费尔（S. Schaefer），《使用小波进行流式曲面重建》，载于《计算机图形学论坛》，第 27 卷，第 5 期。威利在线图书馆，2008 年，第 1411 - 1420 页。5</p></div><p>[46] D. Levin, "Mesh-independent surface interpolation," in Geometric Modeling for Scientific Visualization. Berlin, Heidelberg: Springer Berlin Heidelberg, 2004, pp. 37-49. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[46] D. 莱文（D. Levin），《与网格无关的曲面插值》，载于《科学可视化的几何建模》。德国柏林、海德堡：施普林格柏林海德堡出版社，2004 年，第 37 - 49 页。5</p></div><p>[47] M. Alexa, J. Behr, D. Cohen-Or, S. Fleishman, D. Levin, and C. T. Silva, "Point set surfaces," in Proceedings of the Conference on Visualization '01, ser. VIS '01. USA: IEEE Computer Society, 2001, p. 21-28. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[47] M. 亚历克萨（M. Alexa）、J. 贝尔（J. Behr）、D. 科恩 - 奥尔（D. Cohen - Or）、S. 弗莱什曼（S. Fleishman）、D. 莱文（D. Levin）和 C. T. 席尔瓦（C. T. Silva），《点集曲面》，载于《2001 年可视化会议论文集》，系列编号 VIS '01。美国：电气与电子工程师协会计算机协会，2001 年，第 21 - 28 页。5</p></div><p>[48] A. Adamson and M. Alexa, "Approximating and intersecting surfaces from points," in Proceedings of the 2003 Eurographics/ACM SIG-GRAPH symposium on Geometry processing, 2003, pp. 230-239. 5, 13</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[48] A. 亚当森（Adamson）和 M. 亚历克萨（Alexa），《基于点的曲面近似与相交》，收录于《2003 年欧洲图形学协会/美国计算机协会图形专业组几何处理研讨会论文集》，2003 年，第 230 - 239 页。5, 13</p></div><p>[49] G. Guennebaud and M. Gross, "Algebraic point set surfaces," ACM Trans. Graph., vol. 26, no. 3, p. 23-es, Jul. 2007. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[49] G. 盖内博（Guennebaud）和 M. 格罗斯（Gross），《代数点集曲面》，《美国计算机协会图形学汇刊》，第 26 卷，第 3 期，第 23 - es 页，2007 年 7 月。5</p></div><p>[50] M. Alexa, J. Behr, D. Cohen-Or, S. Fleishman, D. Levin, and C. T. Silva, "Computing and rendering point set surfaces," IEEE Transactions on visualization and computer graphics, vol. 9, no. 1, pp. 3-15, 2003. 5, 8, 21</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[50] M. 亚历克萨（Alexa）、J. 贝尔（Behr）、D. 科恩 - 奥尔（Cohen - Or）、S. 弗莱什曼（Fleishman）、D. 莱文（Levin）和 C. T. 席尔瓦（Silva），《点集曲面的计算与渲染》，《电气与电子工程师协会可视化与计算机图形学汇刊》，第 9 卷，第 1 期，第 3 - 15 页，2003 年。5, 8, 21</p></div><p>[51] S. Fleishman, D. Cohen-Or, and C. T. Silva, "Robust moving least-squares fitting with sharp features," ACM transactions on graphics (TOG), vol. 24, no. 3, pp. 544-552, 2005. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[51] S. 弗莱什曼（Fleishman）、D. 科恩 - 奥尔（Cohen - Or）和 C. T. 席尔瓦（Silva），《具有尖锐特征的鲁棒移动最小二乘法拟合》，《美国计算机协会图形学汇刊》（TOG），第 24 卷，第 3 期，第 544 - 552 页，2005 年。5</p></div><p>[52] R. Kolluri, "Provably good moving least squares," ACM Transactions on Algorithms (TALG), vol. 4, no. 2, pp. 1-25, 2008. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[52] R. 科勒里（Kolluri），《可证明良好的移动最小二乘法》，《美国计算机协会算法汇刊》（TALG），第 4 卷，第 2 期，第 1 - 25 页，2008 年。5</p></div><p>[53] A. C. Öztireli, G. Guennebaud, and M. Gross, "Feature preserving point set surfaces based on non-linear kernel regression," in Computer Graphics Forum, vol. 28, no. 2. Wiley Online Library, 2009, pp. 493- 501.5,7,12,14,15,16,17,23,24,25,26,27</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[53] A. C. 厄兹蒂雷利（Öztireli）、G. 盖内博（Guennebaud）和 M. 格罗斯（Gross），《基于非线性核回归的特征保留点集曲面》，收录于《计算机图形学论坛》，第 28 卷，第 2 期。威利在线图书馆，2009 年，第 493 - 501 页。5,7,12,14,15,16,17,23,24,25,26,27</p></div><p>[54] Z.-Q. Cheng, Y.-Z. Wang, B. Li, K. Xu, G. Dang, and S.-Y. Jin, "A survey of methods for moving least squares surfaces." in Volume graphics, 2008, pp. 9-23. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[54] 程志清（Z. - Q. Cheng）、王宇泽（Y. - Z. Wang）、李博（B. Li）、徐凯（K. Xu）、党岗（G. Dang）和金士尧（S. - Y. Jin），《移动最小二乘曲面方法综述》，收录于《体图形学》，2008 年，第 9 - 23 页。5</p></div><p>[55] R. Schnabel, R. Wahl, and R. Klein, "Efficient ransac for point-cloud shape detection," in Computer graphics forum, vol. 26, no. 2. Wiley Online Library, 2007, pp. 214-226. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[55] R. 施纳贝尔（Schnabel）、R. 瓦尔（Wahl）和 R. 克莱因（Klein），《用于点云形状检测的高效随机抽样一致性算法》，收录于《计算机图形学论坛》，第 26 卷，第 2 期。威利在线图书馆，2007 年，第 214 - 226 页。5</p></div><p>[56] L. Nan and P. Wonka, "Polyfit: Polygonal surface reconstruction from point clouds," in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 2353-2361. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[56] 南亮（L. Nan）和 P. 翁卡（Wonka），《Polyfit：基于点云的多边形曲面重建》，收录于《电气与电子工程师协会国际计算机视觉会议论文集》，2017 年，第 2353 - 2361 页。5</p></div><p>[57] Y. Li, A. Dai, L. Guibas, and M. Nießner, "Database-assisted object retrieval for real-time 3d reconstruction," in Computer Graphics Forum, vol. 34, no. 2. Wiley Online Library, 2015. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[57] 李阳（Y. Li）、戴安（A. Dai）、L. 吉巴斯（Guibas）和 M. 尼斯纳（Nießner），《基于数据库辅助的实时 3D 重建物体检索》，收录于《计算机图形学论坛》，第 34 卷，第 2 期。威利在线图书馆，2015 年。5</p></div><p>[58] Y. M. Kim, N. J. Mitra, Q. Huang, and L. Guibas, "Guided real-time scanning of indoor objects," in Computer Graphics Forum, vol. 32, no. 7. Wiley Online Library, 2013, pp. 177-186. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[58] 金英民（Y. M. Kim）、N. J. 米特拉（Mitra）、黄强（Q. Huang）和 L. 吉巴斯（Guibas），《室内物体的引导式实时扫描》，收录于《计算机图形学论坛》，第 32 卷，第 7 期。威利在线图书馆，2013 年，第 177 - 186 页。5</p></div><p>[59] L. Nan, K. Xie, and A. Sharf, "A search-classify approach for cluttered indoor scene understanding," ACM Transactions on Graphics (TOG), vol. 31, no. 6, pp. 1-10, 2012. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[59] 南亮（L. Nan）、谢凯（K. Xie）和 A. 沙夫（Sharf），《用于杂乱室内场景理解的搜索 - 分类方法》，《美国计算机协会图形学汇刊》（TOG），第 31 卷，第 6 期，第 1 - 10 页，2012 年。5</p></div><p>[60] Y. M. Kim, N. J. Mitra, D.-M. Yan, and L. Guibas, "Acquiring 3d indoor environments with variability and repetition," ACM Transactions on Graphics (TOG), vol. 31, no. 6, pp. 1-11, 2012. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[60] 金英民（Y. M. Kim）、N. J. 米特拉（Mitra）、颜德铭（D. - M. Yan）和 L. 吉巴斯（Guibas），《获取具有可变性和重复性的 3D 室内环境》，《美国计算机协会图形学汇刊》（TOG），第 31 卷，第 6 期，第 1 - 11 页，2012 年。5</p></div><p>[61] M. Pauly, N. J. Mitra, J. Giesen, M. H. Gross, and L. J. Guibas, "Example-based 3d scan completion," in Symposium on Geometry Processing, no. CONF, 2005, pp. 23-32. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[61] M. 保利（Pauly）、N. J. 米特拉（Mitra）、J. 吉森（Giesen）、M. H. 格罗斯（Gross）和 L. J. 吉巴斯（Guibas），《基于实例的 3D 扫描补全》，收录于《几何处理研讨会论文集》，第 CONF 期，2005 年，第 23 - 32 页。5</p></div><p>[62] C.-H. Shen, H. Fu, K. Chen, and S.-M. Hu, "Structure recovery by part assembly," ACM Transactions on Graphics (TOG), vol. 31, no. 6, pp. 1-11, 2012. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[62] 沈春华（C. - H. Shen）、傅浩（H. Fu）、陈凯（K. Chen）和胡世民（S. - M. Hu），《通过部件组装进行结构恢复》，《美国计算机协会图形学汇刊》（TOG），第 31 卷，第 6 期，第 1 - 11 页，2012 年。5</p></div><p>[63] D. Ulyanov, A. Vedaldi, and V. Lempitsky, "Deep image prior," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 9446-9454. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[63] D. 乌利亚诺夫（Ulyanov）、A. 韦尔迪耶（Vedaldi）和 V. 伦皮茨基（Lempitsky），《深度图像先验》，收录于《电气与电子工程师协会计算机视觉与模式识别会议论文集》，2018 年，第 9446 - 9454 页。5</p></div><p>[64] F. Williams, T. Schneider, C. Silva, D. Zorin, J. Bruna, and D. Panozzo, "Deep geometric prior for surface reconstruction," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 10130-10139. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[64] F. Williams, T. Schneider, C. Silva, D. Zorin, J. Bruna, 和 D. Panozzo, "深度几何先验用于表面重建," 发表在2019年IEEE计算机视觉与模式识别会议论文集中, 第10130-10139页. 5</p></div><p>[65] R. Hanocka, G. Metzer, R. Giryes, and D. Cohen-Or, "Point2mesh: A self-prior for deformable meshes," ACM Trans. Graph., vol. 39, no. 4, Jul. 2020. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[65] R. Hanocka, G. Metzer, R. Giryes, 和 D. Cohen-Or, "Point2mesh: 一种用于可变形网格的自先验," ACM图形学期刊, 第39卷, 第4期, 2020年7月. 5</p></div><p>[66] W. Zhao, J. Lei, Y. Wen, J. Zhang, and K. Jia, "Sign-agnostic implicit learning of surface self-similarities for shape modeling and reconstruction from raw point clouds," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 10256-10265.5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[66] W. Zhao, J. Lei, Y. Wen, J. Zhang, 和 K. Jia, "无符号隐式学习表面自相似性用于形状建模和从原始点云重建," 发表在2021年IEEE/CVF计算机视觉与模式识别会议论文集中, 第10256-10265页. 5</p></div><p>[67] R. Hanocka, A. Hertz, N. Fish, R. Giryes, S. Fleishman, and D. Cohen-Or, "Meshcnn: a network with an edge," ACM Transactions on Graphics (TOG), vol. 38, no. 4, pp. 1-12, 2019. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[67] R. Hanocka, A. Hertz, N. Fish, R. Giryes, S. Fleishman, 和 D. Cohen-Or, "Meshcnn: 一种具有边缘的网络," ACM图形学交易 (TOG), 第38卷, 第4期, 第1-12页, 2019年. 5</p></div><p>[68] M. Gadelha, R. Wang, and S. Maji, "Deep manifold prior," in 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), 2021, pp. 1107-1116. 5, 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[68] M. Gadelha, R. Wang, 和 S. Maji, "深度流形先验," 发表在2021年IEEE/CVF国际计算机视觉研讨会 (ICCVW), 第1107-1116页. 5, 6</p></div><p>[69] M. Atzmon, N. Haim, L. Yariv, O. Israelov, H. Maron, and Y. Lipman, "Controlling neural level sets," in Advances in Neural Information Processing Systems, 2019, pp. 2034-2043. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[69] M. Atzmon, N. Haim, L. Yariv, O. Israelov, H. Maron, 和 Y. Lipman, "控制神经水平集," 发表在神经信息处理系统进展, 2019年, 第2034-2043页. 6</p></div><p>[70] J. Lei and K. Jia, "Analytic marching: An analytic meshing solution from deep implicit surface networks," in International Conference on Machine Learning 2020 ICML-20, 7 2020. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[70] J. Lei 和 K. Jia, "解析行进: 一种来自深度隐式表面网络的解析网格解决方案," 发表在2020年国际机器学习会议 ICML-20, 2020年7月. 6</p></div><p>[71] A. Gropp, L. Yariv, N. Haim, M. Atzmon, and Y. Lipman, "Implicit geometric regularization for learning shapes," in Proceedings of Machine Learning and Systems 2020, 2020, pp. 3569-3579. 6, 12, 13, 14, 15, 16,17,23,24,25,26,27</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[71] A. Gropp, L. Yariv, N. Haim, M. Atzmon, 和 Y. Lipman, "用于学习形状的隐式几何正则化," 发表在2020年机器学习与系统会议论文集中, 第3569-3579页. 6, 12, 13, 14, 15, 16, 17, 23, 24, 25, 26, 27</p></div><p>[72] M. Atzmon and Y. Lipman, "Sal: Sign agnostic learning of shapes from raw data," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 2565-2574. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[72] M. Atzmon 和 Y. Lipman, "Sal: 从原始数据中无符号学习形状," 发表在2020年IEEE/CVF计算机视觉与模式识别会议论文集中, 第2565-2574页. 6</p></div><p>[73] ——, “Sald: Sign agnostic learning with derivatives,” in International Conference on Learning Representations, 2021. 6, 12, 14, 15, 16, 17, 23,24,25,26,27</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[73] ——, “Sald: 带导数的无符号学习,” 发表在2021年学习表征国际会议. 6, 12, 14, 15, 16, 17, 23, 24, 25, 26, 27</p></div><p>[74] A. Basher, M. Sarmad, and J. Boutellier, "Lightsal: Lightweight sign agnostic learning for implicit surface representation," arXiv preprint arXiv:2103.14273, 2021. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[74] A. Basher, M. Sarmad, 和 J. Boutellier, "Lightsal: 轻量级无符号学习用于隐式表面表示," arXiv预印本 arXiv:2103.14273, 2021年. 6</p></div><p>[75] T. Davies, D. Nowrouzezahrai, and A. Jacobson, "On the effectiveness of weight-encoded neural implicit 3d shapes," arXiv preprint arXiv:2009.09808, 2021. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[75] T. Davies, D. Nowrouzezahrai, 和 A. Jacobson, "关于权重编码神经隐式3D形状的有效性," arXiv预印本 arXiv:2009.09808, 2021年. 6</p></div><p>[76] F. Williams, M. Trager, J. Bruna, and D. Zorin, "Neural splines: Fitting \(3\mathrm{\;d}\) surfaces with infinitely-wide neural networks," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 9949-9958. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[76] F. Williams, M. Trager, J. Bruna, 和 D. Zorin, "神经样条: 用无限宽神经网络拟合\(3\mathrm{\;d}\)表面," 发表在2021年IEEE/CVF计算机视觉与模式识别会议论文集中, 第9949-9958页. 6</p></div><p>[77] S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, and A. Geiger, "Convolutional occupancy networks," in European Conference on Computer Vision (ECCV). Cham: Springer International Publishing, Aug. 2020.6,7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[77] S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, 和 A. Geiger, "卷积占用网络," 发表在欧洲计算机视觉会议 (ECCV). Cham: Springer国际出版, 2020年8月. 6, 7</p></div><p>[78] P. Erler, P. Guerrero, S. Ohrhallinger, N. J. Mitra, and M. Wimmer, "Points2surf learning implicit surfaces from point clouds," in European Conference on Computer Vision. Springer, 2020, pp. 108-124. 6, 7, 12,13,14,15,16,17,23,24,25,26,27</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[78] P. Erler, P. Guerrero, S. Ohrhallinger, N. J. Mitra, 和 M. Wimmer, "Points2surf 从点云学习隐式表面," 发表在欧洲计算机视觉会议. Springer, 2020年, 第108-124页. 6, 7, 12, 13, 14, 15, 16, 17, 23, 24, 25, 26, 27</p></div><p>[79] M. Yang, Y. Wen, W. Chen, Y. Chen, and K. Jia, "Deep optimized priors for \(3\mathrm{\;d}\) shape modeling and reconstruction," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 3269-3278. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[79] M. Yang, Y. Wen, W. Chen, Y. Chen, 和 K. Jia, "深度优化先验用于\(3\mathrm{\;d}\)形状建模与重建," 载于2021年IEEE/CVF计算机视觉与模式识别会议论文集, 第3269-3278页。6</p></div><p>[80] J. Tang, J. Lei, D. Xu, F. Ma, K. Jia, and L. Zhang, "Sa-convonet: Sign-agnostic optimization of convolutional occupancy networks," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 6504-6513. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[80] J. Tang, J. Lei, D. Xu, F. Ma, K. Jia, 和 L. Zhang, "Sa-convonet: 无符号优化卷积占据网络," 载于2021年IEEE/CVF国际计算机视觉会议论文集, 第6504-6513页。6</p></div><p>[81] Y. Liao, S. Donne, and A. Geiger, "Deep marching cubes: Learning explicit surface representations," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 2916-2925. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[81] Y. Liao, S. Donne, 和 A. Geiger, "深度行进立方体: 学习显式表面表示," 载于2018年IEEE计算机视觉与模式识别会议论文集, 第2916-2925页。6</p></div><p>[82] Z. Chen and H. Zhang, "Learning implicit fields for generative shape modeling," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 5939-5948. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[82] Z. Chen 和 H. Zhang, "学习隐式场用于生成形状建模," 载于2019年IEEE/CVF计算机视觉与模式识别会议论文集, 第5939-5948页。6</p></div><p>[83] Y. Duan, H. Zhu, H. Wang, L. Yi, R. Nevatia, and L. J. Guibas, "Curriculum deepsdf," in European Conference on Computer Vision. Springer, 2020, pp. 51-67. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[83] Y. Duan, H. Zhu, H. Wang, L. Yi, R. Nevatia, 和 L. J. Guibas, "课程深度SDF," 载于欧洲计算机视觉会议。施普林格, 2020, 第51-67页。6</p></div><p>[84] S. Yao, F. Yang, Y. Cheng, and M. G. Mozerov, "3d shapes local geometry codes learning with sdf," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 2110-2117. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[84] S. Yao, F. Yang, Y. Cheng, 和 M. G. Mozerov, "基于SDF的3D形状局部几何编码学习," 载于2021年IEEE/CVF国际计算机视觉会议论文集, 第2110-2117页。6</p></div><p>[85] B. Guillard, F. Stella, and P. Fua, "Meshudf: Fast and differentiable meshing of unsigned distance field networks," arXiv preprint arXiv:2111.14549, 2021. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[85] B. Guillard, F. Stella, 和 P. Fua, "Meshudf: 快速且可微分的无符号距离场网络网格化," arXiv预印本arXiv:2111.14549, 2021。6</p></div><p>[86] S. Giebenhain and B. Goldluecke, "Air-nets: An attention-based framework for locally conditioned implicit representations," in 2021 International Conference on 3D Vision (3DV). IEEE, 2021. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[86] S. Giebenhain 和 B. Goldluecke, "Air-nets: 一种基于注意力的局部条件隐式表示框架," 载于2021年国际3D视觉会议(3DV)。IEEE, 2021。6</p></div><p>[87] X. Yan, L. Lin, N. J. Mitra, D. Lischinski, D. Cohen-Or, and H. Huang, "Shapeformer: Transformer-based shape completion via sparse representation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[87] X. Yan, L. Lin, N. J. Mitra, D. Lischinski, D. Cohen-Or, 和 H. Huang, "Shapeformer: 基于变换器的形状补全通过稀疏表示," 载于2022年IEEE/CVF计算机视觉与模式识别会议论文集。6</p></div><p>[88] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, "Attention is all you need," in Advances in Neural Information Processing Systems, vol. 30, 2017. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[88] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, 和 I. Polosukhin, "注意力是你所需要的一切," 载于神经信息处理系统进展, 第30卷, 2017。6</p></div><p>[89] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, "Point transformer," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 16259-16268. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[89] H. Zhao, L. Jiang, J. Jia, P. H. Torr, 和 V. Koltun, "点变换器," 载于2021年IEEE/CVF国际计算机视觉会议论文集, 第16259-16268页。6</p></div><p>[90] J. Tang, X. Han, M. Tan, X. Tong, and K. Jia, "Skeletonnet: A topology-preserving solution for learning mesh reconstruction of object surfaces from rgb images," IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1-1, 2021. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[90] J. Tang, X. Han, M. Tan, X. Tong, 和 K. Jia, "Skeletonnet: 一种保持拓扑的解决方案，用于从RGB图像学习网格重建物体表面," IEEE模式分析与机器智能汇刊, 第1-1页, 2021。6</p></div><p>[91] J. Pan, X. Han, W. Chen, J. Tang, and K. Jia, "Deep mesh reconstruction from single rgb images via topology modification networks," in 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 9963-9972. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[91] J. Pan, X. Han, W. Chen, J. Tang, 和 K. Jia, "通过拓扑修改网络从单个RGB图像深度网格重建," 载于2019年IEEE/CVF国际计算机视觉会议(ICCV), 2019, 第9963-9972页。6</p></div><p>[92] J. Chibane, T. Alldieck, and G. Pons-Moll, "Implicit functions in feature space for \(3\mathrm{\;d}\) shape reconstruction and completion," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 6970-6981. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[92] J. Chibane, T. Alldieck, 和 G. Pons-Moll, "特征空间中的隐式函数用于\(3\mathrm{\;d}\)形状重建与补全," 载于2020年IEEE/CVF计算机视觉与模式识别会议论文集, 第6970-6981页。7</p></div><p>[93] J. Chibane, A. Mir, and G. Pons-Moll, "Neural unsigned distance fields for implicit function learning," in Advances in Neural Information Processing Systems (NeurIPS), December 2020. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[93] J. Chibane, A. Mir, 和 G. Pons-Moll, "用于隐式函数学习的神经无符号距离场," 载于神经信息处理系统进展(NeurIPS), 2020年12月。7</p></div><p>[94] R. Chabra, J. E. Lenssen, E. Ilg, T. Schmidt, J. Straub, S. Lovegrove, and R. Newcombe, "Deep local shapes: Learning local sdf priors for detailed 3d reconstruction," in Computer Vision - ECCV 2020. Cham: Springer International Publishing, 2020, pp. 608-625. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[94] R. 查布拉（R. Chabra）、J. E. 伦森（J. E. Lenssen）、E. 伊尔格（E. Ilg）、T. 施密特（T. Schmidt）、J. 施特劳布（J. Straub）、S. 洛夫格罗夫（S. Lovegrove）和R. 纽科姆（R. Newcombe），《深度局部形状：学习用于详细三维重建的局部有向距离场先验》（"Deep local shapes: Learning local sdf priors for detailed 3d reconstruction"），载于《计算机视觉——欧洲计算机视觉会议2020》（Computer Vision - ECCV 2020）。尚姆（Cham）：施普林格国际出版公司（Springer International Publishing），2020年，第608 - 625页。7</p></div><p>[95] Z. Mi, Y. Luo, and W. Tao, "Ssrnet: Scalable 3d surface reconstruction network," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 970-979. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[95] 米泽（Z. Mi）、罗宇（Y. Luo）和陶伟（W. Tao），《可扩展三维表面重建网络》（"Ssrnet: Scalable 3d surface reconstruction network"），载于《电气与电子工程师协会/计算机视觉基金会计算机视觉与模式识别会议论文集》（Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition），2020年，第970 - 979页。7</p></div><p>[96] T. Takikawa, J. Litalien, K. Yin, K. Kreis, C. Loop, D. Nowrouzezahrai, A. Jacobson, M. McGuire, and S. Fidler, "Neural geometric level of detail: Real-time rendering with implicit 3d shapes," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 11358-11367. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[96] T. 高川（T. Takikawa）、J. 利塔连（J. Litalien）、K. 尹（K. Yin）、K. 克雷斯（K. Kreis）、C. 卢普（C. Loop）、D. 诺鲁泽扎赫赖（D. Nowrouzezahrai）、A. 雅各布森（A. Jacobson）、M. 麦圭尔（M. McGuire）和S. 菲德勒（S. Fidler），《神经几何细节层次：使用隐式三维形状进行实时渲染》（"Neural geometric level of detail: Real-time rendering with implicit 3d shapes"），载于《电气与电子工程师协会/计算机视觉基金会计算机视觉与模式识别会议论文集》（Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition），2021年，第11358 - 11367页。7</p></div><p>[97] A. Badki, O. Gallo, J. Kautz, and P. Sen, "Meshlet priors for 3d mesh reconstruction," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 2849-2858. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[97] A. 巴德基（A. Badki）、O. 加洛（O. Gallo）、J. 考茨（J. Kautz）和P. 森（P. Sen），《用于三维网格重建的网格片先验》（"Meshlet priors for 3d mesh reconstruction"），载于《电气与电子工程师协会/计算机视觉基金会计算机视觉与模式识别会议论文集》（Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition），2020年，第2849 - 2858页。7</p></div><p>[98] E. Tretschk, A. Tewari, V. Golyanik, M. Zollhöfer, C. Stoll, and C. Theobalt, "Patchnets: Patch-based generalizable deep implicit 3d shape representations," in European Conference on Computer Vision. Springer, 2020, pp. 293-309. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[98] E. 特雷奇克（E. Tretschk）、A. 特瓦里（A. Tewari）、V. 戈利亚尼克（V. Golyanik）、M. 佐尔霍费尔（M. Zollhöfer）、C. 斯托尔（C. Stoll）和C. 特奥巴尔（C. Theobalt），《补丁网络：基于补丁的可泛化深度隐式三维形状表示》（"Patchnets: Patch-based generalizable deep implicit 3d shape representations"），载于《欧洲计算机视觉会议》（European Conference on Computer Vision）。施普林格（Springer），2020年，第293 - 309页。7</p></div><p>[99] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, "Pointnet: Deep learning on point sets for 3d classification and segmentation," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 652-660.7, 9, 12</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[99] 齐晨（C. R. Qi）、苏航（H. Su）、莫健（K. Mo）和桂巴斯（L. J. Guibas），《点云网络：用于三维分类和分割的点集深度学习》（"Pointnet: Deep learning on point sets for 3d classification and segmentation"），载于《电气与电子工程师协会计算机视觉与模式识别会议论文集》（Proceedings of the IEEE conference on computer vision and pattern recognition），2017年，第652 - 660页。7、9、12</p></div><p>[100] M. Tatarchenko, J. Park, V. Koltun, and Q.-Y. Zhou, "Tangent convolutions for dense prediction in 3d," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 3887-3896. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[100] M. 塔塔连科（M. Tatarchenko）、J. 帕克（J. Park）、V. 科尔图恩（V. Koltun）和周启源（Q.-Y. Zhou），《用于三维密集预测的切线卷积》（"Tangent convolutions for dense prediction in 3d"），载于《电气与电子工程师协会计算机视觉与模式识别会议论文集》（Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition），2018年，第3887 - 3896页。7</p></div><p>[101] B. Ummenhofer and V. Koltun, "Adaptive surface reconstruction with multiscale convolutional kernels," in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2021, pp. 5651-5660. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[101] B. 乌门霍费尔（B. Ummenhofer）和V. 科尔图恩（V. Koltun），《使用多尺度卷积核的自适应表面重建》（"Adaptive surface reconstruction with multiscale convolutional kernels"），载于《电气与电子工程师协会/计算机视觉基金会国际计算机视觉会议论文集》（Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)），2021年10月，第5651 - 5660页。7</p></div><p>[102] A. Boulch and R. Marlet, "Poco: Point convolution for surface reconstruction," arXiv preprint arXiv:2201.01831, 2022. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[102] A. 布尔什（A. Boulch）和R. 马利特（R. Marlet），《POCO：用于表面重建的点卷积》（"Poco: Point convolution for surface reconstruction"），预印本arXiv:2201.01831，2022年。7</p></div><p>[103] H. Jiang, J. Cai, J. Zheng, and J. Xiao, "Neighborhood-based neural implicit reconstruction from point clouds," in 2021 International Conference on \({3D}\) Vision (3DV),2021,pp. 1259-1268. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[103] 蒋浩（H. Jiang）、蔡杰（J. Cai）、郑杰（J. Zheng）和肖健（J. Xiao），《基于邻域的点云神经隐式重建》（"Neighborhood-based neural implicit reconstruction from point clouds"），载于《2021年国际\({3D}\)视觉会议》（2021 International Conference on \({3D}\) Vision (3DV)），2021年，第1259 - 1268页。7</p></div><p>[104] D. Attali, J.-D. Boissonnat, and A. Lieutier, "Complexity of the delaunay triangulation of points on surfaces the smooth case," in Proceedings of the nineteenth annual symposium on Computational geometry, 2003, pp. 201-210. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[104] D. 阿塔利（D. Attali）、J.-D. 布瓦索纳（J.-D. Boissonnat）和A. 利耶捷（A. Lieutier），《曲面上点的德劳内三角剖分复杂度：光滑情形》（"Complexity of the delaunay triangulation of points on surfaces the smooth case"），载于《第十九届计算几何年度研讨会论文集》（Proceedings of the nineteenth annual symposium on Computational geometry），2003年，第201 - 210页。7</p></div><p>[105] J.-D. Boissonnat and F. Cazals, "Smooth surface reconstruction via natural neighbour interpolation of distance functions," Computational Geometry, vol. 22, no. 1-3, pp. 185-203, 2002. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[105] J.-D. 布瓦索纳（J.-D. Boissonnat）和F. 卡扎尔斯（F. Cazals），《通过距离函数的自然邻域插值进行光滑表面重建》（"Smooth surface reconstruction via natural neighbour interpolation of distance functions"），《计算几何》（Computational Geometry），第22卷，第1 - 3期，第185 - 203页，2002年。7</p></div><p>[106] F. Lafarge and P. Alliez, "Surface reconstruction through point set structuring," in Computer Graphics Forum, vol. 32, no. 2pt2. Wiley Online Library, 2013, pp. 225-234. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[106] F. 拉法热（F. Lafarge）和P. 阿利埃（P. Alliez），《通过点集结构化进行表面重建》（"Surface reconstruction through point set structuring"），载于《计算机图形学论坛》（Computer Graphics Forum），第32卷，第2部分2期。威利在线图书馆（Wiley Online Library），2013年，第225 - 234页。7</p></div><p>[107] R. R. Paulsen, J. A. Bærentzen, and R. Larsen, "Markov random field surface reconstruction," IEEE transactions on visualization and computer graphics, vol. 16, no. 4, pp. 636-646, 2009. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[107] R. R. 保尔森（R. R. Paulsen）、J. A. 贝伦岑（J. A. Bærentzen）和R. 拉森（R. Larsen），《马尔可夫随机场表面重建》（"Markov random field surface reconstruction"），《电气与电子工程师协会可视化与计算机图形学汇刊》（IEEE transactions on visualization and computer graphics），第16卷，第4期，第636 - 646页，2009年。7</p></div><p>[108] J. Giesen, S. Spalinger, and B. Schölkopf, "Kernel methods for implicit surface modeling," in Advances in Neural Information Processing Systems, L. Saul, Y. Weiss, and L. Bottou, Eds., vol. 17. MIT Press, 2005.7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[108] J. 吉森（J. Giesen）、S. 斯帕林格（S. Spalinger）和B. 朔尔科普夫（B. Schölkopf），《用于隐式表面建模的核方法》（"Kernel methods for implicit surface modeling"），载于《神经信息处理系统进展》（Advances in Neural Information Processing Systems），L. 索尔（L. Saul）、Y. 韦斯（Y. Weiss）和L. 博托（L. Bottou）编，第17卷。麻省理工学院出版社（MIT Press），2005年。7</p></div><p>[109] F. Williams, Z. Gojcic, S. Khamis, D. Zorin, J. Bruna, S. Fidler, and O. Litany, "Neural fields as learnable kernels for 3d reconstruction," 2021.7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[109] F. Williams, Z. Gojcic, S. Khamis, D. Zorin, J. Bruna, S. Fidler, 和 O. Litany, "神经场作为可学习的核用于三维重建," 2021.7</p></div><p>[110] M. Baorui, H. Zhizhong, L. Yu-Shen, and Z. Matthias, "Neural-pull: Learning signed distance functions from point clouds by learning to pull space onto surfaces," in International Conference on Machine Learning (ICML), 2021. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[110] M. Baorui, H. Zhizhong, L. Yu-Shen, 和 Z. Matthias, "神经拉伸：通过学习将空间拉伸到表面来从点云中学习有符号距离函数," 在国际机器学习会议（ICML），2021. 7</p></div><p>[111] Y. Lipman, "Phase transitions, distance functions, and implicit neural representations," in Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, ser. Proceedings of Machine Learning Research, vol. 139. PMLR, 2021, pp. 6702-6712. [Online]. Available: <a href="http://proceedings.mlr.press/v139/lipman21a.html">http://proceedings.mlr.press/v139/lipman21a.html</a> 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[111] Y. Lipman, "相变、距离函数和隐式神经表示," 在第38届国际机器学习会议论文集，ICML 2021，2021年7月18-24日，虚拟活动，系列：机器学习研究论文集，卷139。PMLR，2021，页6702-6712。[在线]。可用： <a href="http://proceedings.mlr.press/v139/lipman21a.html">http://proceedings.mlr.press/v139/lipman21a.html</a> 7</p></div><p>[112] Z. Wang, P. Wang, Q. Dong, J. Gao, S. Chen, S. Xin, and C. Tu, "Neural-imls: Learning implicit moving least-squares for surface reconstruction from unoriented point clouds," arXiv preprint arXiv:2109.04398, 2021. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[112] Z. Wang, P. Wang, Q. Dong, J. Gao, S. Chen, S. Xin, 和 C. Tu, "神经-隐式最小二乘：从无方向点云中学习隐式移动最小二乘进行表面重建," arXiv预印本 arXiv:2109.04398, 2021. 7</p></div><p>[113] P.-S. Wang, Y. Liu, Y.-Q. Yang, and X. Tong, "Spline positional encoding for learning \(3\mathrm{\;d}\) implicit signed distance fields," in Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, Z.-H. Zhou, Ed. International Joint Conferences on Artificial Intelligence Organization, 8 2021, pp. 1091-1097. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[113] P.-S. Wang, Y. Liu, Y.-Q. Yang, 和 X. Tong, "样条位置编码用于学习\(3\mathrm{\;d}\)隐式有符号距离场," 在第三十届国际人工智能联合会议论文集，IJCAI-21，Z.-H. Zhou主编。国际人工智能联合会议组织，2021年8月，页1091-1097。7</p></div><p>[114] M.-J. Rakotosaona, P. Guerrero, N. Aigerman, N. J. Mitra, and M. Ovs-janikov, "Learning delaunay surface elements for mesh reconstruction," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 22-31. 7, 12, 13, 14, 15, 16, 17, 23, 24, 25, 26, 27</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[114] M.-J. Rakotosaona, P. Guerrero, N. Aigerman, N. J. Mitra, 和 M. Ovs-janikov, "学习德劳内表面元素进行网格重建," 在IEEE/CVF计算机视觉与模式识别会议论文集，2021，页22-31。7, 12, 13, 14, 15, 16, 17, 23, 24, 25, 26, 27</p></div><p>[115] S.-L. Liu, H.-X. Guo, H. Pan, P.-S. Wang, X. Tong, and Y. Liu, "Deep implicit moving least-squares functions for \(3\mathrm{\;d}\) reconstruction," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 1788-1797. 7, 12, 14, 15, 16, 17, 23, 24,25,26,27</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[115] S.-L. Liu, H.-X. Guo, H. Pan, P.-S. Wang, X. Tong, 和 Y. Liu, "深度隐式移动最小二乘函数用于\(3\mathrm{\;d}\)重建," 在IEEE/CVF计算机视觉与模式识别会议论文集，2021，页1788-1797。7, 12, 14, 15, 16, 17, 23, 24, 25, 26, 27</p></div><p>[116] L. Li, M. Sung, A. Dubrovina, L. Yi, and L. J. Guibas, "Supervised fitting of geometric primitives to \(3\mathrm{\;d}\) point clouds," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 2652-2660. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[116] L. Li, M. Sung, A. Dubrovina, L. Yi, 和 L. J. Guibas, "几何原语的监督拟合到\(3\mathrm{\;d}\)点云," 在IEEE计算机视觉与模式识别会议论文集，2019，页2652-2660。7</p></div><p>[117] M. Liu, X. Zhang, and H. Su, "Meshing point clouds with predicted intrinsic-extrinsic ratio guidance," in European Conference on Computer Vision. Springer, 2020, pp. 68-84. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[117] M. Liu, X. Zhang, 和 H. Su, "使用预测的内在-外在比率指导对点云进行网格化," 在欧洲计算机视觉会议。施普林格，2020，页68-84。7</p></div><p>[118] J. Gao, W. Chen, T. Xiang, A. Jacobson, M. McGuire, and S. Fidler, "Learning deformable tetrahedral meshes for \(3\mathrm{\;d}\) reconstruction," \({Ad}\) - vances In Neural Information Processing Systems, vol. 33, pp. 9936- 9947, 2020. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[118] J. Gao, W. Chen, T. Xiang, A. Jacobson, M. McGuire, 和 S. Fidler, "学习可变形四面体网格用于\(3\mathrm{\;d}\)重建," \({Ad}\) - 神经信息处理系统进展，卷33，页9936-9947，2020。7</p></div><p>[119] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang, "Pixel2mesh: Generating 3d mesh models from single rgb images," in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 52-67.7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[119] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, 和 Y.-G. Jiang, "Pixel2mesh：从单个RGB图像生成3D网格模型," 在欧洲计算机视觉会议（ECCV）论文集，2018，页52-67。7</p></div><p>[120] H. Kato, Y. Ushiku, and T. Harada, "Neural 3d mesh renderer," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 3907-3916. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[120] H. Kato, Y. Ushiku, 和 T. Harada, "神经3D网格渲染器," 在IEEE计算机视觉与模式识别会议论文集，2018，页3907-3916。7</p></div><p>[121] D. Xiao, S. Lin, Z. Shi, and B. Wang, "Learning modified indicator functions for surface reconstruction," Computers &#x26; Graphics, vol. 102, pp. 309-319, 2022. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[121] 肖迪（D. Xiao）、林思（S. Lin）、施泽（Z. Shi）和王博（B. Wang），《学习用于曲面重建的修正指示函数》，《计算机与图形学》（Computers &#x26; Graphics），第102卷，第309 - 319页，2022年7月。</p></div><p>[122] K. Genova, F. Cole, A. Sud, A. Sarna, and T. Funkhouser, "Local deep implicit functions for \(3\mathrm{\;d}\) shape," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 4857-4866.7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[122] 基诺瓦（K. Genova）、科尔（F. Cole）、苏德（A. Sud）、萨尔纳（A. Sarna）和芬克豪泽（T. Funkhouser），《用于\(3\mathrm{\;d}\)形状的局部深度隐式函数》，收录于《电气与电子工程师协会/计算机视觉基金会计算机视觉与模式识别会议论文集》（Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition），2020年，第4857 - 4866页，7月。</p></div><p>[123] Y. Siddiqui, J. Thies, F. Ma, Q. Shan, M. Nießner, and A. Dai, "Retrievalfuse: Neural 3d scene reconstruction with a database," in Proceedings of the International Conference on Computer Vision (ICCV), 2021. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[123] 西迪基（Y. Siddiqui）、蒂斯（J. Thies）、马飞（F. Ma）、单强（Q. Shan）、尼斯纳（M. Nießner）和戴安（A. Dai），《检索融合：基于数据库的神经三维场景重建》，收录于《国际计算机视觉会议论文集》（Proceedings of the International Conference on Computer Vision (ICCV)），2021年7月。</p></div><p>[124] J. Rossignac, "Shape complexity," The visual computer, vol. 21, no. 12, pp. 985-996, 2005. 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[124] 罗西尼亚克（J. Rossignac），《形状复杂度》，《可视化计算机》（The visual computer），第21卷，第12期，第985 - 996页，2005年8月。</p></div><p>[125] D. Stutz, "A formal definition of watertight meshes," <a href="https://davidstutz.de/a-formal-definition-of-watertight-meshes/">https://davidstutz.de/a-formal-definition-of-watertight-meshes/</a>, 2018, accessed: 2021- 12.8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[125] 施图茨（D. Stutz），《水密网格的形式定义》，<a href="https://davidstutz.de/a">https://davidstutz.de/a</a> - formal - definition - of - watertight - meshes/，2018年，访问时间：2021年12月8日。</p></div><p>[126] J. Kontkanen and S. Laine, "Ambient occlusion fields," in Proceedings of the 2005 symposium on Interactive 3D graphics and games, 2005, pp. 41-48. 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[126] 孔特卡宁（J. Kontkanen）和莱恩（S. Laine），《环境光遮蔽场》，收录于《2005年交互式三维图形与游戏研讨会论文集》（Proceedings of the 2005 symposium on Interactive 3D graphics and games），2005年，第41 - 48页，8月。</p></div><p>[127] M. Gschwandtner, R. Kwitt, A. Uhl, and W. Pree, "Blensor: Blender sensor simulation toolbox," in International Symposium on Visual Computing. Springer, 2011, pp. 199-208. 8, 10</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[127] 格施万特纳（M. Gschwandtner）、基特（R. Kwitt）、乌尔（A. Uhl）和普雷（W. Pree），《Blensor：Blender传感器模拟工具箱》，收录于《国际视觉计算研讨会论文集》（International Symposium on Visual Computing）。施普林格出版社，2011年，第199 - 208页，8月、10月。</p></div><p>[128] L. Li, "Time-of-flight camera-an introduction," Technical white paper, no. SLOA190B, 2014. 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[128] 李磊（L. Li），《飞行时间相机介绍》，技术白皮书，编号SLOA190B，2014年8月。</p></div><p>[129] E. W. Weisstein, "Euler angles," <a href="https://mathworld.wolfram.com/">https://mathworld.wolfram.com/</a>, 2009. 9</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[129] 韦斯斯坦（E. W. Weisstein），《欧拉角》，<a href="https://mathworld.wolfram.com/%EF%BC%8C2009%E5%B9%B49%E6%9C%88%E3%80%82">https://mathworld.wolfram.com/，2009年9月。</a></p></div><p>[130] J. Jiao, L. Yuan, W. Tang, Z. Deng, and Q. Wu, "A post-rectification approach of depth images of kinect v2 for \(3\mathrm{\;d}\) reconstruction of indoor scenes," ISPRS International Journal of Geo-Information, vol. 6, no. 11, p. 349, 2017. 10</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[130] 焦健（J. Jiao）、袁磊（L. Yuan）、唐伟（W. Tang）、邓泽（Z. Deng）和吴强（Q. Wu），《用于室内场景\(3\mathrm{\;d}\)重建的Kinect v2深度图像后校正方法》，《国际摄影测量与遥感学会地理信息国际期刊》（ISPRS International Journal of Geo - Information），第6卷，第11期，第349页，2017年10月。</p></div><p>[131] D. Girardeau-Montaut, "Cloudcompare," France: EDF R&#x26;D Telecom ParisTech, 2016. 11</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[131] 吉拉尔多 - 蒙托（D. Girardeau - Montaut），《Cloudcompare》，法国：法国电力公司研发部、巴黎高科电信学院，2016年11月。</p></div><p>[132] R. B. Rusu, Z. C. Marton, N. Blodow, M. Dolha, and M. Beetz, "Towards 3d point cloud based object maps for household environments," Robotics and Autonomous Systems, vol. 56, no. 11, pp. 927-941, 2008. 12</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[132] 鲁苏（R. B. Rusu）、马尔顿（Z. C. Marton）、布洛多（N. Blodow）、多尔哈（M. Dolha）和贝茨（M. Beetz），《面向家庭环境的基于三维点云的物体地图》，《机器人与自主系统》（Robotics and Autonomous Systems），第56卷，第11期，第927 - 941页，2008年12月。</p></div><p>[133] F. Cazals and M. Pouget, "Estimating differential quantities using polynomial fitting of osculating jets," Computer Aided Geometric Design, vol. 22, no. 2, pp. 121-146, 2005. 12</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[133] 卡扎尔斯（F. Cazals）和普热（M. Pouget），《使用密切喷流多项式拟合估计微分数量》，《计算机辅助几何设计》（Computer Aided Geometric Design），第22卷，第2期，第121 - 146页，2005年12月。</p></div><p>[134] J. Yang, R. Li, Y. Xiao, and Z. Cao, "3d reconstruction from nonuniform point clouds via local hierarchical clustering," in Ninth International Conference on Digital Image Processing (ICDIP 2017), vol. 10420. International Society for Optics and Photonics, 2017, p. 1042038. 12</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[134] 杨杰（J. Yang）、李锐（R. Li）、肖宇（Y. Xiao）和曹泽（Z. Cao），《通过局部层次聚类从非均匀点云进行三维重建》，收录于《第九届国际数字图像处理会议论文集》（Ninth International Conference on Digital Image Processing (ICDIP 2017)），第10420卷。国际光学与光子学学会，2017年，第1042038页，12月。</p></div><p>[135] H. Fan, H. Su, and L. J. Guibas, "A point set generation network for 3d object reconstruction from a single image," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 605-613. 12, 21</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[135] 范浩（H. Fan）、苏航（H. Su）和吉巴斯（L. J. Guibas），《用于从单张图像进行三维物体重建的点集生成网络》，收录于《电气与电子工程师协会计算机视觉与模式识别会议论文集》（Proceedings of the IEEE conference on computer vision and pattern recognition），2017年，第605 - 613页，12月、21月。</p></div><p>[136] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun, "Tanks and temples: Benchmarking large-scale scene reconstruction," ACM Transactions on Graphics (ToG), vol. 36, no. 4, pp. 1-13, 2017. 12, 21</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[136] 克纳皮奇（A. Knapitsch）、帕克（J. Park）、周启宇（Q. - Y. Zhou）和科尔图恩（V. Koltun），《坦克与寺庙：大规模场景重建基准测试》，《ACM图形学汇刊》（ACM Transactions on Graphics (ToG)），第36卷，第4期，第1 - 13页，2017年12月、21月。</p></div><p>[137] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, "The unreasonable effectiveness of deep features as a perceptual metric," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 586-595. 12, 22</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[137] 张（Zhang）、伊索拉（Isola）、埃弗罗斯（Efros）、谢赫特曼（Shechtman）和王（Wang），“深度特征作为感知度量的不合理有效性”，载于《电气与电子工程师协会计算机视觉与模式识别会议论文集》，2018年，第586 - 595页。12, 22</p></div><p>[138] A. Fabri and S. Pion, "Cgal: The computational geometry algorithms library," in Proceedings of the 17th ACM SIGSPATIAL international conference on advances in geographic information systems, 2009, pp. 538-539. 12</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[138] 法布里（Fabri）和皮翁（Pion），“CGAL：计算几何算法库”，载于《第17届美国计算机协会地理信息系统进展国际会议论文集》，2009年，第538 - 539页。12</p></div><p>[139] P. Cignoni, M. Callieri, M. Corsini, M. Dellepiane, F. Ganovelli, and G. Ranzuglia, "MeshLab: an Open-Source Mesh Processing Tool," in Eurographics Italian Chapter Conference, V. Scarano, R. D. Chiara, and U. Erra, Eds. The Eurographics Association, 2008. 12</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[139] 奇尼奥尼（Cignoni）、卡利耶里（Callieri）、科尔西尼（Corsini）、德莱皮安（Dellepiane）、加诺韦利（Ganovelli）和兰祖利亚（Ranzuglia），“MeshLab：一个开源网格处理工具”，载于《欧洲图形学意大利分会会议》，斯卡兰诺（Scarano）、基阿拉（Chiara）和埃尔拉（Erra）编。欧洲图形学协会，2008年。12</p></div><p>[140] J. Lin, X. Shi, Y. Gao, K. Chen, and K. Jia, "Cad-pu: A curvature-adaptive deep learning solution for point set upsampling," arXiv preprint arXiv:2009.04660, 2020. 21</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[140] 林（Lin）、施（Shi）、高（Gao）、陈（Chen）和贾（Jia），“Cad - pu：一种用于点集上采样的曲率自适应深度学习解决方案”，预印本arXiv:2009.04660，2020年。21</p></div><p>[141] 3D Shape Analysis: Fundamentals, Theory, and Applications. Wiley, Mar. 2019. 21</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[141] 《3D形状分析：基础、理论与应用》。威利出版社，2019年3月。21</p></div><p>[142] D. Kingma and J. Ba, "Adam: A method for stochastic optimization," International Conference on Learning Representations, 12 2014. 22</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[142] 金马（Kingma）和巴（Ba），“Adam：一种随机优化方法”，《国际学习表征会议》，2014年12月。22</p></div><h2>Appendix A Technical Details for Computing the Algebraic Surface Complexity</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>附录A 计算代数曲面复杂度的技术细节</h2></div><p>As discussed in Section 4.1.2, we have divided all the object surfaces into different groups according to their algebraic complexities measured by the averaged approximation errors, which are computed using a highest-degree fixed function to fit the local surface patches. We present the details in this section.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>如第4.1.2节所述，我们根据使用最高次固定函数拟合局部曲面片计算出的平均近似误差所衡量的代数复杂度，将所有物体表面分为不同的组。我们将在本节介绍详细内容。</p></div><p>Assume that the underlying surface \({\mathcal{S}}^{ * }\) of a given point cloud \(\mathcal{P}\) is \({\mathcal{C}}^{\infty }\) smooth. By differential geometry,if we approximate the surface \({\mathcal{S}}_{\mathbf{p}}^{ * }\) locally at a point \(\mathbf{p}\) with a polynomial \({\mathcal{S}}_{\mathbf{p}}\left( m\right)\) of degree \(m\) ,we have the following error bound [50]</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>假设给定的点云 \(\mathcal{P}\) 的底层曲面 \({\mathcal{S}}^{ * }\) 是 \({\mathcal{C}}^{\infty }\) 光滑的。根据微分几何，如果我们在点 \(\mathbf{p}\) 处用一个次数为 \(m\) 的多项式 \({\mathcal{S}}_{\mathbf{p}}\left( m\right)\) 局部逼近曲面 \({\mathcal{S}}_{\mathbf{p}}^{ * }\) ，我们有以下误差界 [50]</p></div><p></p>\[\begin{Vmatrix}{{\mathcal{S}}_{\mathbf{p}}^{ * } - {\mathcal{S}}_{\mathbf{p}}\left( m\right) }\end{Vmatrix} \leq  C\left( \begin{Vmatrix}{\mathcal{S}}_{\mathbf{p}}^{*\left( {m + 1}\right) }\end{Vmatrix}\right)  \cdot  {\omega }_{\mathbf{p}}^{m + 1},\]<p></p><p>where \({\omega }_{p}\) denotes the width of local 2D domain centered at \(p\) for \({\mathcal{S}}_{\mathbf{p}}\left( m\right)\) and \(C\left( \begin{Vmatrix}{\mathcal{S}}_{\mathbf{p}}^{ * }{}^{\left( m + 1\right) }\end{Vmatrix}\right)\) is a constant depending on the \({\left( m + 1\right) }^{th}\) derivatives of the local surface. Given a surface \({\mathcal{S}}^{ * }\) , we can define \(\left\{  {\mathbf{p}}_{i}\right\}\) and \(\left\{  {\omega }_{{\mathbf{p}}_{i}}\right\}\) to cover the whole surface by \(\left\{  {{\mathcal{S}}_{{\mathbf{p}}_{i}}\left( m\right) }\right\}\) with no holes. And in this way,the union of \(\left\{  {{\mathcal{S}}_{{\mathbf{p}}_{i}}\left( m\right) }\right\}\) approximates \({\mathcal{S}}^{ * }\) with the following error bound</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \({\omega }_{p}\) 表示以 \(p\) 为中心的 \({\mathcal{S}}_{\mathbf{p}}\left( m\right)\) 的局部二维区域的宽度，\(C\left( \begin{Vmatrix}{\mathcal{S}}_{\mathbf{p}}^{ * }{}^{\left( m + 1\right) }\end{Vmatrix}\right)\) 是一个取决于局部曲面的 \({\left( m + 1\right) }^{th}\) 阶导数的常数。给定一个曲面 \({\mathcal{S}}^{ * }\) ，我们可以定义 \(\left\{  {\mathbf{p}}_{i}\right\}\) 和 \(\left\{  {\omega }_{{\mathbf{p}}_{i}}\right\}\) ，通过 \(\left\{  {{\mathcal{S}}_{{\mathbf{p}}_{i}}\left( m\right) }\right\}\) 无间隙地覆盖整个曲面。通过这种方式，\(\left\{  {{\mathcal{S}}_{{\mathbf{p}}_{i}}\left( m\right) }\right\}\) 的并集以以下误差界逼近 \({\mathcal{S}}^{ * }\)</p></div><p></p>\[{\int }_{{\mathcal{S}}^{ * }}\begin{Vmatrix}{{\mathcal{S}}_{{\mathbf{p}}_{i}}^{ * } - {\mathcal{S}}_{{\mathbf{p}}_{i}}\left( m\right) }\end{Vmatrix} \leq  {\int }_{{\mathcal{S}}^{ * }}C\left( \begin{Vmatrix}{{\mathcal{S}}_{{\mathbf{p}}_{i}}^{ * }{}^{\left( m + 1\right) }}\end{Vmatrix}\right)  \cdot  {\omega }_{{\mathbf{p}}_{i}}^{m + 1}. \tag{17}\]<p></p><p>Considering that the surface \({\mathcal{S}}^{ * }\) is represented as a triangular mesh \({\mathcal{G}}_{{\mathcal{S}}^{ * }}\) ,a discrete version of Eq. (17) can be written as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>考虑到曲面 \({\mathcal{S}}^{ * }\) 表示为一个三角网格 \({\mathcal{G}}_{{\mathcal{S}}^{ * }}\) ，式（17）的离散形式可以写成</p></div><p></p>\[\mathop{\sum }\limits_{{i = 1}}^{n}\begin{Vmatrix}{{\mathcal{S}}_{{\mathbf{v}}_{i}}^{ * } - {\mathcal{S}}_{{\mathbf{v}}_{i}}\left( m\right) }\end{Vmatrix} \leq  \mathop{\sum }\limits_{{i = 1}}^{n}C\left( \begin{Vmatrix}{\mathcal{S}}_{{\mathbf{v}}_{i}}^{ * }{}^{\left( m + 1\right) }\end{Vmatrix}\right)  \cdot  {\omega }_{{\mathbf{v}}_{i}}^{m + 1}, \tag{18}\]<p></p><p>where \({\left\{  {\mathbf{v}}_{i}\right\}  }_{i = 1}^{n}\) are the vertices of \({\mathcal{G}}_{{\mathcal{S}}^{ * }}\) . It is practically reasonable to set \(m = 1\) to have a piecewise linear surface approximation, since our visual system is insensitive to surface smoothness beyond second order [140]. Considering a fixed number \(n\) of vertices for all meshes, which is the case in our benchmark,; it is clear that comparing algebraic complexities of different surfaces is to compare their respective constants \(C\left( \begin{Vmatrix}{\mathcal{S}}_{v}^{ * }{}^{\left( 2\right) }\end{Vmatrix}\right)\) ,which depend on the curvatures of local surfaces. We can compute the curvature as the integral of local, squared normal curvatures</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \({\left\{  {\mathbf{v}}_{i}\right\}  }_{i = 1}^{n}\) 是 \({\mathcal{G}}_{{\mathcal{S}}^{ * }}\) 的顶点。将 \(m = 1\) 设置为具有分段线性表面近似是合理的，因为我们的视觉系统对超过二阶的表面光滑度不敏感 [140]。考虑到所有网格的顶点数量 \(n\) 是固定的，这在我们的基准测试中是这样的；显然，比较不同表面的代数复杂性就是比较它们各自的常数 \(C\left( \begin{Vmatrix}{\mathcal{S}}_{v}^{ * }{}^{\left( 2\right) }\end{Vmatrix}\right)\)，这取决于局部表面的曲率。我们可以将曲率计算为局部平方法线曲率的积分</p></div><p></p>\[\frac{1}{\pi }{\int }_{0}^{\pi }{\kappa }_{n}^{2}\left( {\mathbf{v},\theta }\right) {d\theta }\]<p></p><p></p>\[= \frac{1}{\pi }{\int }_{0}^{\pi }{\left( {\kappa }_{1}\left( \mathbf{v}\right) {\cos }^{2}\left( \theta \right)  + {\kappa }_{2}\left( \mathbf{v}\right) {\sin }^{2}\left( \theta \right) \right) }^{2}{d\theta }\]<p></p><p></p>\[= \frac{3}{8}{\kappa }_{1}^{2}\left( \mathbf{v}\right)  + \frac{2}{8}{\kappa }_{1}\left( \mathbf{v}\right) {\kappa }_{2}\left( \mathbf{v}\right)  + \frac{3}{8}{\kappa }_{2}^{2}\left( \mathbf{v}\right)  \tag{19}\]<p></p><p></p>\[= \frac{3}{2}{\left( \frac{{\kappa }_{1}\left( \mathbf{v}\right)  + {\kappa }_{2}\left( \mathbf{v}\right) }{2}\right) }^{2} - \frac{1}{2}{\kappa }_{1}\left( \mathbf{v}\right) {\kappa }_{2}\left( \mathbf{v}\right)\]<p></p><p></p>\[= \frac{3}{2}{\varkappa }^{2}\left( \mathbf{v}\right)  - \frac{1}{2}\varrho \left( \mathbf{v}\right) ,\]<p></p><p>where \({\kappa }_{n}\left( {\mathbf{v},\theta }\right)\) denotes the normal curvature at \(\mathbf{v}\) in the direction of \(\theta\) using Euler’s famous formula [141]; \({\kappa }_{1}\left( \mathbf{v}\right)\) and \({\kappa }_{2}\left( \mathbf{v}\right)\) denote the principal curvatures; \(\varkappa \left( \mathbf{v}\right)\) and \(\varrho \left( \mathbf{v}\right)\) denote the mean and Gaussian curvatures at \(\mathbf{v}\) ,respectively. For \({\mathcal{S}}^{ * }\) ,we take the expectation of the integral of local, squared normal curvatures over points on the surface as its algebraic complexity</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \({\kappa }_{n}\left( {\mathbf{v},\theta }\right)\) 表示在方向 \(\theta\) 上 \(\mathbf{v}\) 的法线曲率，使用欧拉著名的公式 [141]；\({\kappa }_{1}\left( \mathbf{v}\right)\) 和 \({\kappa }_{2}\left( \mathbf{v}\right)\) 表示主曲率；\(\varkappa \left( \mathbf{v}\right)\) 和 \(\varrho \left( \mathbf{v}\right)\) 分别表示在 \(\mathbf{v}\) 处的平均曲率和高斯曲率。对于 \({\mathcal{S}}^{ * }\)，我们将局部平方法线曲率在表面上的点的积分的期望值作为其代数复杂性</p></div><p></p>\[{\mathbb{E}}_{{\left\{  {\mathbf{v}}_{i}\right\}  }_{i = 1}^{n} \in  {\mathcal{G}}_{{\mathcal{S}}^{ * }}}\left\lbrack  {\frac{3}{2}{\varkappa }^{2}\left( \mathbf{v}\right)  - \frac{1}{2}\varrho \left( \mathbf{v}\right) }\right\rbrack  . \tag{20}\]<p></p><p>Note that a higher value of Eq. (20) indicates a more complicated surface. We finally divide all the object surfaces in our benchmark into three groups respectively of 972,486, and 162 instances (at the ratio of around \(6 : 3 : 1\) for low-,middle-,and high-complexity groups) according to their computed surface complexities.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>注意，方程 (20) 的较高值表示表面更复杂。我们最终将基准测试中的所有物体表面分别划分为三组，分别为 972、486 和 162 个实例（低、中、高复杂度组的比例约为 \(6 : 3 : 1\)），根据它们计算的表面复杂性。</p></div><h2>Appendix B</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>附录 B</h2></div><h2>Details of Evaluation Metrics</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>评估指标的详细信息</h2></div><p>We use three popular metrics of Chamfer Distance (CD) [135], F-score [136], and Normal Consistency Score (NCS) [9], and also a newly proposed neural metric, termed Neural Feature Similarity (NFS), to quantitatively compare surface meshes obtained from different methods. Details are presented as follows.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们使用三种流行的指标：Chamfer 距离 (CD) [135]、F-score [136] 和法线一致性评分 (NCS) [9]，以及一种新提出的神经指标，称为神经特征相似性 (NFS)，以定量比较从不同方法获得的表面网格。详细信息如下。</p></div><h3>B.1 Popular Evaluation Metrics</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>B.1 流行的评估指标</h3></div><p>Chamfer Distance - Chamfer Distance (CD) is a metric between two point sets; in our case, it is defined over two point sets \(\mathcal{P} = {\left\{  {\mathbf{p}}_{i}\right\}  }_{i = 1}^{{n}_{\mathcal{P}}}\) and \(\mathcal{Q} = {\left\{  {\mathbf{q}}_{i}\right\}  }_{i = 1}^{{n}_{\mathcal{Q}}}\) sampled from the reconstructed surface \({\mathcal{G}}_{\mathcal{S}}\) and ground-truth surface \({\mathcal{G}}_{{\mathcal{S}}^{ * }}\) ,respectively. It is a symmetric metric computed as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Chamfer 距离 - Chamfer 距离 (CD) 是两个点集之间的度量；在我们的案例中，它是在从重建表面 \({\mathcal{G}}_{\mathcal{S}}\) 和真实表面 \({\mathcal{G}}_{{\mathcal{S}}^{ * }}\) 中采样的两个点集 \(\mathcal{P} = {\left\{  {\mathbf{p}}_{i}\right\}  }_{i = 1}^{{n}_{\mathcal{P}}}\) 和 \(\mathcal{Q} = {\left\{  {\mathbf{q}}_{i}\right\}  }_{i = 1}^{{n}_{\mathcal{Q}}}\) 上定义的。它是一个对称度量，计算公式为</p></div><p></p>\[\frac{1}{2{n}_{\mathcal{P}}}\mathop{\sum }\limits_{{\mathbf{p} \in  \mathcal{P}}}\mathop{\min }\limits_{{\mathbf{q} \in  \mathcal{Q}}}\parallel \mathbf{p} - \mathbf{q}{\parallel }_{2} + \frac{1}{2{n}_{\mathcal{Q}}}\mathop{\sum }\limits_{{\mathbf{q} \in  \mathcal{Q}}}\mathop{\min }\limits_{{\mathbf{p} \in  \mathcal{P}}}\parallel \mathbf{p} - \mathbf{q}{\parallel }_{2}.\]<p></p><p>We set \({n}_{\mathcal{P}} = {n}_{\mathcal{Q}} = {200}\mathrm{k}\) and \({n}_{\mathcal{P}} = {n}_{\mathcal{Q}} = {1500}\mathrm{k}\) for evaluation on the synthetic data of object surfaces and scene surfaces, respectively.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们分别为物体表面和场景表面的合成数据设置 \({n}_{\mathcal{P}} = {n}_{\mathcal{Q}} = {200}\mathrm{k}\) 和 \({n}_{\mathcal{P}} = {n}_{\mathcal{Q}} = {1500}\mathrm{k}\) 进行评估。</p></div><p>F-score - Similar to CD, F-score is also a metric between two point sets. Different from CD,F-score acts more like an \({l}_{0}\) -norm measurement,since it adopts an indication function \({\mathbf{1}}_{\text{condition }}\left\lbrack  \cdot \right\rbrack\) (shortened as \({\mathbf{1}}_{c}\left\lbrack  \cdot \right\rbrack\) ) to measure the precision and recall between two point sets, and it also takes the harmonic mean between the precision and recall, which means that it will be dominated by the minimum of either precision or recall. Therefore, though it still specializes in measuring the overall similarity between two shapes, it is more sensitive to extreme cases in which either precision or recall is bad. Specifically,let \(\Phi \left( {\mathbf{p};\mathcal{Q}}\right)  = \mathop{\min }\limits_{{\mathbf{q} \in  \mathcal{Q}}}\parallel \mathbf{p} - \mathbf{q}{\parallel }_{2}\) and \(\Phi \left( {\mathbf{q};\mathcal{P}}\right)  = \mathop{\min }\limits_{{\mathbf{p} \in  \mathcal{P}}}\parallel \mathbf{p} - \mathbf{q}{\parallel }_{2}\) ; the precision can be defined as \(D\left( {\mathcal{P};\mathcal{Q},\tau }\right)  = \frac{1}{{n}_{\mathcal{P}}}\mathop{\sum }\limits_{{\mathbf{p} \in  \mathcal{P}}}{\mathbf{1}}_{c}\left\lbrack  {\Phi \left( {\mathbf{p};\mathcal{Q}}\right)  &#x3C; \tau }\right\rbrack\) ,and the recall can be defined as \(D\left( {\mathcal{Q};\mathcal{P},\tau }\right)  = \frac{1}{{n}_{\mathcal{Q}}}\mathop{\sum }\limits_{{\mathbf{q} \in  \mathcal{Q}}}{\mathbf{1}}_{c}\left\lbrack  {\Phi \left( {\mathbf{q};\mathcal{P}}\right)  &#x3C; \tau }\right\rbrack\) . The overall F-score is computed as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>F-score - 类似于CD，F-score也是两个点集之间的度量。不同于CD，F-score更像是一种\({l}_{0}\)-范数测量，因为它采用指示函数\({\mathbf{1}}_{\text{condition }}\left\lbrack  \cdot \right\rbrack\)（简称\({\mathbf{1}}_{c}\left\lbrack  \cdot \right\rbrack\)）来衡量两个点集之间的精确度和召回率，并且它还取精确度和召回率的调和平均，这意味着它将受到精确度或召回率中较小值的主导。因此，尽管它仍然专注于测量两个形状之间的整体相似性，但它对精确度或召回率较差的极端情况更为敏感。具体来说，设\(\Phi \left( {\mathbf{p};\mathcal{Q}}\right)  = \mathop{\min }\limits_{{\mathbf{q} \in  \mathcal{Q}}}\parallel \mathbf{p} - \mathbf{q}{\parallel }_{2}\)和\(\Phi \left( {\mathbf{q};\mathcal{P}}\right)  = \mathop{\min }\limits_{{\mathbf{p} \in  \mathcal{P}}}\parallel \mathbf{p} - \mathbf{q}{\parallel }_{2}\)；精确度可以定义为\(D\left( {\mathcal{P};\mathcal{Q},\tau }\right)  = \frac{1}{{n}_{\mathcal{P}}}\mathop{\sum }\limits_{{\mathbf{p} \in  \mathcal{P}}}{\mathbf{1}}_{c}\left\lbrack  {\Phi \left( {\mathbf{p};\mathcal{Q}}\right)  &#x3C; \tau }\right\rbrack\)，召回率可以定义为\(D\left( {\mathcal{Q};\mathcal{P},\tau }\right)  = \frac{1}{{n}_{\mathcal{Q}}}\mathop{\sum }\limits_{{\mathbf{q} \in  \mathcal{Q}}}{\mathbf{1}}_{c}\left\lbrack  {\Phi \left( {\mathbf{q};\mathcal{P}}\right)  &#x3C; \tau }\right\rbrack\)。整体F-score的计算为</p></div><p></p>\[\frac{{2D}\left( {\mathcal{P};\mathcal{Q},\tau }\right) D\left( {\mathcal{Q};\mathcal{P},\tau }\right) }{D\left( {\mathcal{P};\mathcal{Q},\tau }\right)  + D\left( {\mathcal{Q};\mathcal{P},\tau }\right) } \times  {100},\]<p></p><p>where \(\tau\) is a hyper-parameter controlling the sensitivity of F-score. We set \(\tau  = {0.005}\) with \({n}_{\mathcal{P}} = {n}_{\mathcal{Q}} = {200}\mathrm{k}\) and \(\tau  = {0.03}\) with \({n}_{\mathcal{P}} = {n}_{\mathcal{Q}} = {1500}\mathrm{k}\) for evaluation on the synthetic data of object surfaces and scene surfaces,respectively,and set \({n}_{\mathcal{P}} = {n}_{\mathcal{Q}}\) to be the number of points in the obtained point clouds by high-precision, real scanning (i.e., the ground-truth surfaces of our real-scanned data),with \(\tau  = {0.5}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(\tau\)是控制F-score敏感性的超参数。我们将\(\tau  = {0.005}\)设置为\({n}_{\mathcal{P}} = {n}_{\mathcal{Q}} = {200}\mathrm{k}\)，将\(\tau  = {0.03}\)设置为\({n}_{\mathcal{P}} = {n}_{\mathcal{Q}} = {1500}\mathrm{k}\)，以评估对象表面和场景表面的合成数据，并将\({n}_{\mathcal{P}} = {n}_{\mathcal{Q}}\)设置为通过高精度真实扫描获得的点云中的点数（即我们真实扫描数据的真实表面），与\(\tau  = {0.5}\)。</p></div><p>Normal Consistency Score - Normal Consistency Score (NCS) is a metric measuring the consistency between two vector fields. As such, it measures the difference of higher order information between different surfaces, and is more sensitive to subtle shape differences. It can be computed as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>法线一致性分数 - 法线一致性分数（NCS）是一种度量两个向量场之间一致性的指标。因此，它测量不同表面之间高阶信息的差异，并对微妙的形状差异更为敏感。它可以计算为</p></div><p></p>\[\frac{1}{2}\left( {\frac{1}{{n}_{\mathcal{P}}}\mathop{\sum }\limits_{{\mathbf{p} \in  \mathcal{P}}}\left| \left\langle  {{\mathbf{n}}_{{\mathcal{T}}_{\mathbf{p}}},{\mathbf{n}}_{{\mathcal{T}}_{\Phi \left( {\mathbf{p};\mathcal{Q}}\right) }}}\right\rangle  \right|  + \frac{1}{{n}_{\mathcal{Q}}}\mathop{\sum }\limits_{{\mathbf{q} \in  \mathcal{Q}}}\left| \left\langle  {{\mathbf{n}}_{{\mathcal{T}}_{\mathbf{q}}},{\mathbf{n}}_{{\mathcal{T}}_{\Phi \left( {\mathbf{q};\mathcal{P}}\right) }}}\right\rangle  \right| }\right)\]<p></p><p>where \({\mathbf{n}}_{{\mathcal{T}}_{\mathcal{D}}}\) denotes the normal estimated from the continuous triangular facet \({\mathcal{T}}_{p}\) at point \(p\) . We use \({n}_{{\mathcal{T}}_{p}}\) instead of the \({n}_{p}\) estimated from the discrete point cloud to reduce the error brought by discrete point sampling. We set \({n}_{\mathcal{P}} = {n}_{\mathcal{Q}} = {200}\mathrm{k}\) and \({n}_{\mathcal{P}} = {n}_{\mathcal{Q}} = {1500}\mathrm{k}\) for evaluation on the synthetic data of object surfaces and scene surfaces respectively,and set \({n}_{\mathcal{P}} = {n}_{\mathcal{Q}}\) to be the number of points in the obtained point clouds by high-precision, real scanning (i.e., the ground-truth surfaces of our real-scanned data).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \({\mathbf{n}}_{{\mathcal{T}}_{\mathcal{D}}}\) 表示在点 \(p\) 处从连续三角面片 \({\mathcal{T}}_{p}\) 估计的法线。我们使用 \({n}_{{\mathcal{T}}_{p}}\) 而不是从离散点云估计的 \({n}_{p}\)，以减少离散点采样带来的误差。我们分别设置 \({n}_{\mathcal{P}} = {n}_{\mathcal{Q}} = {200}\mathrm{k}\) 和 \({n}_{\mathcal{P}} = {n}_{\mathcal{Q}} = {1500}\mathrm{k}\) 用于对物体表面和场景表面的合成数据进行评估，并将 \({n}_{\mathcal{P}} = {n}_{\mathcal{Q}}\) 设置为通过高精度真实扫描获得的点云中的点数（即我们真实扫描数据的真实表面）。</p></div><h3>B.2 The Proposed Neural Evaluation Metric</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>B.2 提出的神经评估指标</h3></div><p>Neural Feature Similarity - The aforementioned three metrics are popularly used in existing surface reconstruction evaluation; however, they may fail to capture the shape differences that are more consistent with our human perception. Inspired by [137], we propose to compare the similarity between two shapes in the deep feature space, which depends more on the high-level semantic information and the resulting comparison would be more consistent with human perception. More specifically, given the deep features for different surfaces, we use cosine similarity to compute the proposed Neural Feature Similarity (NFS)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>神经特征相似性 - 前述三种指标在现有的表面重建评估中被广泛使用；然而，它们可能无法捕捉与我们人类感知更一致的形状差异。受到 [137] 的启发，我们提出在深度特征空间中比较两个形状的相似性，这更依赖于高级语义信息，结果的比较将更符合人类感知。更具体地说，给定不同表面的深度特征，我们使用余弦相似性来计算提出的神经特征相似性（NFS）。</p></div><p></p>\[\frac{{\mathbf{g}}_{\mathbf{\theta }}\left( \mathcal{P}\right)  \cdot  {\mathbf{g}}_{\mathbf{\theta }}\left( \mathcal{Q}\right) }{{\begin{Vmatrix}{\mathbf{g}}_{\mathbf{\theta }}\left( \mathcal{P}\right) \end{Vmatrix}}_{2}{\begin{Vmatrix}{\mathbf{g}}_{\mathbf{\theta }}\left( \mathcal{Q}\right) \end{Vmatrix}}_{2}},\]<p></p><p>where the feature-extracting network \({g}_{\theta }\) takes in a point cloud \(\mathcal{P}\) and outputs a feature vector.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中特征提取网络 \({g}_{\theta }\) 输入一个点云 \(\mathcal{P}\) 并输出一个特征向量。</p></div><p>We train the network \({g}_{\theta }\) in a self-supervised manner,such that features of point clouds sampled from the same surface are as invariant as possible, and features of point clouds sampled from different surfaces are different. We use the following self-supervision training objective</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们以自监督的方式训练网络 \({g}_{\theta }\)，使得从同一表面采样的点云特征尽可能不变，而从不同表面采样的点云特征则有所不同。我们使用以下自监督训练目标。</p></div><p></p>\[\mathop{\sum }\limits_{{i,{i}^{\prime }}}\left| {\frac{{\mathbf{g}}_{\mathbf{\theta }}\left( {\mathcal{P}}_{i}\right)  \cdot  {\mathbf{g}}_{\mathbf{\theta }}\left( {\mathcal{P}}_{{i}^{\prime }}\right) }{{\begin{Vmatrix}{\mathbf{g}}_{\mathbf{\theta }}\left( {\mathcal{P}}_{i}\right) \end{Vmatrix}}_{2}{\begin{Vmatrix}{\mathbf{g}}_{\mathbf{\theta }}\left( {\mathcal{P}}_{{i}^{\prime }}\right) \end{Vmatrix}}_{2}} - 1}\right|  + \mathop{\sum }\limits_{{i,j}}\left| \frac{{\mathbf{g}}_{\mathbf{\theta }}\left( {\mathcal{P}}_{i}\right)  \cdot  {\mathbf{g}}_{\mathbf{\theta }}\left( {\mathcal{P}}_{j}\right) }{{\begin{Vmatrix}{\mathbf{g}}_{\mathbf{\theta }}\left( {\mathcal{P}}_{i}\right) \end{Vmatrix}}_{2}{\begin{Vmatrix}{\mathbf{g}}_{\mathbf{\theta }}\left( {\mathcal{P}}_{j}\right) \end{Vmatrix}}_{2}}\right| ,\]<p></p><p>(21)</p><p>where the index pair \(\left\{  {i,{i}^{\prime }}\right\}\) means that the point clouds \(\left\{  {{\mathcal{P}}_{i},{\mathcal{P}}_{{i}^{\prime }}}\right\}\) are sampled from the same surface,and \(\{ i,j\}\) for those from different surfaces. To further improve the accuracy, we take the aligned local surface patches as the input, instead of the whole surface, and take the average of the differences overs all the local surfaces to be the final result. We adopt a 6-layer, MLP-based auto-decoder (with the activation function of Leaky ReLU, and 256 channels per layer) as \({\mathbf{g}}_{\mathbf{\theta }}\left( \cdot \right)\) ,and each local surface patch is represented as a 256 dimensional latent vector. We use Adam [142] with the learning rate initially set as 0.0001 and decreased by half every 200 epochs; we train for a total of 1000 epochs.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中索引对 \(\left\{  {i,{i}^{\prime }}\right\}\) 表示点云 \(\left\{  {{\mathcal{P}}_{i},{\mathcal{P}}_{{i}^{\prime }}}\right\}\) 来自同一表面，而 \(\{ i,j\}\) 表示来自不同表面。为了进一步提高准确性，我们将对齐的局部表面片作为输入，而不是整个表面，并将所有局部表面上的差异的平均值作为最终结果。我们采用一个6层的基于MLP的自编码器（激活函数为Leaky ReLU，每层256个通道）作为 \({\mathbf{g}}_{\mathbf{\theta }}\left( \cdot \right)\)，每个局部表面片表示为一个256维的潜在向量。我们使用Adam [142]，初始学习率设置为0.0001，每200个周期减半；我们总共训练1000个周期。</p></div><h2>Appendix C More Results for the Synthetic Data of Object Surfaces</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>附录C 物体表面合成数据的更多结果</h2></div><p>In this section, we present the overall results on the testing synthetic data of object surfaces as in Table 10.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本节中，我们展示了物体表面测试合成数据的整体结果，如表10所示。</p></div><h2>Appendix D More Results for Studying the Importance of Ori- entations of Surface Normals</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>附录D 研究表面法线方向重要性的更多结果</h2></div><p>In this section, we show more results of scene surfaces and real-scanned data for studying the importance of orientations of surface normals. Table 11 shows qualitative results of scene surfaces that compare methods using surfaces normals, those without using surfaces normals, and those using surface normals only during learning. Those on real-scanned data are shown in Table 12.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本节中，我们展示了场景表面和真实扫描数据的更多结果，以研究表面法线方向的重要性。表11显示了使用表面法线的方法、未使用表面法线的方法以及仅在学习期间使用表面法线的方法的场景表面的定性结果。真实扫描数据的结果见表12。</p></div><h2>Appendix E Experimental Results Without Data Pre- processing</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>附录E 无数据预处理的实验结果</h2></div><p>We have reported and discussed the results in Section 5.2 that are obtained with the pre-processing pipeline. In this section, we report results without the pre-processing pipeline. The results with and without pre-processing are of similar comparative qualities, which are given in Table 13 and Table 14 respectively for synthetic data of object surfaces and synthetic data of scene surfaces. Note that the results of real-scanned data are not involved in this section as the scanners we used automatically pre-process the point clouds with their inbuilt pre-processing methods.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们在第5.2节中报告并讨论了通过预处理管道获得的结果。在本节中，我们报告无预处理管道的结果。无论是有预处理还是无预处理的结果，其比较质量相似，分别在表13和表14中给出，涉及物体表面的合成数据和场景表面的合成数据。请注意，真实扫描数据的结果不在本节中，因为我们使用的扫描仪会自动使用其内置的预处理方法对点云进行预处理。</p></div><h2>Appendix F</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>附录F</h2></div><h2>More Results for the Real-scanned Data</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>真实扫描数据的更多结果</h2></div><p>Fig. 13 shows more qualitative results on the real-scanned data. The results suggest that quality of a scanned point cloud depends heavily on the surface material. Other observations are consistent with those in Section 6.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图13展示了真实扫描数据的更多定性结果。结果表明，扫描点云的质量在很大程度上依赖于表面材料。其他观察结果与第6节中的结果一致。</p></div><!-- Media --><p>TABLE 10: Quantitative results on the testing synthetic data of object surfaces. For those challenges with varying levels of severity, we report results in the format of ". / . / . ", which, from left to right, are results for low-, middle-, and high-level severities of each challenge. Results of the best and second best methods are highlighted in each column.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表10：物体表面测试合成数据的定量结果。对于不同严重程度的挑战，我们以“. / . / . ”的格式报告结果，从左到右分别是每个挑战的低、中、高级别的结果。每列中最佳和第二最佳方法的结果被突出显示。</p></div><table><tbody><tr><td rowspan="2">Algorithms</td><td colspan="6">CD \( \left( {\times {10}^{-4}}\right)  \downarrow \)</td></tr><tr><td>Perfect scanning</td><td>Non-uniform distrubution</td><td>Point-wise noise low- /middle- /high-level</td><td>Point outliers low- /middle- /high-level</td><td>Missing points low- /middle- /high-level</td><td>Misalignment low- /middle- /high-level</td></tr><tr><td>GD [33]</td><td>14.24</td><td>14.87</td><td>16.52 / 18.20 / 29.73</td><td>103.77 /123.19 / 155.98</td><td>44.33 / 64.15 / 110.74</td><td>16.68 / 21.15 / 39.49</td></tr><tr><td>BPA [34]</td><td>14.89</td><td>17.13</td><td>16.59 / 18.58 / 29.30</td><td>14.41 / 14.66 / 16.15</td><td>50.99 / 62.55 / 121.59</td><td>\( {16.61}/{20.88}/{42.46} \)</td></tr><tr><td>SPSR [36]</td><td>14.47</td><td>15.36</td><td>15.50 / 16.05 / 18.16</td><td>14.35 / 14.71 / 28.60</td><td>200.72 /225.66 / 307.75</td><td>\( {15.69}/{17.24}/{25.96} \)</td></tr><tr><td>RIMLS [53]</td><td>15.73</td><td>16.74</td><td>16.13 / 17.17 / 25.88</td><td>17.52 /126.40 / 223.90</td><td>48.93 / 65.36 / 113.44</td><td>17.42 / 21.12 / 41.12</td></tr><tr><td>SALD [73]</td><td>15.10</td><td>14.96</td><td>15.33 / 18.77 / 27.59</td><td>16.83 / 53.65 / 145.05</td><td>40.96 / 55.63 / 105.23</td><td>15.50 / 20.09 / 37.16</td></tr><tr><td>IGR [71]</td><td>18.40</td><td>18.33</td><td>\( {18.21}/{18.57}/{19.20} \)</td><td>49.11 / 43.60 / 67.25</td><td>94.92 /151.53 / 223.64</td><td>19.65 / 20.72 / 25.98</td></tr><tr><td>OccNet [9]</td><td>201.96</td><td>210.80</td><td>209.04 /205.21 / 210.01</td><td>218.23 /225.85 / 299.09</td><td>220.23 /231.65 / 264.24</td><td>207.90 /212.51 / 220.07</td></tr><tr><td>DeepSDF [8]</td><td>229.18</td><td>227.42</td><td>241.28 /230.40 / 239.63</td><td>336.64 /511.36/548.08</td><td>372.86 /378.58 / 447.27</td><td>234.42 /232.16 / 241.64</td></tr><tr><td>LIG [10]</td><td>23.09</td><td>24.03</td><td>23.96 / 22.05 / 26.27</td><td>25.34 /115.38 / 168.64</td><td>52.27 / 70.98 / 115.09</td><td>26.03 / 24.30 / 33.34</td></tr><tr><td>Points2Surf [78]</td><td>17.18</td><td>18.81</td><td>17.74 / 18.48 / 22.63</td><td>28.25 / 83.91 / 144.02</td><td>80.84 /102.18 / 159.02</td><td>17.82 / 20.36 / 33.98</td></tr><tr><td>DSE [114]</td><td>14.26</td><td>15.34</td><td>16.07 / 17.89 / 27.61</td><td>21.32 /100.37 / 158.73</td><td>50.11 / 68.88 / 118.86</td><td>16.06 / 20.06 / 37.98</td></tr><tr><td>IMLSNet [115]</td><td>22.56</td><td>23.17</td><td>22.64 / 22.67 / 23.73</td><td>24.35 / 99.95 / 157.23</td><td>54.93 / 74.35 / 123.95</td><td>23.24 / 23.77 / 30.76</td></tr><tr><td>ParseNet [40]</td><td>162.94</td><td>161.14</td><td>154.11 /135.84 / 139.97</td><td>157.15 /176.38 / 213.48</td><td>187.96 /195.98 / 248.70</td><td>141.95 /136.86 / 141.87</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">算法</td><td colspan="6">光盘 \( \left( {\times {10}^{-4}}\right)  \downarrow \)</td></tr><tr><td>完美扫描</td><td>非均匀分布</td><td>逐点噪声低/中/高级别</td><td>点离群值低/中/高级别</td><td>缺失点低/中/高级别</td><td>未对齐低/中/高级别</td></tr><tr><td>GD [33]</td><td>14.24</td><td>14.87</td><td>16.52 / 18.20 / 29.73</td><td>103.77 /123.19 / 155.98</td><td>44.33 / 64.15 / 110.74</td><td>16.68 / 21.15 / 39.49</td></tr><tr><td>BPA [34]</td><td>14.89</td><td>17.13</td><td>16.59 / 18.58 / 29.30</td><td>14.41 / 14.66 / 16.15</td><td>50.99 / 62.55 / 121.59</td><td>\( {16.61}/{20.88}/{42.46} \)</td></tr><tr><td>SPSR [36]</td><td>14.47</td><td>15.36</td><td>15.50 / 16.05 / 18.16</td><td>14.35 / 14.71 / 28.60</td><td>200.72 /225.66 / 307.75</td><td>\( {15.69}/{17.24}/{25.96} \)</td></tr><tr><td>RIMLS [53]</td><td>15.73</td><td>16.74</td><td>16.13 / 17.17 / 25.88</td><td>17.52 /126.40 / 223.90</td><td>48.93 / 65.36 / 113.44</td><td>17.42 / 21.12 / 41.12</td></tr><tr><td>SALD [73]</td><td>15.10</td><td>14.96</td><td>15.33 / 18.77 / 27.59</td><td>16.83 / 53.65 / 145.05</td><td>40.96 / 55.63 / 105.23</td><td>15.50 / 20.09 / 37.16</td></tr><tr><td>IGR [71]</td><td>18.40</td><td>18.33</td><td>\( {18.21}/{18.57}/{19.20} \)</td><td>49.11 / 43.60 / 67.25</td><td>94.92 /151.53 / 223.64</td><td>19.65 / 20.72 / 25.98</td></tr><tr><td>OccNet [9]</td><td>201.96</td><td>210.80</td><td>209.04 /205.21 / 210.01</td><td>218.23 /225.85 / 299.09</td><td>220.23 /231.65 / 264.24</td><td>207.90 /212.51 / 220.07</td></tr><tr><td>DeepSDF [8]</td><td>229.18</td><td>227.42</td><td>241.28 /230.40 / 239.63</td><td>336.64 /511.36/548.08</td><td>372.86 /378.58 / 447.27</td><td>234.42 /232.16 / 241.64</td></tr><tr><td>LIG [10]</td><td>23.09</td><td>24.03</td><td>23.96 / 22.05 / 26.27</td><td>25.34 /115.38 / 168.64</td><td>52.27 / 70.98 / 115.09</td><td>26.03 / 24.30 / 33.34</td></tr><tr><td>Points2Surf [78]</td><td>17.18</td><td>18.81</td><td>17.74 / 18.48 / 22.63</td><td>28.25 / 83.91 / 144.02</td><td>80.84 /102.18 / 159.02</td><td>17.82 / 20.36 / 33.98</td></tr><tr><td>DSE [114]</td><td>14.26</td><td>15.34</td><td>16.07 / 17.89 / 27.61</td><td>21.32 /100.37 / 158.73</td><td>50.11 / 68.88 / 118.86</td><td>16.06 / 20.06 / 37.98</td></tr><tr><td>IMLSNet [115]</td><td>22.56</td><td>23.17</td><td>22.64 / 22.67 / 23.73</td><td>24.35 / 99.95 / 157.23</td><td>54.93 / 74.35 / 123.95</td><td>23.24 / 23.77 / 30.76</td></tr><tr><td>ParseNet [40]</td><td>162.94</td><td>161.14</td><td>154.11 /135.84 / 139.97</td><td>157.15 /176.38 / 213.48</td><td>187.96 /195.98 / 248.70</td><td>141.95 /136.86 / 141.87</td></tr></tbody></table></div><table><tbody><tr><td rowspan="2">Algorithms</td><td colspan="6">F-score (%) ↑</td></tr><tr><td>Perfect scanning</td><td>Non-uniform distrubution</td><td>Point-wise noise low- /middle- /high-level</td><td>Point outliers low- /middle- /high-level</td><td>Missing points low- /middle- /high-level</td><td>Misalignment low- /middle- /high-level</td></tr><tr><td>GD [33]</td><td>99.66</td><td>99.07</td><td>99.09 / 98.91 / 91.65</td><td>69.42 / 54.59 / 48.63</td><td>91.48 / 87.72 / 80.71</td><td>98.97 / 97.41 / 73.91</td></tr><tr><td>BPA [34]</td><td>98.70</td><td>97.02</td><td>98.63 / 98.51 / 89.59</td><td>99.37 / 99.31 / 98.69</td><td>89.98 / 87.85 / 78.56</td><td>98.56 / 97.24 / 68.27</td></tr><tr><td>SPSR [36]</td><td>99.59</td><td>99.02</td><td>99.51 / 99.46 / 99.16</td><td>99.67 / 99.65 / 97.14</td><td>81.85 / 76.91 / 65.41</td><td>99.42 / 99.27 / 91.99</td></tr><tr><td>RIMLS [53]</td><td>99.27</td><td>98.76</td><td>99.36 / 99.24 / 94.69</td><td>98.66 / 57.78 / 19.84</td><td>90.78 / 87.92 / 81.00</td><td>98.89 / 97.53 / 74.85</td></tr><tr><td>SALD [73]</td><td>99.45</td><td>99.10</td><td>99.54 / 98.94 / 92.48</td><td>98.75 / 88.05 / 59.41</td><td>89.43 / 85.78 / 76.66</td><td>99.48 / 98.09 / 75.83</td></tr><tr><td>IGR [71]</td><td>97.54</td><td>97.79</td><td>97.87 | 97.75 | 97.52</td><td>95.22 / 91.11 / 81.87</td><td>79.30 / 70.49 / 56.99</td><td>97.01 / 96.46 / 92.55</td></tr><tr><td>OccNet [9]</td><td>31.03</td><td>29.90</td><td>29.85 / 29.77 / 27.92</td><td>26.43 / 23.75 / 15.11</td><td>\( {25.20}/{24.52}/{21.68} \)</td><td>31.23 / 29.19 / 26.42</td></tr><tr><td>DeepSDF [8]</td><td>17.79</td><td>18.81</td><td>16.70 / 17.08 / 15.99</td><td>\( {12.10}/{4.15}/{3.07} \)</td><td>15.43 / 14.06 / 13.73</td><td>18.36 / 17.05 / 14.63</td></tr><tr><td>LIG [10]</td><td>96.20</td><td>95.32</td><td>94.50 / 96.99 / 93.80</td><td>96.48 / 76.69 / 57.33</td><td>86.92 / 82.43 / 73.88</td><td>93.80 / 95.01 / 84.50</td></tr><tr><td>Points2Surf [78]</td><td>98.14</td><td>97.72</td><td>98.02 / 97.39 / 94.21</td><td>92.69 / 72.94 / 50.56</td><td>81.42 / 76.46 / 66.68</td><td>97.98 / 96.49 / 82.90</td></tr><tr><td>DSE [114]</td><td>99.64</td><td>98.84</td><td>99.40 / 99.17 / 92.75</td><td>95.13 / 52.21 / 30.82</td><td>90.72 / 87.71 / 80.19</td><td>99.29 / 98.20 / 75.07</td></tr><tr><td>IMLSNet [115]</td><td>94.82</td><td>94.51</td><td>94.41 / 94.36 / 94.40</td><td>94.78 / 64.55 / 38.53</td><td>83.39 / 80.01 / 72.60</td><td>94.18 / 94.14 / 87.44</td></tr><tr><td>ParseNet [40]</td><td>40.52</td><td>38.82</td><td>44.80 / 44.13 / 42.66</td><td>41.20 / 37.21 / 34.04</td><td>37.12 / 41.28 / 41.68</td><td>44.99 / 46.60 / 43.84</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">算法</td><td colspan="6">F值（%）↑</td></tr><tr><td>完美扫描</td><td>非均匀分布</td><td>逐点噪声（低/中/高等级）</td><td>点离群值（低/中/高等级）</td><td>缺失点（低/中/高等级）</td><td>未对齐（低/中/高等级）</td></tr><tr><td>GD [33]</td><td>99.66</td><td>99.07</td><td>99.09 / 98.91 / 91.65</td><td>69.42 / 54.59 / 48.63</td><td>91.48 / 87.72 / 80.71</td><td>98.97 / 97.41 / 73.91</td></tr><tr><td>BPA [34]</td><td>98.70</td><td>97.02</td><td>98.63 / 98.51 / 89.59</td><td>99.37 / 99.31 / 98.69</td><td>89.98 / 87.85 / 78.56</td><td>98.56 / 97.24 / 68.27</td></tr><tr><td>SPSR [36]</td><td>99.59</td><td>99.02</td><td>99.51 / 99.46 / 99.16</td><td>99.67 / 99.65 / 97.14</td><td>81.85 / 76.91 / 65.41</td><td>99.42 / 99.27 / 91.99</td></tr><tr><td>RIMLS [53]</td><td>99.27</td><td>98.76</td><td>99.36 / 99.24 / 94.69</td><td>98.66 / 57.78 / 19.84</td><td>90.78 / 87.92 / 81.00</td><td>98.89 / 97.53 / 74.85</td></tr><tr><td>SALD [73]</td><td>99.45</td><td>99.10</td><td>99.54 / 98.94 / 92.48</td><td>98.75 / 88.05 / 59.41</td><td>89.43 / 85.78 / 76.66</td><td>99.48 / 98.09 / 75.83</td></tr><tr><td>IGR [71]</td><td>97.54</td><td>97.79</td><td>97.87 | 97.75 | 97.52</td><td>95.22 / 91.11 / 81.87</td><td>79.30 / 70.49 / 56.99</td><td>97.01 / 96.46 / 92.55</td></tr><tr><td>OccNet [9]</td><td>31.03</td><td>29.90</td><td>29.85 / 29.77 / 27.92</td><td>26.43 / 23.75 / 15.11</td><td>\( {25.20}/{24.52}/{21.68} \)</td><td>31.23 / 29.19 / 26.42</td></tr><tr><td>DeepSDF [8]</td><td>17.79</td><td>18.81</td><td>16.70 / 17.08 / 15.99</td><td>\( {12.10}/{4.15}/{3.07} \)</td><td>15.43 / 14.06 / 13.73</td><td>18.36 / 17.05 / 14.63</td></tr><tr><td>LIG [10]</td><td>96.20</td><td>95.32</td><td>94.50 / 96.99 / 93.80</td><td>96.48 / 76.69 / 57.33</td><td>86.92 / 82.43 / 73.88</td><td>93.80 / 95.01 / 84.50</td></tr><tr><td>Points2Surf [78]</td><td>98.14</td><td>97.72</td><td>98.02 / 97.39 / 94.21</td><td>92.69 / 72.94 / 50.56</td><td>81.42 / 76.46 / 66.68</td><td>97.98 / 96.49 / 82.90</td></tr><tr><td>DSE [114]</td><td>99.64</td><td>98.84</td><td>99.40 / 99.17 / 92.75</td><td>95.13 / 52.21 / 30.82</td><td>90.72 / 87.71 / 80.19</td><td>99.29 / 98.20 / 75.07</td></tr><tr><td>IMLSNet [115]</td><td>94.82</td><td>94.51</td><td>94.41 / 94.36 / 94.40</td><td>94.78 / 64.55 / 38.53</td><td>83.39 / 80.01 / 72.60</td><td>94.18 / 94.14 / 87.44</td></tr><tr><td>ParseNet [40]</td><td>40.52</td><td>38.82</td><td>44.80 / 44.13 / 42.66</td><td>41.20 / 37.21 / 34.04</td><td>37.12 / 41.28 / 41.68</td><td>44.99 / 46.60 / 43.84</td></tr></tbody></table></div><table><tbody><tr><td rowspan="2">Algorithms</td><td colspan="6">NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>Perfect scanning</td><td>Non-uniform distrubution</td><td>Point-wise noise low- /middle- /high-level</td><td>Point outliers low- /middle- /high-level</td><td>Missing points low- /middle- /high-level</td><td>Misalignment low- /middle- /high-level</td></tr><tr><td>GD [33]</td><td>98.57</td><td>98.05</td><td>94.00 / 87.58 / 68.32</td><td>94.94 / 92.17 / 86.34</td><td>96.44 / 95.17 / 93.19</td><td>94.94 / 84.96 / 65.74</td></tr><tr><td>BPA [34]</td><td>98.37</td><td>97.89</td><td>95.36 / 91.68 / 77.97</td><td>98.17 / 98.07 / 97.17</td><td>95.18 / 94.42 / 91.91</td><td>96.03 / 90.12 / 71.33</td></tr><tr><td>SPSR [36]</td><td>98.58</td><td>98.38</td><td>97.84 / 97.03 / 93.89</td><td>98.61 / 98.56 / 97.87</td><td>91.52 / 89.99 / 85.91</td><td>97.99 / 96.24 / 91.86</td></tr><tr><td>RIMLS [53]</td><td>98.19</td><td>97.77</td><td>97.36 / 95.23 / 86.13</td><td>97.70 / 83.42 / 67.51</td><td>94.43 / 93.27 / 90.93</td><td>96.66 / 92.48 / 79.68</td></tr><tr><td>SALD [73]</td><td>98.67</td><td>98.52</td><td>98.07 / 96.42 / 88.91</td><td>98.39 / 95.32 / 89.92</td><td>96.17 / 95.19 / 91.71</td><td>98.10 / 94.73 / 86.26</td></tr><tr><td>IGR [71]</td><td>97.62</td><td>97.59</td><td>97.64 / 97.52 / 97.10</td><td>96.99 / 96.47 / 94.30</td><td>92.86 / 90.61 / 87.69</td><td>97.35 / 97.04 / 95.97</td></tr><tr><td>OccNet [9]</td><td>79.55</td><td>79.30</td><td>79.43 / 79.64 / 79.05</td><td>79.33 | 78.55 | 74.60</td><td>78.97 / 78.43 / 77.10</td><td>79.08 / 79.58 / 78.69</td></tr><tr><td>DeepSDF [8]</td><td>78.65</td><td>79.11</td><td>78.01 / 77.78 / 78.52</td><td>74.65 / 73.40 / 75.85</td><td>74.97 / 74.52 / 72.29</td><td>78.59 / 78.12 / 77.69</td></tr><tr><td>LIG [10]</td><td>96.49</td><td>95.79</td><td>93.96 / 94.22 / 88.26</td><td>96.81 / 91.66 / 87.75</td><td>91.76 / 89.56 / 85.37</td><td>93.98 / 92.70 / 87.32</td></tr><tr><td>Points2Surf [78]</td><td>95.24</td><td>95.09</td><td>94.97 / 94.62 / 93.44</td><td>94.04 / 87.87 / 83.17</td><td>88.41 / 86.36 / 82.05</td><td>95.14 / 94.48 / 90.33</td></tr><tr><td>DSE [114]</td><td>98.60</td><td>97.86</td><td>94.43 / 87.79 / 71.39</td><td>94.98 / 77.34 / 73.29</td><td>95.43 / 94.40 / 92.49</td><td>95.13 / 86.20 / 68.12</td></tr><tr><td>IMLSNet [115]</td><td>96.13</td><td>95.98</td><td>96.08 / 96.02 / 95.62</td><td>95.75 / 87.45 / 79.76</td><td>92.22 / 90.48 / 86.66</td><td>95.93 / 95.87 / 93.84</td></tr><tr><td>ParseNet [40]</td><td>77.71</td><td>76.89</td><td>78.55 / 80.31 / \( \overline{81.15} \)</td><td>77.73 / 75.46 / 75.48</td><td>75.10 / 75.83 / 74.82</td><td>78.40 / 80.48 / 79.31</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">算法</td><td colspan="6">网络控制系统（NCS） \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>完美扫描</td><td>非均匀分布</td><td>逐点噪声低/中/高级别</td><td>点离群值低/中/高级别</td><td>缺失点低/中/高级别</td><td>未对准低/中/高级别</td></tr><tr><td>梯度下降法（GD） [33]</td><td>98.57</td><td>98.05</td><td>94.00 / 87.58 / 68.32</td><td>94.94 / 92.17 / 86.34</td><td>96.44 / 95.17 / 93.19</td><td>94.94 / 84.96 / 65.74</td></tr><tr><td>信念指派（BPA） [34]</td><td>98.37</td><td>97.89</td><td>95.36 / 91.68 / 77.97</td><td>98.17 / 98.07 / 97.17</td><td>95.18 / 94.42 / 91.91</td><td>96.03 / 90.12 / 71.33</td></tr><tr><td>稀疏点云表面重建（SPSR） [36]</td><td>98.58</td><td>98.38</td><td>97.84 / 97.03 / 93.89</td><td>98.61 / 98.56 / 97.87</td><td>91.52 / 89.99 / 85.91</td><td>97.99 / 96.24 / 91.86</td></tr><tr><td>鲁棒迭代移动最小二乘法（RIMLS） [53]</td><td>98.19</td><td>97.77</td><td>97.36 / 95.23 / 86.13</td><td>97.70 / 83.42 / 67.51</td><td>94.43 / 93.27 / 90.93</td><td>96.66 / 92.48 / 79.68</td></tr><tr><td>稀疏自适应局部密度（SALD） [73]</td><td>98.67</td><td>98.52</td><td>98.07 / 96.42 / 88.91</td><td>98.39 / 95.32 / 89.92</td><td>96.17 / 95.19 / 91.71</td><td>98.10 / 94.73 / 86.26</td></tr><tr><td>迭代高斯细化（IGR） [71]</td><td>97.62</td><td>97.59</td><td>97.64 / 97.52 / 97.10</td><td>96.99 / 96.47 / 94.30</td><td>92.86 / 90.61 / 87.69</td><td>97.35 / 97.04 / 95.97</td></tr><tr><td>占用网络（OccNet） [9]</td><td>79.55</td><td>79.30</td><td>79.43 / 79.64 / 79.05</td><td>79.33 | 78.55 | 74.60</td><td>78.97 / 78.43 / 77.10</td><td>79.08 / 79.58 / 78.69</td></tr><tr><td>深度符号距离函数（DeepSDF） [8]</td><td>78.65</td><td>79.11</td><td>78.01 / 77.78 / 78.52</td><td>74.65 / 73.40 / 75.85</td><td>74.97 / 74.52 / 72.29</td><td>78.59 / 78.12 / 77.69</td></tr><tr><td>局部隐式网格（LIG） [10]</td><td>96.49</td><td>95.79</td><td>93.96 / 94.22 / 88.26</td><td>96.81 / 91.66 / 87.75</td><td>91.76 / 89.56 / 85.37</td><td>93.98 / 92.70 / 87.32</td></tr><tr><td>点云到曲面（Points2Surf） [78]</td><td>95.24</td><td>95.09</td><td>94.97 / 94.62 / 93.44</td><td>94.04 / 87.87 / 83.17</td><td>88.41 / 86.36 / 82.05</td><td>95.14 / 94.48 / 90.33</td></tr><tr><td>动态表面提取（DSE） [114]</td><td>98.60</td><td>97.86</td><td>94.43 / 87.79 / 71.39</td><td>94.98 / 77.34 / 73.29</td><td>95.43 / 94.40 / 92.49</td><td>95.13 / 86.20 / 68.12</td></tr><tr><td>迭代移动最小二乘网络（IMLSNet） [115]</td><td>96.13</td><td>95.98</td><td>96.08 / 96.02 / 95.62</td><td>95.75 / 87.45 / 79.76</td><td>92.22 / 90.48 / 86.66</td><td>95.93 / 95.87 / 93.84</td></tr><tr><td>解析网络（ParseNet） [40]</td><td>77.71</td><td>76.89</td><td>78.55 / 80.31 / \( \overline{81.15} \)</td><td>77.73 / 75.46 / 75.48</td><td>75.10 / 75.83 / 74.82</td><td>78.40 / 80.48 / 79.31</td></tr></tbody></table></div><table><tbody><tr><td rowspan="2">Algorithms</td><td colspan="6">NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>Perfect scanning</td><td>Non-uniform distrubution</td><td>Point-wise noise low- /middle- /high-level</td><td>Point outliers low- /middle- /high-level</td><td>Missing points low- /middle- /high-level</td><td>Misalignment low- /middle- /high-level</td></tr><tr><td>GD [33]</td><td>95.22</td><td>95.19</td><td>91.57 / 84.63 / 56.30</td><td>57.51 / 37.47 / 30.62</td><td>\( \mathbf{{86.87}/{82.93}/{75.28}} \)</td><td>92.04 / 80.69 / 51.41</td></tr><tr><td>BPA [34]</td><td>94.10</td><td>93.60</td><td>90.71 / 87.49 / 65.15</td><td>96.46 / 93.24 / 82.34</td><td>84.23 / 80.36 / 70.09</td><td>91.91 / 81.96 / 59.61</td></tr><tr><td>SPSR [36]</td><td>96.38</td><td>96.22</td><td>95.60 / 94.98 / 91.99</td><td>96.69 / 96.31 / 90.02</td><td>74.48 / 69.34 / 55.89</td><td>\( \mathbf{{95.58}/{94.28}/{87.03}} \)</td></tr><tr><td>RIMLS [53]</td><td>95.01</td><td>94.02</td><td>94.19 / 92.67 / 79.76</td><td>92.61 / 62.01 / 24.49</td><td>83.71 / 79.23 / 69.24</td><td>92.19 / 87.12 / 68.26</td></tr><tr><td>SALD [73]</td><td>96.11</td><td>96.65</td><td>95.66 / 89.72 / 67.18</td><td>83.22 / 51.74 / 34.70</td><td>87.38 / 84.01 / 73.50</td><td>95.38 / 88.26 / 63.13</td></tr><tr><td>IGR [71]</td><td>94.71</td><td>94.22</td><td>94.37 / 94.52 / 93.67</td><td>90.31 / 84.63 / 74.59</td><td>75.56 / 68.14 / 56.29</td><td>93.16 / 92.54 / 90.18</td></tr><tr><td>OccNet [9]</td><td>47.33</td><td>46.55</td><td>46.17 / 46.03 / 45.04</td><td>44.29 / 42.11 / 30.77</td><td>44.18 / 42.46 / 39.09</td><td>46.75 / 45.80 / 43.73</td></tr><tr><td>DeepSDF [8]</td><td>39.94</td><td>40.91</td><td>38.55 / 39.29 / 38.70</td><td>31.69 / 16.65 / 11.92</td><td>33.54 / 31.59 / 28.93</td><td>39.31 / 39.26 / 37.39</td></tr><tr><td>LIG [10]</td><td>91.58</td><td>90.66</td><td>86.71 / 89.55 / 81.03</td><td>89.40 / 66.21 / 46.81</td><td>80.10 / 74.98 / 66.64</td><td>85.20 / 84.34 / 77.92</td></tr><tr><td>Points2Surf [78]</td><td>93.45</td><td>93.23</td><td>92.83 / 92.59 / 90.21</td><td>86.59 / 63.30 / 41.44</td><td>74.47 / 68.53 / 57.08</td><td>92.96 / 91.59 / 83.19</td></tr><tr><td>DSE [114]</td><td>94.50</td><td>94.75</td><td>90.05 / 83.53 / 57.08</td><td>85.39 / 42.32 / 27.29</td><td>85.40 / 79.62 / 71.06</td><td>89.87 / 76.63 / 52.40</td></tr><tr><td>IMLSNet [115]</td><td>90.61</td><td>90.20</td><td>90.13 / 89.97 / 89.66</td><td>89.16 / 52.82 / 29.13</td><td>79.12 / 74.59 / 65.23</td><td>89.68 / 89.19 / 85.55</td></tr><tr><td>ParseNet [40]</td><td>38.54</td><td>37.71</td><td>46.21 / 49.30 / 49.00</td><td>38.19 / 35.98 / 34.58</td><td>36.52 / 38.40 / 35.07</td><td>44.52 / 45.73 / 45.76</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">算法</td><td colspan="6">网络文件系统（NFS） \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>完美扫描</td><td>非均匀分布</td><td>逐点噪声低/中/高级别</td><td>点离群值低/中/高级别</td><td>缺失点低/中/高级别</td><td>未对准低/中/高级别</td></tr><tr><td>梯度下降法（GD） [33]</td><td>95.22</td><td>95.19</td><td>91.57 / 84.63 / 56.30</td><td>57.51 / 37.47 / 30.62</td><td>\( \mathbf{{86.87}/{82.93}/{75.28}} \)</td><td>92.04 / 80.69 / 51.41</td></tr><tr><td>信念传播算法（BPA） [34]</td><td>94.10</td><td>93.60</td><td>90.71 / 87.49 / 65.15</td><td>96.46 / 93.24 / 82.34</td><td>84.23 / 80.36 / 70.09</td><td>91.91 / 81.96 / 59.61</td></tr><tr><td>稀疏点云表面重建算法（SPSR） [36]</td><td>96.38</td><td>96.22</td><td>95.60 / 94.98 / 91.99</td><td>96.69 / 96.31 / 90.02</td><td>74.48 / 69.34 / 55.89</td><td>\( \mathbf{{95.58}/{94.28}/{87.03}} \)</td></tr><tr><td>鲁棒迭代移动最小二乘法（RIMLS） [53]</td><td>95.01</td><td>94.02</td><td>94.19 / 92.67 / 79.76</td><td>92.61 / 62.01 / 24.49</td><td>83.71 / 79.23 / 69.24</td><td>92.19 / 87.12 / 68.26</td></tr><tr><td>自适应局部密度估计（SALD） [73]</td><td>96.11</td><td>96.65</td><td>95.66 / 89.72 / 67.18</td><td>83.22 / 51.74 / 34.70</td><td>87.38 / 84.01 / 73.50</td><td>95.38 / 88.26 / 63.13</td></tr><tr><td>迭代高斯细化（IGR） [71]</td><td>94.71</td><td>94.22</td><td>94.37 / 94.52 / 93.67</td><td>90.31 / 84.63 / 74.59</td><td>75.56 / 68.14 / 56.29</td><td>93.16 / 92.54 / 90.18</td></tr><tr><td>占用网络（OccNet） [9]</td><td>47.33</td><td>46.55</td><td>46.17 / 46.03 / 45.04</td><td>44.29 / 42.11 / 30.77</td><td>44.18 / 42.46 / 39.09</td><td>46.75 / 45.80 / 43.73</td></tr><tr><td>深度有符号距离函数（DeepSDF） [8]</td><td>39.94</td><td>40.91</td><td>38.55 / 39.29 / 38.70</td><td>31.69 / 16.65 / 11.92</td><td>33.54 / 31.59 / 28.93</td><td>39.31 / 39.26 / 37.39</td></tr><tr><td>局部隐式网格（LIG） [10]</td><td>91.58</td><td>90.66</td><td>86.71 / 89.55 / 81.03</td><td>89.40 / 66.21 / 46.81</td><td>80.10 / 74.98 / 66.64</td><td>85.20 / 84.34 / 77.92</td></tr><tr><td>点云到曲面（Points2Surf） [78]</td><td>93.45</td><td>93.23</td><td>92.83 / 92.59 / 90.21</td><td>86.59 / 63.30 / 41.44</td><td>74.47 / 68.53 / 57.08</td><td>92.96 / 91.59 / 83.19</td></tr><tr><td>动态表面提取（DSE） [114]</td><td>94.50</td><td>94.75</td><td>90.05 / 83.53 / 57.08</td><td>85.39 / 42.32 / 27.29</td><td>85.40 / 79.62 / 71.06</td><td>89.87 / 76.63 / 52.40</td></tr><tr><td>迭代移动最小二乘网络（IMLSNet） [115]</td><td>90.61</td><td>90.20</td><td>90.13 / 89.97 / 89.66</td><td>89.16 / 52.82 / 29.13</td><td>79.12 / 74.59 / 65.23</td><td>89.68 / 89.19 / 85.55</td></tr><tr><td>解析网络（ParseNet） [40]</td><td>38.54</td><td>37.71</td><td>46.21 / 49.30 / 49.00</td><td>38.19 / 35.98 / 34.58</td><td>36.52 / 38.40 / 35.07</td><td>44.52 / 45.73 / 45.76</td></tr></tbody></table></div><!-- figureText: Input PC GT GD [33] BPA [34] SPSR [36] RIMLS [53] SALD [73] IGR [71] DSE [114] IMLSNet [115] ParseNet [40] GT OccNet [9] DeepSDF [8] LIG [10] Points2Surf [78] --><img src="https://cdn.noedgeai.com/bo_d163ugjef24c73d1leb0_23.jpg?x=133&#x26;y=396&#x26;w=1540&#x26;h=1466&#x26;r=0"><p>Fig. 13: Additional qualitative results on the real-scanned data.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图13：真实扫描数据的额外定性结果。</p></div><p>TABLE 11: Comparison on the testing synthetic data of scene surfaces among methods without using surface normals \(\left( \times \right)\) , methods using surfaces normals \(\left( \checkmark \right)\) ,and methods using surface normals only during learning \(\left( *\right)\) . Results are in the format of ". / .", where the left one is obtained assuming the availability of ground-truth camera poses, and the right one is obtained without knowing the camera poses. The best and second best methods are highlighted in each column.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表11：在场景表面的测试合成数据上，不使用表面法线的方法 \(\left( \times \right)\)、使用表面法线的方法 \(\left( \checkmark \right)\) 以及仅在学习过程中使用表面法线的方法 \(\left( *\right)\) 之间的比较。结果采用“./.”的格式，其中左边的值是在假设已知真实相机位姿的情况下得到的，右边的值是在不知道相机位姿的情况下得到的。每列中突出显示了最佳和次佳方法。</p></div><table><tbody><tr><td>Algorithms</td><td>Normals</td><td>CD \( \left( {\times {10}^{-3}}\right)  \downarrow \)</td><td>F-score \( \left( \% \right)  \uparrow \)</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>GD [33]</td><td></td><td>33.85 /</td><td>75.95 /</td><td>62.08 /</td><td>41.92 /</td></tr><tr><td>SALD [73]</td><td>✘</td><td>32.43 /</td><td>79.03 /</td><td>\( \mathbf{{91.58}/} \)</td><td>52.79 /</td></tr><tr><td>DSE [114]</td><td></td><td>32.97 /</td><td>77.53 /</td><td>57.99 /</td><td>41.56 /</td></tr><tr><td>BPA [34]</td><td></td><td>\( {45.82}/{57.08} \)</td><td>53.46 / 47.57</td><td>58.25 / 44.04</td><td>44.19 / 32.01</td></tr><tr><td>SPSR [36]</td><td></td><td>\( \mathbf{{30.47}}/{142.65} \)</td><td>83.22 / 68.06</td><td>83.74 / 61.31</td><td>\( \mathbf{{63.03}}/{41.69} \)</td></tr><tr><td>RIMLS [53]</td><td></td><td>\( {41.45}/{55.79} \)</td><td>74.56 / 65.14</td><td>\( {69.93}/{52.16} \)</td><td>\( {38.13}/{26.55} \)</td></tr><tr><td>IGR [71]</td><td>✓</td><td>31.41/ 352.04</td><td>\( \mathbf{{81.63}}/{63.37} \)</td><td>\( \mathbf{{91.26}}/{67.33} \)</td><td>67.58 / 42.20</td></tr><tr><td>OccNet [9]</td><td></td><td>93.12 / 120.85</td><td>37.75 / 32.42</td><td>85.98 / 64.12</td><td>50.34 / 36.75</td></tr><tr><td>DeepSDF [8]</td><td></td><td>\( - / - \)</td><td>- / -</td><td>- 1 -</td><td>- / -</td></tr><tr><td>LIG [10]</td><td></td><td>\( {41.40}/{81.22} \)</td><td>78.03 / 64.12</td><td>88.12 / 64.53</td><td>59.97 / 39.92</td></tr><tr><td>ParseNet [40]</td><td></td><td>- / -</td><td>-1-</td><td>- / -</td><td>\( - / - \)</td></tr><tr><td>Points2Surf [78]</td><td rowspan="2">*</td><td>36.24 /</td><td>76.14 /</td><td>83.60 /</td><td>61.82 /</td></tr><tr><td>IMLSNet [115]</td><td>35.52 /</td><td>78.05 /</td><td>87.17 /</td><td>61.98 /</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>算法</td><td>法线</td><td>CD \( \left( {\times {10}^{-3}}\right)  \downarrow \)</td><td>F-score \( \left( \% \right)  \uparrow \)</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>GD [33]</td><td></td><td>33.85 /</td><td>75.95 /</td><td>62.08 /</td><td>41.92 /</td></tr><tr><td>SALD [73]</td><td>✘</td><td>32.43 /</td><td>79.03 /</td><td>\( \mathbf{{91.58}/} \)</td><td>52.79 /</td></tr><tr><td>DSE [114]</td><td></td><td>32.97 /</td><td>77.53 /</td><td>57.99 /</td><td>41.56 /</td></tr><tr><td>BPA [34]</td><td></td><td>\( {45.82}/{57.08} \)</td><td>53.46 / 47.57</td><td>58.25 / 44.04</td><td>44.19 / 32.01</td></tr><tr><td>SPSR [36]</td><td></td><td>\( \mathbf{{30.47}}/{142.65} \)</td><td>83.22 / 68.06</td><td>83.74 / 61.31</td><td>\( \mathbf{{63.03}}/{41.69} \)</td></tr><tr><td>RIMLS [53]</td><td></td><td>\( {41.45}/{55.79} \)</td><td>74.56 / 65.14</td><td>\( {69.93}/{52.16} \)</td><td>\( {38.13}/{26.55} \)</td></tr><tr><td>IGR [71]</td><td>✓</td><td>31.41/ 352.04</td><td>\( \mathbf{{81.63}}/{63.37} \)</td><td>\( \mathbf{{91.26}}/{67.33} \)</td><td>67.58 / 42.20</td></tr><tr><td>OccNet [9]</td><td></td><td>93.12 / 120.85</td><td>37.75 / 32.42</td><td>85.98 / 64.12</td><td>50.34 / 36.75</td></tr><tr><td>DeepSDF [8]</td><td></td><td>\( - / - \)</td><td>- / -</td><td>- 1 -</td><td>- / -</td></tr><tr><td>LIG [10]</td><td></td><td>\( {41.40}/{81.22} \)</td><td>78.03 / 64.12</td><td>88.12 / 64.53</td><td>59.97 / 39.92</td></tr><tr><td>ParseNet [40]</td><td></td><td>- / -</td><td>-1-</td><td>- / -</td><td>\( - / - \)</td></tr><tr><td>Points2Surf [78]</td><td rowspan="2">*</td><td>36.24 /</td><td>76.14 /</td><td>83.60 /</td><td>61.82 /</td></tr><tr><td>IMLSNet [115]</td><td>35.52 /</td><td>78.05 /</td><td>87.17 /</td><td>61.98 /</td></tr></tbody></table></div><p>TABLE 12: Comparison on the testing real-scanned data among methods without using surface normals \(\left( \times \right)\) ,methods using surfaces normals \(\left( \checkmark \right)\) ,and methods using surface normals only during learning \(\left( *\right)\) . Results are in the format of ". / .",where the left one is obtained assuming the availability of ground-truth camera poses, and the right one is obtained without knowing the camera poses. The best and second best methods are highlighted in each column.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表12：在不使用表面法线\(\left( \times \right)\)、使用表面法线\(\left( \checkmark \right)\)和仅在学习期间使用表面法线\(\left( *\right)\)的方法之间对测试真实扫描数据的比较。结果格式为“./.”，左侧是基于已知真实相机位姿的结果，右侧是未知相机位姿的结果。每列中最佳和第二最佳方法已被突出显示。</p></div><table><tbody><tr><td>Algorithms</td><td>Normals</td><td>CD \( \left( {\times {10}^{-2}}\right)  \downarrow \)</td><td>F-score (%) \( \uparrow \)</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>GD [33]</td><td></td><td>31.72 /</td><td>87.51 /</td><td>88.86 /</td><td>82.20 /</td></tr><tr><td>SALD [73]</td><td>✘</td><td>31.13 /</td><td>87.72 /</td><td>94.68 /</td><td>86.86 /</td></tr><tr><td>DSE [114]</td><td></td><td>32.16 /</td><td>86.88 /</td><td>87.20 /</td><td>76.81 /</td></tr><tr><td>BPA [34]</td><td></td><td>40.37 / 40.94</td><td>80.95 / 80.49</td><td>87.56 / 87.17</td><td>68.69 / 66.80</td></tr><tr><td>SPSR [36]</td><td></td><td>31.05 / 126.89</td><td>87.74 / 80.67</td><td>\( \mathbf{{94.94}/{92.64}} \)</td><td>\( \mathbf{{89.38}/{79.17}} \)</td></tr><tr><td>RIMLS [53]</td><td></td><td>\( {32.80}/{36.39} \)</td><td>87.05 / 85.50</td><td>91.97 / 90.84</td><td>85.19 / 79.64</td></tr><tr><td>IGR [71]</td><td>✓</td><td>32.70 / 467.06</td><td>87.18 / 76.68</td><td>\( \mathbf{{95.99}}/{93.79} \)</td><td>89.10 / 75.04</td></tr><tr><td>OccNet [9]</td><td></td><td>232.71 / 252.81</td><td>17.11 / 16.57</td><td>80.96 / 80.92</td><td>39.70 / 38.52</td></tr><tr><td>DeepSDF [8]</td><td></td><td>263.92 / 552.25</td><td>\( {19.83}/{15.16} \)</td><td>77.95 / 77.66</td><td>40.95 / 35.21</td></tr><tr><td>LIG [10]</td><td></td><td>\( {48.75}/{79.89} \)</td><td>83.76 / 76.38</td><td>92.57 / 89.46</td><td>\( {81.48}/{72.21} \)</td></tr><tr><td>ParseNet [40]</td><td></td><td>149.96 / 217.29</td><td>38.92 / 31.06</td><td>81.51 / 77.30</td><td>45.67 / 38.48</td></tr><tr><td>Points2Surf [78]</td><td rowspan="2">*</td><td>48.93 /</td><td>80.89 /</td><td>89.52 /</td><td>81.83 /</td></tr><tr><td>IMLSNet [115]</td><td>38.46 /</td><td>82.44 /</td><td>93.31 /</td><td>85.30 /</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>算法</td><td>法线</td><td>CD \( \left( {\times {10}^{-2}}\right)  \downarrow \)</td><td>F-score (%) \( \uparrow \)</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>GD [33]</td><td></td><td>31.72 /</td><td>87.51 /</td><td>88.86 /</td><td>82.20 /</td></tr><tr><td>SALD [73]</td><td>✘</td><td>31.13 /</td><td>87.72 /</td><td>94.68 /</td><td>86.86 /</td></tr><tr><td>DSE [114]</td><td></td><td>32.16 /</td><td>86.88 /</td><td>87.20 /</td><td>76.81 /</td></tr><tr><td>BPA [34]</td><td></td><td>40.37 / 40.94</td><td>80.95 / 80.49</td><td>87.56 / 87.17</td><td>68.69 / 66.80</td></tr><tr><td>SPSR [36]</td><td></td><td>31.05 / 126.89</td><td>87.74 / 80.67</td><td>\( \mathbf{{94.94}/{92.64}} \)</td><td>\( \mathbf{{89.38}/{79.17}} \)</td></tr><tr><td>RIMLS [53]</td><td></td><td>\( {32.80}/{36.39} \)</td><td>87.05 / 85.50</td><td>91.97 / 90.84</td><td>85.19 / 79.64</td></tr><tr><td>IGR [71]</td><td>✓</td><td>32.70 / 467.06</td><td>87.18 / 76.68</td><td>\( \mathbf{{95.99}}/{93.79} \)</td><td>89.10 / 75.04</td></tr><tr><td>OccNet [9]</td><td></td><td>232.71 / 252.81</td><td>17.11 / 16.57</td><td>80.96 / 80.92</td><td>39.70 / 38.52</td></tr><tr><td>DeepSDF [8]</td><td></td><td>263.92 / 552.25</td><td>\( {19.83}/{15.16} \)</td><td>77.95 / 77.66</td><td>40.95 / 35.21</td></tr><tr><td>LIG [10]</td><td></td><td>\( {48.75}/{79.89} \)</td><td>83.76 / 76.38</td><td>92.57 / 89.46</td><td>\( {81.48}/{72.21} \)</td></tr><tr><td>ParseNet [40]</td><td></td><td>149.96 / 217.29</td><td>38.92 / 31.06</td><td>81.51 / 77.30</td><td>45.67 / 38.48</td></tr><tr><td>Points2Surf [78]</td><td rowspan="2">*</td><td>48.93 /</td><td>80.89 /</td><td>89.52 /</td><td>81.83 /</td></tr><tr><td>IMLSNet [115]</td><td>38.46 /</td><td>82.44 /</td><td>93.31 /</td><td>85.30 /</td></tr></tbody></table></div><p>TABLE 13: Quantitative results on the testing synthetic data of object surfaces when the input point clouds are NOT pre-processed (cf. Section 5.2 for details). For those challenges with varying levels of severity, we report results in the format of ". / . / . ", which, from left to right, are the results for low-, middle-, and high-level severities of each challenge. Results of the best and second best methods are highlighted in each column.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表13：在输入点云未经过预处理的情况下，物体表面测试合成数据的定量结果（详见第5.2节）。对于不同严重程度的挑战，我们以“. / . / . ”的格式报告结果，从左到右分别是每个挑战的低、中、高级别的结果。每列中最佳和第二最佳方法的结果被突出显示。</p></div><table><tbody><tr><td rowspan="2">Algorithms</td><td colspan="6">CD \( \left( {\times {10}^{-4}}\right)  \downarrow \)</td></tr><tr><td>Perfect scanning</td><td>Non-uniform distrubution</td><td>Point-wise noise low- /middle- /high-level</td><td>Point outliers low- /middle- /high-level</td><td>Missing points low- /middle- /high-level</td><td>Misalignment low- /middle- /high-level</td></tr><tr><td>GD [33]</td><td>14.35</td><td>14.87</td><td>21.39 / 34.53 / 46.66</td><td>210.20 /581.89 / 233.24</td><td>33.66 / 65.23 / 97.21</td><td>21.65 / 40.52 / 56.94</td></tr><tr><td>BPA [34]</td><td>18.60</td><td>32.68</td><td>23.76 / 49.42 / 55.63</td><td>17.81 / 18.94 / 25.73</td><td>83.17 / 78.99 / 126.46</td><td>56.64 / 36.90 / 68.30</td></tr><tr><td>SPSR [36]</td><td>14.53</td><td>15.51</td><td>15.89 / 18.57 / 22.99</td><td>14.29 / 14.97 / 16.50</td><td>194.67 /220.89 / 300.03</td><td>16.39 / 19.72 / 31.65</td></tr><tr><td>RIMLS [53]</td><td>15.83</td><td>17.23</td><td>18.44 / 26.38 / 43.81</td><td>40.14 /344.67 / 375.24</td><td>41.91 / 66.17 / 98.40</td><td>21.89 / 30.58 / 62.71</td></tr><tr><td>SALD [73]</td><td>15.47</td><td>14.99</td><td>22.98 / 29.76 / 43.16</td><td>26.57 / 58.14 / 133.80</td><td>32.38 / 56.09 / 102.65</td><td>20.34 / 28.50 / 51.31</td></tr><tr><td>IGR [71]</td><td>17.38</td><td>18.36</td><td>18.29 / 19.91 / 21.83</td><td>18.88 / 47.09 / 72.82</td><td>149.17 /210.41 / 285.78</td><td>19.14 / 26.10 / 29.28</td></tr><tr><td>OccNet [9]</td><td>205.54</td><td>207.77</td><td>201.56 /214.18 / 205.90</td><td>215.38 /235.77 / 277.20</td><td>213.21 /227.19 / 271.17</td><td>213.89 /215.39 / 231.11</td></tr><tr><td>DeepSDF [8]</td><td>232.13</td><td>238.46</td><td>241.98 /244.25 / 257.42</td><td>496.53 /681.94 / 797.70</td><td>369.52 /362.66 / 419.42</td><td>250.07 /246.97 / 264.91</td></tr><tr><td>LIG [10]</td><td>25.79</td><td>34.28</td><td>26.99 / 30.14 / 29.71</td><td>93.13 /180.10 / 274.45</td><td>88.36 / 93.67 / 134.45</td><td>29.28 / 31.76 / 38.58</td></tr><tr><td>Points2Surf [78]</td><td>20.99</td><td>22.22</td><td>23.47 / 25.21 / 29.34</td><td>70.12 /114.66 / 138.48</td><td>71.47 / 98.03 / 164.85</td><td>21.81 / 26.36 / 38.98</td></tr><tr><td>DSE [114]</td><td>14.38</td><td>15.40</td><td>19.32 / 27.54 / 40.59</td><td>145.71 /227.83 / 254.67</td><td>43.73 /106.05 / 83.63</td><td>19.81 / 28.73 / 49.27</td></tr><tr><td>IMLSNet [115]</td><td>22.98</td><td>23.37</td><td>22.93 / 23.99 / 33.09</td><td>124.95 / 220.04 / 261.79</td><td>46.41 / 69.55 / 107.22</td><td>21.88 / 23.97 / 45.91</td></tr><tr><td>ParseNet [40]</td><td>156.04</td><td>144.21</td><td>165.10 /158.52 / 157.96</td><td>188.92 /221.36 / 238.92</td><td>180.37 / 210.85 / 289.67</td><td>126.48 /126.36 / 154.52</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">算法</td><td colspan="6">CD \( \left( {\times {10}^{-4}}\right)  \downarrow \)</td></tr><tr><td>完美扫描</td><td>非均匀分布</td><td>点噪声低/中/高水平</td><td>点异常值低/中/高水平</td><td>缺失点低/中/高水平</td><td>错位低/中/高水平</td></tr><tr><td>GD [33]</td><td>14.35</td><td>14.87</td><td>21.39 / 34.53 / 46.66</td><td>210.20 /581.89 / 233.24</td><td>33.66 / 65.23 / 97.21</td><td>21.65 / 40.52 / 56.94</td></tr><tr><td>BPA [34]</td><td>18.60</td><td>32.68</td><td>23.76 / 49.42 / 55.63</td><td>17.81 / 18.94 / 25.73</td><td>83.17 / 78.99 / 126.46</td><td>56.64 / 36.90 / 68.30</td></tr><tr><td>SPSR [36]</td><td>14.53</td><td>15.51</td><td>15.89 / 18.57 / 22.99</td><td>14.29 / 14.97 / 16.50</td><td>194.67 /220.89 / 300.03</td><td>16.39 / 19.72 / 31.65</td></tr><tr><td>RIMLS [53]</td><td>15.83</td><td>17.23</td><td>18.44 / 26.38 / 43.81</td><td>40.14 /344.67 / 375.24</td><td>41.91 / 66.17 / 98.40</td><td>21.89 / 30.58 / 62.71</td></tr><tr><td>SALD [73]</td><td>15.47</td><td>14.99</td><td>22.98 / 29.76 / 43.16</td><td>26.57 / 58.14 / 133.80</td><td>32.38 / 56.09 / 102.65</td><td>20.34 / 28.50 / 51.31</td></tr><tr><td>IGR [71]</td><td>17.38</td><td>18.36</td><td>18.29 / 19.91 / 21.83</td><td>18.88 / 47.09 / 72.82</td><td>149.17 /210.41 / 285.78</td><td>19.14 / 26.10 / 29.28</td></tr><tr><td>OccNet [9]</td><td>205.54</td><td>207.77</td><td>201.56 /214.18 / 205.90</td><td>215.38 /235.77 / 277.20</td><td>213.21 /227.19 / 271.17</td><td>213.89 /215.39 / 231.11</td></tr><tr><td>DeepSDF [8]</td><td>232.13</td><td>238.46</td><td>241.98 /244.25 / 257.42</td><td>496.53 /681.94 / 797.70</td><td>369.52 /362.66 / 419.42</td><td>250.07 /246.97 / 264.91</td></tr><tr><td>LIG [10]</td><td>25.79</td><td>34.28</td><td>26.99 / 30.14 / 29.71</td><td>93.13 /180.10 / 274.45</td><td>88.36 / 93.67 / 134.45</td><td>29.28 / 31.76 / 38.58</td></tr><tr><td>Points2Surf [78]</td><td>20.99</td><td>22.22</td><td>23.47 / 25.21 / 29.34</td><td>70.12 /114.66 / 138.48</td><td>71.47 / 98.03 / 164.85</td><td>21.81 / 26.36 / 38.98</td></tr><tr><td>DSE [114]</td><td>14.38</td><td>15.40</td><td>19.32 / 27.54 / 40.59</td><td>145.71 /227.83 / 254.67</td><td>43.73 /106.05 / 83.63</td><td>19.81 / 28.73 / 49.27</td></tr><tr><td>IMLSNet [115]</td><td>22.98</td><td>23.37</td><td>22.93 / 23.99 / 33.09</td><td>124.95 / 220.04 / 261.79</td><td>46.41 / 69.55 / 107.22</td><td>21.88 / 23.97 / 45.91</td></tr><tr><td>ParseNet [40]</td><td>156.04</td><td>144.21</td><td>165.10 /158.52 / 157.96</td><td>188.92 /221.36 / 238.92</td><td>180.37 / 210.85 / 289.67</td><td>126.48 /126.36 / 154.52</td></tr></tbody></table></div><table><tbody><tr><td rowspan="2">Algorithms</td><td colspan="6">F-score (%) ↑</td></tr><tr><td>Perfect scanning</td><td>Non-uniform distrubution</td><td>Point-wise noise low- /middle- /high-level</td><td>Point outliers low- /middle- /high-level</td><td>Missing points low- /middle- /high-level</td><td>Misalignment low- /middle- /high-level</td></tr><tr><td>GD [33]</td><td>99.64</td><td>98.09</td><td>97.85 / 86.53 / 61.66</td><td>52.79 / 46.20 / 41.27</td><td>\( \mathbf{{93.14}/{86.66}/{82.51}} \)</td><td>97.93 / 82.74 / 53.61</td></tr><tr><td>BPA [34]</td><td>93.15</td><td>89.64</td><td>92.90 / 81.79 / 62.05</td><td>94.58 / 94.27 / 93.55</td><td>83.20 / 79.38 / 72.94</td><td>87.90 / 79.74 / 57.62</td></tr><tr><td>SPSR [36]</td><td>99.50</td><td>98.79</td><td>99.54 / 99.36 / 97.26</td><td>99.67 / 99.57 / 99.45</td><td>82.54 / 77.00 / 65.45</td><td>99.48 / 98.76 / 84.70</td></tr><tr><td>RIMLS [53]</td><td>99.18</td><td>98.39</td><td>98.88 / 94.80 / 66.18</td><td>93.64 / 15.89 / 8.25</td><td>91.63 / 86.51 / 81.59</td><td>97.69 / 89.09 / 56.96</td></tr><tr><td>SALD [73]</td><td>99.47</td><td>99.07</td><td>98.47 / 88.80 / 62.65</td><td>97.41 / 87.41 / 73.92</td><td>91.69 / 85.37 / 78.47</td><td>98.82 / 88.21 / 55.47</td></tr><tr><td>IGR [71]</td><td>97.81</td><td>98.23</td><td>98.26 / 97.25 / 96.53</td><td>97.69 / 90.70 / 85.29</td><td>77.76 / 69.29 / 56.88</td><td>97.22 / 96.35 / 90.88</td></tr><tr><td>OccNet [9]</td><td>29.69</td><td>29.58</td><td>30.37 / 27.71 / 25.01</td><td>26.74 / 20.61 / 16.64</td><td>\( {26.25}/{24.22}/{21.88} \)</td><td>29.67 / 28.69 / 22.36</td></tr><tr><td>DeepSDF [8]</td><td>18.56</td><td>17.57</td><td>16.37 / 15.23 / 12.35</td><td>\( {6.32}/{1.69}/{1.56} \)</td><td>14.54 / 14.95 / 13.37</td><td>17.10 / 13.78 / 10.39</td></tr><tr><td>LIG [10]</td><td>93.94</td><td>92.69</td><td>92.37 / 92.18 / 88.73</td><td>86.32 / 68.05 / 42.68</td><td>78.97 / 75.23 / 67.96</td><td>92.31 / 90.28 / 77.47</td></tr><tr><td>Points2Surf [78]</td><td>95.02</td><td>94.31</td><td>93.94 / 93.77 / 91.17</td><td>86.08 / 71.77 / 59.91</td><td>81.36 / 75.14 / 64.84</td><td>94.70 / 93.40 / 79.40</td></tr><tr><td>DSE [114]</td><td>99.59</td><td>98.43</td><td>98.87 / 92.28 / 68.84</td><td>57.21 / 25.77 / 16.87</td><td>91.52 / 86.17 / 82.10</td><td>98.76 / 89.74 / 58.59</td></tr><tr><td>IMLSNet [115]</td><td>95.47</td><td>94.34</td><td>95.30 / 94.44 / 85.38</td><td>75.40 / 42.20 / 30.26</td><td>85.89 / 79.90 / \( \overline{74.89} \)</td><td>96.09 / 94.74 / 66.41</td></tr><tr><td>ParseNet [40]</td><td>40.99</td><td>42.40</td><td>41.24 / 43.34 / 40.14</td><td>38.23 / 35.00 / 32.44</td><td>41.01 / 41.40 / 36.30</td><td>47.91 / 48.22 / 40.85</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">算法</td><td colspan="6">F-score（%）↑</td></tr><tr><td>完美扫描</td><td>非均匀分布</td><td>点噪声低/中/高水平</td><td>点异常值低/中/高水平</td><td>缺失点低/中/高水平</td><td>错位低/中/高水平</td></tr><tr><td>GD [33]</td><td>99.64</td><td>98.09</td><td>97.85 / 86.53 / 61.66</td><td>52.79 / 46.20 / 41.27</td><td>\( \mathbf{{93.14}/{86.66}/{82.51}} \)</td><td>97.93 / 82.74 / 53.61</td></tr><tr><td>BPA [34]</td><td>93.15</td><td>89.64</td><td>92.90 / 81.79 / 62.05</td><td>94.58 / 94.27 / 93.55</td><td>83.20 / 79.38 / 72.94</td><td>87.90 / 79.74 / 57.62</td></tr><tr><td>SPSR [36]</td><td>99.50</td><td>98.79</td><td>99.54 / 99.36 / 97.26</td><td>99.67 / 99.57 / 99.45</td><td>82.54 / 77.00 / 65.45</td><td>99.48 / 98.76 / 84.70</td></tr><tr><td>RIMLS [53]</td><td>99.18</td><td>98.39</td><td>98.88 / 94.80 / 66.18</td><td>93.64 / 15.89 / 8.25</td><td>91.63 / 86.51 / 81.59</td><td>97.69 / 89.09 / 56.96</td></tr><tr><td>SALD [73]</td><td>99.47</td><td>99.07</td><td>98.47 / 88.80 / 62.65</td><td>97.41 / 87.41 / 73.92</td><td>91.69 / 85.37 / 78.47</td><td>98.82 / 88.21 / 55.47</td></tr><tr><td>IGR [71]</td><td>97.81</td><td>98.23</td><td>98.26 / 97.25 / 96.53</td><td>97.69 / 90.70 / 85.29</td><td>77.76 / 69.29 / 56.88</td><td>97.22 / 96.35 / 90.88</td></tr><tr><td>OccNet [9]</td><td>29.69</td><td>29.58</td><td>30.37 / 27.71 / 25.01</td><td>26.74 / 20.61 / 16.64</td><td>\( {26.25}/{24.22}/{21.88} \)</td><td>29.67 / 28.69 / 22.36</td></tr><tr><td>DeepSDF [8]</td><td>18.56</td><td>17.57</td><td>16.37 / 15.23 / 12.35</td><td>\( {6.32}/{1.69}/{1.56} \)</td><td>14.54 / 14.95 / 13.37</td><td>17.10 / 13.78 / 10.39</td></tr><tr><td>LIG [10]</td><td>93.94</td><td>92.69</td><td>92.37 / 92.18 / 88.73</td><td>86.32 / 68.05 / 42.68</td><td>78.97 / 75.23 / 67.96</td><td>92.31 / 90.28 / 77.47</td></tr><tr><td>Points2Surf [78]</td><td>95.02</td><td>94.31</td><td>93.94 / 93.77 / 91.17</td><td>86.08 / 71.77 / 59.91</td><td>81.36 / 75.14 / 64.84</td><td>94.70 / 93.40 / 79.40</td></tr><tr><td>DSE [114]</td><td>99.59</td><td>98.43</td><td>98.87 / 92.28 / 68.84</td><td>57.21 / 25.77 / 16.87</td><td>91.52 / 86.17 / 82.10</td><td>98.76 / 89.74 / 58.59</td></tr><tr><td>IMLSNet [115]</td><td>95.47</td><td>94.34</td><td>95.30 / 94.44 / 85.38</td><td>75.40 / 42.20 / 30.26</td><td>85.89 / 79.90 / \( \overline{74.89} \)</td><td>96.09 / 94.74 / 66.41</td></tr><tr><td>ParseNet [40]</td><td>40.99</td><td>42.40</td><td>41.24 / 43.34 / 40.14</td><td>38.23 / 35.00 / 32.44</td><td>41.01 / 41.40 / 36.30</td><td>47.91 / 48.22 / 40.85</td></tr></tbody></table></div><table><tbody><tr><td rowspan="2">Algorithms</td><td colspan="6">NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>Perfect scanning</td><td>Non-uniform distrubution</td><td>Point-wise noise low- /middle- /high-level</td><td>Point outliers low- /middle- /high-level</td><td>Missing points low- /middle- /high-level</td><td>Misalignment low- /middle- /high-level</td></tr><tr><td>GD [33]</td><td>97.97</td><td>97.70</td><td>80.31 / 69.32 / 57.33</td><td>89.44 / 83.99 / 81.95</td><td>96.45 / 95.10 / 92.79</td><td>80.56 / 65.52 / 57.00</td></tr><tr><td>BPA [34]</td><td>95.10</td><td>95.54</td><td>82.88 / 73.14 / 60.40</td><td>93.38 / 90.07 / 92.08</td><td>93.55 / 91.15 / 91.64</td><td>85.24 / 67.24 / 63.88</td></tr><tr><td>SPSR [36]</td><td>98.15</td><td>97.91</td><td>95.89 / 93.51 / 91.06</td><td>98.14 / 97.11 / 96.98</td><td>91.38 / 89.74 / 85.60</td><td>\( \mathbf{{95.79}/{93.19}/{88.58}} \)</td></tr><tr><td>RIMLS [53]</td><td>97.41</td><td>96.66</td><td>93.10 / 82.03 / 70.01</td><td>94.64 / 71.32 / 65.36</td><td>93.69 / 92.39 / 89.56</td><td>92.01 / 80.65 / 69.78</td></tr><tr><td>SALD [73]</td><td>98.44</td><td>98.27</td><td>90.35 / 87.75 / 77.57</td><td>97.30 / 94.56 / 90.70</td><td>96.84 / 95.05 / 90.76</td><td>92.79 / 87.70 / 78.10</td></tr><tr><td>IGR [71]</td><td>97.23</td><td>97.17</td><td>\( \mathbf{{97.13}/{96.99}/{96.34}} \)</td><td>97.13 / 95.00 / 93.13</td><td>90.53 / 88.92 / 85.10</td><td>96.69 / 96.12 / 94.78</td></tr><tr><td>OccNet [9]</td><td>79.22</td><td>79.60</td><td>79.74 / 79.32 / 79.59</td><td>78.40 / 76.36 / 73.57</td><td>79.25 / 78.71 / 76.77</td><td>79.10 / 79.32 / 78.96</td></tr><tr><td>DeepSDF [8]</td><td>78.14</td><td>78.59</td><td>78.37 / 78.37 / 78.02</td><td>72.72 / 73.01 / 72.32</td><td>74.37 / 75.04 / 72.09</td><td>78.13 / 78.36 / 78.12</td></tr><tr><td>LIG [10]</td><td>95.36</td><td>95.16</td><td>94.67 / 94.25 / 93.59</td><td>93.36 / 88.43 / 83.12</td><td>88.84 / 87.37 / 84.52</td><td>94.73 / 93.99 / 92.58</td></tr><tr><td>Points2Surf [78]</td><td>94.06</td><td>93.24</td><td>93.11 / 92.84 / 91.54</td><td>90.57 / 86.71 / 84.08</td><td>88.35 / 85.21 / 80.18</td><td>92.98 / 92.38 / \( \overline{88.55} \)</td></tr><tr><td>DSE [114]</td><td>98.07</td><td>97.15</td><td>80.51 / 69.65 / 54.34</td><td>75.48 / 66.52 / 65.12</td><td>94.90 / 93.58 / 90.60</td><td>80.88 / 65.81 / 54.49</td></tr><tr><td>IMLSNet [115]</td><td>95.64</td><td>95.57</td><td>95.20 / 95.34 / 93.57</td><td>89.09 / 80.42 / 77.37</td><td>92.72 / 91.10 / 87.88</td><td>95.56 / 95.17 / 89.83</td></tr><tr><td>ParseNet [40]</td><td>77.50</td><td>77.69</td><td>78.33 / 79.53 / 79.42</td><td>75.94 / 74.43 / 72.79</td><td>75.27 / 73.73 / 71.79</td><td>79.76 / 79.29 / 79.11</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">算法</td><td colspan="6">NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>完美扫描</td><td>非均匀分布</td><td>点噪声低/中/高水平</td><td>点异常值低/中/高水平</td><td>缺失点低/中/高水平</td><td>错位低/中/高水平</td></tr><tr><td>GD [33]</td><td>97.97</td><td>97.70</td><td>80.31 / 69.32 / 57.33</td><td>89.44 / 83.99 / 81.95</td><td>96.45 / 95.10 / 92.79</td><td>80.56 / 65.52 / 57.00</td></tr><tr><td>BPA [34]</td><td>95.10</td><td>95.54</td><td>82.88 / 73.14 / 60.40</td><td>93.38 / 90.07 / 92.08</td><td>93.55 / 91.15 / 91.64</td><td>85.24 / 67.24 / 63.88</td></tr><tr><td>SPSR [36]</td><td>98.15</td><td>97.91</td><td>95.89 / 93.51 / 91.06</td><td>98.14 / 97.11 / 96.98</td><td>91.38 / 89.74 / 85.60</td><td>\( \mathbf{{95.79}/{93.19}/{88.58}} \)</td></tr><tr><td>RIMLS [53]</td><td>97.41</td><td>96.66</td><td>93.10 / 82.03 / 70.01</td><td>94.64 / 71.32 / 65.36</td><td>93.69 / 92.39 / 89.56</td><td>92.01 / 80.65 / 69.78</td></tr><tr><td>SALD [73]</td><td>98.44</td><td>98.27</td><td>90.35 / 87.75 / 77.57</td><td>97.30 / 94.56 / 90.70</td><td>96.84 / 95.05 / 90.76</td><td>92.79 / 87.70 / 78.10</td></tr><tr><td>IGR [71]</td><td>97.23</td><td>97.17</td><td>\( \mathbf{{97.13}/{96.99}/{96.34}} \)</td><td>97.13 / 95.00 / 93.13</td><td>90.53 / 88.92 / 85.10</td><td>96.69 / 96.12 / 94.78</td></tr><tr><td>OccNet [9]</td><td>79.22</td><td>79.60</td><td>79.74 / 79.32 / 79.59</td><td>78.40 / 76.36 / 73.57</td><td>79.25 / 78.71 / 76.77</td><td>79.10 / 79.32 / 78.96</td></tr><tr><td>DeepSDF [8]</td><td>78.14</td><td>78.59</td><td>78.37 / 78.37 / 78.02</td><td>72.72 / 73.01 / 72.32</td><td>74.37 / 75.04 / 72.09</td><td>78.13 / 78.36 / 78.12</td></tr><tr><td>LIG [10]</td><td>95.36</td><td>95.16</td><td>94.67 / 94.25 / 93.59</td><td>93.36 / 88.43 / 83.12</td><td>88.84 / 87.37 / 84.52</td><td>94.73 / 93.99 / 92.58</td></tr><tr><td>Points2Surf [78]</td><td>94.06</td><td>93.24</td><td>93.11 / 92.84 / 91.54</td><td>90.57 / 86.71 / 84.08</td><td>88.35 / 85.21 / 80.18</td><td>92.98 / 92.38 / \( \overline{88.55} \)</td></tr><tr><td>DSE [114]</td><td>98.07</td><td>97.15</td><td>80.51 / 69.65 / 54.34</td><td>75.48 / 66.52 / 65.12</td><td>94.90 / 93.58 / 90.60</td><td>80.88 / 65.81 / 54.49</td></tr><tr><td>IMLSNet [115]</td><td>95.64</td><td>95.57</td><td>95.20 / 95.34 / 93.57</td><td>89.09 / 80.42 / 77.37</td><td>92.72 / 91.10 / 87.88</td><td>95.56 / 95.17 / 89.83</td></tr><tr><td>ParseNet [40]</td><td>77.50</td><td>77.69</td><td>78.33 / 79.53 / 79.42</td><td>75.94 / 74.43 / 72.79</td><td>75.27 / 73.73 / 71.79</td><td>79.76 / 79.29 / 79.11</td></tr></tbody></table></div><p>NCS \(\left( {\times {10}^{-2}}\right)  \uparrow\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>NCS \(\left( {\times {10}^{-2}}\right)  \uparrow\)</p></div><table><tbody><tr><td rowspan="2">Algorithms</td><td colspan="6">NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>Perfect scanning</td><td>Non-uniform distrubution</td><td>Point-wise noise low- /middle- /high-level</td><td>Point outliers low- /middle- /high-level</td><td>Missing points low- /middle- /high-level</td><td>Misalignment low- /middle- /high-level</td></tr><tr><td>GD [33]</td><td>94.72</td><td>94.56</td><td>78.07 / 49.98 / 40.95</td><td>26.75 / 25.46 / 21.15</td><td>\( \mathbf{{90.93}/{81.35}/{77.08}} \)</td><td>69.98 / 47.13 / 38.67</td></tr><tr><td>BPA [34]</td><td>89.93</td><td>86.55</td><td>77.09 / 56.07 / 43.78</td><td>88.92 / 84.62 / 82.52</td><td>78.29 / 72.80 / 61.47</td><td>70.53 / 52.23 / 48.60</td></tr><tr><td>SPSR [36]</td><td>95.90</td><td>95.31</td><td>94.84 / 90.69 / 85.53</td><td>95.81 / 95.61 / 94.90</td><td>75.09 / 69.26 / 55.82</td><td>93.90 / 92.91 / 80.91</td></tr><tr><td>RIMLS [53]</td><td>93.49</td><td>92.67</td><td>91.92 / 75.15 / 56.07</td><td>86.43 / 25.17 / 14.58</td><td>84.01 / 79.01 / 69.45</td><td>86.27 / 73.19 / 51.89</td></tr><tr><td>SALD [73]</td><td>95.08</td><td>96.52</td><td>72.18 / 65.01 / 53.61</td><td>61.66 / 39.89 / 30.76</td><td>90.92 / 84.03 / 72.99</td><td>\( {82.22}/{64.24}/{50.80} \)</td></tr><tr><td>IGR [71]</td><td>94.46</td><td>94.61</td><td>94.13 / 93.96 / 91.60</td><td>93.66 / 85.83 / 76.25</td><td>71.72 / 64.63 / 52.78</td><td>93.17 / 91.31 / 88.68</td></tr><tr><td>OccNet [9]</td><td>46.30</td><td>47.22</td><td>47.29 / 45.31 / 44.84</td><td>45.55 / 39.08 / 32.95</td><td>45.53 / 42.96 / 38.41</td><td>46.00 / 45.48 / 41.75</td></tr><tr><td>DeepSDF [8]</td><td>39.54</td><td>39.18</td><td>38.20 / 37.29 / 35.34</td><td>17.18 / 5.37 / 4.03</td><td>33.24 / 34.14 / 28.85</td><td>38.74 / 36.44 / 32.99</td></tr><tr><td>LIG [10]</td><td>89.82</td><td>88.38</td><td>89.19 / 87.15 / 86.05</td><td>77.04 / 55.26 / 29.19</td><td>70.92 / 68.13 / 60.68</td><td>87.83 / 86.15 / 81.80</td></tr><tr><td>Points2Surf [78]</td><td>91.02</td><td>90.35</td><td>90.28 / 88.29 / 86.35</td><td>72.88 / 52.99 / 38.05</td><td>76.78 / 69.20 / 55.50</td><td>90.01 / 87.84 / 79.58</td></tr><tr><td>DSE [114]</td><td>94.10</td><td>93.07</td><td>81.08 / 54.83 / 40.44</td><td>42.61 / 21.45 / 16.33</td><td>84.40 / 79.08 / 68.31</td><td>70.44 / 52.63 / 40.18</td></tr><tr><td>IMLSNet [115]</td><td>90.61</td><td>90.66</td><td>90.08 / 90.17 / 79.69</td><td>52.75 / 25.59 / 14.72</td><td>81.60 / 76.77 / 67.82</td><td>89.98 / 87.68 / 71.86</td></tr><tr><td>ParseNet [40]</td><td>37.24</td><td>39.70</td><td>40.71 / 46.73 / 45.38</td><td>37.68 / 34.04 / 30.64</td><td>37.56 / 35.37 / 30.72</td><td>46.88 / 44.80 / 43.49</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">算法</td><td colspan="6">NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td>完美扫描</td><td>非均匀分布</td><td>逐点噪声低/中/高水平</td><td>逐点异常值低/中/高水平</td><td>缺失点低/中/高水平</td><td>错位低/中/高水平</td></tr><tr><td>GD [33]</td><td>94.72</td><td>94.56</td><td>78.07 / 49.98 / 40.95</td><td>26.75 / 25.46 / 21.15</td><td>\( \mathbf{{90.93}/{81.35}/{77.08}} \)</td><td>69.98 / 47.13 / 38.67</td></tr><tr><td>BPA [34]</td><td>89.93</td><td>86.55</td><td>77.09 / 56.07 / 43.78</td><td>88.92 / 84.62 / 82.52</td><td>78.29 / 72.80 / 61.47</td><td>70.53 / 52.23 / 48.60</td></tr><tr><td>SPSR [36]</td><td>95.90</td><td>95.31</td><td>94.84 / 90.69 / 85.53</td><td>95.81 / 95.61 / 94.90</td><td>75.09 / 69.26 / 55.82</td><td>93.90 / 92.91 / 80.91</td></tr><tr><td>RIMLS [53]</td><td>93.49</td><td>92.67</td><td>91.92 / 75.15 / 56.07</td><td>86.43 / 25.17 / 14.58</td><td>84.01 / 79.01 / 69.45</td><td>86.27 / 73.19 / 51.89</td></tr><tr><td>SALD [73]</td><td>95.08</td><td>96.52</td><td>72.18 / 65.01 / 53.61</td><td>61.66 / 39.89 / 30.76</td><td>90.92 / 84.03 / 72.99</td><td>\( {82.22}/{64.24}/{50.80} \)</td></tr><tr><td>IGR [71]</td><td>94.46</td><td>94.61</td><td>94.13 / 93.96 / 91.60</td><td>93.66 / 85.83 / 76.25</td><td>71.72 / 64.63 / 52.78</td><td>93.17 / 91.31 / 88.68</td></tr><tr><td>OccNet [9]</td><td>46.30</td><td>47.22</td><td>47.29 / 45.31 / 44.84</td><td>45.55 / 39.08 / 32.95</td><td>45.53 / 42.96 / 38.41</td><td>46.00 / 45.48 / 41.75</td></tr><tr><td>DeepSDF [8]</td><td>39.54</td><td>39.18</td><td>38.20 / 37.29 / 35.34</td><td>17.18 / 5.37 / 4.03</td><td>33.24 / 34.14 / 28.85</td><td>38.74 / 36.44 / 32.99</td></tr><tr><td>LIG [10]</td><td>89.82</td><td>88.38</td><td>89.19 / 87.15 / 86.05</td><td>77.04 / 55.26 / 29.19</td><td>70.92 / 68.13 / 60.68</td><td>87.83 / 86.15 / 81.80</td></tr><tr><td>Points2Surf [78]</td><td>91.02</td><td>90.35</td><td>90.28 / 88.29 / 86.35</td><td>72.88 / 52.99 / 38.05</td><td>76.78 / 69.20 / 55.50</td><td>90.01 / 87.84 / 79.58</td></tr><tr><td>DSE [114]</td><td>94.10</td><td>93.07</td><td>81.08 / 54.83 / 40.44</td><td>42.61 / 21.45 / 16.33</td><td>84.40 / 79.08 / 68.31</td><td>70.44 / 52.63 / 40.18</td></tr><tr><td>IMLSNet [115]</td><td>90.61</td><td>90.66</td><td>90.08 / 90.17 / 79.69</td><td>52.75 / 25.59 / 14.72</td><td>81.60 / 76.77 / 67.82</td><td>89.98 / 87.68 / 71.86</td></tr><tr><td>ParseNet [40]</td><td>37.24</td><td>39.70</td><td>40.71 / 46.73 / 45.38</td><td>37.68 / 34.04 / 30.64</td><td>37.56 / 35.37 / 30.72</td><td>46.88 / 44.80 / 43.49</td></tr></tbody></table></div><p>TABLE 14: Quantitative results on the testing synthetic data of scene surfaces when the input point clouds are Not preprocessed (cf. Section 5.2 for details). Results of the best and second best methods are highlighted in each column. "-" indicates that the method cannot produce reasonable results due to their limited generalization.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表14：当输入点云未经过预处理时，场景表面的测试合成数据的定量结果（详见第5.2节）。每列中突出显示了最佳和第二最佳方法的结果。“-”表示该方法由于其有限的泛化能力无法产生合理的结果。</p></div><table><tbody><tr><td>Prior</td><td>Algorithm</td><td>CD \( \left( {\times {10}^{-3}}\right)  \downarrow \)</td><td>F-score (%) ↑</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td rowspan="2">Triangulation- based</td><td>GD [33]</td><td>34.50</td><td>71.33</td><td>56.78</td><td>38.77</td></tr><tr><td>BPA [34]</td><td>49.01</td><td>55.45</td><td>53.52</td><td>39.87</td></tr><tr><td rowspan="2">Smoothness</td><td>SPSR [36]</td><td>30.63</td><td>83.03</td><td>83.50</td><td>62.46</td></tr><tr><td>RIMLS [53]</td><td>42.53</td><td>69.74</td><td>64.83</td><td>33.82</td></tr><tr><td rowspan="2">Modeling</td><td>SALD [73]</td><td>32.87</td><td>75.15</td><td>91.29</td><td>51.36</td></tr><tr><td>IGR [71]</td><td>32.82</td><td>81.37</td><td>90.32</td><td>66.71</td></tr><tr><td>Learning</td><td>OccNet [9]</td><td>102.75</td><td>34.60</td><td>85.01</td><td>45.95</td></tr><tr><td>Semantic</td><td>DeepSDF [8]</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Local</td><td>LIG [10]</td><td>42.03</td><td>67.42</td><td>74.84</td><td>51.86</td></tr><tr><td>Learning</td><td>Points2Surf [78]</td><td>39.60</td><td>75.00</td><td>82.25</td><td>59.33</td></tr><tr><td rowspan="3">Hybird</td><td>DSE [114]</td><td>33.46</td><td>72.35</td><td>52.70</td><td>38.37</td></tr><tr><td>IMLSNet [115]</td><td>36.22</td><td>75.04</td><td>86.45</td><td>59.40</td></tr><tr><td>ParseNet [40]</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>先前</td><td>算法</td><td>CD \( \left( {\times {10}^{-3}}\right)  \downarrow \)</td><td>F-score (%) ↑</td><td>NCS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td><td>NFS \( \left( {\times {10}^{-2}}\right)  \uparrow \)</td></tr><tr><td rowspan="2">基于三角测量</td><td>GD [33]</td><td>34.50</td><td>71.33</td><td>56.78</td><td>38.77</td></tr><tr><td>BPA [34]</td><td>49.01</td><td>55.45</td><td>53.52</td><td>39.87</td></tr><tr><td rowspan="2">平滑性</td><td>SPSR [36]</td><td>30.63</td><td>83.03</td><td>83.50</td><td>62.46</td></tr><tr><td>RIMLS [53]</td><td>42.53</td><td>69.74</td><td>64.83</td><td>33.82</td></tr><tr><td rowspan="2">建模</td><td>SALD [73]</td><td>32.87</td><td>75.15</td><td>91.29</td><td>51.36</td></tr><tr><td>IGR [71]</td><td>32.82</td><td>81.37</td><td>90.32</td><td>66.71</td></tr><tr><td>学习</td><td>OccNet [9]</td><td>102.75</td><td>34.60</td><td>85.01</td><td>45.95</td></tr><tr><td>语义</td><td>DeepSDF [8]</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>局部</td><td>LIG [10]</td><td>42.03</td><td>67.42</td><td>74.84</td><td>51.86</td></tr><tr><td>学习</td><td>Points2Surf [78]</td><td>39.60</td><td>75.00</td><td>82.25</td><td>59.33</td></tr><tr><td rowspan="3">混合</td><td>DSE [114]</td><td>33.46</td><td>72.35</td><td>52.70</td><td>38.37</td></tr><tr><td>IMLSNet [115]</td><td>36.22</td><td>75.04</td><td>86.45</td><td>59.40</td></tr><tr><td>ParseNet [40]</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></div><!-- Media -->
      </body>
    </html>
  