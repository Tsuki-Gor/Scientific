
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <h1>Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>通过3D生成对抗建模学习物体形状的概率潜在空间</h1></div><p>Jiajun Wu*</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>吴佳俊*</p></div><p>MIT CSAIL</p><p>Chengkai Zhang*</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>张承凯*</p></div><p>MIT CSAIL</p><p>Tianfan Xue</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>薛天凡</p></div><p>MIT CSAIL</p><p>William T. Freeman</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>威廉·T·弗里曼</p></div><p>MIT CSAIL, Google Research</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>麻省理工学院计算机科学与人工智能实验室，谷歌研究</p></div><p>Joshua B. Tenenbaum</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>约书亚·B·特嫩鲍姆</p></div><p>MIT CSAIL</p><h2>Abstract</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>摘要</h2></div><p>We study the problem of \(3\mathrm{D}\) object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of \(3\mathrm{D}\) objects,so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们研究了\(3\mathrm{D}\)物体生成的问题。我们提出了一种新颖的框架，即3D生成对抗网络（3D-GAN），该网络通过利用体积卷积网络和生成对抗网络的最新进展，从概率空间生成3D物体。我们模型的好处有三方面：首先，使用对抗标准而非传统启发式标准，使生成器能够隐式捕捉物体结构，并合成高质量的3D物体；其次，生成器建立了从低维概率空间到\(3\mathrm{D}\)物体空间的映射，使我们能够在没有参考图像或CAD模型的情况下采样物体，并探索3D物体流形；第三，对抗判别器提供了一个强大的3D形状描述符，该描述符在无监督学习的情况下学习，广泛应用于3D物体识别。实验表明，我们的方法生成高质量的3D物体，我们无监督学习的特征在3D物体识别上表现出色，性能与监督学习方法相当。</p></div><h2>1 Introduction</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>1 引言</h2></div><p>What makes a 3D generative model of object shapes appealing? We believe a good generative model should be able to synthesize 3D objects that are both highly varied and realistic. Specifically, for 3D objects to have variations, a generative model should be able to go beyond memorizing and recombining parts or pieces from a pre-defined repository to produce novel shapes; and for objects to be realistic, there need to be fine details in the generated examples.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>什么使得3D物体形状的生成模型具有吸引力？我们认为一个好的生成模型应该能够合成既高度多样又真实的3D物体。具体来说，为了使3D物体具有变化，生成模型应该能够超越记忆和重新组合来自预定义库的部分或片段，以产生新颖的形状；而为了使物体真实，生成的示例中需要有细致的细节。</p></div><p>In the past decades, researchers have made impressive progress on 3D object modeling and synthesis [Van Kaick et al., 2011, Tangelder and Veltkamp, 2008, Carlson, 1982], mostly based on meshes or skeletons. Many of these traditional methods synthesize new objects by borrowing parts from objects in existing CAD model libraries. Therefore, the synthesized objects look realistic, but not conceptually novel.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在过去几十年中，研究人员在3D物体建模和合成方面取得了令人瞩目的进展[Van Kaick et al., 2011, Tangelder and Veltkamp, 2008, Carlson, 1982]，主要基于网格或骨架。这些传统方法中的许多通过借用现有CAD模型库中的物体部分来合成新物体。因此，合成的物体看起来真实，但在概念上并不新颖。</p></div><p>Recently, with the advances in deep representation learning and the introduction of large 3D CAD datasets like ShapeNet [Chang et al., 2015, Wu et al., 2015], there have been some inspiring attempts in learning deep object representations based on voxelized objects [Girdhar et al., 2016, Su et al., 2015a, Qi et al., 2016]. Different from part-based methods, many of these generative approaches do not explicitly model the concept of parts or retrieve them from an object repository; instead, they synthesize new objects based on learned object representations. This is a challenging problem because, compared to the space of 2D images, it is more difficult to model the space of 3D shapes due to its higher dimensionality. Their current results are encouraging, but often there still exist artifacts (e.g., fragments or holes) in the generated objects.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最近，随着深度表示学习的进展和像ShapeNet这样的庞大3D CAD数据集的引入[Chang et al., 2015, Wu et al., 2015]，在基于体素化物体学习深度物体表示方面出现了一些鼓舞人心的尝试[Girdhar et al., 2016, Su et al., 2015a, Qi et al., 2016]。与基于部分的方法不同，这些生成方法中的许多并不明确建模部分的概念或从物体库中检索它们；相反，它们基于学习到的物体表示合成新物体。这是一个具有挑战性的问题，因为与2D图像空间相比，由于其更高的维度，建模3D形状空间更为困难。它们目前的结果令人鼓舞，但生成的物体中仍然存在伪影（例如，碎片或孔洞）。</p></div><p>In this paper, we demonstrate that modeling volumetric objects in a general-adversarial manner could be a promising solution to generate objects that are both novel and realistic. Our approach combines the merits of both general-adversarial modeling [Goodfellow et al., 2014, Radford et al., 2016] and volumetric convolutional networks [Maturana and Scherer, 2015, Wu et al., 2015]. Different from traditional heuristic criteria, generative-adversarial modeling introduces an adversarial discriminator to classify whether an object is synthesized or real. This could be a particularly favorable framework for 3D object modeling: as 3D objects are highly structured, a generative-adversarial criterion, but not a voxel-wise independent heuristic one, has the potential to capture the structural difference of two 3D objects. The use of a generative-adversarial loss may also avoid possible criterion-dependent overfitting (e.g., generating mean-shape-like blurred objects when minimizing a mean squared error).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本文中，我们证明了以一般对抗方式建模体积物体可能是生成既新颖又真实的物体的有希望的解决方案。我们的方法结合了一般对抗建模[Goodfellow et al., 2014, Radford et al., 2016]和体积卷积网络[Maturana and Scherer, 2015, Wu et al., 2015]的优点。与传统的启发式标准不同，生成对抗建模引入了一个对抗判别器来分类一个物体是合成的还是真实的。这可能是3D物体建模的一个特别有利的框架：由于3D物体高度结构化，对抗标准而非体素独立的启发式标准有潜力捕捉两个3D物体的结构差异。使用生成对抗损失也可能避免可能的标准依赖过拟合（例如，在最小化均方误差时生成类似均值形状的模糊物体）。</p></div><hr>
<!-- Footnote --><ul>
<li>indicates equal contributions. Emails: {jiajunwu, ckzhang, tfxue, billf, jbt}@mit.edu</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>表示贡献相等。电子邮件：{jiajunwu, ckzhang, tfxue, billf, jbt}@mit.edu</li>
</ul></div><!-- Footnote -->
<hr><p>Modeling 3D objects in a generative-adversarial way offers additional distinctive advantages. First, it becomes possible to sample novel 3D objects from a probabilistic latent space such as a Gaussian or uniform distribution. Second, the discriminator in the generative-adversarial approach carries informative features for \(3\mathrm{D}\) object recognition,as demonstrated in experiments (Section 4). From a different perspective, instead of learning a single feature representation for both generating and recognizing objects [Girdhar et al., 2016, Sharma et al., 2016], our framework learns disentangled generative and discriminative representations for 3D objects without supervision, and applies them on generation and recognition tasks, respectively.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>以生成对抗的方式建模3D对象提供了额外的独特优势。首先，可以从概率潜在空间（如高斯或均匀分布）中采样新颖的3D对象。其次，生成对抗方法中的判别器携带了用于\(3\mathrm{D}\)对象识别的信息特征，如实验所示（第4节）。从不同的角度来看，我们的框架学习了无监督的3D对象的解耦生成和判别表示，并分别应用于生成和识别任务，而不是为生成和识别对象学习单一的特征表示[Girdhar et al., 2016, Sharma et al., 2016]。</p></div><p>We show that our generative representation can be used to synthesize high-quality realistic objects, and our discriminative representation can be used for 3D object recognition, achieving comparable performance with recent supervised methods [Maturana and Scherer, 2015, Shi et al., 2015], and outperforming other unsupervised methods by a large margin. The learned generative and discriminative representations also have wide applications. For example, we show that our network can be combined with a variational autoencoder [Kingma and Welling, 2014, Larsen et al., 2016] to directly reconstruct a 3D object from a 2D input image. Further, we explore the space of object representations and demonstrate that both our generative and discriminative representations carry rich semantic information about 3D objects.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们展示了我们的生成表示可以用于合成高质量的真实对象，而我们的判别表示可以用于3D对象识别，取得了与最近的监督方法[Maturana and Scherer, 2015, Shi et al., 2015]相当的性能，并且在很大程度上超越了其他无监督方法。学习到的生成和判别表示也具有广泛的应用。例如，我们展示了我们的网络可以与变分自编码器[Kingma and Welling, 2014, Larsen et al., 2016]结合，直接从2D输入图像重建3D对象。此外，我们探索了对象表示的空间，并证明我们的生成和判别表示都携带了关于3D对象的丰富语义信息。</p></div><h2>2 Related Work</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>2 相关工作</h2></div><p>Modeling and synthesizing 3D shapes 3D object understanding and generation is an important problem in the graphics and vision community, and the relevant literature is very rich [Carlson, 1982, Tangelder and Veltkamp, 2008, Van Kaick et al., 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Xue et al., 2012, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]. Since decades ago, AI and vision researchers have made inspiring attempts to design or learn 3D object representations, mostly based on meshes and skeletons. Many of these shape synthesis algorithms are nonparametric and they synthesize new objects by retrieving and combining shapes and parts from a database. Recently, Huang et al. [2015] explored generating 3D shapes with pre-trained templates and producing both object structure and surface geometry. Our framework synthesizes objects without explicitly borrow parts from a repository, and requires no supervision during training.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>建模和合成3D形状 3D对象理解和生成是图形和视觉领域中的一个重要问题，相关文献非常丰富[Carlson, 1982, Tangelder and Veltkamp, 2008, Van Kaick et al., 2011, Blanz and Vetter, 1999, Kalogerakis et al., 2012, Chaudhuri et al., 2011, Xue et al., 2012, Kar et al., 2015, Bansal et al., 2016, Wu et al., 2016]。几十年前，人工智能和视觉研究人员已经进行了激动人心的尝试，设计或学习3D对象表示，主要基于网格和骨架。许多这些形状合成算法是非参数的，它们通过从数据库中检索和组合形状和部件来合成新对象。最近，Huang等人[2015]探索了使用预训练模板生成3D形状，并生成对象结构和表面几何。我们的框架在合成对象时不需要明确借用来自库中的部件，并且在训练过程中不需要监督。</p></div><p>Deep learning for 3D data The vision community have witnessed rapid development of deep networks for various tasks. In the field of 3D object recognition, Li et al. [2015], Su et al. [2015b], Girdhar et al. [2016] proposed to learn a joint embedding of 3D shapes and synthesized images, Su et al. [2015a], Qi et al. [2016] focused on learning discriminative representations for 3D object recognition, Wu et al. [2016], Xiang et al. [2015], Choy et al. [2016] discussed 3D object reconstruction from in-the-wild images, possibly with a recurrent network, and Girdhar et al. [2016], Sharma et al. [2016] explored autoencoder-based networks for learning voxel-based object representations. Wu et al. [2015], Rezende et al. [2016], Yan et al. [2016] attempted to generate 3D objects with deep networks,some using \(2\mathrm{D}\) images during training with a \(3\mathrm{D}\) to \(2\mathrm{D}\) projection layer. Many of these networks can be used for 3D shape classification [Su et al., 2015a, Sharma et al., 2016, Maturana and Scherer, 2015], 3D shape retrieval [Shi et al., 2015, Su et al., 2015a], and single image 3D reconstruction [Kar et al., 2015, Bansal et al., 2016, Girdhar et al., 2016], mostly with full supervision. In comparison, our framework requires no supervision for training, is able to generate objects from a probabilistic space, and comes with a rich discriminative 3D shape representation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>深度学习用于3D数据 视觉领域见证了深度网络在各种任务中的快速发展。在3D对象识别领域，Li等人[2015]、Su等人[2015b]、Girdhar等人[2016]提出学习3D形状和合成图像的联合嵌入，Su等人[2015a]、Qi等人[2016]专注于学习3D对象识别的判别表示，Wu等人[2016]、Xiang等人[2015]、Choy等人[2016]讨论了从野外图像进行3D对象重建，可能使用递归网络，Girdhar等人[2016]、Sharma等人[2016]探索了基于自编码器的网络以学习基于体素的对象表示。Wu等人[2015]、Rezende等人[2016]、Yan等人[2016]尝试使用深度网络生成3D对象，有些在训练过程中使用\(2\mathrm{D}\)图像，并带有\(3\mathrm{D}\)到\(2\mathrm{D}\)的投影层。这些网络中的许多可以用于3D形状分类[Su et al., 2015a, Sharma et al., 2016, Maturana and Scherer, 2015]、3D形状检索[Shi et al., 2015, Su et al., 2015a]和单图像3D重建[Kar et al., 2015, Bansal et al., 2016, Girdhar et al., 2016]，大多需要完全监督。相比之下，我们的框架在训练时不需要监督，能够从概率空间生成对象，并提供丰富的判别3D形状表示。</p></div><p>Learning with an adversarial net Generative Adversarial Nets (GAN) [Goodfellow et al., 2014] proposed to incorporate an adversarial discriminator into the procedure of generative modeling. More recently, LAPGAN [Denton et al., 2015] and DC-GAN [Radford et al., 2016] adopted GAN with convolutional networks for image synthesis, and achieved impressive performance. Researchers have also explored the use of GAN for other vision problems. To name a few, Wang and Gupta [2016] discussed how to model image style and structure with sequential GANs, Li and Wand [2016] and Zhu et al. [2016] used GAN for texture synthesis and image editing, respectively, and Im et al. [2016]</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>与对抗网络的学习 生成对抗网络（GAN）[Goodfellow et al., 2014] 提出了将对抗判别器纳入生成建模过程。最近，LAPGAN [Denton et al., 2015] 和 DC-GAN [Radford et al., 2016] 采用了卷积网络的 GAN 进行图像合成，并取得了令人印象深刻的性能。研究人员还探索了 GAN 在其他视觉问题中的应用。举几个例子，Wang 和 Gupta [2016] 讨论了如何使用序列 GAN 建模图像风格和结构，Li 和 Wand [2016] 以及 Zhu et al. [2016] 分别使用 GAN 进行纹理合成和图像编辑，Im et al. [2016]</p></div><!-- Media --><!-- figureText: \( {512} \times  4 \times  4 \times  4 \) \( {64} \times  {32} \times  {32} \times  {32} \) G(z) in 3D Voxel Space \( {64} \times  {64} \times  {64} \) \( {256} \times  8 \times  8 \times  8 \) \( {128} \times  {16} \times  {16} \times  {16} \) Z --><img src="https://cdn.noedgeai.com/bo_d164kjn7aajc7388jsmg_2.jpg?x=376&#x26;y=185&#x26;w=1056&#x26;h=348&#x26;r=0"><p>Figure 1: The generator in 3D-GAN. The discriminator mostly mirrors the generator. developed a recurrent adversarial network for image generation. While previous approaches focus on modeling 2D images, we discuss the use of an adversarial component in modeling 3D objects.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图 1：3D-GAN 中的生成器。判别器大致镜像生成器。开发了一种用于图像生成的递归对抗网络。虽然之前的方法主要集中在建模 2D 图像上，但我们讨论了在建模 3D 物体时使用对抗组件。</p></div><!-- Media --><h2>3 Models</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>3 模型</h2></div><p>In this section we introduce our model for 3D object generation. We first discuss how we build our framework, 3D Generative Adversarial Network (3D-GAN), by leveraging previous advances on volumetric convolutional networks and generative adversarial nets. We then show how to train a variational autoencoder [Kingma and Welling, 2014] simultaneously so that our framework can capture a mapping from a 2D image to a 3D object.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本节中，我们介绍我们的 3D 物体生成模型。我们首先讨论如何通过利用在体积卷积网络和生成对抗网络上的先前进展来构建我们的框架，3D 生成对抗网络（3D-GAN）。然后，我们展示如何同时训练变分自编码器 [Kingma 和 Welling, 2014]，以便我们的框架能够捕捉从 2D 图像到 3D 物体的映射。</p></div><h3>3.1 3D Generative Adversarial Network (3D-GAN)</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.1 3D 生成对抗网络（3D-GAN）</h3></div><p>As proposed in Goodfellow et al. [2014], the Generative Adversarial Network (GAN) consists of a generator and a discriminator, where the discriminator tries to classify real objects and objects synthesized by the generator, and the generator attempts to confuse the discriminator. In our 3D Generative Adversarial Network (3D-GAN),the generator \(G\) maps a 200-dimensional latent vector \(z\) , randomly sampled from a probabilistic latent space,to a \({64} \times  {64} \times  {64}\) cube,representing an object \(G\left( z\right)\) in 3D voxel space. The discriminator \(D\) outputs a confidence value \(D\left( x\right)\) of whether a 3D object input \(x\) is real or synthetic.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>正如 Goodfellow et al. [2014] 所提出的，生成对抗网络（GAN）由生成器和判别器组成，其中判别器试图对真实物体和生成器合成的物体进行分类，而生成器则试图混淆判别器。在我们的 3D 生成对抗网络（3D-GAN）中，生成器 \(G\) 将从概率潜在空间随机采样的 200 维潜在向量 \(z\) 映射到一个 \({64} \times  {64} \times  {64}\) 立方体，表示 3D 体素空间中的一个物体 \(G\left( z\right)\)。判别器 \(D\) 输出一个置信值 \(D\left( x\right)\)，表示输入的 3D 物体 \(x\) 是真实的还是合成的。</p></div><p>Following Goodfellow et al. [2014], we use binary cross entropy as the classification loss, and present our overall adversarial loss function as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>遵循 Goodfellow et al. [2014]，我们使用二元交叉熵作为分类损失，并将我们的整体对抗损失函数表示为</p></div><p></p>\[{L}_{3\mathrm{D} - \mathrm{{GAN}}} = \log D\left( x\right)  + \log \left( {1 - D\left( {G\left( z\right) }\right) }\right) , \tag{1}\]<p></p><p>where \(x\) is a real object in a \({64} \times  {64} \times  {64}\) space,and \(z\) is a randomly sampled noise vector from a distribution \(p\left( z\right)\) . In this work,each dimension of \(z\) is an i.i.d. uniform distribution over \(\left\lbrack  {0,1}\right\rbrack\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(x\) 是 \({64} \times  {64} \times  {64}\) 空间中的真实物体，而 \(z\) 是从分布 \(p\left( z\right)\) 中随机采样的噪声向量。在本工作中，\(z\) 的每个维度都是在 \(\left\lbrack  {0,1}\right\rbrack\) 上的独立同分布均匀分布。</p></div><p>Network structure Inspired by Radford et al. [2016], we design an all-convolutional neural network to generate 3D objects. As shown in Figure 1, the generator consists of five volumetric fully convolutional layers of kernel sizes \(4 \times  4 \times  4\) and strides 2,with batch normalization and ReLU layers added in between and a Sigmoid layer at the end. The discriminator basically mirrors the generator, except that it uses Leaky ReLU [Maas et al., 2013] instead of ReLU layers. There are no pooling or linear layers in our network. More details can be found in the supplementary material.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>网络结构 受 Radford et al. [2016] 的启发，我们设计了一个全卷积神经网络来生成 3D 物体。如图 1 所示，生成器由五个体积全卷积层组成，卷积核大小为 \(4 \times  4 \times  4\)，步幅为 2，层间添加了批归一化和 ReLU 层，最后有一个 Sigmoid 层。判别器基本上镜像生成器，只是它使用 Leaky ReLU [Maas et al., 2013] 而不是 ReLU 层。我们的网络中没有池化或线性层。更多细节可以在补充材料中找到。</p></div><p>Training details A straightforward training procedure is to update both the generator and the discriminator in every batch. However, the discriminator usually learns much faster than the generator, possibly because generating objects in a 3D voxel space is more difficult than differentiating between real and synthetic objects [Goodfellow et al., 2014, Radford et al., 2016]. It then becomes hard for the generator to extract signals for improvement from a discriminator that is way ahead, as all examples it generated would be correctly identified as synthetic with high confidence. Therefore, to keep the training of both networks in pace, we employ an adaptive training strategy: for each batch, the discriminator only gets updated if its accuracy in the last batch is not higher than 80%. We observe this helps to stabilize the training and to produce better results. We set the learning rate of \(G\) to \({0.0025},D\) to \({10}^{-5}\) ,and use a batch size of 100 . We use ADAM [Kingma and Ba,2015] for optimization,with \(\beta  = {0.5}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>训练细节 一种简单的训练程序是在每个批次中同时更新生成器和判别器。然而，判别器通常学习得比生成器快得多，这可能是因为在3D体素空间中生成对象比区分真实和合成对象更困难[Goodfellow et al., 2014, Radford et al., 2016]。因此，生成器很难从远远领先的判别器中提取改进信号，因为它生成的所有示例都会被高置信度地正确识别为合成的。因此，为了保持两个网络的训练步调一致，我们采用了一种自适应训练策略：对于每个批次，只有当判别器在上一个批次中的准确率不高于80%时，才会更新判别器。我们观察到这有助于稳定训练并产生更好的结果。我们将\(G\)的学习率设置为\({0.0025},D\)，并使用批量大小为100。我们使用ADAM [Kingma和Ba, 2015]进行优化，\(\beta  = {0.5}\)。</p></div><h3>3.2 3D-VAE-GAN</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.2 3D-VAE-GAN</h3></div><p>We have discussed how to generate 3D objects by sampling a latent vector \(z\) and mapping it to the object space. In practice, it would also be helpful to infer these latent vectors from observations. For example, if there exists a mapping from a 2D image to the latent representation, we can then recover the 3D object corresponding to that \(2\mathrm{D}\) image. Following this idea, we introduce 3D-VAE-GAN as an extension to 3D-GAN. We add an additional image encoder \(E\) ,which takes a 2D image \(x\) as input and outputs the latent representation vector \(z\) . This is inspired by VAE-GAN proposed by [Larsen et al., 2016], which combines VAE and GAN by sharing the decoder of VAE with the generator of GAN.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们已经讨论了如何通过采样潜在向量\(z\)并将其映射到对象空间来生成3D对象。在实践中，从观察中推断这些潜在向量也是有帮助的。例如，如果存在从2D图像到潜在表示的映射，我们就可以恢复与该\(2\mathrm{D}\)图像对应的3D对象。基于这个想法，我们引入3D-VAE-GAN作为3D-GAN的扩展。我们添加了一个额外的图像编码器\(E\)，它以2D图像\(x\)为输入，输出潜在表示向量\(z\)。这受到[Larsen et al., 2016]提出的VAE-GAN的启发，该方法通过将VAE的解码器与GAN的生成器共享来结合VAE和GAN。</p></div><p>The 3D-VAE-GAN therefore consists of three components: an image encoder \(E\) ,a decoder (the generator \(G\) in 3D-GAN),and a discriminator \(D\) . The image encoder consists of five spatial convolution layers with kernel size \(\{ {11},5,5,5,8\}\) and strides \(\{ 4,2,2,2,1\}\) ,respectively. There are batch normalization and ReLU layers in between, and a sampler at the end to sample a 200 dimensional vector used by the 3D-GAN. The structures of the generator and the discriminator are the same as those in Section 3.1.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>因此，3D-VAE-GAN由三个组件组成：一个图像编码器\(E\)，一个解码器（3D-GAN中的生成器\(G\)）和一个判别器\(D\)。图像编码器由五个空间卷积层组成，卷积核大小为\(\{ {11},5,5,5,8\}\)，步幅为\(\{ 4,2,2,2,1\}\)。中间有批量归一化和ReLU层，最后有一个采样器，用于采样一个200维的向量，供3D-GAN使用。生成器和判别器的结构与3.1节中的相同。</p></div><p>Similar to VAE-GAN [Larsen et al., 2016], our loss function consists of three parts: an object reconstruction loss \({L}_{\text{recon }}\) ,a cross entropy loss \({L}_{3\mathrm{D} - \mathrm{{GAN}}}\) for \(3\mathrm{D} - \mathrm{{GAN}}\) ,and a KL divergence loss \({L}_{\mathrm{{KL}}}\) to restrict the distribution of the output of the encoder. Formally, these loss functions write as</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>与VAE-GAN [Larsen et al., 2016]类似，我们的损失函数由三部分组成：对象重建损失\({L}_{\text{recon }}\)，用于\({L}_{3\mathrm{D} - \mathrm{{GAN}}}\)的交叉熵损失，以及限制编码器输出分布的KL散度损失\({L}_{\mathrm{{KL}}}\)。形式上，这些损失函数写作</p></div><p></p>\[L = {L}_{3\mathrm{D} - \mathrm{{GAN}}} + {\alpha }_{1}{L}_{\mathrm{{KL}}} + {\alpha }_{2}{L}_{\text{recon }}, \tag{2}\]<p></p><p>where \({\alpha }_{1}\) and \({\alpha }_{2}\) are weights of the KL divergence loss and the reconstruction loss. We have</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\({\alpha }_{1}\)和\({\alpha }_{2}\)是KL散度损失和重建损失的权重。我们有</p></div><p></p>\[{L}_{3\mathrm{D} - \mathrm{{GAN}}} = \log D\left( x\right)  + \log \left( {1 - D\left( {G\left( z\right) }\right) }\right) , \tag{3}\]<p></p><p></p>\[{L}_{\mathrm{{KL}}} = {D}_{\mathrm{{KL}}}\left( {q\left( {z \mid  y}\right) \parallel p\left( z\right) }\right) , \tag{4}\]<p></p><p></p>\[{L}_{\text{recon }} = \parallel G\left( {E\left( y\right) }\right)  - x{\parallel }_{2}, \tag{5}\]<p></p><p>where \(x\) is a 3D shape from the training set, \(y\) is its corresponding 2D image,and \(q\left( {z \mid  y}\right)\) is the variational distribution of the latent representation \(z\) . The KL-divergence pushes this variational distribution towards to the prior distribution \(p\left( z\right)\) ,so that the generator can sample the latent representation \(z\) from the same distribution \(p\left( z\right)\) . In this work,we choose \(p\left( z\right)\) a multivariate Gaussian distribution with zero-mean and unit variance. For more details, please refer to Larsen et al. [2016].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(x\)是来自训练集的3D形状，\(y\)是其对应的2D图像，\(q\left( {z \mid  y}\right)\)是潜在表示\(z\)的变分分布。KL散度将这个变分分布推向先验分布\(p\left( z\right)\)，以便生成器可以从相同的分布\(p\left( z\right)\)中采样潜在表示\(z\)。在这项工作中，我们选择\(p\left( z\right)\)为均值为零、方差为一的多元高斯分布。有关更多详细信息，请参阅Larsen et al. [2016]。</p></div><p>Training 3D-VAE-GAN requires both 2D images and their corresponding 3D models. We render 3D shapes in front of background images (16,913 indoor images from the SUN database [Xiao et al., 2010]) in 72 views (from 24 angles and 3 elevations). We set \({\alpha }_{1} = 5,{\alpha }_{2} = {10}^{-4}\) ,and use a similar training strategy as in Section 3.1. See our supplementary material for more details.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>训练3D-VAE-GAN需要2D图像及其对应的3D模型。我们在背景图像（来自SUN数据库的16,913张室内图像[Xiao et al., 2010]）前渲染3D形状，共72个视角（来自24个角度和3个高度）。我们设置\({\alpha }_{1} = 5,{\alpha }_{2} = {10}^{-4}\)，并使用与3.1节中相似的训练策略。有关更多详细信息，请参见我们的补充材料。</p></div><h2>4 Evaluation</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>4 评估</h2></div><p>In this section, we evaluate our framework from various aspects. We first show qualitative results of generated \(3\mathrm{D}\) objects. We then evaluate the unsupervisedly learned representation from the discriminator by using them as features for 3D object classification. We show both qualitative and quantitative results on the popular benchmark ModelNet [Wu et al., 2015]. Further, we evaluate our 3D-VAE-GAN on 3D object reconstruction from a single image, and show both qualitative and quantitative results on the IKEA dataset [Lim et al., 2013].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本节中，我们从各个方面评估我们的框架。我们首先展示生成的\(3\mathrm{D}\)对象的定性结果。然后，我们通过将无监督学习的判别器表示作为3D对象分类的特征进行评估。我们在流行的基准ModelNet [Wu et al., 2015]上展示定性和定量结果。此外，我们在单幅图像的3D对象重建上评估我们的3D-VAE-GAN，并在IKEA数据集 [Lim et al., 2013]上展示定性和定量结果。</p></div><h3>4.1 3D Object Generation</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.1 3D对象生成</h3></div><p>Figure 2 shows 3D objects generated by our 3D-GAN. For this experiment, we train one 3D-GAN for each object category. For generation, we sample 200-dimensional vectors following an i.i.d. uniform distribution over \(\left\lbrack  {0,1}\right\rbrack\) ,and render the largest connected component of each generated object. We compare 3D-GAN with Wu et al. [2015], the state-of-the-art in 3D object synthesis from a probabilistic space, and with a volumetric autoencoder, whose variants have been employed by multiple recent methods [Girdhar et al., 2016, Sharma et al., 2016]. Because an autoencoder does not restrict the distribution of its latent representation,we compute the empirical distribution \({p}_{0}\left( z\right)\) of the latent vector \(z\) of all training examples,fit a Gaussian distribution \({g}_{0}\) to \({p}_{0}\) ,and sample from \({g}_{0}\) . Our algorithm produces 3D objects with much higher quality and more fine-grained details.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2展示了我们3D-GAN生成的3D对象。对于这个实验，我们为每个对象类别训练一个3D-GAN。在生成时，我们从\(\left\lbrack  {0,1}\right\rbrack\)中按照独立同分布的均匀分布抽样200维向量，并渲染每个生成对象的最大连通组件。我们将3D-GAN与Wu等人 [2015]的最新技术进行比较，该技术是从概率空间合成3D对象的最先进方法，并与体积自编码器进行比较，其变体已被多个最近的方法采用 [Girdhar et al., 2016, Sharma et al., 2016]。由于自编码器不限制其潜在表示的分布，我们计算所有训练示例的潜在向量\(z\)的经验分布\({p}_{0}\left( z\right)\)，拟合高斯分布\({g}_{0}\)到\({p}_{0}\)，并从\({g}_{0}\)中抽样。我们的算法生成的3D对象具有更高的质量和更细致的细节。</p></div><p>Compared with previous works, our 3D-GAN can synthesize high-resolution 3D objects with detailed geometries. Figure 3 shows both high-res voxels and down-sampled low-res voxels for comparison Note that it is relatively easy to synthesize a low-res object, but is much harder to obtain a high-res one due to the rapid growth of 3D space. However, object details are only revealed in high resolution.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>与之前的工作相比，我们的3D-GAN能够合成具有详细几何形状的高分辨率3D对象。图3展示了高分辨率体素和下采样的低分辨率体素以供比较。请注意，合成低分辨率对象相对容易，但由于3D空间的快速增长，获得高分辨率对象要困难得多。然而，只有在高分辨率下，物体细节才会显现。</p></div><p>A natural concern to our generative model is whether it is simply memorizing objects from training data. To demonstrate that the network can generalize beyond the training set, we compare synthesized objects with their nearest neighbor in the training set. Since the retrieval objects based on \({\ell }^{2}\) distance in the voxel space are visually very different from the queries, we use the output of the last convolutional</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们生成模型的一个自然担忧是它是否仅仅是在记忆训练数据中的对象。为了证明网络可以超越训练集进行泛化，我们将合成的对象与训练集中最近的邻居进行比较。由于基于体素空间中的\({\ell }^{2}\)距离检索的对象在视觉上与查询非常不同，我们使用判别器中最后一层卷积的输出</p></div><!-- Media --><!-- figureText: Our results \( \left( {{64} \times  {64} \times  {64}}\right) \) NN Objects generated by Wu et al. [2015] (30 × 30 × 30) Car Sofa Gun Chair Car Sofa Table Table Objects generated by a volumetric autoencoder \( \left( {{64} \times  {64} \times  {64}}\right) \) Chair Table --><img src="https://cdn.noedgeai.com/bo_d164kjn7aajc7388jsmg_4.jpg?x=305&#x26;y=189&#x26;w=1187&#x26;h=948&#x26;r=0"><p>Figure 2: Objects generated by 3D-GAN from vectors, without a reference image/object. We show, for the last two objects in each row, the nearest neighbor retrieved from the training set. We see that the generated objects are similar, but not identical, to examples in the training set. For comparison, we show objects generated by the previous state-of-the-art [Wu et al., 2015] (results supplied by the authors). We also show objects generated by autoencoders trained on a single object category, with latent vectors sampled from empirical distribution. See text for details.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2：由3D-GAN从向量生成的对象，没有参考图像/对象。我们展示了每行最后两个对象从训练集中检索到的最近邻。我们看到生成的对象与训练集中的示例相似，但并不完全相同。为了比较，我们展示了由之前的最新技术 [Wu et al., 2015]生成的对象（结果由作者提供）。我们还展示了在单个对象类别上训练的自编码器生成的对象，潜在向量从经验分布中抽样。有关详细信息，请参见文本。</p></div><!-- figureText: High-res Low-res High-res Low-res High-res Low-res High-res Low-res --><img src="https://cdn.noedgeai.com/bo_d164kjn7aajc7388jsmg_4.jpg?x=327&#x26;y=1343&#x26;w=1150&#x26;h=163&#x26;r=0"><p>Figure 3: We present each object at high resolution \(\left( {{64} \times  {64} \times  {64}}\right)\) on the left and at low resolution (down-sampled to \({16} \times  {16} \times  {16}\) ) on the right. While humans can perceive object structure at a relatively low resolution, fine details and variations only appear in high-res objects.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3：我们以高分辨率\(\left( {{64} \times  {64} \times  {64}}\right)\)展示每个对象在左侧，以低分辨率（下采样到\({16} \times  {16} \times  {16}\)）展示在右侧。虽然人类可以在相对低的分辨率下感知对象结构，但细节和变化仅在高分辨率对象中出现。</p></div><p>layer in our discriminator (with a \(2\mathrm{x}\) pooling) as features for retrieval instead. Figure 2 shows that generated objects are similar, but not identical, to the nearest examples in the training set.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在我们的判别器中（使用\(2\mathrm{x}\)池化）作为检索特征的层。图2显示生成的对象与训练集中最近的示例相似，但并不完全相同。</p></div><!-- Media --><h3>4.2 3D Object Classification</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.2 3D对象分类</h3></div><p>We then evaluate the representations learned by our discriminator. A typical way of evaluating representations learned without supervision is to use them as features for classification. To obtain features for an input 3D object, we concatenate the responses of the second, third, and fourth convolution layers in the discriminator,and apply max pooling of kernel sizes \(\{ 8,4,2\}\) ,respectively. We use a linear SVM for classification.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>然后我们评估判别器学习到的表示。无监督学习的表示评估的典型方法是将其作为分类的特征。为了获得输入3D对象的特征，我们将判别器中第二、第三和第四卷积层的响应进行连接，并分别应用核大小为\(\{ 8,4,2\}\)的最大池化。我们使用线性支持向量机进行分类。</p></div><p>Data We train a single 3D-GAN on the seven major object categories (chairs, sofas, tables, boats, airplanes, rifles, and cars) of ShapeNet [Chang et al., 2015]. We use ModelNet [Wu et al., 2015] for testing, following Sharma et al. [2016], Maturana and Scherer [2015], Qi et al. [2016].* Specifically, we evaluate our model on both ModelNet10 and ModelNet40, two subsets of ModelNet that are often</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们在ShapeNet（[Chang et al., 2015]）的七个主要物体类别（椅子、沙发、桌子、船、飞机、步枪和汽车）上训练了一个单一的3D-GAN。我们使用ModelNet（[Wu et al., 2015]）进行测试，遵循Sharma等人（[2016]）、Maturana和Scherer（[2015]）、Qi等人（[2016]）。*具体而言，我们在ModelNet的两个子集ModelNet10和ModelNet40上评估我们的模型，这两个子集通常</p></div><hr>
<!-- Footnote --><p>*For ModelNet, there are two train/test splits typically used. Qi et al. [2016], Shi et al. [2015], Maturana and Scherer [2015] used the train/test split included in the dataset, which we also follow; Wu et al. [2015], Su</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>*对于ModelNet，通常使用两个训练/测试划分。Qi等人（[2016]）、Shi等人（[2015]）、Maturana和Scherer（[2015]）使用了数据集中包含的训练/测试划分，我们也遵循这一点；Wu等人（[2015]）、Su</p></div><!-- Footnote -->
<hr><!-- Media --><table><tbody><tr><td rowspan="2">Supervision</td><td rowspan="2">Pretraining</td><td rowspan="2">\( \mathbf{{Method}} \)</td><td colspan="2">Classification (Accuracy)</td></tr><tr><td>ModelNet40</td><td>ModelNet10</td></tr><tr><td rowspan="6">Category labels</td><td rowspan="2">ImageNet</td><td>MVCNN [Su et al., 2015a]</td><td>90.1%</td><td>-</td></tr><tr><td>MVCNN-MultiRes [Qi et al., 2016]</td><td>91.4%</td><td>-</td></tr><tr><td rowspan="4">None</td><td>3D ShapeNets [Wu et al., 2015]</td><td>77.3%</td><td>83.5%</td></tr><tr><td>DeepPano [Shi et al., 2015]</td><td>77.6%</td><td>85.5%</td></tr><tr><td>VoxNet [Maturana and Scherer, 2015]</td><td>83.0%</td><td>92.0%</td></tr><tr><td>ORION [Sedaghat et al., 2016]</td><td>-</td><td>93.8%</td></tr><tr><td rowspan="5">Unsupervised</td><td rowspan="5">-</td><td>SPH [Kazhdan et al., 2003]</td><td>68.2%</td><td>79.8%</td></tr><tr><td>LFD [Chen et al., 2003]</td><td>75.5%</td><td>79.9%</td></tr><tr><td>T-L Network [Girdhar et al., 2016]</td><td>74.4%</td><td>-</td></tr><tr><td>VConv-DAE [Sharma et al., 2016]</td><td>75.5%</td><td>80.5%</td></tr><tr><td>3D-GAN (ours)</td><td>\( \mathbf{{83.3}}\% \)</td><td>91.0%</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">监督</td><td rowspan="2">预训练</td><td rowspan="2">\( \mathbf{{Method}} \)</td><td colspan="2">分类（准确率）</td></tr><tr><td>ModelNet40</td><td>ModelNet10</td></tr><tr><td rowspan="6">类别标签</td><td rowspan="2">ImageNet</td><td>MVCNN [Su et al., 2015a]</td><td>90.1%</td><td>-</td></tr><tr><td>MVCNN-MultiRes [Qi et al., 2016]</td><td>91.4%</td><td>-</td></tr><tr><td rowspan="4">无</td><td>3D ShapeNets [Wu et al., 2015]</td><td>77.3%</td><td>83.5%</td></tr><tr><td>DeepPano [Shi et al., 2015]</td><td>77.6%</td><td>85.5%</td></tr><tr><td>VoxNet [Maturana and Scherer, 2015]</td><td>83.0%</td><td>92.0%</td></tr><tr><td>ORION [Sedaghat et al., 2016]</td><td>-</td><td>93.8%</td></tr><tr><td rowspan="5">无监督</td><td rowspan="5">-</td><td>SPH [Kazhdan et al., 2003]</td><td>68.2%</td><td>79.8%</td></tr><tr><td>LFD [Chen et al., 2003]</td><td>75.5%</td><td>79.9%</td></tr><tr><td>T-L 网络 [Girdhar et al., 2016]</td><td>74.4%</td><td>-</td></tr><tr><td>VConv-DAE [Sharma et al., 2016]</td><td>75.5%</td><td>80.5%</td></tr><tr><td>3D-GAN（我们的）</td><td>\( \mathbf{{83.3}}\% \)</td><td>91.0%</td></tr></tbody></table></div><p>Table 1: Classification results on the ModelNet dataset. Our 3D-GAN outperforms other unsupervised learning methods by a large margin, and is comparable to some recent supervised learning frameworks.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表1：在ModelNet数据集上的分类结果。我们的3D-GAN在无监督学习方法中表现优异，远超其他方法，并且与一些最近的监督学习框架相当。</p></div><!-- figureText: 85 3D-GAN VoxNet full #objects per class in training Accuracy (%) 80 75 70 40 --><img src="https://cdn.noedgeai.com/bo_d164kjn7aajc7388jsmg_5.jpg?x=313&#x26;y=759&#x26;w=375&#x26;h=315&#x26;r=0"><img src="https://cdn.noedgeai.com/bo_d164kjn7aajc7388jsmg_5.jpg?x=707&#x26;y=762&#x26;w=765&#x26;h=87&#x26;r=0"><p>Figure 5: The effects of individual dimensions of the object vector</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图5：对象向量各个维度的影响</p></div><img src="https://cdn.noedgeai.com/bo_d164kjn7aajc7388jsmg_5.jpg?x=706&#x26;y=922&#x26;w=765&#x26;h=177&#x26;r=0"><p>Figure 4: ModelNet40 classification with limited training data Figure 6: Intra/inter-class interpolation between object vectors</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图4：在有限训练数据下的ModelNet40分类 图6：对象向量之间的类内/类间插值</p></div><!-- Media --><p>used as benchmarks for 3D object classification. Note that the training and test categories are not identical, which also shows the out-of-category generalization power of our 3D-GAN.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>作为3D对象分类的基准。请注意，训练和测试类别并不相同，这也显示了我们3D-GAN的超类泛化能力。</p></div><p>Results We compare with the state-of-the-art methods [Wu et al., 2015, Girdhar et al., 2016, Sharma et al., 2016, Sedaghat et al., 2016] and show per-class accuracy in Table 1. Our representation outperforms other features learned without supervision by a large margin (83.3% vs. 75.5% on ModelNet40, and 91.0% vs 80.5% on ModelNet10) [Girdhar et al., 2016, Sharma et al., 2016]. Further, our classification accuracy is also higher than some recent supervised methods [Shi et al., 2015], and is close to the state-of-the-art voxel-based supervised learning approaches [Maturana and Scherer, 2015, Sedaghat et al., 2016]. Multi-view CNNs [Su et al., 2015a, Qi et al., 2016] outperform us, though their methods are designed for classification, and require rendered multi-view images and an ImageNet-pretrained model.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>结果 我们与最先进的方法进行比较 [Wu et al., 2015, Girdhar et al., 2016, Sharma et al., 2016, Sedaghat et al., 2016]，并在表1中展示每类的准确率。我们的表示在无监督学习中远超其他特征（ModelNet40上为83.3%对75.5%，ModelNet10上为91.0%对80.5%）[Girdhar et al., 2016, Sharma et al., 2016]。此外，我们的分类准确率也高于一些最近的监督方法 [Shi et al., 2015]，并接近最先进的基于体素的监督学习方法 [Maturana and Scherer, 2015, Sedaghat et al., 2016]。多视角CNN [Su et al., 2015a, Qi et al., 2016] 超过了我们的表现，尽管他们的方法是为分类设计的，并且需要渲染的多视角图像和一个在ImageNet上预训练的模型。</p></div><p>3D-GAN also works well with limited training data. As shown in Figure 4, with roughly 25 training samples per class, 3D-GAN achieves comparable performance on ModelNet40 with other unsupervised learning methods trained with at least 80 samples per class.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D-GAN在有限训练数据下也表现良好。如图4所示，使用每类大约25个训练样本，3D-GAN在ModelNet40上的表现与其他至少使用80个样本的无监督学习方法相当。</p></div><h3>4.3 Single Image 3D Reconstruction</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.3 单图像3D重建</h3></div><p>As an application, our show that the 3D-VAE-GAN can perform well on single image 3D reconstruction. Following previous work [Girdhar et al., 2016], we test it on the IKEA dataset [Lim et al., 2013], and show both qualitative and quantitative results.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>作为一个应用，我们展示了3D-VAE-GAN在单图像3D重建上的良好表现。根据之前的工作 [Girdhar et al., 2016]，我们在IKEA数据集 [Lim et al., 2013] 上进行了测试，并展示了定性和定量结果。</p></div><p>Data The IKEA dataset consists of images with IKEA objects. We crop the images so that the objects are centered in the images. Our test set consists of 1,039 objects cropped from 759 images (supplied by the author). The IKEA dataset is challenging because all images are captured in the wild, often with heavy occlusions. We test on all six categories of objects: bed, bookcase, chair, desk, sofa, and table.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>数据 IKEA数据集包含了带有IKEA物体的图像。我们裁剪图像，使物体位于图像中心。我们的测试集由从759张图像中裁剪的1,039个物体组成（由作者提供）。IKEA数据集具有挑战性，因为所有图像都是在自然环境中拍摄的，通常存在严重遮挡。我们测试了六类物体：床、书柜、椅子、桌子、沙发和桌子。</p></div><p>Results We show our results in Figure 7 and Table 2, with performance of a single 3D-VAE-GAN jointly trained on all six categories, as well as the results of six 3D-VAE-GANs separately trained on</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>结果 我们在图7和表2中展示了我们的结果，展示了一个在所有六类上联合训练的单个3D-VAE-GAN的性能，以及六个分别训练的3D-VAE-GAN的结果</p></div><hr>
<!-- Footnote --><p>et al. [2015a], Sharma et al. [2016] used 80 training points and 20 test points in each category for experiments, possibly with viewpoint augmentation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>等 [2015a]，Sharma等 [2016] 在每个类别中使用了80个训练点和20个测试点进行实验，可能进行了视角增强。</p></div><!-- Footnote -->
<hr><!-- Media --><table><tbody><tr><td>Method</td><td>Bed</td><td>Bookcase</td><td>Chair</td><td>Desk</td><td>Sofa</td><td>Table</td><td>Mean</td></tr><tr><td>AlexNet-fc8 [Girdhar et al., 2016]</td><td>29.5</td><td>17.3</td><td>20.4</td><td>19.7</td><td>38.8</td><td>16.0</td><td>23.6</td></tr><tr><td>AlexNet-conv4 [Girdhar et al., 2016]</td><td>38.2</td><td>26.6</td><td>31.4</td><td>26.6</td><td>69.3</td><td>19.1</td><td>35.2</td></tr><tr><td>T-L Network [Girdhar et al., 2016]</td><td>56.3</td><td>30.2</td><td>32.9</td><td>25.8</td><td>71.7</td><td>23.3</td><td>40.0</td></tr><tr><td>3D-VAE-GAN (jointly trained)</td><td>49.1</td><td>31.9</td><td>42.6</td><td>34.8</td><td>79.8</td><td>33.1</td><td>45.2</td></tr><tr><td>3D-VAE-GAN (separately trained)</td><td>63.2</td><td>46.3</td><td>47.2</td><td>40.7</td><td>78.8</td><td>42.3</td><td>53.1</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>方法</td><td>床</td><td>书柜</td><td>椅子</td><td>桌子</td><td>沙发</td><td>桌子</td><td>平均</td></tr><tr><td>AlexNet-fc8 [Girdhar et al., 2016]</td><td>29.5</td><td>17.3</td><td>20.4</td><td>19.7</td><td>38.8</td><td>16.0</td><td>23.6</td></tr><tr><td>AlexNet-conv4 [Girdhar et al., 2016]</td><td>38.2</td><td>26.6</td><td>31.4</td><td>26.6</td><td>69.3</td><td>19.1</td><td>35.2</td></tr><tr><td>T-L 网络 [Girdhar et al., 2016]</td><td>56.3</td><td>30.2</td><td>32.9</td><td>25.8</td><td>71.7</td><td>23.3</td><td>40.0</td></tr><tr><td>3D-VAE-GAN（联合训练）</td><td>49.1</td><td>31.9</td><td>42.6</td><td>34.8</td><td>79.8</td><td>33.1</td><td>45.2</td></tr><tr><td>3D-VAE-GAN（单独训练）</td><td>63.2</td><td>46.3</td><td>47.2</td><td>40.7</td><td>78.8</td><td>42.3</td><td>53.1</td></tr></tbody></table></div><p>Table 2: Average precision for voxel prediction on the IKEA dataset. \({}^{ \dagger  }\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表2：IKEA数据集上体素预测的平均精度。\({}^{ \dagger  }\)</p></div><img src="https://cdn.noedgeai.com/bo_d164kjn7aajc7388jsmg_6.jpg?x=347&#x26;y=491&#x26;w=1132&#x26;h=291&#x26;r=0"><p>Figure 7: Qualitative results of single image 3D reconstruction on the IKEA dataset</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图7：IKEA数据集上单幅图像3D重建的定性结果</p></div><p>each class. Following Girdhar et al. [2016],we evaluate results at resolution \({20} \times  {20} \times  {20}\) ,use the average precision as our evaluation metric, and attempt to align each prediction with the ground-truth over permutations, flips, and translational alignments (up to 10%), as IKEA ground truth objects are not in a canonical viewpoint. In all categories, our model consistently outperforms previous state-of-the-art in voxel-level prediction and other baseline methods. \({}^{ \dagger  }\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>每个类别。根据Girdhar等人[2016]的研究，我们在分辨率\({20} \times  {20} \times  {20}\)下评估结果，使用平均精度作为评估指标，并尝试在排列、翻转和位移对齐（最多10%）的情况下将每个预测与真实值对齐，因为IKEA的真实物体并不在标准视角下。在所有类别中，我们的模型在体素级预测和其他基线方法上始终优于之前的最先进技术。\({}^{ \dagger  }\)</p></div><!-- Media --><h2>5 Analyzing Learned Representations</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>5 分析学习到的表示</h2></div><p>In this section, we look deep into the representations learned by both the generator and the discriminator of 3D-GAN. We start with the 200-dimensional object vector, from which the generator produces various objects. We then visualize neurons in the discriminator, and demonstrate that these units capture informative semantic knowledge of the objects, which justifies its good performance on object classification presented in Section 4.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本节中，我们深入研究3D-GAN的生成器和判别器学习到的表示。我们从200维的物体向量开始，生成器从中生成各种物体。然后，我们可视化判别器中的神经元，并展示这些单元捕捉到物体的有意义的语义知识，这证明了其在第4节中展示的物体分类中的良好表现。</p></div><h3>5.1 The Generative Representation</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.1 生成表示</h3></div><p>We explore three methods for understanding the latent space of vectors for object generation. We first visualize what an individual dimension of the vector represents; we then explore the possibility of interpolating between two object vectors and observe how the generated objects change; last, we present how we can apply shape arithmetic in the latent space.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们探索三种方法来理解用于物体生成的向量潜在空间。我们首先可视化向量的每个维度所代表的内容；然后我们探索在两个物体向量之间插值的可能性，并观察生成的物体如何变化；最后，我们展示如何在潜在空间中应用形状算术。</p></div><p>Visualizing the object vector To visualize the semantic meaning of each dimension, we gradually increase its value,and observe how it affects the generated 3D object. In Figure 5, each column corresponds to one dimension of the object vector, where the red region marks the voxels affected by changing values of that dimension. We observe that some dimensions in the object vector carries semantic knowledge of the object, e.g., the thickness or width of surfaces.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>可视化物体向量 为了可视化每个维度的语义含义，我们逐渐增加其值，并观察它如何影响生成的3D物体。在图5中，每一列对应物体向量的一个维度，红色区域标记了受该维度值变化影响的体素。我们观察到物体向量中的某些维度携带物体的语义知识，例如表面的厚度或宽度。</p></div><p>Interpolation We show results of interpolating between two object vectors in Figure 6. Earlier works demonstrated interpolation between two 2D images of the same category [Dosovitskiy et al., 2015, Radford et al., 2016]. Here we show interpolations both within and across object categories. We observe that for both cases walking over the latent space gives smooth transitions between objects.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>插值 我们在图6中展示了在两个物体向量之间插值的结果。早期的工作展示了同一类别的两个2D图像之间的插值[Dosovitskiy等，2015；Radford等，2016]。在这里，我们展示了在物体类别内和跨类别的插值。我们观察到，在这两种情况下，穿越潜在空间可以实现物体之间的平滑过渡。</p></div><p>Arithmetic Another way of exploring the learned representations is to show arithmetic in the latent space. Previously, Dosovitskiy et al. [2015], Radford et al. [2016] presented that their generative nets are able to encode semantic knowledge of chair or face images in its latent space; Girdhar et al. [2016] also showed that the learned representation for 3D objects behave similarly. We show our shape arithmetic in Figure 8. Different from Girdhar et al. [2016], all of our objects are randomly sampled, requiring no existing 3D CAD models as input.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>算术 探索学习到的表示的另一种方法是展示潜在空间中的算术。之前，Dosovitskiy等人[2015]、Radford等人[2016]展示了他们的生成网络能够在其潜在空间中编码椅子或面部图像的语义知识；Girdhar等人[2016]也展示了3D物体的学习表示表现类似。我们在图8中展示了我们的形状算术。与Girdhar等人[2016]不同，我们的所有物体都是随机采样的，不需要现有的3D CAD模型作为输入。</p></div><h3>5.2 The Discriminative Representation</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.2 判别表示</h3></div><p>We now visualize the neurons in the discriminator. Specifically, we would like to show what input objects, and which part of them produce the highest intensity values for each neuron. To do that,</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们现在可视化判别器中的神经元。具体来说，我们想展示哪些输入物体及其哪些部分为每个神经元产生最高的强度值。为此，</p></div><hr>
<!-- Footnote --><p>\({}^{ \dagger  }\) For methods from Girdhar et al. [2016],the mean values in the last column are higher than the originals in their paper, because we compute per-class accuracy instead of per-instance accuracy.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{ \dagger  }\) 对于Girdhar等人[2016]的方法，最后一列的平均值高于他们论文中的原始值，因为我们计算的是每类的准确率，而不是每个实例的准确率。</p></div><!-- Footnote -->
<hr><!-- Media --><img src="https://cdn.noedgeai.com/bo_d164kjn7aajc7388jsmg_7.jpg?x=343&#x26;y=220&#x26;w=1117&#x26;h=203&#x26;r=0"><p>Figure 8: Shape arithmetic for chairs and tables. The left images show the obtained "arm" vector can be added to other chairs, and the right ones show the "layer" vector can be added to other tables.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图8：椅子和桌子的形状算术。左侧图像显示获得的“手臂”向量可以添加到其他椅子上，右侧图像显示“层”向量可以添加到其他桌子上。</p></div><img src="https://cdn.noedgeai.com/bo_d164kjn7aajc7388jsmg_7.jpg?x=344&#x26;y=508&#x26;w=1119&#x26;h=225&#x26;r=0"><p>Figure 9: Objects and parts that activate specific neurons in the discriminator. For each neuron, we show five objects that activate it most strongly, with colors representing gradients of activations with respect to input voxels.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图9：激活判别器中特定神经元的物体和部分。对于每个神经元，我们展示五个最强烈激活它的物体，颜色表示相对于输入体素的激活梯度。</p></div><!-- Media --><p>for each neuron in the second to last convolutional layer of the discriminator, we iterate through all training objects and exhibit the ones activating the unit most strongly. We further use guided back-propagation [Springenberg et al., 2015] to visualize the parts that produce the activation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>对于判别器倒数第二个卷积层中的每个神经元，我们遍历所有训练物体，并展示激活该单元最强的物体。我们进一步使用引导反向传播[Springenberg等，2015]可视化产生激活的部分。</p></div><p>Figure 9 shows the results. There are two main observations: first, for a single neuron, the objects producing strongest activations have very similar shapes, showing the neuron is selective in terms of the overall object shape; second, the parts that activate the neuron, shown in red, are consistent across these objects, indicating the neuron is also learning semantic knowledge about object parts.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图9显示了结果。主要有两个观察：首先，对于单个神经元，产生最强激活的物体形状非常相似，表明该神经元在整体物体形状方面具有选择性；其次，激活该神经元的部分（以红色显示）在这些物体中是一致的，表明该神经元也在学习关于物体部分的语义知识。</p></div><h2>6 Conclusion</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>6 结论</h2></div><p>In this paper, we proposed 3D-GAN for 3D object generation, as well as 3D-VAE-GAN for learning an image to 3D model mapping. We demonstrated that our models are able to generate novel objects and to reconstruct 3D objects from images. We showed that the discriminator in GAN, learned without supervision, can be used as an informative feature representation for 3D objects, achieving impressive performance on shape classification. We also explored the latent space of object vectors, and presented results on object interpolation, shape arithmetic, and neuron visualization.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本文中，我们提出了用于3D物体生成的3D-GAN，以及用于学习图像到3D模型映射的3D-VAE-GAN。我们展示了我们的模型能够生成新颖的物体并从图像中重建3D物体。我们表明，GAN中的判别器在无监督学习下，可以作为3D物体的信息特征表示，在形状分类上取得了令人印象深刻的表现。我们还探索了物体向量的潜在空间，并展示了物体插值、形状算术和神经元可视化的结果。</p></div><p>Acknowledgement This work is supported by NSF grants #1212849 and #1447476, ONR MURI N00014-16-1-2007, the Center for Brain, Minds and Machines (NSF STC award CCF-1231216), Toyota Research Institute, Adobe, Shell, IARPA MICrONS, and a hardware donation from Nvidia. References</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>致谢 本工作得到了NSF资助#1212849和#1447476，ONR MURI N00014-16-1-2007，脑、心智与机器中心（NSF STC奖CCF-1231216），丰田研究院，Adobe，壳牌，IARPA MICrONS，以及Nvidia的硬件捐赠的支持。参考文献</p></div><p>Aayush Bansal, Bryan Russell, and Abhinav Gupta. Marr revisited: 2d-3d alignment via surface normal prediction. In \({CVPR},{2016.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Aayush Bansal, Bryan Russell, 和 Abhinav Gupta. Marr重访：通过表面法线预测进行2D-3D对齐。 在 \({CVPR},{2016.2}\)</p></div><p>Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In SIGGRAPH, 1999. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Volker Blanz 和 Thomas Vetter. 用于合成3D人脸的可变形模型。 在SIGGRAPH, 1999. 2</p></div><p>Wayne E Carlson. An algorithm and data structure for 3d object synthesis using surface patch intersections. In SIGGRAPH, 1982. 1, 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Wayne E Carlson. 一种用于3D物体合成的算法和数据结构，利用表面补丁交集。 在SIGGRAPH, 1982. 1, 2</p></div><p>Angel X Chang, Thomas Funkhouser, Leonidas Guibas, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 1, 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Angel X Chang, Thomas Funkhouser, Leonidas Guibas等. Shapenet：一个信息丰富的3D模型库。 arXiv预印本arXiv:1512.03012, 2015. 1, 5</p></div><p>Siddhartha Chaudhuri, Evangelos Kalogerakis, Leonidas Guibas, and Vladlen Koltun. Probabilistic reasoning for assembly-based 3d modeling. ACM TOG, 30(4):35, 2011. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Siddhartha Chaudhuri, Evangelos Kalogerakis, Leonidas Guibas, 和 Vladlen Koltun. 基于组装的3D建模的概率推理。 ACM TOG, 30(4):35, 2011. 2</p></div><p>Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming Ouhyoung. On visual similarity based 3d model retrieval. CGF, 2003. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, 和 Ming Ouhyoung. 基于视觉相似性的3D模型检索。 CGF, 2003. 6</p></div><p>Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In \({ECCV},{2016.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, 和 Silvio Savarese. 3D-R2N2：单视图和多视图3D物体重建的统一方法。 在 \({ECCV},{2016.2}\)</p></div><p>Emily L Denton, Soumith Chintala, and Rob Fergus. Deep generative image models using a laplacian pyramid of adversarial networks. In NIPS, 2015. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Emily L Denton, Soumith Chintala, 和 Rob Fergus. 使用对抗网络的拉普拉斯金字塔的深度生成图像模型。 在NIPS, 2015. 2</p></div><p>Alexey Dosovitskiy, Jost Tobias Springenberg, and Thomas Brox. Learning to generate chairs with convolutional neural networks. In \({CVPR},{2015}.7\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Alexey Dosovitskiy, Jost Tobias Springenberg, 和 Thomas Brox. 学习使用卷积神经网络生成椅子。 在 \({CVPR},{2015}.7\)</p></div><p>Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Abhinav Gupta. Learning a predictable and generative vector representation for objects. In \({ECCV},{2016.1},2,4,6,7\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Rohit Girdhar, David F Fouhey, Mikel Rodriguez, 和 Abhinav Gupta. 学习可预测和生成的物体向量表示。 在 \({ECCV},{2016.1},2,4,6,7\)</p></div><p>Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. 2, 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, 和 Yoshua Bengio. 生成对抗网络。 在NIPS, 2014. 2, 3</p></div><p>Haibin Huang, Evangelos Kalogerakis, and Benjamin Marlin. Analysis and synthesis of 3d shape families via deep-learned generative models of surfaces. \({CGF},{34}\left( 5\right)  : {25} - {38},{2015.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Haibin Huang, Evangelos Kalogerakis, 和 Benjamin Marlin. 通过深度学习生成的表面模型分析和合成3D形状家族。 \({CGF},{34}\left( 5\right)  : {25} - {38},{2015.2}\)</p></div><p>Daniel Jiwoong Im, Chris Dongjoo Kim, Hui Jiang, and Roland Memisevic. Generating images with recurrent adversarial networks. arXiv preprint arXiv:1602.05110, 2016. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>丹尼尔·基沃恩·林，克里斯·东久·金，惠·姜，和罗兰·梅米塞维奇。使用递归对抗网络生成图像。arXiv预印本arXiv:1602.05110，2016。2</p></div><p>Evangelos Kalogerakis, Siddhartha Chaudhuri, Daphne Koller, and Vladlen Koltun. A probabilistic model for component-based shape synthesis. ACM TOG, 31(4):55, 2012. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>埃万杰洛斯·卡洛格拉基斯，西达尔塔·乔杜里，达芙妮·科勒，和弗拉德伦·科尔图。基于组件的形状合成的概率模型。ACM TOG，31(4):55，2012。2</p></div><p>Abhishek Kar, Shubham Tulsiani, Joao Carreira, and Jitendra Malik. Category-specific object reconstruction from a single image. In \({CVPR},{2015.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>阿比谢克·卡尔，舒巴姆·图尔萨尼，若昂·卡雷拉，和吉滕德拉·马利克。基于单幅图像的类别特定物体重建。在\({CVPR},{2015.2}\)</p></div><p>Michael Kazhdan, Thomas Funkhouser, and Szymon Rusinkiewicz. Rotation invariant spherical harmonic representation of \(3\mathrm{\;d}\) shape descriptors. In \({SGP},{2003.6}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>迈克尔·卡兹达，托马斯·芬克豪瑟，和西蒙·鲁辛基维奇。旋转不变的球面谐波表示\(3\mathrm{\;d}\)形状描述符。在\({SGP},{2003.6}\)</p></div><p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>迪德里克·P·金马和吉米·巴。Adam：一种随机优化方法。在ICLR，2015。3</p></div><p>Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 2, 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>迪德里克·P·金马和马克斯·韦林。自编码变分贝叶斯。在ICLR，2014。2，3</p></div><p>Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. In \({ICML},{2016.2},4\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>安德斯·博森·林博·拉尔森，索伦·卡埃·桑德比，和奥勒·温特。使用学习的相似性度量进行超越像素的自编码。在\({ICML},{2016.2},4\)</p></div><p>Chuan Li and Michael Wand. Precomputed real-time texture synthesis with markovian generative adversarial networks. arXiv preprint arXiv:1604.04382, 2016. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>李川和迈克尔·旺德。使用马尔可夫生成对抗网络的预计算实时纹理合成。arXiv预印本arXiv:1604.04382，2016。2</p></div><p>Yangyan Li, Hao Su, Charles Ruizhongtai Qi, Noa Fish, Daniel Cohen-Or, and Leonidas J Guibas. Joint embeddings of shapes and images via cnn image purification. ACM TOG, 34(6):234, 2015. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>杨艳·李，郝苏，查尔斯·瑞宗泰·齐，诺亚·菲什，丹尼尔·科恩-奥尔，和利奥尼达斯·J·吉巴斯。通过CNN图像净化的形状和图像的联合嵌入。ACM TOG，34(6):234，2015。2</p></div><p>Joseph J. Lim, Hamed Pirsiavash, and Antonio Torralba. Parsing ikea objects: Fine pose estimation. In ICCV, 2013.4,6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>约瑟夫·J·林，哈梅德·皮尔西亚瓦什，和安东尼奥·托拉尔巴。解析宜家物体：精细姿态估计。在ICCV，2013。4，6</p></div><p>Andrew L Maas,Awni Y Hannun,and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In ICML, 2013. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>安德鲁·L·马斯，阿瓦尼·Y·哈努恩，和安德鲁·Y·吴。整流非线性改善神经网络声学模型。在ICML，2013。3</p></div><p>Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In IROS, 2015. 2, 5, 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>丹尼尔·马图拉纳和塞巴斯蒂安·舍雷尔。Voxnet：用于实时物体识别的3D卷积神经网络。在IROS，2015。2，5，6</p></div><p>Charles R Qi, Hao Su, Matthias Niessner, Angela Dai, Mengyuan Yan, and Leonidas J Guibas. Volumetric and multi-view cnns for object classification on 3d data. In \({CVPR},{2016.1},2,5,6\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>查尔斯·R·齐，郝苏，马蒂亚斯·尼斯纳，安吉拉·戴，孟源·严，和利奥尼达斯·J·吉巴斯。用于3D数据物体分类的体积和多视角CNN。在\({CVPR},{2016.1},2,5,6\)</p></div><p>Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016. 2, 3, 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>亚历克·拉德福德，卢克·梅茨，和索米特·钦塔拉。使用深度卷积生成对抗网络的无监督表示学习。在ICLR，2016。2，3，7</p></div><p>Danilo Jimenez Rezende, SM Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, and Nicolas Heess. Unsupervised learning of \(3\mathrm{\;d}\) structure from images. In NIPS,2016. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>达尼洛·吉门尼斯·雷泽恩德，SM·埃斯拉米，沙基尔·穆罕默德，彼得·巴塔利亚，马克斯·贾德伯格，和尼古拉斯·赫斯。无监督学习\(3\mathrm{\;d}\)结构从图像中。在NIPS，2016。2</p></div><p>Nima Sedaghat, Mohammadreza Zolfaghari, and Thomas Brox. Orientation-boosted voxel nets for 3d object recognition. arXiv preprint arXiv:1604.03351, 2016. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>尼玛·塞达哈特，穆罕默德雷扎·佐尔法哈里，和托马斯·布罗克。用于3D物体识别的方向增强体素网络。arXiv预印本arXiv:1604.03351，2016。6</p></div><p>Abhishek Sharma, Oliver Grau, and Mario Fritz. Vconv-dae: Deep volumetric shape learning without object labels. arXiv preprint arXiv:1604.03755, 2016. 2, 4, 5, 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>阿比谢克·夏尔马、奥利弗·格劳和马里奥·弗里茨。Vconv-dae：无对象标签的深度体积形状学习。arXiv预印本arXiv:1604.03755，2016年。2, 4, 5, 6</p></div><p>Baoguang Shi, Song Bai, Zhichao Zhou, and Xiang Bai. Deeppano: Deep panoramic representation for 3-d shape recognition. IEEE SPL, 22(12):2339-2343, 2015. 2, 5, 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>包光石、宋白、周志超和向白。Deeppano：用于3D形状识别的深度全景表示。IEEE SPL, 22(12):2339-2343，2015年。2, 5, 6</p></div><p>Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. In ICLR Workshop, 2015. 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>约斯特·托比亚斯·斯普林根伯格、阿列克谢·多索维茨基、托马斯·布罗克斯和马丁·里德米勒。追求简单：全卷积网络。在ICLR研讨会，2015年。8</p></div><p>Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for \(3\mathrm{\;d}\) shape recognition. In \({ICCV},{2015}\mathrm{a}.1,2,5,6\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>尚苏、苏布兰苏·马吉、埃万杰洛斯·卡洛格拉基斯和埃里克·勒恩德-米勒。用于\(3\mathrm{\;d}\)形状识别的多视角卷积神经网络。在\({ICCV},{2015}\mathrm{a}.1,2,5,6\)</p></div><p>Hao Su, Charles R Qi, Yangyan Li, and Leonidas Guibas. Render for cnn: Viewpoint estimation in images using cnns trained with rendered \(3\mathrm{\;d}\) model views. In \({ICCV},{2015}\mathrm{\;b}\) . 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>郝苏、查尔斯·R·齐、杨艳·李和利奥尼达斯·吉巴斯。为CNN渲染：使用经过渲染的\(3\mathrm{\;d}\)模型视图在图像中进行视点估计。在\({ICCV},{2015}\mathrm{\;b}\)。2</p></div><p>Johan WH Tangelder and Remco C Veltkamp. A survey of content based 3d shape retrieval methods. Multimedia tools and applications, 39(3):441-471, 2008. 1, 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>约翰·WH·坦格尔德和雷姆科·C·费尔坎普。基于内容的3D形状检索方法综述。多媒体工具与应用，39(3):441-471，2008年。1, 2</p></div><p>Oliver Van Kaick, Hao Zhang, Ghassan Hamarneh, and Daniel Cohen-Or. A survey on shape correspondence. CGF, 2011. 1, 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>奥利弗·范·凯克、郝张、加桑·哈马尔内赫和丹尼尔·科恩-奥尔。形状对应的调查。CGF，2011年。1, 2</p></div><p>Xiaolong Wang and Abhinav Gupta. Generative image modeling using style and structure adversarial networks. In \({ECCV},{2016.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>肖龙·王和阿比纳夫·古普塔。使用风格和结构对抗网络的生成图像建模。在\({ECCV},{2016.2}\)</p></div><p>Jiajun Wu, Tianfan Xue, Joseph J Lim, Yuandong Tian, Joshua B Tenenbaum, Antonio Torralba, and William T Freeman. Single image 3d interpreter network. In \({ECCV},{2016.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>贾俊·吴、天凡·薛、约瑟夫·J·林、元东·田、约书亚·B·特嫩鲍姆、安东尼奥·托拉尔巴和威廉·T·弗里曼。单图像3D解释网络。在\({ECCV},{2016.2}\)</p></div><p>Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In CVPR, 2015. 1, 2, 4, 5, 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>志荣·吴、舒然·宋、阿迪提亚·科斯拉、费舍尔·余、凌光·张、肖欧·唐和简雄·肖。3D形状网：体积形状的深度表示。在CVPR，2015年。1, 2, 4, 5, 6</p></div><p>Yu Xiang, Wongun Choi, Yuanqing Lin, and Silvio Savarese. Data-driven 3d voxel patterns for object category recognition. In \({CVPR},{2015.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>余翔、黄军·崔、袁青·林和西尔维奥·萨瓦雷斯。基于数据的3D体素模式用于物体类别识别。在\({CVPR},{2015.2}\)</p></div><p>Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010. 4</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>简雄·肖、詹姆斯·海斯、克里斯塔·艾辛格、奥德·奥利瓦和安东尼奥·托拉尔巴。阳光数据库：从修道院到动物园的大规模场景识别。在CVPR，2010年。4</p></div><p>Tianfan Xue, Jianzhuang Liu, and Xiaoou Tang. Example-based 3d object reconstruction from line drawings. In CVPR, 2012. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>天凡·薛、简壮·刘和肖欧·唐。基于示例的3D物体重建从线条图。在CVPR，2012年。2</p></div><p>Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, and Honglak Lee. Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision. In NIPS, 2016. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>辛晨·严、季梅·杨、厄尔辛·尤梅尔、义杰·郭和洪乐·李。透视变换网络：学习单视图3D物体重建而无需3D监督。在NIPS，2016年。2</p></div><p>Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and Alexei A Efros. Generative visual manipulation on the natural image manifold. In \({ECCV},{2016.2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>俊彦·朱、菲利普·克雷亨布赫、伊莱·谢赫特曼和阿列克谢·A·埃夫罗斯。在自然图像流形上的生成视觉操控。在\({ECCV},{2016.2}\)</p></div>
      </body>
    </html>
  