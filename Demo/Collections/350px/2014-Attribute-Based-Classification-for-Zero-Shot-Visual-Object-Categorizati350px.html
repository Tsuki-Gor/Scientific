
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>2014-Attribute-Based Classification for Zero-Shot Visual Object Categorizati</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px 350px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      </head>
      <body>
        <div><div class="relative cursor-pointer md-tranlate-menu-layout"></div><div><div data-page="1" id="mark-7bb61fb4-3d53-465c-a881-f901e881216f" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate alive_postion_page_md" data-positiontag-0="0,0"><div style="height: auto;"><span style="display: inline;"><h1><div><div>Attribute-Based Classification for Zero-Shot Visual Object Categorization<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">基于属性的零样本视觉目标分类</div></div></div></h1><div><br></div><div><div><div>Christoph H. Lampert, Hannes Nickisch, and Stefan Harmeling<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">克里斯托夫·H·兰佩特（Christoph H. Lampert）、汉斯·尼克施（Hannes Nickisch）和斯特凡·哈梅林（Stefan Harmeling）</div></div></div></div><div><br></div><div><div><div>Abstract-We study the problem of object recognition for categories for which we have no training examples, a task also called zero-data or zero-shot learning. This situation has hardly been studied in computer vision research, even though it occurs frequently; the world contains tens of thousands of different object classes, and image collections have been formed and suitably annotated for only a few of them. To tackle the problem, we introduce attribute-based classification: Objects are identified based on a high-level description that is phrased in terms of semantic attributes, such as the object's color or shape. Because the identification of each such property transcends the specific learning task at hand, the attribute classifiers can be prelearned independently, for example, from existing image data sets unrelated to the current task. Afterward, new classes can be detected based on their attribute representation, without the need for a new training phase. In this paper, we also introduce a new data set, Animals with Attributes, of over 30,000 images of 50 animal classes, annotated with 85 semantic attributes. Extensive experiments on this and two more data sets show that attribute-based classification indeed is able to categorize images without access to any training images of the target classes.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">摘要 — 我们研究了针对没有训练样本的类别进行目标识别的问题，这一任务也被称为零数据或零样本学习。尽管这种情况经常出现，但在计算机视觉研究中却很少被探讨；世界上存在数以万计的不同目标类别，而图像集仅针对其中少数类别进行了构建和适当标注。为了解决这个问题，我们引入了基于属性的分类方法：目标基于用语义属性（如目标的颜色或形状）表述的高级描述来识别。由于对每个此类属性的识别超越了当前特定的学习任务，因此属性分类器可以独立预学习，例如，从与当前任务无关的现有图像数据集中学习。之后，无需新的训练阶段，就可以基于新类别的属性表示来检测它们。在本文中，我们还引入了一个新的数据集“带属性的动物”（Animals with Attributes），其中包含50种动物类别的3万多张图像，并标注了85个语义属性。在这个数据集以及另外两个数据集上进行的大量实验表明，基于属性的分类方法确实能够在不访问目标类别的任何训练图像的情况下对图像进行分类。</div></div></div></div><div><br></div><div><div><div>Index Terms-Object recognition, vision and scene understanding<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">关键词 — 目标识别、视觉与场景理解</div></div></div></div><div><br></div><h2><div><div>1 INTRODUCTION<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">1 引言</div></div></div></h2><div><br></div><div><div><div>T<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3865" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> HE field of object recognition in natural images has made tremendous progress over the last decade. For specific object classes, in particular faces, pedestrians, and vehicles, reliable and efficient detectors are available, based on the combination of powerful low-level features, such as SIFT [1] or HoG [2], with modern machine learning techniques, such as support vector machines (SVMs) [3], [4] or boosting [5]. However, to achieve good classification accuracy, these systems require a lot of manually labeled training data, typically several thousand example images for each class to be learned.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">T<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3866" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> 在过去十年中，自然图像中的目标识别领域取得了巨大进展。对于特定的目标类别，特别是人脸、行人和车辆，基于强大的低级特征（如SIFT [1] 或HoG [2]）与现代机器学习技术（如支持向量机（SVMs） [3]、[4] 或提升算法 [5]）的结合，已经有了可靠且高效的检测器。然而，为了实现良好的分类准确率，这些系统需要大量手动标注的训练数据，通常每个要学习的类别需要数千张示例图像。</div></div></div></div><div><br></div><div><div><div>While building recognition systems this way is feasible for categories of large common or commercial interest, one cannot expect it to solve object recognition for all natural categories. It has been estimated that humans distinguish between approximately 30,000 basic object categories [6], and many more subordinate ones, such as different breeds of dogs or different car models [7]. It has even been argued that there are infinitely many potentially relevant categorization tasks because humans can create new categories on the fly, for example, "things to bring to a camping trip" [8]. Training conventional object detectors for all these would require millions or billions of well-labeled training images and is likely out of reach for many years, if it is possible at all. Therefore, numerous techniques for reducing the number of necessary training images have been developed, some of which we will discuss in Section 3. However, all of these techniques still require at least some labeled training examples to detect future object instances.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">虽然以这种方式构建识别系统对于具有广泛公共或商业利益的类别是可行的，但不能期望它能解决所有自然类别的目标识别问题。据估计，人类能够区分大约30000种基本目标类别 [6]，以及更多的子类别，如不同品种的狗或不同的汽车型号 [7]。甚至有人认为，存在无限多个潜在相关的分类任务，因为人类可以即时创建新的类别，例如“露营旅行要带的东西” [8]。为所有这些类别训练传统的目标检测器需要数百万甚至数十亿张标注良好的训练图像，即使有可能，在未来许多年内也可能无法实现。因此，已经开发了许多减少所需训练图像数量的技术，其中一些我们将在第3节中讨论。然而，所有这些技术仍然至少需要一些标注的训练示例来检测未来的目标实例。</div></div></div></div><div><br></div><div><div><div>Human learning works differently: Although humans can, of course, learn and generalize well from examples, they are also capable of identifying completely new classes when provided with a high-level description. For example, from the phrase "eight-sided red traffic sign with white writing," we will be able to detect stop signs, and when looking for "large gray animals with long trunks," we will reliably identify elephants. In this work, which extends our original publication [9], we build on this observation and propose a system that is able to classify objects from a list of high-level semantically meaningful properties that we call attributes. The attributes serve as an intermediate layer in a classifier cascade and they enable the system to recognize object classes for which it had not seen a single training example.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">人类的学习方式不同：当然，人类可以从示例中很好地学习和泛化，但当提供高级描述时，他们也能够识别全新的类别。例如，根据“带有白色文字的八边形红色交通标志”这一描述，我们能够识别停车标志；当寻找“长着长鼻子的大型灰色动物”时，我们能够可靠地识别大象。在这项扩展了我们原论文 [9] 的工作中，我们基于这一观察结果，提出了一个系统，该系统能够根据我们称为属性的高级语义有意义属性列表对目标进行分类。这些属性在分类器级联中充当中间层，使系统能够识别它从未见过一个训练示例的目标类别。</div></div></div></div><div><br></div><div><div><div>Clearly, a large number of potential attributes exist and collecting separate training material to learn an ordinary classifier for each of them would be as tedious as doing so for all object classes. Therefore, one of our main contributions in this work is to show how, instead of creating a separate training set for each attribute, we can exploit the fact that meaningful high-level concepts transcend class boundaries. To learn such attributes, we can make use of existing training data by merging images of several object classes. To learn, for example, the attribute striped, we can use images of zebras, bees, and tigers. For the attribute yellow, zebras would not be included, but bees and tigers would still prove useful, possibly together with canary birds. It is this possibility to obtain knowledge about attributes from different object classes and, vice versa, the fact that each attribute can be used for the detection of many object classes that makes our proposed learning method statistically efficient.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">显然，存在大量潜在的属性，为每个属性收集单独的训练材料来学习一个普通的分类器，就像为所有目标类别这样做一样繁琐。因此，我们在这项工作中的主要贡献之一是展示如何不针对每个属性创建单独的训练集，而是利用有意义的高级概念超越类别界限这一事实。为了学习这些属性，我们可以通过合并多个目标类别的图像来利用现有的训练数据。例如，为了学习“有条纹的”这一属性，我们可以使用斑马、蜜蜂和老虎的图像。对于“黄色的”这一属性，斑马的图像不会被纳入，但蜜蜂和老虎的图像可能仍然有用，可能还可以加上金丝雀的图像。正是这种从不同目标类别获取属性知识的可能性，以及反之每个属性可用于检测许多目标类别的事实，使得我们提出的学习方法在统计上是高效的。</div></div></div></div><div><br></div><div><div><div>. Downloaded on March 26,2025 at 13:20:16 UTC from IEEE Xplore. Restrictions apply.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">. 于2025年3月26日13:20:16 UTC从IEEE Xplore下载。适用限制条款。</div></div></div></div><div><br></div><div><div><div>Published by the IEEE Computer Society<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">由IEEE计算机协会出版</div></div></div></div><div><br><hr><ul><li>C.H. Lampert is with the Institute of Science and Technology Austria, Am Campus 1, Klosterneuburg 3400, Austria. E-mail: chl@ist.ac.at.</li></ul><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><ul><li>C.H. 兰珀特（C.H. Lampert）就职于奥地利科技学院（Institute of Science and Technology Austria），地址为奥地利克洛斯特新堡市校园1号（Am Campus 1, Klosterneuburg 3400, Austria）。电子邮件：chl@ist.ac.at。</li></ul></div><br><ul><li>H. Nickisch is with Philips Research, Röntgenstrasse 24-26, 22335 Hamburg, Germany. E-mail: hannes@nickisch.org.</li></ul><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><ul><li>H. 尼克施（H. Nickisch）就职于飞利浦研究院（Philips Research），地址为德国汉堡市伦琴街24 - 26号（Röntgenstrasse 24 - 26, 22335 Hamburg, Germany）。电子邮件：hannes@nickisch.org。</li></ul></div><br><ul><li>S. Harmeling is with the Max Planck Institute for Intelligent Systems, 72076 Tübingen, Germany. E-mail: stefan.harmeling@tuebingen.mpg.de.</li></ul><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><ul><li>S. 哈梅林（S. Harmeling）就职于马克斯·普朗克智能系统研究所（Max Planck Institute for Intelligent Systems），地址为德国图宾根市72076（72076 Tübingen, Germany）。电子邮件：stefan.harmeling@tuebingen.mpg.de。</li></ul></div><br></div><div><div><div>Manuscript received 5 Sept. 2012; revised 15 Mar. 2013; accepted 12 July 2013; published online 29 July 2013.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">稿件于2012年9月5日收到；2013年3月15日修订；2013年7月12日接受；2013年7月29日在线发表。</div></div></div></div><div><br></div><div><div><div>Recommended for acceptance by D. Forsyth.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">由D. 福赛思（D. Forsyth）推荐接受。</div></div></div></div><div><br></div><div><div><div>For information on obtaining reprints of this article, please send e-mail to: tpami@computer.org, and reference IEEECS Log Number TPAMI-2012-09-0701. Digital Object Identifier no. 10.1109/TPAMI.2013.140.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">如需获取本文的重印本信息，请发送电子邮件至：tpami@computer.org，并注明IEEE计算机协会日志编号TPAMI - 2012 - 09 - 0701。数字对象标识符：10.1109/TPAMI.2013.140。</div></div></div></div><div><br></div><div><div><div>Authorized licensed use limited to: JILIN UNIVERSITY. Downloaded or 0162-8828/14/$31.00 © 2014 IEEE<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">授权许可使用仅限于：吉林大学。下载或0162 - 8828/14/$31.00 © 2014 电气与电子工程师协会（IEEE）</div></div></div></div><div><br><hr></div><div><div><div></div></div></div></span></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-ca1f3eaf-c314-4bb6-9f55-05515c3575a6" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="0,0"><div style="height: auto;"><span style="display: inline;"><h2><div><div>2 INFORMATION TRANSFER BY ATTRIBUTE SHARING<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">2 通过属性共享进行信息传递</div></div></div></h2><div><br></div><div><div><div>We begin by formalizing the problem and our intuition from the previous section that the use of attributes allows us to transfer information between object classes. We first define the exact situation of our interest:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们首先将问题形式化，并基于上一节的直觉，即属性的使用使我们能够在对象类别之间传递信息。我们首先定义我们感兴趣的具体情况：</div></div></div></div><div><br></div><div><div><div>Learning with disjoint training and test classes. Let <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3867" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c58 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">X</mi></mrow></math></mjx-assistive-mml></mjx-container> be an arbitrary feature space and let <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3868" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mrow space="4"><mjx-mo class="mjx-n"><mjx-c class="mjx-c7B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2026"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c7D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow><mo>=</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">{</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>y</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>y</mi></mrow><mrow data-mjx-texclass="ORD"><mi>K</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">}</mo></mrow></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3869" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow><mo>=</mo></math></mjx-assistive-mml></mjx-container> <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3870" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c7B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2026"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c7D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">{</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>L</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">}</mo></mrow></math></mjx-assistive-mml></mjx-container> be sets of object categories,also called classes. The task of learning with disjoint training and test classes is to construct a classifier <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3871" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c58 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo>:</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">X</mi></mrow><mo stretchy="false">→</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow></math></mjx-assistive-mml></mjx-container> by making use of training examples <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3872" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2026"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2282"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c58 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>x</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><mo>…</mo><mo>,</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>x</mi></mrow><mrow data-mjx-texclass="ORD"><mi>n</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow><mrow data-mjx-texclass="ORD"><mi>n</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>⊂</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">X</mi></mrow><mo>×</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow></math></mjx-assistive-mml></mjx-container> even if <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3873" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2229"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-var" space="4"><mjx-c class="mjx-c2205 TEX-A"></mjx-c></mjx-mi><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow><mo>∩</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow><mo>=</mo><mi data-mjx-alternate="1">∅</mi><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">使用不相交的训练和测试类别进行学习。设<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3874" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c58 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">X</mi></mrow></math></mjx-assistive-mml></mjx-container>为任意特征空间，设<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3875" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mrow space="4"><mjx-mo class="mjx-n"><mjx-c class="mjx-c7B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2026"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c7D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow><mo>=</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">{</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>y</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>y</mi></mrow><mrow data-mjx-texclass="ORD"><mi>K</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">}</mo></mrow></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3876" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow><mo>=</mo></math></mjx-assistive-mml></mjx-container> <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3877" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c7B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2026"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c7D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">{</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>L</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">}</mo></mrow></math></mjx-assistive-mml></mjx-container>为对象类别集合，也称为类。使用不相交的训练和测试类别进行学习的任务是，即使<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3878" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2229"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-var" space="4"><mjx-c class="mjx-c2205 TEX-A"></mjx-c></mjx-mi><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow><mo>∩</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow><mo>=</mo><mi data-mjx-alternate="1">∅</mi><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container>，也要利用训练示例<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3879" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2026"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2282"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c58 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>x</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><mo>…</mo><mo>,</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>x</mi></mrow><mrow data-mjx-texclass="ORD"><mi>n</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow><mrow data-mjx-texclass="ORD"><mi>n</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>⊂</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">X</mi></mrow><mo>×</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow></math></mjx-assistive-mml></mjx-container>构建分类器<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3880" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c58 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo>:</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">X</mi></mrow><mo stretchy="false">→</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow></math></mjx-assistive-mml></mjx-container>。</div></div></div></div><div><br></div><div><div><div>Fig. 2a illustrates graphically why this task cannot be solved by ordinary multiclass classification: Standard classifiers learn one parameter vector (or other representation) <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3881" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FC TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>α</mi></mrow><mrow data-mjx-texclass="ORD"><mi>k</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> for each training class <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3882" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2026"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>y</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>y</mi></mrow><mrow data-mjx-texclass="ORD"><mi>K</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> . Because the classes <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3883" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2026"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>L</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> are not present during the training step,no parameter vector can be derived for them, and it is impossible to make predictions about these classes for future samples.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图2a以图形方式说明了为什么普通的多类分类无法解决此任务：标准分类器为每个训练类<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3884" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2026"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>y</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>y</mi></mrow><mrow data-mjx-texclass="ORD"><mi>K</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>学习一个参数向量（或其他表示）<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3885" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FC TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>α</mi></mrow><mrow data-mjx-texclass="ORD"><mi>k</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>。由于类<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3886" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2026"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>L</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>在训练步骤中不存在，因此无法为它们导出参数向量，也无法对未来样本的这些类进行预测。</div></div></div></div><div><br></div><div><div><div>To make predictions about classes for which no training data are available, one needs to introduce a coupling between the classes in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3887" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3888" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow></math></mjx-assistive-mml></mjx-container> . Since no training data for the unobserved classes are available, this coupling cannot be learned from samples, but it has to be inserted into the system by human effort. Preferably, the amount of human effort to specify new classes should be small because otherwise collecting and labeling training samples might be a simpler solution.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">为了对没有训练数据的类进行预测，需要在<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3889" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3890" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow></math></mjx-assistive-mml></mjx-container>中的类之间引入一种耦合。由于没有未观察到的类的训练数据，这种耦合不能从样本中学习，而必须由人工插入到系统中。最好，指定新类所需的人工工作量应较小，因为否则收集和标记训练样本可能是一种更简单的解决方案。</div></div></div></div><div><br></div><h3><div><div>2.1 Attribute-Based Classification<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">2.1 基于属性的分类</div></div></div></h3><div><br></div><div><div><div>We propose a solution for learning with disjoint training and test classes by introducing a small set of high-level semantic attributes that can be specified either on a per-class or on a per-image level. While we currently have no formal definition of what should count as an attribute, in the rest of the manuscript, we rely on the following characterization.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们通过引入一小套高级语义属性，为使用不相交的训练和测试类别进行学习提出了一种解决方案，这些属性可以在每个类或每个图像的级别上指定。虽然我们目前没有对什么应算作属性给出正式定义，但在本文的其余部分，我们依赖于以下特征描述。</div></div></div></div><div><br></div><div><div><div>Attributes. We call a property of an object an attribute, if a human has the ability to decide whether the property is present or not for a certain object. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3891" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msup></math></mjx-assistive-mml></mjx-container><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">属性。如果人类能够判断某个对象是否具有某个属性，我们就称该对象的这个属性为属性。<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3892" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></div></div></div></div><div><br></div><div><div><div>Attributes are typically nameable properties, for example, the color of an object, or the presence or absence of a certain body part. Note that the definition allows properties that are not directly visible but related to visual information, such as an animal's natural habitat. Fig. 1 shows examples of classes and attributes.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">属性通常是可命名的属性，例如，对象的颜色，或某个身体部位的存在与否。请注意，该定义允许那些并非直接可见但与视觉信息相关的属性，例如动物的自然栖息地。图1显示了类和属性的示例。</div></div></div></div><div><br></div><div><div><div>An important distinction between attributes and arbitrary features is the aspect of semantics: Humans associate a meaning with a given attribute name. This allows them to create annotation directly in form of attribute values, which can then be used by the computer. Ordinary image features, on the other hand, are typically computable, but they lack the human interpretability.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">属性和任意特征之间的一个重要区别在于语义方面：人类会将给定的属性名称与某种含义关联起来。这使得他们能够直接以属性值的形式创建注释，然后计算机可以使用这些注释。另一方面，普通的图像特征通常是可计算的，但缺乏人类可解释性。</div></div></div></div><div><br><!-- Media --><br><!-- figureText: otter black: yes white: no brown: yes stripes: no water: yes eats fish: yes polar bear black: brown: stripes: water: yes eats fish: yes zebra black: yes white: yes brown: stripes: water: --><br></div><img src="https://cdn.noedgeai.com/0195d2b2-ac8c-7630-a76c-75ecc68f9592_1.jpg?x=868&amp;y=144&amp;w=758&amp;h=641&amp;r=0" alt="https://cdn.noedgeai.com/0195d2b2-ac8c-7630-a76c-75ecc68f9592_1.jpg?x=868&amp;y=144&amp;w=758&amp;h=641&amp;r=0"><div><br></div><div><div><div>Fig. 1. Examples from the Animals with Attributes: object classes with per-class attribute annotation.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图1. 动物属性数据集的示例：带有按类别属性注释的对象类别。</div></div></div></div><div><br><!-- Media --><br></div><div><div><div>It is possible to assign attributes on a per-image basis, or on a per-class basis. The latter is particularly helpful, since it allows the creation of attribute annotation for a new classes with minimal effort. To make use of such attribute annotation, we propose attribute-based classification.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">可以基于每张图像或每个类别来分配属性。后者特别有用，因为它允许以最小的工作量为新类别创建属性注释。为了利用此类属性注释，我们提出了基于属性的分类方法。</div></div></div></div><div><br></div><div><div><div>Attribute-based classification. Assume the situation of learning with disjoint training and test classes. If for each class <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3893" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi><mo>∈</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3894" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi><mo>∈</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow></math></mjx-assistive-mml></mjx-container> ,an attribute representation <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3895" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msup space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c41 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mi>a</mi></mrow><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow></msup><mo>,</mo><msup><mrow data-mjx-texclass="ORD"><mi>a</mi></mrow><mrow data-mjx-texclass="ORD"><mi>y</mi></mrow></msup><mo>∈</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">A</mi></mrow></math></mjx-assistive-mml></mjx-container> is available,then we can learn a nontrivial classifier <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3896" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FC TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c58 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi><mo>:</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">X</mi></mrow><mo stretchy="false">→</mo></math></mjx-assistive-mml></mjx-container> <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3897" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow></math></mjx-assistive-mml></mjx-container> by transferring information between <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3898" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3899" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow></math></mjx-assistive-mml></mjx-container> through <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3900" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c41 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">A</mi></mrow></math></mjx-assistive-mml></mjx-container> .<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">基于属性的分类。假设存在训练类和测试类不相交的学习情况。如果对于每个类别<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3901" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi><mo>∈</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3902" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi><mo>∈</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow></math></mjx-assistive-mml></mjx-container>，都有一个属性表示<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3903" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msup space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c41 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mi>a</mi></mrow><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow></msup><mo>,</mo><msup><mrow data-mjx-texclass="ORD"><mi>a</mi></mrow><mrow data-mjx-texclass="ORD"><mi>y</mi></mrow></msup><mo>∈</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">A</mi></mrow></math></mjx-assistive-mml></mjx-container>可用，那么我们可以通过<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3904" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c41 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">A</mi></mrow></math></mjx-assistive-mml></mjx-container>在<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3905" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3906" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow></math></mjx-assistive-mml></mjx-container>之间传递信息，从而学习一个非平凡的分类器<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3907" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FC TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c58 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi><mo>:</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">X</mi></mrow><mo stretchy="false">→</mo></math></mjx-assistive-mml></mjx-container> <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3908" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow></math></mjx-assistive-mml></mjx-container>。</div></div></div></div><div><br></div><div><div><div>In the rest of this paper, we will demonstrate that attribute-based classification indeed offers a solution to the problem of learning with disjoint training and test classes, and how it can be practically used for object classification. For this, we introduce and compare two generic methods to integrate attributes into multiclass classification.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">在本文的其余部分，我们将证明基于属性的分类确实为训练类和测试类不相交的学习问题提供了解决方案，以及如何将其实际应用于对象分类。为此，我们介绍并比较了两种将属性集成到多类分类中的通用方法。</div></div></div></div><div><br></div><div><div><div>Direct attribute prediction (DAP), illustrated in Fig. 2b, uses an in-between layer of attribute variables to decouple the images from the layer of labels. During training, the output class label of each sample induces a deterministic labeling of the attribute layer. Consequently, any supervised learning method can be used to learn per-attribute parameters <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3909" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>β</mi></mrow><mrow data-mjx-texclass="ORD"><mi>m</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> . At test time,these allow the prediction of attribute values for each test sample, from which the test class labels are inferred. Note that the classes during testing can differ from the classes used for training, as long as the coupling attribute layer is determined in a way that does not require a training phase.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">直接属性预测（DAP），如图2b所示，使用属性变量的中间层将图像与标签层解耦。在训练期间，每个样本的输出类别标签会对属性层进行确定性标记。因此，可以使用任何监督学习方法来学习每个属性的参数<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3910" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>β</mi></mrow><mrow data-mjx-texclass="ORD"><mi>m</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>。在测试时，这些参数允许为每个测试样本预测属性值，并由此推断出测试类别标签。请注意，测试期间的类别可以与训练时使用的类别不同，只要耦合属性层的确定方式不需要训练阶段即可。</div></div></div></div><div><br></div><div><div><div>Indirect attribute prediction (IAP), depicted in Fig. 2c, also uses the attributes to transfer knowledge between classes, but the attributes form a connecting layer between two layers of labels: one for classes that are known at training time and one for classes that are not. The training phase of IAP consists of learning a classifier for each training class, as it would be the case in ordinary multiclass classification. At test time, the predictions for all training classes induce a labeling of the attribute layer, from which a labeling over the test classes is inferred.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">间接属性预测（IAP），如图2c所示，也使用属性在类别之间传递知识，但属性形成了两个标签层之间的连接层：一个用于训练时已知的类别，另一个用于未知的类别。IAP的训练阶段包括为每个训练类别学习一个分类器，就像普通的多类分类那样。在测试时，所有训练类别的预测结果会对属性层进行标记，并由此推断出测试类别的标记。</div></div></div></div><div><br><hr><ol><li value="1">It is not necessary for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3911" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3912" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow></math></mjx-assistive-mml></mjx-container> to be disjoint for the problems described to occur, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3913" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2288 TEX-A"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow><mo>⊈</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow></math></mjx-assistive-mml></mjx-container> is sufficient. However,for the sake of clarity,we only treat the case of disjoint class sets in this work.</li></ol><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><ol><li value="1">对于所描述的问题，<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3914" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow></math></mjx-assistive-mml></mjx-container>和<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3915" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow></math></mjx-assistive-mml></mjx-container>不需要不相交，<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3916" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c5A TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2288 TEX-A"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c59 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Z</mi></mrow><mo>⊈</mo><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">Y</mi></mrow></math></mjx-assistive-mml></mjx-container>就足够了。然而，为了清晰起见，在本文中我们只处理类别集不相交的情况。</li></ol></div><br><ol><li value="2">In this manuscript, we only consider binary-valued attributes. More general forms of attributes have already appeared in the literature; see Section 3.</li></ol><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><ol><li value="2">在本文中，我们只考虑二值属性。更一般形式的属性已经在文献中出现过；详见第3节。</li></ol></div><br><hr></div><div><div><div></div></div></div></span></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-df2c90e7-1471-4623-8530-1bfb54e33f34" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-b8b29947-a70d-4a8e-a773-63d92fc0c1cd" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-1ab843ea-de00-4c6f-8e70-f021ae5bb470" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-6e5bec2c-f689-4ab5-8092-e95ed1b38d63" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-af09c9f6-a470-43f4-8c7e-309b73fbd10a" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-6="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-0a931729-fedd-48f1-b5b5-ad4d7165aa65" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-23aa82d1-6893-4f2f-b8ae-694bc5c6f867" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-59a1ed4c-632a-4323-b7dd-1dd6196efc0c" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-123f8e0f-236e-48bf-9102-b783e4f1b89f" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-3c13e2f3-1f20-4e6e-a34e-cb3cdc74a2b2" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-11="0,0"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-11="0,0"></paragraphpositioning></div></div></div><h2><div><div>REFERENCES<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">参考文献</div></div></div></h2><div><br></div><div><div><div>[1] D.G. Lowe, "Distinctive Image Features from Scale-Invariant Keypoints," Int'l J. Computer Vision, vol. 60, no. 2, pp. 91-110, 2004.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[1] D.G. 洛（D.G. Lowe），“尺度不变关键点的独特图像特征”，《国际计算机视觉杂志》，第60卷，第2期，第91 - 110页，2004年。</div></div></div></div><div><br></div><div><div><div>[2] N. Dalal and B. Triggs, "Histograms of Oriented Gradients for<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[2] N. 达拉尔（N. Dalal）和B. 特里格斯（B. Triggs），“用于</div></div></div></div><div><br></div><div><div><div>Human Detection," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2005.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">人体检测的方向梯度直方图”，《电气与电子工程师协会计算机视觉与模式识别会议（CVPR）论文集》，2005年。</div></div></div></div><div><br></div><div><div><div>[3] B. Schölkopf and A.J. Smola, Learning with Kernels. MIT Press, 2002.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[3] B. 肖尔科普夫（B. Schölkopf）和A.J. 斯莫拉（A.J. Smola），《核方法学习》。麻省理工学院出版社，2002年。</div></div></div></div><div><br></div><div><div><div>[4] C.H. Lampert, "Kernel Methods in Computer Vision," Foundations and Trends in Computer Graphics and Vision, vol. 4, no. 3, pp. 193- 285, 2009.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[4] C.H. 兰珀特（C.H. Lampert），《计算机视觉中的核方法》，《计算机图形与视觉基础与趋势》，第4卷，第3期，第193 - 285页，2009年。</div></div></div></div><div><br></div><div><div><div>[5] R.E. Schapire and Y. Freund, Boosting: Foundations and Algorithms. MIT Press, 2012.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[5] R.E. 沙皮尔（R.E. Schapire）和Y. 弗罗因德（Y. Freund），《提升算法：基础与算法》，麻省理工学院出版社，2012年。</div></div></div></div><div><br></div><div><div><div>[6] I. Biederman, "Recognition by Components: A Theory of Human Image Understanding," Psychological Rev., vol. 94, no. 2, pp. 115- 147, 1987.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[6] I. 比德曼（I. Biederman），《基于组件的识别：人类图像理解理论》，《心理学评论》，第94卷，第2期，第115 - 147页，1987年。</div></div></div></div><div><br></div><div><div><div>[7] B. Yao, A. Khosla, and L. Fei-Fei, "Combining Randomization and Discrimination for Fine-Grained Image Categorization," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2011.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[7] B. 姚（B. Yao）、A. 科斯拉（A. Khosla）和L. 费菲（L. Fei - Fei），《结合随机化与判别进行细粒度图像分类》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（CVPR），2011年。</div></div></div></div><div><br></div><div><div><div>[8] G.L. Murphy, The Big Book of Concepts. MIT Press, 2004.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[8] G.L. 墨菲（G.L. Murphy），《概念大百科》，麻省理工学院出版社，2004年。</div></div></div></div><div><br></div><div><div><div>[9] C.H. Lampert, H. Nickisch, and S. Harmeling, "Learning to Detect Unseen Object Classes by Between-Class Attribute Transfer," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2009.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[9] C.H. 兰珀特（C.H. Lampert）、H. 尼克施（H. Nickisch）和S. 哈梅林（S. Harmeling），《通过类间属性转移学习检测未见物体类别》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（CVPR），2009年。</div></div></div></div><div><br></div><div><div><div>[10] D. Parikh and K. Grauman, "Relative Attributes," Proc. IEEE Int'l Conf. Computer Vision (ICCV), 2011.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[10] D. 帕里克（D. Parikh）和K. 格劳曼（K. Grauman），《相对属性》，《电气与电子工程师协会国际计算机视觉会议论文集》（ICCV），2011年。</div></div></div></div><div><br></div><div><div><div>[11] D.E. Knuth, "Two Notes on Notation," Am. Math. Monthly, vol. 99, no. 5, pp. 403-422, 1992.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[11] D.E. 克努斯（D.E. Knuth），《关于符号的两点注记》，《美国数学月刊》，第99卷，第5期，第403 - 422页，1992年。</div></div></div></div><div><br></div><div><div><div>[12] D.E. Rumelhart, G.E. Hinton, and R.J. Williams, "Learning Internal Representations by Error Propagation," Parallel Distributed Processing, MIT Press, 1986.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[12] D.E. 鲁梅尔哈特（D.E. Rumelhart）、G.E. 辛顿（G.E. Hinton）和R.J. 威廉姆斯（R.J. Williams），《通过误差传播学习内部表示》，《并行分布式处理》，麻省理工学院出版社，1986年。</div></div></div></div><div><br></div><div><div><div>[13] L. Breiman, J.J. Friedman, R.A. Olshen, and C.J. Stone, Classification and Regression Trees. Wadsworth, 1984.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[13] L. 布雷曼（L. Breiman）、J.J. 弗里德曼（J.J. Friedman）、R.A. 奥尔申（R.A. Olshen）和C.J. 斯通（C.J. Stone），《分类与回归树》，沃兹沃思出版社，1984年。</div></div></div></div><div><br></div><div><div><div>[14] M.I. Jordan and R.A. Jacobs, "Hierarchical Mixtures of Experts and the EM Algorithm," Neural Computation, vol. 6, no. 2, pp. 181- 214, 1994.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[14] M.I. 乔丹（M.I. Jordan）和R.A. 雅各布斯（R.A. Jacobs），《专家分层混合模型与期望最大化算法》，《神经计算》，第6卷，第2期，第181 - 214页，1994年。</div></div></div></div><div><br></div><div><div><div>[15] Y. Freund and R.E. Schapire, "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting," J. Computer and System Sciences, vol. 55, no. 1, pp. 119-139, 1997.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[15] Y. 弗罗因德（Y. Freund）和R.E. 沙皮尔（R.E. Schapire），《在线学习的决策理论推广及其在提升算法中的应用》，《计算机与系统科学杂志》，第55卷，第1期，第119 - 139页，1997年。</div></div></div></div><div><br></div><div><div><div>[16] T.G. Dietterich and G. Bakiri, "Solving Multiclass Learning Problems via Error-Correcting Output Codes," J. Artificial Intelligence Research, vol. 2, pp. 263-286, 1995.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[16] T.G. 迪特里希（T.G. Dietterich）和G. 巴基里（G. Bakiri），《通过纠错输出码解决多类学习问题》，《人工智能研究杂志》，第2卷，第263 - 286页，1995年。</div></div></div></div><div><br></div><div><div><div>[17] R. Rifkin and A. Klautau, "In Defense of One-vs-All Classification," J. Machine Learning Research, vol. 5, pp. 101-141, 2004.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[17] R. 里夫金（R. Rifkin）和A. 克劳陶（A. Klautau），《为一对多分类辩护》，《机器学习研究杂志》，第5卷，第101 - 141页，2004年。</div></div></div></div><div><br></div><div><div><div>[18] M. Ranzato, F.J. Huang, Y.-L. Boureau, and Y. LeCun, "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2007.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[18] M. 兰扎托（M. Ranzato）、F.J. 黄（F.J. Huang）、Y. - L. 布雷奥（Y. - L. Boureau）和Y. 勒昆（Y. LeCun），《具有不变特征层次结构的无监督学习及其在物体识别中的应用》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（CVPR），2007年。</div></div></div></div><div><br></div><div><div><div>[19] J. Winn and N. Jojic, "LOCUS: Learning Object Classes with Unsupervised Segmentation," Proc. IEEE Int'l Conf. Computer Vision (ICCV), vol. 1, 2005.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[19] J. 温（J. Winn）和N. 约伊西奇（N. Jojic），《LOCUS：通过无监督分割学习物体类别》，《电气与电子工程师协会国际计算机视觉会议论文集》（ICCV），第1卷，2005年。</div></div></div></div><div><br></div><div><div><div>[20] M.A. Fischler and R.A. Elschlager, "The Representation and Matching of Pictorial Structures," IEEE Trans. Computers, vol. 22, no. 1, pp. 67-92, Jan. 1973.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[20] M.A. 菲施勒（Fischler）和 R.A. 埃尔施拉格（Elschlager），《图像结构的表示与匹配》，《电气与电子工程师协会计算机汇刊》（IEEE Trans. Computers），第 22 卷，第 1 期，第 67 - 92 页，1973 年 1 月。</div></div></div></div><div><br></div><div><div><div>[21] R. Fergus, P. Perona, and A. Zisserman, "Object Class Recognition by Unsupervised Scale-Invariant Learning," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2003.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[21] R. 弗格斯（Fergus）、P. 佩罗纳（Perona）和 A. 齐斯曼（Zisserman），《通过无监督尺度不变学习进行目标类别识别》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)），2003 年。</div></div></div></div><div><br></div><div><div><div>[22] P.F. Felzenszwalb, D. McAllester, and D. Ramanan, "A Discriminatively Trained, Multiscale, Deformable Part Model," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2008.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[22] P.F. 费尔曾斯瓦尔布（Felzenszwalb）、D. 麦卡利斯特（McAllester）和 D. 拉马南（Ramanan），《一种经过判别式训练的多尺度可变形部件模型》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)），2008 年。</div></div></div></div><div><br></div><div><div><div>[23] J.C. Platt, N. Cristianini, and J. Shawe-Taylor, "Large Margin DAGs for Multiclass Classification," Proc. Advances in Neural Information Processing Systems (NIPS), 1999.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[23] J.C. 普拉特（Platt）、N. 克里斯蒂亚尼尼（Cristianini）和 J. 肖韦 - 泰勒（Shawe - Taylor），《用于多类分类的大间隔有向无环图》，《神经信息处理系统进展会议论文集》（Proc. Advances in Neural Information Processing Systems (NIPS)），1999 年。</div></div></div></div><div><br></div><div><div><div>[24] A. Torralba and K.P. Murphy, "Sharing Visual Features for Multiclass and Multiview Object Detection," IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 29, no. 5, pp. 854-869, May 2007.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[24] A. 托拉尔巴（Torralba）和 K.P. 墨菲（Murphy），《用于多类和多视图目标检测的视觉特征共享》，《电气与电子工程师协会模式分析与机器智能汇刊》（IEEE Trans. Pattern Analysis and Machine Intelligence），第 29 卷，第 5 期，第 854 - 869 页，2007 年 5 月。</div></div></div></div><div><br></div><div><div><div>[25] P. Zehnder, E.K. Meier, and L.J.V. Gool, "An Efficient Shared Multi-Class Detection Cascade," Proc. British Machine Vision Conf. (BMVC), 2008.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[25] P. 曾德（Zehnder）、E.K. 迈尔（Meier）和 L.J.V. 古尔（Gool），《一种高效的共享多类检测级联》，《英国机器视觉会议论文集》（Proc. British Machine Vision Conf. (BMVC)），2008 年。</div></div></div></div><div><br></div><div><div><div>[26] E. Miller, N. Matsakis, and P. Viola, "Learning from One Example through Shared Densities on Transforms," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2000.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[26] E. 米勒（Miller）、N. 马塔斯基斯（Matsakis）和 P. 维奥拉（Viola），《通过变换上的共享密度从单个示例中学习》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)），2000 年。</div></div></div></div><div><br></div><div><div><div>[27] F.F. Li, R. Fergus, and P. Perona, "One-Shot Learning of Object Categories," IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 28, no. 4, pp. 594-611, Apr. 2006.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[27] F.F. 李（Li）、R. 弗格斯（Fergus）和 P. 佩罗纳（Perona），《目标类别的一次性学习》，《电气与电子工程师协会模式分析与机器智能汇刊》（IEEE Trans. Pattern Analysis and Machine Intelligence），第 28 卷，第 4 期，第 594 - 611 页，2006 年 4 月。</div></div></div></div><div><br></div><div><div><div>[28] E. Bart and S. Ullman, "Cross-Generalization: Learning Novel Classes from a Single Example by Feature Replacement," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2005.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[28] E. 巴特（Bart）和 S. 厄尔曼（Ullman），《交叉泛化：通过特征替换从单个示例中学习新类别》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)），2005 年。</div></div></div></div><div><br></div><div><div><div>[29] H. Larochelle, D. Erhan, and Y. Bengio, "Zero-Data Learning of New Tasks," Proc. 23rd Nat'l Conf. Artificial Intelligence, vol. 1, no. 2, pp. 646-651, 2008.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[29] H. 拉罗谢尔（Larochelle）、D. 埃尔汗（Erhan）和 Y. 本吉奥（Bengio），《新任务的零数据学习》，《第 23 届全国人工智能会议论文集》（Proc. 23rd Nat'l Conf. Artificial Intelligence），第 1 卷，第 2 期，第 646 - 651 页，2008 年。</div></div></div></div><div><br></div><div><div><div>[30] K. Yanai and K. Barnard, "Image Region Entropy: A Measure of<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[30] K. 矢内（Yanai）和 K. 巴纳德（Barnard），《图像区域熵：一种衡量</div></div></div></div><div><br></div><div><div><div>Visualness of Web Images Associated with One Concept," Proc. 13th Ann. ACM Int'l Conf. Multimedia, pp. 419-422, 2005.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">与一个概念相关的网络图像视觉性的指标》，《第 13 届年度美国计算机协会国际多媒体会议论文集》（Proc. 13th Ann. ACM Int'l Conf. Multimedia），第 419 - 422 页，2005 年。</div></div></div></div><div><br></div><div><div><div>[31] J. Van De Weijer, C. Schmid, J. Verbeek, and D. Larlus, "Learning Color Names for Real-World Applications," IEEE Trans. Image Processing, vol. 18, no. 7, pp. 1512-1523, July 2009.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[31] J. 范德韦杰尔（Van De Weijer）、C. 施密德（Schmid）、J. 韦贝克（Verbeek）和 D. 拉卢斯（Larlus），《为现实世界应用学习颜色名称》，《电气与电子工程师协会图像处理汇刊》（IEEE Trans. Image Processing），第 18 卷，第 7 期，第 1512 - 1523 页，2009 年 7 月。</div></div></div></div><div><br></div><div><div><div>[32] V. Ferrari and A. Zisserman, "Learning Visual Attributes," Proc. Advances in Neural Information Processing Systems (NIPS), 2008.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[32] V. 法拉利（Ferrari）和 A. 齐斯曼（Zisserman），《学习视觉属性》，《神经信息处理系统进展会议论文集》（Proc. Advances in Neural Information Processing Systems (NIPS)），2008 年。</div></div></div></div><div><br></div><div><div><div>[33] A.F. Smeaton, P. Over, and W. Kraaij, "Evaluation Campaigns and TRECVid," Proc. Eighth ACM Int'l Workshop Multimedia Information Retrieval, 2006.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[33] A.F. 斯米顿（Smeaton）、P. 奥弗（Over）和 W. 克拉伊杰（Kraaij），《评估活动与视频检索评测（TRECVid）》，《第 8 届美国计算机协会国际多媒体信息检索研讨会论文集》（Proc. Eighth ACM Int'l Workshop Multimedia Information Retrieval），2006 年。</div></div></div></div><div><br></div><div><div><div>[34] N. Kumar, P.N. Belhumeur, and S.K. Nayar, "Facetracer: A Search Engine for Large Collections of Images with Faces," Proc. European Conf. Computer Vision (ECCV), 2008.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[34] N. 库马尔（Kumar）、P.N. 贝尔休默尔（Belhumeur）和 S.K. 纳亚尔（Nayar），《面部追踪器：一种用于大规模人脸图像集合的搜索引擎》，《欧洲计算机视觉会议论文集》（Proc. European Conf. Computer Vision (ECCV)），2008 年。</div></div></div></div><div><br></div><div><div><div>[35] N. Kumar, A. Berg, P. Belhumeur, and S. Nayar, "Describable Visual Attributes for Face Verification and Image Search," IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 33, no. 10, pp. 1962-1977, Oct. 2011.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[35] N. 库马尔（N. Kumar）、A. 伯格（A. Berg）、P. 贝尔休默（P. Belhumeur）和 S. 纳亚尔（S. Nayar），《用于人脸验证和图像搜索的可描述视觉属性》，《电气与电子工程师协会模式分析与机器智能汇刊》，第 33 卷，第 10 期，第 1962 - 1977 页，2011 年 10 月。</div></div></div></div><div><br></div><div><div><div>[36] D. Parikh and K. Grauman, "Interactively Building a Discriminative Vocabulary of Nameable Attributes," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp. 1681-1688, 2011.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[36] D. 帕里克（D. Parikh）和 K. 格劳曼（K. Grauman），《交互式构建可命名属性的区分性词汇表》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（CVPR），第 1681 - 1688 页，2011 年。</div></div></div></div><div><br></div><div><div><div>[37] V. Sharmanska, N. Quadrianto, and C.H. Lampert, "Augmented Attribute Representations" Proc. European Conf. Computer Vision (ECCV), 2012.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[37] V. 沙曼斯卡（V. Sharmanska）、N. 夸德里安托（N. Quadrianto）和 C.H. 兰佩特（C.H. Lampert），《增强属性表示》，《欧洲计算机视觉会议论文集》（ECCV），2012 年。</div></div></div></div><div><br></div><div><div><div>[38] K. Yanai and K. Barnard, "Image Region Entropy: A Measure of 'Visualness' of Web Images Associated with One Concept," Proc. 13th Ann. ACM Int'l Conf. Multimedia, 2005.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[38] K. 矢内（K. Yanai）和 K. 巴纳德（K. Barnard），《图像区域熵：与一个概念相关的网络图像的“视觉性”度量》，《第 13 届年度美国计算机协会国际多媒体会议论文集》，2005 年。</div></div></div></div><div><br></div><div><div><div>[39] J. Wang, K. Markert, and M. Everingham, "Learning Models for Object Recognition from Natural Language Descriptions," Proc. British Machine Vision Conf. (BMVC), 2009.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[39] J. 王（J. Wang）、K. 马克特（K. Markert）和 M. 埃弗里ingham（M. Everingham），《从自然语言描述中学习目标识别模型》，《英国机器视觉会议论文集》（BMVC），2009 年。</div></div></div></div><div><br></div><div><div><div>[40] T.L. Berg, A.C. Berg, and J. Shih, "Automatic Attribute Discovery and Characterization from Noisy Web Images," Proc. European Conf. Computer Vision (ECCV), 2010.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[40] T.L. 伯格（T.L. Berg）、A.C. 伯格（A.C. Berg）和 J. 施（J. Shih），《从嘈杂的网络图像中自动发现和表征属性》，《欧洲计算机视觉会议论文集》（ECCV），2010 年。</div></div></div></div><div><br></div><div><div><div>[41] L.J. Li, H. Su, Y. Lim, and L. Fei-Fei, "Objects as Attributes for Scene Classification," Proc. First Int'l Workshop Parts and Attributes at European Conf. Computer Vision, 2010.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[41] L.J. 李（L.J. Li）、H. 苏（H. Su）、Y. 林（Y. Lim）和 L. 费菲（L. Fei - Fei），《将物体作为场景分类的属性》，《欧洲计算机视觉会议第一届国际部件与属性研讨会论文集》，2010 年。</div></div></div></div><div><br></div><div><div><div>[42] Y. Wang and G. Mori, "A Discriminative Latent Model of Object Classes and Attributes," Proc. European Conf. Computer Vision (ECCV), pp. 155-168, 2010.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[42] Y. 王（Y. Wang）和 G. 森（G. Mori），《目标类别和属性的区分性潜在模型》，《欧洲计算机视觉会议论文集》（ECCV），第 155 - 168 页，2010 年。</div></div></div></div><div><br></div><div><div><div>[43] X. Yu and Y. Aloimonos, "Attribute-Based Transfer Learning for Object Categorization with Zero/One Training Example," Proc. European Conf. Computer Vision (ECCV), pp. 127-140, 2010.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[43] X. 余（X. Yu）和 Y. 阿洛伊莫诺斯（Y. Aloimonos），《基于属性的零/单训练示例目标分类迁移学习》，《欧洲计算机视觉会议论文集》（ECCV），第 127 - 140 页，2010 年。</div></div></div></div><div><br></div><div><div><div>[44] W.J. Scheirer, N. Kumar, P.N. Belhumeur, and T.E. Boult, "Multi-Attribute Spaces: Calibration for Attribute Fusion and Similarity Search," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2012.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[44] W.J. 谢勒（W.J. Scheirer）、N. 库马尔（N. Kumar）、P.N. 贝尔休默（P.N. Belhumeur）和 T.E. 博尔特（T.E. Boult），《多属性空间：属性融合和相似性搜索的校准》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（CVPR），2012 年。</div></div></div></div><div><br></div><div><div><div>[45] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth, "Describing Objects by their Attributes," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2009.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[45] A. 法尔哈迪（A. Farhadi）、I. 恩德斯（I. Endres）、D. 霍耶姆（D. Hoiem）和 D. 福赛思（D. Forsyth），《通过属性描述物体》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（CVPR），2009 年。</div></div></div></div><div><br></div><div><div><div>[46] G. Patterson and J. Hays, "SUN Attribute Database: Discovering, Annotating, and Recognizing Scene Attributes," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2012.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[46] G. 帕特森（G. Patterson）和 J. 海斯（J. Hays），《SUN 属性数据库：发现、标注和识别场景属性》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（CVPR），2012 年。</div></div></div></div><div><br></div><div><div><div>[47] J. Liu, B. Kuipers, and S. Savarese, "Recognizing Human Actions by Attributes," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2011.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[47] J. 刘（J. Liu）、B. 库伊佩斯（B. Kuipers）和 S. 萨瓦雷塞（S. Savarese），《通过属性识别人类动作》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（CVPR），2011 年。</div></div></div></div><div><br></div><div><div><div>[48] R. Feris, B. Siddiquie, Y. Zhai, J. Petterson, L. Brown, and S. Pankanti, "Attribute-Based Vehicle Search in Crowded Surveillance Videos," Proc. ACM Int'l Conf. Multimedia Retrieval (ICMR), article 18, 2011.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[48] R. 费里斯（R. Feris）、B. 西迪基（B. Siddiquie）、Y. 翟（Y. Zhai）、J. 彼得森（J. Petterson）、L. 布朗（L. Brown）和 S. 潘坎蒂（S. Pankanti），《拥挤监控视频中基于属性的车辆搜索》，《美国计算机协会国际多媒体检索会议论文集》（ICMR），文章编号 18，2011 年。</div></div></div></div><div><br></div><div><div><div>[49] M. Rohrbach, M. Stark, G. Szarvas, I. Gurevych, and B. Schiele, "What Helps Where-and Why? Semantic Relatedness for Knowledge Transfer," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2010.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[49] M. 罗尔巴赫（M. Rohrbach）、M. 斯塔克（M. Stark）、G. 萨尔瓦斯（G. Szarvas）、I. 古雷维奇（I. Gurevych）和 B. 席勒（B. Schiele），《什么在何处有帮助以及为什么？知识转移的语义相关性》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（CVPR），2010 年。</div></div></div></div><div><br></div><div><div><div>[50] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C. Berg, and T.L. Berg, "Baby Talk: Understanding and Generating Simple Image Descriptions," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp. 1601-1608, 2011.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[50] G. 库尔卡尼（G. Kulkarni）、V. 普雷姆拉吉（V. Premraj）、S. 达尔（S. Dhar）、S. 李（S. Li）、Y. 崔（Y. Choi）、A.C. 伯格（A.C. Berg）和 T.L. 伯格（T.L. Berg），《婴儿语言：理解和生成简单图像描述》，《电气与电子工程师协会计算机视觉与模式识别会议论文集》（CVPR），第 1601 - 1608 页，2011 年。</div></div></div></div><div><br></div><div><div><div>[51] A. Kovashka, D. Parikh, and K. Grauman, "Whittlesearch: Image Search with Relative Attribute Feedback," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2012.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[51] A. 科瓦什卡（A. Kovashka）、D. 帕里克（D. Parikh）和 K. 格劳曼（K. Grauman），《细粒度搜索：基于相对属性反馈的图像搜索》（"Whittlesearch: Image Search with Relative Attribute Feedback"），《电气与电子工程师协会计算机视觉与模式识别会议论文集》（Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)），2012 年。</div></div></div></div><div><br></div><div><div><div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-effc7f91-5ffa-472e-b97d-79de8dd64d53" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-12="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div><paragraphpositioning data-position-12="0,0">[52] A. Parkash and D. Parikh, "Attributes for Classifier Feedback," Proc. European Conf. Computer Vision (ECCV), 2012.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[52] A. 帕卡什（A. Parkash）和 D. 帕里克（D. Parikh），《用于分类器反馈的属性》（"Attributes for Classifier Feedback"），《欧洲计算机视觉会议论文集》（Proc. European Conf. Computer Vision (ECCV)），2012 年。</div></paragraphpositioning></div></div></div><div><br></div><div><div><div>[53] D. Osherson, E.E. Smith, T.S. Myers, E. Shafir, and M. Stob, "Extrapolating Human Probability Judgment," Theory and Decision, vol. 36, no. 2, pp. 103-129, 1994.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[53] D. 奥舍尔森（D. Osherson）、E.E. 史密斯（E.E. Smith）、T.S. 迈尔斯（T.S. Myers）、E. 沙菲尔（E. Shafir）和 M. 斯托布（M. Stob），《推断人类概率判断》（"Extrapolating Human Probability Judgment"），《理论与决策》（Theory and Decision），第 36 卷，第 2 期，第 103 - 129 页，1994 年。</div></div></div></div><div><br></div><div><div><div>[54] S.A. Sloman, "Feature-Based Induction," Cognitive Psychology, vol. 25, pp. 231-280, 1993.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[54] S.A. 斯洛曼（S.A. Sloman），《基于特征的归纳》（"Feature-Based Induction"），《认知心理学》（Cognitive Psychology），第 25 卷，第 231 - 280 页，1993 年。</div></div></div></div><div><br></div><div><div><div>[55] T. Hansen, M. Olkkonen, S. Walter, and K.R. Gegenfurtner, "Memory Modulates Color Appearance," Nature Neuroscience, vol. 9, pp. 1367-1368, 2006.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[55] T. 汉森（T. Hansen）、M. 奥尔科宁（M. Olkkonen）、S. 沃尔特（S. Walter）和 K.R. 格根富特纳（K.R. Gegenfurtner），《记忆调节颜色外观》（"Memory Modulates Color Appearance"），《自然神经科学》（Nature Neuroscience），第 9 卷，第 1367 - 1368 页，2006 年。</div></div></div></div><div><br></div><div><div><div>[56] D.N. Osherson, J. Stern, O. Wilkie, M. Stob, and E.E. Smith, "Default Probability," Cognitive Science, vol. 15, no. 2, pp. 251-269, 1991.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[56] D.N. 奥舍尔森（D.N. Osherson）、J. 斯特恩（J. Stern）、O. 威尔基（O. Wilkie）、M. 斯托布（M. Stob）和 E.E. 史密斯（E.E. Smith），《默认概率》（"Default Probability"），《认知科学》（Cognitive Science），第 15 卷，第 2 期，第 251 - 269 页，1991 年。</div></div></div></div><div><br></div><div><div><div>[57] C. Kemp, J.B. Tenenbaum, T.L. Griffiths, T. Yamada, and N. Ueda, "Learning Systems of Concepts with an Infinite Relational Model," Proc. Nat'l Conf. Artificial Intelligence (AAAI), 2006.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[57] C. 肯普（C. Kemp）、J.B. 特南鲍姆（J.B. Tenenbaum）、T.L. 格里菲思（T.L. Griffiths）、T. 山田（T. Yamada）和 N. 上田（N. Ueda），《使用无限关系模型学习概念系统》（"Learning Systems of Concepts with an Infinite Relational Model"），《美国人工智能协会会议论文集》（Proc. Nat'l Conf. Artificial Intelligence (AAAI)），2006 年。</div></div></div></div><div><br></div><div><div><div>[58] K.E.A. van de Sande, T. Gevers, and C.G.M. Snoek, "Evaluation of Color Descriptors for Object and Scene Recognition," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2008.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[58] K.E.A. 范德桑德（K.E.A. van de Sande）、T. 赫弗斯（T. Gevers）和 C.G.M. 斯诺克（C.G.M. Snoek），《用于物体和场景识别的颜色描述符评估》（"Evaluation of Color Descriptors for Object and Scene Recognition"），《电气与电子工程师协会计算机视觉与模式识别会议论文集》（Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)），2008 年。</div></div></div></div><div><br></div><div><div><div>[59] A. Bosch, A. Zisserman, and X. Muñoz, "Representing Shape with a Spatial Pyramid Kernel," Proc. Int'l Conf. Content-Based Image and Video Retrieval (CIVR), 2007.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[59] A. 博施（A. Bosch）、A. 齐斯曼（A. Zisserman）和 X. 穆尼奥斯（X. Muñoz），《使用空间金字塔核表示形状》（"Representing Shape with a Spatial Pyramid Kernel"），《国际基于内容的图像和视频检索会议论文集》（Proc. Int'l Conf. Content-Based Image and Video Retrieval (CIVR)），2007 年。</div></div></div></div><div><br></div><div><div><div>[60] H. Bay, A. Ess, T. Tuytelaars, and L.J.V. Gool, "Speeded-Up Robust Features (SURF)," Computer Vision and Image Understanding, vol. 110, no. 3, pp. 346-359, 2008.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[60] H. 贝（H. Bay）、A. 埃斯（A. Ess）、T. 蒂耶特拉尔斯（T. Tuytelaars）和 L.J.V. 古尔（L.J.V. Gool），《加速稳健特征（SURF）》（"Speeded-Up Robust Features (SURF)"），《计算机视觉与图像理解》（Computer Vision and Image Understanding），第 110 卷，第 3 期，第 346 - 359 页，2008 年。</div></div></div></div><div><br></div><div><div><div>[61] E. Shechtman and M. Irani, "Matching Local Self-Similarities across Images and Videos," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2007.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[61] E. 谢赫特曼（E. Shechtman）和 M. 伊拉尼（M. Irani），《跨图像和视频匹配局部自相似性》（"Matching Local Self-Similarities across Images and Videos"），《电气与电子工程师协会计算机视觉与模式识别会议论文集》（Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)），2007 年。</div></div></div></div><div><br></div><div><div><div>[62] J. Xiao, J. Hays, K.A. Ehinger, A. Oliva, and A. Torralba, "SUN Database: Large-Scale Scene Recognition from Abbey to Zoo," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), pp. 3485-3492, 2010.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[62] J. 肖（J. Xiao）、J. 海斯（J. Hays）、K.A. 埃辛格（K.A. Ehinger）、A. 奥利瓦（A. Oliva）和 A. 托拉尔巴（A. Torralba），《SUN 数据库：从修道院到动物园的大规模场景识别》（"SUN Database: Large-Scale Scene Recognition from Abbey to Zoo"），《电气与电子工程师协会计算机视觉与模式识别会议论文集》（Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)），第 3485 - 3492 页，2010 年。</div></div></div></div><div><br></div><div><div><div>[63] J.C. Platt, "Probabilities for SV Machines," Advances in Large Margin Classifiers. MIT Press, 2000.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[63] J.C. 普拉特（J.C. Platt），《支持向量机的概率》（"Probabilities for SV Machines"），《大间隔分类器进展》（Advances in Large Margin Classifiers）。麻省理工学院出版社，2000 年。</div></div></div></div><div><br></div><div><div><div>[64] L. Torresani, M. Szummer, and A. Fitzgibbon, "Efficient Object Category Recognition Using Classemes," Proc. European Conf. Computer Vision (ECCV), pp. 776-789, Sept. 2010.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[64] L. 托雷萨尼（L. Torresani）、M. 苏默（M. Szummer）和 A. 菲茨吉本（A. Fitzgibbon），《使用类元进行高效物体类别识别》（"Efficient Object Category Recognition Using Classemes"），《欧洲计算机视觉会议论文集》（Proc. European Conf. Computer Vision (ECCV)），第 776 - 789 页，2010 年 9 月。</div></div></div></div><div><br></div><div><div><div>[65] K.D. Tang, M.F. Tappen, R. Sukthankar, and C.H. Lampert, "Optimizing One-Shot Recognition with Micro-Set Learning," Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2010.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[65] K.D. 唐（K.D. Tang）、M.F. 塔彭（M.F. Tappen）、R. 苏克坦卡尔（R. Sukthankar）和 C.H. 兰佩特（C.H. Lampert），《通过微集学习优化一次性识别》（"Optimizing One-Shot Recognition with Micro-Set Learning"），《电气与电子工程师协会计算机视觉与模式识别会议论文集》（Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)），2010 年。</div></div></div></div><div><br></div><div><div><div>[66] W.J. Scheirer, A. Rocha, A. Sapkota, and T.E. Boult, "Toward Open Set Recognition," IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 35, no. 7, pp. 1757-1772, July 2013.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[66] W.J. 谢勒（W.J. Scheirer）、A. 罗查（A. Rocha）、A. 萨普科塔（A. Sapkota）和 T.E. 博尔特（T.E. Boult），《迈向开放集识别》（"Toward Open Set Recognition"），《电气与电子工程师协会模式分析与机器智能汇刊》（IEEE Trans. Pattern Analysis and Machine Intelligence），第 35 卷，第 7 期，第 1757 - 1772 页，2013 年 7 月。</div></div></div></div><div><br></div><div><div><div>[67] T. Tommasi, N. Quadrianto, B. Caputo, and C.H. Lampert, "Beyond Data Set Bias: Multi-Task Unaligned Shared Knowledge Transfer," Proc. Asian Conf. Computer Vision (ACCV), 2012.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">[67] T. 托马西（T. Tommasi）、N. 夸德里安托（N. Quadrianto）、B. 卡普托（B. Caputo）和 C.H. 兰佩特（C.H. Lampert），《超越数据集偏差：多任务未对齐共享知识迁移》，《亚洲计算机视觉会议论文集》（ACCV），2012 年。</div></div></div></div><div><br><!-- Media --><br></div><div style="position: relative; display: block;"><img draggable="false" src="https://cdn.noedgeai.com/0195d2b2-ac8c-7630-a76c-75ecc68f9592_12.jpg?x=70&amp;y=1431&amp;w=234&amp;h=278&amp;r=0" alt="" style="display: block;"></div><div><br><!-- Media --><br></div><div><div><div>Christoph H. Lampert received the PhD degree in mathematics from the University of Bonn in 2003. He was a senior researcher at the German Research Center for Artificial Intelligence in Kaiserslautern and a senior research scientist at the Max Planck Institute for Biological Cybernetics in Tübingen. He is currently an assistant professor at the Institute of Science and Technology Austria, where he heads a research group for computer vision and machine learning. He has received several international and national awards for his research, including the Best Paper Prize of CVPR 2008 and Best Student Paper Award of ECCV 2008. In 2012, he was awarded an ERC Starting Grant by the European Research Council. He is an associate editor of the IEEE Transactions on Pattern Analysis and Machine Intelligence and an action editor for the Journal of Machine Learning Research.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">克里斯托夫·H·兰佩特（Christoph H. Lampert）于 2003 年在波恩大学（University of Bonn）获得数学博士学位。他曾是凯泽斯劳滕德国人工智能研究中心的高级研究员，以及图宾根马克斯·普朗克生物控制论研究所的高级研究科学家。他目前是奥地利科学技术研究所的助理教授，领导着一个计算机视觉和机器学习研究小组。他的研究获得了多项国际和国内奖项，包括 2008 年计算机视觉与模式识别会议（CVPR）最佳论文奖和 2008 年欧洲计算机视觉会议（ECCV）最佳学生论文奖。2012 年，他获得了欧洲研究理事会的欧洲研究理事会启动基金。他是《电气与电子工程师协会模式分析与机器智能汇刊》（IEEE Transactions on Pattern Analysis and Machine Intelligence）的副主编，以及《机器学习研究杂志》（Journal of Machine Learning Research）的行动编辑。</div></div></div></div><div><br><!-- Media --><br></div><div style="position: relative; display: block;"><img draggable="false" src="https://cdn.noedgeai.com/0195d2b2-ac8c-7630-a76c-75ecc68f9592_12.jpg?x=866&amp;y=132&amp;w=226&amp;h=274&amp;r=0" alt="" style="display: block;"></div><div><br><!-- Media --><br></div><div><div><div>Hannes Nickisch received degrees from the Université de Nantes, France, in 2004, and the Technical University Berlin, Germany, in 2006. During his PhD work at the Max Planck Institute for Biological Cybernetics, Tübingen, Germany, he worked on large-scale approximate Bayesian inference and magnetic resonance image reconstruction. Since 2011, he has been with Philips Research, Hamburg, Germany. His research interests include medical image processing, machine learning, and biophysical modeling.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">汉内斯·尼克施（Hannes Nickisch）于 2004 年在法国南特大学（Université de Nantes）获得学位，并于 2006 年在德国柏林工业大学（Technical University Berlin）获得学位。在德国图宾根马克斯·普朗克生物控制论研究所攻读博士期间，他致力于大规模近似贝叶斯推理和磁共振图像重建的研究。自 2011 年以来，他一直在德国汉堡的飞利浦研究中心工作。他的研究兴趣包括医学图像处理、机器学习和生物物理建模。</div></div></div></div><div><br><!-- Media --><br></div><div style="position: relative; display: block;"><img draggable="false" src="https://cdn.noedgeai.com/0195d2b2-ac8c-7630-a76c-75ecc68f9592_12.jpg?x=866&amp;y=454&amp;w=224&amp;h=280&amp;r=0" alt="" style="display: block;"></div><div><br><!-- Media --><br></div><div><div><div>Stefan Harmeling studied mathematics and logic at the University of Münster (Dipl Math 1998) and computer science with an emphasis on artificial intelligence at Stanford University (MSc 2000). During his doctoral studies, he was a member of Prof. Klaus-Robert Müller's research group at the Fraunhofer Institute FIRST (Dr rer nat 2004). Thereafter, he was a Marie Curie fellow at the University of Edinburgh from 2005 to 2007, before joining the Max Planck Institute for Biological Cybernetics/Intelligent Systems. He is currently a senior research scientist in Prof. Bernhard Schölkopf's Department of Empirical Inference at the Max Planck Institute for Intelligent Systems (formerly Biological Cybernetics). His research interests include machine learning, image processing, computational photography, and probabilistic inference. In 2011, he received the DAGM Paper Prize, and in 2012 the Günter Petzow Prize for outstanding work at the Max Planck Institute for Intelligent Systems.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">斯特凡·哈梅林（Stefan Harmeling）在明斯特大学（University of Münster）学习数学和逻辑学（1998 年获得数学文凭），并在斯坦福大学（Stanford University）重点学习计算机科学中的人工智能（2000 年获得理学硕士学位）。在攻读博士学位期间，他是弗劳恩霍夫 FIRST 研究所克劳斯 - 罗伯特·米勒（Klaus - Robert Müller）教授研究小组的成员（2004 年获得自然科学博士学位）。此后，他于 2005 年至 2007 年在爱丁堡大学担任玛丽·居里研究员，之后加入马克斯·普朗克生物控制论/智能系统研究所。他目前是马克斯·普朗克智能系统研究所（前身为生物控制论研究所）伯恩哈德·朔尔科普夫（Bernhard Schölkopf）教授实证推理系的高级研究科学家。他的研究兴趣包括机器学习、图像处理、计算摄影和概率推理。2011 年，他获得了德国模式识别协会（DAGM）论文奖，2012 年获得了马克斯·普朗克智能系统研究所的京特·佩措夫奖（Günter Petzow Prize），以表彰他的杰出工作。</div></div></div></div><div><br></div><div><div><div><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3863" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22B3 TEX-A"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⊳</mo></math></mjx-assistive-mml></mjx-container> For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3864" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22B3 TEX-A"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⊳</mo></math></mjx-assistive-mml></mjx-container> 有关此或任何其他计算主题的更多信息，请访问我们的数字图书馆：www.computer.org/publications/dlib。</div></div></div></div><div><div><div></div></div></div></span></div></div></div></div></div></div>
      </body>
    </html>
  