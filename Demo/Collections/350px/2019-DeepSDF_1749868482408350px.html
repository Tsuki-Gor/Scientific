
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px 350px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <h1>DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>DeepSDF：学习用于形状表示的连续有符号距离函数</h1></div><p>Jeong Joon Park \({}^{1,3}\) Peter Florence \({}^{2,3}\) Julian Straub \({}^{3}\) Richard Newcombe \({}^{3}\) Steven Lovegrove \({}^{3}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>郑俊博 \({}^{1,3}\) 彼得·弗洛伦斯 \({}^{2,3}\) 朱利安·斯特劳布 \({}^{3}\) 理查德·纽科姆 \({}^{3}\) 史蒂文·洛夫格罗夫 \({}^{3}\)</p></div><p>\({}^{1}\) University of Washington \({}^{2}\) Massachusetts Institute of Technology \({}^{3}\) Facebook Reality Labs</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{1}\) 华盛顿大学 \({}^{2}\) 麻省理工学院 \({}^{3}\) Facebook现实实验室</p></div><!-- Media --><img src="https://cdn.noedgeai.com/bo_d164utbef24c73d1llu0_0.jpg?x=137&#x26;y=550&#x26;w=1471&#x26;h=484&#x26;r=0"><p>Figure 1: DeepSDF represents signed distance functions (SDFs) of shapes via latent code-conditioned feed-forward decoder networks. Above images are raycast renderings of DeepSDF interpolating between two shapes in the learned shape latent space. Best viewed digitally.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图1：DeepSDF通过潜在代码条件的前馈解码器网络表示形状的有符号距离函数（SDF）。上面的图像是DeepSDF在学习的形状潜在空间中插值两个形状的光线投射渲染。最佳数字查看。</p></div><!-- Media --><h2>Abstract</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>摘要</h2></div><p>Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing \({3D}\) geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes' interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>计算机图形学、3D计算机视觉和机器人社区提出了多种表示\({3D}\)几何体以进行渲染和重建的方法。这些方法在保真度、效率和压缩能力之间提供了权衡。在本工作中，我们介绍了DeepSDF，一种学习的连续有符号距离函数（SDF）表示，能够高质量地表示形状、插值和从部分和噪声3D输入数据中完成。DeepSDF与其经典对应物一样，通过连续的体积场表示形状的表面：场中某点的大小表示到表面边界的距离，符号指示该区域是位于形状内部（-）还是外部（+），因此我们的表示隐式地将形状的边界编码为学习函数的零水平集，同时显式地表示空间的分类是形状内部的一部分还是不是。虽然经典的SDF无论是解析形式还是离散体素形式通常表示单个形状的表面，但DeepSDF可以表示整个形状类。此外，我们展示了在学习的3D形状表示和完成方面的最先进性能，同时与之前的工作相比将模型大小减少了一个数量级。</p></div><h2>1. Introduction</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>1. 引言</h2></div><p>Deep convolutional networks which are a mainstay of image-based approaches grow quickly in space and time complexity when directly generalized to the 3rd spatial dimension, and more classical and compact surface representations such as triangle or quad meshes pose problems in training since we may need to deal with an unknown number of vertices and arbitrary topology. These challenges have limited the quality, flexibility and fidelity of deep learning approaches when attempting to either input \(3\mathrm{D}\) data for processing or produce \(3\mathrm{D}\) inferences for object segmentation and reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>深度卷积网络是基于图像的方法的主流，当直接推广到第三个空间维度时，空间和时间复杂度迅速增长，而更经典和紧凑的表面表示，如三角形或四边形网格，在训练中会遇到问题，因为我们可能需要处理未知数量的顶点和任意拓扑。这些挑战限制了深度学习方法在尝试输入\(3\mathrm{D}\)数据进行处理或生成\(3\mathrm{D}\)推断以进行对象分割和重建时的质量、灵活性和保真度。</p></div><p>In this work, we present a novel representation and approach for generative \(3\mathrm{D}\) modeling that is efficient,expressive, and fully continuous. Our approach uses the concept of a SDF, but unlike common surface reconstruction techniques which discretize this SDF into a regular grid for evaluation and measurement denoising [14], we instead learn a generative model to produce such a continuous field.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本工作中，我们提出了一种新颖的生成\(3\mathrm{D}\)建模表示和方法，该方法高效、富有表现力且完全连续。我们的方法使用SDF的概念，但与常见的表面重建技术不同，这些技术将SDF离散化为规则网格以进行评估和测量去噪[14]，我们则学习一个生成模型来产生这样的连续场。</p></div><p>The proposed continuous representation may be intuitively understood as a learned shape-conditioned classifier for which the decision boundary is the surface of the shape itself, as shown in Fig. 2. Our approach shares the generative aspect of other works seeking to map a latent space to a distribution of complex shapes in 3D [52], but critically differs in the central representation. While the notion of an implicit surface defined as a SDF is widely known in the computer vision and graphics communities, to our knowledge no prior works have attempted to directly learn continuous, generalizable 3D generative models of SDFs.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>所提出的连续表示可以直观地理解为一个学习的形状条件分类器，其决策边界就是形状本身的表面，如图2所示。我们的方法与其他寻求将潜在空间映射到3D中复杂形状分布的工作共享生成方面[52]，但在中心表示上有重要区别。虽然在计算机视觉和图形学社区中，作为SDF定义的隐式表面的概念广为人知，但据我们所知，之前没有工作尝试直接学习连续的、可推广的3D生成模型的SDF。</p></div><hr>
<!-- Footnote --><p>Work performed while Park and Florence were interns at Facebook.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在帕克和弗洛伦斯担任Facebook实习生期间进行的工作。</p></div><!-- Footnote -->
<hr><!-- Media --><!-- figureText: Decision boundary 0 \( {SDF} > 0 \) \( {SD}{F}^{\prime } < 0 \) (c) --><img src="https://cdn.noedgeai.com/bo_d164utbef24c73d1llu0_1.jpg?x=221&#x26;y=217&#x26;w=552&#x26;h=366&#x26;r=0"><p>Figure 2: Our DeepSDF representation applied to the Stanford Bunny: (a) depiction of the underlying implicit surface \({SDF} = 0\) trained on sampled points inside \({SDF} &#x3C; 0\) and outside \({SDF} > 0\) the surface, (b) 2D cross-section of the signed distance field, (c) rendered 3D surface recovered from \({SDF} = 0\) . Note that (b) and (c) are recovered via DeepSDF.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2：我们的DeepSDF表示应用于斯坦福兔子：（a）在表面内\({SDF} &#x3C; 0\)和外部\({SDF} > 0\)采样点上训练的基础隐式表面的描绘，（b）有符号距离场的2D横截面，（c）从\({SDF} = 0\)恢复的渲染3D表面。请注意，（b）和（c）是通过DeepSDF恢复的。</p></div><!-- Media --><p>Our contributions include: (i) the formulation of generative shape-conditioned 3D modeling with a continuous implicit surface, (ii) a learning method for 3D shapes based on a probabilistic auto-decoder, and (iii) the demonstration and application of this formulation to shape modeling and completion. Our models produce high quality continuous surfaces with complex topologies, and obtain state-of-the-art results in quantitative comparisons for shape reconstruction and completion. As an example of the effectiveness of our method,our models use only \({7.4}\mathrm{{MB}}\) (megabytes) of memory to represent entire classes of shapes (for example, thousands of 3D chair models) - this is, for example, less than half the memory footprint ( \({16.8}\mathrm{{MB}}\) ) of a single uncompressed \({512}^{3}3\mathrm{D}\) bitmap.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的贡献包括：（i）基于连续隐式表面的生成形状条件3D建模的公式化，（ii）基于概率自编码器的3D形状学习方法，以及（iii）该公式在形状建模和补全中的演示和应用。我们的模型生成高质量的连续表面，具有复杂的拓扑结构，并在形状重建和补全的定量比较中获得了最先进的结果。作为我们方法有效性的一个例子，我们的模型仅使用 \({7.4}\mathrm{{MB}}\)（兆字节）的内存来表示整个形状类别（例如，数千个3D椅子模型）——这例如少于单个未压缩 \({512}^{3}3\mathrm{D}\) 位图的内存占用（\({16.8}\mathrm{{MB}}\)）。</p></div><h2>2. Related Work</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>2. 相关工作</h2></div><p>We review three main areas of related work: \(3\mathrm{D}\) representations for shape learning (Sec. 2.1), techniques for learning generative models (Sec. 2.2), and shape completion (Sec. 2.3).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们回顾了三个主要的相关工作领域：\(3\mathrm{D}\) 形状学习的表示（第2.1节），生成模型学习的技术（第2.2节），以及形状补全（第2.3节）。</p></div><h3>2.1. Representations for 3D Shape Learning</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>2.1. 3D形状学习的表示</h3></div><p>Representations for data-driven 3D learning approaches can be largely classified into three categories: point-based, mesh-based, and voxel-based methods. While some applications such as 3D-point-cloud-based object classification are well suited to these representations, we address their limitations in expressing continuous surfaces with complex topologies.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>数据驱动的3D学习方法的表示大致可以分为三类：基于点的、基于网格的和基于体素的方法。虽然一些应用（如基于3D点云的物体分类）非常适合这些表示，但我们讨论了它们在表达具有复杂拓扑的连续表面方面的局限性。</p></div><p>Point-based. A point cloud is a lightweight 3D representation that closely matches the raw data that many sensors (i.e. LiDARs, depth cameras) provide, and hence is a natural fit for applying 3D learning. PointNet [36], for example, uses max-pool operations to extract global shape features, and the technique is widely used as an encoder for point generation networks \(\left\lbrack  {{55},1}\right\rbrack\) . There is a sizable list of related works for learning on point clouds [37, 51, 56]. A primary limitation, however, of learning with point clouds is that they do not describe topology and are not suitable for producing watertight surfaces.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于点的。点云是一种轻量级的3D表示，紧密匹配许多传感器（即激光雷达、深度相机）提供的原始数据，因此非常适合应用3D学习。例如，PointNet [36] 使用最大池化操作提取全局形状特征，该技术被广泛用作点生成网络的编码器 \(\left\lbrack  {{55},1}\right\rbrack\)。关于点云学习的相关工作有相当多的列表 [37, 51, 56]。然而，使用点云学习的一个主要局限性是它们不描述拓扑，且不适合生成密闭表面。</p></div><p>Mesh-based. Various approaches represent classes of similarly shaped objects, such as morphable human body parts, with predefined template meshes and some of these models demonstrate high fidelity shape generation results [2, 31]. Other recent works [3] use poly-cube mapping [48] for shape optimization. While the use of template meshes is convenient and naturally provides \(3\mathrm{D}\) correspondences,it can only model shapes with fixed mesh topology.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于网格的。各种方法使用预定义的模板网格表示形状相似的物体类别，例如可变形的人体部位，其中一些模型展示了高保真度的形状生成结果 [2, 31]。其他最近的工作 [3] 使用多立方体映射 [48] 进行形状优化。虽然使用模板网格方便，并自然提供 \(3\mathrm{D}\) 对应关系，但它只能建模具有固定网格拓扑的形状。</p></div><p>Other mesh-based methods use existing \(\left\lbrack  {{45},{33}}\right\rbrack\) or learned \(\left\lbrack  {{19},{20}}\right\rbrack\) parameterization techniques to describe \(3\mathrm{D}\) surfaces by morphing 2D planes. The quality of such representations depends on parameterization algorithms that are often sensitive to input mesh quality and cutting strategies. To address this, recent data-driven approaches [55, 19] learn the parameterization task with deep networks. They report, however, that (a) multiple planes are required to describe complex topologies but (b) the generated surface patches are not stitched, i.e. the produced shape is not closed. To generate a closed mesh, sphere parameterization may be used \(\left\lbrack  {{19},{20}}\right\rbrack\) ,but the resulting shape is limited to the topological sphere. Other works related to learning on meshes propose to use new convolution and pooling operations for meshes [16, 50] or general graphs [8].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其他基于网格的方法使用现有的 \(\left\lbrack  {{45},{33}}\right\rbrack\) 或学习的 \(\left\lbrack  {{19},{20}}\right\rbrack\) 参数化技术，通过变形2D平面来描述 \(3\mathrm{D}\) 表面。这种表示的质量依赖于参数化算法，而这些算法通常对输入网格质量和切割策略敏感。为了解决这个问题，最近的数据驱动方法 [55, 19] 使用深度网络学习参数化任务。然而，他们报告称：（a）描述复杂拓扑需要多个平面，但（b）生成的表面补丁没有拼接，即生成的形状不是闭合的。为了生成闭合网格，可以使用球面参数化 \(\left\lbrack  {{19},{20}}\right\rbrack\)，但生成的形状仅限于拓扑球。与网格学习相关的其他工作建议使用新的卷积和池化操作来处理网格 [16, 50] 或一般图 [8]。</p></div><p>Voxel-based. Voxels, which non-parametrically describe volumes with 3D grids of values, are perhaps the most natural extension into the \(3\mathrm{D}\) domain of the well-known learning paradigms (i.e., convolution) that have excelled in the 2D image domain. The most straightforward variant of voxel-based learning is to use a dense occupancy grid (occupied / not occupied). Due to the cubically growing compute and memory requirements, however, current methods are only able to handle low resolutions \(\left( {128}^{3}\right.\) or below). As such, voxel-based approaches do not preserve fine shape details \(\left\lbrack  {{54},{13}}\right\rbrack\) ,and additionally voxels visually appear significantly different than high-fidelity shapes, since when rendered their normals are not smooth. Octree-based methods \(\left\lbrack  {{49},{41},{22}}\right\rbrack\) alleviate the compute and memory limitations of dense voxel methods, extending for example the ability to learn at up to \({512}^{3}\) resolution [49],but even this resolution is far from producing shapes that are visually compelling.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于体素的。体素以非参数方式描述具有3D值网格的体积，可能是众所周知的学习范式（即卷积）在2D图像领域的最自然扩展。基于体素学习的最直接变体是使用密集占用网格（占用/未占用）。然而，由于计算和内存需求呈立方增长，目前的方法只能处理低分辨率\(\left( {128}^{3}\right.\)或更低。因此，基于体素的方法无法保留细微的形状细节\(\left\lbrack  {{54},{13}}\right\rbrack\)，而且体素在视觉上与高保真形状显著不同，因为在渲染时它们的法线并不平滑。基于八叉树的方法\(\left\lbrack  {{49},{41},{22}}\right\rbrack\)缓解了密集体素方法的计算和内存限制，例如扩展到高达\({512}^{3}\)的学习能力[49]，但即使这个分辨率也远未产生视觉上引人注目的形状。</p></div><p>Aside from occupancy grids, and more closely related to our approach, it is also possible to use a 3D grid of vox-els to represent a signed distance function. This inherits from the success of fusion approaches that use a truncated SDF (TSDF), pioneered in [14, 35], to combine noisy depth maps into a single 3D model. Voxel-based SDF representations have been extensively used for 3D shape learning \(\left\lbrack  {{57},{15},{46}}\right\rbrack\) ,but their use of discrete voxels is expensive in memory. As a result, these approaches generally present low resolution shapes. Wavelet transform-based methods [27] and dimensionality reduction techniques [27] for distance fields were reported, but they encode the SDF volume of each individual scene rather than a dataset of shapes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>除了占用网格，更与我们的方法密切相关的是，也可以使用3D体素网格来表示有符号距离函数。这继承了使用截断SDF（TSDF）的融合方法的成功，该方法在[14, 35]中开创，旨在将嘈杂的深度图合并为单个3D模型。基于体素的SDF表示已广泛用于3D形状学习\(\left\lbrack  {{57},{15},{46}}\right\rbrack\)，但它们使用离散体素在内存中是昂贵的。因此，这些方法通常呈现低分辨率形状。基于小波变换的方法[27]和距离场的降维技术[27]被报道，但它们编码的是每个单独场景的SDF体积，而不是形状数据集。</p></div><p>Recently, concurrently to our work, binary implicit surface representations were used by \(\left\lbrack  {{12},{34}}\right\rbrack\) ,where they train deep networks across classes of shapes to classify \(3\mathrm{D}\) points as inside or outside of a shape. Note that the binary occupancy function is a special case of SDF, considering only 'sign' of SDF values. As DeepSDF models metric signed distance to the surface, it can be used to raycast against surfaces and compute surface normals with its gradients.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最近，与我们的工作同时，二进制隐式表面表示被\(\left\lbrack  {{12},{34}}\right\rbrack\)使用，他们训练深度网络对形状类别进行分类，将\(3\mathrm{D}\)点标记为形状内部或外部。请注意，二进制占用函数是SDF的特例，仅考虑SDF值的“符号”。由于DeepSDF模型度量到表面的有符号距离，它可以用于射线投射到表面并使用其梯度计算表面法线。</p></div><h3>2.2. Representation Learning Techniques</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>2.2. 表示学习技术</h3></div><p>Modern representation learning techniques aim at automatically discovering a set of features that compactly but expressively describe data. For a more extensive review of the field, we refer to Bengio et al. [4].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>现代表示学习技术旨在自动发现一组紧凑但富有表现力的数据描述特征。有关该领域的更广泛回顾，请参见Bengio等人[4]。</p></div><p>Generative Adversial Networks. GANs [18] and their variants \(\left\lbrack  {{11},{39},{26},{28}}\right\rbrack\) learn deep embeddings of target data, from which realistic images are sampled, by training discriminators adversarially against generators. In the 3D domain, Wu et al. [52] trains a GAN to generate objects in a voxel form, while Hamu et al. [20] uses multiple parameterization planes to generate shapes of topological spheres. On the downside, training for GANs is known to be unstable.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>生成对抗网络。GANs [18]及其变体\(\left\lbrack  {{11},{39},{26},{28}}\right\rbrack\)通过对抗性训练鉴别器与生成器，学习目标数据的深度嵌入，从中采样出逼真的图像。在3D领域，Wu等人[52]训练GAN以生成体素形式的物体，而Hamu等人[20]使用多个参数化平面生成拓扑球体的形状。缺点是，GAN的训练被认为是不稳定的。</p></div><p>Auto-encoders. The ability of auto-encoders as a representation learning tool has been evidenced by the vast variety of 3D shape learning works in the literature \(\left\lbrack  {{15},{46},2,{19},{53}}\right\rbrack\) who adopt auto-encoders for representation learning. Recent \(3\mathrm{D}\) vision works \(\left\lbrack  {5,2,{31}}\right\rbrack\) often adopt a variational auto-encoder (VAE) learning scheme, in which bottleneck features are perturbed with Gaussian noise to encourage smooth and complete latent spaces. The regularization on the latent vectors enables exploring the embedding space with gradient descent or random sampling.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>自编码器。自编码器作为表示学习工具的能力已在文献中大量3D形状学习工作中得到证明\(\left\lbrack  {{15},{46},2,{19},{53}}\right\rbrack\)，这些工作采用自编码器进行表示学习。最近的\(3\mathrm{D}\)视觉工作\(\left\lbrack  {5,2,{31}}\right\rbrack\)通常采用变分自编码器（VAE）学习方案，其中瓶颈特征受到高斯噪声的扰动，以鼓励平滑和完整的潜在空间。对潜在向量的正则化使得能够通过梯度下降或随机采样探索嵌入空间。</p></div><p>Optimizing Latent Vectors. Instead of using the full auto-encoder, an alternative is to learn compact data representations by training decoder-only networks. This idea goes back to at least the work of Tan et al. [47] which simultaneously optimizes the latent vectors assigned to each data point and the decoder weights through back-propagation. For inference, an optimal latent vector is searched to match the new observation with fixed decoder parameters. Similar approaches have been extensively studied in \(\left\lbrack  {{40},7,{38}}\right\rbrack\) ,for applications including noise reduction, missing measurement completions, and fault detections. Recent approaches \(\left\lbrack  {6,{17}}\right\rbrack\) extend the technique by applying deep architectures. Throughout the paper we refer to this class of networks as auto-decoders, for they are trained with self-reconstruction loss on decoder-only architectures.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>优化潜在向量。与使用完整的自编码器相比，另一种选择是通过训练仅解码器网络来学习紧凑的数据表示。这一想法至少可以追溯到 Tan 等人的工作 [47]，该工作通过反向传播同时优化分配给每个数据点的潜在向量和解码器权重。对于推理，搜索一个最佳潜在向量以匹配具有固定解码器参数的新观察。类似的方法在 \(\left\lbrack  {{40},7,{38}}\right\rbrack\) 中得到了广泛研究，应用包括降噪、缺失测量补全和故障检测。最近的方法 \(\left\lbrack  {6,{17}}\right\rbrack\) 通过应用深度架构扩展了这一技术。在整篇论文中，我们将这一类网络称为自解码器，因为它们是在仅解码器架构上通过自重建损失进行训练的。</p></div><h3>2.3. Shape Completion</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>2.3. 形状补全</h3></div><p>\(3\mathrm{D}\) shape completion related works aim to infer unseen parts of the original shape given sparse or partial input observations. This task is analogous to image-inpainting in 2D computer vision.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\(3\mathrm{D}\) 形状补全相关工作旨在根据稀疏或部分输入观察推断原始形状的未见部分。这个任务类似于二维计算机视觉中的图像修复。</p></div><p>Classical surface reconstruction methods complete a point cloud into a dense surface by fitting radial basis function (RBF) [9] to approximate implicit surface functions, or by casting the reconstruction from oriented point clouds as a Poisson problem [29]. These methods only model a single shape rather than a dataset.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>经典的表面重建方法通过拟合径向基函数 (RBF) [9] 来将点云补全为密集表面，以近似隐式表面函数，或通过将从定向点云的重建视为泊松问题 [29]。这些方法仅建模单一形状，而不是数据集。</p></div><p>Various recent methods use data-driven approaches for the 3D completion task. Most of these methods adopt encoder-decoder architectures to reduce partial inputs of occupancy voxels [54], discrete SDF voxels [15], depth maps [42], RGB images [13, 53] or point clouds [46] into a latent vector and subsequently generate a prediction of full volumetric shape based on learned priors.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>各种最近的方法使用数据驱动的方法进行 3D 补全任务。这些方法中的大多数采用编码器-解码器架构，将占用体素的部分输入 [54]、离散 SDF 体素 [15]、深度图 [42]、RGB 图像 [13, 53] 或点云 [46] 转换为潜在向量，并随后基于学习的先验生成完整体积形状的预测。</p></div><h2>3. Modeling SDFs with Neural Networks</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>3. 用神经网络建模 SDF</h2></div><p>In this section we present DeepSDF, our continuous shape learning approach. We describe modeling shapes as the zero iso-surface decision boundaries of feed-forward networks trained to represent SDFs. A signed distance function is a continuous function that, for a given spatial point, outputs the point's distance to the closest surface, whose sign encodes whether the point is inside (negative) or outside (positive) of the watertight surface:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本节中，我们介绍 DeepSDF，我们的连续形状学习方法。我们描述将形状建模为训练以表示 SDF 的前馈网络的零等值面决策边界。带符号距离函数是一个连续函数，对于给定的空间点，输出该点到最近表面的距离，其符号编码该点是在水密表面内部（负）还是外部（正）：</p></div><p></p>\[{SDF}\left( \mathbf{x}\right)  = s : \mathbf{x} \in  {\mathbb{R}}^{3},s \in  \mathbb{R}. \tag{1}\]<p></p><p>The underlying surface is implicitly represented by the iso-surface of \({SDF}\left( \cdot \right)  = 0\) . A view of this implicit surface can be rendered through raycasting or rasterization of a mesh obtained with, for example, Marching Cubes [32].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基础表面通过 \({SDF}\left( \cdot \right)  = 0\) 的等值面隐式表示。可以通过光线投射或网格的光栅化来渲染该隐式表面的视图，例如，通过 Marching Cubes [32] 获得。</p></div><p>Our key idea is to directly regress the continuous SDF from point samples using deep neural networks. The resulting trained network is able to predict the SDF value of a given query position, from which we can extract the zero level-set surface by evaluating spatial samples. Such surface representation can be intuitively understood as a spatial classifier for which the decision boundary is the surface of the shape itself (Fig. 2). As a universal function approximator [24], deep feed-forward networks in theory can learn continuous SDFs with arbitrary precision. Yet, the precision of the approximation in practice is limited by the finite number of point samples that guide the decision boundaries and the finite capacity of the network due to restricted compute power.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的关键思想是直接使用深度神经网络从点样本回归连续 SDF。训练后的网络能够预测给定查询位置的 SDF 值，从中我们可以通过评估空间样本提取零水平集表面。这种表面表示可以直观地理解为空间分类器，其决策边界就是形状本身的表面（图 2）。作为一种通用函数逼近器 [24]，深度前馈网络理论上可以以任意精度学习连续 SDF。然而，实际中逼近的精度受到指导决策边界的有限点样本数量和由于计算能力受限而导致的网络有限容量的限制。</p></div><!-- Media --><!-- figureText: (x,y,z) SDF Code SDF (x,y,z) (b) Coded Shape DeepSDF (a) Single Shape DeepSDF --><img src="https://cdn.noedgeai.com/bo_d164utbef24c73d1llu0_3.jpg?x=137&#x26;y=201&#x26;w=713&#x26;h=193&#x26;r=0"><p>Figure 3: DeepSDF network outputs SDF value at a 3D query location. While (a) the network can memorize a single shape, (b) conditioning the network with a code vector allows DeepSDF to model a large space of shapes, where the shape information is contained in the code vector that is concatenated with the query point.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图 3：DeepSDF 网络在 3D 查询位置输出 SDF 值。虽然 (a) 网络可以记忆单一形状，但 (b) 用代码向量对网络进行条件化使 DeepSDF 能够建模大量形状，其中形状信息包含在与查询点连接的代码向量中。</p></div><!-- Media --><p>The most direct application of this approach is to train a single deep network for a given target shape as depicted in Fig. 3a. Given a target shape,we prepare a set of pairs \(X\) composed of 3D point samples and their SDF values:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这种方法的最直接应用是为给定目标形状训练一个单一的深度网络，如图 3a 所示。给定目标形状，我们准备一组由 3D 点样本及其 SDF 值组成的对 \(X\)：</p></div><p></p>\[X \mathrel{\text{:=}} \{ \left( {\mathbf{x},s}\right)  : {SDF}\left( \mathbf{x}\right)  = s\} . \tag{2}\]<p></p><p>We train the parameters \(\theta\) of a multi-layer fully-connected neural network \({f}_{\theta }\) on the training set \(S\) to make \({f}_{\theta }\) a good approximator of the given SDF in the target domain \(\Omega\) :</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们在训练集 \(S\) 上训练多层全连接神经网络 \({f}_{\theta }\) 的参数 \(\theta\)，使 \({f}_{\theta }\) 成为给定目标域 \(\Omega\) 中 SDF 的良好逼近器：</p></div><p></p>\[{f}_{\theta }\left( \mathbf{x}\right)  \approx  {SDF}\left( \mathbf{x}\right) ,\forall \mathbf{x} \in  \Omega . \tag{3}\]<p></p><p>The training is done by minimizing the sum over losses between the predicted and real SDF values of points in \(X\) under the following \({L}_{1}\) loss function:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>训练是通过最小化在以下 \({L}_{1}\) 损失函数下，预测的 SDF 值与 \(X\) 中点的真实 SDF 值之间的损失总和来完成的：</p></div><p></p>\[\mathcal{L}\left( {{f}_{\theta }\left( \mathbf{x}\right) ,s}\right)  = \left| {\operatorname{clamp}\left( {{f}_{\theta }\left( \mathbf{x}\right) ,\delta }\right)  - \operatorname{clamp}\left( {s,\delta }\right) }\right| , \tag{4}\]<p></p><p>where \(\operatorname{clamp}\left( {x,\delta }\right)  \mathrel{\text{:=}} \min \left( {\delta ,\max \left( {-\delta ,x}\right) }\right)\) introduces the parameter \(\delta\) to control the distance from the surface over which we expect to maintain a metric SDF. Larger values of \(\delta\) allow for fast ray-tracing since each sample gives information of safe step sizes [23]. Smaller \(\delta\) can be used to concentrate network capacity on details near the surface. We used \(\delta  = {0.1}\) in practice. (see supplementary for details)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(\operatorname{clamp}\left( {x,\delta }\right)  \mathrel{\text{:=}} \min \left( {\delta ,\max \left( {-\delta ,x}\right) }\right)\) 引入了参数 \(\delta\) 来控制我们期望保持度量 SDF 的表面距离。较大的 \(\delta\) 值允许快速光线追踪，因为每个样本提供安全步长的信息 [23]。较小的 \(\delta\) 可以用于将网络容量集中在靠近表面的细节上。我们在实践中使用了 \(\delta  = {0.1}\)。（详见补充材料）</p></div><p>Once trained, the surface is implicitly represented as the zero iso-surface of \({f}_{\theta }\left( \mathbf{x}\right)\) ,which can be visualized through raycasting or marching cubes. Another nice property of this approach is that accurate normals can be analytically computed by calculating the spatial derivative \(\partial {f}_{\theta }\left( \mathbf{x}\right) /\partial \mathbf{x}\) via back-propogation through the network.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>一旦训练完成，表面就隐式地表示为 \({f}_{\theta }\left( \mathbf{x}\right)\) 的零等值面，可以通过光线投射或 marching cubes 进行可视化。这种方法的另一个优点是，可以通过反向传播计算空间导数 \(\partial {f}_{\theta }\left( \mathbf{x}\right) /\partial \mathbf{x}\) 来精确计算法线。</p></div><h2>4. Learning the Latent Space of Shapes</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>4. 学习形状的潜在空间</h2></div><p>Training a specific neural network for each shape is neither feasible nor very useful. Instead, we want a model that can represent a wide variety of shapes, discover their common properties, and embed them in a low dimensional latent space. To this end,we introduce a latent vector \(\mathbf{z}\) ,which can be thought of as encoding the desired shape, as a second input to the neural network as depicted in Fig. 3b. Conceptually,we map this latent vector to a \(3\mathrm{D}\) shape represented by a continuous SDF. Formally, for some shape indexed by \(i,{f}_{\theta }\) is now a function of a latent code \({\mathbf{z}}_{i}\) and a query \(3\mathrm{D}\) location \(\mathbf{x}\) ,which outputs the shape’s approximate SDF:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为每个形状训练一个特定的神经网络既不可行也不太有用。相反，我们希望有一个模型能够表示各种形状，发现它们的共同属性，并将它们嵌入到低维潜在空间中。为此，我们引入了一个潜在向量 \(\mathbf{z}\)，可以被视为编码所需形状，作为神经网络的第二个输入，如图 3b 所示。从概念上讲，我们将这个潜在向量映射到由连续 SDF 表示的 \(3\mathrm{D}\) 形状。形式上，对于某个由 \(i,{f}_{\theta }\) 索引的形状，现在是潜在代码 \({\mathbf{z}}_{i}\) 和查询 \(3\mathrm{D}\) 位置 \(\mathbf{x}\) 的函数，输出形状的近似 SDF：</p></div><p></p>\[{f}_{\theta }\left( {{\mathbf{z}}_{i},\mathbf{x}}\right)  \approx  {SD}{F}^{i}\left( \mathbf{x}\right) . \tag{5}\]<p></p><!-- Media --><!-- figureText: Input Output Output Backprogate ... Codes (b) Auto-decoder Code (a) Auto-encoder --><img src="https://cdn.noedgeai.com/bo_d164utbef24c73d1llu0_3.jpg?x=936&#x26;y=199&#x26;w=636&#x26;h=277&#x26;r=0"><p>Figure 4: Different from an auto-encoder whose latent code is produced by the encoder, an auto-decoder directly accepts a latent vector as an input. A randomly initialized latent vector is assigned to each data point in the beginning of training, and the latent vectors are optimized along with the decoder weights through standard backpropagation. During inference, decoder weights are fixed, and an optimal latent vector is estimated.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图 4：不同于由编码器生成潜在代码的自编码器，自解码器直接接受潜在向量作为输入。在训练开始时，每个数据点分配一个随机初始化的潜在向量，并且潜在向量与解码器权重一起通过标准反向传播进行优化。在推理过程中，解码器权重是固定的，并估计一个最优的潜在向量。</p></div><!-- Media --><p>By conditioning the network output on a latent vector, this formulation allows modeling multiple SDFs with a single neural network. Given the decoding model \({f}_{\theta }\) ,the continuous surface associated with a latent vector \(\mathbf{z}\) is similarly represented with the zero iso-surface of \({f}_{\theta }\left( {\mathbf{z},\mathbf{x}}\right)\) ,and the shape can again be discretized for visualization by, for example, raycasting or Marching Cubes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>通过将网络输出条件化于潜在向量，这种公式允许使用单个神经网络建模多个 SDF。给定解码模型 \({f}_{\theta }\)，与潜在向量 \(\mathbf{z}\) 相关的连续表面同样表示为 \({f}_{\theta }\left( {\mathbf{z},\mathbf{x}}\right)\) 的零等值面，形状可以再次通过例如光线投射或 Marching Cubes 进行离散化以便可视化。</p></div><p>Throughout the paper we use the coded shape DeepSDF model of Fig. 3b whose decoder is a feed-forward network composed of eight fully connected layers, each of them applied with dropouts. All internal layers are 512-dimensional and have ReLU non-linearities. The output non-linearity regressing the SDF value is tanh. We found training with batch-normalization [25] to be unstable and applied the weight-normalization technique instead [43]. For training, we use the Adam optimizer [30].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在整篇论文中，我们使用图 3b 的编码形状 DeepSDF 模型，其解码器是一个由八个全连接层组成的前馈网络，每个层都应用了 dropout。所有内部层都是 512 维的，并具有 ReLU 非线性。回归 SDF 值的输出非线性是 tanh。我们发现使用批量归一化 [25] 进行训练不稳定，因此改用权重归一化技术 [43]。在训练中，我们使用 Adam 优化器 [30]。</p></div><p>In the next section we explain training the decoding model \({f}_{\theta }\left( {\mathbf{z},\mathbf{x}}\right)\) and introduce the ’auto-decoder’ formulation for encoder-less training of shape-coded DeepSDF.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在下一节中，我们解释解码模型 \({f}_{\theta }\left( {\mathbf{z},\mathbf{x}}\right)\) 的训练，并介绍无编码器的形状编码 DeepSDF 的“自解码器”公式。</p></div><h3>4.1. Motivating Encoder-less Learning</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.1. 激励无编码器学习</h3></div><p>Auto-encoders and encoder-decoder networks are widely used for representation learning as their bottleneck features tend to form natural latent variable representations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>自编码器和编码器-解码器网络广泛用于表示学习，因为它们的瓶颈特征往往形成自然的潜在变量表示。</p></div><p>Recently, in applications such as modeling depth maps [5], faces [2], and body shapes [31] a full encoder-decoder network is trained but only the decoder is retained for inference, where they search for an optimal latent vector given some input observation. Since the trained encoder is unused at test time, it is unclear whether (1) training encoder is an effective use of computational resources and (2) it is necessary for researchers to design encoders for various \(3\mathrm{D}\) input representations (e.g. points, meshes, octrees, etc).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最近，在深度图 [5]、面部 [2] 和身体形状 [31] 等建模应用中，训练了完整的编码器-解码器网络，但仅保留解码器用于推理，在此过程中它们根据某些输入观察搜索最优潜在向量。由于训练好的编码器在测试时未被使用，因此不清楚 (1) 训练编码器是否是计算资源的有效利用，以及 (2) 研究人员是否有必要为各种 \(3\mathrm{D}\) 输入表示（例如点、网格、八叉树等）设计编码器。</p></div><p>This motivates us to use an auto-decoder for learning a shape embedding without an encoder as depicted in Fig. 4b. We show that learning continuous SDFs with auto-decoder leads to high quality 3D generative models. Further, we develop a probabilistic formulation for training and testing the auto-decoder that naturally introduces latent space regularization for improved generalization. To the best of our knowledge, this work is the first to introduce the auto-decoder learning method to the 3D learning community.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这激励我们使用自动解码器来学习形状嵌入，而不使用编码器，如图4b所示。我们表明，使用自动解码器学习连续的SDF（有符号距离函数）可以产生高质量的3D生成模型。此外，我们开发了一种用于训练和测试自动解码器的概率公式，自然引入了潜在空间正则化，以改善泛化能力。根据我们所知，这项工作是首次将自动解码器学习方法引入3D学习领域。</p></div><h3>4.2. Auto-decoder-based DeepSDF Formulation</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.2. 基于自动解码器的DeepSDF公式</h3></div><p>To derive the auto-decoder-based shape-coded DeepSDF formulation we adopt a probabilistic perspective. Given a dataset of \(N\) shapes represented with signed distance function \({SD}{F}^{i}{}_{i = 1}^{N}\) ,we prepare a set of \(K\) point samples and their signed distance values:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了推导基于自动解码器的形状编码DeepSDF公式，我们采用概率视角。给定一个由\(N\)形状表示的有符号距离函数\({SD}{F}^{i}{}_{i = 1}^{N}\)的数据集，我们准备一组\(K\)点样本及其有符号距离值：</p></div><p></p>\[{X}_{i} = \left\{  {\left( {{\mathbf{x}}_{j},{s}_{j}}\right)  : {s}_{j} = {SD}{F}^{i}\left( {\mathbf{x}}_{j}\right) }\right\}  . \tag{6}\]<p></p><p>For an auto-decoder, as there is no encoder, each latent code \({\mathbf{z}}_{i}\) is paired with training shape \({X}_{i}\) . The posterior over shape code \({\mathbf{z}}_{i}\) given the shape SDF samples \({X}_{i}\) can be decomposed as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>对于自动解码器，由于没有编码器，每个潜在代码\({\mathbf{z}}_{i}\)与训练形状\({X}_{i}\)配对。给定形状SDF样本\({X}_{i}\)的形状代码\({\mathbf{z}}_{i}\)的后验可以分解为：</p></div><p></p>\[{p}_{\theta }\left( {{\mathbf{z}}_{i} \mid  {X}_{i}}\right)  = p\left( {\mathbf{z}}_{i}\right) \mathop{\prod }\limits_{{\left( {{\mathbf{x}}_{j},{\mathbf{s}}_{j}}\right)  \in  {X}_{i}}}{p}_{\theta }\left( {{\mathbf{s}}_{j} \mid  {z}_{i};{\mathbf{x}}_{j}}\right) , \tag{7}\]<p></p><p>where \(\theta\) parameterizes the SDF likelihood. In the latent shape-code space, we assume the prior distribution over codes \(p\left( {\mathbf{z}}_{\mathbf{i}}\right)\) to be a zero-mean multivariate-Gaussian with a spherical covariance \({\sigma }^{2}I\) . This prior encapsulates the notion that the shape codes should be concentrated and we empirically found it was needed to infer a compact shape manifold and to help converge to good solutions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(\theta\)参数化SDF的似然性。在潜在形状代码空间中，我们假设代码\(p\left( {\mathbf{z}}_{\mathbf{i}}\right)\)的先验分布为零均值多元高斯分布，具有球形协方差\({\sigma }^{2}I\)。这个先验概念包含了形状代码应集中在一起的想法，我们通过经验发现，这对于推断紧凑的形状流形和帮助收敛到良好的解决方案是必要的。</p></div><p>In the auto-decoder-based DeepSDF formulation we express the SDF likelihood via a deep feed-forward network \({f}_{\theta }\left( {{\mathbf{z}}_{i},{\mathbf{x}}_{j}}\right)\) and,without loss of generality,assume that the likelihood takes the form:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在基于自动解码器的DeepSDF公式中，我们通过深度前馈网络\({f}_{\theta }\left( {{\mathbf{z}}_{i},{\mathbf{x}}_{j}}\right)\)表达SDF的似然性，并且在不失一般性的情况下，假设似然性呈现以下形式：</p></div><p></p>\[{p}_{\theta }\left( {{\mathbf{s}}_{j} \mid  {z}_{i};{\mathbf{x}}_{j}}\right)  = \exp \left( {-\mathcal{L}\left( {{f}_{\theta }\left( {{\mathbf{z}}_{i},{\mathbf{x}}_{j}}\right) ,{s}_{j}}\right) }\right) . \tag{8}\]<p></p><p>The SDF prediction \({\widetilde{s}}_{j} = {f}_{\theta }\left( {{\mathbf{z}}_{i},{\mathbf{x}}_{j}}\right)\) is represented using a fully-connected network. \(\mathcal{L}\left( {{\widetilde{s}}_{j},{s}_{j}}\right)\) is a loss function penalizing the deviation of the network prediction from the actual SDF value \({s}_{j}\) . One example for the cost function is the standard \({L}_{2}\) loss function which amounts to assuming Gaussian noise on the SDF values. In practice we use the clamped \({L}_{1}\) cost from Eq. 4 for reasons outlined previously.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>SDF预测\({\widetilde{s}}_{j} = {f}_{\theta }\left( {{\mathbf{z}}_{i},{\mathbf{x}}_{j}}\right)\)使用全连接网络表示。\(\mathcal{L}\left( {{\widetilde{s}}_{j},{s}_{j}}\right)\)是一个损失函数，惩罚网络预测与实际SDF值\({s}_{j}\)之间的偏差。成本函数的一个例子是标准\({L}_{2}\)损失函数，这相当于假设SDF值上存在高斯噪声。在实践中，我们使用来自方程4的钳制\({L}_{1}\)成本，原因如前所述。</p></div><p>At training time we maximize the joint log posterior over all training shapes with respect to the individual shape codes \({\left\{  {z}_{i}\right\}  }_{i = 1}^{N}\) and the network parameters \(\theta\) :</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在训练时，我们最大化所有训练形状的联合对数后验，相对于各个形状代码\({\left\{  {z}_{i}\right\}  }_{i = 1}^{N}\)和网络参数\(\theta\)：</p></div><p></p>\[\underset{\theta ,{\left\{  {\mathbf{z}}_{i}\right\}  }_{i = 1}^{N}}{\arg \min }\mathop{\sum }\limits_{{i = 1}}^{N}\left( {\mathop{\sum }\limits_{{j = 1}}^{K}\mathcal{L}\left( {{f}_{\theta }\left( {{\mathbf{z}}_{i},{\mathbf{x}}_{j}}\right) ,{s}_{j}}\right)  + \frac{1}{{\sigma }^{2}}{\begin{Vmatrix}{\mathbf{z}}_{i}\end{Vmatrix}}_{2}^{2}}\right) . \tag{9}\]<p></p><!-- Media --><img src="https://cdn.noedgeai.com/bo_d164utbef24c73d1llu0_4.jpg?x=901&#x26;y=202&#x26;w=675&#x26;h=212&#x26;r=0"><p>Figure 5: Compared to car shapes memorized using OGN [49] (right), our models (left) preserve details and render visually pleasing results as DeepSDF provides oriented surace normals.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图5：与使用OGN [49]（右）记忆的汽车形状相比，我们的模型（左）保留了细节，并呈现出视觉上令人愉悦的结果，因为DeepSDF提供了定向表面法线。</p></div><!-- Media --><p>At inference time,after training and fixing \(\theta\) ,a shape code \({\mathbf{z}}_{i}\) for shape \({X}_{i}\) can be estimated via Maximum-a-Posterior (MAP) estimation as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在推理时，经过训练并固定\(\theta\)后，可以通过最大后验估计（MAP）估计形状\({X}_{i}\)的形状代码\({\mathbf{z}}_{i}\)：</p></div><p></p>\[\widehat{\mathbf{z}} = \underset{\mathbf{z}}{\arg \min }\mathop{\sum }\limits_{{\left( {{\mathbf{x}}_{j},{\mathbf{s}}_{j}}\right)  \in  X}}\mathcal{L}\left( {{f}_{\theta }\left( {\mathbf{z},{\mathbf{x}}_{j}}\right) ,{s}_{j}}\right)  + \frac{1}{{\sigma }^{2}}\parallel \mathbf{z}{\parallel }_{2}^{2}. \tag{10}\]<p></p><p>Crucially,this formulation is valid for SDF samples \(X\) of arbitrary size and distribution because the gradient of the loss with respect to \(z\) can be computed separately for each SDF sample. This implies that DeepSDF can handle any form of partial observations such as depth maps. This is a major advantage over the auto-encoder framework whose encoder expects a test input similar to the training data, e.g. shape completion networks of \(\left\lbrack  {{15},{56}}\right\rbrack\) require preparing training data of partial shapes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>关键是，这个公式对于任意大小和分布的SDF样本\(X\)都是有效的，因为损失相对于\(z\)的梯度可以为每个SDF样本单独计算。这意味着DeepSDF可以处理任何形式的部分观测，例如深度图。这是相对于自动编码器框架的一个主要优势，因为其编码器期望测试输入与训练数据相似，例如，形状补全网络\(\left\lbrack  {{15},{56}}\right\rbrack\)需要准备部分形状的训练数据。</p></div><p>To incorporate the latent shape code, we stack the code vector and the sample location as depicted in Fig. 3b and feed it into the same fully-connected NN described previously at the input layer and additionally at the 4th layer. We again use the Adam optimizer [30]. The latent vector \(\mathbf{z}\) is initialized randomly from \(\mathcal{N}\left( {0,{0.01}^{2}}\right)\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了结合潜在形状代码，我们将代码向量和样本位置堆叠，如图3b所示，并将其输入到前面描述的同一全连接神经网络的输入层，并额外在第4层。我们再次使用Adam优化器[30]。潜在向量\(\mathbf{z}\)是从\(\mathcal{N}\left( {0,{0.01}^{2}}\right)\)随机初始化的。</p></div><p>Note that while both VAE and the proposed auto-decoder formulation share the zero-mean Gaussian prior on the latent codes, we found that the the stochastic nature of the VAE optimization did not lead to good training results.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>请注意，尽管VAE和提出的自编码器公式在潜在编码上共享零均值高斯先验，但我们发现VAE优化的随机性质并未导致良好的训练结果。</p></div><h2>5. Data Preparation</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>5. 数据准备</h2></div><p>To train our continuous SDF model, we prepare the SDF samples \(X\) (Eq. 2) for each mesh,which consists of 3D points and their SDF values. While SDF can be computed through a distance transform for any watertight shapes from real or synthetic data, we train with synthetic objects, (e.g. ShapeNet [10]), for which we are provided complete 3D shape meshes. To prepare data, we start by normalizing each mesh to a unit sphere and sampling 500,000 spatial points \(x\) ’s: we sample more aggressively near the surface of the object as we want to capture a more detailed SDF near the surface. For an ideal oriented watertight mesh, computing the signed distance value of \(\mathbf{x}\) would only involve finding the closest triangle, but we find that human designed meshes are commonly not watertight and contain undesired internal structures. To obtain the shell of a mesh with proper orientation, we set up equally spaced virtual cameras around the object, and densely sample surface points,denoted \({P}_{s}\) ,with surface normals oriented towards the camera. Double sided triangles visible from both orientations (indicating that the shape is not closed) cause problems in this case, so we discard mesh objects containing too many of such faces. Then,for each \(x\) ,we find the closest point in \({P}_{s}\) ,from which the \({SDF}\left( \mathbf{x}\right)\) can be computed. We refer readers to supplementary material for further details.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了训练我们的连续SDF模型，我们为每个网格准备SDF样本\(X\)（公式2），这些样本由3D点及其SDF值组成。虽然可以通过距离变换计算任何密闭形状的SDF，无论是来自真实数据还是合成数据，但我们使用合成物体（例如ShapeNet [10]）进行训练，这些物体提供了完整的3D形状网格。为了准备数据，我们首先将每个网格归一化为单位球体，并采样500,000个空间点\(x\)：我们在物体表面附近进行更积极的采样，因为我们希望捕捉到表面附近更详细的SDF。对于理想的定向密闭网格，计算\(\mathbf{x}\)的有符号距离值只需找到最近的三角形，但我们发现人造网格通常不是密闭的，并且包含不必要的内部结构。为了获得具有适当方向的网格外壳，我们在物体周围设置等间距的虚拟相机，并密集采样表面点，记作\({P}_{s}\)，其表面法线朝向相机。从两个方向可见的双面三角形（表明形状不是封闭的）在这种情况下会造成问题，因此我们丢弃包含过多此类面的网格对象。然后，对于每个\(x\)，我们找到\({P}_{s}\)中最近的点，从中可以计算出\({SDF}\left( \mathbf{x}\right)\)。有关更多细节，请参阅补充材料。</p></div><!-- Media --><table><tbody><tr><td>Method</td><td>Type</td><td>Discretization</td><td>Complex topologies</td><td>Closed surfaces</td><td>Surface normals</td><td>Model size (GB)</td><td>Inf. time (s)</td><td>Eval. tasks</td></tr><tr><td>3D-EPN [15]</td><td>Voxel SDF</td><td>\( {32}^{3} \) voxels</td><td>✓</td><td>✓</td><td>✓</td><td>0.42</td><td>-</td><td>C</td></tr><tr><td>OGN [49]</td><td>Octree</td><td>\( {256}^{3} \) voxels</td><td>✓</td><td>✓</td><td></td><td>0.54</td><td>0.32</td><td>K</td></tr><tr><td>AtlasNet -Sphere [19]</td><td>Parametric mesh</td><td>1 patch</td><td></td><td>✓</td><td></td><td>0.015</td><td>0.01</td><td>K, U</td></tr><tr><td>AtlasNet -25 [19]</td><td>Parametric mesh</td><td>25 patches</td><td>✓</td><td></td><td></td><td>0.172</td><td>0.32</td><td>K, U</td></tr><tr><td>DeepSDF (ours)</td><td>Continuous SDF</td><td>none</td><td>✓</td><td>✓</td><td>✓</td><td>0.0074</td><td>9.72</td><td>K, U, C</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>方法</td><td>类型</td><td>离散化</td><td>复杂拓扑</td><td>封闭表面</td><td>表面法线</td><td>模型大小 (GB)</td><td>推理时间 (秒)</td><td>评估任务</td></tr><tr><td>3D-EPN [15]</td><td>体素 SDF</td><td>\( {32}^{3} \) 体素</td><td>✓</td><td>✓</td><td>✓</td><td>0.42</td><td>-</td><td>C</td></tr><tr><td>OGN [49]</td><td>八叉树</td><td>\( {256}^{3} \) 体素</td><td>✓</td><td>✓</td><td></td><td>0.54</td><td>0.32</td><td>K</td></tr><tr><td>AtlasNet -球体 [19]</td><td>参数化网格</td><td>1 个补丁</td><td></td><td>✓</td><td></td><td>0.015</td><td>0.01</td><td>K, U</td></tr><tr><td>AtlasNet -25 [19]</td><td>参数化网格</td><td>25 个补丁</td><td>✓</td><td></td><td></td><td>0.172</td><td>0.32</td><td>K, U</td></tr><tr><td>DeepSDF (我们的)</td><td>连续 SDF</td><td>无</td><td>✓</td><td>✓</td><td>✓</td><td>0.0074</td><td>9.72</td><td>K, U, C</td></tr></tbody></table></div><p>Table 1: Overview of the benchmarked methods. AtlasNet-Sphere can only describe topological-spheres, voxel/octree occupancy methods (i.e. OGN) only provide 8 directions for normals, and AtlasNet does not provide oriented normals. Our tasks evaluated are: (K) representing known shapes, (U) representing unknown shapes, and (C) shape completion.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表1：基准方法概述。AtlasNet-Sphere 只能描述拓扑球体，体素/八叉树占用方法（即 OGN）仅提供 8 个法线方向，而 AtlasNet 不提供定向法线。我们评估的任务包括：（K）表示已知形状，（U）表示未知形状，以及（C）形状补全。</p></div><!-- Media --><h2>6. Results</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>6. 结果</h2></div><p>We conduct a number of experiments to show the representational power of DeepSDF, both in terms of its ability to describe geometric details and its generalization capability to learn a desirable shape embedding space. Largely, we propose four main experiments designed to test its ability to 1) represent training data, 2) use learned feature representation to reconstruct unseen shapes, 3) apply shape priors to complete partial shapes, and 4) learn smooth and complete shape embedding space from which we can sample new shapes. For all experiments we use the popular ShapeNet [10] dataset.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们进行了一系列实验，以展示 DeepSDF 的表现力，包括其描述几何细节的能力和学习理想形状嵌入空间的泛化能力。总体而言，我们提出了四个主要实验，旨在测试其能力：1）表示训练数据，2）使用学习到的特征表示重建未见形状，3）应用形状先验以补全部分形状，以及 4）学习平滑且完整的形状嵌入空间，从中我们可以采样新形状。所有实验均使用流行的 ShapeNet [10] 数据集。</p></div><p>We select a representative set of \(3\mathrm{D}\) learning approaches to comparatively evaluate aforementioned criteria: a recent octree-based method (OGN) [49], a mesh-based method (AtlasNet) [19], and a volumetric SDF-based shape completion method (3D-EPN) [15] (Table 1). These works show state-of-the-art performance in their respective representations and tasks, so we omit comparisons with the works that have already been compared: e.g. OGN's octree model outperforms regular voxel approaches, while AtlasNet compares itself with various points, mesh, or voxel based methods and 3D-EPN with various completion methods.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们选择了一组具有代表性的 \(3\mathrm{D}\) 学习方法，以比较评估上述标准：一种最近的基于八叉树的方法（OGN） [49]，一种基于网格的方法（AtlasNet） [19]，以及一种基于体积 SDF 的形状补全方法（3D-EPN） [15]（表1）。这些工作在各自的表示和任务中显示出最先进的性能，因此我们省略了与已经比较过的工作的比较：例如，OGN 的八叉树模型优于常规体素方法，而 AtlasNet 与各种点、网格或体素方法进行比较，3D-EPN 则与各种补全方法进行比较。</p></div><h3>6.1. Representing Known 3D Shapes</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.1. 表示已知的 3D 形状</h3></div><p>First, we evaluate the capacity of the model to represent known shapes, i.e. shapes that were in the training set, from only a restricted-size latent code - testing the limit of expressive capability of the representations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>首先，我们评估模型表示已知形状的能力，即仅从有限大小的潜在编码中表示训练集中存在的形状——测试表示的表现能力的极限。</p></div><!-- Media --><table><tbody><tr><td>Method \\metric</td><td>CD, mean</td><td>CD, median</td><td>EMD, mean</td><td>EMD, median</td></tr><tr><td>OGN</td><td>0.167</td><td>0.127</td><td>0.043</td><td>0.042</td></tr><tr><td>AtlasNet-Sph.</td><td>0.210</td><td>0.185</td><td>0.046</td><td>0.045</td></tr><tr><td>AtlasNet-25</td><td>0.157</td><td>0.140</td><td>0.060</td><td>0.060</td></tr><tr><td>DeepSDF</td><td>0.084</td><td>0.058</td><td>0.043</td><td>0.042</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>方法 \\度量</td><td>CD，均值</td><td>CD，中位数</td><td>EMD，均值</td><td>EMD，中位数</td></tr><tr><td>OGN</td><td>0.167</td><td>0.127</td><td>0.043</td><td>0.042</td></tr><tr><td>AtlasNet-球面</td><td>0.210</td><td>0.185</td><td>0.046</td><td>0.045</td></tr><tr><td>AtlasNet-25</td><td>0.157</td><td>0.140</td><td>0.060</td><td>0.060</td></tr><tr><td>DeepSDF</td><td>0.084</td><td>0.058</td><td>0.043</td><td>0.042</td></tr></tbody></table></div><p>Table 2: Comparison for representing known shapes (K) for cars trained on ShapeNet. \(\mathrm{{CD}} =\) Chamfer Distance(30,000points) multiplied by \({10}^{3}\) ,EMD = Earth Mover’s Distance (500 points).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表2：在ShapeNet上训练的汽车已知形状（K）表示的比较。 \(\mathrm{{CD}} =\) Chamfer距离（30,000点）乘以 \({10}^{3}\)，EMD = 地球搬运工距离（500点）。</p></div><!-- Media --><p>Quantitative comparison in Table 2 shows that the proposed DeepSDF significantly beats OGN and AtlasNet in Chamfer distance against the true shape computed with a large number of points(30,000). The difference in earth mover distance (EMD) is smaller because 500 points do not well capture the additional precision. Figure 5 shows a qualitative comparison of DeepSDF to OGN.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表2中的定量比较显示，所提出的DeepSDF在Chamfer距离上显著优于OGN和AtlasNet，针对用大量点（30,000）计算的真实形状。地球搬运工距离（EMD）的差异较小，因为500个点无法很好地捕捉额外的精度。图5展示了DeepSDF与OGN的定性比较。</p></div><h3>6.2. Representing Test 3D Shapes (auto-encoding)</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.2. 表示测试3D形状（自编码）</h3></div><p>For encoding unknown shapes, i.e. shapes in the held-out test set, DeepSDF again significantly outperforms AtlasNet on a wide variety of shape classes and metrics as shown in Table 3. Note that AtlasNet performs reasonably well at classes of shapes that have mostly consistent topology without holes (like planes) but struggles more on classes that commonly have holes, like chairs. This is shown in Fig. 6 where AtlasNet fails to represent the fine detail of the back of the chair. Figure 7 shows more examples of detailed reconstructions on test data from DeepSDF as well as two example failure cases.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>对于编码未知形状，即在保留的测试集中形状，DeepSDF在各种形状类别和指标上再次显著优于AtlasNet，如表3所示。请注意，AtlasNet在大多数拓扑一致且没有孔的形状类别（如平面）上表现相当不错，但在常常有孔的类别（如椅子）上则表现较差。这在图6中得以体现，AtlasNet未能表示椅子背部的细节。图7展示了DeepSDF在测试数据上的详细重建示例以及两个失败案例。</p></div><h3>6.3. Shape Completion</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.3. 形状补全</h3></div><p>A major advantage of the proposed DeepSDF approach for representation learning is that inference can be performed from an arbitrary number of SDF samples. In the DeepSDF framework, shape completion amounts to solving for the shape code that best explains a partial shape observation via Eq. 10. Given the shape-code a complete shape can be rendered using the priors encoded in the decoder.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>所提出的DeepSDF表示学习方法的一个主要优势是可以从任意数量的SDF样本中进行推断。在DeepSDF框架中，形状补全相当于通过方程10求解最佳解释部分形状观察的形状编码。给定形状编码，可以使用解码器中编码的先验信息渲染完整形状。</p></div><!-- Media --><!-- figureText: (a) Ground-truth (b) Our Result (c) [19]-25 patch (d) [19]-sphere (e) Our Result (f) [19]-25 patch --><img src="https://cdn.noedgeai.com/bo_d164utbef24c73d1llu0_6.jpg?x=134&#x26;y=203&#x26;w=1446&#x26;h=367&#x26;r=0"><p>Figure 6: Reconstruction comparison between DeepSDF and AtlasNet [19] (with 25-plane and sphere parameterization) for test shapes. Note that AtlasNet fails to capture the fine details of the chair, and that (f) shows holes on the surface of sofa and the plane.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6：DeepSDF与AtlasNet [19]（具有25个平面和球体参数化）在测试形状上的重建比较。请注意，AtlasNet未能捕捉椅子的细节，并且（f）显示了沙发和平面表面的孔。</p></div><img src="https://cdn.noedgeai.com/bo_d164utbef24c73d1llu0_6.jpg?x=141&#x26;y=680&#x26;w=1463&#x26;h=265&#x26;r=0"><p>Figure 7: Reconstruction of test shapes. From left to right alternating: ground truth shape and our reconstruction. The two right most columns show failure modes of DeepSDF. These failures are likely due to lack of training data and failure of minimization convergence.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图7：测试形状的重建。从左到右交替：真实形状和我们的重建。最右侧的两列显示了DeepSDF的失败模式。这些失败可能是由于缺乏训练数据和最小化收敛失败所致。</p></div><table><tbody><tr><td>CD, mean</td><td>chair</td><td>plane</td><td>table</td><td>lamp</td><td>sofa</td></tr><tr><td>AtlasNet-Sph.</td><td>0.752</td><td>0.188</td><td>0.725</td><td>2.381</td><td>0.445</td></tr><tr><td>AtlasNet-25</td><td>0.368</td><td>0.216</td><td>0.328</td><td>1.182</td><td>0.411</td></tr><tr><td>DeepSDF</td><td>0.204</td><td>0.143</td><td>0.553</td><td>0.832</td><td>0.132</td></tr><tr><td colspan="6">CD, median</td></tr><tr><td>AtlasNet-Sph.</td><td>0.511</td><td>0.079</td><td>0.389</td><td>2.180</td><td>0.330</td></tr><tr><td>AtlasNet-25</td><td>0.276</td><td>0.065</td><td>0.195</td><td>0.993</td><td>0.311</td></tr><tr><td>DeepSDF</td><td>0.072</td><td>0.036</td><td>0.068</td><td>0.219</td><td>0.088</td></tr><tr><td colspan="6">EMD, mean</td></tr><tr><td>AtlasNet-Sph.</td><td>0.071</td><td>0.038</td><td>0.060</td><td>0.085</td><td>0.050</td></tr><tr><td>AtlasNet-25</td><td>0.064</td><td>0.041</td><td>0.073</td><td>0.062</td><td>0.063</td></tr><tr><td>DeepSDF</td><td>0.049</td><td>0.033</td><td>0.050</td><td>0.059</td><td>0.047</td></tr><tr><td colspan="6">Mesh acc., mean</td></tr><tr><td>AtlasNet-Sph.</td><td>0.033</td><td>0.013</td><td>0.032</td><td>0.054</td><td>0.017</td></tr><tr><td>AtlasNet-25</td><td>0.018</td><td>0.013</td><td>0.014</td><td>0.042</td><td>0.017</td></tr><tr><td>DeepSDF</td><td>0.009</td><td>0.004</td><td>0.012</td><td>0.013</td><td>0.004</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>CD，均值</td><td>椅子</td><td>飞机</td><td>桌子</td><td>灯</td><td>沙发</td></tr><tr><td>AtlasNet-Sph.</td><td>0.752</td><td>0.188</td><td>0.725</td><td>2.381</td><td>0.445</td></tr><tr><td>AtlasNet-25</td><td>0.368</td><td>0.216</td><td>0.328</td><td>1.182</td><td>0.411</td></tr><tr><td>DeepSDF</td><td>0.204</td><td>0.143</td><td>0.553</td><td>0.832</td><td>0.132</td></tr><tr><td colspan="6">CD，中位数</td></tr><tr><td>AtlasNet-Sph.</td><td>0.511</td><td>0.079</td><td>0.389</td><td>2.180</td><td>0.330</td></tr><tr><td>AtlasNet-25</td><td>0.276</td><td>0.065</td><td>0.195</td><td>0.993</td><td>0.311</td></tr><tr><td>DeepSDF</td><td>0.072</td><td>0.036</td><td>0.068</td><td>0.219</td><td>0.088</td></tr><tr><td colspan="6">EMD，均值</td></tr><tr><td>AtlasNet-Sph.</td><td>0.071</td><td>0.038</td><td>0.060</td><td>0.085</td><td>0.050</td></tr><tr><td>AtlasNet-25</td><td>0.064</td><td>0.041</td><td>0.073</td><td>0.062</td><td>0.063</td></tr><tr><td>DeepSDF</td><td>0.049</td><td>0.033</td><td>0.050</td><td>0.059</td><td>0.047</td></tr><tr><td colspan="6">网格精度，均值</td></tr><tr><td>AtlasNet-Sph.</td><td>0.033</td><td>0.013</td><td>0.032</td><td>0.054</td><td>0.017</td></tr><tr><td>AtlasNet-25</td><td>0.018</td><td>0.013</td><td>0.014</td><td>0.042</td><td>0.017</td></tr><tr><td>DeepSDF</td><td>0.009</td><td>0.004</td><td>0.012</td><td>0.013</td><td>0.004</td></tr></tbody></table></div><p>Table 3: Comparison for representing unknown shapes (U) for various classes of ShapeNet. Mesh accuracy as defined in [44] is the minimum distance \(d\) such that \({90}\%\) of generated points are within \(d\) of the ground truth mesh. Lower is better for all metrics.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表3：不同类别ShapeNet中表示未知形状（U）的比较。根据[44]定义的网格精度是生成点与真实网格之间的最小距离\(d\)，使得\({90}\%\)的生成点在真实网格的\(d\)范围内。所有指标越低越好。</p></div><!-- Media --><p>We test our completion scheme using single view depth observations which is a common use-case and maps well to our architecture without modification. Note that we currently require the depth observations in the canonical shape frame of reference.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们使用单视图深度观测测试我们的补全方案，这是一种常见的用例，并且与我们的架构很好地匹配，无需修改。请注意，我们目前要求深度观测在规范形状参考框架中。</p></div><p>To generate SDF point samples from the depth image observation, we sample two points for each depth observation, each of them located \(\eta\) distance away from the measured surface point (along surface normal estimate). With small \(\eta\) we approximate the signed distance value of those points to be \(\eta\) and \(- \eta\) ,respectively. We solve for Eq. 10 with loss function of Eq. 4 using clamp value of \(\eta\) . Additionally, we incorporate free-space observations, (i.e. empty-space between surface and camera), by sampling points along the freespace-direction and enforce larger-than-zero constraints. The freespace loss is \(\left| {{f}_{\theta }\left( {\mathbf{z},{\mathbf{x}}_{j}}\right) }\right|\) if \({f}_{\theta }\left( {\mathbf{z},{\mathbf{x}}_{j}}\right)  &#x3C; 0\) and 0 otherwise.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了从深度图像观测生成SDF点样本，我们为每个深度观测采样两个点，每个点距离测量的表面点\(\eta\)（沿表面法线估计）远。通过小的\(\eta\)，我们将这些点的有符号距离值近似为\(\eta\)和\(- \eta\)。我们使用Eq. 4的损失函数和Eq. 10求解，使用\(\eta\)的限制值。此外，我们通过沿自由空间方向采样点并强制大于零的约束，结合自由空间观测（即表面与相机之间的空白空间）。如果\({f}_{\theta }\left( {\mathbf{z},{\mathbf{x}}_{j}}\right)  &#x3C; 0\)，则自由空间损失为\(\left| {{f}_{\theta }\left( {\mathbf{z},{\mathbf{x}}_{j}}\right) }\right|\)，否则为0。</p></div><!-- Media --><table><tbody><tr><td rowspan="2">Method \\Metric</td><td colspan="4">lower is better</td><td colspan="2">higher is better</td></tr><tr><td>CD, med.</td><td>CD, mean</td><td>EMD</td><td>Mesh acc.</td><td>Mesh comp.</td><td>Cos sim.</td></tr><tr><td colspan="7">chair</td></tr><tr><td>3D-EPN</td><td>2.25</td><td>2.83</td><td>0.084</td><td>0.059</td><td>0.209</td><td>0.752</td></tr><tr><td>DeepSDF</td><td>1.28</td><td>2.11</td><td>0.071</td><td>0.049</td><td>0.500</td><td>0.766</td></tr><tr><td colspan="7">plane</td></tr><tr><td>3D-EPN</td><td>1.63</td><td>2.19</td><td>0.063</td><td>0.040</td><td>0.165</td><td>0.710</td></tr><tr><td>DeepSDF</td><td>0.37</td><td>1.16</td><td>0.049</td><td>0.032</td><td>0.722</td><td>0.823</td></tr><tr><td colspan="7">sofa</td></tr><tr><td>3D-EPN</td><td>2.03</td><td>2.18</td><td>0.071</td><td>0.049</td><td>0.254</td><td>0.742</td></tr><tr><td>DeepSDF</td><td>0.82</td><td>1.59</td><td>0.059</td><td>0.041</td><td>0.541</td><td>0.810</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">方法 \\指标</td><td colspan="4">越低越好</td><td colspan="2">越高越好</td></tr><tr><td>CD，中位数</td><td>CD，均值</td><td>EMD</td><td>网格精度</td><td>网格压缩</td><td>余弦相似度</td></tr><tr><td colspan="7">椅子</td></tr><tr><td>3D-EPN</td><td>2.25</td><td>2.83</td><td>0.084</td><td>0.059</td><td>0.209</td><td>0.752</td></tr><tr><td>DeepSDF</td><td>1.28</td><td>2.11</td><td>0.071</td><td>0.049</td><td>0.500</td><td>0.766</td></tr><tr><td colspan="7">平面</td></tr><tr><td>3D-EPN</td><td>1.63</td><td>2.19</td><td>0.063</td><td>0.040</td><td>0.165</td><td>0.710</td></tr><tr><td>DeepSDF</td><td>0.37</td><td>1.16</td><td>0.049</td><td>0.032</td><td>0.722</td><td>0.823</td></tr><tr><td colspan="7">沙发</td></tr><tr><td>3D-EPN</td><td>2.03</td><td>2.18</td><td>0.071</td><td>0.049</td><td>0.254</td><td>0.742</td></tr><tr><td>DeepSDF</td><td>0.82</td><td>1.59</td><td>0.059</td><td>0.041</td><td>0.541</td><td>0.810</td></tr></tbody></table></div><p>Table 4: Comparison for shape completion (C) from partial range scans of unknown shapes from ShapeNet.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表4：来自ShapeNet未知形状的部分范围扫描的形状补全比较（C）。</p></div><!-- Media --><p>Given the SDF point samples and empty space points, we similarly optimize the latent vector using MAP estimation. Tab. 4 and Figs.(8,9)respectively shows quantitative and qualitative shape completion results. Compared to one of the most recent completion approaches [15] using volumetric shape representation, our continuous SDF approach produces more visually pleasing and accurate shape reconstructions. While a few recent shape completion methods were presented \(\left\lbrack  {{21},{53}}\right\rbrack\) ,we could not find the code to run the comparisons, and their underlying 3D representation is voxel grid which we extensively compare against.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>给定SDF点样本和空白空间点，我们同样使用MAP估计优化潜在向量。表4和图（8,9）分别显示了定量和定性形状补全结果。与最近的一种使用体积形状表示的补全方法[15]相比，我们的连续SDF方法产生了更具视觉吸引力和准确性的形状重建。尽管最近提出了一些形状补全方法\(\left\lbrack  {{21},{53}}\right\rbrack\)，但我们无法找到运行比较的代码，而它们的基础3D表示是体素网格，我们对此进行了广泛比较。</p></div><!-- Media --><!-- figureText: (a) Input Depth (b) Completion (ours) (c) Second View (ours) (d) Ground truth (e) 3D-EPN --><img src="https://cdn.noedgeai.com/bo_d164utbef24c73d1llu0_7.jpg?x=126&#x26;y=199&#x26;w=1468&#x26;h=761&#x26;r=0"><p>Figure 8: For a given depth image visualized as a green point cloud, we show a comparison of shape completions from our DeepSDF approach against the true shape and 3D-EPN.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图8：对于给定的深度图像可视化为绿色点云，我们展示了我们的DeepSDF方法与真实形状和3D-EPN的形状补全比较。</p></div><!-- figureText: (a) Noisy Input Point Cloud (b) Shape Completion --><img src="https://cdn.noedgeai.com/bo_d164utbef24c73d1llu0_7.jpg?x=138&#x26;y=1091&#x26;w=709&#x26;h=205&#x26;r=0"><p>Figure 9: Demonstration of DeepSDF shape completion from a partial noisy point cloud. Input here is generated by perturbing the 3D point cloud positions generated by the ground truth depth map by 1.5% of the plane length. We provide a comprehensive analysis of robustness to noise in the supplementary material.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图9：从部分噪声点云演示DeepSDF形状补全。这里的输入是通过扰动由真实深度图生成的3D点云位置生成的，扰动幅度为平面长度的1.5%。我们在补充材料中提供了对噪声鲁棒性的全面分析。</p></div><!-- Media --><h3>6.4. Latent Space Shape Interpolation</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.4. 潜在空间形状插值</h3></div><p>To show that our learned shape embedding is complete and continuous, we render the results of the decoder when a pair of shapes are interpolated in the latent vector space (Fig. 1). The results suggests that the embedded continuous SDF's are of meaningful shapes and that our representation extracts common interpretable shape features, such as the arms of a chair, that interpolate linearly in the latent space.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了表明我们学习的形状嵌入是完整且连续的，我们在潜在向量空间中插值一对形状时渲染解码器的结果（图1）。结果表明，嵌入的连续SDF具有有意义的形状，我们的表示提取了共同的可解释形状特征，例如椅子的手臂，这些特征在潜在空间中线性插值。</p></div><h2>7. Conclusion &#x26; Future Work</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>7. 结论与未来工作</h2></div><p>DeepSDF significantly outperforms the applicable benchmarked methods across shape representation and completion tasks and simultaneously addresses the goals of representing complex topologies, closed surfaces, while providing high quality surface normals of the shape. However, while point-wise forward sampling of a shape's SDF is efficient, shape completion (auto-decoding) takes considerably more time during inference due to the need for explicit optimization over the latent vector. We look to increase performance by replacing ADAM optimization with more efficient Gauss-Newton or similar methods that make use of the analytic derivatives of the model.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>DeepSDF在形状表示和补全任务中显著优于适用的基准方法，同时解决了表示复杂拓扑、封闭表面的目标，同时提供形状的高质量表面法线。然而，尽管逐点前向采样形状的SDF是高效的，但形状补全（自动解码）在推理过程中需要显式优化潜在向量，因此耗时 considerably。我们希望通过用更高效的高斯-牛顿或类似方法替换ADAM优化来提高性能，这些方法利用模型的解析导数。</p></div><p>DeepSDF models enable representation of more complex shapes without discretization errors with significantly less memory than previous state-of-the-art results as shown in Table 1,demonstrating an exciting route ahead for \(3\mathrm{D}\) shape learning. The clear ability to produce quality latent shape space interpolation opens the door to reconstruction algorithms operating over scenes built up of such efficient encodings. However, DeepSDF currently assumes models are in a canonical pose and as such completion in-the-wild requires explicit optimization over a \({SE}\left( 3\right)\) transformation space increasing inference time. Finally, to represent the true space-of-possible-scenes including dynamics and textures in a single embedding remains a major challenge, one which we continue to explore.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>DeepSDF模型能够在没有离散化误差的情况下表示更复杂的形状，所需内存显著低于之前的最先进结果，如表1所示，展示了形状学习的激动人心的前景。清晰的生成高质量潜在形状空间插值的能力为基于这种高效编码构建的场景的重建算法打开了大门。然而，DeepSDF目前假设模型处于规范姿态，因此在实际环境中的补全需要在\({SE}\left( 3\right)\)变换空间上进行显式优化，从而增加推理时间。最后，表示包括动态和纹理的真实可能场景空间在单一嵌入中仍然是一个重大挑战，我们将继续探索。</p></div><p>[1] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas. Learning representations and generative models for \(3\mathrm{\;d}\) point clouds. 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[1] P. Achlioptas, O. Diamanti, I. Mitliagkas, 和 L. Guibas. 学习点云的表示和生成模型。2018。</p></div><p>[2] T. Bagautdinov, C. Wu, J. Saragih, P. Fua, and Y. Sheikh. Modeling facial geometry using compositional vaes. 1:1.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[2] T. Bagautdinov, C. Wu, J. Saragih, P. Fua, 和 Y. Sheikh. 使用组合变分自编码器建模面部几何。1:1。</p></div><p>[3] P. Baqué, E. Remelli, F. Fleuret, and P. Fua. Geodesic convolutional shape optimization. arXiv preprint arXiv:1802.04016, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[3] P. Baqué, E. Remelli, F. Fleuret, 和 P. Fua. 测地线卷积形状优化。arXiv预印本arXiv:1802.04016，2018。</p></div><p>[4] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. TPAMI, 35(8):1798-1828, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[4] Y. Bengio, A. Courville, 和 P. Vincent. 表示学习：综述与新视角。TPAMI, 35(8):1798-1828, 2013。</p></div><p>[5] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and A. J. Davison. Codeslam-learning a compact, optimis-able representation for dense visual slam. arXiv preprint arXiv:1804.00874, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[5] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, 和 A. J. Davison. Codeslam-学习一种紧凑的、可优化的稠密视觉slam表示。arXiv预印本arXiv:1804.00874，2018。</p></div><p>[6] P. Bojanowski, A. Joulin, D. Lopez-Pas, and A. Szlam. Optimizing the latent space of generative networks. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 600-609. PMLR, 10- 15 Jul 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[6] P. Bojanowski, A. Joulin, D. Lopez-Pas, 和 A. Szlam. 优化生成网络的潜在空间。在J. Dy和A. Krause主编的第35届国际机器学习会议论文集中，机器学习研究论文集第80卷，页600-609。PMLR，2018年7月10-15日。</p></div><p>[7] M. Bouakkaz and M.-F. Harkat. Combined input training and radial basis function neural networks based nonlinear principal components analysis model applied for process monitoring. In \({IJCCI}\) ,pages \({483} - {492},{2012}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[7] M. Bouakkaz 和 M.-F. Harkat. 结合输入训练和径向基函数神经网络的非线性主成分分析模型应用于过程监控。在 \({IJCCI}\) , 页 \({483} - {492},{2012}\) .</p></div><p>[8] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[8] J. Bruna, W. Zaremba, A. Szlam 和 Y. LeCun. 图上的谱网络和局部连接网络。arXiv 预印本 arXiv:1312.6203, 2013.</p></div><p>[9] J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell, W. R. Fright, B. C. McCallum, and T. R. Evans. Reconstruction and representation of \(3\mathrm{\;d}\) objects with radial basis functions. In SIGGRAPH, pages 67-76. ACM, 2001.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[9] J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell, W. R. Fright, B. C. McCallum 和 T. R. Evans. 使用径向基函数重建和表示 \(3\mathrm{\;d}\) 对象。在 SIGGRAPH, 页 67-76. ACM, 2001.</p></div><p>[10] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[10] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su 等. Shapenet: 一个信息丰富的 3D 模型库。arXiv 预印本 arXiv:1512.03012, 2015.</p></div><p>[11] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In NIPS, pages 2172-2180, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[11] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever 和 P. Abbeel. Infogan: 通过信息最大化生成对抗网络进行可解释的表示学习。在 NIPS, 页 2172-2180, 2016.</p></div><p>[12] Z. Chen and H. Zhang. Learning implicit fields for generative shape modeling. arXiv preprint arXiv:1812.02822, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[12] Z. Chen 和 H. Zhang. 学习隐式场以进行生成形状建模。arXiv 预印本 arXiv:1812.02822, 2018.</p></div><p>[13] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In \({ECCV}\) ,pages 628-644. Springer,2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[13] C. B. Choy, D. Xu, J. Gwak, K. Chen 和 S. Savarese. 3d-r2n2: 单视图和多视图 3D 对象重建的统一方法。在 \({ECCV}\) , 页 628-644. Springer, 2016.</p></div><p>[14] B. Curless and M. Levoy. A volumetric method for building complex models from range images. In SIGGRAPH, pages 303-312. ACM, 1996.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[14] B. Curless 和 M. Levoy. 一种基于体积的方法从范围图像构建复杂模型。在 SIGGRAPH, 页 303-312. ACM, 1996.</p></div><p>[15] A. Dai, C. Ruizhongtai Qi, and M. Niessner. Shape completion using 3d-encoder-predictor cnns and shape synthesis. In CVPR, pages 5868-5877, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[15] A. Dai, C. Ruizhongtai Qi 和 M. Niessner. 使用 3D 编码器-预测器 CNN 和形状合成进行形状补全。在 CVPR, 页 5868-5877, 2017.</p></div><p>[16] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In NIPS, pages 3844-3852, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[16] M. Defferrard, X. Bresson 和 P. Vandergheynst. 在图上使用快速局部谱滤波的卷积神经网络。在 NIPS, 页 3844-3852, 2016.</p></div><p>[17] J. Fan and J. Cheng. Matrix completion by deep matrix factorization. Neural Networks, 98:34-41, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[17] J. Fan 和 J. Cheng. 通过深度矩阵分解进行矩阵补全。神经网络, 98:34-41, 2018.</p></div><p>[18] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, pages 2672-2680, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[18] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville 和 Y. Bengio. 生成对抗网络。在 NIPS, 页 2672-2680, 2014.</p></div><p>[19] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry. Atlasnet: A papier-m\^ ach\'e approach to learning 3d surface generation. arXiv preprint arXiv:1802.05384, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[19] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell 和 M. Aubry. Atlasnet: 一种纸浆法学习 3D 表面生成。arXiv 预印本 arXiv:1802.05384, 2018.</p></div><p>[20] H. B. Hamu, H. Maron, I. Kezurer, G. Avineri, and Y. Lipman. Multi-chart generative surface modeling. arXiv preprint arXiv:1806.02143, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[20] H. B. Hamu, H. Maron, I. Kezurer, G. Avineri 和 Y. Lipman. 多图生成表面建模。arXiv 预印本 arXiv:1806.02143, 2018.</p></div><p>[21] X. Han, Z. Li, H. Huang, E. Kalogerakis, and Y. Yu. High-resolution shape completion using deep neural networks for global structure and local geometry inference.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[21] X. Han, Z. Li, H. Huang, E. Kalogerakis 和 Y. Yu. 使用深度神经网络进行全局结构和局部几何推断的高分辨率形状补全。</p></div><p>[22] C. Häne, S. Tulsiani, and J. Malik. Hierarchical surface prediction for \(3\mathrm{\;d}\) object reconstruction. In \({3D}\) Vision (3DV), 2017 International Conference on, pages 412-420. IEEE, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[22] C. Häne, S. Tulsiani 和 J. Malik. 用于 \(3\mathrm{\;d}\) 对象重建的层次表面预测。在 \({3D}\) 视觉 (3DV), 2017 国际会议, 页 412-420. IEEE, 2017.</p></div><p>[23] J. C. Hart. Sphere tracing: A geometric method for the an-tialiased ray tracing of implicit surfaces. The Visual Computer, 12(10):527-545, 1996.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[23] J. C. Hart. 球体追踪：一种用于隐式表面抗锯齿光线追踪的几何方法。《视觉计算机》，12(10)：527-545，1996。</p></div><p>[24] K. Hornik, M. Stinchcombe, and H. White. Multilayer feed-forward networks are universal approximators. Neural networks, 2(5):359-366, 1989.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[24] K. Hornik, M. Stinchcombe, 和 H. White. 多层前馈网络是通用逼近器。《神经网络》，2(5)：359-366，1989。</p></div><p>[25] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[25] S. Ioffe 和 C. Szegedy. 批量归一化：通过减少内部协变量偏移加速深度网络训练。arXiv 预印本 arXiv:1502.03167，2015。</p></div><p>[26] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[26] P. Isola, J.-Y. Zhu, T. Zhou, 和 A. A. Efros. 使用条件对抗网络进行图像到图像的转换。arXiv 预印本，2017。</p></div><p>[27] M. W. Jones. Distance field compression. Journal of WSCG, 12(2):199-204, 2004.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[27] M. W. Jones. 距离场压缩。《WSCG期刊》，12(2)：199-204，2004。</p></div><p>[28] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[28] T. Karras, T. Aila, S. Laine, 和 J. Lehtinen. GAN的渐进式生长以提高质量、稳定性和变化性。arXiv 预印本 arXiv:1710.10196，2017。</p></div><p>[29] M. Kazhdan and H. Hoppe. Screened poisson surface reconstruction. ACM TOG, 32(3):29, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[29] M. Kazhdan 和 H. Hoppe. 筛选泊松表面重建。《ACM TOG》，32(3)：29，2013。</p></div><p>[30] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[30] D. P. Kingma 和 J. Ba. Adam：一种随机优化方法。arXiv 预印本 arXiv:1412.6980，2014。</p></div><p>[31] O. Litany, A. Bronstein, M. Bronstein, and A. Makadia. Deformable shape completion with graph convolutional autoen-coders. CVPR, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[31] O. Litany, A. Bronstein, M. Bronstein, 和 A. Makadia. 使用图卷积自编码器进行可变形形状补全。CVPR，2017。</p></div><p>[32] W. E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3d surface construction algorithm. In SIGGRAPH, volume 21, pages 163-169. ACM, 1987.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[32] W. E. Lorensen 和 H. E. Cline. 行进立方体：一种高分辨率3D表面构建算法。在SIGGRAPH，卷21，页163-169。ACM，1987。</p></div><p>[33] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym, E. Yumer, V. G. Kim, and Y. Lipman. Convolutional neural networks on surfaces via seamless toric covers. 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[33] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym, E. Yumer, V. G. Kim, 和 Y. Lipman. 通过无缝环面覆盖在表面上进行卷积神经网络。2017。</p></div><p>[34] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy networks: Learning \(3\mathrm{\;d}\) reconstruction in function space. arXiv preprint arXiv:1812.03828, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[34] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, 和 A. Geiger. 占用网络：在函数空间中学习\(3\mathrm{\;d}\)重建。arXiv 预印本 arXiv:1812.03828，2018。</p></div><p>[35] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon. Kinectfusion: Real-time dense surface mapping and tracking. In ISMAR, pages 127-136. IEEE, 2011.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[35] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, P. Kohi, J. Shotton, S. Hodges, 和 A. Fitzgibbon. Kinectfusion：实时密集表面映射和跟踪。在ISMAR，页127-136。IEEE，2011。</p></div><p>[36] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for \(3\mathrm{\;d}\) classification and segmentation. In \({CVPR}\) ,pages 652-660,2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[36] C. R. Qi, H. Su, K. Mo, 和 L. J. Guibas. Pointnet：用于\(3\mathrm{\;d}\)分类和分割的点集深度学习。在\({CVPR}\)，页652-660，2017。</p></div><p>[37] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In NIPS, pages 5099-5108, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[37] C. R. Qi, L. Yi, H. Su, 和 L. J. Guibas. Pointnet++：在度量空间中对点集进行深度层次特征学习。在NIPS，页5099-5108，2017。</p></div><p>[38] Z. Qunxiong and L. Chengfei. Dimensionality reduction with input training neural network and its application in chemical process modelling. Chinese Journal of Chemical Engineering, 14(5):597-603, 2006.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[38] Z. Qunxiong 和 L. Chengfei. 使用输入训练神经网络的降维及其在化学过程建模中的应用。《中国化学工程杂志》，14(5)：597-603，2006。</p></div><p>[39] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[39] A. Radford, L. Metz, 和 S. Chintala. 使用深度卷积生成对抗网络的无监督表示学习. arXiv 预印本 arXiv:1511.06434, 2015.</p></div><p>[40] V. Reddy and M. Mavrovouniotis. An input-training neural network approach for gross error detection and sensor replacement. Chemical Engineering Research and Design, 76(4):478-489, 1998.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[40] V. Reddy 和 M. Mavrovouniotis. 一种用于粗大错误检测和传感器替换的输入训练神经网络方法. 化学工程研究与设计, 76(4):478-489, 1998.</p></div><p>[41] G. Riegler, A. O. Ulusoy, and A. Geiger. Octnet: Learning deep 3d representations at high resolutions. In \({CVPR}\) ,pages 6620-6629. IEEE, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[41] G. Riegler, A. O. Ulusoy, 和 A. Geiger. Octnet: 在高分辨率下学习深度3D表示. 在 \({CVPR}\) , 页6620-6629. IEEE, 2017.</p></div><p>[42] J. Rock, T. Gupta, J. Thorsen, J. Gwak, D. Shin, and D. Hoiem. Completing 3d object shape from one depth image. In \({CVPR}\) ,pages 2484-2493,2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[42] J. Rock, T. Gupta, J. Thorsen, J. Gwak, D. Shin, 和 D. Hoiem. 从一幅深度图像完成3D物体形状. 在 \({CVPR}\) , 页2484-2493, 2015.</p></div><p>[43] T. Salimans and D. P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In NIPS, pages 901-909, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[43] T. Salimans 和 D. P. Kingma. 权重归一化: 一种简单的重新参数化方法以加速深度神经网络的训练. 在 NIPS, 页901-909, 2016.</p></div><p>[44] S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and R. Szeliski. A comparison and evaluation of multi-view stereo reconstruction algorithms. pages 519-528. IEEE, 2006.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[44] S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, 和 R. Szeliski. 多视图立体重建算法的比较与评估. 页519-528. IEEE, 2006.</p></div><p>[45] A. Sinha, J. Bai, and K. Ramani. Deep learning 3d shape surfaces using geometry images. In \({ECCV}\) ,pages 223-240. Springer, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[45] A. Sinha, J. Bai, 和 K. Ramani. 使用几何图像进行深度学习3D形状表面. 在 \({ECCV}\) , 页223-240. Springer, 2016.</p></div><p>[46] D. Stutz and A. Geiger. Learning 3d shape completion from laser scan data with weak supervision. In \({CVPR}\) ,pages 1955-1964, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[46] D. Stutz 和 A. Geiger. 从激光扫描数据中学习3D形状补全，采用弱监督. 在 \({CVPR}\) , 页1955-1964, 2018.</p></div><p>[47] S. Tan and M. L. Mayrovouniotis. Reducing data dimensionality through optimizing neural network inputs. AIChE Journal, 41(6):1471-1480, 1995.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[47] S. Tan 和 M. L. Mayrovouniotis. 通过优化神经网络输入来减少数据维度. AIChE 杂志, 41(6):1471-1480, 1995.</p></div><p>[48] M. Tarini, K. Hormann, P. Cignoni, and C. Montani. Polycube-maps. In ACM TOG, volume 23, pages 853-860. ACM, 2004.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[48] M. Tarini, K. Hormann, P. Cignoni, 和 C. Montani. 多面体映射. 在 ACM TOG, 卷23, 页853-860. ACM, 2004.</p></div><p>[49] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs. In \({ICCV},{2017}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[49] M. Tatarchenko, A. Dosovitskiy, 和 T. Brox. 八叉树生成网络: 高分辨率3D输出的高效卷积架构. 在 \({ICCV},{2017}\) .</p></div><p>[50] N. Verma, E. Boyer, and J. Verbeek. Feastnet: Feature-steered graph convolutions for \(3\mathrm{\;d}\) shape analysis.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[50] N. Verma, E. Boyer, 和 J. Verbeek. Feastnet: 特征引导的图卷积用于 \(3\mathrm{\;d}\) 形状分析.</p></div><p>[51] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon. Dynamic graph cnn for learning on point clouds. arXiv preprint arXiv:1801.07829, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[51] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, 和 J. M. Solomon. 动态图CNN用于点云学习. arXiv 预印本 arXiv:1801.07829, 2018.</p></div><p>[52] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. In NIPS, pages 82-90, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[52] J. Wu, C. Zhang, T. Xue, B. Freeman, 和 J. Tenenbaum. 通过3D生成对抗建模学习物体形状的概率潜在空间. 在 NIPS, 页82-90, 2016.</p></div><p>[53] J. Wu, C. Zhang, X. Zhang, Z. Zhang, W. T. Freeman, and J. B. Tenenbaum. Learning shape priors for single-view 3d completion and reconstruction. arXiv preprint arXiv:1809.05068, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[53] J. Wu, C. Zhang, X. Zhang, Z. Zhang, W. T. Freeman, 和 J. B. Tenenbaum. 学习单视图3D补全和重建的形状先验. arXiv 预印本 arXiv:1809.05068, 2018.</p></div><p>[54] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In \({CVPR}\) ,pages 1912-1920,2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[54] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, 和 J. Xiao. 3D形状网: 一种体积形状的深度表示. 在 \({CVPR}\) , 页1912-1920, 2015.</p></div><p>[55] Y. Yang, C. Feng, Y. Shen, and D. Tian. Foldingnet: Interpretable unsupervised learning on \(3\mathrm{\;d}\) point clouds. arXiv preprint arXiv:1712.07262, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[55] Y. Yang, C. Feng, Y. Shen, 和 D. Tian. Foldingnet: 可解释的无监督学习在\(3\mathrm{\;d}\)点云上. arXiv预印本 arXiv:1712.07262, 2017.</p></div><p>[56] W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert. Pcn: Point completion network. In \({3DV},{2018}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[56] W. Yuan, T. Khot, D. Held, C. Mertz, 和 M. Hebert. Pcn: 点补全网络. 在\({3DV},{2018}\).</p></div><p>[57] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, and T. Funkhouser. 3dmatch: Learning local geometric descriptors from rgb-d reconstructions. In CVPR, pages 199-208. IEEE, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[57] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, 和 T. Funkhouser. 3dmatch: 从rgb-d重建中学习局部几何描述符. 在CVPR, 页199-208. IEEE, 2017.</p></div>
      </body>
    </html>
  