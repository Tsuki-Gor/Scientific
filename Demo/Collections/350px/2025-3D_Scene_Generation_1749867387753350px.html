
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px 350px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <h1>3D Scene Generation: A Survey</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>3D场景生成：综述</h1></div><p>Beichen Wen*, Haozhe Xie*, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>文北辰*, 谢浩哲*, 陈兆熙, 洪方舟, 刘子维</p></div><p>Abstract-3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative \(\mathrm{{Al}},3\mathrm{D}\) vision,and embodied intelligence. To track ongoing developments,we maintain an up-to-date project page: <a href="https://github.com/hzxie/Awesome-3D-Scene-Generation">https://github.com/hzxie/Awesome-3D-Scene-Generation</a>.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>摘要-3D场景生成旨在合成空间结构化、语义丰富和照片真实的环境，应用于沉浸式媒体、机器人技术、自动驾驶和具身人工智能等领域。早期基于程序规则的方法提供了可扩展性，但多样性有限。最近在深度生成模型（如GANs、扩散模型）和3D表示（如NeRF、3D高斯）方面的进展，使得学习真实世界场景分布成为可能，提高了保真度、多样性和视图一致性。最近的进展如扩散模型通过将生成重新构建为图像或视频合成问题，架起了3D场景合成与照片真实之间的桥梁。本综述系统地概述了最先进的方法，将其组织为四种范式：程序生成、基于神经网络的生成、基于图像的生成和基于视频的生成。我们分析了它们的技术基础、权衡和代表性结果，并回顾了常用的数据集、评估协议和下游应用。最后，我们讨论了在生成能力、3D表示、数据和注释以及评估方面的关键挑战，并概述了包括更高保真度、物理感知和交互生成以及统一感知-生成模型等有前景的方向。本综述整理了3D场景生成的最新进展，并强调了生成视觉与具身智能交叉领域的有前景方向。为了跟踪持续的发展，我们维护一个最新的项目页面：<a href="https://github.com/hzxie/Awesome-3D-Scene-Generation%E3%80%82">https://github.com/hzxie/Awesome-3D-Scene-Generation。</a></p></div><p>Index Terms-3D Scene Generation, Generative Models, AI Generated Content, 3D Vision</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>索引词-3D场景生成、生成模型、人工智能生成内容、3D视觉</p></div><h2>1 INTRODUCTION</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>1 引言</h2></div><p>T\({}^{1}\mathrm{{HE}}\) goal of generating \(3\mathrm{D}\) scenes is to create a spatially structured, semantically meaningful, and visually realistic 3D environment. As a cornerstone of computer vision, it supports a wide range of applications, from immersive filmmaking [1], [2] and expansive game worlds [3], [4], [5] to architectural visualization [6], [7]. It also plays a crucial role in AR/VR [8], [9], [10], robotics simulation [11], [12], and autonomous driving [13], [14] by providing high-fidelity environments for training and testing. Beyond these applications, 3D scene generation is vital for advancing embodied AI [15], [16], [17] and world models [18], [19], [20], which depend on diverse, high-quality scenes for learning and evaluation. Realistic scene synthesis enhances AI agents' ability to navigate, interact, and adapt, driving progress in autonomous systems and virtual simulations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>生成\(3\mathrm{D}\)场景的目标是创建一个空间结构化、语义丰富和视觉真实的3D环境。作为计算机视觉的基石，它支持广泛的应用，从沉浸式电影制作[1]、广阔的游戏世界[3]、[4]、[5]到建筑可视化[6]、[7]。它在增强现实/虚拟现实[8]、[9]、[10]、机器人仿真[11]、[12]和自动驾驶[13]、[14]中也发挥着至关重要的作用，为训练和测试提供高保真的环境。除了这些应用，3D场景生成对于推进具身人工智能[15]、[16]、[17]和世界模型[18]、[19]、[20]至关重要，这些模型依赖于多样化、高质量的场景进行学习和评估。真实的场景合成增强了人工智能代理的导航、互动和适应能力，推动了自主系统和虚拟仿真的进展。</p></div><p>As shown in Figure 1, 3D scene generation has gained significant attention in recent years. Early scene generation methods relied on procedural generation using rule-based algorithms [21] and manually designed assets [22], offering scalability and control in game design [23], urban planning [24], [25], and architecture [26], [27]. However, their reliance on predefined rules and deterministic algorithms limits diversity, requiring extensive human intervention for realistic or varied scenes [28]. Advances in deep generative models (e.g., GANs [29], Diffusion models [30]), enable neural networks to synthesize diverse, realistic spatial structures by learning real-world distributions. Combined with innovations in 3D representations like NeRF [31] and 3D Gaussians [32], neural 3D-based generation methods enhance geometry fidelity, rendering efficiency, and view consistency, making them ideal for photorealistic scene synthesis and immersive virtual environments. Starting from a single image, image-based scene generation methods leverage camera pose transformations and image outpainting to iteratively synthesize perpetual views [33], [34] or panoramic local environments [35], [36]. Benefit from the rapid advancement of video diffusion models [37], [38], video generation quality has significantly improved, leading to a surge in \(3\mathrm{D}\) scene generation research over the past two years. These methods formulate 3D scene generation as a form of video generation and enhance view consistency through temporal modeling [39]. The integration of dynamic 3D representations [40], [41] further facilitates the synthesis of immersive and dynamic environments [42], [43].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>如图1所示，近年来3D场景生成受到了显著关注。早期的场景生成方法依赖于使用基于规则的算法[21]和手动设计的资产[22]进行程序生成，在游戏设计[23]、城市规划[24]、[25]和建筑[26]、[27]中提供了可扩展性和控制。然而，它们对预定义规则和确定性算法的依赖限制了多样性，需要大量人工干预以实现真实或多样的场景[28]。深度生成模型（如GANs[29]、扩散模型[30]）的进展使得神经网络能够通过学习真实世界分布合成多样化、真实的空间结构。结合NeRF[31]和3D高斯[32]等3D表示的创新，基于神经网络的生成方法提高了几何保真度、渲染效率和视图一致性，使其成为照片真实场景合成和沉浸式虚拟环境的理想选择。从单个图像开始，基于图像的场景生成方法利用相机姿态变换和图像外绘，迭代合成永久视图[33]、[34]或全景局部环境[35]、[36]。得益于视频扩散模型[37]、[38]的快速发展，视频生成质量显著提高，导致过去两年3D场景生成研究的激增。这些方法将3D场景生成形式化为视频生成的一种形式，并通过时间建模增强视图一致性[39]。动态3D表示的整合[40]、[41]进一步促进了沉浸式和动态环境的合成[42]、[43]。</p></div><!-- Media --><!-- figureText: 120 93 61 40 20 14 2021 2022 2023 2024 2025 2 Procedural Generation 100 Neural 3D-based Generation I Image-based Generation 80 Video-based Generation #Papers 60 40 29 20 5 7 0 Before 2018 2019 2020 --><img src="https://cdn.noedgeai.com/bo_d163kf3ef24c73d1lc90_0.jpg?x=920&#x26;y=972&#x26;w=732&#x26;h=362&#x26;r=0"><p>Fig. 1. Annual statistics of 3D scene generation papers in computer vision conferences, journals, and preprints. The notable rise in publications and the evolving trends in recent years highlight the need for a comprehensive survey. Note that the data for 2025 reflects papers published up until April 30th.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图1. 计算机视觉会议、期刊和预印本中3D场景生成论文的年度统计数据。近年来出版物的显著增加和不断演变的趋势突显了进行全面综述的必要性。请注意，2025年的数据反映了截至4月30日发表的论文。</p></div><!-- Media --><hr>
<!-- Footnote --><ul>
<li>This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOET2EP20221- 0012, MOE-T2EP20223- 0002), and under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). (Corresponding author: Ziwei Liu.)</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>本研究得到了新加坡教育部的支持，属于其MOE AcRF Tier 2（MOET2EP20221- 0012，MOE-T2EP20223- 0002），以及RIE2020行业对接基金 - 行业合作项目（IAF-ICP）资助计划，并得到了行业合作伙伴的现金和实物贡献。（通讯作者：刘子维。）</li>
</ul></div><ul>
<li>The authors are with S-Lab, Nanyang Technological University, Singapore 637335 (email: <a href="mailto:beichen002@ntu.edu.sg">beichen002@ntu.edu.sg</a>; <a href="mailto:haozhe.xie@ntu.edu.sg">haozhe.xie@ntu.edu.sg</a>; <a href="mailto:zhaoxi001@ntu.edu.sg">zhaoxi001@ntu.edu.sg</a>; <a href="mailto:fangzhou001@ntu.edu.sg">fangzhou001@ntu.edu.sg</a>; <a href="mailto:ziwei.liu@ntu.edu.sg">ziwei.liu@ntu.edu.sg</a>) * denotes equal contribution.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>作者来自新加坡南洋理工大学S-Lab，邮政编码637335（电子邮件：<a href="mailto:beichen002@ntu.edu.sg">beichen002@ntu.edu.sg</a>；<a href="mailto:haozhe.xie@ntu.edu.sg">haozhe.xie@ntu.edu.sg</a>；<a href="mailto:zhaoxi001@ntu.edu.sg">zhaoxi001@ntu.edu.sg</a>；<a href="mailto:fangzhou001@ntu.edu.sg">fangzhou001@ntu.edu.sg</a>；<a href="mailto:ziwei.liu@ntu.edu.sg">ziwei.liu@ntu.edu.sg</a>）* 表示同等贡献。</li>
</ul></div><!-- Footnote -->
<hr><!-- Media --><!-- figureText: Task Definition and Voxel Grid Point Cloud Mesh Neural Fields Image Sequence Autoregressive Model GAN Diffusion Model Procedural Generator Rule-based Gen. (§ 3.1.1) Optm.-based Gen. (§ 3.1.2) LLM-based Gen. (§ 3.1.3) Scene Parameters (§ 3.2.1) Scene Graph (§ 3.2.2) Semantic Layout (§ 3.2.3) Implicit Layout (§ 3.2.4) Holistic Gen. (§ 3.3.1) Iterative Gen. (§ 3.3.2) Two-stage Gen. (§ 3.4.1) Indoor Datasets (§ 4.1.1) Natural Datasets (§ 4.1.2) Urban Datasets (§ 4.1.3) Metrics-based Eval. (§ 4.2.1) Benchmark-based Eval. (§ 4.2.2) Human Eval. (§ 4.2.3) 3D Scene Representa- tions(§2.2) Introduction (§ 1) Preliminaries (§ 2) Generative Models Procedural Gen. (§ 3.1) Neural 3D-based Gen. Methods: A (§3.2) 3D Scene Generation Hierarchical Taxonomy Image-based Gen. (§ 3.3) Video-based Gen. (§ 3.4) Datasets (§ 4.1) Datasets and Evaluation (§ 4) Evaluation (§ 4.2) 3D Scene Editing (§ 5.1) Human-Scene Interaction (§5.2) Applications and Tasks (§ 5) Robotics (§ 5.4) Autonomous Driving (§5.5) Challenges and Challenges (§ 6.1) Future Directions (§ 6) Future Directions (§ 6.2) --><img src="https://cdn.noedgeai.com/bo_d163kf3ef24c73d1lc90_1.jpg?x=168&#x26;y=118&#x26;w=714&#x26;h=1127&#x26;r=0"><p>Fig. 2. The overall structure of our comprehensive survey. Our survey presents three core contributions: 1) a summary of key representations and generative models in 3D scene generation, 2) a hierarchical taxonomy systematically organizing intertwined papers with in-depth analysis, and 3) an exploration of datasets, evaluation metrics, applications, along with an outlook on challenges and future directions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2. 我们综合调查的整体结构。我们的调查提出了三个核心贡献：1）对3D场景生成中的关键表示和生成模型的总结，2）一个系统地组织交织论文的分层分类法，并进行深入分析，3）对数据集、评估指标、应用的探索，以及对挑战和未来方向的展望。</p></div><!-- Media --><p>Compared to generating \(3\mathrm{D}\) objects and avatars,generating \(3\mathrm{D}\) scenes presents significantly greater challenges across several dimensions. 1) Scale: Objects and avatars typically exist within a fixed, limited spatial extent, while scenes must accommodate multiple entities across a much larger and more variable spatial scale. 2) Structural complexity: Scenes involve complex spatial and semantic relationships among diverse objects, requiring the model to ensure both functional coherence and overall plausibility. 3) Data availability: While large-scale datasets for object-and avatar-level generation are abundant, high-quality, annotated 3D scene datasets remain scarce and expensive to collect. 4) Fine-grained control: Scene generation often demands user control over attributes like object placement, zoning, and style, which remain difficult to incorporate in a flexible and interpretable way.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>与生成\(3\mathrm{D}\)对象和头像相比，生成\(3\mathrm{D}\)场景在多个维度上面临显著更大的挑战。1）规模：对象和头像通常存在于固定的、有限的空间范围内，而场景必须容纳多个实体，跨越更大且更可变的空间规模。2）结构复杂性：场景涉及多样对象之间复杂的空间和语义关系，要求模型确保功能一致性和整体合理性。3）数据可用性：虽然用于对象和头像级生成的大规模数据集丰富，但高质量、带注释的3D场景数据集仍然稀缺且收集成本高。4）细粒度控制：场景生成通常要求用户对对象放置、区域划分和风格等属性进行控制，而这些在灵活和可解释的方式中仍然难以实现。</p></div><p>Despite rapid progress in \(3\mathrm{D}\) scene generation,the field lacks a comprehensive survey that systematically categorizes existing approaches, highlights key challenges, and identifies future directions. Prior surveys focus on narrow domains such as procedural generation [44], [45], indoor scenes [46], [47], autonomous driving [48], and text-driven generation [49], [50], offering limited perspectives. Broader surveys on general 3D or 4D content generation [51], [52], [53], [54], [55], [56] often treat scene generation only peripherally, leading to fragmented coverage. Although many existing works explore aspects of scene generation, their broader focus often leads to a fragmented understanding that overlooks critical components. Some works focus on specific subdomains, such as diffusion models [55], text-driven scene generation [52], or 4D generation [56], while others neglect key representations like 3D Gaussians [51] and image sequences [53], [54], as well as important paradigms like procedural and video-based generation [51], [53], [54]. Surveys on world models [18], [57], [58] primarily address video prediction in driving scenarios, offering only a partial view. These gaps call for a comprehensive, up-to-date survey that consolidates recent advances and maps out the evolving landscape of \(3\mathrm{D}\) scene generation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>尽管\(3\mathrm{D}\)场景生成领域取得了快速进展，但该领域缺乏一项全面的调查，系统地分类现有方法，突出关键挑战，并识别未来方向。之前的调查集中于狭窄的领域，如程序生成[44]，[45]，室内场景[46]，[47]，自动驾驶[48]，以及文本驱动生成[49]，[50]，提供的视角有限。关于一般3D或4D内容生成的更广泛调查[51]，[52]，[53]，[54]，[55]，[56]通常仅在边缘上处理场景生成，导致覆盖面碎片化。尽管许多现有工作探讨了场景生成的各个方面，但它们的更广泛关注往往导致对关键组成部分的理解碎片化。一些工作专注于特定子领域，如扩散模型[55]，文本驱动场景生成[52]，或4D生成[56]，而其他工作则忽视了关键表示，如3D高斯[51]和图像序列[53]，[54]，以及重要范式，如程序生成和基于视频的生成[51]，[53]，[54]。关于世界模型的调查[18]，[57]，[58]主要涉及驾驶场景中的视频预测，仅提供部分视角。这些空白呼唤一项全面、最新的调查，以整合最近的进展并描绘\(3\mathrm{D}\)场景生成的演变格局。</p></div><p>Contributions. This survey offers a structured overview of recent advances in \(3\mathrm{D}\) scene generation. We categorize existing methods into four types: procedural, neural 3D-based, image-based, and video-based generation, highlighting their paradigms and trade-offs. We also review key applications in scene editing, human-scene interaction, embodied AI, robotics, and autonomous driving. Additionally, we examine commonly used scene representations, datasets, and evaluation protocols, and identify current limitations in generative capacity, controllability, and realism. Finally, we outline future directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>贡献。本调查提供了对\(3\mathrm{D}\)场景生成近期进展的结构化概述。我们将现有方法分为四种类型：程序生成、基于神经网络的3D生成、基于图像的生成和基于视频的生成，突出它们的范式和权衡。我们还回顾了场景编辑、人机场景交互、具身人工智能、机器人技术和自动驾驶等关键应用。此外，我们检查了常用的场景表示、数据集和评估协议，并识别了生成能力、可控性和现实性方面的当前局限性。最后，我们概述了未来方向，包括更高的保真度、物理感知和交互生成，以及统一的感知-生成模型。</p></div><p>Scope. This survey primarily focuses on approaches for generating \(3\mathrm{D}\) scenes in \(3\mathrm{D}\) scene representations. Notably, these generative methods aim to synthesize diverse \(3\mathrm{D}\) scenes,whereas \(3\mathrm{D}\) reconstruction methods can only generate a single scene from a given input. For a review of reconstruction approaches, readers may refer to [59], [60]. Furthermore, this survey excludes general video generation [38], [61] and general 3D object generation [62], [63], [64] methods, even though they have demonstrated some capability in 3D scene generation. This survey complements existing reviews on 3D generative models [51], [52], [53], [54], [55], as none provide a comprehensive overview of 3D scene generation or its relevant insights.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>范围。本调查主要关注生成\(3\mathrm{D}\)场景在\(3\mathrm{D}\)场景表示中的方法。值得注意的是，这些生成方法旨在合成多样的\(3\mathrm{D}\)场景，而\(3\mathrm{D}\)重建方法只能从给定输入生成单一场景。有关重建方法的综述，读者可以参考[59]，[60]。此外，本调查不包括一般视频生成[38]，[61]和一般3D对象生成[62]，[63]，[64]方法，尽管它们在3D场景生成中展示了一定的能力。本调查补充了现有的3D生成模型综述[51]，[52]，[53]，[54]，[55]，因为没有一篇提供3D场景生成或其相关见解的全面概述。</p></div><p>Organization. A summary of this survey's structure is presented in Figure 2. Section 2 provides the foundational concepts, including task definition and formulation, 3D scene representations, and generative models. Section 3 categorizes existing approaches into four types, detailing each category's paradigm, strengths, and weaknesses. Section 4 introduces relevant datasets and evaluation metrics. Section 5 reviews various downstream tasks related to 3D scene generation. Finally, Section 6 discusses current challenges, future directions, and concludes the survey.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>组织。本调查结构的摘要见图2。第2节提供基础概念，包括任务定义和公式化、3D场景表示和生成模型。第3节将现有方法分为四类，详细说明每个类别的范式、优缺点。第4节介绍相关数据集和评估指标。第5节回顾与3D场景生成相关的各种下游任务。最后，第6节讨论当前挑战、未来方向，并总结本调查。</p></div><h2>2 PRELIMINARIES</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>2 前言</h2></div><h3>2.1 Task Definition and Formulation</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>2.1 任务定义和公式化</h3></div><p>3D scene generation maps an input \(\mathbf{x}\) (e.g.,random noise, text,images,or other conditions) to a \(3\mathbf{D}\) scene representation \(\mathbf{S}\) (Sec. 2.2) using a generative model \(\mathcal{G}\) (Sec. 2.3).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D场景生成将输入\(\mathbf{x}\)（例如，随机噪声、文本、图像或其他条件）映射到\(3\mathbf{D}\)场景表示\(\mathbf{S}\)（第2.2节），使用生成模型\(\mathcal{G}\)（第2.3节）。</p></div><p></p>\[\mathcal{G} : \mathbf{x} \rightarrow  \mathbf{S} \tag{1}\]<p></p><p>The generated scene \(\mathbf{S}\) is spatially coherent,implicitly or explicitly defines 3D geometry, and enables multi-view rendering or 3D reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>生成的场景\(\mathbf{S}\)在空间上是一致的，隐式或显式地定义了3D几何，并支持多视图渲染或3D重建。</p></div><h3>2.2 3D Scene Representations</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>2.2 3D场景表示</h3></div><p>Various 3D scene representations have been developed and utilized in computer vision and graphics. In this section, we provide an overview of the key \(3\mathrm{D}\) scene representations, discussing their structures, properties, and suitability for 3D scene generation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>各种3D场景表示已在计算机视觉和图形学中开发和利用。在本节中，我们提供关键\(3\mathrm{D}\)场景表示的概述，讨论它们的结构、属性及其在3D场景生成中的适用性。</p></div><p>Voxel Grid. A voxel grid is a 3D array \(\mathbf{V} \in  {\mathbb{R}}^{H \times  W \times  D}\) , where \(H,W\) ,and \(D\) represent the height,width,and depth of the grid, respectively. Each voxel stores properties such as occupancy or signed distance values [65], enabling structured volumetric scene representation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>体素网格。体素网格是一个3D数组\(\mathbf{V} \in  {\mathbb{R}}^{H \times  W \times  D}\)，其中\(H,W\)、\(D\)分别表示网格的高度、宽度和深度。每个体素存储占用或有符号距离值等属性[65]，使得结构化体积场景表示成为可能。</p></div><p>Point Cloud. A point cloud is an unordered set of \(N\) 3D points \(\mathbf{P} = {\left\{  {\mathbf{p}}_{i} \mid  {\mathbf{p}}_{i} \in  {\mathbb{R}}^{3}\right\}  }_{i = 1}^{N}\) that approximates an object’s surface. Unlike voxel grids, point clouds are sparse, unstructured, memory-efficient, and are commonly generated from depth sensors, LiDAR, and structure-from-motion [66].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>点云。点云是一个无序的\(N\) 3D点集合\(\mathbf{P} = {\left\{  {\mathbf{p}}_{i} \mid  {\mathbf{p}}_{i} \in  {\mathbb{R}}^{3}\right\}  }_{i = 1}^{N}\)，近似于物体的表面。与体素网格不同，点云是稀疏的、无结构的、内存高效的，通常由深度传感器、激光雷达和运动重建生成[66]。</p></div><p>Mesh. A polygonal mesh \(\mathbf{M} = \left\{  {{\mathbf{M}}_{V},{\mathbf{M}}_{E},{\mathbf{M}}_{F}}\right\}\) defines a 3D surface through vertices \({\mathbf{M}}_{V}\) (points in space),edges \({\mathbf{M}}_{E}\) (pairwise connections between vertices),and faces \({\mathbf{M}}_{F}\) (flat polygons, such as triangles or quads). It provides explicit connectivity information, making them ideal for modeling the surfaces of \(3\mathrm{D}\) scenes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>网格。多边形网格\(\mathbf{M} = \left\{  {{\mathbf{M}}_{V},{\mathbf{M}}_{E},{\mathbf{M}}_{F}}\right\}\)通过顶点\({\mathbf{M}}_{V}\)（空间中的点）、边\({\mathbf{M}}_{E}\)（顶点之间的成对连接）和面\({\mathbf{M}}_{F}\)（平面多边形，如三角形或四边形）定义3D表面。它提供明确的连接信息，使其非常适合建模\(3\mathrm{D}\)场景的表面。</p></div><p>Neural Fields. Signed Distance Field (SDF) [67] and Neural Radiance Field (NeRF) [31] are continuous implicit functions that can be parameterized by neural networks. SDF maps a spatial position \(\mathbf{x} \in  {\mathbb{R}}^{3}\) to a signed distance \(s\left( \mathbf{x}\right)  \in  \mathbb{R}\) ,defining a surface as its zero-level set. NeRF maps \(\mathbf{x}\) and a view direction \(\mathbf{r} \in  {\mathbb{R}}^{3}\) to a volume density \(\sigma \left( {\mathbf{x},\mathbf{r}}\right)  \in  {\mathbb{R}}^{ + }\) and color \(\mathbf{c}\left( {\mathbf{x},\mathbf{r}}\right)  \in  {\mathbb{R}}^{3}\) . SDF is rendered using sphere tracing [68], while NeRF uses differentiable volume rendering [69], [70].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>神经场。带符号距离场（SDF）[67]和神经辐射场（NeRF）[31]是可以通过神经网络参数化的连续隐式函数。SDF将空间位置\(\mathbf{x} \in  {\mathbb{R}}^{3}\)映射到带符号距离\(s\left( \mathbf{x}\right)  \in  \mathbb{R}\)，将表面定义为其零水平集。NeRF将\(\mathbf{x}\)和视角方向\(\mathbf{r} \in  {\mathbb{R}}^{3}\)映射到体积密度\(\sigma \left( {\mathbf{x},\mathbf{r}}\right)  \in  {\mathbb{R}}^{ + }\)和颜色\(\mathbf{c}\left( {\mathbf{x},\mathbf{r}}\right)  \in  {\mathbb{R}}^{3}\)。SDF使用球体追踪[68]进行渲染，而NeRF使用可微分体积渲染[69]，[70]。</p></div><p>3D Gaussians. 3D Gaussians [32] represent 3D scenes using \(N\) 3D Gaussian primitives \(\mathbf{G} = {\left\{  \left( {\mu }_{i},{\sum }_{i},{\mathbf{c}}_{i},{\alpha }_{i}\right) \right\}  }_{i = 1}^{N}\) ,where \({\mu }_{i} \in  {\mathbb{R}}^{3}\) is the center, \({\sum }_{i} \in  {\mathbb{R}}^{3 \times  3}\) defines the anisotropic shape, \({\mathbf{c}}_{i} \in  {\mathbb{R}}^{3}\) is the RGB color,and \({\alpha }_{i} \in  \left\lbrack  {0,1}\right\rbrack\) is the opacity. The image can be rendered by rasterizing 3D Gaussians onto a 2D plane.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D高斯。3D高斯[32]使用\(N\) 3D高斯原语\(\mathbf{G} = {\left\{  \left( {\mu }_{i},{\sum }_{i},{\mathbf{c}}_{i},{\alpha }_{i}\right) \right\}  }_{i = 1}^{N}\)表示3D场景，其中\({\mu }_{i} \in  {\mathbb{R}}^{3}\)是中心，\({\sum }_{i} \in  {\mathbb{R}}^{3 \times  3}\)定义各向异性形状，\({\mathbf{c}}_{i} \in  {\mathbb{R}}^{3}\)是RGB颜色，\({\alpha }_{i} \in  \left\lbrack  {0,1}\right\rbrack\)是不透明度。图像可以通过将3D高斯光栅化到2D平面上进行渲染。</p></div><p>Image Sequence. An image sequence, implicitly encoding the scene’s 3D structure with \(N\) images from different viewpoints,e.g., \(\mathbf{C} = {\left\{  {\mathbf{I}}_{i} \in  {\mathbb{R}}^{H \times  W \times  3}\right\}  }_{i = 1}^{N}\) ,is a crucial \(3\mathrm{D}\) scene representation widely used in image- and video-based generation methods,where the \(3\mathrm{D}\) structure can be inferred through multi-view reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图像序列。图像序列隐式编码场景的3D结构，包含来自不同视角的\(N\)图像，例如\(\mathbf{C} = {\left\{  {\mathbf{I}}_{i} \in  {\mathbb{R}}^{H \times  W \times  3}\right\}  }_{i = 1}^{N}\)，是图像和视频生成方法中广泛使用的关键\(3\mathrm{D}\)场景表示，其中\(3\mathrm{D}\)结构可以通过多视图重建推断。</p></div><h3>2.3 Generative Models</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>2.3 生成模型</h3></div><p>Generative models synthesize data by either learning statistical patterns (e.g., AR models, VAEs [71], GANs [29], diffusion models [30]) or applying predefined rules (e.g., procedural generators). While the former approximates data distributions for novel outputs, the latter constructs structured 3D scenes through deterministic or stochastic logic without learned priors. In this section, we briefly introduce representative generative models in \(3\mathrm{D}\) scene generation, highlighting their characteristics and mechanisms.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>生成模型通过学习统计模式（例如，AR模型、变分自编码器（VAEs）[71]、生成对抗网络（GANs）[29]、扩散模型[30]）或应用预定义规则（例如，过程生成器）来合成数据。前者近似数据分布以生成新输出，而后者通过确定性或随机逻辑构建结构化的3D场景，而不依赖于学习的先验。在本节中，我们简要介绍在\(3\mathrm{D}\)场景生成中具有代表性的生成模型，突出它们的特征和机制。</p></div><p>Autoregressive Models (AR models) generate data sequentially, where each element is conditioned on the previously generated elements. A common formalization of AR models involves the factorization of the joint probability distribution of data into a product of conditional probabilities \(p\left( \mathbf{x}\right)  = \mathop{\prod }\limits_{{t = 1}}^{T}p\left( {{\mathbf{x}}_{\mathbf{t}} \mid  {\mathbf{x}}_{ &#x3C; \mathbf{t}}}\right)\) . This decomposition follows directly from the chain rule of probability and ensures that each element \({\mathbf{x}}_{\mathbf{t}}\) is generated sequentially,conditioned on all previous elements. The probability \(p\left( {{\mathbf{x}}_{\mathbf{t}} \mid  {\mathbf{x}}_{ &#x3C; \mathbf{t}}}\right)\) is modeled by deep generative networks [72], [73], which learn to capture the dependencies between the data.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>自回归模型（AR模型）顺序生成数据，其中每个元素都依赖于先前生成的元素。AR模型的常见形式化涉及将数据的联合概率分布分解为条件概率的乘积\(p\left( \mathbf{x}\right)  = \mathop{\prod }\limits_{{t = 1}}^{T}p\left( {{\mathbf{x}}_{\mathbf{t}} \mid  {\mathbf{x}}_{ &#x3C; \mathbf{t}}}\right)\)。这种分解直接遵循概率的链式法则，确保每个元素\({\mathbf{x}}_{\mathbf{t}}\)是顺序生成的，依赖于所有先前的元素。概率\(p\left( {{\mathbf{x}}_{\mathbf{t}} \mid  {\mathbf{x}}_{ &#x3C; \mathbf{t}}}\right)\)由深度生成网络[72]，[73]建模，这些网络学习捕捉数据之间的依赖关系。</p></div><p>Variational Autoencoders (VAEs) [71] are generative models that encode data into a probabilistic latent space and decode it back. Given an input \(\mathbf{x}\) ,the encoder maps it to a latent distribution \(q\left( {\mathbf{z} \mid  \mathbf{x}}\right)\) parameterized by a mean \(\mu\) and variance \({\sigma }^{2}\) ,where \(\mathbf{z} = \mu  + \sigma  \cdot  \epsilon ,\;\epsilon  \sim  \mathcal{N}\left( {0,I}\right)\) . The decoder reconstructs \(\mathbf{x}\) from \(\mathbf{z}\) . Using the reparameterization trick, VAEs enable backpropagation through stochastic sampling. The loss function combines reconstruction loss (to preserve input features) and KL divergence (to regularize the latent space), which allows VAEs to generate smooth and meaningful data variations. However, since VAEs optimize likelihood, they often spread probability mass beyond the true data manifold, causing blurry and less detailed generated samples [74], [75].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>变分自编码器（VAEs）[71] 是一种生成模型，它将数据编码到一个概率潜在空间中并将其解码回来。给定输入 \(\mathbf{x}\)，编码器将其映射到一个由均值 \(\mu\) 和方差 \({\sigma }^{2}\) 参数化的潜在分布 \(q\left( {\mathbf{z} \mid  \mathbf{x}}\right)\)，其中 \(\mathbf{z} = \mu  + \sigma  \cdot  \epsilon ,\;\epsilon  \sim  \mathcal{N}\left( {0,I}\right)\)。解码器从 \(\mathbf{z}\) 重建 \(\mathbf{x}\)。通过重参数化技巧，VAEs 使得通过随机采样进行反向传播成为可能。损失函数结合了重建损失（以保留输入特征）和 KL 散度（以规范化潜在空间），这使得 VAEs 能够生成平滑且有意义的数据变体。然而，由于 VAEs 优化似然性，它们往往将概率质量扩展到真实数据流形之外，导致生成的样本模糊且细节较少 [74]，[75]。</p></div><p>Generative Adversarial Networks (GANs) [29] consist of two networks - the Generator \(\mathcal{G}\) and the Discriminator \(\mathcal{D}\) - that compete in a minimax game. The Generator \(\mathcal{G}\) takes random noise \(\mathbf{z}\) and generates fake data \(\mathcal{G}\left( \mathbf{z}\right)\) ,while the Discriminator \(\mathcal{D}\) tries to distinguish real data \(\mathbf{x}\) from fake data \(\mathcal{G}\left( \mathbf{z}\right)\) . The objective is to optimize the Generator to create realistic data that the Discriminator cannot distinguish from real data, and to train the Discriminator to classify real and fake data correctly, which can be represented by the objective function</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>生成对抗网络（GANs）[29] 由两个网络组成——生成器 \(\mathcal{G}\) 和判别器 \(\mathcal{D}\)——它们在一个极小极大游戏中竞争。生成器 \(\mathcal{G}\) 接收随机噪声 \(\mathbf{z}\) 并生成假数据 \(\mathcal{G}\left( \mathbf{z}\right)\)，而判别器 \(\mathcal{D}\) 则试图区分真实数据 \(\mathbf{x}\) 和假数据 \(\mathcal{G}\left( \mathbf{z}\right)\)。目标是优化生成器以创建判别器无法区分的真实数据，并训练判别器正确分类真实和假数据，这可以通过目标函数表示。</p></div><p></p>\[\mathop{\min }\limits_{\mathcal{G}}\mathop{\max }\limits_{\mathcal{D}}{\mathbb{E}}_{\mathbf{x} \sim  {p}_{\text{data }}\left( \mathbf{x}\right) }\left\lbrack  {\log \mathcal{D}\left( \mathbf{x}\right) }\right\rbrack   + {\mathbb{E}}_{\mathbf{z} \sim  {p}_{z}\left( \mathbf{z}\right) }\left\lbrack  {\log \left( {1 - \mathcal{D}\left( {\mathcal{G}\left( \mathbf{z}\right) }\right) }\right) }\right\rbrack\]<p></p><p>(2)</p><p>where \({p}_{\text{data }}\left( \mathbf{x}\right)\) is the real data distribution and \({p}_{z}\left( \mathbf{z}\right)\) is the random noise distribution. A key drawback of GANs is that they can be difficult to train, often suffering from issues like mode collapse and instability [76].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \({p}_{\text{data }}\left( \mathbf{x}\right)\) 是真实数据分布，\({p}_{z}\left( \mathbf{z}\right)\) 是随机噪声分布。GANs 的一个主要缺点是它们可能难以训练，常常面临模式崩溃和不稳定等问题 [76]。</p></div><p>Diffusion Models [30] are generative models that operate by gradually adding noise to data in a forward process, transforming it into pure noise, and then learning to reverse this process by denoising to recover the original data. The forward process is modeled as a Markov chain, where each step \({\mathbf{x}}_{t}\) is obtained by adding Gaussian noise to the previous step \({\mathbf{x}}_{t - 1}\) ,defined by \({\mathbf{x}}_{t} = \sqrt{1 - {\beta }_{t}}{\mathbf{x}}_{t - 1} + \sqrt{{\beta }_{t}}{\epsilon }_{t}\) ,where \({\epsilon }_{t}\) is Gaussian noise and \({\beta }_{t}\) controls the noise schedule. The reverse process aims to model \(p\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{t}}\right)\) ,learning how to reverse the added noise and regenerate the original data. While these models generate high-quality data and are more stable than GANs, they are computationally expensive and slow due to the iterative denoising process [77].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>扩散模型 [30] 是一种生成模型，通过在前向过程中逐渐向数据添加噪声，将其转化为纯噪声，然后学习通过去噪来逆转这一过程以恢复原始数据。前向过程被建模为马尔可夫链，其中每一步 \({\mathbf{x}}_{t}\) 是通过向前一步 \({\mathbf{x}}_{t - 1}\) 添加高斯噪声获得的，定义为 \({\mathbf{x}}_{t} = \sqrt{1 - {\beta }_{t}}{\mathbf{x}}_{t - 1} + \sqrt{{\beta }_{t}}{\epsilon }_{t}\)，其中 \({\epsilon }_{t}\) 是高斯噪声，\({\beta }_{t}\) 控制噪声调度。反向过程旨在建模 \(p\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{t}}\right)\)，学习如何逆转添加的噪声并再生原始数据。虽然这些模型生成高质量数据且比 GANs 更稳定，但由于迭代去噪过程，它们在计算上是昂贵且缓慢的 [77]。</p></div><p>Procedural Generators [44] are algorithmic systems that synthesize 3D scenes through iterative application of parametric rules and mathematical operations. These generators transform an initial state \({\mathbf{S}}_{0}\) (e.g.,a geometric primitive or empty scene) into a structured output \({\mathbf{S}}_{n}\) via recursive or iterative processes governed by \({\mathbf{S}}_{t + 1} = \mathcal{R}\left( {{\mathbf{S}}_{t},\Theta }\right)\) ,where</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>程序生成器 [44] 是通过参数规则和数学运算的迭代应用合成 3D 场景的算法系统。这些生成器通过受 \({\mathbf{S}}_{t + 1} = \mathcal{R}\left( {{\mathbf{S}}_{t},\Theta }\right)\) 支配的递归或迭代过程，将初始状态 \({\mathbf{S}}_{0}\)（例如，一个几何原始体或空场景）转化为结构化输出 \({\mathbf{S}}_{n}\)，其中</p></div><!-- Media --><p>TABLE 1</p><p>General comparison of 3D scene generation categories across key characteristics. Individual methods may vary</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>对 3D 场景生成类别在关键特征上的一般比较。各个方法可能有所不同。</p></div><table><tbody><tr><td>Characteristic</td><td>Procedural Gen.</td><td>Neural 3D-based Gen.</td><td>Image-based Gen.</td><td>Video-based Gen.</td></tr><tr><td>Realism</td><td>★★☆: Stylized or repeti- tive textures</td><td>★★☆: Limited by the quality of 3D datasets</td><td>★★★: Photorealistic but lacks accurate depth</td><td>★★★: High-quality tem- poral coherence</td></tr><tr><td>Diversity</td><td>★☆☆: Limited variations due to predefined assets</td><td>★★☆: Diversity depends on training data</td><td>★★★: Rich variations from real-world images</td><td>★★★: Rich variations from real-world videos</td></tr><tr><td>View Consistency</td><td>★★★：3D-consistent rep- resentations/rendering</td><td>★★★: 3D-consistent rep- resentations</td><td>★★☆: Usually adopts ex- plicit 3D representation</td><td>★☆☆: Implicit geometry estimation, less reliable</td></tr><tr><td>Semantic Consistency</td><td>★★★: Procedure ensures cross-view coherence</td><td>★★★: 3D priors preserve cross-view coherence</td><td>★★☆: No global context; lack cross-view coherence</td><td>★★☆: Frame-level coher- ence but possible drift</td></tr><tr><td>Efficiency</td><td>★☆☆: Usually slow; can be faster for lower quality</td><td>★☆☆: Costly due to com- plex representations</td><td>★★☆: Efficient per frame but lacks reuse</td><td>★★☆: Costly due to se- quential inference</td></tr><tr><td>Controllability</td><td>★☆☆: Limited by prede- fined rules or constraints</td><td>★☆☆: Rarely support text or image conditions</td><td>★★☆: Controlled mainly by text or images</td><td>★★★: Controlled by di- verse conditions</td></tr><tr><td>Physical Plausibility</td><td>★★★: Guaranteed by physics engines</td><td>★★☆: Constrained by 3D geometry</td><td>★☆☆: Hard to infer from the static context</td><td>★☆☆: Achieved through temporal modeling</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>特征</td><td>过程生成</td><td>基于神经网络的3D生成</td><td>基于图像的生成</td><td>基于视频的生成</td></tr><tr><td>真实感</td><td>★★☆: 风格化或重复的纹理</td><td>★★☆: 受限于3D数据集的质量</td><td>★★★: 照片级真实感但缺乏准确的深度</td><td>★★★: 高质量的时间一致性</td></tr><tr><td>多样性</td><td>★☆☆: 由于预定义资产而有限的变化</td><td>★★☆: 多样性依赖于训练数据</td><td>★★★: 来自真实世界图像的丰富变化</td><td>★★★: 来自真实世界视频的丰富变化</td></tr><tr><td>视图一致性</td><td>★★★：3D一致的表示/渲染</td><td>★★★: 3D一致的表示</td><td>★★☆: 通常采用显式3D表示</td><td>★☆☆: 隐式几何估计，可靠性较低</td></tr><tr><td>语义一致性</td><td>★★★: 过程确保跨视图一致性</td><td>★★★: 3D先验保持跨视图一致性</td><td>★★☆: 无全局上下文；缺乏跨视图一致性</td><td>★★☆: 帧级一致性但可能漂移</td></tr><tr><td>效率</td><td>★☆☆: 通常较慢；对于低质量可以更快</td><td>★☆☆: 由于复杂表示而成本高</td><td>★★☆: 每帧效率高但缺乏重用</td><td>★★☆: 由于顺序推理而成本高</td></tr><tr><td>可控性</td><td>★☆☆: 受限于预定义规则或约束</td><td>★☆☆: 很少支持文本或图像条件</td><td>★★☆: 主要通过文本或图像进行控制</td><td>★★★: 通过多样化条件进行控制</td></tr><tr><td>物理合理性</td><td>★★★: 由物理引擎保证</td><td>★★☆: 受限于3D几何</td><td>★☆☆: 难以从静态上下文中推断</td><td>★☆☆: 通过时间建模实现</td></tr></tbody></table></div><!-- Media --><p>\(\mathcal{R}\) represents a set of predefined rules (e.g.,subdivision, perturbation,or spatial partitioning),and \(\Theta\) denotes tunable parameters (e.g., seed values, perturbation amplitudes, or recursion depth). The rules \(\mathcal{R}\) define deterministic or constrained stochastic operations, ensuring reproducibility when \(\Theta\) is fixed.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\(\mathcal{R}\)表示一组预定义规则（例如，细分、扰动或空间划分），而\(\Theta\)表示可调参数（例如，种子值、扰动幅度或递归深度）。规则\(\mathcal{R}\)定义了确定性或受限的随机操作，确保在\(\Theta\)固定时的可重复性。</p></div><h2>3 Methods: A HIERARCHICAL TAXONOMY</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>3 方法：一个层次分类法</h2></div><p>We classify existing methods into four categories based on their generation paradigms illustrated in Figures 3 to 6:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们根据图3到图6中所示的生成范式将现有方法分为四类：</p></div><ul>
<li>Procedural Generation creates 3D scenes using predefined rules, enforced constraints, or prior knowledge from LLMs, resulting in high-quality outputs that integrate seamlessly with graphics engines.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>程序生成使用预定义规则、强制约束或来自大型语言模型（LLMs）的先验知识创建3D场景，生成与图形引擎无缝集成的高质量输出。</li>
</ul></div><ul>
<li>Neural 3D-based Generation employs 3D-aware generative architectures to synthesize scene layouts for object placement or directly generate \(3\mathrm{D}\) representations such as voxels, point clouds, meshes, NeRFs, and 3D Gaussians.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>基于神经网络的3D生成采用3D感知生成架构合成物体放置的场景布局或直接生成\(3\mathrm{D}\)表示，如体素、点云、网格、NeRF和3D高斯。</li>
</ul></div><ul>
<li>Image-based Generation uses 2D image generators to synthesize images either in one step or iteratively, sometimes followed by \(3\mathrm{D}\) reconstruction for geometric consistency.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>基于图像的生成使用2D图像生成器合成图像，可以一步完成或迭代进行，有时随后进行\(3\mathrm{D}\)重建以保持几何一致性。</li>
</ul></div><ul>
<li>Video-based Generation uses video generators to create both 3D scenes with spatial movement and \(4\mathrm{D}\) scenes that evolve over time,capturing dynamic changes in both space and time.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>基于视频的生成使用视频生成器创建具有空间运动的3D场景和随时间演变的\(4\mathrm{D}\)场景，捕捉空间和时间的动态变化。</li>
</ul></div><h3>3.1 Procedural Generation</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.1 程序生成</h3></div><p>Procedural generation methods automatically generate \(3\mathrm{D}\) scenes by following predefined rules or constraints. They are widely used in computer graphics to create diverse environments, including terrains, vegetation, rivers, roads, rooms, buildings, and entire cities. As shown in Table 1, procedural generation methods offer high efficiency and spatial consistency, but often require careful tuning to achieve realism and user control. The paradigms of these methods are illustrated in Figure 3, which can be further categorized into rule-based, optimization-based, and LLM-based generation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>程序生成方法通过遵循预定义规则或约束自动生成\(3\mathrm{D}\)场景。它们广泛应用于计算机图形学中，以创建多样化的环境，包括地形、植被、河流、道路、房间、建筑和整个城市。如表1所示，程序生成方法提供了高效率和空间一致性，但通常需要仔细调整以实现真实感和用户控制。这些方法的范式在图3中进行了说明，可以进一步分为基于规则、基于优化和基于LLM的生成。</p></div><!-- Media --><!-- figureText: Rules 3D Scene Grammar Representation Ret. 3D Scene Representation Layout SG. / Ret 3D Scene </>言 Representation Code Parameters Interaction Fractal 口 Parameters Optm Interaction Constraints C Text Layout --><img src="https://cdn.noedgeai.com/bo_d163kf3ef24c73d1lc90_3.jpg?x=909&#x26;y=716&#x26;w=751&#x26;h=398&#x26;r=0"><p>Fig. 3. The paradigms of procedural methods for 3D scene generation. (a) Rule-based generation methods follow predefined rules to generate \(3\mathrm{D}\) scenes. (b) Optimization-based generation finds an optimized scene under different constraints. (c) LLM-based generation uses large language models (LLMs) for tasks like layout design and object selection, or to generate code that controls other generators. Note that dashed arrows denote optional operations. "Optm.", "Ret.", and "SG." denote "Optimization", "Retrieval", and "Shape Generation", respectively. "Interaction" refers to user actions such as click, drag, or selection during the generation process.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3. 3D场景生成的程序方法范式。(a) 基于规则的生成方法遵循预定义规则生成\(3\mathrm{D}\)场景。(b) 基于优化的生成在不同约束下寻找优化场景。(c) 基于LLM的生成使用大型语言模型（LLMs）进行布局设计和物体选择等任务，或生成控制其他生成器的代码。请注意，虚线箭头表示可选操作。“Optm.”、“Ret.”和“SG.”分别表示“优化”、“检索”和“形状生成”。“交互”指的是在生成过程中用户的操作，如点击、拖动或选择。</p></div><!-- Media --><h4>3.1.1 Rule-based Generation</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.1.1 基于规则的生成</h4></div><p>Rule-based procedural generation encompasses a range of approaches that construct \(3\mathrm{D}\) scenes through explicit rules and algorithms. These methods directly generate scene geometry, which is then rendered for visualization. Common techniques include fractal-based, grammar-based, simulation-driven, and example-based generation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于规则的程序生成包含一系列通过显式规则和算法构建\(3\mathrm{D}\)场景的方法。这些方法直接生成场景几何，然后进行可视化渲染。常见技术包括基于分形、基于语法、驱动模拟和基于示例的生成。</p></div><p>Fractals [121], [122], [123] are mathematical structures that exhibit self-similarity across scales. Fractal-based methods are widely applied in terrain modeling and texture synthesis, as they efficiently generate visually complex patterns while requiring minimal storage. Techniques such as midpoint displacement [124], [125] and fractional Brownian motion [126] (fBM) generate multi-scale details that resemble natural landscapes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>分形[121]、[122]、[123]是展示自相似性的数学结构。基于分形的方法广泛应用于地形建模和纹理合成，因为它们能够高效生成视觉上复杂的图案，同时占用最小的存储空间。中点位移[124]、[125]和分数布朗运动[126]（fBM）等技术生成与自然景观相似的多尺度细节。</p></div><p>Grammar-based methods consist of an alphabet of symbols, an initial axiom, and a set of rewriting rules. Each generated symbol encodes geometric commands for complex shape generation. CityEngine [3] extends L-systems [127] for the generation of road networks and building geometry to create cities. Müller et al. [6] build upon shape grammars [128] to model highly detailed 3D buildings.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于语法的方法由符号字母表、初始公理和一组重写规则组成。每个生成的符号编码用于复杂形状生成的几何命令。CityEngine[3]扩展了L系统[127]，用于生成道路网络和建筑几何以创建城市。Müller等人[6]在形状语法[128]的基础上建模高度详细的3D建筑。</p></div><!-- Media --><p>TABLE 2</p><p>Summary and comparison of representative works for 3D scene generation. The table compares scene types (e.g., I indoor, N nature, U urban) and conditioning modalities (e.g., x unconditioned, T text, T image, Io top-down image, c constraint, M motion, G scene graph, V semantic volume, \({s}_{0}\) semantic map, \(\mathrm{L}\) LiDAR, \(\mathrm{B}\) bounding box, \(\mathrm{c}\) camera pose, \(\mathrm{A}\) user action) across various \(3\mathrm{D}\) scene representations. Note that "Optm.", "Gen.", "Rep.", and "Seq." are short for "Optimization", "Generative", "Representation", and "Sequence", respectively.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D场景生成的代表性工作的总结与比较。该表比较了场景类型（例如，I室内，N自然，U城市）和条件模式（例如，x无条件，T文本，T图像，Io自上而下图像，c约束，M运动，G场景图，V语义体积，\({s}_{0}\)语义地图，\(\mathrm{L}\)激光雷达，\(\mathrm{B}\)边界框，\(\mathrm{c}\)相机姿态，\(\mathrm{A}\)用户操作）在各种\(3\mathrm{D}\)场景表示中的比较。请注意，“Optm.”、“Gen.”、“Rep.”和“Seq.”分别是“优化”、“生成”、“表示”和“序列”的缩写。</p></div><table><tbody><tr><td colspan="2">Category</td><td>Method</td><td>Venue</td><td>Gen. Model</td><td>Scene Type</td><td>Condition</td><td>3D Scene Rep.</td></tr><tr><td rowspan="11">Procedural</td><td rowspan="4">Rule-based</td><td>Musgrave et al. [78]</td><td>SIGGRAPH’89</td><td>Procedural</td><td>\( \mathrm{N} \)</td><td>✘</td><td>Mesh</td></tr><tr><td>CityEngine [3]</td><td>SIGGRAPH’01</td><td>Procedural</td><td>U</td><td>\( {\mathrm{I}}_{\mathrm{D}} \)</td><td>Mesh</td></tr><tr><td>Cordonnier et al. [79]</td><td>TOG’17</td><td>Procedural</td><td>\( \mathrm{N} \)</td><td>V</td><td>Mesh</td></tr><tr><td>Infinigen [80]</td><td>CVPR’23</td><td>Procedural</td><td>\( \mathrm{N} \)</td><td>✘</td><td>Mesh</td></tr><tr><td rowspan="4">Optm.-based</td><td>Make it home [28]</td><td>TOG’11</td><td>Procedural</td><td>I</td><td>✘</td><td>Mesh</td></tr><tr><td>Wu et al. [27]</td><td>CGF’18</td><td>Procedural</td><td>I</td><td>C</td><td>Mesh</td></tr><tr><td>ProcTHOR [15]</td><td>NeurIPS’22</td><td>Procedural</td><td>I</td><td>\( {\mathrm{I}}_{\mathrm{D}} \)</td><td>Mesh</td></tr><tr><td>Infinigen Indoors [81]</td><td>CVPR’24</td><td>Procedural</td><td>I</td><td>C</td><td>Mesh</td></tr><tr><td rowspan="3">LLM-based</td><td>LayoutGPT [82]</td><td>NeurIPS’23</td><td>Procedural</td><td>I</td><td>T</td><td>Mesh</td></tr><tr><td>3D-GPT [83]</td><td>3DV’25</td><td>Procedural</td><td>\( \mathrm{N} \)</td><td>T</td><td>Mesh</td></tr><tr><td>Scene \( \mathcal{X}\left\lbrack  {84}\right\rbrack \)</td><td>AAAI’25</td><td>Procedural</td><td>\( \mathrm{{NU}} \)</td><td>T</td><td>Mesh</td></tr><tr><td rowspan="21">Neural 3D-based</td><td rowspan="4">Scene Parameters</td><td>DeepSynth [85]</td><td>TOG’18</td><td>GAN</td><td>I</td><td>\( {\mathrm{I}}_{\mathrm{D}} \)</td><td>Mesh</td></tr><tr><td>ATISS [86]</td><td>NeurIPS’21</td><td>Autoregressive</td><td>I</td><td>\( {\mathrm{I}}_{\mathrm{D}} \)</td><td>Mesh</td></tr><tr><td>MIME [87]</td><td>CVPR’23</td><td>Autoregressive</td><td>I</td><td>\( {\mathrm{I}}_{\mathrm{D}} + \mathrm{M} \)</td><td>Mesh</td></tr><tr><td>DiffuScene [88]</td><td>CVPR’24</td><td>Diffusion</td><td>I</td><td>T</td><td>Mesh</td></tr><tr><td rowspan="5">Scene Graph</td><td>PlanIT [89]</td><td>TOG’19</td><td>Autoregressive</td><td>I</td><td>\( {\mathrm{I}}_{\mathrm{D}} \)</td><td>Mesh</td></tr><tr><td>GRAINS [90]</td><td>TOG’19</td><td>VAE</td><td>I</td><td>✘</td><td>Mesh</td></tr><tr><td>Graph-to-3D [91]</td><td>ICCV’21</td><td>VAE</td><td>I</td><td>G</td><td>SDF</td></tr><tr><td>CommonScenes [92]</td><td>NeurIPS’23</td><td>Diffusion</td><td>I</td><td>G</td><td>Mesh</td></tr><tr><td>InstructScene [93]</td><td>ICLR’24</td><td>Diffusion</td><td>I</td><td>T</td><td>Mesh</td></tr><tr><td rowspan="7">Semantic Layout</td><td>GANcraft [94]</td><td>ICCV’21</td><td>GAN</td><td>\( \mathrm{N} \)</td><td>V</td><td>NeRF</td></tr><tr><td>CC3D [95]</td><td>ICCV’23</td><td>GAN</td><td>\( \mathrm{I} \) U</td><td>\( {\mathrm{S}}_{\mathrm{D}} \)</td><td>NeRF</td></tr><tr><td>InfiniCity [96]</td><td>ICCV’23</td><td>GAN</td><td>U</td><td>✘</td><td>NeRF</td></tr><tr><td>SceneDreamer [97]</td><td>TPAMI’23</td><td>GAN</td><td>\( \mathrm{N} \)</td><td>✘</td><td>NeRF</td></tr><tr><td>CityDreamer [98]</td><td>CVPR’24</td><td>GAN</td><td>U</td><td>✘</td><td>NeRF</td></tr><tr><td>Comp3D [99]</td><td>3DV’24</td><td>Diffusion</td><td>\( \mathrm{N} \)</td><td>\( \mathrm{T} + \mathrm{V} \)</td><td>NeRF</td></tr><tr><td>BlockFusion [100]</td><td>TOG’24</td><td>Diffusion</td><td>\( \mathrm{I}\mathrm{N}\mathrm{l} \)U</td><td>\( {\mathrm{S}}_{\mathrm{D}} \)</td><td>SDF</td></tr><tr><td rowspan="5">Implicit Layout</td><td>GSN [101]</td><td>ICCV’21</td><td>GAN</td><td>I</td><td>x / i</td><td>NeRF</td></tr><tr><td>GAUDI [102]</td><td>NeurIPS’22</td><td>Diffusion</td><td>I</td><td>\( \mathrm{X}/\mathrm{T}/\mathrm{I} \)</td><td>NeRF</td></tr><tr><td>NeuralField-LDM [103]</td><td>CVPR’23</td><td>Diffusion</td><td>U</td><td>\( \mathrm{x}/\mathrm{I} \)</td><td>NeRF</td></tr><tr><td>\( {\mathcal{X}}^{3}\left\lbrack  {104}\right\rbrack \)</td><td>CVPR’24</td><td>VAE&#x26;Diffusion</td><td>U</td><td>X / L</td><td>Voxel Grid</td></tr><tr><td>Director3D [105]</td><td>NeurIPS’24</td><td>Diffusion</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T</td><td>3D Gaussians</td></tr><tr><td rowspan="13">Image-based</td><td rowspan="5">Holistic</td><td>ImmerseGAN [106]</td><td>3DV’22</td><td>GAN</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T / I</td><td>Image Seq.</td></tr><tr><td>MVDiffusion [36]</td><td>NeurIPS’23</td><td>Diffusion</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T</td><td>Image Seq.</td></tr><tr><td>PanFusion [107]</td><td>CVPR’24</td><td>Diffusion</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T</td><td>Image Seq.</td></tr><tr><td>PERF [108]</td><td>TPAMI’24</td><td>Diffusion</td><td>I</td><td>I</td><td>NeRF</td></tr><tr><td>LayerPano3D [109]</td><td>SIGGRAPH’25</td><td>Diffusion</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T</td><td>3D Gaussians</td></tr><tr><td rowspan="8">Iterative</td><td>PixelSynth [110]</td><td>ICCV’21</td><td>VAE</td><td>I</td><td>I</td><td>Point Cloud</td></tr><tr><td>GFVS [111]</td><td>ICCV’21</td><td>GAN</td><td>\( \mathrm{I}\mathrm{N} \)</td><td>I</td><td>Image Seq.</td></tr><tr><td>Infinite Nature [33]</td><td>ICCV’21</td><td>GAN</td><td>\( \mathrm{N} \)</td><td>I</td><td>Image Seq.</td></tr><tr><td>3D Cinemagraphy [112]</td><td>CVPR’23</td><td>GAN</td><td>\( \mathrm{N} \)</td><td>I</td><td>Point Cloud</td></tr><tr><td>Text2Room [113]</td><td>ICCV’23</td><td>Diffusion</td><td>I</td><td>T / I</td><td>Mesh</td></tr><tr><td>Text2NeRF [114]</td><td>CVPR’24</td><td>Diffusion</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T / I</td><td>NeRF</td></tr><tr><td>WonderJourney [115]</td><td>CVPR’24</td><td>Diffusion</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T / I</td><td>Point Cloud</td></tr><tr><td>LucidDreamer [116]</td><td>arXiv’23</td><td>Diffusion</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T / I</td><td>3D Gaussians</td></tr><tr><td rowspan="7">Video-based</td><td rowspan="2">Two-stage</td><td>4Real [117]</td><td>NeurIPS’24</td><td>Diffusion</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T</td><td>3D Gaussians</td></tr><tr><td>DimensionX [42]</td><td>arXiv’24</td><td>Diffusion</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T / I</td><td>3D Gaussians</td></tr><tr><td rowspan="5">One-stage</td><td>MagicDrive [39]</td><td>ICLR’24</td><td>Diffusion</td><td>U</td><td>\( \mathrm{T} + {\mathrm{S}}_{\mathrm{D}} + \mathrm{B} + \mathrm{C} \)</td><td>Image Seq.</td></tr><tr><td>Vista [118]</td><td>NeurIPS’24</td><td>Diffusion</td><td>U</td><td>T / I / A</td><td>Image Seq.</td></tr><tr><td>Gen \( \mathcal{X}\mathrm{D}\left\lbrack  {119}\right\rbrack \)</td><td>ICLR’25</td><td>Diffusion</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>I \( / \) C</td><td>Image Seq.</td></tr><tr><td>4K4DGen [43]</td><td>ICLR’25</td><td>Diffusion</td><td>\( \mathrm{{NU}} \)</td><td>I</td><td>3D Gaussians</td></tr><tr><td>GameGen-X [120]</td><td>ICLR’25</td><td>Diffusion</td><td>\( \mathrm{{NU}} \)</td><td>T / A</td><td>Image Seq.</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td colspan="2">类别</td><td>方法</td><td>场地</td><td>生成模型</td><td>场景类型</td><td>条件</td><td>3D场景表示</td></tr><tr><td rowspan="11">过程生成</td><td rowspan="4">基于规则</td><td>Musgrave等 [78]</td><td>SIGGRAPH’89</td><td>过程生成</td><td>\( \mathrm{N} \)</td><td>✘</td><td>网格</td></tr><tr><td>CityEngine [3]</td><td>SIGGRAPH’01</td><td>过程生成</td><td>U</td><td>\( {\mathrm{I}}_{\mathrm{D}} \)</td><td>网格</td></tr><tr><td>Cordonnier等 [79]</td><td>TOG’17</td><td>过程生成</td><td>\( \mathrm{N} \)</td><td>V</td><td>网格</td></tr><tr><td>Infinigen [80]</td><td>CVPR’23</td><td>过程生成</td><td>\( \mathrm{N} \)</td><td>✘</td><td>网格</td></tr><tr><td rowspan="4">基于优化</td><td>Make it home [28]</td><td>TOG’11</td><td>过程生成</td><td>I</td><td>✘</td><td>网格</td></tr><tr><td>Wu等 [27]</td><td>CGF’18</td><td>过程生成</td><td>I</td><td>C</td><td>网格</td></tr><tr><td>ProcTHOR [15]</td><td>NeurIPS’22</td><td>过程生成</td><td>I</td><td>\( {\mathrm{I}}_{\mathrm{D}} \)</td><td>网格</td></tr><tr><td>Infinigen室内 [81]</td><td>CVPR’24</td><td>过程生成</td><td>I</td><td>C</td><td>网格</td></tr><tr><td rowspan="3">基于LLM</td><td>LayoutGPT [82]</td><td>NeurIPS’23</td><td>过程生成</td><td>I</td><td>T</td><td>网格</td></tr><tr><td>3D-GPT [83]</td><td>3DV’25</td><td>过程生成</td><td>\( \mathrm{N} \)</td><td>T</td><td>网格</td></tr><tr><td>场景 \( \mathcal{X}\left\lbrack  {84}\right\rbrack \)</td><td>AAAI’25</td><td>过程生成</td><td>\( \mathrm{{NU}} \)</td><td>T</td><td>网格</td></tr><tr><td rowspan="21">神经3D生成</td><td rowspan="4">场景参数</td><td>DeepSynth [85]</td><td>TOG’18</td><td>GAN</td><td>I</td><td>\( {\mathrm{I}}_{\mathrm{D}} \)</td><td>网格</td></tr><tr><td>ATISS [86]</td><td>NeurIPS’21</td><td>自回归</td><td>I</td><td>\( {\mathrm{I}}_{\mathrm{D}} \)</td><td>网格</td></tr><tr><td>MIME [87]</td><td>CVPR’23</td><td>自回归</td><td>I</td><td>\( {\mathrm{I}}_{\mathrm{D}} + \mathrm{M} \)</td><td>网格</td></tr><tr><td>DiffuScene [88]</td><td>CVPR’24</td><td>扩散</td><td>I</td><td>T</td><td>网格</td></tr><tr><td rowspan="5">场景图</td><td>PlanIT [89]</td><td>TOG’19</td><td>自回归</td><td>I</td><td>\( {\mathrm{I}}_{\mathrm{D}} \)</td><td>网格</td></tr><tr><td>GRAINS [90]</td><td>TOG’19</td><td>变分自编码器 (VAE)</td><td>I</td><td>✘</td><td>网格</td></tr><tr><td>图到3D [91]</td><td>ICCV’21</td><td>变分自编码器 (VAE)</td><td>I</td><td>G</td><td>隐式距离函数 (SDF)</td></tr><tr><td>CommonScenes [92]</td><td>NeurIPS’23</td><td>扩散</td><td>I</td><td>G</td><td>网格</td></tr><tr><td>InstructScene [93]</td><td>ICLR’24</td><td>扩散</td><td>I</td><td>T</td><td>网格</td></tr><tr><td rowspan="7">语义布局</td><td>GANcraft [94]</td><td>ICCV’21</td><td>GAN</td><td>\( \mathrm{N} \)</td><td>V</td><td>神经辐射场 (NeRF)</td></tr><tr><td>CC3D [95]</td><td>ICCV’23</td><td>GAN</td><td>\( \mathrm{I} \) U</td><td>\( {\mathrm{S}}_{\mathrm{D}} \)</td><td>神经辐射场 (NeRF)</td></tr><tr><td>InfiniCity [96]</td><td>ICCV’23</td><td>GAN</td><td>U</td><td>✘</td><td>神经辐射场 (NeRF)</td></tr><tr><td>SceneDreamer [97]</td><td>TPAMI’23</td><td>GAN</td><td>\( \mathrm{N} \)</td><td>✘</td><td>神经辐射场 (NeRF)</td></tr><tr><td>城市梦想家 [98]</td><td>CVPR’24</td><td>GAN</td><td>U</td><td>✘</td><td>神经辐射场 (NeRF)</td></tr><tr><td>Comp3D [99]</td><td>3DV’24</td><td>扩散</td><td>\( \mathrm{N} \)</td><td>\( \mathrm{T} + \mathrm{V} \)</td><td>神经辐射场 (NeRF)</td></tr><tr><td>块融合 [100]</td><td>TOG’24</td><td>扩散</td><td>\( \mathrm{I}\mathrm{N}\mathrm{l} \)U</td><td>\( {\mathrm{S}}_{\mathrm{D}} \)</td><td>隐式距离函数 (SDF)</td></tr><tr><td rowspan="5">隐式布局</td><td>GSN [101]</td><td>ICCV’21</td><td>GAN</td><td>I</td><td>x / i</td><td>神经辐射场 (NeRF)</td></tr><tr><td>高迪 [102]</td><td>NeurIPS’22</td><td>扩散</td><td>I</td><td>\( \mathrm{X}/\mathrm{T}/\mathrm{I} \)</td><td>神经辐射场 (NeRF)</td></tr><tr><td>神经场-LDM [103]</td><td>CVPR’23</td><td>扩散</td><td>U</td><td>\( \mathrm{x}/\mathrm{I} \)</td><td>神经辐射场 (NeRF)</td></tr><tr><td>\( {\mathcal{X}}^{3}\left\lbrack  {104}\right\rbrack \)</td><td>CVPR’24</td><td>变分自编码器与扩散</td><td>U</td><td>X / L</td><td>体素网格</td></tr><tr><td>Director3D [105]</td><td>NeurIPS’24</td><td>扩散</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T</td><td>3D 高斯分布</td></tr><tr><td rowspan="13">基于图像的</td><td rowspan="5">整体的</td><td>沉浸生成对抗网络 [106]</td><td>3DV’22</td><td>GAN</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T / I</td><td>图像序列</td></tr><tr><td>MVDiffusion [36]</td><td>NeurIPS’23</td><td>扩散</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T</td><td>图像序列</td></tr><tr><td>全景融合 [107]</td><td>CVPR’24</td><td>扩散</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T</td><td>图像序列</td></tr><tr><td>PERF [108]</td><td>TPAMI’24</td><td>扩散</td><td>I</td><td>I</td><td>神经辐射场 (NeRF)</td></tr><tr><td>层全景3D [109]</td><td>SIGGRAPH’25</td><td>扩散</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T</td><td>3D 高斯分布</td></tr><tr><td rowspan="8">迭代的</td><td>像素合成 [110]</td><td>ICCV’21</td><td>变分自编码器 (VAE)</td><td>I</td><td>I</td><td>点云</td></tr><tr><td>GFVS [111]</td><td>ICCV’21</td><td>GAN</td><td>\( \mathrm{I}\mathrm{N} \)</td><td>I</td><td>图像序列</td></tr><tr><td>无限自然 [33]</td><td>ICCV’21</td><td>GAN</td><td>\( \mathrm{N} \)</td><td>I</td><td>图像序列</td></tr><tr><td>3D 动态影像 [112]</td><td>CVPR’23</td><td>GAN</td><td>\( \mathrm{N} \)</td><td>I</td><td>点云</td></tr><tr><td>文本到房间 [113]</td><td>ICCV’23</td><td>扩散</td><td>I</td><td>T / I</td><td>网格</td></tr><tr><td>文本到神经辐射场 [114]</td><td>CVPR’24</td><td>扩散</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T / I</td><td>神经辐射场 (NeRF)</td></tr><tr><td>奇妙之旅 [115]</td><td>CVPR’24</td><td>扩散</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T / I</td><td>点云</td></tr><tr><td>清醒梦者 [116]</td><td>arXiv’23</td><td>扩散</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T / I</td><td>3D 高斯分布</td></tr><tr><td rowspan="7">基于视频的</td><td rowspan="2">两阶段的</td><td>4Real [117]</td><td>NeurIPS’24</td><td>扩散</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T</td><td>3D 高斯分布</td></tr><tr><td>DimensionX [42]</td><td>arXiv’24</td><td>扩散</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>T / I</td><td>3D 高斯分布</td></tr><tr><td rowspan="5">单阶段的</td><td>魔法驱动 [39]</td><td>ICLR’24</td><td>扩散</td><td>U</td><td>\( \mathrm{T} + {\mathrm{S}}_{\mathrm{D}} + \mathrm{B} + \mathrm{C} \)</td><td>图像序列</td></tr><tr><td>视野 [118]</td><td>NeurIPS’24</td><td>扩散</td><td>U</td><td>T / I / A</td><td>图像序列</td></tr><tr><td>生成 \( \mathcal{X}\mathrm{D}\left\lbrack  {119}\right\rbrack \)</td><td>ICLR’25</td><td>扩散</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>I \( / \) C</td><td>图像序列</td></tr><tr><td>4K4D生成 [43]</td><td>ICLR’25</td><td>扩散</td><td>\( \mathrm{{NU}} \)</td><td>I</td><td>3D 高斯分布</td></tr><tr><td>游戏生成-X [120]</td><td>ICLR’25</td><td>扩散</td><td>\( \mathrm{{NU}} \)</td><td>T / A</td><td>图像序列</td></tr></tbody></table></div><!-- Media --><p>Simulation-based procedural generation creates realistic 3D environments by modeling natural and artificial processes. Some methods simulate erosion effects [78], [129], [130] and hydrology [131], [132], [133] to generate terrain with high fidelity. Vegetation simulations model plant growth under resource competition [79], [134], [135] and climate change [136]. In urban contexts, ecosystem-based approach populates cities with vegetation [137], while others simulate city growth and resource distribution to generate settlements that evolve organically over time [138], [139].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于仿真的程序生成通过模拟自然和人工过程创建逼真的3D环境。一些方法模拟侵蚀效应[78]，[129]，[130]和水文[131]，[132]，[133]以生成高保真的地形。植被模拟在资源竞争[79]，[134]，[135]和气候变化[136]下模拟植物生长。在城市环境中，基于生态系统的方法为城市增添植被[137]，而其他方法则模拟城市增长和资源分配，以生成随时间自然演变的定居点[138]，[139]。</p></div><p>Example-based procedural methods are proposed to improve controllability. These techniques take a small user-provided example and generate a larger scene by expanding its boundary [140], [141] or matching features [142], [143]. Inverse procedural generation attempts to provide high-level control over the generation process. These methods apply optimization functions to infer parameters from procedural algorithms [26], [144] or learn a global distribution for scene arrangement [145].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>提出了基于示例的程序方法以提高可控性。这些技术采用用户提供的小示例，通过扩展其边界[140]，[141]或匹配特征[142]，[143]生成更大的场景。逆向程序生成试图提供对生成过程的高级控制。这些方法应用优化函数从程序算法[26]，[144]中推断参数或学习场景布置的全局分布[145]。</p></div><p>The aforementioned techniques are often combined to harness their complementary strengths for generating large-scale, diverse scenes. For example, Citygen [146] integrates road networks and building generation for cityscapes, while Infinigen [80] combines material, terrain, plant, and creature generators for infinite natural scenes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>上述技术通常结合使用，以利用它们的互补优势生成大规模、多样化的场景。例如，Citygen[146]集成了城市景观的道路网络和建筑生成，而Infinigen[80]结合了材料、地形、植物和生物生成器以生成无限的自然场景。</p></div><h4>3.1.2 Optimization-based Generation</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.1.2 基于优化的生成</h4></div><p>Optimization-based generation formulates scene synthesis as an optimization problem that minimizes objectives encoding predefined constraints. These constraints, typically derived from physics rules, functionality, or design principles, are embedded into cost functions and optimized using stochastic or sampling-based methods. Alternatively, statistical approaches learn spatial relationships from data and guide the layout process through probabilistic sampling. Some systems support user-defined constraints and user interactions to enable controllable and semantically meaningful generation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于优化的生成将场景合成公式化为一个优化问题，旨在最小化编码预定义约束的目标。这些约束通常源自物理规则、功能或设计原则，嵌入到成本函数中，并使用随机或基于采样的方法进行优化。或者，统计方法从数据中学习空间关系，并通过概率采样指导布局过程。一些系统支持用户定义的约束和用户交互，以实现可控且语义上有意义的生成。</p></div><p>Some approaches formulate physical and spatial constraints as cost functions and apply stochastic optimization methods for scene generation. Physical-level constraints include object interpenetration, stability, and friction [147]. Layout-level constraints, including functional relationships (e.g., co-occurrence, accessibility), interior design guidelines(e.g., symmetry, alignment, co-circularity), and human behavior patterns, have also been considered [28], [148], [149]. High-level constraints such as scene type, size, and layout can be specified by users [15], [27], [150], enabling more controllable and semantically meaningful scene synthesis. Leveraging existing procedural generation pipelines, Infinigen Indoors [81] introduces a constraint specification API, allowing users to define custom constraints and achieve highly controllable scene generation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>一些方法将物理和空间约束公式化为成本函数，并应用随机优化方法进行场景生成。物理级别的约束包括物体相互穿透、稳定性和摩擦[147]。布局级别的约束，包括功能关系（例如，共现、可达性）、室内设计指南（例如，对称性、对齐、共圆性）和人类行为模式，也被考虑在内[28]，[148]，[149]。用户可以指定场景类型、大小和布局等高级约束[15]，[27]，[150]，从而实现更可控且语义上有意义的场景合成。利用现有的程序生成管道，Infinigen Indoors[81]引入了约束规范API，允许用户定义自定义约束，实现高度可控的场景生成。</p></div><p>Other methods adopt data-driven models to learn object arrangement patterns from annotated data, transforming scene generation into a probabilistic sampling problem. Bayesian networks are commonly used [151], [152], [153] to capture conditional dependencies between objects, while graph-based models [154], [155], [156] model spatial hierarchies or relational structures to improve spatial reasoning and object placement accuracy.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其他方法采用数据驱动模型从标注数据中学习物体排列模式，将场景生成转变为一个概率采样问题。贝叶斯网络通常被使用[151]，[152]，[153]，以捕捉物体之间的条件依赖关系，而基于图的模型[154]，[155]，[156]则建模空间层次或关系结构，以提高空间推理和物体放置的准确性。</p></div><h4>3.1.3 LLM-based Generation</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.1.3 基于大型语言模型的生成</h4></div><p>Large Language Models [157] (LLMs) and Vision-language models [158] (VLMs) have introduced a new paradigm in procedural generation by enabling text-driven scene synthesis, allowing users to specify environments through natural language descriptions, offering greater flexibility and user control over scene design.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>大型语言模型[157]（LLMs）和视觉-语言模型[158]（VLMs）通过实现文本驱动的场景合成，引入了程序生成的新范式，使用户能够通过自然语言描述指定环境，从而提供更大的灵活性和用户对场景设计的控制。</p></div><p>Several approaches use LLMs to generate scene layouts, such as object parameters [82], [159], [160], [161], [162], [163], [164], [165], [166] and scene graph [167], [168], [169], [170], [171], [172]. Based on these layouts, 3D geometries can be obtained through object retrieval or shape generation. Specifically, LayoutGPT [82] guides LLMs using generation prompts and structural templates to produce object parameters for retrieving assets. CityCraft [161] guides land-use planning with LLMs and retrieves building assets from a database to construct detailed urban environments. I-Design [167] and Deng et al. [168] use graph-based object representations to model inter-object semantics more effectively. To support more stylized and versatile scene generation, GraphDreamer [170] and Cube [172] generate scene graphs via LLMs, treating nodes as objects and enabling compositional scene generation through \(3\mathrm{D}\) object generation models. The Scene Language [165] introduces a language-based scene representation composed of a program, words, and embeddings, which can be generated by LLMs and rendered using traditional, neural, or hybrid graphics pipelines.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>几种方法使用LLMs生成场景布局，例如物体参数[82]，[159]，[160]，[161]，[162]，[163]，[164]，[165]，[166]和场景图[167]，[168]，[169]，[170]，[171]，[172]。基于这些布局，可以通过物体检索或形状生成获得3D几何体。具体而言，LayoutGPT[82]使用生成提示和结构模板指导LLMs生成物体参数以检索资产。CityCraft[161]利用LLMs指导土地使用规划，并从数据库中检索建筑资产以构建详细的城市环境。I-Design[167]和邓等[168]使用基于图的物体表示更有效地建模物体间的语义。为了支持更具风格化和多样化的场景生成，GraphDreamer[170]和Cube[172]通过LLMs生成场景图，将节点视为物体，并通过\(3\mathrm{D}\)物体生成模型实现组合场景生成。场景语言[165]引入了一种基于语言的场景表示，由程序、单词和嵌入组成，可以由LLMs生成，并使用传统、神经或混合图形管道进行渲染。</p></div><p>Other methods utilize LLMs as agents to control procedural generation by adjusting parameters of rule-based system or modifying operations within procedural generation software. Liu et al. [173] employ LLMs to fine-tune parameters in rule-based landscape generation, optimizing procedural workflows with learned priors. 3D-GPT [83] and SceneCraft [174] generate Python scripts to control existing procedural frameworks, such as Infinigen [80] and Blender \({}^{1}\) ,allowing direct manipulation of procedural assets. Holodeck [175] generates 3D environment through multiple rounds of conversation with an LLM, including floor and wall texturize, door and window generation, object selection and placement. City \(\mathcal{X}\left\lbrack  {24}\right\rbrack\) and Scene \(\mathcal{X}\left\lbrack  {84}\right\rbrack\) use a multi-agent system for different stages of generation, producing Python codes for layout, terrain, building, and road generation through Blender rendering. WorldCraft [176] further incorporates object generation and animation modules.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其他方法利用大型语言模型（LLMs）作为代理，通过调整基于规则的系统参数或修改程序生成软件中的操作来控制程序生成。刘等人 [173] 使用 LLMs 来微调基于规则的景观生成中的参数，优化具有学习先验的程序工作流程。3D-GPT [83] 和 SceneCraft [174] 生成 Python 脚本以控制现有的程序框架，如 Infinigen [80] 和 Blender \({}^{1}\)，允许直接操作程序资产。Holodeck [175] 通过与 LLM 的多轮对话生成 3D 环境，包括地板和墙壁纹理、门窗生成、物体选择和放置。City \(\mathcal{X}\left\lbrack  {24}\right\rbrack\) 和 Scene \(\mathcal{X}\left\lbrack  {84}\right\rbrack\) 使用多代理系统进行不同阶段的生成，通过 Blender 渲染生成布局、地形、建筑和道路的 Python 代码。WorldCraft [176] 进一步结合了物体生成和动画模块。</p></div><h3>3.2 Neural 3D-based Generation</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.2 基于神经网络的 3D 生成</h3></div><p>Neural 3D-based methods generate 3D scene representations using generative models trained on datasets with 3D annotations. Recent advancements in NeRF and 3D Gaus-sians have further enhanced the fidelity and realism. As shown in Table 1, these methods achieve high view and semantic consistency, but their controllability and efficiency remain limited. As shown in Figure 4, the methods are categorized into four types based on the spatial arrangement that controls the layout of generated \(3\mathrm{D}\) scenes: scene parameters, scene graph, semantic layout, and implicit layout.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于神经网络的 3D 方法使用在具有 3D 注释的数据集上训练的生成模型生成 3D 场景表示。最近在 NeRF 和 3D 高斯模型方面的进展进一步增强了保真度和真实感。如表 1 所示，这些方法实现了高视图和语义一致性，但它们的可控性和效率仍然有限。如图 4 所示，这些方法根据控制生成 \(3\mathrm{D}\) 场景布局的空间排列分为四类：场景参数、场景图、语义布局和隐式布局。</p></div><h4>3.2.1 Scene Parameters</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.2.1 场景参数</h4></div><p>Scene parameters offer a compact way to represent object arrangements, implicitly capturing inter-object relationships without relying on explicit scene graphs. These parameters typically encompass an object's location, size, orientation, class, and shape latent code. As illustrated in Figure 4a, these methods first generate scene parameters as an intermediate representation, which is then used to synthesize the final 3D scene.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>场景参数提供了一种紧凑的方式来表示物体排列，隐式捕捉物体之间的关系，而不依赖于显式的场景图。这些参数通常包括物体的位置、大小、方向、类别和形状潜在编码。如图 4a 所示，这些方法首先生成场景参数作为中间表示，然后用于合成最终的 3D 场景。</p></div><hr>
<!-- Footnote --><ol>
<li><a href="https://www.blender.org/">https://www.blender.org/</a></li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol>
<li><a href="https://www.blender.org/">https://www.blender.org/</a></li>
</ol></div><!-- Footnote -->
<hr><!-- Media --><!-- figureText: AText Parameter Parameters Retrieval 3D Scene Representation Shape Gen Retrieval 3D Scene Dec Shape Gen 3D Scene Neural 3D Representation Generator 3D Scene Dec Representation Image Generator b A Text Enc. Scene Graph Semantic Layout C Text B Text d Enc. Image Implicit Layout --><img src="https://cdn.noedgeai.com/bo_d163kf3ef24c73d1lc90_6.jpg?x=130&#x26;y=119&#x26;w=757&#x26;h=442&#x26;r=0"><p>Fig. 4. The paradigms of neural 3D-based methods for 3D scene generation. These paradigms use (a) scene parameters, (b) scene graphs, (c) semantic layouts, and (d) implicit layouts as intermediate representations to control the spatial arrangement of generated 3D scenes. These representations, either user-provided or produced by generative models, are then converted into 3D scene representations (e.g., voxel grid, mesh, NeRF, or 3D Gaussians) via retrieval or decoding. Note that dashed arrows denote optional operations. "Enc." and "Dec." stand for "Encoder" and "Decoder", respectively. "Shape Gen." represents "Shape Generation".</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图 4. 基于神经网络的 3D 场景生成方法的范式。这些范式使用 (a) 场景参数、(b) 场景图、(c) 语义布局和 (d) 隐式布局作为中间表示来控制生成的 3D 场景的空间排列。这些表示可以是用户提供的或由生成模型生成的，然后通过检索或解码转换为 3D 场景表示（例如，体素网格、网格、NeRF 或 3D 高斯）。请注意，虚线箭头表示可选操作。“Enc.” 和 “Dec.” 分别代表“编码器”和“解码器”。“Shape Gen.” 代表“形状生成”。</p></div><!-- Media --><p>DeepSynth [85], FastSynth [177], Zhang et al. [178], and Sync2Gen [179] adopt CNN-based architectures that utilize top-down image-based scene representations, sequentially inserting objects by predicting their parameters. Subsequent works explore more advanced models, such as transformers and diffusion models. ATISS [86], SceneFormer [180], COFS [181], and Nie et al. [182] use transformers to au-toregressively generate object parameters. RoomDesigner [183] refines this process by decoupling layout and shape generation, ensuring shape compatibility in indoor scenes. CASAGPT [184] leverages cuboids as intermediate object representations to better avoid object collisions. De-BaRA [185] adopts a diffusion model for object parameter generation, while PhyScene [186] further integrates physical constraints for physical plausibility and interactivity.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>DeepSynth [85]、FastSynth [177]、张等人 [178] 和 Sync2Gen [179] 采用基于 CNN 的架构，利用自上而下的基于图像的场景表示，通过预测物体参数逐步插入物体。后续工作探索了更先进的模型，如变换器和扩散模型。ATISS [86]、SceneFormer [180]、COFS [181] 和 Nie 等人 [182] 使用变换器自回归生成物体参数。RoomDesigner [183] 通过解耦布局和形状生成来优化这一过程，确保室内场景中的形状兼容性。CASAGPT [184] 利用立方体作为中间物体表示，以更好地避免物体碰撞。De-BaRA [185] 采用扩散模型生成物体参数，而 PhyScene [186] 进一步整合物理约束以确保物理合理性和交互性。</p></div><p>To improve controllability in text-driven scene generation, RelScene [187] employs BERT [188] to align spatial relationships with textual descriptions in latent space. Dif-fuScene [88] leverages latent diffusion models [189] to generate object parameters from text inputs, followed by object retrieval. Ctrl-Room [190] and SceneFactor [191] employ LDMs to generate coarse object layouts from text prompts, with fine-grained appearance obtained via panorama generation and geometric diffusion model, respectively. Epstein et al. [192], SceneWiz3D [193], and DreamScene [194] adopt a multi-stage approach, first generating an initial object layout, then refining object geometry using Score Distillation Sampling (SDS) [195], followed by a global refinement step to improve compositional consistency.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了提高文本驱动场景生成的可控性，RelScene [187] 使用 BERT [188] 将空间关系与潜在空间中的文本描述对齐。Dif-fuScene [88] 利用潜在扩散模型 [189] 从文本输入生成物体参数，然后进行物体检索。Ctrl-Room [190] 和 SceneFactor [191] 使用 LDMs 从文本提示生成粗略的物体布局，细致的外观则通过全景生成和几何扩散模型分别获得。Epstein 等人 [192]、SceneWiz3D [193] 和 DreamScene [194] 采用多阶段方法，首先生成初始物体布局，然后使用评分蒸馏采样（SDS）[195] 精炼物体几何，最后进行全局精炼步骤以提高组合一致性。</p></div><p>Human movement and interactions often influence the organization of environments, where motion patterns and physical contact inform the arrangement of objects and scene layouts. Pose2Room [196] introduces an end-to-end generative model that predicts the bounding boxes of furniture in a room from human motion. SUMMON [197] and MIME [87] further improve semantic consistency and physical affordances by generating objects with meshes that align with human-scene contact. Vuong et al. [198] propose a multi-conditional diffusion model that integrates text prompts to enhance controllability. To ensure physically plausible layouts free from contact or collisions, INFER-ACT [199] optimizes scene layout generation while simultaneously simulating human movement in a physics-based environment using reinforcement learning.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>人类的运动和互动常常影响环境的组织，其中运动模式和物理接触影响物体的排列和场景布局。Pose2Room [196] 引入了一种端到端的生成模型，该模型根据人类运动预测房间中家具的边界框。SUMMON [197] 和 MIME [87] 通过生成与人类-场景接触对齐的网格物体，进一步提高了语义一致性和物理可用性。Vuong 等 [198] 提出了一个多条件扩散模型，集成文本提示以增强可控性。为了确保物理上合理的布局不受接触或碰撞的影响，INFER-ACT [199] 在使用强化学习的物理基础环境中优化场景布局生成，同时模拟人类运动。</p></div><h4>3.2.2 Scene Graph</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.2.2 场景图</h4></div><p>Scene graphs offer a structured, symbolic representation of 3D scenes, with nodes representing objects and edges capturing their spatial relationships. Incorporating scene graphs allows generative models to enforce spatial constraints and preserve relational consistency, facilitating the creation of well-structured 3D environments. Following the paradigm illustrated in Figure \(4\mathrm{\;b}\) ,scene graphs,whether generated by models or provided as input, function as layout priors that guide the decoding process to create \(3\mathrm{D}\) scene representations by object retrieval or shape generation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>场景图提供了3D场景的结构化符号表示，节点表示物体，边捕捉它们的空间关系。结合场景图使生成模型能够强制执行空间约束并保持关系一致性，从而促进创建结构良好的3D环境。按照图 \(4\mathrm{\;b}\) 中所示的范式，场景图，无论是由模型生成还是作为输入提供，作为布局先验，指导解码过程通过物体检索或形状生成创建 \(3\mathrm{D}\) 场景表示。</p></div><p>Early data-driven approaches [200], [201], [202], [203] represent spatial relationships between objects using scene graphs, which serve as a blueprint for 3D scene generation through object retrieval and placement. Subsequent works enhance graph representations and introduce advanced generative models. PlanIT [89] employs a deep graph generative model to synthesize scene graphs, followed by an image-based network for object instantiation. GRAINS [90] adopts a recursive VAE to learn scene structures as hierarchical graphs, which can be decoded into object bounding boxes. 3D-SLN [204] utilizes scene graphs as a structural prior for 3D scene layout generation, ensuring spatial coherence, and further incorporates differentiable rendering to synthesize realistic images. Meta-Sim [205] and Meta-Sim2 [206] use scene graphs to structure scene generation, optimizing parameters for visual realism and synthesizing diverse \(3\mathrm{D}\) scenes using rendering engines.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>早期的数据驱动方法 [200], [201], [202], [203] 使用场景图表示物体之间的空间关系，场景图作为通过物体检索和放置生成3D场景的蓝图。后续工作增强了图表示并引入了先进的生成模型。PlanIT [89] 采用深度图生成模型合成场景图，随后使用基于图像的网络进行物体实例化。GRAINS [90] 采用递归变分自编码器（VAE）学习场景结构作为层次图，可以解码为物体边界框。3D-SLN [204] 利用场景图作为3D场景布局生成的结构先验，确保空间一致性，并进一步结合可微渲染合成真实图像。Meta-Sim [205] 和 Meta-Sim2 [206] 使用场景图来结构化场景生成，优化视觉真实感的参数，并使用渲染引擎合成多样的 \(3\mathrm{D}\) 场景。</p></div><p>Previous methods enable scene generation from scene graphs but rely on object retrieval or direct synthesis, limiting geometric diversity. To address this, Graph-to-3D [91] introduces a graph-based VAE that jointly optimizes layout and shape. SceneHGN [207] represents scenes as hierarchical graphs spanning from high-level layout to fine-grained object geometry, using a hierarchical VAE for structured generation. CommonScenes [92] and EchoScene [208] propose scene graph diffusion models with a dual-branch design for layout and shape, capturing both global scene-object relationships and local inter-object interactions. MMG-Dreamer [209] introduces a mixed-modality graph for meticulous control of object geometry.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>以前的方法使得可以从场景图生成场景，但依赖于物体检索或直接合成，限制了几何多样性。为了解决这个问题，Graph-to-3D [91] 引入了一种基于图的变分自编码器（VAE），共同优化布局和形状。SceneHGN [207] 将场景表示为从高层布局到细粒度物体几何的层次图，使用层次VAE进行结构化生成。CommonScenes [92] 和 EchoScene [208] 提出了具有双分支设计的场景图扩散模型，用于布局和形状，捕捉全局场景-物体关系和局部物体间交互。MMG-Dreamer [209] 引入了一种混合模态图，以精细控制物体几何。</p></div><p>Recent methods improve controllability by integrating human input. SEK [210] encodes scene knowledge as a scene graph within a conditioned diffusion model for sketch-driven scene generation. InstructScene [93] integrates text encoders with graph-based generative models for text-driven scene synthesis. To generalize scene-graph-based generation to broader scenes, Liu et al. [211] map scene graphs onto a Bird's Eye View (BEV) embedding map, which guides a diffusion model for large-scale outdoor scene synthesis. HiScene [212] leverages VLM-guided occlusion reasoning and video diffusion-based amodal completion to generate editable 3D scenes with compositional object identities from a single isometric view.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最近的方法通过整合人类输入来提高可控性。SEK [210] 将场景知识编码为条件扩散模型中的场景图，用于草图驱动的场景生成。InstructScene [93] 将文本编码器与基于图的生成模型结合，用于文本驱动的场景合成。为了将基于场景图的生成推广到更广泛的场景，Liu 等 [211] 将场景图映射到鸟瞰图（BEV）嵌入图上，指导扩散模型进行大规模户外场景合成。HiScene [212] 利用VLM引导的遮挡推理和基于视频的模态完成，从单一等距视图生成可编辑的3D场景，具有组合物体身份。</p></div><h4>3.2.3 Semantic Layout</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.2.3 语义布局</h4></div><p>Semantic layouts serve as an intermediate representation that encodes the structural and semantic organization of a 3D scene. It provides high-level guidance for 3D scene generation, ensuring controllability and coherence in the placement of objects and scene elements. As shown in Figure 4c, semantic layouts, whether user-provided or generated, act as precise constraints for generative models, guiding 3D scene generation while enabling optional textural prompts for style control.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>语义布局作为一种中间表示，编码3D场景的结构和语义组织。它为3D场景生成提供高层次的指导，确保物体和场景元素的放置具有可控性和一致性。如图4c所示，语义布局，无论是用户提供还是生成的，作为生成模型的精确约束，指导3D场景生成，同时启用可选的纹理提示以控制风格。</p></div><p>A 2D semantic layout consists of a 2D semantic map, sometimes including additional maps such as height maps, viewed from a top-down perspective. CC3D [95] generates a \(3\mathrm{D}\) feature volume conditioned on a \(2\mathrm{D}\) semantic map, which serves as a NeRF for neural rendering. Berf-Scene [213] incorporates positional encoding and low-pass filtering to make the \(3\mathrm{D}\) representation equivariant to the BEV map, enabling controllable and scalable 3D scene generation. Frankenstein [214] encodes scene components into a compact triplane [215], generated via a diffusion process conditioned on a 2D semantic layout. BlockFusion [100] introduces a latent triplane extrapolation mechanism for unbounded scene expansion. Incorporating a height map with the semantic map enables the direct conversion of \(2\mathrm{D}\) layouts into 3D voxel worlds, essential for urban and natural scenes where building structures and terrain elevation provide important priors. InfiniCity [96] utilizes Infinity-GAN [216] to generate infinite-scale 2D layouts, which are then used to create a watertight semantic voxel world, with textures synthesized through neural rendering. For natural scene generation, SceneDreamer [97] employs a neural hash grid to capture generalizable features across various landscapes, modeling a space- and scene-varied hyperspace. To address the diversity of buildings in urban environments, CityDreamer [98] and GaussianCity [217] break down the generation process into distinct background and building components. CityDreamer4D [218] further integrates dynamic traffic systems to generate an expansive \(4\mathrm{D}\) city.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>二维语义布局由二维语义图组成，有时还包括额外的图，如高度图，从俯视角度查看。CC3D [95] 生成一个基于\(2\mathrm{D}\)语义图的\(3\mathrm{D}\)特征体积，作为神经渲染的NeRF。Berf-Scene [213]结合位置编码和低通滤波，使\(3\mathrm{D}\)表示与BEV图等变，从而实现可控和可扩展的3D场景生成。Frankenstein [214]将场景组件编码为紧凑的三平面[215]，通过基于二维语义布局的扩散过程生成。BlockFusion [100]引入了一种潜在三平面外推机制，用于无限场景扩展。将高度图与语义图结合，使得\(2\mathrm{D}\)布局能够直接转换为3D体素世界，这对于建筑结构和地形高度提供重要先验的城市和自然场景至关重要。InfiniCity [96]利用Infinity-GAN [216]生成无限规模的二维布局，然后用于创建一个密闭的语义体素世界，纹理通过神经渲染合成。对于自然场景生成，SceneDreamer [97]采用神经哈希网格捕捉各种景观中的可泛化特征，建模一个空间和场景变化的超空间。为了解决城市环境中建筑的多样性，CityDreamer [98]和GaussianCity [217]将生成过程分解为不同的背景和建筑组件。CityDreamer4D [218]进一步整合动态交通系统，以生成一个广阔的\(4\mathrm{D}\)城市。</p></div><p>A 3D semantic layout offers enhanced capability to represent more complex 3D layouts compared to 2D, improving controllability, typically by using voxels or 3D bounding boxes. GANcraft [94] uses voxels as the 3D semantic layout, optimizing a neural field with pseudo-ground truth and adversarial training. UrbanGIRAFFE [219] and Dis-CoScene [220] break down the scene into stuff, objects, and sky, and adopt compositional neural fields for scene generation. By incorporating score distillation sampling (SDS) [195], 3D semantic layouts offer better control over text-guided scene generation, improving the alignment of generated scenes with textual descriptions. Comp3D [99], CompoNeRF [221], Set-the-Scene [222], and Layout-your- 3D [223] generate 3D scenes with compositional NeRFs using pre-defined customizable layouts as object proxies. SceneCraft [224] and Layout2Scene [225] generate indoor scenes by distilling the pretrained diffusion models. Urban Architect [226] integrates geometric and semantic constraints with SDS, leveraging the scalable hash grid to ensure better view-consistency in urban scene generation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>三维语义布局相比于二维布局提供了更强的能力来表示更复杂的三维布局，提高了可控性，通常通过使用体素或3D边界框来实现。GANcraft [94]使用体素作为三维语义布局，优化一个带有伪真实值和对抗训练的神经场。UrbanGIRAFFE [219]和Dis-CoScene [220]将场景分解为物体、物品和天空，并采用组合神经场进行场景生成。通过结合评分蒸馏采样（SDS）[195]，三维语义布局提供了更好的文本引导场景生成控制，提高了生成场景与文本描述的对齐。Comp3D [99]、CompoNeRF [221]、Set-the-Scene [222]和Layout-your-3D [223]使用预定义的可定制布局作为对象代理，生成带有组合NeRF的3D场景。SceneCraft [224]和Layout2Scene [225]通过蒸馏预训练的扩散模型生成室内场景。Urban Architect [226]将几何和语义约束与SDS结合，利用可扩展的哈希网格确保城市场景生成中的更好视图一致性。</p></div><h4>3.2.4 Implicit Layout</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.2.4 隐式布局</h4></div><p>Implicit layouts are feature maps that encode the spatial structure of a 3D scene. As shown in Figure 4d, these layouts manifest as latent features of different dimensions. Encoders learn to embed 3D scene layout information into latent feature maps, which are then used by the decoder to generate 3D scenes in the form of NeRF, 3D Gaussians, or voxel grids.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>隐式布局是编码三维场景空间结构的特征图。如图4d所示，这些布局表现为不同维度的潜在特征。编码器学习将三维场景布局信息嵌入潜在特征图中，然后解码器使用这些特征图生成以NeRF、3D高斯或体素网格形式的三维场景。</p></div><p>Recent advances in representations like NeRFs and 3D Gaussians have enabled neural networks to directly generate and render high-fidelity RGB images from latent feature maps. Some methods leverage these representations to produce appearance-consistent \(3\mathrm{D}\) scenes with photorealistic quality. NeRF-VAE [227] encodes shared information across multiple scenes using a VAE. GIRAFFE [228] represents scenes as compositional generative neural fields to disentangle objects from background. GSN [101] and Persistent Nature [229] adopt GAN-based architectures to generate 2D latent grids as implicit scene layouts, which are sampled along camera rays to guide NeRF rendering. GAUDI [102] employs a diffusion model to learn scene features and camera poses jointly, decoding them into a tri-plane and pose for NeRF-based rendering control. NeuralField-LDM [103] decomposes NeRF scenes into a hierarchical latent structure that includes 3D voxel, 2D BEV, and 1D global representations. Hierarchical diffusion models are then trained on this tri-latent space for generation. Director3D [105] uses a Gaussian-driven multi-view latent diffusion model to generate pixel-aligned and unbounded 3D Gaussians along a generated trajectory, followed by SDS refinement. Prometheus [230] and SplatFlow [231] learn a compressed latent space from multi-view images, and decode this latent space into pixel-aligned 3DGS representations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最近在NeRF和3D高斯等表示方面的进展使得神经网络能够直接从潜在特征图生成和渲染高保真RGB图像。一些方法利用这些表示生成外观一致的\(3\mathrm{D}\)场景，具有照片级真实感。NeRF-VAE [227]使用VAE编码多个场景之间的共享信息。GIRAFFE [228]将场景表示为组合生成神经场，以解耦物体与背景。GSN [101]和Persistent Nature [229]采用基于GAN的架构生成作为隐式场景布局的二维潜在网格，这些网格沿着相机光线进行采样以指导NeRF渲染。GAUDI [102]采用扩散模型共同学习场景特征和相机姿态，将其解码为三平面和姿态，以控制基于NeRF的渲染。NeuralField-LDM [103]将NeRF场景分解为包括3D体素、2D BEV和1D全局表示的层次潜在结构。然后在这个三潜在空间上训练层次扩散模型以进行生成。Director3D [105]使用高斯驱动的多视图潜在扩散模型生成像素对齐和无限的3D高斯，沿着生成的轨迹进行SDS精炼。Prometheus [230]和SplatFlow [231]从多视图图像中学习压缩的潜在空间，并将该潜在空间解码为像素对齐的3DGS表示。</p></div><p>Another branch of work focuses more on generating semantic structure and scene geometry, typically using voxel grids as representations. These methods are not immediately renderable but can be textured through external rendering pipelines. Lee et al. [232] introduce discrete and latent diffusion models to generate and complete \(3\mathrm{D}\) scenes consisting of multiple objects, represented as semantic voxel grids. Due to the computational challenges posed by voxel grids,DiffInDScene [233],PDD [234], \({\mathcal{X}}^{3}\) [104],and LT3SD [235] use a hierarchical diffusion pipeline to generate large-scale and fine-grained 3D scenes efficiently. SemC-ity [236] employs a tri-plane representation for 3D semantic scenes, allowing for generation and editing by manipulating the tri-plane space during diffusion. NuiScene [237] encodes the local scene chunks into vector sets, and uses a diffusion model to generate neighboring chunks for unbounded outdoor scenes. DynamicCity [238] tackles dynamic scene generation by employing Padded Rollout to unfold Hex-plane [239] into 2D feature maps and applying diffusion for denoising, enabling 4D scene generation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>另一项工作分支更侧重于生成语义结构和场景几何，通常使用体素网格作为表示。这些方法不能立即渲染，但可以通过外部渲染管道进行纹理处理。Lee等人[232]引入离散和潜在扩散模型来生成和完成由多个对象组成的\(3\mathrm{D}\)场景，这些对象表示为语义体素网格。由于体素网格带来的计算挑战，DiffInDScene [233]、PDD [234]、\({\mathcal{X}}^{3}\) [104]和LT3SD [235]使用分层扩散管道高效生成大规模和细粒度的3D场景。SemC-ity [236]采用三平面表示法来处理3D语义场景，通过在扩散过程中操控三平面空间来实现生成和编辑。NuiScene [237]将局部场景块编码为向量集，并使用扩散模型生成无界户外场景的相邻块。DynamicCity [238]通过采用填充展开将六边形平面[239]展开为2D特征图，并应用扩散去噪，解决动态场景生成问题，从而实现4D场景生成。</p></div><h3>3.3 Image-based Generation</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.3 基于图像的生成</h3></div><p>The limited availability of annotated \(3\mathrm{D}\) datasets constrains the generation of \(3\mathrm{D}\) scenes. Image-based generation attempts to bridge the gap between 2D and 3D generation. As shown in Table 1, they offer photorealism and diversity with efficient per-frame processing but struggle with depth accuracy, long-range semantic consistency, and view coherence. The methods fall into two categories: holistic and iterative generation, as illustrated in Figure 5. Holistic generation produces a complete scene image in a single step, while iterative generation gradually expands the scene through extrapolation, generating a sequence of images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>标注的\(3\mathrm{D}\)数据集的有限可用性限制了\(3\mathrm{D}\)场景的生成。基于图像的生成试图弥合2D和3D生成之间的差距。如表1所示，它们提供了照片级真实感和多样性，并且每帧处理效率高，但在深度准确性、长距离语义一致性和视图一致性方面存在困难。这些方法分为两类：整体生成和迭代生成，如图5所示。整体生成在一步中生成完整的场景图像，而迭代生成通过外推逐步扩展场景，生成一系列图像。</p></div><!-- Media --><!-- figureText: Text Image 3D Scene Representation 3D Scene Representation Camera Shift Image Generator b Image Image Generator Render --><img src="https://cdn.noedgeai.com/bo_d163kf3ef24c73d1lc90_8.jpg?x=129&#x26;y=116&#x26;w=762&#x26;h=335&#x26;r=0"><p>Fig. 5. The paradigms of image-based methods for 3D scene generation. (a) Holistic generation creates an entire scene image in one step. (b) Iterative generation progressively extends the scene by extrapolating a sequence of images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图5. 基于图像的3D场景生成方法的范式。(a) 整体生成在一步中创建整个场景图像。(b) 迭代生成通过外推一系列图像逐步扩展场景。</p></div><!-- Media --><h4>3.3.1 Holistic Generation</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.3.1 整体生成</h4></div><p>As shown in Figure 5a, holistic generation in 3D scene generation often relies on panoramic images, which provide a full \({360}^{ \circ  } \times  {180}^{ \circ  }\) field of view,ensuring spatial continuity and explicit geometric constraints. This makes them particularly effective in mitigating scene inconsistencies that arise in perspective views.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>如图5a所示，3D场景生成中的整体生成通常依赖全景图像，这些图像提供完整的\({360}^{ \circ  } \times  {180}^{ \circ  }\)视野，确保空间连续性和明确的几何约束。这使得它们在减轻透视视图中出现的场景不一致性方面特别有效。</p></div><p>Given an RGB image, early methods [240], [241], [242], [243], [244], [245] use GANs for image outpainting to fill masked regions in panoramas. More recent approaches employ advanced generative models (e.g., Co-ModGAN [246] and VQGAN [247]) for greater diversity and content control. ImmerseGAN [106] leverages CoModGAN for user-controlled generation. OmniDreamer [248] and Dream360 [249] use VQGAN to generate diverse and high-resolution panoramas. Leveraging advances in latent diffusion models (LDM) [189], PanoDiffusion [250] enhances scene structure awareness by integrating depth into a bimodal diffusion framework.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>给定一幅RGB图像，早期方法[240]、[241]、[242]、[243]、[244]、[245]使用GAN进行图像外绘，以填补全景中的遮挡区域。最近的方法采用先进的生成模型（例如，Co-ModGAN [246]和VQGAN [247]）以获得更大的多样性和内容控制。ImmerseGAN [106]利用CoModGAN进行用户控制的生成。OmniDreamer [248]和Dream360 [249]使用VQGAN生成多样化和高分辨率的全景图像。利用潜在扩散模型（LDM）[189]的进展，PanoDiffusion [250]通过将深度集成到双模扩散框架中，增强了场景结构意识。</p></div><p>Text-to-image models (e.g., CLIP [251], LDM [189]) enable text-driven panorama generation. Text2Light [35] uses CLIP for text-based generation and hierarchical samplers to extract and piece together panoramic patches based on the input text. Some approaches [252], [253] leverage diffusion models to generate high-resolution planar panoramas. However, they fail to guarantee the continuity at image boundaries, which is essential in creating a seamless viewing experience. To address this, MVDiffusion [36], DiffCol-lage [254], and CubeDiff [255] generate multi-view consistent images and align them into a closed-loop panorama for smooth transitions. StitchDiffusion [256], Diffusion360 [257], PanoDiff [258], and PanFusion [107] adopt padding and cropping strategies at boundaries to maintain the continuity.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>文本到图像模型（例如，CLIP [251]、LDM [189]）使得基于文本的全景生成成为可能。Text2Light [35]使用CLIP进行基于文本的生成，并使用分层采样器根据输入文本提取和拼接全景补丁。一些方法[252]、[253]利用扩散模型生成高分辨率平面全景。然而，它们未能保证图像边界的连续性，这在创建无缝观看体验中至关重要。为了解决这个问题，MVDiffusion [36]、DiffCol-lage [254]和CubeDiff [255]生成多视图一致的图像，并将其对齐成闭环全景以实现平滑过渡。StitchDiffusion [256]、Diffusion360 [257]、PanoDiff [258]和PanFusion [107]在边界处采用填充和裁剪策略以保持连续性。</p></div><p>Recent methods extend single-view panorama generation to multi-view for immersive scene exploration, following two main strategies: one directly generates multiview panoramic images with diffusion models [259], while the other applies 3D reconstruction (e.g., surface reconstruction [190], [260], [261], NeRF [108], and 3D Gaussian Splatting [109], [262], [263], [264], [265]) as post-processing. In this context, LayerPano3D [109] breaks the generated panorama into depth-based layers, filling in unseen content to help create complex scene hierarchies.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最近的方法将单视图全景生成扩展到多视图，以便进行沉浸式场景探索，遵循两种主要策略：一种是直接使用扩散模型生成多视图全景图像[259]，另一种是将3D重建（例如，表面重建[190]、[260]、[261]、NeRF [108]和3D高斯溅射[109]、[262]、[263]、[264]、[265]）作为后处理。在这种情况下，LayerPano3D [109]将生成的全景分解为基于深度的层，填补未见内容以帮助创建复杂的场景层次结构。</p></div><p>Another research direction focuses on generating geometrically consistent street-view panoramas from satellite images. Some methods [266], [267], [268] integrate geometric priors into GAN-based frameworks to learn cross-view mappings. Others [269], [270], [271] estimate 3D structures from satellite images and synthesize textures for rendered street-view panoramas.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>另一个研究方向集中在从卫星图像生成几何一致的街景全景图。一些方法[266]，[267]，[268]将几何先验整合到基于GAN的框架中，以学习跨视图映射。其他方法[269]，[270]，[271]从卫星图像估计3D结构，并为渲染的街景全景合成纹理。</p></div><h4>3.3.2 Iterative Generation</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.3.2 迭代生成</h4></div><p>As shown in Figure 5b, iterative generation starts with an initial 2D image, either provided by the user or generated from text prompts. To generate large-scale 3D scenes, these methods progressively extrapolate the scene along a predefined trajectory. By expanding and refining content step by step, they continuously optimize the 3D scene representation, enhancing geometric and structural coherence.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>如图5b所示，迭代生成从初始2D图像开始，该图像可以由用户提供或从文本提示生成。为了生成大规模3D场景，这些方法沿着预定义轨迹逐步外推场景。通过逐步扩展和细化内容，它们不断优化3D场景表示，增强几何和结构的一致性。</p></div><p>Given a single image, early methods infer 3D scene representations and use them to render novel views. These representations include point clouds [110], [272], [273], [274], multi-plane images [275], [276], depth maps [277], and meshes [278]. Despite enabling fast rendering, these representations limit camera movement due to their finite spatial extent. To enable unrestricted camera movement, Infinite Nature [33], InfiniteNature-Zero [34], Pathdreamer [279], and SGAM [280] follow a "render-refine-repeat" manner, iteratively warping previous views and outpainting missing regions. DiffDreamer [281] improves multi-view consistency by conditioning on multiple past and future frames using a diffusion model. Rather than using explicit 3D representations, GFVS [111] and LOTR [282] encode images and camera poses directly, using transformers to generate novel views. Tseng et al. [283], Photoconsistent-NVS [284], and ODIN [285] improve long-term view synthesis consistency with a pose-guided diffusion model. CAT3D [286] uses a multi-view LDM to generate novel views from input images, followed by 3D reconstruction for interactive rendering. Similarly, Bolt3D [287] generates scene appearance and geometry through multi-view diffusion but directly outputs 3D Gaussians to avoid time-consuming optimization.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>给定单个图像，早期方法推断3D场景表示并利用它们渲染新视图。这些表示包括点云[110]，[272]，[273]，[274]，多平面图像[275]，[276]，深度图[277]和网格[278]。尽管能够快速渲染，这些表示由于其有限的空间范围限制了相机移动。为了实现无限制的相机移动，Infinite Nature[33]、InfiniteNature-Zero[34]、Pathdreamer[279]和SGAM[280]采用“渲染-细化-重复”的方式，迭代地扭曲先前的视图并填补缺失区域。DiffDreamer[281]通过使用扩散模型对多个过去和未来帧进行条件处理，改善了多视图一致性。GFVS[111]和LOTR[282]直接编码图像和相机姿态，使用变换器生成新视图。Tseng等[283]、Photoconsistent-NVS[284]和ODIN[285]通过姿态引导的扩散模型提高了长期视图合成的一致性。CAT3D[286]使用多视图LDM从输入图像生成新视图，随后进行3D重建以实现交互式渲染。类似地，Bolt3D[287]通过多视图扩散生成场景外观和几何，但直接输出3D高斯以避免耗时的优化。</p></div><p>Text-driven scene generation boosts diversity and controllability by leveraging pretrained text-to-image diffusion models [189], [288]. Without requiring extensive domain-specific training, these methods iteratively shift the camera view, outpaint images based on text prompts. PanoGen [289], AOG-Net [290], PanoFree [291], OPa-Ma [292], and Invisible Stitch [293] iteratively outpaint images in perspective view and seamlessly stitch them into a panoramic scene. Other approaches leverage depth estimator [294], [295], [296] to merge RGB images into a unified 3D scene. SceneScape [297], Text2Room [113], and iControl3D [298] use 3D meshes as an intermediary proxy to fuse diffusion-generated images into a coherent 3D scene representation iteratively. WonderJourney [115] adopts a point cloud representation and leverages a VLM-guided re-generation strategy to ensure visual fidelity. Text2NeRF [114] and 3D-SceneDreamer [299] adopt NeRF-based representations to mitigate error accumulation in geometry and appearance, improving adaptability across scenarios. Scene123 [300] further enhances photorealism by using a GAN framework, where the discriminator compares outputs from the video generator with those from the scene generator. By introducing 3D Gaussian Splatting [32], Lucid-Dreamer [116], Text2Immersion [301], WonderWorld [302], RealmDreamer [303], BloomScene [304], and Wonder-Turbo [305] adopt 3D Gaussians as 3D scene representations for higher quality and faster rendering. Leveraging recent advancements in powerful large reconstruction models [306], [307], [308], [309], [310], SynCity [311] enables training-free generation of high-quality \(3\mathrm{D}\) scenes by iteratively performing image outpainting, \(3\mathrm{D}\) object generation, and stitching.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于文本的场景生成通过利用预训练的文本到图像扩散模型[189]，[288]提升了多样性和可控性。这些方法无需广泛的领域特定训练，迭代地移动相机视角，基于文本提示外绘图像。PanoGen[289]、AOG-Net[290]、PanoFree[291]、OPa-Ma[292]和Invisible Stitch[293]在透视视图中迭代外绘图像，并将其无缝拼接成全景场景。其他方法利用深度估计器[294]，[295]，[296]将RGB图像合并为统一的3D场景。SceneScape[297]、Text2Room[113]和iControl3D[298]使用3D网格作为中介代理，迭代地将扩散生成的图像融合为一致的3D场景表示。WonderJourney[115]采用点云表示，并利用VLM引导的再生策略以确保视觉保真度。Text2NeRF[114]和3D-SceneDreamer[299]采用基于NeRF的表示，以减轻几何和外观中的误差积累，提高在不同场景中的适应性。Scene123[300]通过使用GAN框架进一步增强了照片真实感，其中鉴别器比较视频生成器的输出与场景生成器的输出。通过引入3D高斯溅射[32]，Lucid-Dreamer[116]、Text2Immersion[301]、WonderWorld[302]、RealmDreamer[303]、BloomScene[304]和Wonder-Turbo[305]采用3D高斯作为3D场景表示，以实现更高质量和更快渲染。利用最近在强大大规模重建模型[306]，[307]，[308]，[309]，[310]方面的进展，SynCity[311]通过迭代执行图像外绘、\(3\mathrm{D}\)对象生成和拼接，实现了无训练生成高质量\(3\mathrm{D}\)场景。</p></div><!-- Media --><!-- figureText: Generator 1 _____。 _____。 _____。 _____. Single-view Video Multi-view Static Video _____. _____. 3D Scene D Representation Multi-view Video All MANS 3D Scene 。 Representation 0.0 Single- or Multi-view Video Image Generator 2 Text Image Video Generator BEV Action --><img src="https://cdn.noedgeai.com/bo_d163kf3ef24c73d1lc90_9.jpg?x=132&#x26;y=121&#x26;w=758&#x26;h=461&#x26;r=0"><p>Fig. 6. The paradigms of video-based methods for 3D scene generation. (a) Two-stage methods employ two generators, at least one being a video generator, to synthesize multi-view videos while utilizing dynamic 3D scene representations for consistency and exploration. (b) One-stage methods produce single or multi-view videos within a unified process and model, optionally optimizing dynamic 3D scene representations. Note that dashed arrows denote optional operations. "Ctrl." stands for "Control".</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6. 基于视频的方法进行3D场景生成的范式。(a) 两阶段方法使用两个生成器，其中至少一个是视频生成器，以合成多视图视频，同时利用动态3D场景表示以保持一致性和探索性。(b) 一阶段方法在统一的过程中和模型中生成单视图或多视图视频，选择性地优化动态3D场景表示。请注意，虚线箭头表示可选操作。“Ctrl.”代表“控制”。</p></div><!-- Media --><p>Another research direction conducts iterative view synthesis and image animation simultaneously to build a dynamic 3D scene from a single image. 3D Cinemagra-phy [112] and Make-It-4D [312] use layered depth images (LDIs) to build feature point clouds and animate scenes via motion estimation and 3D scene flow. 3D-MOM [313] first optimizes 3D Gaussians by generating multi-view images from a single image, and then optimizes 4D Gaussians [40] by estimating consistent motion across views.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>另一个研究方向同时进行迭代视图合成和图像动画，以从单幅图像构建动态3D场景。3D动态摄影（3D Cinemagraphy）[112]和Make-It-4D [312]使用分层深度图像（LDIs）构建特征点云，并通过运动估计和3D场景流来动画化场景。3D-MOM [313]首先通过从单幅图像生成多视图图像来优化3D高斯（Gaussians），然后通过估计视图间一致的运动来优化4D高斯（4D Gaussians）[40]。</p></div><h3>3.4 Video-based Generation</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.4 基于视频的生成</h3></div><p>Recent advances in video diffusion models [38], [61] have demonstrated significant progress in generating high-quality video content. Building on these advancements, video-based 3D scene generation methods produce image sequences, enabling the synthesis of immersive and dynamic environments. As shown in Table 1, they provide high realism and diversity through sequential generation, benefiting from temporal coherence across frames. However, they face challenges in ensuring consistent view alignment. These methods can be divided into two-stage and one-stage categories, with their paradigms illustrated in Figure 6.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最近在视频扩散模型（video diffusion models）[38]，[61]方面的进展显示出在生成高质量视频内容方面的显著进展。在这些进展的基础上，基于视频的3D场景生成方法生成图像序列，使得合成沉浸式和动态环境成为可能。如表1所示，它们通过顺序生成提供高真实感和多样性，受益于帧间的时间一致性。然而，它们在确保视图一致性方面面临挑战。这些方法可以分为两阶段和单阶段类别，其范式在图6中说明。</p></div><h4>3.4.1 Two-stage Generation</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.4.1 两阶段生成</h4></div><p>As shown in Figure 6a, two-stage generation divides the generation into two stages, each targeting multi-view spatial consistency and multi-frame temporal coherence separately. To further improve view consistency, these generated sequences are subsequently used to optimize a dynamic 3D scene representation (e.g., 4D Gaussians [40], Deformable Gaussians [41]). VividDream [314] first constructs a static 3D scene through iterative image outpainting, then renders multi-view videos covering the entire scene and applies time-reversal [315] to animate them, creating dynamic videos across viewpoints. PaintScene4D [316] first generates a video from a text description using video diffusion, then refines it through iterative warping and inpainting at each timestamp to maintain multi-view consistency. Similarly, 4Real [117], DimensionX [42], and Free4D [317] first generate a coherent reference video and then extend view angles using frame-conditioned video generation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>如图6a所示，两阶段生成将生成过程分为两个阶段，每个阶段分别针对多视图空间一致性和多帧时间一致性。为了进一步提高视图一致性，这些生成的序列随后用于优化动态3D场景表示（例如，4D高斯（4D Gaussians）[40]，可变形高斯（Deformable Gaussians）[41]）。VividDream [314]首先通过迭代图像外绘构建静态3D场景，然后渲染覆盖整个场景的多视图视频，并应用时间反转（time-reversal）[315]来动画化它们，创建跨视点的动态视频。PaintScene4D [316]首先使用视频扩散从文本描述生成视频，然后通过在每个时间戳进行迭代扭曲和修补来保持多视图一致性。类似地，4Real [117]、DimensionX [42]和Free4D [317]首先生成一致的参考视频，然后使用帧条件视频生成扩展视角。</p></div><h4>3.4.2 One-stage Generation</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.4.2 单阶段生成</h4></div><p>As shown in Figure 6b, one-stage generation consolidates generation into a single process, implicitly capturing spatiotemporal consistency to produce single- or multi-view videos from any viewpoint and timestep within a unified model. Some approaches [318], [319], [320], [321], [322], [323], [324], [325] adopt video diffusion models for iterative view extrapolation, followed by 3DGS optimization to build a static scene. To generate dynamic scenes,Gen \(\mathcal{X}\mathrm{D}\) [119] and CAT4D [326] adopt distinct multiview-temporal strategies to construct multi-view video models capable of generating all views across all timestamps. StarGen [327] and Streetscapes [328] use past frames as guidance for video generation, enhancing long-range scene synthesis through an autoregressive approach. By utilizing the natural multiview 3D prior of panoramic images, 4K4DGen [43] samples perspective images from a static panorama, animates them, and aligns them into a dynamic panorama. 360DVD [329], Imagine360 [330], Genex [331], and DynamicScaler [332] integrate panoramic constraints into video diffusion models to generate spherical-consistent panoramic videos.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>如图6b所示，单阶段生成将生成过程整合为一个单一过程，隐式捕捉时空一致性，从任何视点和时间步生成单视图或多视图视频，均在统一模型内。一些方法[318]，[319]，[320]，[321]，[322]，[323]，[324]，[325]采用视频扩散模型进行迭代视图外推，随后进行3DGS优化以构建静态场景。为了生成动态场景，Gen \(\mathcal{X}\mathrm{D}\) [119]和CAT4D [326]采用不同的多视图-时间策略构建能够生成所有视图和所有时间戳的多视图视频模型。StarGen [327]和Streetscapes [328]使用过去的帧作为视频生成的指导，通过自回归方法增强长距离场景合成。通过利用全景图像的自然多视图3D先验，4K4DGen [43]从静态全景中采样透视图像，对其进行动画处理，并将其对齐为动态全景。360DVD [329]、Imagine360 [330]、Genex [331]和DynamicScaler [332]将全景约束集成到视频扩散模型中，以生成球面一致的全景视频。</p></div><p>In scene generation for video games and autonomous driving, these methods enhance both control and realism by integrating various control signals as conditions. In open-world gaming environments, vast datasets comprising user inputs and rendered videos enable models like DIAMOND [333], GameNGen [334], Oasis [335], GameGen- \(\mathbb{X}\) [120],and WORLDMEM [336] to predict future frames based on user interactions, creating responsive virtual environments as neural game engines. In autonomous driving, models such as DriveDreamer [337], MagicDrive [39], Drive-WM [338], and GAIA-1 [339] utilize inputs like text, bounding boxes, Bird's Eye View (BEV) maps, and driver actions to control video generation for complex driving scenarios. Recent works further enhance view consistency [340], [341], [342], [343], [344], [345], [346], [347], [348], [349], [350], expand control capabilities [118], [351], [352], [353], [354], enable 3D-level control via occupancy [355], [356], [357], [358], [359], support multimodal output [360], [361], [362], and improve generation speed [363] and sequence length [364], [365], [366], [367].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在视频游戏和自动驾驶的场景生成中，这些方法通过将各种控制信号作为条件进行集成，增强了控制和真实感。在开放世界游戏环境中，由用户输入和渲染视频组成的大型数据集使得像DIAMOND [333]、GameNGen [334]、Oasis [335]、GameGen- \(\mathbb{X}\) [120]和WORLDMEM [336]等模型能够基于用户交互预测未来帧，创建响应式虚拟环境作为神经游戏引擎。在自动驾驶中，DriveDreamer [337]、MagicDrive [39]、Drive-WM [338]和GAIA-1 [339]等模型利用文本、边界框、鸟瞰图（BEV）地图和驾驶员动作等输入来控制复杂驾驶场景的视频生成。最近的工作进一步增强了视图一致性[340]，[341]，[342]，[343]，[344]，[345]，[346]，[347]，[348]，[349]，[350]，扩展控制能力[118]，[351]，[352]，[353]，[354]，通过占用实现3D级控制[355]，[356]，[357]，[358]，[359]，支持多模态输出[360]，[361]，[362]，并提高生成速度[363]和序列长度[364]，[365]，[366]，[367]。</p></div><h2>4 DATASETS AND EVALUATION</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>4 数据集与评估</h2></div><h3>4.1 Datasets</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.1 数据集</h3></div><p>We summarize the commonly used datasets for \(3\mathrm{D}\) scene generation in Table 3, grouping them by scene type into three categories: indoor, natural, and urban.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们在表3中总结了用于\(3\mathrm{D}\)场景生成的常用数据集，按场景类型分为三类：室内、自然和城市。</p></div><h4>4.1.1 Indoor Datasets</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.1.1 室内数据集</h4></div><p>Existing indoor datasets are either captured from real-world scenes using RGB or RGB-D sensors or professionally designed with curated 3D CAD furniture models.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>现有的室内数据集要么是使用RGB或RGB-D传感器从真实场景捕获的，要么是使用精心设计的3D CAD家具模型专业制作的。</p></div><p>Real-world datasets are captured from physical scenes using sensors like depth, DSLR, or panoramic cameras. Early datasets provide RGB-D or panoramic images with semantic labels (e.g., NYUv2 [369], 2D-3D-S [372]), while recent ones like ScanNet [375] and Matterport3D [374] offer 3D reconstructions with dense meshes and instance-level annotations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>真实世界的数据集是使用深度、单反或全景相机等传感器从物理场景捕获的。早期的数据集提供带有语义标签的RGB-D或全景图像（例如，NYUv2 [369]，2D-3D-S [372]），而最近的数据集如ScanNet [375]和Matterport3D [374]则提供具有密集网格和实例级注释的3D重建。</p></div><ul>
<li>SUN360 [368] contains 67,583 high-res \({360}^{ \circ  } \times\) \({180}^{ \circ  }\) panoramic images in equirectangular format, manually categorized into 80 scene types.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>SUN360 [368]包含67,583张高分辨率\({360}^{ \circ  } \times\) \({180}^{ \circ  }\)全景图像，手动分类为80种场景类型。</li>
</ul></div><ul>
<li>NYUv2 [369] provides 1,449 densely annotated RGB-D images from 464 indoor scenes, covering per-pixel semantics and instance-level objects.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>NYUv2 [369]提供来自464个室内场景的1,449张密集注释的RGB-D图像，覆盖逐像素语义和实例级对象。</li>
</ul></div><ul>
<li>SUN-RGBD [370] offers 10,335 RGB-D images and reconstructed point cloud, with rich annotations including room types, 2D polygons, 3D bounding boxes, camera poses, and room layouts.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>SUN-RGBD [370]提供10,335张RGB-D图像和重建的点云，丰富的注释包括房间类型、2D多边形、3D边界框、相机姿态和房间布局。</li>
</ul></div><ul>
<li>SceneNN [371] offers 502K RGB-D frames from 100 indoor scenes with reconstructed meshes, textured models, camera poses, and both object-oriented and axis-aligned bounding boxes.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>SceneNN [371]提供来自100个室内场景的502K RGB-D帧，具有重建的网格、纹理模型、相机姿态，以及面向对象和轴对齐的边界框。</li>
</ul></div><ul>
<li>2D-3D-S [372] includes over 70,000 panoramic images from six indoor areas, with aligned depth, surface normals, semantic labels, point clouds, meshes, global XYZ maps, and full camera metadata.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>2D-3D-S [372]包括来自六个室内区域的超过70,000张全景图像，具有对齐的深度、表面法线、语义标签、点云、网格、全局XYZ地图和完整的相机元数据。</li>
</ul></div><ul>
<li>Laval Indoor [373] offers \({2.2}\mathrm{\;K}\) high-res indoor panoramas (7768 ×3884) with HDR lighting from various settings such as homes, offices, and factories.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>Laval Indoor [373]提供\({2.2}\mathrm{\;K}\)高分辨率室内全景图（7768 ×3884），具有来自家庭、办公室和工厂等各种环境的HDR照明。</li>
</ul></div><ul>
<li>Matterport3D [374] contains 10,800 panoramic images from 194,400 RGB-D views in 90 buildings, with dense camera trajectories, aligned depth maps, and semantic labels.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>Matterport3D [374]包含来自90栋建筑的194,400个RGB-D视图的10,800张全景图像，具有密集的相机轨迹、对齐的深度图和语义标签。</li>
</ul></div><ul>
<li>ScanNet [375] offers 2.5M RGB-D frames in 1,513 scans from 707 distinct spaces with camera poses, surface reconstructions,dense \(3\mathrm{D}\) semantic labels, and aligned CAD models.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>ScanNet [375]提供来自707个不同空间的1,513个扫描中的250万RGB-D帧，具有相机姿态、表面重建、密集\(3\mathrm{D}\)语义标签和对齐的CAD模型。</li>
</ul></div><ul>
<li>Replica [377] provides high-quality 3D reconstructions of 35 rooms across 18 scenes, featuring PBR textures, HDR lighting, and semantic annotations.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>Replica [377]提供18个场景中35个房间的高质量3D重建，具有PBR纹理、HDR照明和语义注释。</li>
</ul></div><ul>
<li>RealEstate \({10}\mathrm{\;K}\) [376] contains 10 million frames from \({10}\mathrm{\;K}\) YouTube videos,featuring both indoor and outdoor scenes with per-frame camera parameters.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>RealEstate \({10}\mathrm{\;K}\) [376]包含来自\({10}\mathrm{\;K}\) YouTube视频的1000万帧，涵盖室内和室外场景，具有逐帧相机参数。</li>
</ul></div><ul>
<li>3DSSG [378] provides scene graphs for 478 indoor rooms from 3RScan [398], with 93 object attributes, 40 relationship types, and 534 semantic classes.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>3DSSG [378]为来自3RScan [398]的478个室内房间提供场景图，具有93个对象属性、40种关系类型和534个语义类别。</li>
</ul></div><ul>
<li>HM3D [379] offers 1,000 high-res 3D reconstructions of indoor spaces across residential, commercial, and civic buildings.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>HM3D [379]提供1000个高分辨率室内空间的3D重建，涵盖住宅、商业和市政建筑。</li>
</ul></div><ul>
<li>ScanNet++ [380] includes 1,000+ scenes captured with laser scanner, DSLR, and iPhone RGB-D, featuring fine-grained semantics and long-tail categories.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>ScanNet++ [380] 包含 1,000 多个使用激光扫描仪、单反相机和 iPhone RGB-D 捕获的场景，具有细粒度语义和长尾类别。</li>
</ul></div><ul>
<li>DL3DV-10K [381] contains 51.2M frames from 10,510 video sequences across 65 indoor and semi-outdoor locations, featuring varied visual conditions such as reflections and different lighting.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>DL3DV-10K [381] 包含来自 65 个室内和半户外地点的 10,510 个视频序列中的 51.2M 帧，具有反射和不同光照等多样的视觉条件。</li>
</ul></div><p>Synthetic indoor datasets overcome real-world limitations like limited diversity, occlusion, and costly annotation. Using designed layouts and textured 3D assets, datasets like SUNCG [382] and 3D-FRONT [385] offer large-scale, diverse scenes. Some [383], [384] leverage advanced rendering for photorealistic images with accurate \(2\mathrm{D}\) labels.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>合成室内数据集克服了现实世界的局限，如有限的多样性、遮挡和昂贵的标注。使用设计的布局和纹理 3D 资产，像 SUNCG [382] 和 3D-FRONT [385] 这样的数据集提供了大规模、多样的场景。一些 [383]，[384] 利用先进的渲染技术生成具有准确 \(2\mathrm{D}\) 标签的照片级真实图像。</p></div><ul>
<li>SceneSynth [152] includes 130 indoor scenes (e.g., studies, kitchens, living rooms) with 1,723 unique models from Google 3D Warehouse.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>SceneSynth [152] 包含 130 个室内场景（例如，书房、厨房、客厅），拥有来自 Google 3D Warehouse 的 1,723 个独特模型。</li>
</ul></div><ul>
<li>SUNCG [382] offers 45,622 manually designed scenes,featuring \({404}\mathrm{\;K}\) rooms and \({5.7}\mathrm{M}\) object instances from 2,644 meshes across 84 categories.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>SUNCG [382] 提供 45,622 个手动设计的场景，包含来自 84 个类别的 2,644 个网格的 \({404}\mathrm{\;K}\) 房间和 \({5.7}\mathrm{M}\) 物体实例。</li>
</ul></div><ul>
<li>Structured3D [383] includes 196.5K images from \({3.5}\mathrm{\;K}\) professionally designed houses with detailed 3D annotations (e.g., lines, planes).</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>Structured3D [383] 包含来自 \({3.5}\mathrm{\;K}\) 专业设计房屋的 196.5K 图像，具有详细的 3D 注释（例如，线条、平面）。</li>
</ul></div><ul>
<li>Hypersim [384] provides 77.4K photorealistic renders with PBR materials and lighting for realistic view synthesis.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>Hypersim [384] 提供 77.4K 照片级真实渲染，使用 PBR 材料和照明进行真实视图合成。</li>
</ul></div><ul>
<li>3D-FRONT [385] offers 6,813 professionally designed houses and 18,797 diversely furnished rooms, populated with high-quality textured \(3\mathrm{D}\) objects from 3D-FUTURE [399].</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>3D-FRONT [385] 提供 6,813 个专业设计的房屋和 18,797 个多样化的家具房间，配备来自 3D-FUTURE [399] 的高质量纹理 \(3\mathrm{D}\) 物体。</li>
</ul></div><ul>
<li>SG-FRONT [92] augments 3D-FRONT with scene graph annotations.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>SG-FRONT [92] 通过场景图注释增强 3D-FRONT。</li>
</ul></div><h4>4.1.2 Natural Datasets</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.1.2 自然数据集</h4></div><p>Datasets for natural scenes are still limited, mainly due to the difficulties of large-scale collection and annotation in open outdoor environments. However, several notable efforts have been made to advance research in this area.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>自然场景的数据集仍然有限，主要是由于在开放户外环境中进行大规模收集和标注的困难。然而，在这一领域已经做出了一些显著的努力以推动研究。</p></div><ul>
<li>Laval Outdoor [386] provides 205 high-res HDR panoramas of diverse natural and urban scenes.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>Laval Outdoor [386] 提供 205 张高分辨率 HDR 全景图，展示多样的自然和城市场景。</li>
</ul></div><ul>
<li>LHQ [387] offers 91,693 curated landscape images from Unsplash and Flickr, designed for high-quality image generation tasks.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>LHQ [387] 提供来自 Unsplash 和 Flickr 的 91,693 张精心策划的风景图像，旨在用于高质量图像生成任务。</li>
</ul></div><ul>
<li>ACID [33] features 2.1M drone-captured frames from 891 YouTube videos of coastal regions, with 3D camera trajectories obtained via structure-from-motion.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>ACID [33] 包含来自 891 个 YouTube 视频的 2.1M 无人机捕获帧，涵盖沿海地区，3D 相机轨迹通过运动重建获得。</li>
</ul></div><h4>4.1.3 Urban Datasets</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.1.3 城市数据集</h4></div><p>Urban datasets are built from real-world imagery or synthesized using game engines, providing images and annotations in \(2\mathrm{D}\) or \(3\mathrm{D}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>城市数据集由真实世界图像构建或使用游戏引擎合成，提供 \(2\mathrm{D}\) 或 \(3\mathrm{D}\) 的图像和注释。</p></div><p>Real-world datasets mainly focus on driving scenes, represented by KITTI [388], Waymo [391], and nuScenes [392], due to the significant attention autonomous driving has received over the past decade. Another major source is Google's street views and aerial views, exemplified by HoliCity [393] and GoogleEarth [98]. These datasets provide rich annotations, such as semantic segmentation and instance segmentation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>现实世界的数据集主要集中在驾驶场景上，代表性的数据集有KITTI [388]、Waymo [391]和nuScenes [392]，这是因为在过去十年中，自动驾驶受到了极大的关注。另一个主要来源是谷歌的街景和航空视图，以HoliCity [393]和GoogleEarth [98]为例。这些数据集提供了丰富的注释，如语义分割和实例分割。</p></div><!-- Media --><p>TABLE 3</p><p>Summary and comparison of popular datasets for 3D scene generation. The scene types \({}^{11},\mathrm{\;N}\) ,and \({}^{11}\) represent "Indoor","Nature",and "Urban". The annotations \({}_{G},{P}_{1}\) ,and \({}_{M}\) denote scene graph,camera pose,and motion annotations (e.g.,optical flow),respectively. \({S}_{2}/{I}_{2}/{B}_{2}\) and \({\mathrm{S}}_{3}/{\mathrm{I}}_{3}/{\mathrm{B}}_{3}\) represent \(2\mathrm{D}/3\mathrm{D}\) semantic maps,instance maps,and bounding boxes. \(\mathrm{P},\mathrm{N},\mathrm{I}\) ,and \(\mathrm{v}\) indicate procedural,neural \(3\mathrm{D}\) -based, image-based,and video-based generation,respectively. Mesh \({}^{\mathrm{R}}\) and \({\mathrm{{PCD}}}^{\mathrm{R}}\) are reconstructed mesh and point clouds,respectively. Note that "-" in #Images, 3D Model, and Annotations indicates datasets do not provide these; "-" in #Scenes and Area means the information cannot be inferred.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>流行数据集在3D场景生成中的总结与比较。场景类型\({}^{11},\mathrm{\;N}\)、\({}^{11}\)代表“室内”、“自然”和“城市”。注释\({}_{G},{P}_{1}\)、\({}_{M}\)分别表示场景图、相机姿态和运动注释（例如，光流）。\({S}_{2}/{I}_{2}/{B}_{2}\)和\({\mathrm{S}}_{3}/{\mathrm{I}}_{3}/{\mathrm{B}}_{3}\)代表\(2\mathrm{D}/3\mathrm{D}\)语义地图、实例地图和边界框。\(\mathrm{P},\mathrm{N},\mathrm{I}\)和\(\mathrm{v}\)分别表示基于过程、神经\(3\mathrm{D}\)、基于图像和基于视频的生成。网格\({}^{\mathrm{R}}\)和\({\mathrm{{PCD}}}^{\mathrm{R}}\)分别是重建的网格和点云。请注意，#Images、3D模型和注释中的“-”表示数据集未提供这些信息；#Scenes和区域中的“-”表示无法推断该信息。</p></div><table><tbody><tr><td>Dataset</td><td>\( \mathbf{{Year}} \)</td><td>Type</td><td>Source</td><td>\( \mathbf{\# {Images}} \)</td><td>#Scenes</td><td>Area</td><td>3D Model</td><td>Annotations</td><td>Used by</td><td>URL</td></tr><tr><td>SUN360 [368]</td><td>2012</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>Real</td><td>67.5K</td><td>-</td><td>-</td><td>-</td><td>-</td><td>I</td><td></td></tr><tr><td>NYUv2 [369]</td><td>2012</td><td>I</td><td>Real</td><td>1.4K</td><td>464</td><td>-</td><td>-</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>Sun-RGBD [370]</td><td>2015</td><td>I</td><td>Real</td><td>10.3K</td><td>-</td><td>-</td><td>PCDR</td><td>\( {\mathrm{S}}_{2}{\mathrm{\;B}}_{3} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>SceneNN [371]</td><td>2016</td><td>I</td><td>Real</td><td>502K</td><td>100</td><td>2,260 \( {\mathrm{\;m}}^{2} \)</td><td>Mesh \( {}^{\mathrm{R}} \)</td><td>\( {\mathrm{S}}_{3}{\mathrm{I}}_{3}{\mathrm{\;B}}_{3}\mathrm{P} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>2D-3D-S [372]</td><td>2017</td><td>I</td><td>Real</td><td>70.5K</td><td>270</td><td>\( 6,{000}{\mathrm{\;m}}^{2} \)</td><td>Mesh \( {}^{\mathrm{R}},{\mathrm{{PCD}}}^{\mathrm{R}} \)</td><td>\( \begin{array}{lllll} {\mathrm{S}}_{2} &#x26; {\mathrm{I}}_{2} &#x26; {\mathrm{\;S}}_{3} &#x26; {\mathrm{I}}_{3} &#x26; \mathrm{P} \end{array} \)</td><td>I</td><td>C</td></tr><tr><td>Laval Indoor [373]</td><td>2017</td><td>I</td><td>Real</td><td>2.2K</td><td>-</td><td>-</td><td>-</td><td>-</td><td>I</td><td>C</td></tr><tr><td>Matterport3D [374]</td><td>2017</td><td>I</td><td>Real</td><td>10.8K</td><td>90</td><td>\( {0.102}\;\mathrm{k{m}^{2}} \)</td><td>Mesh \( {}^{\mathrm{R}} \)</td><td>\( \begin{array}{lllll} {\mathrm{S}}_{2} &#x26; {\mathrm{I}}_{2} &#x26; {\mathrm{\;S}}_{3} &#x26; {\mathrm{I}}_{3} &#x26; \mathrm{P} \end{array} \)</td><td>\( \mathrm{N}\;\mathrm{I} \)</td><td>C</td></tr><tr><td>ScanNet [375]</td><td>2017</td><td>I</td><td>Real</td><td>2.5M</td><td>707</td><td>\( {39},{980}{\mathrm{\;m}}^{2} \)</td><td>Mesh \( {}^{\mathrm{R}} \) , CAD</td><td>\( \begin{array}{lllll} {\mathrm{S}}_{2} &#x26; {\mathrm{I}}_{2} &#x26; {\mathrm{\;S}}_{3} &#x26; {\mathrm{I}}_{3} &#x26; \mathrm{P} \end{array} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>RealEstate10K [376]</td><td>2018</td><td>\( \mathrm{I}\;\mathrm{U} \)</td><td>Real</td><td>10M</td><td>-</td><td>-</td><td>-</td><td>P</td><td>\( \mathrm{N}\;\mathrm{I}\;\mathrm{V} \)</td><td>C</td></tr><tr><td>Replica [377]</td><td>2019</td><td>I</td><td>Real</td><td>-</td><td>18</td><td>\( 2,{190}{\mathrm{\;m}}^{2} \)</td><td>Mesh \( {}^{\mathrm{R}} \)</td><td>\( {\mathrm{S}}_{3}\;{\mathrm{I}}_{3} \)</td><td>\( \mathrm{N}\;\mathrm{I} \)</td><td>C</td></tr><tr><td>3DSSG [378]</td><td>2020</td><td>I</td><td>Real</td><td>363K</td><td>478</td><td>-</td><td>Mesh \( {}^{\mathrm{R}} \)</td><td>\( {\mathrm{S}}_{3}{\mathrm{I}}_{3}\mathrm{{GP}} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>HM3D [379]</td><td>2021</td><td>I</td><td>Real</td><td>-</td><td>1,000</td><td>\( {0.365}\;\mathrm{k{m}^{2}} \)</td><td>Mesh \( {}^{\mathrm{R}} \)</td><td>-</td><td>I</td><td>C</td></tr><tr><td>ScanNet++ [380]</td><td>2023</td><td>I</td><td>Real</td><td>11.1M</td><td>1006</td><td>-</td><td>Mesh \( {}^{\mathrm{R}} \)</td><td>\( {\mathrm{S}}_{3}{\mathrm{I}}_{3}\mathrm{P} \)</td><td>\( \mathrm{N} \)</td><td>G</td></tr><tr><td>DL3DV-10K [381]</td><td>2024</td><td>\( \mathrm{N} \)\( \mathrm{{NU}} \)</td><td>Real</td><td>51.2M</td><td>-</td><td>-</td><td>-</td><td>\( \mathrm{P} \)</td><td>\( \mathrm{N}\mathrm{V} \)</td><td>G</td></tr><tr><td>SceneSynth [152]</td><td>2012</td><td>I</td><td>Synthetic</td><td>-</td><td>130</td><td>-</td><td>CAD</td><td>-</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>SUNCG [382]</td><td>2017</td><td>I</td><td>Synthetic</td><td>-</td><td>45,622</td><td>\( {24}{\mathrm{\;{km}}}^{2} \)</td><td>CAD</td><td>\( {\mathrm{S}}_{3}\;{\mathrm{I}}_{3} \)</td><td>\( \mathrm{{PN}} \)</td><td></td></tr><tr><td>Structured3D [383]</td><td>2020</td><td>I</td><td>Synthetic</td><td>196.5K</td><td>3500</td><td>-</td><td>CAD</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2}\;{\mathrm{\;S}}_{3}\;{\mathrm{I}}_{3} \)</td><td>\( \mathrm{N}\;\mathrm{I} \)</td><td>C</td></tr><tr><td>Hypersim [384]</td><td>2021</td><td>I</td><td>Synthetic</td><td>77.4K</td><td>461</td><td>-</td><td>CAD</td><td>\( {\mathrm{S}}_{2}{\mathrm{I}}_{2}\mathrm{P} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>3D-FRONT [385]</td><td>2021</td><td>I</td><td>Synthetic</td><td>-</td><td>6,813</td><td>\( {0.51}\;\mathrm{k{m}^{2}} \)</td><td>CAD</td><td>-</td><td>\( \mathrm{{PN}} \)</td><td>G</td></tr><tr><td>SG-FRONT [92]</td><td>2023</td><td>I</td><td>Synthetic</td><td>-</td><td>6,813</td><td>\( {0.51}\;\mathrm{k{m}^{2}} \)</td><td>CAD</td><td>G</td><td>\( \mathrm{N} \)</td><td>G</td></tr><tr><td>Laval Outdoor [386]</td><td>2017</td><td>\( \mathrm{N} \) U</td><td>Real</td><td>0.2K</td><td>-</td><td>-</td><td>-</td><td>-</td><td>I</td><td>C</td></tr><tr><td>LHQ [387]</td><td>2021</td><td>\( \mathrm{N} \)</td><td>Real</td><td>91.7K</td><td>-</td><td>-</td><td>-</td><td>-</td><td>\( \mathrm{N}\;\mathrm{I} \)</td><td>G</td></tr><tr><td>ACID [33]</td><td>2021</td><td>\( \mathrm{N} \)</td><td>Real</td><td>2.1M</td><td>-</td><td>-</td><td>-</td><td>P</td><td>\( \mathrm{N}\;\mathrm{I}\;\mathrm{V} \)</td><td>G</td></tr><tr><td>KITTI [388]</td><td>2012</td><td>U</td><td>Real</td><td>15K</td><td>1</td><td>-</td><td>LiDAR</td><td>\( {\mathrm{S}}_{2}{\mathrm{I}}_{2}{\mathrm{\;B}}_{3}\mathrm{\;P}\mathrm{\;M} \)</td><td>\( \mathrm{N}\;\mathrm{I} \)</td><td>G</td></tr><tr><td>Cityscapes [389]</td><td>2016</td><td>U</td><td>Real</td><td>25K</td><td>50</td><td>-</td><td>-</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2} \)</td><td>I</td><td>C</td></tr><tr><td>SemanticKITTI [390]</td><td>2019</td><td>U</td><td>Real</td><td>-</td><td>1</td><td>-</td><td>LiDAR</td><td>\( {\mathrm{S}}_{3}\mathrm{P} \)</td><td>\( \mathrm{N} \)</td><td>G</td></tr><tr><td>Waymo [391]</td><td>2020</td><td>U</td><td>Real</td><td>1M</td><td>3</td><td>\( {76}{\mathrm{\;{km}}}^{2} \)</td><td>LiDAR</td><td>\( {\mathrm{B}}_{2}{\mathrm{\;B}}_{3}\mathrm{P} \)</td><td>\( \mathrm{N}\;\mathrm{V} \)</td><td>G</td></tr><tr><td>nuScenes [392]</td><td>2020</td><td>U</td><td>Real</td><td>1.4M</td><td>2</td><td>\( 5{\mathrm{\;{km}}}^{2} \)</td><td>LiDAR</td><td>\( {\mathrm{S}}_{3}{\mathrm{\;B}}_{3}\mathrm{P} \)</td><td>\( \mathrm{N}\mathrm{V} \)</td><td>G</td></tr><tr><td>HoliCity [393]</td><td>2020</td><td>U</td><td>Real</td><td>6.3K</td><td>1</td><td>\( {20}\mathrm{\;{km}}{}^{2} \)</td><td>CAD</td><td>\( {\mathrm{S}}_{2}\mathrm{P} \)</td><td>\( \mathrm{N} \)</td><td>G</td></tr><tr><td>OmniCity [394]</td><td>2023</td><td>U</td><td>Real</td><td>108.6K</td><td>1</td><td>-</td><td>-</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2} \)</td><td>\( \mathrm{N} \)</td><td>G</td></tr><tr><td>KITTI-360 [395]</td><td>2023</td><td>U</td><td>Real</td><td>150K</td><td>1</td><td>-</td><td>LiDAR</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2}\;{\mathrm{S}}_{3}\;{\mathrm{I}}_{3}\;{\mathrm{B}}_{3}\;\mathrm{P} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>GoogleEarth [98]</td><td>2024</td><td>U</td><td>Real</td><td>24K</td><td>1</td><td>\( {25}{\mathrm{\;{km}}}^{2} \)</td><td>Voxel Grid</td><td>\( \begin{array}{lllll} {\mathrm{S}}_{2} &#x26; {\mathrm{I}}_{2} &#x26; {\mathrm{\;S}}_{3} &#x26; {\mathrm{I}}_{3} &#x26; \mathrm{P} \end{array} \)</td><td>\( \mathrm{N} \)</td><td>[7]</td></tr><tr><td>OSM [98]</td><td>2024</td><td>U</td><td>Real</td><td>-</td><td>80</td><td>\( 6,{000}\mathrm{\;{km}}{}^{2} \)</td><td>-</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>CARLA [13]</td><td>2017</td><td>U</td><td>Synthetic</td><td>\( \infty \)</td><td>13</td><td>-</td><td>LiDAR</td><td>\( {\mathrm{S}}_{2}{\mathrm{I}}_{2}{\mathrm{\;S}}_{3}{\mathrm{I}}_{3}\mathrm{\;P}\mathrm{\;M} \)</td><td>\( \mathrm{N} \)I V</td><td>C</td></tr><tr><td>Virtual-KITTI-2 [396]</td><td>2020</td><td>U</td><td>Synthetic</td><td>42.5K</td><td>1</td><td>-</td><td>-</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2}\;{\mathrm{\;B}}_{2}\;\mathrm{\;P}\;\mathrm{M} \)</td><td>I</td><td>C</td></tr><tr><td>CarlaSC [397]</td><td>2022</td><td>U</td><td>Synthetic</td><td>43.2K</td><td>8</td><td>-</td><td>Voxel Grid</td><td>\( {\mathrm{S}}_{3}\mathrm{\;P}\mathrm{\;M} \)</td><td>\( \mathrm{N} \)</td><td>G</td></tr><tr><td>CityTopia [218]</td><td>2025</td><td>U</td><td>Synthetic</td><td>37.5K</td><td>11</td><td>\( {36}{\mathrm{\;{km}}}^{2} \)</td><td>Voxel Grid</td><td>\( \begin{array}{lllll} {\mathrm{S}}_{2} &#x26; {\mathrm{I}}_{2} &#x26; {\mathrm{\;S}}_{3} &#x26; {\mathrm{I}}_{3} &#x26; \mathrm{P} \end{array} \)</td><td>\( \mathrm{N} \)</td><td>G</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>数据集</td><td>\( \mathbf{{Year}} \)</td><td>类型</td><td>来源</td><td>\( \mathbf{\# {Images}} \)</td><td>#场景</td><td>区域</td><td>3D模型</td><td>注释</td><td>使用者</td><td>网址</td></tr><tr><td>SUN360 [368]</td><td>2012</td><td>\( \mathrm{I}\mathrm{N}\mathrm{U} \)</td><td>真实</td><td>67.5K</td><td>-</td><td>-</td><td>-</td><td>-</td><td>I</td><td></td></tr><tr><td>NYUv2 [369]</td><td>2012</td><td>I</td><td>真实</td><td>1.4K</td><td>464</td><td>-</td><td>-</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>Sun-RGBD [370]</td><td>2015</td><td>I</td><td>真实</td><td>10.3K</td><td>-</td><td>-</td><td>PCDR</td><td>\( {\mathrm{S}}_{2}{\mathrm{\;B}}_{3} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>SceneNN [371]</td><td>2016</td><td>I</td><td>真实</td><td>502K</td><td>100</td><td>2,260 \( {\mathrm{\;m}}^{2} \)</td><td>网格 \( {}^{\mathrm{R}} \)</td><td>\( {\mathrm{S}}_{3}{\mathrm{I}}_{3}{\mathrm{\;B}}_{3}\mathrm{P} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>2D-3D-S [372]</td><td>2017</td><td>I</td><td>真实</td><td>70.5K</td><td>270</td><td>\( 6,{000}{\mathrm{\;m}}^{2} \)</td><td>网格 \( {}^{\mathrm{R}},{\mathrm{{PCD}}}^{\mathrm{R}} \)</td><td>\( \begin{array}{lllll} {\mathrm{S}}_{2} &#x26; {\mathrm{I}}_{2} &#x26; {\mathrm{\;S}}_{3} &#x26; {\mathrm{I}}_{3} &#x26; \mathrm{P} \end{array} \)</td><td>I</td><td>C</td></tr><tr><td>Laval室内 [373]</td><td>2017</td><td>I</td><td>真实</td><td>2.2K</td><td>-</td><td>-</td><td>-</td><td>-</td><td>I</td><td>C</td></tr><tr><td>Matterport3D [374]</td><td>2017</td><td>I</td><td>真实</td><td>10.8K</td><td>90</td><td>\( {0.102}\;\mathrm{k{m}^{2}} \)</td><td>网格 \( {}^{\mathrm{R}} \)</td><td>\( \begin{array}{lllll} {\mathrm{S}}_{2} &#x26; {\mathrm{I}}_{2} &#x26; {\mathrm{\;S}}_{3} &#x26; {\mathrm{I}}_{3} &#x26; \mathrm{P} \end{array} \)</td><td>\( \mathrm{N}\;\mathrm{I} \)</td><td>C</td></tr><tr><td>ScanNet [375]</td><td>2017</td><td>I</td><td>真实</td><td>2.5M</td><td>707</td><td>\( {39},{980}{\mathrm{\;m}}^{2} \)</td><td>网格 \( {}^{\mathrm{R}} \) , CAD</td><td>\( \begin{array}{lllll} {\mathrm{S}}_{2} &#x26; {\mathrm{I}}_{2} &#x26; {\mathrm{\;S}}_{3} &#x26; {\mathrm{I}}_{3} &#x26; \mathrm{P} \end{array} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>RealEstate10K [376]</td><td>2018</td><td>\( \mathrm{I}\;\mathrm{U} \)</td><td>真实</td><td>10M</td><td>-</td><td>-</td><td>-</td><td>P</td><td>\( \mathrm{N}\;\mathrm{I}\;\mathrm{V} \)</td><td>C</td></tr><tr><td>Replica [377]</td><td>2019</td><td>I</td><td>真实</td><td>-</td><td>18</td><td>\( 2,{190}{\mathrm{\;m}}^{2} \)</td><td>网格 \( {}^{\mathrm{R}} \)</td><td>\( {\mathrm{S}}_{3}\;{\mathrm{I}}_{3} \)</td><td>\( \mathrm{N}\;\mathrm{I} \)</td><td>C</td></tr><tr><td>3DSSG [378]</td><td>2020</td><td>I</td><td>真实</td><td>363K</td><td>478</td><td>-</td><td>网格 \( {}^{\mathrm{R}} \)</td><td>\( {\mathrm{S}}_{3}{\mathrm{I}}_{3}\mathrm{{GP}} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>HM3D [379]</td><td>2021</td><td>I</td><td>真实</td><td>-</td><td>1,000</td><td>\( {0.365}\;\mathrm{k{m}^{2}} \)</td><td>网格 \( {}^{\mathrm{R}} \)</td><td>-</td><td>I</td><td>C</td></tr><tr><td>ScanNet++ [380]</td><td>2023</td><td>I</td><td>真实</td><td>11.1M</td><td>1006</td><td>-</td><td>网格 \( {}^{\mathrm{R}} \)</td><td>\( {\mathrm{S}}_{3}{\mathrm{I}}_{3}\mathrm{P} \)</td><td>\( \mathrm{N} \)</td><td>G</td></tr><tr><td>DL3DV-10K [381]</td><td>2024</td><td>\( \mathrm{N} \)\( \mathrm{{NU}} \)</td><td>真实</td><td>51.2M</td><td>-</td><td>-</td><td>-</td><td>\( \mathrm{P} \)</td><td>\( \mathrm{N}\mathrm{V} \)</td><td>G</td></tr><tr><td>SceneSynth [152]</td><td>2012</td><td>I</td><td>合成</td><td>-</td><td>130</td><td>-</td><td>CAD</td><td>-</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>SUNCG [382]</td><td>2017</td><td>I</td><td>合成</td><td>-</td><td>45,622</td><td>\( {24}{\mathrm{\;{km}}}^{2} \)</td><td>CAD</td><td>\( {\mathrm{S}}_{3}\;{\mathrm{I}}_{3} \)</td><td>\( \mathrm{{PN}} \)</td><td></td></tr><tr><td>Structured3D [383]</td><td>2020</td><td>I</td><td>合成</td><td>196.5K</td><td>3500</td><td>-</td><td>CAD</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2}\;{\mathrm{\;S}}_{3}\;{\mathrm{I}}_{3} \)</td><td>\( \mathrm{N}\;\mathrm{I} \)</td><td>C</td></tr><tr><td>Hypersim [384]</td><td>2021</td><td>I</td><td>合成</td><td>77.4K</td><td>461</td><td>-</td><td>CAD</td><td>\( {\mathrm{S}}_{2}{\mathrm{I}}_{2}\mathrm{P} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>3D-FRONT [385]</td><td>2021</td><td>I</td><td>合成</td><td>-</td><td>6,813</td><td>\( {0.51}\;\mathrm{k{m}^{2}} \)</td><td>CAD</td><td>-</td><td>\( \mathrm{{PN}} \)</td><td>G</td></tr><tr><td>SG-FRONT [92]</td><td>2023</td><td>I</td><td>合成</td><td>-</td><td>6,813</td><td>\( {0.51}\;\mathrm{k{m}^{2}} \)</td><td>CAD</td><td>G</td><td>\( \mathrm{N} \)</td><td>G</td></tr><tr><td>Laval户外 [386]</td><td>2017</td><td>\( \mathrm{N} \) U</td><td>真实</td><td>0.2K</td><td>-</td><td>-</td><td>-</td><td>-</td><td>I</td><td>C</td></tr><tr><td>LHQ [387]</td><td>2021</td><td>\( \mathrm{N} \)</td><td>真实</td><td>91.7K</td><td>-</td><td>-</td><td>-</td><td>-</td><td>\( \mathrm{N}\;\mathrm{I} \)</td><td>G</td></tr><tr><td>ACID [33]</td><td>2021</td><td>\( \mathrm{N} \)</td><td>真实</td><td>2.1M</td><td>-</td><td>-</td><td>-</td><td>P</td><td>\( \mathrm{N}\;\mathrm{I}\;\mathrm{V} \)</td><td>G</td></tr><tr><td>KITTI [388]</td><td>2012</td><td>U</td><td>真实</td><td>15K</td><td>1</td><td>-</td><td>激光雷达</td><td>\( {\mathrm{S}}_{2}{\mathrm{I}}_{2}{\mathrm{\;B}}_{3}\mathrm{\;P}\mathrm{\;M} \)</td><td>\( \mathrm{N}\;\mathrm{I} \)</td><td>G</td></tr><tr><td>Cityscapes [389]</td><td>2016</td><td>U</td><td>真实</td><td>25K</td><td>50</td><td>-</td><td>-</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2} \)</td><td>I</td><td>C</td></tr><tr><td>SemanticKITTI [390]</td><td>2019</td><td>U</td><td>真实</td><td>-</td><td>1</td><td>-</td><td>激光雷达</td><td>\( {\mathrm{S}}_{3}\mathrm{P} \)</td><td>\( \mathrm{N} \)</td><td>G</td></tr><tr><td>Waymo [391]</td><td>2020</td><td>U</td><td>真实</td><td>1M</td><td>3</td><td>\( {76}{\mathrm{\;{km}}}^{2} \)</td><td>激光雷达</td><td>\( {\mathrm{B}}_{2}{\mathrm{\;B}}_{3}\mathrm{P} \)</td><td>\( \mathrm{N}\;\mathrm{V} \)</td><td>G</td></tr><tr><td>nuScenes [392]</td><td>2020</td><td>U</td><td>真实</td><td>1.4M</td><td>2</td><td>\( 5{\mathrm{\;{km}}}^{2} \)</td><td>激光雷达</td><td>\( {\mathrm{S}}_{3}{\mathrm{\;B}}_{3}\mathrm{P} \)</td><td>\( \mathrm{N}\mathrm{V} \)</td><td>G</td></tr><tr><td>HoliCity [393]</td><td>2020</td><td>U</td><td>真实</td><td>6.3K</td><td>1</td><td>\( {20}\mathrm{\;{km}}{}^{2} \)</td><td>CAD</td><td>\( {\mathrm{S}}_{2}\mathrm{P} \)</td><td>\( \mathrm{N} \)</td><td>G</td></tr><tr><td>OmniCity [394]</td><td>2023</td><td>U</td><td>真实</td><td>108.6K</td><td>1</td><td>-</td><td>-</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2} \)</td><td>\( \mathrm{N} \)</td><td>G</td></tr><tr><td>KITTI-360 [395]</td><td>2023</td><td>U</td><td>真实</td><td>150K</td><td>1</td><td>-</td><td>激光雷达</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2}\;{\mathrm{S}}_{3}\;{\mathrm{I}}_{3}\;{\mathrm{B}}_{3}\;\mathrm{P} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>GoogleEarth [98]</td><td>2024</td><td>U</td><td>真实</td><td>24K</td><td>1</td><td>\( {25}{\mathrm{\;{km}}}^{2} \)</td><td>体素网格</td><td>\( \begin{array}{lllll} {\mathrm{S}}_{2} &#x26; {\mathrm{I}}_{2} &#x26; {\mathrm{\;S}}_{3} &#x26; {\mathrm{I}}_{3} &#x26; \mathrm{P} \end{array} \)</td><td>\( \mathrm{N} \)</td><td>[7]</td></tr><tr><td>OSM [98]</td><td>2024</td><td>U</td><td>真实</td><td>-</td><td>80</td><td>\( 6,{000}\mathrm{\;{km}}{}^{2} \)</td><td>-</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2} \)</td><td>\( \mathrm{N} \)</td><td>C</td></tr><tr><td>CARLA [13]</td><td>2017</td><td>U</td><td>合成</td><td>\( \infty \)</td><td>13</td><td>-</td><td>激光雷达</td><td>\( {\mathrm{S}}_{2}{\mathrm{I}}_{2}{\mathrm{\;S}}_{3}{\mathrm{I}}_{3}\mathrm{\;P}\mathrm{\;M} \)</td><td>\( \mathrm{N} \)I V</td><td>C</td></tr><tr><td>Virtual-KITTI-2 [396]</td><td>2020</td><td>U</td><td>合成</td><td>42.5K</td><td>1</td><td>-</td><td>-</td><td>\( {\mathrm{S}}_{2}\;{\mathrm{I}}_{2}\;{\mathrm{\;B}}_{2}\;\mathrm{\;P}\;\mathrm{M} \)</td><td>I</td><td>C</td></tr><tr><td>CarlaSC [397]</td><td>2022</td><td>U</td><td>合成</td><td>43.2K</td><td>8</td><td>-</td><td>体素网格</td><td>\( {\mathrm{S}}_{3}\mathrm{\;P}\mathrm{\;M} \)</td><td>\( \mathrm{N} \)</td><td>G</td></tr><tr><td>CityTopia [218]</td><td>2025</td><td>U</td><td>合成</td><td>37.5K</td><td>11</td><td>\( {36}{\mathrm{\;{km}}}^{2} \)</td><td>体素网格</td><td>\( \begin{array}{lllll} {\mathrm{S}}_{2} &#x26; {\mathrm{I}}_{2} &#x26; {\mathrm{\;S}}_{3} &#x26; {\mathrm{I}}_{3} &#x26; \mathrm{P} \end{array} \)</td><td>\( \mathrm{N} \)</td><td>G</td></tr></tbody></table></div><!-- Media --><ul>
<li>KITTI [388], captured in Karlsruhe, includes stereo and optical flow pairs, \({39.2}\mathrm{\;{km}}\) of visual odometry, and \({200}\mathrm{\;K} + 3\mathrm{D}\) object annotations,using a Velodyne LiDAR, GPS/IMU, and a stereo camera rig with grayscale and color cameras.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>KITTI [388]，在卡尔斯鲁厄捕获，包括立体和光流对，\({39.2}\mathrm{\;{km}}\) 视觉里程计和 \({200}\mathrm{\;K} + 3\mathrm{D}\) 物体注释，使用Velodyne激光雷达、GPS/IMU和带有灰度和彩色相机的立体相机设备。</li>
</ul></div><ul>
<li>SemanticKITTI [390] extends KITTI with dense point-wise semantics for full \({360}^{ \circ  }\mathrm{{LiDAR}}\) scans.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>SemanticKITTI [390] 在KITTI的基础上扩展了密集的逐点语义，用于完整的 \({360}^{ \circ  }\mathrm{{LiDAR}}\) 扫描。</li>
</ul></div><ul>
<li>KITTI-360 [395] extends KITTI with \({73.7}\mathrm{\;{km}}\) of driving, 150K+ images, 1B 3D points, and dense 2D/3D labels,using a setup of two \({180}^{ \circ  }\) fisheye side cameras, a front stereo camera, and two LiDARs.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>KITTI-360 [395] 在KITTI的基础上扩展了 \({73.7}\mathrm{\;{km}}\) 驾驶，150K+ 图像，1B 3D点和密集的2D/3D标签，使用两台 \({180}^{ \circ  }\) 鱼眼侧相机、一台前立体相机和两台激光雷达。</li>
</ul></div><ul>
<li>Cityscapes [389] provides street-view videos from 50 cities,with \(5\mathrm{\;K}\) pixel-level and \({20}\mathrm{\;K}\) coarse annotations for strong and weak supervision.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>Cityscapes [389] 提供来自50个城市的街景视频，具有 \(5\mathrm{\;K}\) 像素级和 \({20}\mathrm{\;K}\) 粗略注释，以实现强监督和弱监督。</li>
</ul></div><ul>
<li>Waymo [391] offers \(1\mathrm{M}\) frames from1,150 20s scenes (6.4 hour total) with \({12}\mathrm{M}3\mathrm{D}\) and \({9.9}\mathrm{M}2\mathrm{D}\) boxes,collected in San Francisco, Mountain View, and Phoenix using 5 LiDARs and 5 high-res pinhole cameras.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>Waymo [391] 提供来自1,150个20秒场景（总计6.4小时）的 \(1\mathrm{M}\) 帧，带有 \({12}\mathrm{M}3\mathrm{D}\) 和 \({9.9}\mathrm{M}2\mathrm{D}\) 盒子，收集于旧金山、山景城和凤凰城，使用5个激光雷达和5个高分辨率针孔相机。</li>
</ul></div><ul>
<li>nuScenes [392] provides 1.4M images and \({390}\mathrm{\;K}\) LiDAR sweeps from 1,000 20s scenes in Boston and Singapore, using 6 cameras, 1 LiDAR, 5 RADARs, GPS, and IMU, with 3D box tracking for 23 classes.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>nuScenes [392] 提供来自波士顿和新加坡1,000个20秒场景的1.4M图像和 \({390}\mathrm{\;K}\) 激光雷达扫描，使用6个相机、1个激光雷达、5个雷达、GPS和IMU，并对23个类别进行3D盒子跟踪。</li>
</ul></div><ul>
<li>HoliCity [393] aligns 6,300 high-res panoramas (13312 \(\times\) 6656) with CAD models of downtown London for image-CAD fusion.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>HoliCity [393] 将6,300个高分辨率全景图（13312 \(\times\) 6656）与伦敦市中心的CAD模型对齐，以实现图像-CAD融合。</li>
</ul></div><ul>
<li>OmniCity [394] provides \({100}\mathrm{K} +\) pixel-annotated street,satellite,and panorama images from \({25}\mathrm{\;K}\) locations in New York City.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>OmniCity [394] 提供来自纽约市 \({25}\mathrm{\;K}\) 地点的 \({100}\mathrm{K} +\) 像素注释街道、卫星和全景图像。</li>
</ul></div><ul>
<li>GoogleEarth [98] offers 24K New York images from 400 Google Earth \({}^{2}\) trajectories with \(2\mathrm{D}/3\mathrm{D}\) semantic and instance masks plus camera parameters.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>GoogleEarth [98] 提供来自400个Google Earth \({}^{2}\) 轨迹的24K纽约图像，带有 \(2\mathrm{D}/3\mathrm{D}\) 语义和实例掩码以及相机参数。</li>
</ul></div><ul>
<li>OSM dataset [98],sourced from Open Street Map \({}^{3}\) , provides bird's eye view semantic maps, height fields, and vector data of roads, buildings, and land use across 80+ cities worldwide.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>OSM数据集 [98]，来源于开放街图 \({}^{3}\)，提供鸟瞰视角的语义地图、高度场和80多个城市的道路、建筑和土地使用的矢量数据。</li>
</ul></div><p>Real-world annotations are costly and viewpoint-limited. Synthetic datasets like CARLA [13] and City-Topia [218], built in game engines, provide diverse street and drone views with rich \(2\mathrm{D}/3\mathrm{D}\) annotations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>现实世界的注释成本高且视角有限。像CARLA [13] 和 City-Topia [218] 这样的合成数据集，基于游戏引擎构建，提供多样的街道和无人机视图，具有丰富的 \(2\mathrm{D}/3\mathrm{D}\) 注释。</p></div><ul>
<li>CARLA [13] is an open-source simulator based on Unreal Engine, offering diverse urban environments, sensor simulations (camera, LiDAR, radar), and customizable driving scenarios with control over weather, lighting, traffic, and pedestrian behaviors, enabling unlimited rendering of RGB images and corresponding \(2\mathrm{D}/3\mathrm{D}\) annotations.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>CARLA [13] 是一个基于虚幻引擎的开源模拟器，提供多样的城市环境、传感器模拟（相机、激光雷达、雷达）和可定制的驾驶场景，控制天气、光照、交通和行人行为，实现RGB图像及相应的 \(2\mathrm{D}/3\mathrm{D}\) 注释的无限渲染。</li>
</ul></div><ul>
<li>CarlaSC [397] offers \({43.2}\mathrm{\;K}\) frames of semantic scenes from 24 sequences across 8 maps, captured by virtual LiDAR sensors in the CARLA simulator under varying traffic conditions.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>CarlaSC [397] 提供来自8个地图的24个序列的 \({43.2}\mathrm{\;K}\) 语义场景帧，由CARLA模拟器中的虚拟激光雷达传感器在不同交通条件下捕获。</li>
</ul></div><ul>
<li>Virtual-KITTI-2 [396] replicates 5 KITTI sequences using Unity, offering photorealistic video under varying conditions with dense annotations for depth, segmentation, optical flow, and object tracking.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>Virtual-KITTI-2 [396] 使用Unity复制5个KITTI序列，提供在不同条件下的照片级真实视频，并对深度、分割、光流和物体跟踪进行密集注释。</li>
</ul></div><ul>
<li>CityTopia [218] provides 37.5K photorealistic frames with fine-grained \(2\mathrm{D}/3\mathrm{D}\) annotations from 11 procedural cities in Unreal Engine, featuring varied lighting and aerial/street-view perspectives.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>CityTopia [218] 提供来自11个程序生成城市的37.5K照片级真实帧，具有细粒度的 \(2\mathrm{D}/3\mathrm{D}\) 注释，采用虚幻引擎，具有多样的光照和空中/街景视角。</li>
</ul></div><h3>4.2 Evaluation</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.2 评估</h3></div><p>Evaluating 3D scene generation methods is essential for comparing different methods across different domains. Various metrics have been proposed to assess key aspects of generated scenes, including geometric accuracy, structural consistency, visual realism, diversity, and physical plausibility. This section summarizes and discusses commonly used evaluation metrics in 3D scene generation, highlighting their relevance to different generation paradigms and focuses.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>评估3D场景生成方法对于比较不同领域的不同方法至关重要。已经提出了各种指标来评估生成场景的关键方面，包括几何准确性、结构一致性、视觉真实感、多样性和物理合理性。本节总结并讨论了3D场景生成中常用的评估指标，强调它们与不同生成范式和重点的相关性。</p></div><h4>4.2.1 Metrics-based Evaluation</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.2.1 基于指标的评估</h4></div><p>Fidelity is evaluated by using metrics from image and video generation to assess the visual quality and realism of generated scenes, particularly for renderable outputs like NeRFs, 3D Gaussians, or image sequences. Fréchet Inception Distance (FID) [400], Kernel Inception Distance (KID) [401], and Inception Score (IS) [402] are widely used to evaluate the distributional similarity between rendered images and real samples. FID and KID compute statistical distances between feature distributions extracted from a pre-trained Inception network, while IS measures both image quality and diversity based on classification confidence. SwAV-FID [403], FDD [404], and FID \({}_{\text{CLIP }}\) [405] explore alternative feature spaces for better correlation with human evaluations. No-reference image quality metrics such as Natural Image Quality Evaluator (NIQE) [406], Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) [407] are used to estimate perceptual quality directly from the image statistics. CLIP-IQA [408] combines CLIP features with learned IQA models to better align with human perception under textual or semantic conditioning. For assessing photorealism specifically in 3D space, F3D [234] is a 3D adaptation of FID, which is based on a pre-trained autoencoder with a 3D CNN architecture. In addition to perceptual scores, some metrics evaluate distributional alignment between generated and real samples. Minimum Matching Distance (MMD) [409] quantifies the average pairwise distance between closest points across distributions, Coverage (COV) [409] measures how well generated samples cover the target distribution, and 1-Nearest Neighbor Accuracy (1-NNA) [410] estimates mode collapse or overfitting by classifying samples using nearest-neighbor retrieval.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>保真度通过使用图像和视频生成的指标来评估生成场景的视觉质量和真实感，特别是对于可渲染的输出，如NeRF、3D高斯或图像序列。Fréchet Inception Distance (FID) [400]、Kernel Inception Distance (KID) [401]和Inception Score (IS) [402]被广泛用于评估渲染图像与真实样本之间的分布相似性。FID和KID计算从预训练的Inception网络提取的特征分布之间的统计距离，而IS则基于分类置信度测量图像质量和多样性。SwAV-FID [403]、FDD [404]和FID \({}_{\text{CLIP }}\) [405]探索替代特征空间，以更好地与人类评估相关。无参考图像质量指标，如自然图像质量评估器 (NIQE) [406]、盲/无参考图像空间质量评估器 (BRISQUE) [407]用于直接从图像统计中估计感知质量。CLIP-IQA [408]将CLIP特征与学习的IQA模型结合，以更好地与文本或语义条件下的人类感知对齐。为了专门评估3D空间中的照片真实感，F3D [234]是FID的3D适应版本，基于具有3D CNN架构的预训练自编码器。除了感知分数外，一些指标还评估生成样本与真实样本之间的分布对齐。最小匹配距离 (MMD) [409]量化分布之间最近点的平均成对距离，覆盖率 (COV) [409]测量生成样本覆盖目标分布的程度，1-最近邻准确率 (1-NNA) [410]通过使用最近邻检索对样本进行分类来估计模式崩溃或过拟合。</p></div><p>Spatial Consistency metrics assess the \(3\mathrm{D}\) geometry and multi-view alignment of the generated scenes. For depth error, pseudo ground-truth depth maps can be obtained using state-of-the-art monocular depth estimation models, while the depth map of the scene can be obtained using reliable Structure-from-Motion (SfM) pipelines such as COLMAP [66]. For camera pose error, COLMAP is also used to estimate camera trajectories from the rendered sequences. The distances between these predicted values and ground-truth are computed using distance functions, such as L2 distance, RMSE, and Scale-Invariant Root Mean Square Error (SI-RMSE) [411].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>空间一致性指标评估生成场景的\(3\mathrm{D}\)几何形状和多视图对齐。对于深度误差，可以使用最先进的单目深度估计模型获得伪地面真实深度图，而场景的深度图可以使用可靠的运动重建 (SfM) 流水线，如COLMAP [66]获得。对于相机姿态误差，COLMAP也用于从渲染序列中估计相机轨迹。这些预测值与地面真实值之间的距离使用距离函数计算，如L2距离、均方根误差 (RMSE) 和尺度不变均方根误差 (SI-RMSE) [411]。</p></div><p>Temporal Coherence is a critical metric for evaluating the generated \(3\mathrm{D}\) scenes across time,particularly in dynamic scenes or video-based outputs. Flow warping error (FE) [412] measures the temporal stability of a video by computing the warping error of optical flow between two frames. Fréchet Video Distance (FVD) [413] builds on the principles underlying FID and introduces a different feature representation that captures the temporal coherence of a video, in addition to the quality of each frame. Focusing on the complex motion patterns in generated videos, Fréchet Video Motion Distance (FVMD) [414] designs explicit motion features based on keypoint tracking, measuring the similarity between these features via the Fréchet distance for evaluating the motion coherence of the generated videos.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>时间一致性是评估生成\(3\mathrm{D}\)场景在时间上的关键指标，特别是在动态场景或基于视频的输出中。流变形误差 (FE) [412]通过计算两个帧之间光流的变形误差来测量视频的时间稳定性。Fréchet Video Distance (FVD) [413]基于FID的原理，并引入不同的特征表示，捕捉视频的时间一致性，除了每帧的质量。关注生成视频中的复杂运动模式，Fréchet Video Motion Distance (FVMD) [414]基于关键点跟踪设计显式运动特征，通过Fréchet距离测量这些特征之间的相似性，以评估生成视频的运动一致性。</p></div><p>Controllability evaluates the ability to respond to user inputs. CLIP Score [415] leverages a pre-trained CLIP model to measure the alignment between generated images and conditioning text, reflecting how faithfully the generation follows user-specified prompts.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>可控性评估响应用户输入的能力。CLIP Score [415]利用预训练的CLIP模型来测量生成图像与条件文本之间的对齐程度，反映生成如何忠实地遵循用户指定的提示。</p></div><p>Diversity means the ability to produce varied outputs. Category distribution KL divergence (CKL) [177] compares the object category distribution in the synthesized scenes to that of the training set, with lower divergence indicating better diversity. Scene Classification Accuracy (SCA) [177] uses a trained classifier to distinguish between real and generated scenes, measuring how well the distribution of synthetic scenes matches that of real scenes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>多样性意味着产生多样化输出的能力。类别分布KL散度 (CKL) [177]比较合成场景中的物体类别分布与训练集的分布，较低的散度表示更好的多样性。场景分类准确率 (SCA) [177]使用训练好的分类器区分真实场景和生成场景，测量合成场景的分布与真实场景的分布的匹配程度。</p></div><p>Plausibility measures how well generated scenes obey physical and semantic constraints. Collision rate measures the proportion of collision objects among all generated objects within a scene. Out-of-bounds object area (OBA) assesses the cumulated out-of-bounds object area in a scene.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>合理性衡量生成场景遵循物理和语义约束的程度。碰撞率衡量场景中所有生成物体中碰撞物体的比例。越界物体面积 (OBA) 评估场景中累积的越界物体面积。</p></div><hr>
<!-- Footnote --><ol start="2">
<li><a href="https://earth.google.com/studio">https://earth.google.com/studio</a></li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="2">
<li><a href="https://earth.google.com/studio">https://earth.google.com/studio</a></li>
</ol></div><ol start="3">
<li><a href="https://openstreetmap.org">https://openstreetmap.org</a></li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="3">
<li><a href="https://openstreetmap.org">https://openstreetmap.org</a></li>
</ol></div><!-- Footnote -->
<hr><h4>4.2.2 Benchmark-based Evaluation</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.2.2 基于基准的评估</h4></div><p>To promote fair, reproducible, and comprehensive evaluation of diverse \(3\mathrm{D}\) scene generation methods,recent research has increasingly embraced standardized benchmark suites that integrate multiple metrics, task configurations, and quality dimensions. This trend marks a shift from relying only on isolated quantitative metrics to adopting more holistic, task-aligned evaluations that better reflect the complexity of real-world applications.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了促进对多样化\(3\mathrm{D}\)场景生成方法的公平、可重复和全面的评估，最近的研究越来越多地采用标准化基准套件，这些套件整合了多个指标、任务配置和质量维度。这一趋势标志着从仅依赖孤立的定量指标转向采用更全面、与任务对齐的评估，这些评估更好地反映了现实世界应用的复杂性。</p></div><p>Q-Align [416] adopts large multi-modal models (LMMs) to predict visual quality scores that align with human judgment. It covers three core dimensions: Image Quality Assessment (IQA), Image Aesthetic Assessment (IAA), and Video Quality Assessment (VQA). During inference, the mean opinion scores are collected and re-weighted to obtain the LMM-predicted score.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Q-Align [416] 采用大型多模态模型（LMMs）来预测与人类判断一致的视觉质量评分。它涵盖三个核心维度：图像质量评估（IQA）、图像美学评估（IAA）和视频质量评估（VQA）。在推理过程中，收集平均意见分数并重新加权以获得LMM预测的分数。</p></div><p>VideoScore [417] enables video quality evaluation by training on a large-scale human-feedback dataset. It provides assessment across five aspects: Visual Quality (VQ), Temporal Consistency (TC), Dynamic Degree (DD), Text-to-Video Alignment (TVA), and Factual Consistency (FC).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>VideoScore [417] 通过在大规模人类反馈数据集上进行训练来实现视频质量评估。它在五个方面提供评估：视觉质量（VQ）、时间一致性（TC）、动态程度（DD）、文本到视频对齐（TVA）和事实一致性（FC）。</p></div><p>VBench [418] and VBench++ [419] are comprehensive and versatile benchmark suites for video generation. They comprise 16 dimensions in video generation (e.g., subject identity inconsistency, motion smoothness, temporal flickering, and spatial relationship, etc). VBench-2.0 [420] further addresses more complex challenges associated with intrinsic faithfulness, including commonsense reasoning, physics-based realism, human motion, and creative composition.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>VBench [418] 和 VBench++ [419] 是用于视频生成的全面且多功能的基准套件。它们包含视频生成中的16个维度（例如，主体身份不一致、运动平滑性、时间闪烁和空间关系等）。VBench-2.0 [420] 进一步解决与内在真实性相关的更复杂挑战，包括常识推理、基于物理的现实主义、人类运动和创意构图。</p></div><p>WorldScore [421] unifies the evaluation of \(3\mathrm{D},4\mathrm{D}\) ,and video models on their ability to generate a world following instructions. It formulates the evaluation of \(3\mathrm{D}\) scene generation to a sequence of next-scene generation tasks guided by camera trajectories, jointly measures controllability, quality, and dynamics in various fine-grained features.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>WorldScore [421] 统一了对\(3\mathrm{D},4\mathrm{D}\)和视频模型在生成遵循指令的世界方面的评估。它将\(3\mathrm{D}\)场景生成的评估形式化为一系列由相机轨迹引导的下一场景生成任务，联合测量各种细粒度特征中的可控性、质量和动态性。</p></div><h4>4.2.3 Human Evaluation</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.2.3 人类评估</h4></div><p>User studies remain an essential component for capturing subjective qualities of \(3\mathrm{D}\) scene generation that are difficult to quantify through automated metrics, such as visual appeal, realism, and perceptual coherence.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>用户研究仍然是捕捉\(3\mathrm{D}\)场景生成主观质量的重要组成部分，这些质量通过自动化指标难以量化，例如视觉吸引力、现实主义和感知一致性。</p></div><p>Participants are typically asked to rank or rate generated scenes based on multiple aspects, including photorealism, aesthetics, input alignment (e.g., text or layout), 3D consistency across views, and physical or semantic plausibility. Ideally, participants should include both domain experts (e.g., 3D artists, designers, researchers) and normal users. Their feedback offers complementary perspectives: experts may provide more critical and structured insights, while non-experts better reflect general user impressions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>参与者通常被要求根据多个方面对生成的场景进行排名或评分，包括照片真实感、美学、输入对齐（例如，文本或布局）、视图间的3D一致性，以及物理或语义的合理性。理想情况下，参与者应包括领域专家（例如，3D艺术家、设计师、研究人员）和普通用户。他们的反馈提供了互补的视角：专家可能提供更具批判性和结构化的见解，而非专家则更好地反映一般用户的印象。</p></div><p>Although human evaluations are resource-intensive and inherently subjective, they provide essential qualitative insights that complement other evaluation methods by capturing human preferences in real-world contexts. Platforms like Prolific \({}^{4}\) and Amazon Mechanical Turk (AMT) \({}^{5}\) facilitate the recruitment of diverse participants and enable efficient scaling of user studies.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>尽管人类评估资源密集且本质上主观，但它们提供了重要的定性见解，补充了其他评估方法，通过捕捉现实世界背景下的人类偏好。像Prolific \({}^{4}\)和亚马逊机械土耳其（AMT） \({}^{5}\)这样的平台促进了多样参与者的招募，并实现了用户研究的高效扩展。</p></div><h2>5 APPLICATIONS AND TASKS</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>5 应用与任务</h2></div><p>The rapid progress in \(3\mathrm{D}\) scene generation has enabled diverse applications across various related domains. This section highlights key areas of \(3\mathrm{D}\) scene generation applications, including 3D scene editing, human-scene interaction, embodied AI, robotics, and autonomous driving.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在\(3\mathrm{D}\)场景生成方面的快速进展使得在各种相关领域实现了多样化的应用。本节重点介绍\(3\mathrm{D}\)场景生成应用的关键领域，包括3D场景编辑、人场景交互、具身人工智能、机器人技术和自动驾驶。</p></div><h3>5.1 3D Scene Editing</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.1 3D场景编辑</h3></div><p>3D scene editing involves altering a scene's appearance and structure, from individual object modifications to complete environment customization. It broadly includes texture editing, which focuses on generating stylized or realistic surface appearances, and layout editing, which involves arranging objects in a physically and semantically plausible manner.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D场景编辑涉及改变场景的外观和结构，从单个对象的修改到完整环境的定制。它广泛包括纹理编辑，专注于生成风格化或真实的表面外观，以及布局编辑，涉及以物理和语义合理的方式排列对象。</p></div><p>Texturing and stylization aim to create aesthetic and stylish appearances based on user specifications. While recent advances achieve impressive results on scanned meshes [422], [423], [424] or synthetic indoor datasets [425], [426], [427], they are constrained by incomplete geometry from reconstructions or extensive manual modeling. To address these limitations, recent methods leverage 3D scene generation to synthesize complete and semantically consistent scenes, directly supporting texture generation tasks. Methods such as Ctrl-Room [190], ControlRoom3D [261], RoomTex [428], and DreamSpace [429] employ holistic generation techniques to create panoramic room textures, followed by detailed refinement. Beyond direct generation, 3D scene generation also benefits the evaluation of texturing methods. InstanceTex [430] generates texture across both existing datasets and new scenes generated by EchoScene [208], improving the diversity and robustness of benchmark evaluations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>纹理化和风格化旨在根据用户规格创建美观和时尚的外观。尽管最近的进展在扫描网格[422]、[423]、[424]或合成室内数据集[425]、[426]、[427]上取得了令人印象深刻的结果，但它们受到重建或广泛手动建模所导致的不完整几何形状的限制。为了解决这些限制，最近的方法利用3D场景生成合成完整且语义一致的场景，直接支持纹理生成任务。像Ctrl-Room [190]、ControlRoom3D [261]、RoomTex [428]和DreamSpace [429]等方法采用整体生成技术创建全景房间纹理，随后进行详细的精细化。除了直接生成，3D场景生成还促进了纹理方法的评估。InstanceTex [430] 在现有数据集和由EchoScene [208]生成的新场景中生成纹理，提高了基准评估的多样性和鲁棒性。</p></div><p>3D scene layout editing focuses on arranging objects within a scene to produce semantically meaningful and physically plausible configurations. Several methods, such as LEGO-Net [431], CabiNet [432], and DeBaRA [185], address the rearrangement of existing scenes. These approaches use object-level attributes, such as class labels, positions, and orientations, to produce more organized and regular arrangements. Some methods support more interactive and dynamic layout editing. For example, SceneEx-pander [433] and SceneDirector [434] enable real-time editing through intuitive user interactions, such as modifying room shapes or moving objects, and automatically update surrounding objects to maintain spatial coherence. Recent advances in compositional generative NeRF further push the boundary of layout control to enable editing of implicit representation. DisCoScene [220], Neural Assets [435], and Lift3D [436] enable object-level editing by adjusting control signals such as spatial locations or latent features, allowing for flexible and controllable scene manipulation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D场景布局编辑专注于在场景中排列对象，以产生语义上有意义和物理上合理的配置。几种方法，如LEGO-Net [431]、CabiNet [432]和DeBaRA [185]，解决了现有场景的重新排列。这些方法使用对象级属性，如类别标签、位置和方向，来生成更有组织和规则的排列。一些方法支持更互动和动态的布局编辑。例如，SceneEx-pander [433]和SceneDirector [434]通过直观的用户交互实现实时编辑，如修改房间形状或移动对象，并自动更新周围对象以保持空间一致性。最近在组合生成NeRF方面的进展进一步推动了布局控制的边界，使得隐式表示的编辑成为可能。DisCoScene [220]、Neural Assets [435]和Lift3D [436]通过调整控制信号，如空间位置或潜在特征，实现对象级编辑，从而允许灵活和可控的场景操作。</p></div><hr>
<!-- Footnote --><ol start="4">
<li><a href="https://www.prolific.com">https://www.prolific.com</a></li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="4">
<li><a href="https://www.prolific.com">https://www.prolific.com</a></li>
</ol></div><ol start="5">
<li><a href="https://www.mturk.com">https://www.mturk.com</a></li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="5">
<li><a href="https://www.mturk.com">https://www.mturk.com</a></li>
</ol></div><!-- Footnote -->
<hr><h3>5.2 Human-Scene Interaction</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.2 人类-场景交互</h3></div><p>Human-Scene Interaction (HSI) focuses on modeling how humans interact with and influence their environment. Realistic character animation and behavior modeling require synthesizing believable interactions between virtual characters and their environments. Recent advances in HSI have made notable progress in generating realistic and physically plausible human motions within 3D environments [437], [438], [439], as well as creating scenes that align with specific motion sequences [87], [197], [198].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>人类-场景交互（HSI）专注于建模人类如何与环境互动并影响环境。逼真的角色动画和行为建模需要合成虚拟角色与其环境之间可信的互动。最近在HSI方面的进展在生成3D环境中逼真和物理上合理的人类动作方面取得了显著进展 [437]、[438]、[439]，以及创建与特定动作序列一致的场景 [87]、[197]、[198]。</p></div><p>To generate human motion conditioned on scene environments, some approaches [437], [440], [441], [442] directly learn from datasets containing scanned indoor scenes and captured human motion [443], [444], [445]. However, these datasets are often limited in scalability and restricted to static scenes, prohibiting the modeling of dynamic human-object interactions. Some other works [438], [439], [446], [447], [448] employ simulated environments with reinforcement learning to generate physically plausible motion. Yet, due to high setup costs, these simulations often rely on simplified scenes, introducing a sim-to-real gap between synthetic training and real-world applications where environments are more complex and diverse.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了生成基于场景环境的人类动作，一些方法 [437]、[440]、[441]、[442] 直接从包含扫描室内场景和捕捉人类动作的数据集中学习 [443]、[444]、[445]。然而，这些数据集通常在可扩展性方面有限，并且仅限于静态场景，禁止建模动态的人类-物体交互。一些其他工作 [438]、[439]、[446]、[447]、[448] 采用强化学习的模拟环境来生成物理上合理的动作。然而，由于高昂的设置成本，这些模拟通常依赖于简化的场景，导致合成训练与现实世界应用之间存在模拟与现实的差距，因为现实环境更复杂和多样。</p></div><p>Recent efforts like GenZI [449] have initially addressed this issue by lifting the generated human in \(2\mathrm{D}\) images into 3D, enabling zero-shot generalization to novel scenes. Although GenZI still depends on pre-designed synthetic scenes for evaluation, it highlights the potential of combining scene generation with motion generation to scale HSI data more effectively. Integrating high-quality 3D scene generation is essential for advancing scalable and realistic HSI research, particularly by jointly considering human affordances, motion feasibility, and scene semantics.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最近的努力，如GenZI [449]，最初通过将生成的人类在 \(2\mathrm{D}\) 图像中提升到3D，解决了这个问题，实现了对新场景的零样本泛化。尽管GenZI仍然依赖于预设计的合成场景进行评估，但它突显了将场景生成与动作生成相结合的潜力，以更有效地扩展HSI数据。整合高质量的3D场景生成对于推动可扩展和逼真的HSI研究至关重要，特别是通过共同考虑人类能力、动作可行性和场景语义。</p></div><h3>5.3 Embodied AI</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.3 具身人工智能</h3></div><p>In embodied AI, agents interact with environments to develop high-level semantic understanding and goal-directed behaviors. 3D scene generation supports this by providing visually and functionally rich environments that enable tasks like navigation, exploration, and instruction following, with an emphasis on cognitive reasoning over precise physical control.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在具身人工智能中，代理与环境互动，以发展高级语义理解和目标导向行为。3D场景生成通过提供视觉上和功能上丰富的环境来支持这一点，使得导航、探索和遵循指令等任务成为可能，强调认知推理而非精确的物理控制。</p></div><p>Simulated environments are typically built from reconstructed real-world data [379], [450] or manually designed scenes [451], [452], but both approaches have limitations: real-world datasets suffer from quality and annotation issues, while manual creation is labor-intensive and difficult to scale. In this context, 3D scene generation offers a scalable, diverse, and physically plausible alternative for creating simulated environments for embodied AI research. For indoor environments, ProcTHOR [15] uses procedural generation to produce scenes that follow realistic layouts and physical constraints. Holodeck [175] leverages LLM to generate \(3\mathrm{D}\) environments that match user-supplied prompts automatically. InfiniteWorld [453] further expands assets with different textures for more diverse and stylish scenes. PhyScene [186] integrates physics and interactivity constraints into a conditional diffusion model to synthesize plausibly interactive environments. Architect [454] employs iterative image-based inpainting to populate scenes with large furniture and small objects, enriching scene complexity. Beyond indoor settings, procedural methods have also enabled city-scale simulation. MetaUrban [17], GRU-topia [16], and URBAN-SIM [455] construct diverse, large-scale urban environments for embodied agents. Embod-iedCity [456] provides a high-quality 3D real environment based on a real city, supporting various agents, continuous decision-making, and systematic benchmark tasks for embodied intelligence.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>模拟环境通常是从重建的现实世界数据 [379]、[450] 或手动设计的场景 [451]、[452] 构建的，但这两种方法都有局限性：现实世界数据集存在质量和注释问题，而手动创建则劳动密集且难以扩展。在这种情况下，3D场景生成为创建用于具身人工智能研究的模拟环境提供了一种可扩展、多样和物理上合理的替代方案。对于室内环境，ProcTHOR [15] 使用过程生成来生成遵循现实布局和物理约束的场景。Holodeck [175] 利用LLM自动生成与用户提供的提示匹配的 \(3\mathrm{D}\) 环境。InfiniteWorld [453] 进一步扩展了具有不同纹理的资产，以创建更丰富和时尚的场景。PhyScene [186] 将物理和互动约束集成到条件扩散模型中，以合成可信的互动环境。Architect [454] 采用迭代图像修复技术为场景填充大型家具和小物件，丰富场景复杂性。除了室内环境，过程方法还使城市规模的模拟成为可能。MetaUrban [17]、GRU-topia [16] 和URBAN-SIM [455] 为具身代理构建多样化的大规模城市环境。EmbodiedCity [456] 提供了基于真实城市的高质量3D真实环境，支持各种代理、连续决策和系统基准任务，以促进具身智能。</p></div><h3>5.4 Robotics</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.4 机器人技术</h3></div><p>In robotics, 3D scene generation enables learning of low-level skills like manipulation and control within physically realistic environments. These scenes are typically embedded in simulators, where accurate modeling of dynamics and contact is crucial for training robots to perceive, plan, and act effectively in the real world.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在机器人技术中，3D场景生成使得在物理上真实的环境中学习低级技能，如操作和控制成为可能。这些场景通常嵌入在模拟器中，其中动态和接触的准确建模对于训练机器人有效感知、规划和行动至关重要。</p></div><p>Simulated environments have become a central tool for developing robotic capabilities across various tasks, including complex manipulation and locomotion. However, recent approaches in robot learning [457], [458], [459], [460], [461], [462] require tremendous human effort to construct these environments and the corresponding demonstrations, restricting the scalability of robot learning even in simulated worlds. RoboGen [463] and RoboVerse [464] automate task, scene, and supervision generation through a propose-generate-learn cycle, where agents propose skills, generate environments with plausible object layouts, and learn with minimal human input. Eurekaverse [465] further scales skill learning by using LLMs to progressively generate diverse and increasingly challenging terrains, forming an adaptive curriculum for parkour training.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>模拟环境已成为开发机器人在各种任务（包括复杂操作和运动）能力的核心工具。然而，最近的机器人学习方法[457]，[458]，[459]，[460]，[461]，[462]需要大量人力来构建这些环境及相应的演示，限制了机器人学习在模拟世界中的可扩展性。RoboGen[463]和RoboVerse[464]通过提出-生成-学习循环自动化任务、场景和监督生成，其中代理提出技能，生成具有合理物体布局的环境，并在最小人力输入下进行学习。Eurekaverse[465]进一步通过使用大型语言模型（LLMs）逐步生成多样化和日益具有挑战性的地形，形成适应性课程以进行跑酷训练。</p></div><p>Beyond explicitly constructing simulated environments, 3D scene generation also serves as world models for predicting future frames that visually represent intended actions, enabling robots to simulate and predict complex manipulation tasks in virtual settings. Robotics-focused video generation models [466], [467], [468], [469], [470], [471], [472], [473], [474], [475], [476] aim to synthesize videos conditioned on inputs like text or images, specifically to help robots visualize and plan complex manipulation tasks by predicting future action sequences in a physically plausible way. Instead of directly generating video frames, some methods [477], [478], [479] leverage NeRFs and dynamic 3D Gaussians to capture the spatial and semantic complexity of real-world environments, enabling more accurate motion estimation and planning.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>除了明确构建模拟环境，3D场景生成还作为世界模型，用于预测未来帧，直观地表示预期动作，使机器人能够在虚拟环境中模拟和预测复杂的操作任务。专注于机器人技术的视频生成模型[466]，[467]，[468]，[469]，[470]，[471]，[472]，[473]，[474]，[475]，[476]旨在合成基于文本或图像等输入条件的视频，特别是帮助机器人通过以物理合理的方式预测未来动作序列来可视化和规划复杂的操作任务。一些方法[477]，[478]，[479]则利用神经辐射场（NeRFs）和动态3D高斯来捕捉现实环境的空间和语义复杂性，从而实现更准确的运动估计和规划。</p></div><h3>5.5 Autonomous Driving</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.5 自动驾驶</h3></div><p>3D scene generation is increasingly important in autonomous driving, offering controllable, scalable, and diverse simulations of real-world environments. These capabilities help overcome limitations of real-world datasets and environments. It supports key components of self-driving systems, such as predictive modeling and data generation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D场景生成在自动驾驶中变得越来越重要，提供可控、可扩展和多样化的现实环境模拟。这些能力有助于克服现实数据集和环境的局限性。它支持自动驾驶系统的关键组件，如预测建模和数据生成。</p></div><p>Several 3D scene generation methods serve as world models for autonomous driving, enabling future scene prediction, risk anticipation, and the planning of safer, more efficient actions. Some [39], [118], [337], [338], [339], [355], [364], [366] focus on predicting future video frames, while others [480], [481], [482], [483], [484] generate 3D occupancies to model the environment explicitly. With high-fidelity generation, DriveArena [351] and DrivingSphere [359] introduce closed-loop simulators for training and evaluating autonomous driving agents, enabling agents to learn and evolve in a closed-loop manner continuously.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>几种3D场景生成方法作为自动驾驶的世界模型，使未来场景预测、风险预判和更安全、更高效的行动规划成为可能。一些方法[39]，[118]，[337]，[338]，[339]，[355]，[364]，[366]专注于预测未来视频帧，而其他方法[480]，[481]，[482]，[483]，[484]则生成3D占用模型以明确建模环境。通过高保真生成，DriveArena[351]和DrivingSphere[359]引入了闭环模拟器，用于训练和评估自动驾驶代理，使代理能够以闭环方式持续学习和进化。</p></div><p>Autonomous driving demands large, diverse datasets, but real-world collections like nuScenes [392], KITTI [388], and Waymo [391] are costly and rarely capture critical corner cases. Controllable video-based generation methods [341], [343], [344], [345], [353] address this by synthesizing diverse driving scenarios with flexible control over weather, lighting, and traffic conditions, especially for rare and safety-critical events. 6 Challenges and Future Directions</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>自动驾驶需要大量多样化的数据集，但现实世界的数据集如nuScenes[392]、KITTI[388]和Waymo[391]成本高昂，且很少捕捉到关键的边缘案例。可控的视频生成方法[341]，[343]，[344]，[345]，[353]通过合成多样化的驾驶场景，灵活控制天气、光照和交通条件，特别是针对稀有和安全关键事件，解决了这一问题。6 挑战与未来方向</p></div><h3>6.1 Challenges</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.1 挑战</h3></div><p>Despite recent advancements, 3D scene generation still has significant potential for improvement.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>尽管最近取得了进展，3D场景生成仍有显著的改进潜力。</p></div><p>Generative Capacity. Existing generative models exhibit a trade-off in jointly satisfying photorealism, 3D consistency, and controllability. Procedural and neural 3D-based approaches excel at generating geometrically coherent scenes with controllable spatial layouts, but often fall short in producing photorealistic textures and lighting. In contrast, image- and video-based generation models achieve high visual realism, yet struggle to maintain 3D consistency, resulting in artifacts such as distorted geometry, unrealistic object interactions, or implausible physical dynamics. As a result, current models still find it challenging to synthesize complex, multi-object scenes that are both visually plausible and physically grounded.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>生成能力。现有的生成模型在同时满足照片真实感、3D一致性和可控性方面存在权衡。程序化和基于神经网络的3D方法在生成几何一致的场景和可控的空间布局方面表现出色，但在生成照片真实的纹理和光照方面往往不尽如人意。相比之下，基于图像和视频的生成模型实现了高视觉真实感，但在保持3D一致性方面存在困难，导致出现失真几何、不现实的物体交互或不合理的物理动态等伪影。因此，当前模型仍然难以合成既视觉上合理又物理上扎实的复杂多物体场景。</p></div><p>3D Representation. The evolution of 3D scene representations has progressed from geometry-centric formats such as voxel grids and point clouds, both of which struggle to capture photorealistic appearance, to NeRFs, which improve visual quality but remain inefficient and lack explicit geometry. Recent advances like 3D Gaussians offer better efficiency but still lack geometric grounding, limiting their applicability to tasks like relighting or physical interaction. Mesh- and Bézier-triangle-based methods [485], [486], [487] partially address these limitations by introducing explicit surface representations, yet they are largely confined to object-level generation. Scene-level representations that are compact, physically meaningful, and visually realistic remain an open challenge, hindering progress in controllable and generalizable \(3\mathrm{D}\) scene generation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D表示。3D场景表示的发展经历了从几何中心格式（如体素网格和点云）到NeRFs的演变，后者提高了视觉质量，但效率低下且缺乏明确的几何结构。最近的进展如3D高斯提供了更好的效率，但仍然缺乏几何基础，限制了其在重新照明或物理交互等任务中的适用性。基于网格和贝塞尔三角形的方法[485]，[486]，[487]部分解决了这些限制，通过引入明确的表面表示，但它们在很大程度上仍限于物体级生成。紧凑、物理上有意义且视觉上真实的场景级表示仍然是一个开放的挑战，阻碍了可控和可推广的\(3\mathrm{D}\)场景生成的进展。</p></div><p>Data and Annotations. The progress of \(3\mathrm{D}\) scene generation is tightly bound to dataset quality. Synthetic datasets offer precise annotations but suffer from limited content diversity and suboptimal photorealism due to rendering constraints in current game engines. In contrast, real-world scans provide visually realistic imagery but often lack sufficient annotations. While image- and video-based generative methods alleviate annotation needs, they still struggle to capture accurate 3D geometry, often resulting in spatial distortions. Additionally, existing datasets rarely include rich meta-data, such as physical affordances, material attributes, or interaction cues, hindering broader applications in robotics, embodied AI, and physical simulation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>数据与注释。\(3\mathrm{D}\)场景生成的进展与数据集质量密切相关。合成数据集提供精确的注释，但由于当前游戏引擎的渲染限制，内容多样性有限且光真实感不佳。相比之下，现实世界的扫描提供了视觉上真实的图像，但通常缺乏足够的注释。虽然基于图像和视频的生成方法减轻了注释需求，但它们仍然难以捕捉准确的三维几何形状，常常导致空间扭曲。此外，现有数据集很少包含丰富的元数据，如物理可用性、材料属性或交互线索，这限制了在机器人技术、具身人工智能和物理仿真中的更广泛应用。</p></div><p>Evaluation. A persistent challenge in 3D scene generation is the lack of unified evaluation protocols. Methods often rely on disparate metrics, hindering consistent comparison. Benchmark-based efforts [420], [421] have partially addressed this by introducing standardized and human-aligned evaluation frameworks. However, current benchmarks are largely conditioned on text or images, with limited support for other inputs such as layouts, actions, or trajectories. Moreover, evaluations still primarily focus on image and video fidelity, offering an insufficient assessment of underlying 3D geometry and physical plausibility. Recent work like Eval3D [488] introduces a benchmark that begins to address 3D structural, semantic, and geometric consistency, though it remains limited to object-level generation and lacks scene-level complexity.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>评估。三维场景生成中的一个持续挑战是缺乏统一的评估协议。方法通常依赖于不同的指标，妨碍了一致的比较。基于基准的努力[420]，[421]通过引入标准化和人类对齐的评估框架部分解决了这个问题。然而，当前的基准主要依赖于文本或图像，对其他输入如布局、动作或轨迹的支持有限。此外，评估仍主要集中在图像和视频的保真度上，无法充分评估潜在的三维几何形状和物理合理性。最近的工作如Eval3D[488]引入了一个基准，开始解决三维结构、语义和几何一致性的问题，尽管它仍然局限于对象级生成，缺乏场景级复杂性。</p></div><h3>6.2 Future Directions</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.2 未来方向</h3></div><p>Given the substantial progress made and the key challenges outlined above, we believe that future research in 3D scene generation can advance in the following directions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>鉴于取得的重大进展和上述关键挑战，我们相信未来在三维场景生成方面的研究可以朝以下方向发展。</p></div><p>Better Fidelity. High-fidelity 3D scene generation demands coherence in geometry, texture, lighting, and multi-view consistency. While current methods often trade off between geometric accuracy or visual richness, future models should focus on bridging this gap that jointly reason about structure and appearance. Key goals include improved material and lighting modeling, consistent object identity across views, and capturing subtle cues like shadows and occlusions. Achieving scene-level fidelity also means aligning local details with global spatial and semantic coherence, enabling more realistic and useful 3D environments.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>更好的保真度。高保真的三维场景生成要求几何、纹理、光照和多视图一致性之间的协调。虽然当前的方法通常在几何准确性和视觉丰富性之间进行权衡，但未来的模型应专注于弥合这一差距，联合考虑结构和外观。关键目标包括改进材料和光照建模、在视图之间保持一致的对象身份，以及捕捉阴影和遮挡等细微线索。实现场景级保真度还意味着将局部细节与全局空间和语义一致性对齐，从而实现更真实和更有用的三维环境。</p></div><p>Physical-aware Generation. Despite impressive visual progress, current methods often overlook the physical plausibility of generated scenes. To ensure that object placements and articulations conform to physical laws, future work should incorporate physics priors, constraints, or simulations into the generation process. Emerging approaches that integrate physics-based feedback, such as differentiable simulators [489], offer a promising path toward jointly optimizing structure, semantics, and physical behavior. These capabilities are especially important for embodied AI and robotics, where agents depend on physically consistent environments for effective planning and control.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>物理感知生成。尽管在视觉上取得了令人印象深刻的进展，但当前的方法往往忽视生成场景的物理合理性。为了确保对象的放置和关节符合物理法则，未来的工作应将物理先验、约束或仿真纳入生成过程。新兴的方法，如可微分模拟器[489]，提供了一条有前景的路径，朝着联合优化结构、语义和物理行为的方向发展。这些能力对于具身人工智能和机器人技术尤为重要，因为代理依赖于物理一致的环境进行有效的规划和控制。</p></div><p>Interactive Scene Generation. Recent advances in \(4\mathrm{D}\) scene generation have enabled the creation of dynamic environments with movable objects. However, these scenes remain largely non-interactive, where objects do not respond to user inputs or environmental changes. As a result, current generative models produce passive rather than reactive experiences. A key future direction is interactive scene generation, where scenes contain interactive objects that can respond meaningfully to physical interactions, user commands, or contextual variations. Achieving this will require models to go beyond geometry and motion, incorporating reasoning about object affordances, causal relationships, and multi-agent dynamics.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>交互式场景生成。最近在\(4\mathrm{D}\)场景生成方面的进展使得创建具有可移动对象的动态环境成为可能。然而，这些场景仍然在很大程度上是非交互式的，物体不响应用户输入或环境变化。因此，当前的生成模型产生的是被动而非反应式的体验。一个关键的未来方向是交互式场景生成，其中场景包含能够对物理交互、用户命令或上下文变化做出有意义响应的交互对象。实现这一目标将要求模型超越几何和运动，纳入对对象可用性、因果关系和多代理动态的推理。</p></div><p>Unified Perception-Generation. A promising frontier lies in unifying perception and generation under a shared model. Tasks such as segmentation, reconstruction, and scene synthesis benefit from common spatial and semantic priors. Moreover, generation tasks inherently require an understanding of the input modalities. A unified architecture could leverage bidirectional capabilities: enhancing generative performance via perceptual grounding and improving scene understanding through generative modeling. Such models could serve as general-purpose backbones for embodied agents, supporting joint reasoning across vision, language, and 3D spatial representations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>统一感知-生成。一个有前景的前沿在于在共享模型下统一感知和生成。分割、重建和场景合成等任务受益于共同的空间和语义先验。此外，生成任务本质上需要理解输入模态。统一架构可以利用双向能力：通过感知基础增强生成性能，并通过生成建模改善场景理解。这种模型可以作为具身代理的通用骨干，支持在视觉、语言和三维空间表示之间的联合推理。</p></div><h2>REFERENCES</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>参考文献</h2></div><p>[1] B. Mendiburu, 3D movie making: stereoscopic digital cinema from script to screen. Routledge, 2012.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[1] B. Mendiburu, 3D电影制作：从剧本到银幕的立体数字电影。Routledge, 2012。</p></div><p>[2] N. Anantrasirichai and D. Bull, "Artificial intelligence in the creative industries: a review," Artificial Intelligence Review, vol. 55, no. 1, pp. 589-656, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[2] N. Anantrasirichai和D. Bull，“创意产业中的人工智能：综述，”人工智能评论，第55卷，第1期，第589-656页，2022。</p></div><p>[3] Y. I. H. Parish and P. Müller, "Procedural modeling of cities," in SIGGRAPH, 2001.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[3] Y. I. H. Parish和P. Müller，“城市的程序建模，”在SIGGRAPH，2001。</p></div><p>[4] G. N. Yannakakis and J. Togelius, Artificial intelligence and games. Springer, 2018, vol. 2.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[4] G. N. Yannakakis和J. Togelius，人工智能与游戏。Springer, 2018，第2卷。</p></div><p>[5] T. Short and T. Adams, Procedural generation in game design. CRC Press, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[5] T. Short和T. Adams，游戏设计中的程序生成。CRC出版社，2017。</p></div><p>[6] P. Müller, P. Wonka, S. Haegler, A. Ulmer, and L. V. Gool, "Procedural modeling of buildings," ACM TOG, vol. 25, no. 3, pp. 614-623, 2006.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[6] P. Müller, P. Wonka, S. Haegler, A. Ulmer和L. V. Gool，“建筑的程序建模，”ACM TOG，第25卷，第3期，第614-623页，2006。</p></div><p>[7] K. Chang, C. Cheng, J. Luo, S. Murata, M. Nourbakhsh, and Y. Tsuji, "Building-GAN: Graph-conditioned architectural volumetric design generation," in ICCV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[7] K. Chang, C. Cheng, J. Luo, S. Murata, M. Nourbakhsh, 和 Y. Tsuji, "Building-GAN: 图条件建筑体量设计生成," 在 ICCV, 2021.</p></div><p>[8] S. M. LaValle, Virtual reality. Cambridge university press, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[8] S. M. LaValle, 虚拟现实. 剑桥大学出版社, 2023.</p></div><p>[9] L. Lee, T. Braud, P. Y. Zhou, L. Wang, D. Xu, Z. Lin, A. Kumar, C. Bermejo, and P. Hui, "All one needs to know about metaverse: A complete survey on technological singularity, virtual ecosystem, and research agenda," Foundations and Trends in Human-Computer Interaction, vol. 18, no. 2-3, pp. 100-337, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[9] L. Lee, T. Braud, P. Y. Zhou, L. Wang, D. Xu, Z. Lin, A. Kumar, C. Bermejo, 和 P. Hui, "关于元宇宙的一切: 技术奇点、虚拟生态系统和研究议程的完整调查," 人机交互基础与趋势, 第 18 卷, 第 2-3 期, 第 100-337 页, 2024.</p></div><p>[10] M. M. Soliman, E. Ahmed, A. Darwish, and A. E. Hassanien, "Artificial intelligence powered metaverse: analysis, challenges and future perspectives," Artificial Intelligence Review, vol. 57, no. 2, p. 36, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[10] M. M. Soliman, E. Ahmed, A. Darwish, 和 A. E. Hassanien, "人工智能驱动的元宇宙: 分析、挑战与未来展望," 人工智能评论, 第 57 卷, 第 2 期, 第 36 页, 2024.</p></div><p>[11] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. Martín-Martín, C. Wang, G. Levine, W. Ai, B. J. Martinez, H. Yin, M. Lingelbach, M. Hwang, A. Hiranaka, S. Garlanka, A. Aydin, S. Lee, J. Sun, M. Anvari, M. Sharma, D. Bansal, S. Hunter, K. Kim, A. Lou, C. R. Matthews, I. Villa-Renteria, J. H. Tang, C. Tang, F. Xia, Y. Li, S. Savarese, H. Gweon, C. K. Liu, J. Wu, and L. Fei-Fei, "BEHAVIOR-1K: A human-centered, embodied AI benchmark with 1,000 everyday activities and realistic simulation," arXiv 2403.09227, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[11] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. Martín-Martín, C. Wang, G. Levine, W. Ai, B. J. Martinez, H. Yin, M. Lingelbach, M. Hwang, A. Hiranaka, S. Garlanka, A. Aydin, S. Lee, J. Sun, M. Anvari, M. Sharma, D. Bansal, S. Hunter, K. Kim, A. Lou, C. R. Matthews, I. Villa-Renteria, J. H. Tang, C. Tang, F. Xia, Y. Li, S. Savarese, H. Gweon, C. K. Liu, J. Wu, 和 L. Fei-Fei, "BEHAVIOR-1K: 一个以人为中心的、具身的人工智能基准，包含 1,000 种日常活动和真实模拟," arXiv 2403.09227, 2024.</p></div><p>[12] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fu-sai, L. Groom, K. Hausman, B. Ichter, S. Jakubczak, T. Jones, L. Ke, S. Levine, A. Li-Bell, M. Mothukuri, S. Nair, K. Pertsch, L. X. Shi, J. Tanner,Q. Vuong,A. Walling,H. Wang,and U. Zhilinsky," \({\pi }_{0}\) : A vision-language-action flow model for general robot control," arXiv 2410.24164, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[12] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fu-sai, L. Groom, K. Hausman, B. Ichter, S. Jakubczak, T. Jones, L. Ke, S. Levine, A. Li-Bell, M. Mothukuri, S. Nair, K. Pertsch, L. X. Shi, J. Tanner, Q. Vuong, A. Walling, H. Wang, 和 U. Zhilinsky, "\({\pi }_{0}\): 一种用于通用机器人控制的视觉-语言-动作流模型," arXiv 2410.24164, 2024.</p></div><p>[13] A. Dosovitskiy, G. Ros, F. Codevilla, A. M. López, and V. Koltun, "CARLA: an open urban driving simulator," in CoRL, vol. 78, 2017, pp. 1-16.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[13] A. Dosovitskiy, G. Ros, F. Codevilla, A. M. López, 和 V. Koltun, "CARLA: 一个开放的城市驾驶模拟器," 在 CoRL, 第 78 卷, 2017, 第 1-16 页.</p></div><p>[14] L. Chen, P. Wu, K. Chitta, B. Jaeger, A. Geiger, and H. Li, "End-to-end autonomous driving: Challenges and frontiers," IEEE TPAMI, vol. 46, no. 12, pp. 10164-10183, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[14] L. Chen, P. Wu, K. Chitta, B. Jaeger, A. Geiger, 和 H. Li, "端到端自动驾驶: 挑战与前沿," IEEE TPAMI, 第 46 卷, 第 12 期, 第 10164-10183 页, 2024.</p></div><p>[15] M. Deitke, E. VanderBilt, A. Herrasti, L. Weihs, K. Ehsani, J. Salvador, W. Han, E. Kolve, A. Kembhavi, and R. Mottaghi, "Proc-THOR: Large-scale embodied AI using procedural generation," in NeurIPS, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[15] M. Deitke, E. VanderBilt, A. Herrasti, L. Weihs, K. Ehsani, J. Salvador, W. Han, E. Kolve, A. Kembhavi, 和 R. Mottaghi, "Proc-THOR: 使用过程生成的大规模具身人工智能," 在 NeurIPS, 2022.</p></div><p>[16] H. Wang, J. Chen, W. Huang, Q. Ben, T. Wang, B. Mi, T. Huang, S. Zhao, Y. Chen, S. Yang, P. Cao, W. Yu, Z. Ye, J. Li, J. Long, Z. Wang, H. Wang, Y. Zhao, Z. Tu, Y. Qiao, D. Lin, and J. Pang, "GRUtopia: Dream general robots in a city at scale," arXiv 2407.10943, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[16] H. Wang, J. Chen, W. Huang, Q. Ben, T. Wang, B. Mi, T. Huang, S. Zhao, Y. Chen, S. Yang, P. Cao, W. Yu, Z. Ye, J. Li, J. Long, Z. Wang, H. Wang, Y. Zhao, Z. Tu, Y. Qiao, D. Lin, 和 J. Pang, "GRUtopia: 在城市规模中梦想通用机器人," arXiv 2407.10943, 2024.</p></div><p>[17] W. Wu, H. He, Y. Wang, C. Duan, J. He, Z. Liu, Q. Li, and B. Zhou, "MetaUrban: A simulation platform for embodied AI in urban spaces," in ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[17] W. Wu, H. He, Y. Wang, C. Duan, J. He, Z. Liu, Q. Li, 和 B. Zhou, "MetaUrban: 一个用于城市空间中具身人工智能的模拟平台," 在 ICLR, 2025.</p></div><p>[18] Z. Zhu, X. Wang, W. Zhao, C. Min, N. Deng, M. Dou, Y. Wang, B. Shi, K. Wang, C. Zhang, Y. You, Z. Zhang, D. Zhao, L. Xiao, J. Zhao, J. Lu, and G. Huang, "Is sora a world simulator? A comprehensive survey on general world models and beyond," arXiv 2405.03520, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[18] Z. Zhu, X. Wang, W. Zhao, C. Min, N. Deng, M. Dou, Y. Wang, B. Shi, K. Wang, C. Zhang, Y. You, Z. Zhang, D. Zhao, L. Xiao, J. Zhao, J. Lu, 和 G. Huang, "sora 是一个世界模拟器吗？关于一般世界模型及其更广泛应用的综合调查," arXiv 2405.03520, 2024.</p></div><p>[19] N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai, P. Chat-topadhyay, Y. Chen, Y. Cui, Y. Ding et al., "Cosmos world foundation model platform for physical AI," arXiv 2501.03575, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[19] N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai, P. Chat-topadhyay, Y. Chen, Y. Cui, Y. Ding 等, "宇宙世界基础模型平台用于物理人工智能," arXiv 2501.03575, 2025.</p></div><p>[20] D. Liu, J. Zhang, A.-D. Dinh, E. Park, S. Zhang, and C. Xu, "Generative physical AI in vision: A survey," arXiv 2501.10928, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[20] D. Liu, J. Zhang, A.-D. Dinh, E. Park, S. Zhang, 和 C. Xu, "视觉中的生成物理人工智能：一项调查," arXiv 2501.10928, 2025.</p></div><p>[21] H. Jiang, D. Yan, X. Zhang, and P. Wonka, "Selection expressions for procedural modeling," IEEE TVCG, vol. 26, no. 4, pp. 1775- 1788, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[21] H. Jiang, D. Yan, X. Zhang, 和 P. Wonka, "程序建模的选择表达," IEEE TVCG, vol. 26, no. 4, pp. 1775-1788, 2020.</p></div><p>[22] W. Zhao, Y. Cao, J. Xu, Y. Dong, and Y. Shan, "DI-PCG: diffusion-based efficient inverse procedural content generation for high-quality 3D asset creation," arXiv 2412.15200, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[22] W. Zhao, Y. Cao, J. Xu, Y. Dong, 和 Y. Shan, "DI-PCG：基于扩散的高效逆程序内容生成用于高质量3D资产创建," arXiv 2412.15200, 2024.</p></div><p>[23] M. Hendrikx, S. A. Meijer, J. V. D. Velden, and A. Iosup, "Procedural content generation for games: A survey," ACM Transactions on Multimedia Computing, Communications, and Applications, vol. 9, no. 1, pp. 1:1-1:22, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[23] M. Hendrikx, S. A. Meijer, J. V. D. Velden, 和 A. Iosup, "游戏的程序内容生成：一项调查," ACM Transactions on Multimedia Computing, Communications, and Applications, vol. 9, no. 1, pp. 1:1-1:22, 2013.</p></div><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[24] S. Zhang, M. Zhou, Y. Wang, C. Luo, R. Wang, Y. Li, X. Yin, Z. Zhang, 和 J. Peng, "CityX：可控的程序内容生成用于无限制的3D城市," arXiv 2407.17572, 2024.</p></div><p>[25] Y. Yang, J. Wang, E. Vouga, and P. Wonka, "Urban pattern: layout design by hierarchical domain splitting," ACM TOG, vol. 32, no. 6, pp. 181:1-181:12, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[25] Y. Yang, J. Wang, E. Vouga, 和 P. Wonka, "城市模式：通过分层领域划分进行布局设计," ACM TOG, vol. 32, no. 6, pp. 181:1-181:12, 2013.</p></div><p>[26] J. O. Talton, Y. Lou, S. Lesser, J. Duke, R. Mech, and V. Koltun, "Metropolis procedural modeling," ACM TOG, vol. 30, no. 2, pp. 11:1-11:14, 2011.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[26] J. O. Talton, Y. Lou, S. Lesser, J. Duke, R. Mech, 和 V. Koltun, "大都市程序建模," ACM TOG, vol. 30, no. 2, pp. 11:1-11:14, 2011.</p></div><p>[27] W. Wu, L. Fan, L. Liu, and P. Wonka, "Miqp-based layout design for building interiors," Computer Graphics Forum, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[27] W. Wu, L. Fan, L. Liu, 和 P. Wonka, "基于Miqp的建筑室内布局设计," 计算机图形学论坛, 2018.</p></div><p>[28] L. Yu, S. K. Yeung, C. Tang, D. Terzopoulos, T. F. Chan, and S. J. Osher, "Make it home: automatic optimization of furniture arrangement," ACM TOG, vol. 30, no. 4, p. 86, 2011.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[28] L. Yu, S. K. Yeung, C. Tang, D. Terzopoulos, T. F. Chan, 和 S. J. Osher, "让它成为家：家具布置的自动优化," ACM TOG, vol. 30, no. 4, p. 86, 2011.</p></div><p>[29] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, and Y. Bengio, "Generative adversarial networks," in NIPS, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[29] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, 和 Y. Bengio, "生成对抗网络," 在NIPS, 2014.</p></div><p>[30] J. Ho, A. Jain, and P. Abbeel, "Denoising diffusion probabilistic models," in NeurIPS, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[30] J. Ho, A. Jain, 和 P. Abbeel, "去噪扩散概率模型," 在NeurIPS, 2020.</p></div><p>[31] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ra-mamoorthi, and R. Ng, "NeRF: Representing scenes as neural radiance fields for view synthesis," in ECCV, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[31] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ra-mamoorthi, 和 R. Ng, "NeRF：将场景表示为神经辐射场以进行视图合成," 在ECCV, 2020.</p></div><p>[32] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis, "3D Gaussian splatting for real-time radiance field rendering," ACM TOG, vol. 42, no. 4, pp. 139:1-139:14, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[32] B. Kerbl, G. Kopanas, T. Leimkühler, 和 G. Drettakis, "用于实时辐射场渲染的3D高斯溅射," ACM TOG, vol. 42, no. 4, pp. 139:1-139:14, 2023.</p></div><p>[33] A. Liu, A. Makadia, R. Tucker, N. Snavely, V. Jampani, and A. Kanazawa, "Infinite Nature: Perpetual view generation of natural scenes from a single image," in ICCV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[33] A. Liu, A. Makadia, R. Tucker, N. Snavely, V. Jampani, 和 A. Kanazawa, "无限自然：从单张图像生成自然场景的永久视图," 在ICCV, 2021.</p></div><p>[34] Z. Li, Q. Wang, N. Snavely, and A. Kanazawa, "InfiniteNature-Zero: Learning perpetual view generation of natural scenes from single images," in ECCV, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[34] Z. Li, Q. Wang, N. Snavely, 和 A. Kanazawa, "InfiniteNature-Zero：从单张图像学习自然场景的永久视图生成," 在ECCV, 2022.</p></div><p>[35] Z. Chen, G. Wang, and Z. Liu, "Text2Light: Zero-shot text-driven HDR panorama generation," ACM TOG, vol. 41, no. 6, pp. 195:1- 195:16, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[35] Z. Chen, G. Wang, 和 Z. Liu, "Text2Light：零样本文本驱动的HDR全景生成," ACM TOG, vol. 41, no. 6, pp. 195:1-195:16, 2022.</p></div><p>[36] S. Tang, F. Zhang, J. Chen, P. Wang, and Y. Furukawa, "MVD-iffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion," in NeurIPS, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[36] S. Tang, F. Zhang, J. Chen, P. Wang, 和 Y. Furukawa, "MVD-扩散：实现具有对应感知的整体多视图图像生成," 在NeurIPS, 2023.</p></div><p>[37] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, V. Jampani, and R. Rombach, "Stable video diffusion: Scaling latent video diffusion models to large datasets," arXiv 2311.15127, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[37] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, V. Jampani, 和 R. Rombach, "稳定视频扩散：将潜在视频扩散模型扩展到大数据集," arXiv 2311.15127, 2023.</p></div><p>[38] Y. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun, J. Gao, L. He, and L. Sun, "Sora: A review on background, technology, limitations, and opportunities of large vision models," arXiv 2402.17177, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[38] Y. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun, J. Gao, L. He, 和 L. Sun, "Sora：大型视觉模型的背景、技术、局限性和机会的综述," arXiv 2402.17177, 2024.</p></div><p>[39] R. Gao, K. Chen, E. Xie, L. Hong, Z. Li, D. Yeung, and Q. Xu, "MagicDrive: Street view generation with diverse 3D geometry control," in ICLR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[39] R. Gao, K. Chen, E. Xie, L. Hong, Z. Li, D. Yeung, 和 Q. Xu, "MagicDrive：具有多样化3D几何控制的街景生成," 在ICLR, 2024.</p></div><p>[40] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang, "4D Gaussian splatting for real-time dynamic scene rendering," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[40] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, 和 X. Wang, "用于实时动态场景渲染的4D高斯溅射," 在CVPR, 2024.</p></div><p>[41] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, "Deformable 3D Gaussians for high-fidelity monocular dynamic scene reconstruction," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[41] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, 和 X. Jin, "可变形3D高斯用于高保真单目动态场景重建," 在CVPR, 2024.</p></div><p>[42] W. Sun, S. Chen, F. Liu, Z. Chen, Y. Duan, J. Zhang, and Y. Wang, "DimensionX: Create any 3D and 4D scenes from a single image with controllable video diffusion," arXiv 2411.04928, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[42] W. Sun, S. Chen, F. Liu, Z. Chen, Y. Duan, J. Zhang, 和 Y. Wang, "DimensionX: 从单张图像创建任何3D和4D场景，具有可控的视频扩散," arXiv 2411.04928, 2024.</p></div><p>[43] R. Li, P. Pan, B. Yang, D. Xu, S. Zhou, X. Zhang, Z. Li, A. Kadambi, Z. Wang, and Z. Fan, "4K4DGen: Panoramic 4D generation at 4K resolution," in ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[43] R. Li, P. Pan, B. Yang, D. Xu, S. Zhou, X. Zhang, Z. Li, A. Kadambi, Z. Wang, 和 Z. Fan, "4K4DGen: 以4K分辨率进行全景4D生成," 在ICLR, 2025.</p></div><p>[44] R. M. Smelik, T. Tutenel, R. Bidarra, and B. Benes, "A survey on procedural modelling for virtual worlds," Computer Graphics Forum, vol. 33, no. 6, pp. 31-50, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[44] R. M. Smelik, T. Tutenel, R. Bidarra, 和 B. Benes, "虚拟世界程序建模的调查," 计算机图形学论坛, 第33卷，第6期，页31-50, 2014.</p></div><p>[45] E. Cogo, E. Krupalija, I. Prazina, S. Becirovic, V. Okanovic, S. Rizvic, and R. T. Mulahasanovic, "A survey of procedural modelling methods for layout generation of virtual scenes," Computer Graphics Forum, vol. 43, no. 1, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[45] E. Cogo, E. Krupalija, I. Prazina, S. Becirovic, V. Okanovic, S. Rizvic, 和 R. T. Mulahasanovic, "虚拟场景布局生成的程序建模方法调查," 计算机图形学论坛, 第43卷，第1期, 2024.</p></div><p>[46] S. Zhang, S. Zhang, Y. Liang, and P. Hall, "A survey of 3D indoor scene synthesis," Journal of Computer Science and Technology, vol. 34, no. 3, pp. 594-608, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[46] S. Zhang, S. Zhang, Y. Liang, 和 P. Hall, "3D室内场景合成的调查," 计算机科学与技术杂志, 第34卷，第3期，页594-608, 2019.</p></div><p>[47] A. G. Patil, S. G. Patil, M. Li, M. Fisher, M. Savva, and H. Zhang, "Advances in data-driven analysis and synthesis of 3D indoor scenes," Computer Graphics Forum, vol. 43, no. 1, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[47] A. G. Patil, S. G. Patil, M. Li, M. Fisher, M. Savva, 和 H. Zhang, "基于数据的3D室内场景分析与合成的进展," 计算机图形学论坛, 第43卷，第1期, 2024.</p></div><p>[48] D. V. Ayyildiz, A. J. Alnaser, S. Taj, M. Zakaria, and L. G. Jaimes, "A survey of learning techniques for virtual scene generation," SAE International Journal of Connected and Automated Vehicles, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[48] D. V. Ayyildiz, A. J. Alnaser, S. Taj, M. Zakaria, 和 L. G. Jaimes, "虚拟场景生成学习技术的调查," SAE国际连接与自动化车辆杂志, 2024.</p></div><p>[49] H. Wang, X. Xiang, W. Xia, and J. Xue, "A survey on text-driven 360-degree panorama generation," arXiv 2502.14799, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[49] H. Wang, X. Xiang, W. Xia, 和 J. Xue, "基于文本的360度全景生成的调查," arXiv 2502.14799, 2025.</p></div><p>[50] M. A. Ghorab and A. Lakhfif, "Text to 3D, 2D scene generation systems, frameworks and approaches: a survey," in Pattern Analysis and Intelligent Systems, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[50] M. A. Ghorab 和 A. Lakhfif, "文本到3D、2D场景生成系统、框架和方法：一项调查," 在模式分析与智能系统, 2022.</p></div><p>[51] Z. Shi, S. Peng, Y. Xu, Y. Liao, and Y. Shen, "Deep generative models on 3D representations: A survey," arXiv 2210.15663, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[51] Z. Shi, S. Peng, Y. Xu, Y. Liao, 和 Y. Shen, "3D表示的深度生成模型：一项调查," arXiv 2210.15663, 2022.</p></div><p>[52] C. Li, C. Zhang, A. Waghwase, L. Lee, F. Rameau, Y. Yang, S. Bae, and C. S. Hong, "Generative AI meets 3D: A survey on text-to-3D in AIGC era," arXiv 2305.06131, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[52] C. Li, C. Zhang, A. Waghwase, L. Lee, F. Rameau, Y. Yang, S. Bae, 和 C. S. Hong, "生成性人工智能与3D的结合：AIGC时代文本到3D的调查," arXiv 2305.06131, 2023.</p></div><p>[53] X. Li, Q. Zhang, D. Kang, W. Cheng, Y. Gao, J. Zhang, Z. Liang, J. Liao, Y. Cao, and Y. Shan, "Advances in 3D generation: A survey," arXiv 2401.17807, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[53] X. Li, Q. Zhang, D. Kang, W. Cheng, Y. Gao, J. Zhang, Z. Liang, J. Liao, Y. Cao, 和 Y. Shan, "3D生成的进展：一项调查," arXiv 2401.17807, 2024.</p></div><p>[54] J. Liu, X. Huang, T. Huang, L. Chen, Y. Hou, S. Tang, Z. Liu, W. Ouyang, W. Zuo, J. Jiang, and X. Liu, "A comprehensive survey on 3D content generation," arXiv 2402.01166, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[54] J. Liu, X. Huang, T. Huang, L. Chen, Y. Hou, S. Tang, Z. Liu, W. Ouyang, W. Zuo, J. Jiang, 和 X. Liu, "3D内容生成的综合调查," arXiv 2402.01166, 2024.</p></div><p>[55] Z. Wang, D. Li, and R. Jiang, "Diffusion models in 3D vision: A survey," arXiv 2410.04738, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[55] Z. Wang, D. Li, 和 R. Jiang, "3D视觉中的扩散模型：一项调查," arXiv 2410.04738, 2024.</p></div><p>[56] Q. Miao, K. Li, J. Quan, Z. Min, S. Ma, Y. Xu, Y. Yang, and Y. Luo, "Advances in 4D generation: A survey," arXiv 2503.14501, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[56] Q. Miao, K. Li, J. Quan, Z. Min, S. Ma, Y. Xu, Y. Yang, 和 Y. Luo, "4D生成的进展：一项调查," arXiv 2503.14501, 2025.</p></div><p>[57] J. Ding, Y. Zhang, Y. Shang, Y. Zhang, Z. Zong, J. Feng, Y. Yuan, H. Su, N. Li, N. Sukiennik, F. Xu, and Y. Li, "Understanding world or predicting future? A comprehensive survey of world models," arXiv 2411.14499, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[57] J. Ding, Y. Zhang, Y. Shang, Y. Zhang, Z. Zong, J. Feng, Y. Yuan, H. Su, N. Li, N. Sukiennik, F. Xu, 和 Y. Li, "理解世界还是预测未来？世界模型的综合调查," arXiv 2411.14499, 2024.</p></div><p>[58] T. Feng, W. Wang, and Y. Yang, "A survey of world models for autonomous driving," arXiv 2501.11260, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[58] T. Feng, W. Wang, 和 Y. Yang, "自动驾驶的世界模型调查," arXiv 2501.11260, 2025.</p></div><p>[59] X. Han, H. Laga, and M. Bennamoun, "Image-based 3D object reconstruction: State-of-the-art and trends in the deep learning era," IEEE TPAMI, vol. 43, no. 5, pp. 1578-1604, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[59] X. Han, H. Laga, 和 M. Bennamoun, "基于图像的3D物体重建：深度学习时代的最新进展和趋势," IEEE TPAMI, vol. 43, no. 5, pp. 1578-1604, 2021.</p></div><p>[60] Z. Huang, Y. Wen, Z. Wang, J. Ren, and K. Jia, "Surface reconstruction from point clouds: A survey and a benchmark," IEEE TPAMI, vol. 46, no. 12, pp. 9727-9748, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[60] Z. Huang, Y. Wen, Z. Wang, J. Ren, 和 K. Jia, "从点云进行表面重建：调查与基准," IEEE TPAMI, vol. 46, no. 12, pp. 9727-9748, 2024.</p></div><p>[61] Z. Xing, Q. Feng, H. Chen, Q. Dai, H. Hu, H. Xu, Z. Wu, and Y. Jiang, "A survey on video diffusion models," ACM Computing Surveys, vol. 57, no. 2, pp. 41:1-41:42, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[61] Z. Xing, Q. Feng, H. Chen, Q. Dai, H. Hu, H. Xu, Z. Wu, 和 Y. Jiang, "视频扩散模型的调查," ACM Computing Surveys, vol. 57, no. 2, pp. 41:1-41:42, 2025.</p></div><p>[62] U. Singer, S. Sheynin, A. Polyak, O. Ashual, I. Makarov, F. Kokki-nos, N. Goyal, A. Vedaldi, D. Parikh, J. Johnson, and Y. Taigman, "Text-to-4D dynamic scene generation," in ICML, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[62] U. Singer, S. Sheynin, A. Polyak, O. Ashual, I. Makarov, F. Kokki-nos, N. Goyal, A. Vedaldi, D. Parikh, J. Johnson, 和 Y. Taigman, "文本到4D动态场景生成," 在ICML, 2023.</p></div><p>[63] D. Xu, H. Liang, N. P. Bhatt, H. Hu, H. Liang, K. N. Plataniotis, and Z. Wang, "Comp4D: Llm-guided compositional 4D scene generation," arXiv 2403.16993, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[63] D. Xu, H. Liang, N. P. Bhatt, H. Hu, H. Liang, K. N. Plataniotis, 和 Z. Wang, "Comp4D：基于大语言模型的组合4D场景生成," arXiv 2403.16993, 2024.</p></div><p>[64] Y. Zheng, X. Li, K. Nagano, S. Liu, O. Hilliges, and S. D. Mello, "A unified approach for text-and image-guided 4D scene generation," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[64] Y. Zheng, X. Li, K. Nagano, S. Liu, O. Hilliges, 和 S. D. Mello, "一种统一的文本和图像引导的4D场景生成方法," 在CVPR, 2024.</p></div><p>[65] B. Curless and M. Levoy, "A volumetric method for building complex models from range images," in SIGGRAPH, 1996.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[65] B. Curless 和 M. Levoy, "一种基于体积的方法用于从范围图像构建复杂模型," 在SIGGRAPH, 1996.</p></div><p>[66] J. L. Schönberger and J. Frahm, "Structure-from-motion revisited," in CVPR, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[66] J. L. Schönberger 和 J. Frahm, "运动重建结构的再探讨," 在CVPR, 2016.</p></div><p>[67] J. J. Park, P. R. Florence, J. Straub, R. A. Newcombe, and S. Love-grove, "DeepSDF: Learning continuous signed distance functions for shape representation," in CVPR, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[67] J. J. Park, P. R. Florence, J. Straub, R. A. Newcombe, 和 S. Love-grove, "DeepSDF：学习用于形状表示的连续符号距离函数," 在CVPR, 2019.</p></div><p>[68] J. C. Hart, "Sphere tracing: a geometric method for the antialiased ray tracing of implicit surfaces," The Visual Computer, vol. 12, no. 10, pp. 527-545, 1996.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[68] J. C. Hart, "球体追踪：一种用于隐式表面抗锯齿光线追踪的几何方法," The Visual Computer, vol. 12, no. 10, pp. 527-545, 1996.</p></div><p>[69] J. T. Kajiya and B. V. Herzen, "Ray tracing volume densities," in SIGGRAPH, 1984.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[69] J. T. Kajiya 和 B. V. Herzen, "光线追踪体积密度," 在SIGGRAPH, 1984.</p></div><p>[70] N. L. Max, "Optical models for direct volume rendering," IEEE TVCG, vol. 1, no. 2, pp. 99-108, 1995.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[70] N. L. Max, "直接体积渲染的光学模型," IEEE TVCG, vol. 1, no. 2, pp. 99-108, 1995.</p></div><p>[71] D. P. Kingma and M. Welling, "Auto-encoding variational bayes," in ICLR, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[71] D. P. Kingma 和 M. Welling, "自编码变分贝叶斯," 在ICLR, 2014.</p></div><p>[72] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," in NeurIPS, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[72] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, 和 I. Polosukhin, "注意力机制是你所需要的一切," 在NeurIPS, 2017.</p></div><p>[73] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. W. Battaglia, "Learning deep generative models of graphs," arXiv 1803.03324, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[73] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, 和 P. W. Battaglia, "学习图的深度生成模型," arXiv 1803.03324, 2018.</p></div><p>[74] D. J. Rezende and F. Viola, "Taming VAEs," arXiv 1810.00597, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[74] D. J. Rezende 和 F. Viola, "驯服变分自编码器," arXiv 1810.00597, 2018.</p></div><p>[75] J. Lucas, G. Tucker, R. B. Grosse, and M. Norouzi, "Don't Blame the ELBO! A linear VAE perspective on posterior collapse," in NeurIPS, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[75] J. Lucas, G. Tucker, R. B. Grosse, 和 M. Norouzi, "不要责怪ELBO！关于后验崩溃的线性变分自编码器视角," 在NeurIPS, 2019.</p></div><p>[76] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, "Improved training of wasserstein gans," in NIPS, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[76] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, 和 A. C. Courville, "改进的Wasserstein生成对抗网络训练," 在NIPS, 2017.</p></div><p>[77] P. Dhariwal and A. Q. Nichol, "Diffusion models beat gans on image synthesis," in NeurIPS, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[77] P. Dhariwal 和 A. Q. Nichol, "扩散模型在图像合成中超越生成对抗网络," 在NeurIPS, 2021.</p></div><p>[78] F. K. Musgrave, C. E. Kolb, and R. S. Mace, "The synthesis and rendering of eroded fractal terrains," in SIGGRAPH, 1989.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[78] F. K. Musgrave, C. E. Kolb, 和 R. S. Mace, "侵蚀分形地形的合成与渲染," 在SIGGRAPH, 1989.</p></div><p>[79] G. Cordonnier, E. Galin, J. Gain, B. Benes, E. Guérin, A. Peytavie, and M. Cani, "Authoring landscapes by combining ecosystem and terrain erosion simulation," ACM TOG, vol. 36, no. 4, pp. 134:1-134:12, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[79] G. Cordonnier, E. Galin, J. Gain, B. Benes, E. Guérin, A. Peytavie, 和 M. Cani, "通过结合生态系统和地形侵蚀模拟创作景观," ACM TOG, vol. 36, no. 4, pp. 134:1-134:12, 2017.</p></div><p>[80] A. Raistrick, L. Lipson, Z. Ma, L. Mei, M. Wang, Y. Zuo, K. Kayan, H. Wen, B. Han, Y. Wang, A. Newell, H. Law, A. Goyal, K. Yang, and J. Deng, "Infinite photorealistic worlds using procedural generation," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[80] A. Raistrick, L. Lipson, Z. Ma, L. Mei, M. Wang, Y. Zuo, K. Kayan, H. Wen, B. Han, Y. Wang, A. Newell, H. Law, A. Goyal, K. Yang, 和 J. Deng, "使用程序生成的无限真实感世界," 在CVPR, 2023.</p></div><p>[81] A. Raistrick, L. Mei, K. Kayan, D. Yan, Y. Zuo, B. Han, H. Wen, M. Parakh, S. Alexandropoulos, L. Lipson, Z. Ma, and J. Deng, "Infinigen Indoors: Photorealistic indoor scenes using procedural generation," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[81] A. Raistrick, L. Mei, K. Kayan, D. Yan, Y. Zuo, B. Han, H. Wen, M. Parakh, S. Alexandropoulos, L. Lipson, Z. Ma, 和 J. Deng, "Infinigen室内：使用程序生成的真实感室内场景," 在CVPR, 2024.</p></div><p>[82] W. Feng, W. Zhu, T. Fu, V. Jampani, A. R. Akula, X. He, S. Basu, X. E. Wang, and W. Y. Wang, "LayoutGPT: Compositional visual planning and generation with large language models," in NeurIPS, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[82] W. Feng, W. Zhu, T. Fu, V. Jampani, A. R. Akula, X. He, S. Basu, X. E. Wang, 和 W. Y. Wang, "LayoutGPT：使用大型语言模型的组合视觉规划与生成," 在NeurIPS, 2023.</p></div><p>[83] C. Sun, J. Han, W. Deng, X. Wang, Z. Qin, and S. Gould, "3D-GPT: Procedural 3D modeling with large language models," in 3DV, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[83] C. Sun, J. Han, W. Deng, X. Wang, Z. Qin, 和 S. Gould, "3D-GPT：使用大型语言模型的程序化3D建模," 在3DV, 2025.</p></div><p>[84] M. Zhou, J. Hou, C. Luo, Y. Wang, Z. Zhang, and J. Peng, "SceneX: procedural controllable large-scale scene generation via large-language models," in AAAI, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[84] M. Zhou, J. Hou, C. Luo, Y. Wang, Z. Zhang, 和 J. Peng, "SceneX：通过大型语言模型进行程序化可控的大规模场景生成," 在AAAI, 2025.</p></div><p>[85] K. Wang, M. Savva, A. X. Chang, and D. Ritchie, "Deep convolutional priors for indoor scene synthesis," ACM TOG, vol. 37, no. 4, p. 70, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[85] K. Wang, M. Savva, A. X. Chang, 和 D. Ritchie, "用于室内场景合成的深度卷积先验," ACM TOG, vol. 37, no. 4, p. 70, 2018.</p></div><p>[86] D. Paschalidou, A. Kar, M. Shugrina, K. Kreis, A. Geiger, and S. Fidler, "ATISS: autoregressive transformers for indoor scene synthesis," in NeurIPS, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[86] D. Paschalidou, A. Kar, M. Shugrina, K. Kreis, A. Geiger, 和 S. Fidler, "ATISS：用于室内场景合成的自回归变换器," 在NeurIPS, 2021.</p></div><p>[87] H. Yi, C. P. Huang, S. Tripathi, L. Hering, J. Thies, and M. J. Black, "MIME: human-aware 3D scene generation," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[87] H. Yi, C. P. Huang, S. Tripathi, L. Hering, J. Thies, 和 M. J. Black, "MIME：人类感知的3D场景生成," 在CVPR, 2023.</p></div><p>[88] J. Tang, Y. Nie, L. Markhasin, A. Dai, J. Thies, and M. Nießner, "DiffuScene: Denoising diffusion models for generative indoor scene synthesis," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[88] J. Tang, Y. Nie, L. Markhasin, A. Dai, J. Thies, 和 M. Nießner, "DiffuScene：用于生成室内场景合成的去噪扩散模型," 在CVPR, 2024.</p></div><p>[89] K. Wang, Y. Lin, B. Weissmann, M. Savva, A. X. Chang, and D. Ritchie, "PlanIT: planning and instantiating indoor scenes with relation graph and spatial prior networks," ACM TOG, vol. 38, no. 4, pp. 132:1-132:15, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[89] K. Wang, Y. Lin, B. Weissmann, M. Savva, A. X. Chang, 和 D. Ritchie, "PlanIT: 使用关系图和空间先验网络规划和实例化室内场景," ACM TOG, vol. 38, no. 4, pp. 132:1-132:15, 2019.</p></div><p>[90] M. Li, A. G. Patil, K. Xu, S. Chaudhuri, O. Khan, A. Shamir, C. Tu, B. Chen, D. Cohen-Or, and H. R. Zhang, "GRAINS: generative recursive autoencoders for indoor scenes," ACM TOG, vol. 38, no. 2, pp. 12:1-12:16, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[90] M. Li, A. G. Patil, K. Xu, S. Chaudhuri, O. Khan, A. Shamir, C. Tu, B. Chen, D. Cohen-Or, 和 H. R. Zhang, "GRAINS: 用于室内场景的生成递归自编码器," ACM TOG, vol. 38, no. 2, pp. 12:1-12:16, 2019.</p></div><p>[91] H. Dhamo, F. Manhardt, N. Navab, and F. Tombari, "Graph-to- 3D: End-to-end generation and manipulation of 3D scenes using scene graphs," in ICCV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[91] H. Dhamo, F. Manhardt, N. Navab, 和 F. Tombari, "Graph-to-3D: 使用场景图的端到端3D场景生成和操作," 在 ICCV, 2021.</p></div><p>[92] G. Zhai, E. P. Örnek, S. Wu, Y. Di, F. Tombari, N. Navab, and B. Busam, "CommonScenes: Generating commonsense 3D indoor scenes with scene graph diffusion," in NeurIPS, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[92] G. Zhai, E. P. Örnek, S. Wu, Y. Di, F. Tombari, N. Navab, 和 B. Busam, "CommonScenes: 通过场景图扩散生成常识3D室内场景," 在 NeurIPS, 2023.</p></div><p>[93] C. Lin and Y. Mu, "InstructScene: Instruction-driven 3D indoor scene synthesis with semantic graph prior," in ICLR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[93] C. Lin 和 Y. Mu, "InstructScene: 基于指令的3D室内场景合成与语义图先验," 在 ICLR, 2024.</p></div><p>[94] Z. Hao, A. Mallya, S. J. Belongie, and M. Liu, "GANcraft: Unsupervised 3D neural rendering of minecraft worlds," in ICCV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[94] Z. Hao, A. Mallya, S. J. Belongie, 和 M. Liu, "GANcraft: 无监督的Minecraft世界3D神经渲染," 在 ICCV, 2021.</p></div><p>[95] S. Bahmani, J. J. Park, D. Paschalidou, X. Yan, G. Wetzstein, L. J. Guibas, and A. Tagliasacchi, "CC3D: layout-conditioned generation of compositional 3D scenes," in ICCV, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[95] S. Bahmani, J. J. Park, D. Paschalidou, X. Yan, G. Wetzstein, L. J. Guibas, 和 A. Tagliasacchi, "CC3D: 布局条件下的组合3D场景生成," 在 ICCV, 2023.</p></div><p>[96] C. H. Lin, H. Lee, W. Menapace, M. Chai, A. Siarohin, M. Yang, and S. Tulyakov, "InfiniCity: Infinite-scale city synthesis," in ICCV, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[96] C. H. Lin, H. Lee, W. Menapace, M. Chai, A. Siarohin, M. Yang, 和 S. Tulyakov, "InfiniCity: 无限规模城市合成," 在 ICCV, 2023.</p></div><p>[97] Z. Chen, G. Wang, and Z. Liu, "SceneDreamer: Unbounded 3D scene generation from 2D image collections," IEEE TPAMI, vol. 45, no. 12, pp. 15562-15576, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[97] Z. Chen, G. Wang, 和 Z. Liu, "SceneDreamer: 从2D图像集合生成无限3D场景," IEEE TPAMI, vol. 45, no. 12, pp. 15562-15576, 2023.</p></div><p>[98] H. Xie, Z. Chen, F. Hong, and Z. Liu, "CityDreamer: Compositional generative model of unbounded 3D cities," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[98] H. Xie, Z. Chen, F. Hong, 和 Z. Liu, "CityDreamer: 组合生成模型的无限3D城市," 在 CVPR, 2024.</p></div><p>[99] R. Po and G. Wetzstein, "Compositional 3D scene generation using locally conditioned diffusion," in 3DV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[99] R. Po 和 G. Wetzstein, "使用局部条件扩散的组合3D场景生成," 在 3DV, 2024.</p></div><p>[100] Z. Wu, Y. Li, H. Yan, T. Shang, W. Sun, S. Wang, R. Cui, W. Liu, H. Sato, H. Li, and P. Ji, "BlockFusion: Expandable 3D scene generation using latent tri-plane extrapolation," ACM TOG, vol. 43, no. 4, pp. 43:1-43:17, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[100] Z. Wu, Y. Li, H. Yan, T. Shang, W. Sun, S. Wang, R. Cui, W. Liu, H. Sato, H. Li, 和 P. Ji, "BlockFusion: 使用潜在三平面外推的可扩展3D场景生成," ACM TOG, vol. 43, no. 4, pp. 43:1-43:17, 2024.</p></div><p>[101] T. DeVries, M. Á. Bautista, N. Srivastava, G. W. Taylor, and J. M. Susskind, "Unconstrained scene generation with locally conditioned radiance fields," in ICCV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[101] T. DeVries, M. Á. Bautista, N. Srivastava, G. W. Taylor, 和 J. M. Susskind, "使用局部条件辐射场的无约束场景生成," 在 ICCV, 2021.</p></div><p>[102] M. Á. Bautista, P. Guo, S. Abnar, W. Talbott, A. Toshev, Z. Chen, L. Dinh, S. Zhai, H. Goh, D. Ulbricht, A. Dehghan, and J. M. Susskind, "GAUDI: A neural architect for immersive 3D scene generation," in NeurIPS, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[102] M. Á. Bautista, P. Guo, S. Abnar, W. Talbott, A. Toshev, Z. Chen, L. Dinh, S. Zhai, H. Goh, D. Ulbricht, A. Dehghan, 和 J. M. Susskind, "GAUDI: 一种用于沉浸式3D场景生成的神经架构," 在 NeurIPS, 2022.</p></div><p>[103] S. W. Kim, B. Brown, K. Yin, K. Kreis, K. Schwarz, D. Li, R. Rombach, A. Torralba, and S. Fidler, "NeuralField-LDM: Scene generation with hierarchical latent diffusion models," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[103] S. W. Kim, B. Brown, K. Yin, K. Kreis, K. Schwarz, D. Li, R. Rombach, A. Torralba, 和 S. Fidler, "NeuralField-LDM: 使用分层潜在扩散模型的场景生成," 在 CVPR, 2023.</p></div><p>[104] X. Ren, J. Huang, X. Zeng, K. Museth, S. Fidler, and F. Williams, "XCube: Large-scale 3D generative modeling using sparse voxel hierarchies," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[104] X. Ren, J. Huang, X. Zeng, K. Museth, S. Fidler, 和 F. Williams, "XCube: 大规模 3D 生成建模使用稀疏体素层次," 在 CVPR, 2024.</p></div><p>[105] X. Li, Z. Lai, L. Xu, Y. Qu, L. Cao, S. Zhang, B. Dai, and R. Ji, "Director3D: Real-world camera trajectory and 3D scene generation from text," in NeurIPS, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[105] X. Li, Z. Lai, L. Xu, Y. Qu, L. Cao, S. Zhang, B. Dai, 和 R. Ji, "Director3D: 从文本生成真实世界相机轨迹和 3D 场景," 在 NeurIPS, 2024.</p></div><p>[106] M. R. K. Dastjerdi, Y. Hold-Geoffroy, J. Eisenmann, S. Kho-dadadeh,and J. Lalonde,"Guided co-modulated GAN for \({360}^{ \circ  }\) field of view extrapolation," in 3DV, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[106] M. R. K. Dastjerdi, Y. Hold-Geoffroy, J. Eisenmann, S. Kho-dadadeh, 和 J. Lalonde, "引导共同调制 GAN 用于 \({360}^{ \circ  }\) 视场外推," 在 3DV, 2022.</p></div><p>[107] C. Zhang, Q. Wu, C. C. Gambardella, X. Huang, D. Phung, W. Ouyang,and J. Cai,"Taming stable diffusion for text to \({360}^{ \circ  }\) panorama image generation," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[107] C. Zhang, Q. Wu, C. C. Gambardella, X. Huang, D. Phung, W. Ouyang, 和 J. Cai, "驯化稳定扩散用于文本到 \({360}^{ \circ  }\) 全景图像生成," 在 CVPR, 2024.</p></div><p>[108] G. Wang, P. Wang, Z. Chen, W. Wang, C. C. Loy, and Z. Liu, "PERF: panoramic neural radiance field from a single panorama," IEEE TPAMI, vol. 46, no. 10, pp. 6905-6918, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[108] G. Wang, P. Wang, Z. Chen, W. Wang, C. C. Loy, 和 Z. Liu, "PERF: 从单个全景图生成全景神经辐射场," IEEE TPAMI, 第 46 卷, 第 10 期, 第 6905-6918 页, 2024.</p></div><p>[109] S. Yang, J. Tan, M. Zhang, T. Wu, Y. Li, G. Wetzstein, Z. Liu, and D. Lin, "LayerPano3D: Layered 3D panorama for hyper-immersive scene generation," in SIGGRAPH, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[109] S. Yang, J. Tan, M. Zhang, T. Wu, Y. Li, G. Wetzstein, Z. Liu, 和 D. Lin, "LayerPano3D: 分层 3D 全景用于超沉浸式场景生成," 在 SIGGRAPH, 2025.</p></div><p>[110] C. Rockwell, D. F. Fouhey, and J. Johnson, "PixelSynth: Generating a 3D-consistent experience from a single image," in ICCV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[110] C. Rockwell, D. F. Fouhey, 和 J. Johnson, "PixelSynth: 从单个图像生成 3D 一致体验," 在 ICCV, 2021.</p></div><p>[111] R. Rombach, P. Esser, and B. Ommer, "Geometry-free view synthesis: Transformers and no 3D priors," in ICCV,2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[111] R. Rombach, P. Esser, 和 B. Ommer, "无几何视图合成: 变换器和无 3D 先验," 在 ICCV, 2021.</p></div><p>[112] X. Li, Z. Cao, H. Sun, J. Zhang, K. Xian, and G. Lin, "3D cinemagraphy from a single image," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[112] X. Li, Z. Cao, H. Sun, J. Zhang, K. Xian, 和 G. Lin, "从单个图像生成 3D 动态影像," 在 CVPR, 2023.</p></div><p>[113] L. Höllein, A. Cao, A. Owens, J. Johnson, and M. Nießner, "Text2Room: Extracting textured 3D meshes from 2D text-to-image models," in ICCV, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[113] L. Höllein, A. Cao, A. Owens, J. Johnson, 和 M. Nießner, "Text2Room: 从 2D 文本到图像模型提取纹理 3D 网格," 在 ICCV, 2023.</p></div><p>[114] J. Zhang, X. Li, Z. Wan, C. Y. Wang, and J. Liao, "Text2NeRF: Text-driven 3D scene generation with neural radiance fields," IEEE TVCG, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[114] J. Zhang, X. Li, Z. Wan, C. Y. Wang, 和 J. Liao, "Text2NeRF: 基于文本的神经辐射场 3D 场景生成," IEEE TVCG, 2024.</p></div><p>[115] H. Yu, H. Duan, J. Hur, K. Sargent, M. Rubinstein, W. T. Freeman, F. Cole, D. Sun, N. Snavely, J. Wu, and C. Herrmann, "Wonder-Journey: Going from anywhere to everywhere," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[115] H. Yu, H. Duan, J. Hur, K. Sargent, M. Rubinstein, W. T. Freeman, F. Cole, D. Sun, N. Snavely, J. Wu, 和 C. Herrmann, "Wonder-Journey: 从任何地方到任何地方," 在 CVPR, 2024.</p></div><p>[116] J. Chung, S. Lee, H. Nam, J. Lee, and K. M. Lee, "LucidDreamer: Domain-free generation of 3D Gaussian splatting scenes," arXiv 2311.13384, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[116] J. Chung, S. Lee, H. Nam, J. Lee, 和 K. M. Lee, "LucidDreamer: 无域生成 3D 高斯喷溅场景," arXiv 2311.13384, 2023.</p></div><p>[117] H. Yu, C. Wang, P. Zhuang, W. Menapace, A. Siarohin, J. Cao, L. A. Jeni, S. Tulyakov, and H. Lee, "4Real: Towards photorealistic 4D scene generation via video diffusion models," in NeurIPS, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[117] H. Yu, C. Wang, P. Zhuang, W. Menapace, A. Siarohin, J. Cao, L. A. Jeni, S. Tulyakov, 和 H. Lee, "4Real: 通过视频扩散模型实现逼真的 4D 场景生成," 在 NeurIPS, 2024.</p></div><p>[118] S. Gao, J. Yang, L. Chen, K. Chitta, Y. Qiu, A. Geiger, J. Zhang, and H. Li, "Vista: A generalizable driving world model with high fidelity and versatile controllability," in NeurIPS, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[118] S. Gao, J. Yang, L. Chen, K. Chitta, Y. Qiu, A. Geiger, J. Zhang, 和 H. Li, "Vista: 一种具有高保真度和多功能可控性的通用驾驶世界模型," 在 NeurIPS, 2024.</p></div><p>[119] Y. Zhao, C. Lin, K. Lin, Z. Yan, L. Li, Z. Yang, J. Wang, G. H. Lee, and L. Wang, "GenXD: generating any 3D and 4D scenes," ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[119] Y. Zhao, C. Lin, K. Lin, Z. Yan, L. Li, Z. Yang, J. Wang, G. H. Lee, 和 L. Wang, "GenXD: 生成任意 3D 和 4D 场景," ICLR, 2025.</p></div><p>[120] H. Che, X. He, Q. Liu, C. Jin, and H. Chen, "GameGen-X: Interactive open-world game video generation," in ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[120] H. Che, X. He, Q. Liu, C. Jin, 和 H. Chen, "GameGen-X: 互动开放世界游戏视频生成," 载于 ICLR, 2025.</p></div><p>[121] B. Mandelbrot, "How long is the coast of britain? statistical self-similarity and fractional dimension," Science, vol. 156, no. 3775, pp. 636-638, 1967.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[121] B. Mandelbrot, "英国的海岸线有多长？统计自相似性与分数维度," 科学, 第156卷, 第3775期, 第636-638页, 1967年。</p></div><p>[122] M. B. B, "The fractal geometry of nature," New York, 1983.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[122] M. B. B, "自然的分形几何," 纽约, 1983年。</p></div><p>[123] A. Fournier, D. S. Fussell, and L. C. Carpenter, "Computer rendering of stochastic models," Commun. ACM, vol. 25, no. 6, pp. 371-384, 1982.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[123] A. Fournier, D. S. Fussell, 和 L. C. Carpenter, "随机模型的计算机渲染," 通信. ACM, 第25卷, 第6期, 第371-384页, 1982年。</p></div><p>[124] P. Przemyslaw and H. Mark, "A fractal model of mountains and rivers," in Graphics Interface, 1993.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[124] P. Przemyslaw 和 H. Mark, "山脉和河流的分形模型," 在图形界面, 1993年。</p></div><p>[125] F. Belhadj and P. Audibert, "Modeling landscapes with ridges and rivers: bottom up approach," in GRAPHITE, 2005.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[125] F. Belhadj 和 P. Audibert, "用山脊和河流建模景观：自下而上的方法," 在GRAPHITE, 2005年。</p></div><p>[126] M. B. B and V. N. J. W, "Fractional brownian motions, fractional noises and applications," SIAM review, vol. 10, no. 4, pp. 422-437, 1968.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[126] M. B. B 和 V. N. J. W, "分数布朗运动、分数噪声及其应用," SIAM评论, 第10卷, 第4期, 第422-437页, 1968年。</p></div><p>[127] L. Aristid, "Mathematical models for cellular interactions in development i. filaments with one-sided inputs," Journal of theoretical biology, vol. 18, no. 3, pp. 280-299, 1968.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[127] L. Aristid, "发育中细胞相互作用的数学模型 I. 单侧输入的细丝," 理论生物学杂志, 第18卷, 第3期, 第280-299页, 1968年。</p></div><p>[128] G. Stiny and J. Gips, "Shape grammars and the generative specification of painting and sculpture," in Information Processing, 1971.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[128] G. Stiny 和 J. Gips, "形状语法与绘画和雕塑的生成规范," 在信息处理, 1971年。</p></div><p>[129] A. D. Kelley, M. C. Malin, and G. M. Nielson, "Terrain simulation using a model of stream erosion," in SIGGRAPH, 1988.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[129] A. D. Kelley, M. C. Malin, 和 G. M. Nielson, "使用溪流侵蚀模型的地形模拟," 在SIGGRAPH, 1988年。</p></div><p>[130] G. Cordonnier, G. Jouvet, A. Peytavie, J. Braun, M. Cani, B. Benes, E. Galin, E. Guérin, and J. Gain, "Forming terrains by glacial erosion," ACM TOG, vol. 42, no. 4, pp. 61:1-61:14, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[130] G. Cordonnier, G. Jouvet, A. Peytavie, J. Braun, M. Cani, B. Benes, E. Galin, E. Guérin, 和 J. Gain, "通过冰川侵蚀形成地形," ACM TOG, 第42卷, 第4期, 第61:1-61:14页, 2023年。</p></div><p>[131] J. Génevaux, E. Galin, E. Guérin, A. Peytavie, and B. Benes, "Terrain generation using procedural models based on hydrology," ACM TOG, vol. 32, no. 4, pp. 143:1-143:13, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[131] J. Génevaux, E. Galin, E. Guérin, A. Peytavie, 和 B. Benes, "基于水文学的程序模型生成地形," ACM TOG, 第32卷, 第4期, 第143:1-143:13页, 2013年。</p></div><p>[132] H. Schott, A. Paris, L. Fournier, E. Guérin, and E. Galin, "Large-scale terrain authoring through interactive erosion simulation," ACM TOG, vol. 42, no. 5, pp. 162:1-162:15, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[132] H. Schott, A. Paris, L. Fournier, E. Guérin, 和 E. Galin, "通过交互式侵蚀模拟进行大规模地形创作," ACM TOG, 第42卷, 第5期, 第162:1-162:15页, 2023年。</p></div><p>[133] A. Paris, E. Guérin, P. Collon, and E. Galin, "Authoring and simulating meandering rivers," ACM TOG, vol. 42, no. 6, pp. 239:1-239:14, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[133] A. Paris, E. Guérin, P. Collon, 和 E. Galin, "创作和模拟蜿蜒的河流," ACM TOG, 第42卷, 第6期, 第239:1-239:14页, 2023年。</p></div><p>[134] O. Deussen, P. Hanrahan, B. Lintermann, R. Mech, M. Pharr, and P. Prusinkiewicz, "Realistic modeling and rendering of plant ecosystems," in SIGGRAPH, 1998.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[134] O. Deussen, P. Hanrahan, B. Lintermann, R. Mech, M. Pharr, 和 P. Prusinkiewicz, "植物生态系统的真实建模与渲染," 在SIGGRAPH, 1998年。</p></div><p>[135] M. Makowski, T. Hädrich, J. Scheffczyk, D. L. Michels, S. Pirk, and W. Palubicki, "Synthetic silviculture: multi-scale modeling of plant ecosystems," ACM TOG, vol. 38, no. 4, pp. 131:1-131:14, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[135] M. Makowski, T. Hädrich, J. Scheffczyk, D. L. Michels, S. Pirk, 和 W. Palubicki, "合成林业：植物生态系统的多尺度建模," ACM TOG, 第38卷, 第4期, 第131:1-131:14页, 2019年。</p></div><p>[136] W. Palubicki, M. Makowski, W. Gajda, T. Hädrich, D. L. Michels, and S. Pirk, "Ecoclimates: climate-response modeling of vegetation," ACM TOG, vol. 41, no. 4, pp. 155:1-155:19, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[136] W. Palubicki, M. Makowski, W. Gajda, T. Hädrich, D. L. Michels, 和 S. Pirk, "生态气候：植被的气候响应建模," ACM TOG, 第41卷, 第4期, 第155:1-155:19页, 2022年。</p></div><p>[137] B. Benes, M. Abdul-Massih, P. Jarvis, D. G. Aliaga, and C. A. Vanegas, "Urban ecosystem design," in Symposium on Interactive 3D Graphics and Games, I3D, 2011.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[137] B. Benes, M. Abdul-Massih, P. Jarvis, D. G. Aliaga, 和 C. A. Vanegas, "城市生态系统设计," 在互动3D图形与游戏研讨会, I3D, 2011.</p></div><p>[138] C. A. Vanegas, D. G. Aliaga, B. Benes, and P. Waddell, "Interactive design of urban spaces using geometrical and behavioral modeling," ACM TOG, vol. 28, no. 5, p. 111, 2009.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[138] C. A. Vanegas, D. G. Aliaga, B. Benes, 和 P. Waddell, "使用几何和行为建模的城市空间互动设计," ACM TOG, 第28卷, 第5期, 第111页, 2009.</p></div><p>[139] B. Weber, P. Müller, P. Wonka, and M. H. Gross, "Interactive geometric simulation of \(4\mathrm{D}\) cities," Computer Graphics Forum, vol. 28, no. 2, pp. 481-492, 2009.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[139] B. Weber, P. Müller, P. Wonka, 和 M. H. Gross, "城市的互动几何模拟," 计算机图形论坛, 第28卷, 第2期, 第481-492页, 2009.</p></div><p>[140] P. Merrell, "Example-based model synthesis," in SI3D, 2007.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[140] P. Merrell, "基于示例的模型合成," 在SI3D, 2007.</p></div><p>[141] P. Merrell and D. Manocha, "Continuous model synthesis," ACM TOG, vol. 27, no. 5, p. 158, 2008.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[141] P. Merrell 和 D. Manocha, "连续模型合成," ACM TOG, 第27卷, 第5期, 第158页, 2008.</p></div><p>[142] H. Zhou, J. Sun, G. Turk, and J. M. Rehg, "Terrain synthesis from digital elevation models," IEEE TVCG, vol. 13, no. 4, pp. 834-848, 2007.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[142] H. Zhou, J. Sun, G. Turk, 和 J. M. Rehg, "从数字高程模型合成地形," IEEE TVCG, 第13卷, 第4期, 第834-848页, 2007.</p></div><p>[143] G. Nishida, I. Garcia-Dorado, and D. G. Aliaga, "Example-driven procedural urban roads," Comput. Graph. Forum, vol. 35, no. 6, pp. 5-17, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[143] G. Nishida, I. Garcia-Dorado, 和 D. G. Aliaga, "基于示例的程序化城市道路," 计算机图形论坛, 第35卷, 第6期, 第5-17页, 2016.</p></div><p>[144] C. A. Vanegas, I. Garcia-Dorado, D. G. Aliaga, B. Benes, and P. Waddell, "Inverse design of urban procedural models," ACM TOG, vol. 31, no. 6, pp. 168:1-168:11, 2012.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[144] C. A. Vanegas, I. Garcia-Dorado, D. G. Aliaga, B. Benes, 和 P. Waddell, "城市程序化模型的逆向设计," ACM TOG, 第31卷, 第6期, 第168:1-168:11页, 2012.</p></div><p>[145] A. Emilien, U. Vimont, M. Cani, P. Poulin, and B. Benes, "World-brush: interactive example-based synthesis of procedural virtual worlds," ACM TOG, vol. 34, no. 4, pp. 106:1-106:11, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[145] A. Emilien, U. Vimont, M. Cani, P. Poulin, 和 B. Benes, "世界画笔：基于示例的程序化虚拟世界的互动合成," ACM TOG, 第34卷, 第4期, 第106:1-106:11页, 2015.</p></div><p>[146] G. Kelly and H. McCabe, "Citygen: An interactive system for procedural city generation," in Fifth International Conference on Game Design and Technology, 2007.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[146] G. Kelly 和 H. McCabe, "Citygen：一个用于程序化城市生成的互动系统," 在第五届国际游戏设计与技术会议, 2007.</p></div><p>[147] K. Xu, J. Stewart, and E. Fiume, "Constraint-based automatic placement for scene composition," in Graphics Interface, 2002.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[147] K. Xu, J. Stewart, 和 E. Fiume, "基于约束的场景组合自动放置," 在图形界面, 2002.</p></div><p>[148] P. Merrell, E. Schkufza, Z. Li, M. Agrawala, and V. Koltun, "Interactive furniture layout using interior design guidelines," ACM TOG, vol. 30, no. 4, p. 87, 2011.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[148] P. Merrell, E. Schkufza, Z. Li, M. Agrawala, 和 V. Koltun, "使用室内设计指南的互动家具布局," ACM TOG, 第30卷, 第4期, 第87页, 2011.</p></div><p>[149] P. Kán and H. Kaufmann, "Automatic furniture arrangement using greedy cost minimization," in VR, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[149] P. Kán 和 H. Kaufmann, "使用贪婪成本最小化的自动家具排列," 在VR, 2018.</p></div><p>[150] Y. Zhao, K. Lin, Z. Jia, Q. Gao, G. Thattai, J. Thomason, and G. S. Sukhatme, "LUMINOUS: indoor scene generation for embodied AI challenges," arXiv 2111.05527, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[150] Y. Zhao, K. Lin, Z. Jia, Q. Gao, G. Thattai, J. Thomason, 和 G. S. Sukhatme, "LUMINOUS：为具身AI挑战生成室内场景," arXiv 2111.05527, 2021.</p></div><p>[151] P. Merrell, E. Schkufza, and V. Koltun, "Computer-generated residential building layouts," ACM TOG, vol. 29, no. 6, p. 181, 2010.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[151] P. Merrell, E. Schkufza, 和 V. Koltun, "计算机生成的住宅建筑布局," ACM TOG, 第29卷, 第6期, 第181页, 2010.</p></div><p>[152] M. Fisher, D. Ritchie, M. Savva, T. A. Funkhouser, and P. Hanra-han,"Example-based synthesis of 3D object arrangements," ACM TOG, vol. 31, no. 6, pp. 135:1-135:11, 2012.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[152] M. Fisher, D. Ritchie, M. Savva, T. A. Funkhouser, 和 P. Hanrahan,"基于示例的3D物体排列合成," ACM TOG, 第31卷, 第6期, 第135:1-135:11页, 2012.</p></div><p>[153] L. Yu, S. K. Yeung, and D. Terzopoulos, "The Clutterpalette: An interactive tool for detailing indoor scenes," IEEE TVCG, vol. 22, no. 2, pp. 1138-1148, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[153] L. Yu, S. K. Yeung, 和 D. Terzopoulos, "Clutterpalette：一个用于详细描述室内场景的交互工具," IEEE TVCG, vol. 22, no. 2, pp. 1138-1148, 2016.</p></div><p>[154] S. Qi, Y. Zhu, S. Huang, C. Jiang, and S. Zhu, "Human-Centric indoor scene synthesis using stochastic grammar," in CVPR, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[154] S. Qi, Y. Zhu, S. Huang, C. Jiang, 和 S. Zhu, "以人为中心的室内场景合成使用随机语法," 在 CVPR, 2018.</p></div><p>[155] S. Zhang, S. Zhang, W. Xie, C. Luo, Y. Yang, and H. Fu, "Fast \(3\mathrm{D}\) indoor scene synthesis by learning spatial relation priors of objects," IEEE TVCG, vol. 28, no. 9, pp. 3082-3092, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[155] S. Zhang, S. Zhang, W. Xie, C. Luo, Y. Yang, 和 H. Fu, "通过学习物体的空间关系先验快速\(3\mathrm{D}\)室内场景合成," IEEE TVCG, vol. 28, no. 9, pp. 3082-3092, 2022.</p></div><p>[156] S. Zhang, Y. Li, Y. He, Y. Yang, and S. Zhang, "MageAdd: Real-time interaction simulation for scene synthesis," in ACM MM, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[156] S. Zhang, Y. Li, Y. He, Y. Yang, 和 S. Zhang, "MageAdd：场景合成的实时交互模拟," 在 ACM MM, 2021.</p></div><p>[157] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe, "Training language models to follow instructions with human feedback," in NeurIPS, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[157] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, 和 R. Lowe, "训练语言模型以遵循人类反馈的指令," 在 NeurIPS, 2022.</p></div><p>[158] OpenAI, "GPT-4 technical report," arXiv 2303.08774, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[158] OpenAI, "GPT-4技术报告," arXiv 2303.08774, 2023.</p></div><p>[159] Y. Yang, J. Lu, Z. Zhao, Z. Luo, J. J. Q. Yu, V. Sanchez, and F. Zheng, "LLplace: The 3D indoor scene layout generation and editing via large language model," arXiv 2406.03866, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[159] Y. Yang, J. Lu, Z. Zhao, Z. Luo, J. J. Q. Yu, V. Sanchez, 和 F. Zheng, "LLplace：通过大型语言模型生成和编辑3D室内场景布局," arXiv 2406.03866, 2024.</p></div><p>[160] R. Aguina-Kang, M. Gumin, D. H. Han, S. Morris, S. J. Yoo, A. Ganeshan, R. K. Jones, Q. A. Wei, K. Fu, and D. Ritchie, "Open-universe indoor scene generation using LLM program synthesis and uncurated object databases," arXiv 2403.09675, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[160] R. Aguina-Kang, M. Gumin, D. H. Han, S. Morris, S. J. Yoo, A. Ganeshan, R. K. Jones, Q. A. Wei, K. Fu, 和 D. Ritchie, "使用LLM程序合成和未整理对象数据库的开放宇宙室内场景生成," arXiv 2403.09675, 2024.</p></div><p>[161] J. Deng, W. Chai, J. Huang, Z. Zhao, Q. Huang, M. Gao, J. Guo, S. Hao, W. Hu, J. Hwang, X. Li, and G. Wang, "CityCraft: A real crafter for 3D city generation," arXiv /2406.04983, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[161] J. Deng, W. Chai, J. Huang, Z. Zhao, Q. Huang, M. Gao, J. Guo, S. Hao, W. Hu, J. Hwang, X. Li, 和 G. Wang, "CityCraft：一个真实的3D城市生成工具," arXiv /2406.04983, 2024.</p></div><p>[162] F. Sun, W. Liu, S. Gu, D. Lim, G. Bhat, F. Tombari, M. Li, N. Haber, and J. Wu, "LayoutVLM: Differentiable optimization of 3D layout via vision-language models," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[162] F. Sun, W. Liu, S. Gu, D. Lim, G. Bhat, F. Tombari, M. Li, N. Haber, 和 J. Wu, "LayoutVLM：通过视觉-语言模型对3D布局的可微优化," 在 CVPR, 2025.</p></div><p>[163] R. Fu, Z. Wen, Z. Liu, and S. Sridhar, "AnyHome: Open-vocabulary generation of structured and textured 3D homes," in \({ECCV},{2024}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[163] R. Fu, Z. Wen, Z. Liu, 和 S. Sridhar, "AnyHome：开放词汇生成结构化和纹理化的3D家居," 在\({ECCV},{2024}\) .</p></div><p>[164] B. M. Öcal, M. Tatarchenko, S. Karaoglu, and T. Gevers, "SceneTeller: Language-to-3D scene generation," in ECCV,2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[164] B. M. Öcal, M. Tatarchenko, S. Karaoglu, 和 T. Gevers, "SceneTeller：语言到3D场景生成," 在 ECCV, 2024.</p></div><p>[165] Y. Zhang, Z. Li, M. Zhou, S. Wu, and J. Wu, "The Scene Language: Representing scenes with programs, words, and embeddings," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[165] Y. Zhang, Z. Li, M. Zhou, S. Wu, 和 J. Wu, "场景语言：用程序、单词和嵌入表示场景," 在 CVPR, 2025.</p></div><p>[166] X. Zhou, X. Ran, Y. Xiong, J. He, Z. Lin, Y. Wang, D. Sun, and M. Yang, "GALA3D: towards text-to-3D complex scene generation via layout-guided generative Gaussian splatting," in ICML, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[166] X. Zhou, X. Ran, Y. Xiong, J. He, Z. Lin, Y. Wang, D. Sun, 和 M. Yang, "GALA3D：通过布局引导的生成高斯点云实现文本到3D复杂场景生成," 在 ICML, 2024.</p></div><p>[167] A. Çelen, G. Han, K. Schindler, L. V. Gool, I. Armeni, A. Obukhov, and X. Wang, "I-Design: Personalized LLM interior designer," arXiv 2404.02838, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[167] A. Çelen, G. Han, K. Schindler, L. V. Gool, I. Armeni, A. Obukhov, 和 X. Wang, "I-Design：个性化的LLM室内设计师," arXiv 2404.02838, 2024.</p></div><p>[168] W. Deng, M. Qi, and H. Ma, "Global-local tree search in vlms for 3D indoor scene generation," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[168] W. Deng, M. Qi, 和 H. Ma, "在vlms中进行全球-局部树搜索以生成3D室内场景," 在CVPR, 2025.</p></div><p>[169] L. Liu, S. Chen, S. Jia, J. Shi, Z. Jiang, C. Jin, W. Zongkai, J. Hwang, and L. Li, "Graph canvas for controllable 3D scene generation," arXiv 2412.00091, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[169] L. Liu, S. Chen, S. Jia, J. Shi, Z. Jiang, C. Jin, W. Zongkai, J. Hwang, 和 L. Li, "可控的3D场景生成的图形画布," arXiv 2412.00091, 2024.</p></div><p>[170] G. Gao, W. Liu, A. Chen, A. Geiger, and B. Schölkopf, "Graph-Dreamer: Compositional 3D scene synthesis from scene graphs," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[170] G. Gao, W. Liu, A. Chen, A. Geiger, 和 B. Schölkopf, "图形梦幻者：基于场景图的组合3D场景合成," 在CVPR, 2024.</p></div><p>[171] X. Li, H. Li, H. Chen, T. Mu, and S. Hu, "DIScene: Object decoupling and interaction modeling for complex scene generation," in SIGGRAPH Asia, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[171] X. Li, H. Li, H. Chen, T. Mu, 和 S. Hu, "DIScene：复杂场景生成的对象解耦和交互建模," 在SIGGRAPH Asia, 2024.</p></div><p>[172] K. Bhat, N. Khanna, K. Channa, T. Zhou, Y. Zhu, X. Sun, C. Shang, A. Sudarshan, M. Chu, D. Li, K. Deng, J. Fauconnier, T. Verhuls-donck, M. Agrawala, K. Fatahalian, A. Weiss, C. Reiser, R. K. Chirravuri, R. Kandur, A. Pelaez, A. Garg, M. Palleschi, J. Wang, S. Litz, L. Liu, A. Li, D. Harmon, D. Liu, L. Feng, D. Goupil, L. Kuczynski, J. Yoon, N. Marri, P. Zhuang, Y. Zhang, B. Yin, H. Jiang, M. van Workum, T. Lane, B. Erickson, S. Pathare, K. Price, A. Singh, and D. Baszucki, "Cube: A roblox view of 3D intelligence," arXiv 2503.15475, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[172] K. Bhat, N. Khanna, K. Channa, T. Zhou, Y. Zhu, X. Sun, C. Shang, A. Sudarshan, M. Chu, D. Li, K. Deng, J. Fauconnier, T. Verhuls-donck, M. Agrawala, K. Fatahalian, A. Weiss, C. Reiser, R. K. Chirravuri, R. Kandur, A. Pelaez, A. Garg, M. Palleschi, J. Wang, S. Litz, L. Liu, A. Li, D. Harmon, D. Liu, L. Feng, D. Goupil, L. Kuczynski, J. Yoon, N. Marri, P. Zhuang, Y. Zhang, B. Yin, H. Jiang, M. van Workum, T. Lane, B. Erickson, S. Pathare, K. Price, A. Singh, 和 D. Baszucki, "Cube：对3D智能的Roblox视角," arXiv 2503.15475, 2025.</p></div><p>[173] J. Liu, S. Zhang, C. Zhang, and S. Zhang, "Controllable procedural generation of landscapes," in ACM MM, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[173] J. Liu, S. Zhang, C. Zhang, 和 S. Zhang, "可控的程序生成景观," 在ACM MM, 2024.</p></div><p>[174] Z. Hu, A. Iscen, A. Jain, T. Kipf, Y. Yue, D. A. Ross, C. Schmid, and A. Fathi, "SceneCraft: An LLM agent for synthesizing 3D scenes as blender code," in ICML, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[174] Z. Hu, A. Iscen, A. Jain, T. Kipf, Y. Yue, D. A. Ross, C. Schmid, 和 A. Fathi, "SceneCraft：用于合成3D场景的LLM代理作为Blender代码," 在ICML, 2024.</p></div><p>[175] Y. Yang, F. Sun, L. Weihs, E. VanderBilt, A. Herrasti, W. Han, J. Wu, N. Haber, R. Krishna, L. Liu, C. Callison-Burch, M. Yatskar, A. Kembhavi, and C. Clark, "Holodeck: Language guided generation of 3D embodied AI environments," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[175] Y. Yang, F. Sun, L. Weihs, E. VanderBilt, A. Herrasti, W. Han, J. Wu, N. Haber, R. Krishna, L. Liu, C. Callison-Burch, M. Yatskar, A. Kembhavi, 和 C. Clark, "Holodeck：语言引导的3D具身AI环境生成," 在CVPR, 2024.</p></div><p>[176] X. Liu, C. Tang, and Y. Tai, "WorldCraft: Photo-realistic 3D world creation and customization via LLM agents," arXiv 2502.15601, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[176] X. Liu, C. Tang, 和 Y. Tai, "WorldCraft：通过LLM代理进行照片级真实感的3D世界创建和定制," arXiv 2502.15601, 2025.</p></div><p>[177] D. Ritchie, K. Wang, and Y. Lin, "Fast and flexible indoor scene synthesis via deep convolutional generative models," in CVPR, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[177] D. Ritchie, K. Wang, 和 Y. Lin, "通过深度卷积生成模型快速灵活地合成室内场景," 在CVPR, 2019.</p></div><p>[178] Z. Zhang, Z. Yang, C. Ma, L. Luo, A. Huth, E. Vouga, and Q. Huang, "Deep generative modeling for scene synthesis via hybrid representations," ACM TOG, vol. 39, no. 2, pp. 17:1-17:21, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[178] Z. Zhang, Z. Yang, C. Ma, L. Luo, A. Huth, E. Vouga, 和 Q. Huang, "通过混合表示进行场景合成的深度生成建模," ACM TOG, vol. 39, no. 2, pp. 17:1-17:21, 2020.</p></div><p>[179] H. Yang, Z. Zhang, S. Yan, H. Huang, C. Ma, Y. Zheng, C. Bajaj, and Q. Huang, "Scene synthesis via uncertainty-driven attribute synchronization," in ICCV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[179] H. Yang, Z. Zhang, S. Yan, H. Huang, C. Ma, Y. Zheng, C. Bajaj, 和 Q. Huang, "通过不确定性驱动的属性同步进行场景合成," 在ICCV, 2021.</p></div><p>[180] X. Wang, C. Yeshwanth, and M. Nießner, "SceneFormer: Indoor scene generation with transformers," in 3DV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[180] X. Wang, C. Yeshwanth, 和 M. Nießner, "SceneFormer：使用变换器生成室内场景," 在3DV, 2021.</p></div><p>[181] W. R. Para, P. Guerrero, N. J. Mitra, and P. Wonka, "COFS: controllable furniture layout synthesis," in SIGGRAPH, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[181] W. R. Para, P. Guerrero, N. J. Mitra, 和 P. Wonka, "COFS：可控的家具布局合成," 在SIGGRAPH, 2023.</p></div><p>[182] Y. Nie, A. Dai, X. Han, and M. Nießner, "Learning 3D scene priors with 2D supervision," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[182] Y. Nie, A. Dai, X. Han, 和 M. Nießner, "通过2D监督学习3D场景先验," 在CVPR, 2023.</p></div><p>[183] Y. Zhao, Z. Zhao, J. Li, S. Dong, and S. Gao, "RoomDesigner: Encoding anchor-latents for style-consistent and shape-compatible indoor scene generation," in 3DV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[183] Y. Zhao, Z. Zhao, J. Li, S. Dong, 和 S. Gao, "RoomDesigner: 编码锚点潜变量以实现风格一致和形状兼容的室内场景生成," 在 3DV, 2024.</p></div><p>[184] W. Feng, H. Zhou, J. Liao, L. Cheng, and W. Zhou, "CasaGPT: cuboid arrangement and scene assembly for interior design," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[184] W. Feng, H. Zhou, J. Liao, L. Cheng, 和 W. Zhou, "CasaGPT: 立方体排列和场景组装用于室内设计," 在 CVPR, 2025.</p></div><p>[185] L. Maillard, N. Sereyjol-Garros, T. Durand, and M. Ovsjanikov, "DeBaRA: Denoising-based 3D room arrangement generation," in NeurIPS, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[185] L. Maillard, N. Sereyjol-Garros, T. Durand, 和 M. Ovsjanikov, "DeBaRA: 基于去噪的 3D 房间排列生成," 在 NeurIPS, 2024.</p></div><p>[186] Y. Yang, B. Jia, P. Zhi, and S. Huang, "PhyScene: Physically interactable 3D scene synthesis for embodied AI," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[186] Y. Yang, B. Jia, P. Zhi, 和 S. Huang, "PhyScene: 物理可交互的 3D 场景合成用于具身 AI," 在 CVPR, 2024.</p></div><p>[187] Z. Ye, X. Zheng, Y. Liu, and Y. Peng, "RelScene: A benchmark and baseline for spatial relations in text-driven 3D scene generation," in ACM MM, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[187] Z. Ye, X. Zheng, Y. Liu, 和 Y. Peng, "RelScene: 文本驱动的 3D 场景生成中的空间关系基准和基线," 在 ACM MM, 2024.</p></div><p>[188] J. Devlin, M. Chang, K. Lee, and K. Toutanova, "BERT: pretraining of deep bidirectional transformers for language understanding," in NAACL-HLT, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[188] J. Devlin, M. Chang, K. Lee, 和 K. Toutanova, "BERT: 深度双向变换器的预训练用于语言理解," 在 NAACL-HLT, 2019.</p></div><p>[189] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-resolution image synthesis with latent diffusion models," in CVPR, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[189] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, 和 B. Ommer, "使用潜在扩散模型进行高分辨率图像合成," 在 CVPR, 2022.</p></div><p>[190] C. Fang, X. Hu, K. Luo, and P. Tan, "Ctrl-Room: Controllable text-to-3D room meshes generation with layout constraints," in 3DV, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[190] C. Fang, X. Hu, K. Luo, 和 P. Tan, "Ctrl-Room: 具有布局约束的可控文本到 3D 房间网格生成," 在 3DV, 2025.</p></div><p>[191] A. Bokhovkin, Q. Meng, S. Tulsiani, and A. Dai, "SceneFactor: Factored latent 3D diffusion for controllable 3D scene generation," arXiv 2412.01801, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[191] A. Bokhovkin, Q. Meng, S. Tulsiani, 和 A. Dai, "SceneFactor: 可控 3D 场景生成的分解潜在 3D 扩散," arXiv 2412.01801, 2024.</p></div><p>[192] D. Epstein, B. Poole, B. Mildenhall, A. A. Efros, and A. Holynski, "Disentangled 3D scene generation with layout learning," in ICML, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[192] D. Epstein, B. Poole, B. Mildenhall, A. A. Efros, 和 A. Holynski, "通过布局学习进行解耦的 3D 场景生成," 在 ICML, 2024.</p></div><p>[193] Q. Zhang, C. Wang, A. Siarohin, P. Zhuang, Y. Xu, C. Yang, D. Lin, B. Zhou, S. Tulyakov, and H. Lee, "SceneWiz3D: Towards text-guided 3D scene composition," arXiv 2312.08885, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[193] Q. Zhang, C. Wang, A. Siarohin, P. Zhuang, Y. Xu, C. Yang, D. Lin, B. Zhou, S. Tulyakov, 和 H. Lee, "SceneWiz3D: 朝着文本引导的 3D 场景构图," arXiv 2312.08885, 2023.</p></div><p>[194] H. Li, H. Shi, W. Zhang, W. Wu, Y. Liao, L. Wang, L. Lee, and P. Y. Zhou, "DreamScene: 3D Gaussian-based text-to-3D scene generation via formation pattern sampling," in ECCV,2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[194] H. Li, H. Shi, W. Zhang, W. Wu, Y. Liao, L. Wang, L. Lee, 和 P. Y. Zhou, "DreamScene: 基于 3D 高斯的文本到 3D 场景生成通过形成模式采样," 在 ECCV, 2024.</p></div><p>[195] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall, "DreamFusion: Text-to-3D using 2D diffusion," in ICLR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[195] B. Poole, A. Jain, J. T. Barron, 和 B. Mildenhall, "DreamFusion: 使用 2D 扩散的文本到 3D," 在 ICLR, 2023.</p></div><p>[196] Y. Nie, A. Dai, X. Han, and M. Nießner, "Pose2Room: Understanding 3D scenes from human activities," in ECCV, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[196] Y. Nie, A. Dai, X. Han, 和 M. Nießner, "Pose2Room: 从人类活动理解 3D 场景," 在 ECCV, 2022.</p></div><p>[197] S. Ye, Y. Wang, J. Li, D. Park, C. K. Liu, H. Xu, and J. Wu, "Scene synthesis from human motion," in SIGGRAPH Asia, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[197] S. Ye, Y. Wang, J. Li, D. Park, C. K. Liu, H. Xu, 和 J. Wu, "基于人类运动的场景合成," 在 SIGGRAPH Asia, 2022.</p></div><p>[198] V. D. An, M. N. Vu, T. Nguyen, B. Huang, D. Nguyen, T. Vo, and A. Nguyen, "Language-driven scene synthesis using multi-conditional diffusion model," in NeurIPS, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[198] V. D. An, M. N. Vu, T. Nguyen, B. Huang, D. Nguyen, T. Vo, 和 A. Nguyen, "基于语言的场景合成使用多条件扩散模型," 在 NeurIPS, 2023.</p></div><p>[199] J. Li, T. Huang, Q. Zhu, and T. Wong, "Physics-based scene layout generation from human motion," in SIGGRAPH, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[199] J. Li, T. Huang, Q. Zhu, 和 T. Wong, "基于物理的人类运动场景布局生成," 在 SIGGRAPH, 2024.</p></div><p>[200] A. X. Chang, M. Savva, and C. D. Manning, "Learning spatial knowledge for text to 3D scene generation," in EMNLP, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[200] A. X. Chang, M. Savva, 和 C. D. Manning, "用于文本到3D场景生成的空间知识学习," 在 EMNLP, 2014.</p></div><p>[201] Z. S. Kermani, Z. Liao, P. Tan, and H. Zhang, "Learning 3D scene synthesis from annotated RGB-D images," Computer Graphics Forum, vol. 35, no. 5, pp. 197-206, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[201] Z. S. Kermani, Z. Liao, P. Tan, 和 H. Zhang, "从标注的RGB-D图像中学习3D场景合成," 计算机图形学论坛, 第35卷，第5期，页197-206, 2016.</p></div><p>[202] Q. Fu, X. Chen, X. Wang, S. Wen, B. Zhou, and H. Fu, "Adaptive synthesis of indoor scenes via activity-associated object relation graphs," ACM TOG, vol. 36, no. 6, pp. 201:1-201:13, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[202] Q. Fu, X. Chen, X. Wang, S. Wen, B. Zhou, 和 H. Fu, "通过活动关联对象关系图自适应合成室内场景," ACM TOG, 第36卷，第6期，页201:1-201:13, 2017.</p></div><p>[203] R. Ma, A. G. Patil, M. Fisher, M. Li, S. Pirk, B. Hua, S. Yeung, X. Tong, L. J. Guibas, and H. Zhang, "Language-driven synthesis of 3D scenes from scene databases," ACM TOG, vol. 37, no. 6, p. 212, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[203] R. Ma, A. G. Patil, M. Fisher, M. Li, S. Pirk, B. Hua, S. Yeung, X. Tong, L. J. Guibas, 和 H. Zhang, "基于语言的3D场景合成来自场景数据库," ACM TOG, 第37卷，第6期，页212, 2018.</p></div><p>[204] A. Luo, Z. Zhang, J. Wu, and J. B. Tenenbaum, "End-to-end optimization of scene layout," in CVPR, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[204] A. Luo, Z. Zhang, J. Wu, 和 J. B. Tenenbaum, "场景布局的端到端优化," 在 CVPR, 2020.</p></div><p>[205] A. Kar, A. Prakash, M. Liu, E. Cameracci, J. Yuan, M. Rusiniak, D. Acuna, A. Torralba, and S. Fidler, "Meta-sim: Learning to generate synthetic datasets," in ICCV, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[205] A. Kar, A. Prakash, M. Liu, E. Cameracci, J. Yuan, M. Rusiniak, D. Acuna, A. Torralba, 和 S. Fidler, "Meta-sim: 学习生成合成数据集," 在 ICCV, 2019.</p></div><p>[206] J. Devaranjan, A. Kar, and S. Fidler, "Meta-sim2: Unsupervised learning of scene structure for synthetic data generation," in ECCV, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[206] J. Devaranjan, A. Kar, 和 S. Fidler, "Meta-sim2: 无监督学习场景结构用于合成数据生成," 在 ECCV, 2020.</p></div><p>[207] L. Gao, J. Sun, K. Mo, Y. Lai, L. J. Guibas, and J. Yang, "Scene-HGN: Hierarchical graph networks for \(3\mathrm{D}\) indoor scene generation with fine-grained geometry," IEEE TPAMI, vol. 45, no. 7, pp. 8902-8919, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[207] L. Gao, J. Sun, K. Mo, Y. Lai, L. J. Guibas, 和 J. Yang, "Scene-HGN: 用于\(3\mathrm{D}\)室内场景生成的分层图网络与细粒度几何," IEEE TPAMI, 第45卷，第7期，页8902-8919, 2023.</p></div><p>[208] G. Zhai, E. P. Örnek, D. Z. Chen, R. Liao, Y. Di, N. Navab, F. Tombari, and B. Busam, "EchoScene: Indoor scene generation via information echo over scene graph diffusion," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[208] G. Zhai, E. P. Örnek, D. Z. Chen, R. Liao, Y. Di, N. Navab, F. Tombari, 和 B. Busam, "EchoScene: 通过场景图扩散的信息回声生成室内场景," 在 ECCV, 2024.</p></div><p>[209] Z. Yang, K. Lu, C. Zhang, J. Qi, H. Jiang, R. Ma, S. Yin, Y. Xu, M. Xing, Z. Xiao, J. Long, X. Liu, and G. Zhai, "MMGDreamer: mixed-modality graph for geometry-controllable 3D indoor scene generation," in AAAI, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[209] Z. Yang, K. Lu, C. Zhang, J. Qi, H. Jiang, R. Ma, S. Yin, Y. Xu, M. Xing, Z. Xiao, J. Long, X. Liu, 和 G. Zhai, "MMGDreamer: 用于几何可控3D室内场景生成的混合模态图," 在 AAAI, 2025.</p></div><p>[210] Z. Wu, M. Feng, Y. Wang, H. Xie, W. Dong, B. Miao, and A. Mian, "External knowledge enhanced 3D scene generation from sketch," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[210] Z. Wu, M. Feng, Y. Wang, H. Xie, W. Dong, B. Miao, 和 A. Mian, "从草图生成增强外部知识的3D场景," 在 ECCV, 2024.</p></div><p>[211] Y. Liu, X. Li, Y. Zhang, L. Qi, X. Li, W. Wang, C. Li, X. Li, and M.- H. Yang, "Controllable 3D outdoor scene generation via scene graphs," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[211] Y. Liu, X. Li, Y. Zhang, L. Qi, X. Li, W. Wang, C. Li, X. Li, 和 M.- H. Yang, "通过场景图可控的3D户外场景生成," 在 CVPR, 2025.</p></div><p>[212] W. Dong, B. Yang, Z. Yang, Y. Li, T. Hu, H. Bao, Y. Ma, and Z. Cui, "HiScene: creating hierarchical 3D scenes with isometric view generation," arXiv 2504.13072, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[212] W. Dong, B. Yang, Z. Yang, Y. Li, T. Hu, H. Bao, Y. Ma, 和 Z. Cui, "HiScene: 创建具有等距视图生成的分层3D场景," arXiv 2504.13072, 2025.</p></div><p>[213] Q. Zhang, Y. Xu, Y. Shen, B. Dai, B. Zhou, and C. Yang, "Berf-Scene: Bev-conditioned equivariant radiance fields for infinite 3D scene generation," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[213] Q. Zhang, Y. Xu, Y. Shen, B. Dai, B. Zhou, 和 C. Yang, "Berf-Scene: Bev条件等变辐射场用于无限3D场景生成," 在 CVPR, 2024.</p></div><p>[214] H. Yan, Y. Li, Z. Wu, S. Chen, W. Sun, T. Shang, W. Liu, T. Chen, X. Dai, C. Ma, H. Li, and P. Ji, "Frankenstein: Generating semantic-compositional \(3\mathrm{D}\) scenes in one tri-plane," in SIGGRAPH Asia, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[214] H. Yan, Y. Li, Z. Wu, S. Chen, W. Sun, T. Shang, W. Liu, T. Chen, X. Dai, C. Ma, H. Li, 和 P. Ji, "弗兰肯斯坦：在一个三维平面中生成语义组合\(3\mathrm{D}\)场景," 在SIGGRAPH Asia, 2024.</p></div><p>[215] E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. D. Mello, O. Gallo, L. J. Guibas, J. Tremblay, S. Khamis, T. Karras, and G. Wetzstein, "Efficient geometry-aware 3D generative adversarial networks," in CVPR, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[215] E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. D. Mello, O. Gallo, L. J. Guibas, J. Tremblay, S. Khamis, T. Karras, 和 G. Wetzstein, "高效的几何感知3D生成对抗网络," 在CVPR, 2022.</p></div><p>[216] C. H. Lin, H. Lee, Y. Cheng, S. Tulyakov, and M. Yang, "Infinity-GAN: Towards infinite-pixel image synthesis," in ICLR, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[216] C. H. Lin, H. Lee, Y. Cheng, S. Tulyakov, 和 M. Yang, "无限生成对抗网络：朝着无限像素图像合成的方向," 在ICLR, 2022.</p></div><p>[217] H. Xie, Z. Chen, F. Hong, and Z. Liu, "Generative Gaussian splatting for unbounded 3D city generation," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[217] H. Xie, Z. Chen, F. Hong, 和 Z. Liu, "生成高斯溅射用于无限制的3D城市生成," 在CVPR, 2025.</p></div><p>[218] H. Xie, Z. Chen, F. Hong, and Z. Liu, "Compositional generative model of unbounded 4D cities," arXiv 2501.08983, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[218] H. Xie, Z. Chen, F. Hong, 和 Z. Liu, "无限制4D城市的组合生成模型," arXiv 2501.08983, 2025.</p></div><p>[219] Y. Yang, Y. Yang, H. Guo, R. Xiong, Y. Wang, and Y. Liao, "UrbanGIRAFFE: Representing urban scenes as compositional generative neural feature fields," in ICCV, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[219] Y. Yang, Y. Yang, H. Guo, R. Xiong, Y. Wang, 和 Y. Liao, "UrbanGIRAFFE: 将城市场景表示为组合生成神经特征场," 在ICCV, 2023.</p></div><p>[220] Y. Xu, M. Chai, Z. Shi, S. Peng, I. Skorokhodov, A. Siarohin, C. Yang, Y. Shen, H. Lee, B. Zhou, and S. Tulyakov, "DisCoScene: Spatially disentangled generative radiance fields for controllable 3D-aware scene synthesis," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[220] Y. Xu, M. Chai, Z. Shi, S. Peng, I. Skorokhodov, A. Siarohin, C. Yang, Y. Shen, H. Lee, B. Zhou, 和 S. Tulyakov, "DisCoScene: 空间解耦的生成辐射场用于可控的3D感知场景合成," 在CVPR, 2023.</p></div><p>[221] Y. Lin, H. Bai, S. Li, H. Lu, X. Lin, H. Xiong, and L. Wang, "CompoNeRF: Text-guided multi-object compositional nerf with editable 3D scene layout," arXiv 2303.13843, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[221] Y. Lin, H. Bai, S. Li, H. Lu, X. Lin, H. Xiong, 和 L. Wang, "CompoNeRF: 文本引导的多对象组合nerf与可编辑的3D场景布局," arXiv 2303.13843, 2023.</p></div><p>[222] D. Cohen-Bar, E. Richardson, G. Metzer, R. Giryes, and D. Cohen-Or, "Set-the-Scene: Global-local training for generating controllable nerf scenes," in ICCV, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[222] D. Cohen-Bar, E. Richardson, G. Metzer, R. Giryes, 和 D. Cohen-Or, "Set-the-Scene: 生成可控nerf场景的全局-局部训练," 在ICCV, 2023.</p></div><p>[223] J. Zhou, X. Li, L. Qi, and M. Yang, "Layout-your-3D: Controllable and precise 3D generation with 2D blueprint," arXiv 2410.15391, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[223] J. Zhou, X. Li, L. Qi, 和 M. Yang, "Layout-your-3D: 使用2D蓝图进行可控和精确的3D生成," arXiv 2410.15391, 2024.</p></div><p>[224] X. Yang, Y. Man, J. Chen, and Y. Wang, "SceneCraft: Layout-guided 3D scene generation," in NeurIPS, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[224] X. Yang, Y. Man, J. Chen, 和 Y. Wang, "SceneCraft: 布局引导的3D场景生成," 在NeurIPS, 2024.</p></div><p>[225] M. Chen, L. Wang, S. Ao, Y. Zhang, K. Xu, and Y. Guo, "Layout2Scene: 3D semantic layout guided scene generation via geometry and appearance diffusion priors," arXiv 2501.02519, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[225] M. Chen, L. Wang, S. Ao, Y. Zhang, K. Xu, 和 Y. Guo, "Layout2Scene: 通过几何和外观扩散先验进行3D语义布局引导的场景生成," arXiv 2501.02519, 2025.</p></div><p>[226] F. Lu, K. Lin, Y. Xu, H. Li, G. Chen, and C. Jiang, "Urban Architect: Steerable 3D urban scene generation with layout prior," arXiv 2404.06780, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[226] F. Lu, K. Lin, Y. Xu, H. Li, G. Chen, 和 C. Jiang, "Urban Architect: 使用布局先验的可操控3D城市场景生成," arXiv 2404.06780, 2024.</p></div><p>[227] A. R. Kosiorek, H. Strathmann, D. Zoran, P. Moreno, R. Schneider, S. Mokrá, and D. J. Rezende, "NeRF-VAE: A geometry aware 3D scene generative model," in ICML, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[227] A. R. Kosiorek, H. Strathmann, D. Zoran, P. Moreno, R. Schneider, S. Mokrá, 和 D. J. Rezende, "NeRF-VAE: 一种几何感知的3D场景生成模型," 在ICML, 2021.</p></div><p>[228] M. Niemeyer and A. Geiger, "GIRAFFE: representing scenes as compositional generative neural feature fields," in CVPR, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[228] M. Niemeyer 和 A. Geiger, "GIRAFFE: 将场景表示为组合生成神经特征场," 在CVPR, 2021.</p></div><p>[229] L. Chai, R. Tucker, Z. Li, P. Isola, and N. Snavely, "Persistent Nature: A generative model of unbounded 3D worlds," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[229] L. Chai, R. Tucker, Z. Li, P. Isola, 和 N. Snavely, "持久自然: 一种无限制3D世界的生成模型," 在CVPR, 2023.</p></div><p>[230] Y. Yang, J. Shao, X. Li, Y. Shen, A. Geiger, and Y. Liao, "Prometheus: 3D-aware latent diffusion models for feed-forward text-to-3D scene generation," arXiv 2412.21117, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[230] Y. Yang, J. Shao, X. Li, Y. Shen, A. Geiger, 和 Y. Liao, "Prometheus: 用于前馈文本到3D场景生成的3D感知潜在扩散模型," arXiv 2412.21117, 2024.</p></div><p>[231] H. Go, B. Park, J. Jang, J. Kim, S. Kwon, and C. Kim, "Splat-Flow: Multi-view rectified flow model for 3D gaussian splatting synthesis," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[231] H. Go, B. Park, J. Jang, J. Kim, S. Kwon, 和 C. Kim, "Splat-Flow: 用于3D高斯溅射合成的多视角校正流模型," 在CVPR, 2025.</p></div><p>[232] J. Lee, W. Im, S. Lee, and S. Yoon, "Diffusion probabilistic models for scene-scale 3D categorical data," arXiv 2301.00527, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[232] J. Lee, W. Im, S. Lee, 和 S. Yoon, "用于场景规模3D分类数据的扩散概率模型," arXiv 2301.00527, 2023.</p></div><p>[233] X. Ju, Z. Huang, Y. Li, G. Zhang, Y. Qiao, and H. Li, "DiffInD-Scene: Diffusion-based high-quality 3D indoor scene generation," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[233] X. Ju, Z. Huang, Y. Li, G. Zhang, Y. Qiao, 和 H. Li, "DiffInD-Scene: 基于扩散的高质量3D室内场景生成," 在CVPR, 2024.</p></div><p>[234] Y. Liu, X. Li, X. Li, L. Qi, C. Li, and M. Yang, "Pyramid diffusion for fine 3D large scene generation," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[234] Y. Liu, X. Li, X. Li, L. Qi, C. Li, 和 M. Yang, "金字塔扩散用于精细的3D大场景生成," 在ECCV, 2024.</p></div><p>[235] Q. Meng, L. Li, M. Nießner, and A. Dai, "LT3SD: latent trees for 3D scene diffusion," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[235] Q. Meng, L. Li, M. Nießner, 和 A. Dai, "LT3SD: 用于3D场景扩散的潜在树," 在CVPR, 2025.</p></div><p>[236] J. Lee, S. Lee, C. Jo, W. Im, J. Seon, and S. Yoon, "SemCity: Semantic scene generation with triplane diffusion," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[236] J. Lee, S. Lee, C. Jo, W. Im, J. Seon, 和 S. Yoon, "SemCity: 基于三平面扩散的语义场景生成," 在CVPR, 2024.</p></div><p>[237] H. Lee, Q. Han, and A. X. Chang, "NuiScene: Exploring efficient generation of unbounded outdoor scenes," arXiv 2503.16375, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[237] H. Lee, Q. Han, 和 A. X. Chang, "NuiScene: 探索无界户外场景的高效生成," arXiv 2503.16375, 2025.</p></div><p>[238] H. Bian, L. Kong, H. Xie, L. Pan, Y. Qiao, and Z. Liu, "DynamicC-ity: Large-scale 4D occupancy generation from dynamic scenes," in ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[238] H. Bian, L. Kong, H. Xie, L. Pan, Y. Qiao, 和 Z. Liu, "DynamicC-ity: 从动态场景生成大规模4D占用," 在ICLR, 2025.</p></div><p>[239] A. Cao and J. Johnson, "HexPlane: A fast representation for dynamic scenes," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[239] A. Cao 和 J. Johnson, "HexPlane: 动态场景的快速表示," 在CVPR, 2023.</p></div><p>[240] N. Akimoto, S. Kasai, M. Hayashi, and Y. Aoki, "360-degree image completion by two-stage conditional gans," in ICIP, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[240] N. Akimoto, S. Kasai, M. Hayashi, 和 Y. Aoki, "通过两阶段条件生成对抗网络进行360度图像补全," 在ICIP, 2019.</p></div><p>[241] J. S. Sumantri and I. K. Park, "360 panorama synthesis from a sparse set of images with unknown field of view," in WACV, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[241] J. S. Sumantri 和 I. K. Park, "从稀疏图像集生成360全景合成，视场未知," 在WACV, 2020.</p></div><p>[242] G. Somanath and D. Kurz, "HDR environment map estimation for real-time augmented reality," in CVPR, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[242] G. Somanath 和 D. Kurz, "实时增强现实的HDR环境图估计," 在CVPR, 2021.</p></div><p>[243] T. Hara, Y. Mukuta, and T. Harada, "Spherical image generation from a single image by considering scene symmetry," in AAAI, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[243] T. Hara, Y. Mukuta, 和 T. Harada, "通过考虑场景对称性从单幅图像生成球形图像," 在AAAI, 2021.</p></div><p>[244] T. Hara, Y. Mukuta, and T. Harada, "Spherical image generation from a few normal-field-of-view images by considering scene symmetry," IEEE TPAMI, vol. 45, no. 5, pp. 6339-6353, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[244] T. Hara, Y. Mukuta, 和 T. Harada, "通过考虑场景对称性从少量正常视场图像生成球形图像," IEEE TPAMI, vol. 45, no. 5, pp. 6339-6353, 2023.</p></div><p>[245] C. Oh, W. Cho, Y. Chae, D. Park, L. Wang, and K. Yoon, "BIPS: bi-modal indoor panorama synthesis via residual depth-aided adversarial learning," in ECCV, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[245] C. Oh, W. Cho, Y. Chae, D. Park, L. Wang, 和 K. Yoon, "BIPS: 通过残差深度辅助对抗学习进行双模态室内全景合成," 在ECCV, 2022.</p></div><p>[246] S. Zhao, J. Cui, Y. Sheng, Y. Dong, X. Liang, E. I. Chang, and Y. Xu, "Large scale image completion via co-modulated generative adversarial networks," in ICLR, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[246] S. Zhao, J. Cui, Y. Sheng, Y. Dong, X. Liang, E. I. Chang, 和 Y. Xu, "通过共同调制生成对抗网络进行大规模图像补全," 在ICLR, 2021.</p></div><p>[247] P. Esser, R. Rombach, and B. Ommer, "Taming transformers for high-resolution image synthesis," in CVPR, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[247] P. Esser, R. Rombach, 和 B. Ommer, "驯化变换器以进行高分辨率图像合成," 在CVPR, 2021.</p></div><p>[248] N. Akimoto, Y. Matsuo, and Y. Aoki, "Diverse plausible 360- degree image outpainting for efficient 3DCG background creation," in CVPR, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[248] N. Akimoto, Y. Matsuo, 和 Y. Aoki, "多样化合理的360度图像外延以高效创建3DCG背景," 在CVPR, 2022.</p></div><p>[249] H. Ai, Z. Cao, H. Lu, C. Chen, J. Ma, P. Zhou, T. Kim, P. Hui, and L. Wang, "Dream360: Diverse and immersive outdoor virtual scene creation via transformer-based \({360}^{ \circ  }\) image outpainting," IEEE TVCG, vol. 30, no. 5, pp. 2734-2744, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[249] H. Ai, Z. Cao, H. Lu, C. Chen, J. Ma, P. Zhou, T. Kim, P. Hui, 和 L. Wang, "Dream360: 通过基于变换器的 \({360}^{ \circ  }\) 图像外推创建多样化和沉浸式户外虚拟场景," IEEE TVCG, vol. 30, no. 5, pp. 2734-2744, 2024.</p></div><p>[250] T. Wu, C. Zheng, and T. Cham, "PanoDiffusion: 360-degree panorama outpainting via diffusion," in ICLR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[250] T. Wu, C. Zheng, 和 T. Cham, "PanoDiffusion: 通过扩散进行360度全景外推," 在 ICLR, 2024.</p></div><p>[251] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, "Learning transferable visual models from natural language supervision," in ICML, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[251] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, 和 I. Sutskever, "从自然语言监督中学习可转移的视觉模型," 在 ICML, 2021.</p></div><p>[252] Y. Lee, K. Kim, H. Kim, and M. Sung, "SyncDiffusion: Coherent montage via synchronized joint diffusions," in NeurIPS, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[252] Y. Lee, K. Kim, H. Kim, 和 M. Sung, "SyncDiffusion: 通过同步联合扩散实现一致的蒙太奇," 在 NeurIPS, 2023.</p></div><p>[253] O. Bar-Tal, L. Yariv, Y. Lipman, and T. Dekel, "MultiDiffusion: Fusing diffusion paths for controlled image generation," in ICML, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[253] O. Bar-Tal, L. Yariv, Y. Lipman, 和 T. Dekel, "MultiDiffusion: 融合扩散路径以控制图像生成," 在 ICML, 2023.</p></div><p>[254] Q. Zhang, J. Song, X. Huang, Y. Chen, and M. Liu, "DiffCollage: Parallel generation of large content with diffusion models," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[254] Q. Zhang, J. Song, X. Huang, Y. Chen, 和 M. Liu, "DiffCollage: 使用扩散模型并行生成大内容," 在 CVPR, 2023.</p></div><p>[255] N. Kalischek, M. Oechsle, F. Manhardt, P. Henzler, K. Schindler, and F. Tombari, "CubeDiff: Repurposing diffusion-based image models for panorama generation," in ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[255] N. Kalischek, M. Oechsle, F. Manhardt, P. Henzler, K. Schindler, 和 F. Tombari, "CubeDiff: 重新利用基于扩散的图像模型进行全景生成," 在 ICLR, 2025.</p></div><p>[256] H. Wang, X. Xiang, Y. Fan, and J. Xue, "Customizing 360-degree panoramas through text-to-image diffusion models," in WACV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[256] H. Wang, X. Xiang, Y. Fan, 和 J. Xue, "通过文本到图像扩散模型定制360度全景," 在 WACV, 2024.</p></div><p>[257] M. Feng, J. Liu, M. Cui, and X. Xie, "Diffusion360: Seamless 360 degree panoramic image generation based on diffusion models," arXiv 2311.13141, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[257] M. Feng, J. Liu, M. Cui, 和 X. Xie, "Diffusion360: 基于扩散模型的无缝360度全景图像生成," arXiv 2311.13141, 2023.</p></div><p>[258] J. Wang, Z. Chen, J. Ling, R. Xie, and L. Song, "360-degree panorama generation from few unregistered nfov images," in ACM MM, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[258] J. Wang, Z. Chen, J. Ling, R. Xie, 和 L. Song, "从少量未注册的nfov图像生成360度全景," 在 ACM MM, 2023.</p></div><p>[259] W. Ye, C. Ji, Z. Chen, J. Gao, X. Huang, S. Zhang, W. Ouyang, T. He, C. Zhao, and G. Zhang, "DiffPano: Scalable and consistent text to panorama generation with spherical epipolar-aware diffusion," in NeurIPS, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[259] W. Ye, C. Ji, Z. Chen, J. Gao, X. Huang, S. Zhang, W. Ouyang, T. He, C. Zhao, 和 G. Zhang, "DiffPano: 使用球面极线感知扩散进行可扩展和一致的文本到全景生成," 在 NeurIPS, 2024.</p></div><p>[260] G. B. M. Stan, D. Wofk, S. Fox, A. Redden, W. Saxton, J. Yu, E. Aflalo, S. Tseng, F. Nonato, M. Müller, and V. Lal, "LDM3D: latent diffusion model for 3D," arXiv 2305.10853, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[260] G. B. M. Stan, D. Wofk, S. Fox, A. Redden, W. Saxton, J. Yu, E. Aflalo, S. Tseng, F. Nonato, M. Müller, 和 V. Lal, "LDM3D: 3D的潜在扩散模型," arXiv 2305.10853, 2023.</p></div><p>[261] J. Schult, S. S. Tsai, L. Höllein, B. Wu, J. Wang, C. Ma, K. Li, X. Wang, F. Wimbauer, Z. He, P. Zhang, B. Leibe, P. Vajda, and J. Hou, "ControlRoom3D: Room generation using semantic proxy rooms," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[261] J. Schult, S. S. Tsai, L. Höllein, B. Wu, J. Wang, C. Ma, K. Li, X. Wang, F. Wimbauer, Z. He, P. Zhang, B. Leibe, P. Vajda, 和 J. Hou, "ControlRoom3D: 使用语义代理房间生成房间," 在 CVPR, 2024.</p></div><p>[262] S. Zhou, Z. Fan, D. Xu, H. Chang, P. Chari, T. Bharadwaj, S. You, Z. Wang, and A. Kadambi, "DreamScene360: Unconstrained text-to-3D scene generation with panoramic Gaussian splatting," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[262] S. Zhou, Z. Fan, D. Xu, H. Chang, P. Chari, T. Bharadwaj, S. You, Z. Wang, 和 A. Kadambi, "DreamScene360: 使用全景高斯喷溅进行无约束的文本到3D场景生成," 在 ECCV, 2024.</p></div><p>[263] Y. Ma, D. Zhan, and Z. Jin, "FastScene: Text-driven fast indoor 3D scene generation via panoramic Gaussian splatting," in IJCAI, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[263] Y. Ma, D. Zhan, 和 Z. Jin, "FastScene: 通过全景高斯喷溅进行文本驱动的快速室内3D场景生成," 在 IJCAI, 2024.</p></div><p>[264] H. Zhou, X. Cheng, W. Yu, Y. Tian, and L. Yuan, "HoloDreamer: Holistic 3D panoramic world generation from text descriptions," arXiv 2407.15187, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[264] H. Zhou, X. Cheng, W. Yu, Y. Tian, 和 L. Yuan, "HoloDreamer: 从文本描述生成整体3D全景世界," arXiv 2407.15187, 2024.</p></div><p>[265] W. Li, Y. Mi, F. Cai, Z. Yang, W. Zuo, X. Wang, and X. Fan, "SceneDreamer360: Text-driven 3D-consistent scene generation with panoramic Gaussian splatting," arXiv 2408.13711, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[265] W. Li, Y. Mi, F. Cai, Z. Yang, W. Zuo, X. Wang, 和 X. Fan, "SceneDreamer360: 基于文本的360度一致场景生成与全景高斯喷溅," arXiv 2408.13711, 2024.</p></div><p>[266] X. Lu, Z. Li, Z. Cui, M. R. Oswald, M. Pollefeys, and R. Qin, "Geometry-aware satellite-to-ground image synthesis for urban areas," in CVPR, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[266] X. Lu, Z. Li, Z. Cui, M. R. Oswald, M. Pollefeys, 和 R. Qin, "几何感知的卫星到地面图像合成用于城市区域," 在 CVPR, 2020.</p></div><p>[267] Y. Shi, D. Campbell, X. Yu, and H. Li, "Geometry-guided street-view panorama synthesis from satellite imagery," IEEE TPAMI, vol. 44, no. 12, pp. 10009-10022, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[267] Y. Shi, D. Campbell, X. Yu, 和 H. Li, "基于几何引导的街景全景合成从卫星图像," IEEE TPAMI, vol. 44, no. 12, pp. 10009-10022, 2022.</p></div><p>[268] S. Wu, H. Tang, X. Jing, H. Zhao, J. Qian, N. Sebe, and Y. Yan, "Cross-view panorama image synthesis," IEEE TMM, vol. 25, pp. 3546-3559, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[268] S. Wu, H. Tang, X. Jing, H. Zhao, J. Qian, N. Sebe, 和 Y. Yan, "跨视角全景图像合成," IEEE TMM, vol. 25, pp. 3546-3559, 2023.</p></div><p>[269] Z. Li, Z. Li, Z. Cui, R. Qin, M. Pollefeys, and M. R. Oswald, "Sat2vid: Street-view panoramic video synthesis from a single satellite image," in ICCV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[269] Z. Li, Z. Li, Z. Cui, R. Qin, M. Pollefeys, 和 M. R. Oswald, "Sat2vid: 从单个卫星图像合成街景全景视频," 在 ICCV, 2021.</p></div><p>[270] M. Qian, J. Xiong, G. Xia, and N. Xue, "Sat2Density: Faithful density learning from satellite-ground image pairs," in ICCV, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[270] M. Qian, J. Xiong, G. Xia, 和 N. Xue, "Sat2Density: 从卫星-地面图像对中忠实学习密度," 在 ICCV, 2023.</p></div><p>[271] N. Xu and R. Qin, "Geospecific view generation geometry-context aware high-resolution ground view inference from satellite views," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[271] N. Xu 和 R. Qin, "地理特定视图生成几何上下文感知的高分辨率地面视图推断从卫星视图," 在 ECCV, 2024.</p></div><p>[272] S. Niklaus, L. Mai, J. Yang, and F. Liu, "3D ken burns effect from a single image," ACM TOG, vol. 38, no. 6, pp. 184:1-184:15, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[272] S. Niklaus, L. Mai, J. Yang, 和 F. Liu, "从单个图像生成3D肯·伯恩斯效果," ACM TOG, vol. 38, no. 6, pp. 184:1-184:15, 2019.</p></div><p>[273] O. Wiles, G. Gkioxari, R. Szeliski, and J. Johnson, "SynSin: End-to-end view synthesis from a single image," in CVPR, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[273] O. Wiles, G. Gkioxari, R. Szeliski, 和 J. Johnson, "SynSin: 从单个图像的端到端视图合成," 在 CVPR, 2020.</p></div><p>[274] J. Y. Koh, H. Agrawal, D. Batra, R. Tucker, A. Waters, H. Lee, Y. Yang, J. Baldridge, and P. Anderson, "Simple and effective synthesis of indoor 3D scenes," in AAAI, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[274] J. Y. Koh, H. Agrawal, D. Batra, R. Tucker, A. Waters, H. Lee, Y. Yang, J. Baldridge, 和 P. Anderson, "简单有效的室内3D场景合成," 在 AAAI, 2023.</p></div><p>[275] R. Tucker and N. Snavely, "Single-view view synthesis with multiplane images," in CVPR, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[275] R. Tucker 和 N. Snavely, "使用多平面图像的单视图合成," 在 CVPR, 2020.</p></div><p>[276] T. A. Habtegebrial, V. Jampani, O. Gallo, and D. Stricker, "Generative view synthesis: From single-view semantics to novel-view images," in NeurIPS, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[276] T. A. Habtegebrial, V. Jampani, O. Gallo, 和 D. Stricker, "生成视图合成: 从单视图语义到新视图图像," 在 NeurIPS, 2020.</p></div><p>[277] M. Shih, S. Su, J. Kopf, and J. Huang, "3D photography using context-aware layered depth inpainting," in CVPR, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[277] M. Shih, S. Su, J. Kopf, 和 J. Huang, "使用上下文感知的分层深度修复进行3D摄影," 在 CVPR, 2020.</p></div><p>[278] R. Hu, N. Ravi, A. C. Berg, and D. Pathak, "Worldsheet: Wrapping the world in a 3D sheet for view synthesis from a single image," in ICCV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[278] R. Hu, N. Ravi, A. C. Berg, 和 D. Pathak, "世界表: 将世界包裹在3D表单中以进行从单个图像的视图合成," 在 ICCV, 2021.</p></div><p>[279] J. Y. Koh, H. Lee, Y. Yang, J. Baldridge, and P. Anderson, "Path-dreamer: A world model for indoor navigation," in ICCV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[279] J. Y. Koh, H. Lee, Y. Yang, J. Baldridge, 和 P. Anderson, "路径梦想者: 室内导航的世界模型," 在 ICCV, 2021.</p></div><p>[280] Y. Shen, W. Ma, and S. Wang, "SGAM: building a virtual 3D world through simultaneous generation and mapping," in NeurIPS, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[280] Y. Shen, W. Ma, 和 S. Wang, "SGAM: 通过同时生成和映射构建虚拟3D世界," 在 NeurIPS, 2022.</p></div><p>[281] S. Cai, E. R. Chan, S. Peng, M. Shahbazi, A. Obukhov, L. V. Gool, and G. Wetzstein, "DiffDreamer: Towards consistent unsupervised single-view scene extrapolation with conditional diffusion models," in ICCV, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[281] S. Cai, E. R. Chan, S. Peng, M. Shahbazi, A. Obukhov, L. V. Gool, 和 G. Wetzstein, "DiffDreamer: 朝着一致的无监督单视图场景外推与条件扩散模型," 在 ICCV, 2023.</p></div><p>[282] X. Ren and X. Wang, "Look outside the room: Synthesizing A consistent long-term 3D scene video from A single image," in CVPR, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[282] X. Ren 和 X. Wang, "走出房间: 从单张图像合成一致的长期 3D 场景视频," 在 CVPR, 2022.</p></div><p>[283] H. Tseng, Q. Li, C. Kim, S. Alsisan, J. Huang, and J. Kopf, "Consistent view synthesis with pose-guided diffusion models," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[283] H. Tseng, Q. Li, C. Kim, S. Alsisan, J. Huang, 和 J. Kopf, "基于姿态引导的扩散模型进行一致视图合成," 在 CVPR, 2023.</p></div><p>[284] J. J. Yu, F. Forghani, K. G. Derpanis, and M. A. Brubaker, "Long-term photometric consistent novel view synthesis with diffusion models," in ICCV, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[284] J. J. Yu, F. Forghani, K. G. Derpanis, 和 M. A. Brubaker, "基于扩散模型的长期光度一致新视图合成," 在 ICCV, 2023.</p></div><p>[285] M. Wallingford, A. Bhattad, A. Kusupati, V. Ramanujan, M. Deitke, A. Kembhavi, R. Mottaghi, W. Ma, and A. Farhadi, "From an Image to a Scene: Learning to imagine the world from a million \({360}^{ \circ  }\) videos," in NeurIPS,2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[285] M. Wallingford, A. Bhattad, A. Kusupati, V. Ramanujan, M. Deitke, A. Kembhavi, R. Mottaghi, W. Ma, 和 A. Farhadi, "从图像到场景: 学习从百万个\({360}^{ \circ  }\)视频中想象世界," 在 NeurIPS, 2024.</p></div><p>[286] R. Gao, A. Holynski, P. Henzler, A. Brussee, R. Martin-Brualla, P. P. Srinivasan, J. T. Barron, and B. Poole, "CAT3D: create anything in 3D with multi-view diffusion models," in NeurIPS, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[286] R. Gao, A. Holynski, P. Henzler, A. Brussee, R. Martin-Brualla, P. P. Srinivasan, J. T. Barron, 和 B. Poole, "CAT3D: 使用多视图扩散模型创建任何 3D 物体," 在 NeurIPS, 2024.</p></div><p>[287] S. Szymanowicz, J. Y. Zhang, P. P. Srinivasan, R. Gao, A. Brussee, A. Holynski, R. Martin-Brualla, J. T. Barron, and P. Henzler, "Bolt3D: Generating 3D scenes in seconds," arXiv 2503.14445, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[287] S. Szymanowicz, J. Y. Zhang, P. P. Srinivasan, R. Gao, A. Brussee, A. Holynski, R. Martin-Brualla, J. T. Barron, 和 P. Henzler, "Bolt3D: 在几秒钟内生成 3D 场景," arXiv 2503.14445, 2025.</p></div><p>[288] L. Zhang, A. Rao, and M. Agrawala, "Adding conditional control to text-to-image diffusion models," in ICCV, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[288] L. Zhang, A. Rao, 和 M. Agrawala, "为文本到图像扩散模型添加条件控制," 在 ICCV, 2023.</p></div><p>[289] J. Li and M. Bansal, "PanoGen: Text-conditioned panoramic environment generation for vision-and-language navigation," in NeurIPS, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[289] J. Li 和 M. Bansal, "PanoGen: 用于视觉和语言导航的文本条件全景环境生成," 在 NeurIPS, 2023.</p></div><p>[290] Z. Lu, K. Hu, C. Wang, L. Bai, and Z. Wang, "Autoregressive omni-aware outpainting for open-vocabulary 360-degree image generation," in AAAI, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[290] Z. Lu, K. Hu, C. Wang, L. Bai, 和 Z. Wang, "自回归全知外推用于开放词汇 360 度图像生成," 在 AAAI, 2024.</p></div><p>[291] A. Liu, Z. Li, Z. Chen, N. Li, Y. Xu, and B. A. Plummer, "PanoFree: tuning-free holistic multi-view image generation with cross-view self-guidance," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[291] A. Liu, Z. Li, Z. Chen, N. Li, Y. Xu, 和 B. A. Plummer, "PanoFree: 无需调优的整体多视图图像生成与跨视图自我引导," 在 ECCV, 2024.</p></div><p>[292] P. Gao, K. Yao, T. Ye, S. Wang, Y. Yao, and X. Wang, "Opa-ma: Text guided mamba for 360-degree image out-painting," arXiv 2407.10923, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[292] P. Gao, K. Yao, T. Ye, S. Wang, Y. Yao, 和 X. Wang, "Opa-ma: 文本引导的 360 度图像外推," arXiv 2407.10923, 2024.</p></div><p>[293] P. Engstler, A. Vedaldi, I. Laina, and C. Rupprecht, "Invisible Stitch: Generating smooth 3D scenes with depth inpainting," in 3DV, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[293] P. Engstler, A. Vedaldi, I. Laina, 和 C. Rupprecht, "隐形缝合: 生成具有深度修复的平滑 3D 场景," 在 3DV, 2025.</p></div><p>[294] R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun, "Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer," IEEE TPAMI, vol. 44, no. 3, pp. 1623-1637, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[294] R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, 和 V. Koltun, "朝着稳健的单目深度估计: 混合数据集以实现零样本跨数据集迁移," IEEE TPAMI, vol. 44, no. 3, pp. 1623-1637, 2022.</p></div><p>[295] S. F. Bhat, R. Birkl, D. Wofk, P. Wonka, and M. Müller, "Zoedepth: Zero-shot transfer by combining relative and metric depth," arXiv 2302.12288, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[295] S. F. Bhat, R. Birkl, D. Wofk, P. Wonka, 和 M. Müller, "Zoedepth: 通过结合相对和度量深度实现零样本迁移," arXiv 2302.12288, 2023.</p></div><p>[296] S. M. H. Miangoleh, S. Dille, L. Mai, S. Paris, and Y. Aksoy, "Boosting monocular depth estimation models to high-resolution via content-adaptive multi-resolution merging," in CVPR, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[296] S. M. H. Miangoleh, S. Dille, L. Mai, S. Paris, 和 Y. Aksoy, "通过内容自适应多分辨率合并提升单目深度估计模型至高分辨率," 在 CVPR, 2021.</p></div><p>[297] R. Fridman, A. Abecasis, Y. Kasten, and T. Dekel, "SceneScape: Text-driven consistent scene generation," in NeurIPS, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[297] R. Fridman, A. Abecasis, Y. Kasten, 和 T. Dekel, "SceneScape: 基于文本的一致场景生成," 在 NeurIPS, 2023.</p></div><p>[298] X. Li, Y. Wu, J. Cen, J. Peng, K. Wang, K. Xian, Z. Wang, Z. Cao, and G. Lin, "iControl3D: An interactive system for controllable 3D scene generation," in ACM MM, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[298] X. Li, Y. Wu, J. Cen, J. Peng, K. Wang, K. Xian, Z. Wang, Z. Cao, 和 G. Lin, "iControl3D: 一种可控的交互式 3D 场景生成系统," 在 ACM MM, 2024.</p></div><p>[299] S. Zhang, Y. Zhang, Q. Zheng, R. Ma, W. Hua, H. Bao, W. Xu, and C. Zou, "3D-SceneDreamer: Text-driven 3D-consistent scene generation," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[299] S. Zhang, Y. Zhang, Q. Zheng, R. Ma, W. Hua, H. Bao, W. Xu, 和 C. Zou, "3D-SceneDreamer: 基于文本的 3D 一致场景生成," 在 CVPR, 2024.</p></div><p>[300] Y. Yang, F. Yin, J. Fan, X. Chen, W. Li, and G. Yu, "Scene123: One prompt to 3D scene generation via video-assisted and consistency-enhanced MAE," arXiv 2408.05477, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[300] Y. Yang, F. Yin, J. Fan, X. Chen, W. Li, 和 G. Yu, "Scene123: 通过视频辅助和一致性增强的 MAE 实现从一个提示到 3D 场景生成," arXiv 2408.05477, 2024.</p></div><p>[301] H. Ouyang, K. Heal, S. Lombardi, and T. Sun, "Text2Immersion: generative immersive scene with 3D Gaussians," arXiv 2312.09242, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[301] H. Ouyang, K. Heal, S. Lombardi, 和 T. Sun, "Text2Immersion: 生成性沉浸式场景与 3D 高斯分布," arXiv 2312.09242, 2023.</p></div><p>[302] H. Yu, H. Duan, C. Herrmann, W. T. Freeman, and J. Wu, "WonderWorld: Interactive 3D scene generation from a single image," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[302] H. Yu, H. Duan, C. Herrmann, W. T. Freeman, 和 J. Wu, "WonderWorld: 从单张图像生成交互式 3D 场景," 在 CVPR, 2025.</p></div><p>[303] J. Shriram, A. Trevithick, L. Liu, and R. Ramamoorthi, "Realm-Dreamer: text-driven 3D scene generation with inpainting and depth diffusion," in 3DV, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[303] J. Shriram, A. Trevithick, L. Liu, 和 R. Ramamoorthi, "Realm-Dreamer: 基于文本的 3D 场景生成与修补和深度扩散," 在 3DV, 2025.</p></div><p>[304] X. Hou, M. Li, D. Yang, J. Chen, Z. Qian, X. Zhao, Y. Jiang, J. Wei, Q. Xu, and L. Zhang, "BloomScene: Lightweight structured 3D gaussian splatting for crossmodal scene generation," in AAAI, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[304] X. Hou, M. Li, D. Yang, J. Chen, Z. Qian, X. Zhao, Y. Jiang, J. Wei, Q. Xu, 和 L. Zhang, "BloomScene: 轻量级结构化 3D 高斯点云生成用于跨模态场景生成," 在 AAAI, 2025.</p></div><p>[305] C. Ni, X. Wang, Z. Zhu, W. Wang, H. Li, G. Zhao, J. Li, W. Qin, G. Huang, and W. Mei, "WonderTurbo: Generating interactive 3D world in 0.72 seconds," arXiv 2504.02261, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[305] C. Ni, X. Wang, Z. Zhu, W. Wang, H. Li, G. Zhao, J. Li, W. Qin, G. Huang, 和 W. Mei, "WonderTurbo: 在 0.72 秒内生成交互式 3D 世界," arXiv 2504.02261, 2025.</p></div><p>[306] Z. Chen, J. Tang, Y. Dong, Z. Cao, F. Hong, Y. Lan, T. Wang, H. Xie, T. Wu, S. Saito, L. Pan, D. Lin, and Z. Liu, "3DTopia-XL: scaling high-quality 3D asset generation via primitive diffusion," in \({CVPR},{2024}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[306] Z. Chen, J. Tang, Y. Dong, Z. Cao, F. Hong, Y. Lan, T. Wang, H. Xie, T. Wu, S. Saito, L. Pan, D. Lin, 和 Z. Liu, "3DTopia-XL: 通过原始扩散扩展高质量 3D 资产生成," 在 \({CVPR},{2024}\) .</p></div><p>[307] J. Xiang, Z. Lv, S. Xu, Y. Deng, R. Wang, B. Zhang, D. Chen, X. Tong, and J. Yang, "Structured 3D latents for scalable and versatile 3D generation," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[307] J. Xiang, Z. Lv, S. Xu, Y. Deng, R. Wang, B. Zhang, D. Chen, X. Tong, 和 J. Yang, "结构化 3D 潜变量用于可扩展和多功能的 3D 生成," 在 CVPR, 2025.</p></div><p>[308] Y. Lan, F. Hong, S. Yang, S. Zhou, X. Meng, B. Dai, X. Pan, and C. C. Loy, "LN3Diff: scalable latent neural fields diffusion for speedy 3D generation," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[308] Y. Lan, F. Hong, S. Yang, S. Zhou, X. Meng, B. Dai, X. Pan, 和 C. C. Loy, "LN3Diff: 可扩展的潜在神经场扩散用于快速 3D 生成," 在 ECCV, 2024.</p></div><p>[309] F. Hong, J. Tang, Z. Cao, M. Shi, T. Wu, Z. Chen, T. Wang, L. Pan, D. Lin, and Z. Liu, "3DTopia: large text-to-3D generation model with hybrid diffusion priors," arXiv 2403.02234, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[309] F. Hong, J. Tang, Z. Cao, M. Shi, T. Wu, Z. Chen, T. Wang, L. Pan, D. Lin, 和 Z. Liu, "3DTopia: 具有混合扩散先验的大型文本到 3D 生成模型," arXiv 2403.02234, 2024.</p></div><p>[310] Y. Lan, S. Zhou, Z. Lyu, F. Hong, S. Yang, B. Dai, X. Pan, and C. C. Loy, "GaussianAnything: interactive point cloud latent diffusion for 3D generation," in ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[310] Y. Lan, S. Zhou, Z. Lyu, F. Hong, S. Yang, B. Dai, X. Pan, 和 C. C. Loy, "GaussianAnything: 交互式点云潜在扩散用于 3D 生成," 在 ICLR, 2025.</p></div><p>[311] P. Engstler, A. Shtedritski, I. Laina, C. Rupprecht, and A. Vedaldi, "SynCity: Training-free generation of 3D worlds," arXiv 2503.16420, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[311] P. Engstler, A. Shtedritski, I. Laina, C. Rupprecht, 和 A. Vedaldi, "SynCity: 无需训练的 3D 世界生成," arXiv 2503.16420, 2025.</p></div><p>[312] L. Shen, X. Li, H. Sun, J. Peng, K. Xian, Z. Cao, and G. Lin, "Make-it-4D: Synthesizing a consistent long-term dynamic scene video from a single image," in ACM MM, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[312] L. Shen, X. Li, H. Sun, J. Peng, K. Xian, Z. Cao, 和 G. Lin, "Make-it-4D: 从单幅图像合成一致的长期动态场景视频," 发表在 ACM MM, 2023.</p></div><p>[313] I.-H. Jin, H. Choo, S.-H. Jeong, H. Park, J. Kim, O. joon Kwon, and K. Kong, "Optimizing 4D Gaussians for dynamic scene video from single landscape images," in ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[313] I.-H. Jin, H. Choo, S.-H. Jeong, H. Park, J. Kim, O. joon Kwon, 和 K. Kong, "优化4D高斯模型以从单一风景图像生成动态场景视频," 在ICLR, 2025.</p></div><p>[314] Y. Lee, Y. Chen, A. Wang, T. Liao, B. Y. Feng, and J. Huang, "VividDream: Generating 3D scene with ambient dynamics," ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[314] Y. Lee, Y. Chen, A. Wang, T. Liao, B. Y. Feng, 和 J. Huang, "VividDream: 生成具有环境动态的3D场景," ICLR, 2025.</p></div><p>[315] H. Feng, Z. Ding, Z. Xia, S. Niklaus, V. F. Abrevaya, M. J. Black, and X. Zhang, "Explorative inbetweening of time and space," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[315] H. Feng, Z. Ding, Z. Xia, S. Niklaus, V. F. Abrevaya, M. J. Black, 和 X. Zhang, "时间与空间的探索性插值," 在ECCV, 2024.</p></div><p>[316] V. Gupta, Y. Man, and Y. Wang, "PaintScene4D: Consistent 4D scene generation from text prompts," arXiv 2412.04471, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[316] V. Gupta, Y. Man, 和 Y. Wang, "PaintScene4D: 从文本提示生成一致的4D场景," arXiv 2412.04471, 2024.</p></div><p>[317] T. Liu, Z. Huang, Z. Chen, G. Wang, S. Hu, I. Shen, H. Sun, Z. Cao, W. Li, and Z. Liu, "Free4D: Tuning-free 4D scene generation with spatial-temporal consistency," arXiv 2503.20785, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[317] T. Liu, Z. Huang, Z. Chen, G. Wang, S. Hu, I. Shen, H. Sun, Z. Cao, W. Li, 和 Z. Liu, "Free4D: 无需调优的4D场景生成与时空一致性," arXiv 2503.20785, 2025.</p></div><p>[318] K. Liu, L. Shao, and S. Lu, "Novel view extrapolation with video diffusion priors," arXiv 2411.14208, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[318] K. Liu, L. Shao, 和 S. Lu, "基于视频扩散先验的新视角外推," arXiv 2411.14208, 2024.</p></div><p>[319] W. Yu, J. Xing, L. Yuan, W. Hu, X. Li, Z. Huang, X. Gao, T. Wong, Y. Shan, and Y. Tian, "ViewCrafter: Taming video diffusion models for high-fidelity novel view synthesis," arXiv 2409.02048, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[319] W. Yu, J. Xing, L. Yuan, W. Hu, X. Li, Z. Huang, X. Gao, T. Wong, Y. Shan, 和 Y. Tian, "ViewCrafter: 驯服视频扩散模型以实现高保真新视角合成," arXiv 2409.02048, 2024.</p></div><p>[320] L. Chen, Z. Zhou, M. Zhao, Y. Wang, G. Zhang, W. Huang, H. Sun, J.-R. Wen, and C. Li, "FlexWorld: Progressively expanding 3D scenes for flexiable-view synthesis," arXiv 2503.13265, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[320] L. Chen, Z. Zhou, M. Zhao, Y. Wang, G. Zhang, W. Huang, H. Sun, J.-R. Wen, 和 C. Li, "FlexWorld: 渐进扩展3D场景以实现灵活视角合成," arXiv 2503.13265, 2025.</p></div><p>[321] H. Feng, Z. Zuo, J.-H. Pan, K.-H. Hui, Y. Shao, Q. Dou, W. Xie, and Z. Liu, "WonderVerse: Extendable 3D scene generation with video generative models," arXiv 2503.09160, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[321] H. Feng, Z. Zuo, J.-H. Pan, K.-H. Hui, Y. Shao, Q. Dou, W. Xie, 和 Z. Liu, "WonderVerse: 可扩展的3D场景生成与视频生成模型," arXiv 2503.09160, 2025.</p></div><p>[322] S. Zhang, J. Li, X. Fei, H. Liu, and Y. Duan, "Scene Splatter: Momentum 3D scene generation from single image with video diffusion model," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[322] S. Zhang, J. Li, X. Fei, H. Liu, 和 Y. Duan, "场景溅射: 从单一图像生成动量3D场景与视频扩散模型," 在CVPR, 2025.</p></div><p>[323] J. Hao, P. Wang, H. Wang, X. Zhang, and Z. Guo, "GaussVideo-Dreamer: 3D scene generation with video diffusion and inconsistency-aware gaussian splatting," arXiv 2504.10001, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[323] J. Hao, P. Wang, H. Wang, X. Zhang, 和 Z. Guo, "GaussVideo-Dreamer: 结合视频扩散与不一致感知高斯溅射的3D场景生成," arXiv 2504.10001, 2025.</p></div><p>[324] H. Liang, J. Cao, V. Goel, G. Qian, S. Korolev, D. Terzopoulos, K. N. Plataniotis, S. Tulyakov, and J. Ren, "Wonderland: Navigating 3D scenes from a single image," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[324] H. Liang, J. Cao, V. Goel, G. Qian, S. Korolev, D. Terzopoulos, K. N. Plataniotis, S. Tulyakov, 和 J. Ren, "Wonderland: 从单一图像导航3D场景," 在CVPR, 2025.</p></div><p>[325] H. Wang, F. Liu, J. Chi, and Y. Duan, "VideoScene: Distilling video diffusion model to generate \(3\mathrm{D}\) scenes in one step," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[325] H. Wang, F. Liu, J. Chi, 和 Y. Duan, "VideoScene: 提炼视频扩散模型以一步生成\(3\mathrm{D}\)场景," 在CVPR, 2025.</p></div><p>[326] R. Wu, R. Gao, B. Poole, A. Trevithick, C. Zheng, J. T. Barron, and A. Holynski, "CAT4D: create anything in 4D with multi-view video diffusion models," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[326] R. Wu, R. Gao, B. Poole, A. Trevithick, C. Zheng, J. T. Barron, 和 A. Holynski, "CAT4D: 使用多视角视频扩散模型在4D中创建任何事物," 在CVPR, 2025.</p></div><p>[327] S. Zhai, Z. Ye, J. Liu, W. Xie, J. Hu, Z. Peng, H. Xue, D. Chen, X. Wang, L. Yang, N. Wang, H. Liu, and G. Zhang, "StarGen: A spatiotemporal autoregression framework with video diffusion model for scalable and controllable scene generation," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[327] S. Zhai, Z. Ye, J. Liu, W. Xie, J. Hu, Z. Peng, H. Xue, D. Chen, X. Wang, L. Yang, N. Wang, H. Liu, 和 G. Zhang, "StarGen: 一个具有视频扩散模型的时空自回归框架，用于可扩展和可控的场景生成," 在CVPR, 2025.</p></div><p>[328] B. Deng, R. Tucker, Z. Li, L. J. Guibas, N. Snavely, and G. Wet-zstein, "Streetscapes: Large-scale consistent street view generation using autoregressive video diffusion," in SIGGRAPH, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[328] B. Deng, R. Tucker, Z. Li, L. J. Guibas, N. Snavely, 和 G. Wet-zstein, "街景：使用自回归视频扩散的大规模一致街景生成," 在 SIGGRAPH, 2024.</p></div><p>[329] Q. Wang, W. Li, C. Mou, X. Cheng, and J. Zhang, "360DVD: Controllable panorama video generation with 360-degree video diffusion model," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[329] Q. Wang, W. Li, C. Mou, X. Cheng, 和 J. Zhang, "360DVD：使用360度视频扩散模型的可控全景视频生成," 在 CVPR, 2024.</p></div><p>[330] J. Tan, S. Yang, T. Wu, J. He, Y. Guo, Z. Liu, and D. Lin, "Imagine360: Immersive 360 video generation from perspective anchor," arXiv 2412.03552, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[330] J. Tan, S. Yang, T. Wu, J. He, Y. Guo, Z. Liu, 和 D. Lin, "Imagine360：从视角锚点生成沉浸式360视频," arXiv 2412.03552, 2024.</p></div><p>[331] T. Lu, T. Shu, A. L. Yuille, D. Khashabi, and J. Chen, "Generative world explorer," ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[331] T. Lu, T. Shu, A. L. Yuille, D. Khashabi, 和 J. Chen, "生成世界探索者," ICLR, 2025.</p></div><p>[332] J. Liu, S. Lin, Y. Li, and M. Yang, "DynamicScaler: Seamless and scalable video generation for panoramic scenes," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[332] J. Liu, S. Lin, Y. Li, 和 M. Yang, "动态缩放器：无缝且可扩展的全景场景视频生成," 在 CVPR, 2025.</p></div><p>[333] E. Alonso, A. Jelley, V. Micheli, A. Kanervisto, A. J. Storkey, T. Pearce, and F. Fleuret, "Diffusion for world modeling: Visual details matter in atari," in NeurIPS, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[333] E. Alonso, A. Jelley, V. Micheli, A. Kanervisto, A. J. Storkey, T. Pearce, 和 F. Fleuret, "用于世界建模的扩散：视觉细节在Atari中很重要," 在 NeurIPS, 2024.</p></div><p>[334] D. Valevski, Y. Leviathan, M. Arar, and S. Fruchter, "Diffusion models are real-time game engines," in ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[334] D. Valevski, Y. Leviathan, M. Arar, 和 S. Fruchter, "扩散模型是实时游戏引擎," 在 ICLR, 2025.</p></div><p>[335] Decart, J. Quevedo, Q. McIntyre, S. Campbell, X. Chen, and R. Wachen, "Oasis: A universe in a transformer," 2024. [Online]. Available: <a href="https://oasis-model.github.io/">https://oasis-model.github.io/</a></p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[335] Decart, J. Quevedo, Q. McIntyre, S. Campbell, X. Chen, 和 R. Wachen, "绿洲：变换器中的宇宙," 2024. [在线]. 可用： <a href="https://oasis-model.github.io/">https://oasis-model.github.io/</a></p></div><p>[336] Z. Xiao, Y. Lan, Y. Zhou, W. Ouyang, S. Yang, Y. Zeng, and X. Pan, "WORLDMEM: Long-term consistent world simulation with memory," arXiv 2504.12369, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[336] Z. Xiao, Y. Lan, Y. Zhou, W. Ouyang, S. Yang, Y. Zeng, 和 X. Pan, "WORLDMEM：具有记忆的长期一致世界模拟," arXiv 2504.12369, 2025.</p></div><p>[337] X. Wang, Z. Zhu, G. Huang, X. Chen, and J. Lu, "DriveDreamer: Towards real-world-driven world models for autonomous driving," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[337] X. Wang, Z. Zhu, G. Huang, X. Chen, 和 J. Lu, "DriveDreamer：面向自主驾驶的现实驱动世界模型," 在 ECCV, 2024.</p></div><p>[338] Y. Wang, J. He, L. Fan, H. Li, Y. Chen, and Z. Zhang, "Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[338] Y. Wang, J. He, L. Fan, H. Li, Y. Chen, 和 Z. Zhang, "驶向未来：基于世界模型的多视角视觉预测与规划用于自主驾驶," 在 CVPR, 2024.</p></div><p>[339] A. Hu, L. Russell, H. Yeo, Z. Murez, G. Fedoseev, A. Kendall, J. Shotton, and G. Corrado, "GAIA-1: A generative world model for autonomous driving," arXiv 2309.17080, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[339] A. Hu, L. Russell, H. Yeo, Z. Murez, G. Fedoseev, A. Kendall, J. Shotton, 和 G. Corrado, "GAIA-1：用于自主驾驶的生成世界模型," arXiv 2309.17080, 2023.</p></div><p>[340] X. Li, Y. Zhang, and X. Ye, "DrivingDiffusion: layout-guided multi-view driving scenarios video generation with latent diffusion model," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[340] X. Li, Y. Zhang, 和 X. Ye, "DrivingDiffusion：基于布局引导的多视角驾驶场景视频生成与潜在扩散模型," 在 ECCV, 2024.</p></div><p>[341] B. Xie, Y. Liu, T. Wang, J. Cao, and X. Zhang, "Glad: A streaming scene generator for autonomous driving," in ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[341] B. Xie, Y. Liu, T. Wang, J. Cao, 和 X. Zhang, "Glad：用于自主驾驶的流媒体场景生成器," 在 ICLR, 2025.</p></div><p>[342] R. Gao, K. Chen, Z. Li, L. Hong, Z. Li, and Q. Xu, "MagicDrive3D: Controllable 3D generation for any-view rendering in street scenes," arXiv 2405.14475, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[342] R. Gao, K. Chen, Z. Li, L. Hong, Z. Li, 和 Q. Xu, "MagicDrive3D：用于街景中任意视角渲染的可控3D生成," arXiv 2405.14475, 2024.</p></div><p>[343] J. Mao, B. Li, B. Ivanovic, Y. Chen, Y. Wang, Y. You, C. Xiao, D. Xu, M. Pavone, and Y. Wang, "DreamDrive: Generative 4D scene modeling from street view images," arXiv 2501.00601, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[343] J. Mao, B. Li, B. Ivanovic, Y. Chen, Y. Wang, Y. You, C. Xiao, D. Xu, M. Pavone, 和 Y. Wang, "DreamDrive：从街景图像生成的生成4D场景建模," arXiv 2501.00601, 2025.</p></div><p>[344] Y. Yan, Z. Xu, H. Lin, H. Jin, H. Guo, Y. Wang, K. Zhan, X. Lang, H. Bao, X. Zhou, and S. Peng, "StreetCrafter: Street view synthesis with controllable video diffusion models," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[344] Y. Yan, Z. Xu, H. Lin, H. Jin, H. Guo, Y. Wang, K. Zhan, X. Lang, H. Bao, X. Zhou, 和 S. Peng, "StreetCrafter: 可控视频扩散模型的街景合成," 在 CVPR, 2025.</p></div><p>[345] J. Mei, Y. Ma, X. Yang, L. Wen, T. Wei, M. Dou, B. Shi, and Y. Liu, "DreamForge: Motion-aware autoregressive video generation for multi-view driving scenes," arXiv 2409.04003, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[345] J. Mei, Y. Ma, X. Yang, L. Wen, T. Wei, M. Dou, B. Shi, 和 Y. Liu, "DreamForge: 运动感知的自回归视频生成用于多视角驾驶场景," arXiv 2409.04003, 2024.</p></div><p>[346] Y. Wen, Y. Zhao, Y. Liu, F. Jia, Y. Wang, C. Luo, C. Zhang, T. Wang, X. Sun, and X. Zhang, "Panacea: Panoramic and controllable video generation for autonomous driving," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[346] Y. Wen, Y. Zhao, Y. Liu, F. Jia, Y. Wang, C. Luo, C. Zhang, T. Wang, X. Sun, 和 X. Zhang, "Panacea: 用于自动驾驶的全景和可控视频生成," 在 CVPR, 2024.</p></div><p>[347] H. Lu, X. Wu, S. Wang, X. Qin, X. Zhang, J. Han, W. Zuo, and J. Tao, "Seeing Beyond Views: Multi-view driving scene video generation with holistic attention," arXiv 2412.03520, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[347] H. Lu, X. Wu, S. Wang, X. Qin, X. Zhang, J. Han, W. Zuo, 和 J. Tao, "超越视角: 具有整体注意力的多视角驾驶场景视频生成," arXiv 2412.03520, 2024.</p></div><p>[348] D. Liang, D. Zhang, X. Zhou, S. Tu, T. Feng, X. Li, Y. Zhang, M. Du, X. Tan, and X. Bai, "Seeing the Future, Perceiving the Future: A unified driving world model for future generation and perception," arXiv 2503.13587, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[348] D. Liang, D. Zhang, X. Zhou, S. Tu, T. Feng, X. Li, Y. Zhang, M. Du, X. Tan, 和 X. Bai, "看见未来，感知未来: 一个统一的驾驶世界模型用于未来生成和感知," arXiv 2503.13587, 2025.</p></div><p>[349] J. Guo, Y. Ding, X. Chen, S. Chen, B. Li, Y. Zou, X. Lyu, F. Tan, X. Qi, Z. Li, and H. Zhao, "DiST-4D: Disentangled spatiotemporal diffusion with metric depth for \(4\mathrm{D}\) driving scene generation," arXiv 2503.15208, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[349] J. Guo, Y. Ding, X. Chen, S. Chen, B. Li, Y. Zou, X. Lyu, F. Tan, X. Qi, Z. Li, 和 H. Zhao, "DiST-4D: 具有度量深度的解耦时空扩散用于\(4\mathrm{D}\)驾驶场景生成," arXiv 2503.15208, 2025.</p></div><p>[350] L. Russell, A. Hu, L. Bertoni, G. Fedoseev, J. Shotton, E. Arani, and G. Corrado, "GAIA-2: A controllable multi-view generative world model for autonomous driving," arXiv 2503.20523, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[350] L. Russell, A. Hu, L. Bertoni, G. Fedoseev, J. Shotton, E. Arani, 和 G. Corrado, "GAIA-2: 一个可控的多视角生成世界模型用于自动驾驶," arXiv 2503.20523, 2025.</p></div><p>[351] X. Yang, L. Wen, Y. Ma, J. Mei, X. Li, T. Wei, W. Lei, D. Fu, P. Cai, M. Dou, B. Shi, L. He, Y. Liu, and Y. Qiao, "DriveArena: A closed-loop generative simulation platform for autonomous driving," arXiv 2408.00415, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[351] X. Yang, L. Wen, Y. Ma, J. Mei, X. Li, T. Wei, W. Lei, D. Fu, P. Cai, M. Dou, B. Shi, L. He, Y. Liu, 和 Y. Qiao, "DriveArena: 一个闭环生成仿真平台用于自动驾驶," arXiv 2408.00415, 2024.</p></div><p>[352] G. Zhao, X. Wang, Z. Zhu, X. Chen, G. Huang, X. Bao, and X. Wang, "DriveDreamer-2: LLM-enhanced world models for diverse driving video generation," in AAAI, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[352] G. Zhao, X. Wang, Z. Zhu, X. Chen, G. Huang, X. Bao, 和 X. Wang, "DriveDreamer-2: LLM增强的世界模型用于多样化驾驶视频生成," 在 AAAI, 2025.</p></div><p>[353] J. Jiang, G. Hong, L. Zhou, E. Ma, H. Hu, X. Zhou, J. Xiang, F. Liu, K. Yu, H. Sun, K. Zhan, P. Jia, and M. Zhang, "DiVE: Dit-based video generation with enhanced control," arXiv 2409.01595, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[353] J. Jiang, G. Hong, L. Zhou, E. Ma, H. Hu, X. Zhou, J. Xiang, F. Liu, K. Yu, H. Sun, K. Zhan, P. Jia, 和 M. Zhang, "DiVE: 基于Dit的视频生成与增强控制," arXiv 2409.01595, 2024.</p></div><p>[354] M. Hassan, S. Stapf, A. Rahimi, P. M. B. Rezende, Y. Haghighi, D. Brüggemann, I. Katircioglu, L. Zhang, X. Chen, S. Saha, M. Cannici, E. Aljalbout, B. Ye, X. Wang, A. Davtyan, M. Salz-mann, D. Scaramuzza, M. Pollefeys, P. Favaro, and A. Alahi, "GEM: A generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[354] M. Hassan, S. Stapf, A. Rahimi, P. M. B. Rezende, Y. Haghighi, D. Brüggemann, I. Katircioglu, L. Zhang, X. Chen, S. Saha, M. Cannici, E. Aljalbout, B. Ye, X. Wang, A. Davtyan, M. Salzmann, D. Scaramuzza, M. Pollefeys, P. Favaro, 和 A. Alahi, "GEM: 一个可泛化的自我视觉多模态世界模型用于细粒度自我运动、物体动态和场景组合控制," 在 CVPR, 2025.</p></div><p>[355] J. Lu, Z. Huang, Z. Yang, J. Zhang, and L. Zhang, "WoVoGen: world volume-aware diffusion for controllable multi-camera driving scene generation," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[355] J. Lu, Z. Huang, Z. Yang, J. Zhang, 和 L. Zhang, "WoVoGen: 世界体积感知扩散用于可控的多摄像头驾驶场景生成," 在 ECCV, 2024.</p></div><p>[356] L. Li, W. Qiu, Y. Cai, X. Yan, Q. Lian, B. Liu, and Y. Chen, "SyntheOcc: Synthesize geometric-controlled street view images through 3D semantic mpis," arXiv 2410.00337, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[356] L. Li, W. Qiu, Y. Cai, X. Yan, Q. Lian, B. Liu, 和 Y. Chen, "SyntheOcc: 通过3D语义mpis合成几何控制的街景图像," arXiv 2410.00337, 2024.</p></div><p>[357] Y. Lu, X. Ren, J. Yang, T. Shen, Z. Wu, J. Gao, Y. Wang, S. Chen, M. Chen, S. Fidler, and J. Huang, "InfiniCube: Unbounded and controllable dynamic 3D driving scene generation with world-guided video models," arXiv 2412.03934, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[357] Y. Lu, X. Ren, J. Yang, T. Shen, Z. Wu, J. Gao, Y. Wang, S. Chen, M. Chen, S. Fidler, 和 J. Huang, "InfiniCube: 使用世界引导视频模型生成无限且可控的动态3D驾驶场景," arXiv 2412.03934, 2024.</p></div><p>[358] B. Li, J. Guo, H. Liu, Y. Zou, Y. Ding, X. Chen, H. Zhu, F. Tan, C. Zhang, T. Wang, S. Zhou, L. Zhang, X. Qi, H. Zhao, M. Yang, W. Zeng, and X. Jin, "UniScene: Unified occupancy-centric driving scene generation," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[358] B. Li, J. Guo, H. Liu, Y. Zou, Y. Ding, X. Chen, H. Zhu, F. Tan, C. Zhang, T. Wang, S. Zhou, L. Zhang, X. Qi, H. Zhao, M. Yang, W. Zeng, 和 X. Jin, "UniScene: 统一的以占用为中心的驾驶场景生成," 在CVPR, 2025.</p></div><p>[359] T. Yan, D. Wu, W. Han, J. Jiang, X. Zhou, K. Zhan, C. Xu, and J. Shen, "DrivingSphere: Building a high-fidelity 4D world for closed-loop simulation," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[359] T. Yan, D. Wu, W. Han, J. Jiang, X. Zhou, K. Zhan, C. Xu, 和 J. Shen, "DrivingSphere: 构建高保真4D世界以进行闭环仿真," 在CVPR, 2025.</p></div><p>[360] Y. Zhang, S. Gong, K. Xiong, X. Ye, X. Tan, F. Wang, J. Huang, H. Wu, and H. Wang, "BEVWorld: A multimodal world model for autonomous driving via unified BEV latent space," arXiv 2407.05679, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[360] Y. Zhang, S. Gong, K. Xiong, X. Ye, X. Tan, F. Wang, J. Huang, H. Wu, 和 H. Wang, "BEVWorld: 通过统一的BEV潜在空间为自动驾驶提供多模态世界模型," arXiv 2407.05679, 2024.</p></div><p>[361] Z. Wu, J. Ni, X. Wang, Y. Guo, R. Chen, L. Lu, J. Dai, and Y. Xiong, "HoloDrive: Holistic 2D-3D multi-modal street scene generation for autonomous driving," arXiv 2412.01407, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[361] Z. Wu, J. Ni, X. Wang, Y. Guo, R. Chen, L. Lu, J. Dai, 和 Y. Xiong, "HoloDrive: 为自动驾驶生成整体的2D-3D多模态街景," arXiv 2412.01407, 2024.</p></div><p>[362] Y. Wu, H. Zhang, T. Lin, L. Huang, S. Luo, R. Wu, C. Qiu, W. Ke, and T. Zhang, "Generating multimodal driving scenes via next-scene prediction," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[362] Y. Wu, H. Zhang, T. Lin, L. Huang, S. Luo, R. Wu, C. Qiu, W. Ke, 和 T. Zhang, "通过下一场景预测生成多模态驾驶场景," 在CVPR, 2025.</p></div><p>[363] W. Wu, X. Guo, W. Tang, T. Huang, C. Wang, D. Chen, and C. Ding, "DriveScape: Towards high-resolution controllable multi-view driving video generation," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[363] W. Wu, X. Guo, W. Tang, T. Huang, C. Wang, D. Chen, 和 C. Ding, "DriveScape: 朝着高分辨率可控的多视角驾驶视频生成迈进," 在CVPR, 2025.</p></div><p>[364] E. Ma, L. Zhou, T. Tang, Z. Zhang, D. Han, J. Jiang, K. Zhan, P. Jia, X. Lang, H. Sun, D. Lin, and K. Yu, "Unleashing generalization of end-to-end autonomous driving with controllable long video generation," arXiv 2406.01349, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[364] E. Ma, L. Zhou, T. Tang, Z. Zhang, D. Han, J. Jiang, K. Zhan, P. Jia, X. Lang, H. Sun, D. Lin, 和 K. Yu, "释放端到端自动驾驶的泛化能力，通过可控的长视频生成," arXiv 2406.01349, 2024.</p></div><p>[365] R. Gao, K. Chen, B. Xiao, L. Hong, Z. Li, and Q. Xu, "MagicDrive-V2: High-resolution long video generation for autonomous driving with adaptive control," arXiv 2411.13807, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[365] R. Gao, K. Chen, B. Xiao, L. Hong, Z. Li, 和 Q. Xu, "MagicDrive-V2: 具有自适应控制的高分辨率长视频生成用于自动驾驶," arXiv 2411.13807, 2024.</p></div><p>[366] X. Hu, W. Yin, M. Jia, J. Deng, X. Guo, Q. Zhang, X. Long, and P. Tan, "DrivingWorld: Constructing world model for autonomous driving via video GPT," arXiv 2412.19505, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[366] X. Hu, W. Yin, M. Jia, J. Deng, X. Guo, Q. Zhang, X. Long, 和 P. Tan, "DrivingWorld: 通过视频GPT构建自动驾驶的世界模型," arXiv 2412.19505, 2024.</p></div><p>[367] J. Ni, Y. Guo, Y. Liu, R. Chen, L. Lu, and Z. Wu, "MaskGWM: A generalizable driving world model with video mask reconstruction," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[367] J. Ni, Y. Guo, Y. Liu, R. Chen, L. Lu, 和 Z. Wu, "MaskGWM: 一种具有视频掩码重建的可泛化驾驶世界模型," 在CVPR, 2025.</p></div><p>[368] J. Xiao, K. A. Ehinger, A. Oliva, and A. Torralba, "Recognizing scene viewpoint using panoramic place representation," in CVPR, 2012.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[368] J. Xiao, K. A. Ehinger, A. Oliva, 和 A. Torralba, "使用全景地点表示识别场景视角," 在CVPR, 2012.</p></div><p>[369] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, "Indoor segmentation and support inference from RGBD images," in ECCV, 2012.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[369] N. Silberman, D. Hoiem, P. Kohli, 和 R. Fergus, "从RGBD图像进行室内分割和支持推断," 在ECCV, 2012.</p></div><p>[370] S. Song, S. P. Lichtenberg, and J. Xiao, "SUN RGB-D: A RGB-D scene understanding benchmark suite," in CVPR, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[370] S. Song, S. P. Lichtenberg, 和 J. Xiao, "SUN RGB-D: 一个RGB-D场景理解基准套件," 在CVPR, 2015.</p></div><p>[371] B. Hua, Q. Pham, D. T. Nguyen, M. Tran, L. Yu, and S. Yeung, "SceneNN: A scene meshes dataset with annotations," in 3DV, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[371] B. Hua, Q. Pham, D. T. Nguyen, M. Tran, L. Yu, 和 S. Yeung, "SceneNN: 一个带注释的场景网格数据集," 在 3DV, 2016.</p></div><p>[372] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, "Joint 2D-3D-Semantic data for indoor scene understanding," arXiv 1702.01105, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[372] I. Armeni, S. Sax, A. R. Zamir, 和 S. Savarese, "用于室内场景理解的联合2D-3D-语义数据," arXiv 1702.01105, 2017.</p></div><p>[373] M. Gardner, K. Sunkavalli, E. Yumer, X. Shen, E. Gambaretto, C. Gagné, and J. Lalonde, "Learning to predict indoor illumination from a single image," ACM TOG, vol. 36, no. 6, pp. 176:1- 176:14, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[373] M. Gardner, K. Sunkavalli, E. Yumer, X. Shen, E. Gambaretto, C. Gagné, 和 J. Lalonde, "从单张图像中学习预测室内照明," ACM TOG, vol. 36, no. 6, pp. 176:1- 176:14, 2017.</p></div><p>[374] A. X. Chang, A. Dai, T. A. Funkhouser, M. Halber, M. Nießner, M. Savva, S. Song, A. Zeng, and Y. Zhang, "Matterport3D: Learning from RGB-D data in indoor environments," in 3DV, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[374] A. X. Chang, A. Dai, T. A. Funkhouser, M. Halber, M. Nießner, M. Savva, S. Song, A. Zeng, 和 Y. Zhang, "Matterport3D: 从RGB-D数据中学习室内环境," 在 3DV, 2017.</p></div><p>[375] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A. Funkhouser, and M. Nießner, "ScanNet: Richly-annotated 3D reconstructions of indoor scenes," in CVPR, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[375] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A. Funkhouser, 和 M. Nießner, "ScanNet: 丰富注释的室内场景3D重建," 在 CVPR, 2017.</p></div><p>[376] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely, "Stereo magnification: learning view synthesis using multiplane images," ACM TOG, vol. 37, no. 4, p. 65, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[376] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, 和 N. Snavely, "立体放大: 使用多平面图像学习视图合成," ACM TOG, vol. 37, no. 4, p. 65, 2018.</p></div><p>[377] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Y. Ren, S. Verma, A. Clarkson, M. Yan,</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[377] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Y. Ren, S. Verma, A. Clarkson, M. Yan,</p></div><p>B. Budge, Y. Yan, X. Pan, J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira, M. Savva, D. Batra, H. M. Strasdat, R. De Nardi, M. Goesele, S. Lovegrove, and R. A. Newcombe, "The replica dataset: A digital replica of indoor spaces," arXiv 1906.05797, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>B. Budge, Y. Yan, X. Pan, J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira, M. Savva, D. Batra, H. M. Strasdat, R. De Nardi, M. Goesele, S. Lovegrove, 和 R. A. Newcombe, "复制数据集: 室内空间的数字复制品," arXiv 1906.05797, 2019.</p></div><p>[378] J. Wald, H. Dhamo, N. Navab, and F. Tombari, "Learning 3D semantic scene graphs from 3D indoor reconstructions," in CVPR, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[378] J. Wald, H. Dhamo, N. Navab, 和 F. Tombari, "从3D室内重建中学习3D语义场景图," 在 CVPR, 2020.</p></div><p>[379] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. M. Turner, E. Undersander, W. Galuba, A. West-bury, A. X. Chang, M. Savva, Y. Zhao, and D. Batra, "Habitat-Matterport 3D dataset (HM3D): 1000 large-scale 3D environments for embodied AI," in NeurIPS, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[379] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. M. Turner, E. Undersander, W. Galuba, A. West-bury, A. X. Chang, M. Savva, Y. Zhao, 和 D. Batra, "Habitat-Matterport 3D数据集 (HM3D): 1000个大规模3D环境用于具身AI," 在 NeurIPS, 2021.</p></div><p>[380] C. Yeshwanth, Y. Liu, M. Nießner, and A. Dai, "ScanNet++: A high-fidelity dataset of \(3\mathrm{D}\) indoor scenes," in ICCV,2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[380] C. Yeshwanth, Y. Liu, M. Nießner, 和 A. Dai, "ScanNet++: 一个高保真度的室内场景数据集," 在 ICCV, 2023.</p></div><p>[381] L. Ling, Y. Sheng, Z. Tu, W. Zhao, C. Xin, K. Wan, L. Yu, Q. Guo, Z. Yu, Y. Lu, X. Li, X. Sun, R. Ashok, A. Mukherjee, H. Kang, X. Kong, G. Hua, T. Zhang, B. Benes, and A. Bera, "DL3DV-10K: A large-scale scene dataset for deep learning-based 3D vision," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[381] L. Ling, Y. Sheng, Z. Tu, W. Zhao, C. Xin, K. Wan, L. Yu, Q. Guo, Z. Yu, Y. Lu, X. Li, X. Sun, R. Ashok, A. Mukherjee, H. Kang, X. Kong, G. Hua, T. Zhang, B. Benes, 和 A. Bera, "DL3DV-10K: 一个大规模场景数据集用于基于深度学习的3D视觉," 在 CVPR, 2024.</p></div><p>[382] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. A. Funkhouser, "Semantic scene completion from a single depth image," in CVPR, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[382] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, 和 T. A. Funkhouser, "从单个深度图像进行语义场景补全," 在 CVPR, 2017.</p></div><p>[383] J. Zheng, J. Zhang, J. Li, R. Tang, S. Gao, and Z. Zhou, "Structured3D: A large photo-realistic dataset for structured 3D modeling," in ECCV, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[383] J. Zheng, J. Zhang, J. Li, R. Tang, S. Gao, 和 Z. Zhou, "Structured3D: 一个用于结构化3D建模的大型照片级真实数据集," 在 ECCV, 2020.</p></div><p>[384] M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. Á. Bautista, N. Paczan, R. Webb, and J. M. Susskind, "Hypersim: A photore-alistic synthetic dataset for holistic indoor scene understanding," in \({ICC}\widehat{V},{2021}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[384] M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. Á. Bautista, N. Paczan, R. Webb, 和 J. M. Susskind, "Hypersim: 一个用于整体室内场景理解的照片级真实合成数据集," 在 \({ICC}\widehat{V},{2021}\) .</p></div><p>[385] H. Fu, B. Cai, L. Gao, L. Zhang, J. Wang, C. Li, Q. Zeng, C. Sun, R. Jia, B. Zhao, and H. Zhang, "3D-FRONT: 3D furnished rooms with layouts and semantics," in ICCV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[385] H. Fu, B. Cai, L. Gao, L. Zhang, J. Wang, C. Li, Q. Zeng, C. Sun, R. Jia, B. Zhao, 和 H. Zhang, "3D-FRONT: 带有布局和语义的3D家具房间," 在 ICCV, 2021.</p></div><p>[386] Y. Hold-Geoffroy, A. Athawale, and J. Lalonde, "Deep sky modeling for single image outdoor lighting estimation," in CVPR, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[386] Y. Hold-Geoffroy, A. Athawale, 和 J. Lalonde, "单图像户外照明估计的深度天空建模," 在 CVPR, 2019.</p></div><p>[387] I. Skorokhodov, G. Sotnikov, and M. Elhoseiny, "Aligning latent and image spaces to connect the unconnectable," in ICCV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[387] I. Skorokhodov, G. Sotnikov, 和 M. Elhoseiny, "对齐潜在空间和图像空间以连接不可连接的事物," 在 ICCV, 2021.</p></div><p>[388] A. Geiger, P. Lenz, and R. Urtasun, "Are we ready for autonomous driving? the KITTI vision benchmark suite," in CVPR, 2012.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[388] A. Geiger, P. Lenz, 和 R. Urtasun, "我们准备好进行自动驾驶了吗？KITTI视觉基准套件," 在 CVPR, 2012.</p></div><p>[389] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, "The Cityscapes dataset for semantic urban scene understanding," in CVPR, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[389] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, 和 B. Schiele, "Cityscapes数据集用于语义城市场景理解," 在 CVPR, 2016.</p></div><p>[390] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stach-niss, and J. Gall, "SemanticKITTI: A dataset for semantic scene understanding of LiDAR sequences," in ICCV, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[390] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stach-niss, 和 J. Gall, "SemanticKITTI: 一个用于LiDAR序列的语义场景理解的数据集," 在 ICCV, 2019.</p></div><p>[391] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam, H. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi, Y. Zhang, J. Shlens, Z. Chen, and D. Anguelov, "Scalabil-ity in perception for autonomous driving: Waymo open dataset," in \({CVPR},{2020}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[391] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam, H. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi, Y. Zhang, J. Shlens, Z. Chen, 和 D. Anguelov, "自动驾驶感知的可扩展性：Waymo开放数据集," 在 \({CVPR},{2020}\) .</p></div><p>[392] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, "nuScenes: A multimodal dataset for autonomous driving," in CVPR, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[392] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, 和 O. Beijbom, "nuScenes: 一个用于自动驾驶的多模态数据集," 在 CVPR, 2020.</p></div><p>[393] Y. Zhou, J. Huang, X. Dai, L. Luo, Z. Chen, and Y. Ma, "Holicity: A city-scale data platform for learning holistic 3D structures," arXiv 2008.03286, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[393] Y. Zhou, J. Huang, X. Dai, L. Luo, Z. Chen, 和 Y. Ma, "Holicity: 一个用于学习整体3D结构的城市规模数据平台," arXiv 2008.03286, 2020.</p></div><p>[394] W. Li, Y. Lai, L. Xu, Y. Xiangli, J. Yu, C. He, G. Xia, and D. Lin, "OmniCity: Omnipotent city understanding with multi-level and multi-view images," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[394] W. Li, Y. Lai, L. Xu, Y. Xiangli, J. Yu, C. He, G. Xia, 和 D. Lin, "OmniCity: 通过多层次和多视角图像实现全能城市理解," 在 CVPR, 2023.</p></div><p>[395] Y. Liao, J. Xie, and A. Geiger, "KITTI-360: A novel dataset and benchmarks for urban scene understanding in 2D and 3D," IEEE TPAMI, vol. 45, no. 3, pp. 3292-3310, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[395] Y. Liao, J. Xie, 和 A. Geiger, "KITTI-360: 一个用于2D和3D城市场景理解的新数据集和基准," IEEE TPAMI, vol. 45, no. 3, pp. 3292-3310, 2023.</p></div><p>[396] Y. Cabon, N. Murray, and M. Humenberger, "Virtual KITTI 2," arXiv 2001.10773, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[396] Y. Cabon, N. Murray, 和 M. Humenberger, "虚拟KITTI 2," arXiv 2001.10773, 2020.</p></div><p>[397] J. Wilson, J. Song, Y. Fu, A. Zhang, A. Capodieci, P. Jayakumar, K. Barton, and M. Ghaffari, "MotionSC: Data set and network for real-time semantic mapping in dynamic environments," IEEE Robotics Autom. Lett., vol. 7, no. 3, pp. 8439-8446, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[397] J. Wilson, J. Song, Y. Fu, A. Zhang, A. Capodieci, P. Jayakumar, K. Barton, 和 M. Ghaffari, "MotionSC: 动态环境中实时语义映射的数据集和网络," IEEE Robotics Autom. Lett., vol. 7, no. 3, pp. 8439-8446, 2022.</p></div><p>[398] J. Wald, A. Avetisyan, N. Navab, F. Tombari, and M. Nießner, "RIO: 3D object instance re-localization in changing indoor environments," in ICCV, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[398] J. Wald, A. Avetisyan, N. Navab, F. Tombari, 和 M. Nießner, "RIO: 在变化的室内环境中进行3D物体实例重新定位," 在 ICCV, 2019.</p></div><p>[399] H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. J. Maybank, and D. Tao, "3D-FUTURE: 3D furniture shape with texture," IJCV, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[399] H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. J. Maybank, 和 D. Tao, "3D-FUTURE: 具有纹理的3D家具形状," IJCV, 2021.</p></div><p>[400] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, "Gans trained by a two time-scale update rule converge to a local nash equilibrium," in NIPS, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[400] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, 和 S. Hochreiter, "通过两种时间尺度更新规则训练的GAN收敛到局部纳什均衡," 在 NIPS, 2017.</p></div><p>[401] M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton, "De-mystifying MMD gans," in ICLR, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[401] M. Binkowski, D. J. Sutherland, M. Arbel, 和 A. Gretton, "揭开MMD GAN的神秘面纱," 在 ICLR, 2018.</p></div><p>[402] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, "Improved techniques for training gans," in NIPS, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[402] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung, A. Radford, 和 X. Chen, "改进GAN训练技术," 在 NIPS, 2016.</p></div><p>[403] S. Morozov, A. Voynov, and A. Babenko, "On self-supervised image representations for GAN evaluation," in ICLR, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[403] S. Morozov, A. Voynov, 和 A. Babenko, "关于自监督图像表示的GAN评估," 在 ICLR, 2021.</p></div><p>[404] G. Stein, J. C. Cresswell, R. Hosseinzadeh, Y. Sui, B. L. Ross, V. Villecroze, Z. Liu, A. L. Caterini, J. E. T. Taylor, and G. Loaiza-Ganem, "Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models," in NeurIPS, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[404] G. Stein, J. C. Cresswell, R. Hosseinzadeh, Y. Sui, B. L. Ross, V. Villecroze, Z. Liu, A. L. Caterini, J. E. T. Taylor, 和 G. Loaiza-Ganem, "揭示生成模型评估指标的缺陷及其对扩散模型的不公正对待," 在 NeurIPS, 2023.</p></div><p>[405] T. Kynkäänniemi, T. Karras, M. Aittala, T. Aila, and J. Lehtinen, "The role of imagenet classes in fréchet inception distance," in ICLR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[405] T. Kynkäänniemi, T. Karras, M. Aittala, T. Aila, 和 J. Lehtinen, "ImageNet类别在Fréchet Inception距离中的作用," 在 ICLR, 2023.</p></div><p>[406] A. Mittal, A. K. Moorthy, and A. C. Bovik, "No-reference image quality assessment in the spatial domain," IEEE TIP, vol. 21, no. 12, pp. 4695-4708, 2012.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[406] A. Mittal, A. K. Moorthy, 和 A. C. Bovik, "空间域中的无参考图像质量评估," IEEE TIP, vol. 21, no. 12, pp. 4695-4708, 2012.</p></div><p>[407] A. Mittal, R. Soundararajan, and A. C. Bovik, "Making a "completely blind" image quality analyzer," IEEE Signal Process. Lett., vol. 20, no. 3, pp. 209-212, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[407] A. Mittal, R. Soundararajan, 和 A. C. Bovik, "制作一个'完全盲目的'图像质量分析器," IEEE Signal Process. Lett., vol. 20, no. 3, pp. 209-212, 2013.</p></div><p>[408] J. Wang, K. C. K. Chan, and C. C. Loy, "Exploring CLIP for assessing the look and feel of images," in AAAI, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[408] J. Wang, K. C. K. Chan, 和 C. C. Loy, "探索 CLIP 以评估图像的外观和感觉," 在 AAAI, 2023.</p></div><p>[409] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. J. Guibas, "Learning representations and generative models for 3D point clouds," in ICML, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[409] P. Achlioptas, O. Diamanti, I. Mitliagkas, 和 L. J. Guibas, "学习 3D 点云的表示和生成模型," 在 ICML, 2018.</p></div><p>[410] D. Lopez-Paz and M. Oquab, "Revisiting classifier two-sample tests," in ICLR, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[410] D. Lopez-Paz 和 M. Oquab, "重新审视分类器两样本检验," 在 ICLR, 2017.</p></div><p>[411] D. Eigen, C. Puhrsch, and R. Fergus, "Depth map prediction from a single image using a multi-scale deep network," in NIPS, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[411] D. Eigen, C. Puhrsch, 和 R. Fergus, "使用多尺度深度网络从单幅图像预测深度图," 在 NIPS, 2014.</p></div><p>[412] W. Lai, J. Huang, O. Wang, E. Shechtman, E. Yumer, and M. Yang, "Learning blind video temporal consistency," in ECCV,2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[412] W. Lai, J. Huang, O. Wang, E. Shechtman, E. Yumer, 和 M. Yang, "学习盲视频时间一致性," 在 ECCV, 2018.</p></div><p>[413] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly, "FVD: A new metric for video generation," in ICLR, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[413] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski, 和 S. Gelly, "FVD: 一种用于视频生成的新指标," 在 ICLR, 2019.</p></div><p>[414] J. Liu, Y. Qu, Q. Yan, X. Zeng, L. Wang, and R. Liao, "Fréchet video motion distance: A metric for evaluating motion consistency in videos," arXiv 2407.16124, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[414] J. Liu, Y. Qu, Q. Yan, X. Zeng, L. Wang, 和 R. Liao, "Fréchet 视频运动距离: 一种评估视频运动一致性的指标," arXiv 2407.16124, 2024.</p></div><p>[415] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi, "CLIP-Score: A reference-free evaluation metric for image captioning," in EMNLP, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[415] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, 和 Y. Choi, "CLIP-Score: 一种无参考的图像字幕评估指标," 在 EMNLP, 2021.</p></div><p>[416] H. Wu, Z. Zhang, W. Zhang, C. Chen, L. Liao, C. Li, Y. Gao, A. Wang, E. Zhang, W. Sun, Q. Yan, X. Min, G. Zhai, and W. Lin, "Q-Align: Teaching LMMs for visual scoring via discrete text-defined levels," in ICML, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[416] H. Wu, Z. Zhang, W. Zhang, C. Chen, L. Liao, C. Li, Y. Gao, A. Wang, E. Zhang, W. Sun, Q. Yan, X. Min, G. Zhai, 和 W. Lin, "Q-Align: 通过离散文本定义的级别教导 LMMs 进行视觉评分," 在 ICML, 2024.</p></div><p>[417] X. He, D. Jiang, G. Zhang, M. Ku, A. Soni, S. Siu, H. Chen, A. Chandra, Z. Jiang, A. Arulraj, K. Wang, Q. D. Do, Y. Ni, B. Lyu, Y. Narsupalli, R. Fan, Z. Lyu, Y. Lin, and W. Chen, "VideoScore: Building automatic metrics to simulate fine-grained human feedback for video generation," arXiv 2406.15252, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[417] X. He, D. Jiang, G. Zhang, M. Ku, A. Soni, S. Siu, H. Chen, A. Chandra, Z. Jiang, A. Arulraj, K. Wang, Q. D. Do, Y. Ni, B. Lyu, Y. Narsupalli, R. Fan, Z. Lyu, Y. Lin, 和 W. Chen, "VideoScore: 构建自动指标以模拟细粒度人类反馈用于视频生成," arXiv 2406.15252, 2024.</p></div><p>[418] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit, Y. Wang, X. Chen, L. Wang, D. Lin, Y. Qiao, and Z. Liu, "VBench: Comprehensive benchmark suite for video generative models," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[418] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit, Y. Wang, X. Chen, L. Wang, D. Lin, Y. Qiao, 和 Z. Liu, "VBench: 视频生成模型的综合基准套件," 在 CVPR, 2024.</p></div><p>[419] Z. Huang, F. Zhang, X. Xu, Y. He, J. Yu, Z. Dong, Q. Ma, N. Chanpaisit, C. Si, Y. Jiang, Y. Wang, X. Chen, Y. Chen, L. Wang, D. Lin, Y. Qiao, and Z. Liu, "VBench++: Comprehensive and versatile benchmark suite for video generative models," arXiv 2411.13503, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[419] Z. Huang, F. Zhang, X. Xu, Y. He, J. Yu, Z. Dong, Q. Ma, N. Chanpaisit, C. Si, Y. Jiang, Y. Wang, X. Chen, Y. Chen, L. Wang, D. Lin, Y. Qiao, 和 Z. Liu, "VBench++: 视频生成模型的综合和多功能基准套件," arXiv 2411.13503, 2024.</p></div><p>[420] D. Zheng, Z. Huang, H. Liu, K. Zou, Y. He, F. Zhang, Y. Zhang, J. He, W.-S. Zheng, Y. Qiao, and Z. Liu, "VBench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness," arXiv 2503.21755, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[420] D. Zheng, Z. Huang, H. Liu, K. Zou, Y. He, F. Zhang, Y. Zhang, J. He, W.-S. Zheng, Y. Qiao, 和 Z. Liu, "VBench-2.0: 推进视频生成基准套件以实现内在真实性," arXiv 2503.21755, 2025.</p></div><p>[421] H. Duan, H.-X. Yu, S. Chen, L. Fei-Fei, and J. Wu, "WorldScore: A unified evaluation benchmark for world generation," arXiv 2504.00983, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[421] H. Duan, H.-X. Yu, S. Chen, L. Fei-Fei, 和 J. Wu, "WorldScore: 世界生成的统一评估基准," arXiv 2504.00983, 2025.</p></div><p>[422] L. Höllein, J. Johnson, and M. Nießner, "StyleMesh: Style transfer for indoor 3D scene reconstructions," in CVPR, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[422] L. Höllein, J. Johnson, 和 M. Nießner, "StyleMesh: 室内 3D 场景重建的风格迁移," 在 CVPR, 2022.</p></div><p>[423] L. Song, L. Cao, H. Xu, K. Kang, F. Tang, J. Yuan, and Z. Yang, "RoomDreamer: Text-driven 3D indoor scene synthesis with coherent geometry and texture," in ACM MM, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[423] L. Song, L. Cao, H. Xu, K. Kang, F. Tang, J. Yuan, 和 Z. Yang, "RoomDreamer: 基于文本的3D室内场景合成，具有一致的几何形状和纹理," 在ACM MM, 2023.</p></div><p>[424] Y. Chen, H. Huang, T. Vu, K. Shum, and S. Yeung, "StyleCity: Large-scale 3D urban scenes stylization," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[424] Y. Chen, H. Huang, T. Vu, K. Shum, 和 S. Yeung, "StyleCity: 大规模3D城市场景风格化," 在ECCV, 2024.</p></div><p>[425] D. Z. Chen, H. Li, H. Lee, S. Tulyakov, and M. Nießner, "SceneTex: High-quality texture synthesis for indoor scenes via diffusion priors," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[425] D. Z. Chen, H. Li, H. Lee, S. Tulyakov, 和 M. Nießner, "SceneTex: 通过扩散先验实现高质量室内场景纹理合成," 在CVPR, 2024.</p></div><p>[426] Z. Huang, W. Yu, X. Cheng, C. Zhao, Y. Ge, M. Guo, L. Yuan, and Y. Tian, "Roompainter: View-integrated diffusion for consistent indoor scene texturing," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[426] Z. Huang, W. Yu, X. Cheng, C. Zhao, Y. Ge, M. Guo, L. Yuan, 和 Y. Tian, "Roompainter: 视图集成扩散用于一致的室内场景纹理," 在CVPR, 2025.</p></div><p>[427] I. Hwang, H. Kim, and Y. M. Kim, "Text2Scene: Text-driven indoor scene stylization with part-aware details," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[427] I. Hwang, H. Kim, 和 Y. M. Kim, "Text2Scene: 基于文本的室内场景风格化，具有部件感知细节," 在CVPR, 2023.</p></div><p>[428] Q. Wang, R. Lu, X. Xu, J. Wang, M. Y. Wang, B. Dai, G. Zeng, and D. Xu, "RoomTex: Texturing compositional indoor scenes via iterative inpainting," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[428] Q. Wang, R. Lu, X. Xu, J. Wang, M. Y. Wang, B. Dai, G. Zeng, 和 D. Xu, "RoomTex: 通过迭代修复实现组合室内场景的纹理," 在ECCV, 2024.</p></div><p>[429] B. Yang, W. Dong, L. Ma, W. Hu, X. Liu, Z. Cui, and Y. Ma, "DreamSpace: Dreaming your room space with text-driven panoramic texture propagation," in VR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[429] B. Yang, W. Dong, L. Ma, W. Hu, X. Liu, Z. Cui, 和 Y. Ma, "DreamSpace: 通过基于文本的全景纹理传播梦幻你的房间空间," 在VR, 2024.</p></div><p>[430] M. Yang, J. Guo, Y. Chen, L. Chen, P. Li, Z. Cheng, X. Zhang, and H. Huang, "InstanceTex: Instance-level controllable texture synthesis for 3D scenes via diffusion priors," in SIGGRAPH Asia, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[430] M. Yang, J. Guo, Y. Chen, L. Chen, P. Li, Z. Cheng, X. Zhang, 和 H. Huang, "InstanceTex: 通过扩散先验实现3D场景的实例级可控纹理合成," 在SIGGRAPH Asia, 2024.</p></div><p>[431] Q. A. Wei, S. Ding, J. J. Park, R. Sajnani, A. Poulenard, S. Sridhar, and L. J. Guibas, "LEGO-Net: Learning regular rearrangements of objects in rooms," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[431] Q. A. Wei, S. Ding, J. J. Park, R. Sajnani, A. Poulenard, S. Sridhar, 和 L. J. Guibas, "LEGO-Net: 学习房间中物体的规则重排," 在CVPR, 2023.</p></div><p>[432] A. Murali, A. Mousavian, C. Eppner, A. Fishman, and D. Fox, "CabiNet: Scaling neural collision detection for object rearrangement with procedural scene generation," in ICRA, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[432] A. Murali, A. Mousavian, C. Eppner, A. Fishman, 和 D. Fox, "CabiNet: 通过程序化场景生成扩展物体重排的神经碰撞检测," 在ICRA, 2023.</p></div><p>[433] S. Zhang, J. Huang, L. Yue, J. Zhang, J. Liu, Y. Lai, and S. Zhang, "SceneExpander: Real-time scene synthesis for interactive floor plan editing," in ACM MM, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[433] S. Zhang, J. Huang, L. Yue, J. Zhang, J. Liu, Y. Lai, 和 S. Zhang, "SceneExpander: 实时场景合成用于交互式平面图编辑," 在ACM MM, 2024.</p></div><p>[434] S. Zhang, H. Tam, Y. Li, K. Ren, H. Fu, and S. Zhang, "SceneDirec-tor: Interactive scene synthesis by simultaneously editing multiple objects in real-time," IEEE TVCG, vol. 30, no. 8, pp. 4558-4569, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[434] S. Zhang, H. Tam, Y. Li, K. Ren, H. Fu, 和 S. Zhang, "SceneDirector: 通过实时同时编辑多个对象实现交互式场景合成," IEEE TVCG, vol. 30, no. 8, pp. 4558-4569, 2024.</p></div><p>[435] Z. Wu, Y. Rubanova, R. Kabra, D. A. Hudson, I. Gilitschenski, Y. Aytar, S. van Steenkiste, K. R. Allen, and T. Kipf, "Neural Assets: 3D-aware multi-object scene synthesis with image diffusion models," in NeurIPS, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[435] Z. Wu, Y. Rubanova, R. Kabra, D. A. Hudson, I. Gilitschenski, Y. Aytar, S. van Steenkiste, K. R. Allen, 和 T. Kipf, "Neural Assets: 具有图像扩散模型的3D感知多物体场景合成," 在NeurIPS, 2024.</p></div><p>[436] L. Li, Q. Lian, L. Wang, N. Ma, and Y. Chen, "Lift3D: Synthesize 3D training data by lifting 2D GAN to 3D generative radiance field," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[436] L. Li, Q. Lian, L. Wang, N. Ma, 和 Y. Chen, "Lift3D: 通过将2D GAN提升到3D生成辐射场合成3D训练数据," 在CVPR, 2023.</p></div><p>[437] S. Huang, Z. Wang, P. Li, B. Jia, T. Liu, Y. Zhu, W. Liang, and S. Zhu, "Diffusion-based generation, optimization, and planning in 3D scenes," in CVPR, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[437] S. Huang, Z. Wang, P. Li, B. Jia, T. Liu, Y. Zhu, W. Liang, 和 S. Zhu, "基于扩散的3D场景生成、优化和规划," 在CVPR, 2023.</p></div><p>[438] M. Hassan, Y. Guo, T. Wang, M. J. Black, S. Fidler, and X. B. Peng, "Synthesizing physical character-scene interactions," in SIGGRAPH, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[438] M. Hassan, Y. Guo, T. Wang, M. J. Black, S. Fidler, 和 X. B. Peng, "合成物理角色与场景的交互," 在SIGGRAPH, 2023.</p></div><p>[439] L. Pan, Z. Yang, Z. Dou, W. Wang, B. Huang, B. Dai, T. Komura, and J. Wang, "Tokenhsi: Unified synthesis of physical human-scene interactions through task tokenization," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[439] L. Pan, Z. Yang, Z. Dou, W. Wang, B. Huang, B. Dai, T. Komura, 和 J. Wang, "Tokenhsi: 通过任务标记化统一合成物理人类场景交互," 在 CVPR, 2025.</p></div><p>[440] J. Wang, Y. Rong, J. Liu, S. Yan, D. Lin, and B. Dai, "Towards diverse and natural scene-aware \(3\mathrm{D}\) human motion synthesis," in CVPR, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[440] J. Wang, Y. Rong, J. Liu, S. Yan, D. Lin, 和 B. Dai, "朝着多样化和自然场景感知的人类运动合成," 在 CVPR, 2022.</p></div><p>[441] K. Zhao, S. Wang, Y. Zhang, T. Beeler, and S. Tang, "Compositional human-scene interaction synthesis with semantic control," in \({ECCV},{2022}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[441] K. Zhao, S. Wang, Y. Zhang, T. Beeler, 和 S. Tang, "具有语义控制的组合人类场景交互合成," 在 \({ECCV},{2022}\) .</p></div><p>[442] F. Hong, V. Guzov, H. J. Kim, Y. Ye, R. A. Newcombe, Z. Liu, and L. Ma, "EgoLM: multi-modal language model of egocentric motions," arXiv 2409.18127, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[442] F. Hong, V. Guzov, H. J. Kim, Y. Ye, R. A. Newcombe, Z. Liu, 和 L. Ma, "EgoLM: 自我中心运动的多模态语言模型," arXiv 2409.18127, 2024.</p></div><p>[443] M. Hassan, V. Choutas, D. Tzionas, and M. J. Black, "Resolving 3D human pose ambiguities with 3D scene constraints," in ICCV, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[443] M. Hassan, V. Choutas, D. Tzionas, 和 M. J. Black, "通过 3D 场景约束解决 3D 人体姿态歧义," 在 ICCV, 2019.</p></div><p>[444] Z. Wang, Y. Chen, T. Liu, Y. Zhu, W. Liang, and S. Huang, "HUMANISE: language-conditioned human motion generation in 3D scenes," in NeurIPS, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[444] Z. Wang, Y. Chen, T. Liu, Y. Zhu, W. Liang, 和 S. Huang, "HUMANISE: 在 3D 场景中基于语言的人类运动生成," 在 NeurIPS, 2022.</p></div><p>[445] L. Ma, Y. Ye, F. Hong, V. Guzov, Y. Jiang, R. Postyeni, L. Pesqueira, A. Gamino, V. Baiyya, H. J. Kim, K. Bailey, D. S. Fosas, C. K. Liu, Z. Liu, J. Engel, R. De Nardi, and R. A. Newcombe, "Nymeria: A massive collection of multimodal egocentric daily motion in the wild," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[445] L. Ma, Y. Ye, F. Hong, V. Guzov, Y. Jiang, R. Postyeni, L. Pesqueira, A. Gamino, V. Baiyya, H. J. Kim, K. Bailey, D. S. Fosas, C. K. Liu, Z. Liu, J. Engel, R. De Nardi, 和 R. A. Newcombe, "Nymeria: 一大批多模态自我中心日常运动数据集," 在 ECCV, 2024.</p></div><p>[446] K. Zhao, Y. Zhang, S. Wang, T. Beeler, and S. Tang, "Synthesizing diverse human motions in 3D indoor scenes," in ICCV, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[446] K. Zhao, Y. Zhang, S. Wang, T. Beeler, 和 S. Tang, "在 3D 室内场景中合成多样化的人类运动," 在 ICCV, 2023.</p></div><p>[447] L. Pan, J. Wang, B. Huang, J. Zhang, H. Wang, X. Tang, and Y. Wang, "Synthesizing physically plausible human motions in 3D scenes," in \({3DV},{2024}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[447] L. Pan, J. Wang, B. Huang, J. Zhang, H. Wang, X. Tang, 和 Y. Wang, "在 3D 场景中合成物理上合理的人类运动," 在 \({3DV},{2024}\) .</p></div><p>[448] W. Wang, L. Pan, Z. Dou, J. Mei, Z. Liao, Y. Lou, Y. Wu, L. Yang, J. Wang, and T. Komura, "SIMS: Simulating stylized human-scene interactions with retrieval-augmented script generation," arXiv 2411.19921, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[448] W. Wang, L. Pan, Z. Dou, J. Mei, Z. Liao, Y. Lou, Y. Wu, L. Yang, J. Wang, 和 T. Komura, "SIMS: 通过检索增强脚本生成模拟风格化的人类场景交互," arXiv 2411.19921, 2025.</p></div><p>[449] L. Li and A. Dai, "GenZI: Zero-shot 3D human-scene interaction generation," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[449] L. Li 和 A. Dai, "GenZI: 零样本 3D 人类场景交互生成," 在 CVPR, 2024.</p></div><p>[450] A. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. M. Turner, N. Maestre, M. Mukadam, D. S. Chaplot, O. Maksymets, A. Gokaslan, V. Vondrus, S. Dharur, F. Meier, W. Galuba, A. X. Chang, Z. Kira, V. Koltun, J. Malik, M. Savva, and D. Batra, "Habitat 2.0: Training home assistants to rearrange their habitat," in NeurIPS, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[450] A. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. M. Turner, N. Maestre, M. Mukadam, D. S. Chaplot, O. Maksymets, A. Gokaslan, V. Vondrus, S. Dharur, F. Meier, W. Galuba, A. X. Chang, Z. Kira, V. Koltun, J. Malik, M. Savva, 和 D. Batra, "Habitat 2.0: 训练家庭助手重新安排其栖息地," 在 NeurIPS, 2021.</p></div><p>[451] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi, "AI2-THOR: an interactive 3D environment for visual AI," arXiv 1712.05474, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[451] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, 和 A. Farhadi, "AI2-THOR: 一个用于视觉 AI 的交互式 3D 环境," arXiv 1712.05474, 2017.</p></div><p>[452] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. Martín-Martín, C. Wang, G. Levine, M. Lingelbach, J. Sun, M. Anvari, M. Hwang, M. Sharma, A. Aydin, D. Bansal, S. Hunter, K. Kim, A. Lou, C. R. Matthews, I. Villa-Renteria, J. H. Tang, C. Tang, F. Xia, S. Savarese, H. Gweon, C. K. Liu, J. Wu, and L. Fei-Fei, "BEHAVIOR-1K: A benchmark for embodied AI with 1,000 everyday activities and realistic simulation," in CoRL, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[452] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. Martín-Martín, C. Wang, G. Levine, M. Lingelbach, J. Sun, M. Anvari, M. Hwang, M. Sharma, A. Aydin, D. Bansal, S. Hunter, K. Kim, A. Lou, C. R. Matthews, I. Villa-Renteria, J. H. Tang, C. Tang, F. Xia, S. Savarese, H. Gweon, C. K. Liu, J. Wu, 和 L. Fei-Fei, "BEHAVIOR-1K: 一个包含1,000个日常活动和真实模拟的具身人工智能基准," 在 CoRL, 2022.</p></div><p>[453] P. Ren, M. Li, Z. Luo, X. Song, Z. Chen, W. Liufu, Y. Yang, H. Zheng, R. Xu, Z. Huang, T. Ding, L. Xie, K. Zhang, C. Fu, Y. Liu, L. Lin, F. Zheng, and X. Liang, "InfiniteWorld: A unified scalable simulation framework for general visual-language robot interaction," arXiv 2412.05789, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[453] P. Ren, M. Li, Z. Luo, X. Song, Z. Chen, W. Liufu, Y. Yang, H. Zheng, R. Xu, Z. Huang, T. Ding, L. Xie, K. Zhang, C. Fu, Y. Liu, L. Lin, F. Zheng, 和 X. Liang, "InfiniteWorld: 一个统一的可扩展模拟框架，用于一般视觉-语言机器人交互," arXiv 2412.05789, 2024.</p></div><p>[454] Y. Wang, X. Qiu, J. Liu, Z. Chen, J. Cai, Y. Wang, T. J. Wang, Z. Xian, and C. Gan, "Architect: Generating vivid and interactive 3D scenes with hierarchical 2D inpainting," in NeurIPS, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[454] Y. Wang, X. Qiu, J. Liu, Z. Chen, J. Cai, Y. Wang, T. J. Wang, Z. Xian, 和 C. Gan, "Architect: 通过分层2D修复生成生动且互动的3D场景," 在 NeurIPS, 2024.</p></div><p>[455] W. Wu, H. He, C. Zhang, J. He, S. Z. Zhao, R. Gong, Q. Li, and B. Zhou, "Towards autonomous micromobility through scalable urban simulation," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[455] W. Wu, H. He, C. Zhang, J. He, S. Z. Zhao, R. Gong, Q. Li, 和 B. Zhou, "通过可扩展城市模拟实现自主微移动," 在 CVPR, 2025.</p></div><p>[456] C. Gao, B. Zhao, W. Zhang, J. Mao, J. Zhang, Z. Zheng, F. Man, J. Fang, Z. Zhou, J. Cui, X. Chen, and Y. Li, "EmbodiedCity: A benchmark platform for embodied agent in real-world city environment," arXiv 2410.09604, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[456] C. Gao, B. Zhao, W. Zhang, J. Mao, J. Zhang, Z. Zheng, F. Man, J. Fang, Z. Zhou, J. Cui, X. Chen, 和 Y. Li, "EmbodiedCity: 一个用于真实城市环境中具身代理的基准平台," arXiv 2410.09604, 2024.</p></div><p>[457] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mot-taghi, L. Zettlemoyer, and D. Fox, "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks," in CVPR, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[457] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mot-taghi, L. Zettlemoyer, 和 D. Fox, "ALFRED: 一个用于解释日常任务的基础指令的基准," 在 CVPR, 2020.</p></div><p>[458] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, "CALVIN: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks," RA-L, vol. 7, no. 3, pp. 7327- 7334, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[458] O. Mees, L. Hermann, E. Rosete-Beas, 和 W. Burgard, "CALVIN: 一个用于长期机器人操作任务的语言条件策略学习基准," RA-L, vol. 7, no. 3, pp. 7327-7334, 2022.</p></div><p>[459] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone, "LIBERO: benchmarking knowledge transfer for lifelong robot learning," in NeurIPS, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[459] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, 和 P. Stone, "LIBERO: 终身机器人学习的知识转移基准," 在 NeurIPS, 2023.</p></div><p>[460] R. Gong, J. Huang, Y. Zhao, H. Geng, X. Gao, Q. Wu, W. Ai, Z. Zhou, D. Terzopoulos, S. Zhu, B. Jia, and S. Huang, "ARNOLD: A benchmark for language-grounded task learning with continuous states in realistic 3D scenes," in ICCV, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[460] R. Gong, J. Huang, Y. Zhao, H. Geng, X. Gao, Q. Wu, W. Ai, Z. Zhou, D. Terzopoulos, S. Zhu, B. Jia, 和 S. Huang, "ARNOLD: 一个用于在真实3D场景中进行语言基础任务学习的基准," 在 ICCV, 2023.</p></div><p>[461] X. Li, K. Hsu, J. Gu, O. Mees, K. Pertsch, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani, S. Levine, J. Wu, C. Finn, H. Su, Q. Vuong, and T. Xiao, "Evaluating real-world robot manipulation policies in simulation," in CoRL, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[461] X. Li, K. Hsu, J. Gu, O. Mees, K. Pertsch, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani, S. Levine, J. Wu, C. Finn, H. Su, Q. Vuong, 和 T. Xiao, "在模拟中评估真实世界的机器人操作策略," 在 CoRL, 2024.</p></div><p>[462] S. Nasiriany, A. Maddukuri, L. Zhang, A. Parikh, A. Lo, A. Joshi, A. Mandlekar, and Y. Zhu, "RoboCasa: Large-scale simulation of everyday tasks for generalist robots," arXiv 2406.02523, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[462] S. Nasiriany, A. Maddukuri, L. Zhang, A. Parikh, A. Lo, A. Joshi, A. Mandlekar, 和 Y. Zhu, "RoboCasa: 大规模日常任务模拟用于通用机器人," arXiv 2406.02523, 2024.</p></div><p>[463] Y. Wang, Z. Xian, F. Chen, T. Wang, Y. Wang, K. Fragkiadaki, Z. Erickson, D. Held, and C. Gan, "RoboGen: Towards unleashing infinite data for automated robot learning via generative simulation," in ICML, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[463] Y. Wang, Z. Xian, F. Chen, T. Wang, Y. Wang, K. Fragkiadaki, Z. Erickson, D. Held, 和 C. Gan, "RoboGen: 朝着通过生成模拟释放无限数据以实现自动化机器人学习," 在 ICML, 2024.</p></div><p>[464] H. Geng, F. Wang, S. Wei, Y. Li, B. Wang, B. An, C. T. Cheng, H. Lou, P. Li, Y.-J. Wang, Y. Liang, D. Goetting, C. Xu, H. Chen, Y. Qian, Y. Geng, J. Mao, W. Wan, M. Zhang, J. Lyu, S. Zhao, J. Zhang, J. Zhang, C. Zhao, H. Lu, Y. Ding, R. Gong, Y. Wang, Y. Kuang, R. Wu, B. Jia, C. Sferrazza, H. Dong, S. Huang, Y. Wang, J. Malik, and P. Abbeel, "RoboVerse: Towards a unified platform, dataset and benchmark for scalable and generalizable robot learning," arXiv 2504.18904, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[464] H. Geng, F. Wang, S. Wei, Y. Li, B. Wang, B. An, C. T. Cheng, H. Lou, P. Li, Y.-J. Wang, Y. Liang, D. Goetting, C. Xu, H. Chen, Y. Qian, Y. Geng, J. Mao, W. Wan, M. Zhang, J. Lyu, S. Zhao, J. Zhang, J. Zhang, C. Zhao, H. Lu, Y. Ding, R. Gong, Y. Wang, Y. Kuang, R. Wu, B. Jia, C. Sferrazza, H. Dong, S. Huang, Y. Wang, J. Malik, 和 P. Abbeel, "RoboVerse: 朝着一个统一的平台、数据集和基准，以实现可扩展和可泛化的机器人学习," arXiv 2504.18904, 2025.</p></div><p>[465] W. Liang, S. Wang, H. Wang, O. Bastani, D. Jayaraman, and Y. J. Ma, "Eurekaverse: Environment curriculum generation via large language models," arXiv 2411.01775, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[465] W. Liang, S. Wang, H. Wang, O. Bastani, D. Jayaraman, 和 Y. J. Ma, "Eurekaverse: 通过大型语言模型生成环境课程," arXiv 2411.01775, 2024.</p></div><p>[466] Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schuurmans, and P. Abbeel, "Learning universal policies via text-guided video generation," in NeurIPS, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[466] Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schuurmans, 和 P. Abbeel, "通过文本引导的视频生成学习通用策略," 在 NeurIPS, 2023.</p></div><p>[467] A. Ajay, S. Han, Y. Du, S. Li, A. Gupta, T. S. Jaakkola, J. B. Tenenbaum, L. P. Kaelbling, A. Srivastava, and P. Agrawal, "Compositional foundation models for hierarchical planning," in NeurIPS, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[467] A. Ajay, S. Han, Y. Du, S. Li, A. Gupta, T. S. Jaakkola, J. B. Tenenbaum, L. P. Kaelbling, A. Srivastava, 和 P. Agrawal, "用于分层规划的组合基础模型," 在 NeurIPS, 2023.</p></div><p>[468] J. Cen, C. Wu, X. Liu, S. Yin, Y. Pei, J. Yang, Q. Chen, N. Duan, and J. Zhang, "Using left and right brains together: Towards vision and language planning," in ICML, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[468] J. Cen, C. Wu, X. Liu, S. Yin, Y. Pei, J. Yang, Q. Chen, N. Duan, 和 J. Zhang, "左右脑协同工作: 朝着视觉和语言规划," 在 ICML, 2024.</p></div><p>[469] Q. Bu, J. Zeng, L. Chen, Y. Yang, G. Zhou, J. Yan, P. Luo, H. Cui, Y. Ma, and H. Li, "Closed-loop visuomotor control with generative expectation for robotic manipulation," in NeurIPS, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[469] Q. Bu, J. Zeng, L. Chen, Y. Yang, G. Zhou, J. Yan, P. Luo, H. Cui, Y. Ma, 和 H. Li, "带有生成期望的闭环视觉运动控制用于机器人操作," 在 NeurIPS, 2024.</p></div><p>[470] H. Wu, Y. Jing, C. Cheang, G. Chen, J. Xu, X. Li, M. Liu, H. Li, and T. Kong, "Unleashing large-scale video generative pre-training for visual robot manipulation," in ICLR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[470] H. Wu, Y. Jing, C. Cheang, G. Chen, J. Xu, X. Li, M. Liu, H. Li, 和 T. Kong, "释放大规模视频生成预训练以实现视觉机器人操作," 在 ICLR, 2024.</p></div><p>[471] C. Cheang, G. Chen, Y. Jing, T. Kong, H. Li, Y. Li, Y. Liu, H. Wu, J. Xu, Y. Yang, H. Zhang, and M. Zhu, "GR-2: A generative video-language-action model with web-scale knowledge for robot manipulation," arXiv 2410.06158, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[471] C. Cheang, G. Chen, Y. Jing, T. Kong, H. Li, Y. Li, Y. Liu, H. Wu, J. Xu, Y. Yang, H. Zhang, 和 M. Zhu, "GR-2: 一种具有网络规模知识的生成视频-语言-动作模型用于机器人操作," arXiv 2410.06158, 2024.</p></div><p>[472] Y. Hong, B. Liu, M. Wu, Y. Zhai, K. Chang, L. Li, K. Lin, C. Lin, J. Wang, Z. Yang, Y. Wu, and L. Wang, "SlowFast-VGen: Slow-fast learning for action-driven long video generation," in ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[472] Y. Hong, B. Liu, M. Wu, Y. Zhai, K. Chang, L. Li, K. Lin, C. Lin, J. Wang, Z. Yang, Y. Wu, 和 L. Wang, "SlowFast-VGen: 基于动作驱动的长视频生成的慢-快学习," 在 ICLR, 2025.</p></div><p>[473] Y. Hu, Y. Guo, P. Wang, X. Chen, Y. Wang, J. Zhang, K. Sreenath, C. Lu, and J. Chen, "Video Prediction Policy: A generalist robot policy with predictive visual representations," in ICML, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[473] Y. Hu, Y. Guo, P. Wang, X. Chen, Y. Wang, J. Zhang, K. Sreenath, C. Lu, 和 J. Chen, "视频预测策略: 一种具有预测视觉表示的通用机器人策略," 在 ICML, 2025.</p></div><p>[474] Z. Ren, Y. Wei, X. Guo, Y. Zhao, B. Kang, J. Feng, and X. Jin, "Vide-oWorld: Exploring knowledge learning from unlabeled videos," arXiv 2501.09781, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[474] Z. Ren, Y. Wei, X. Guo, Y. Zhao, B. Kang, J. Feng, 和 X. Jin, "Vide-oWorld: 探索从未标记视频中学习知识," arXiv 2501.09781, 2025.</p></div><p>[475] H. A. Alhaija, J. Alvarez, M. Bala, T. Cai, T. Cao, L. Cha, J. Chen, M. Chen, F. Ferroni, S. Fidler, D. Fox, Y. Ge, J. Gu, A. Hassani, M. Isaev, P. Jannaty, S. Lan, T. Lasser, H. Ling, M.-Y. Liu, X. Liu, Y. Lu, A. Luo, Q. Ma, H. Mao, F. Ramos, X. Ren, T. Shen, S. Tang, T.-C. Wang, J. Wu, J. Xu, S. Xu, K. Xie, Y. Ye, X. Yang, X. Zeng, and Y. Zeng, "Cosmos-Transfer1: Conditional world generation with adaptive multimodal control," arXiv 2503.14492, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[475] H. A. Alhaija, J. Alvarez, M. Bala, T. Cai, T. Cao, L. Cha, J. Chen, M. Chen, F. Ferroni, S. Fidler, D. Fox, Y. Ge, J. Gu, A. Hassani, M. Isaev, P. Jannaty, S. Lan, T. Lasser, H. Ling, M.-Y. Liu, X. Liu, Y. Lu, A. Luo, Q. Ma, H. Mao, F. Ramos, X. Ren, T. Shen, S. Tang, T.-C. Wang, J. Wu, J. Xu, S. Xu, K. Xie, Y. Ye, X. Yang, X. Zeng, 和 Y. Zeng, "Cosmos-Transfer1: 具有自适应多模态控制的条件世界生成," arXiv 2503.14492, 2025.</p></div><p>[476] H. Zhen, Q. Sun, H. Zhang, J. Li, S. Zhou, Y. Du, and C. Gan, "TesserAct: learning 4D embodied world models," arXiv 2504.20995, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[476] H. Zhen, Q. Sun, H. Zhang, J. Li, S. Zhou, Y. Du, 和 C. Gan, "TesserAct: 学习4D具身世界模型," arXiv 2504.20995, 2025.</p></div><p>[477] Y. Ze, G. Yan, Y. Wu, A. Macaluso, Y. Ge, J. Ye, N. Hansen, L. E. Li, and X. Wang, "GNFactor: Multi-task real robot learning with generalizable neural feature fields," in CoRL, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[477] Y. Ze, G. Yan, Y. Wu, A. Macaluso, Y. Ge, J. Ye, N. Hansen, L. E. Li, 和 X. Wang, "GNFactor: 具有可泛化神经特征场的多任务真实机器人学习," 在 CoRL, 2023.</p></div><p>[478] S. Dasgupta, A. Gupta, S. Tuli, and R. Paul, "ActNeRF: Uncertainty-aware active learning of nerf-based object models for robot manipulators using visual and re-orientation actions," in IROS, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[478] S. Dasgupta, A. Gupta, S. Tuli, 和 R. Paul, "ActNeRF: 基于视觉和重新定向动作的机器人操纵器的不确定性感知主动学习nerf模型," 在 IROS, 2024.</p></div><p>[479] G. Lu, S. Zhang, Z. Wang, C. Liu, J. Lu, and Y. Tang, "ManiGaus-sian: Dynamic gaussian splatting for multi-task robotic manipulation," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[479] G. Lu, S. Zhang, Z. Wang, C. Liu, J. Lu, 和 Y. Tang, "ManiGaus-sian: 用于多任务机器人操纵的动态高斯点云," 在 ECCV, 2024.</p></div><p>[480] W. Zheng, W. Chen, Y. Huang, B. Zhang, Y. Duan, and J. Lu, "Oc-cWorld: Learning a 3D occupancy world model for autonomous driving," in ECCV, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[480] W. Zheng, W. Chen, Y. Huang, B. Zhang, Y. Duan, 和 J. Lu, "Oc-cWorld: 学习用于自主驾驶的3D占用世界模型," 在 ECCV, 2024.</p></div><p>[481] Y. Yang, J. Mei, Y. Ma, S. Du, W. Chen, Y. Qian, Y. Feng, and Y. Liu, "Driving in the occupancy world: Vision-centric 4D occupancy forecasting and planning via world models for autonomous driving," in AAAI, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[481] Y. Yang, J. Mei, Y. Ma, S. Du, W. Chen, Y. Qian, Y. Feng, 和 Y. Liu, "在占用世界中驾驶: 通过世界模型进行以视觉为中心的4D占用预测和规划，用于自主驾驶," 在 AAAI, 2025.</p></div><p>[482] J. Ma, X. Chen, J. Huang, J. Xu, Z. Luo, J. Xu, W. Gu, R. Ai, and H. Wang, "Cam4DOcc: Benchmark for camera-only 4D occupancy forecasting in autonomous driving applications," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[482] J. Ma, X. Chen, J. Huang, J. Xu, Z. Luo, J. Xu, W. Gu, R. Ai, 和 H. Wang, "Cam4DOcc: 用于自主驾驶应用的仅基于相机的4D占用预测基准," 在 CVPR, 2024.</p></div><p>[483] J. Wei, S. Yuan, P. Li, Q. Hu, Z. Gan, and W. Ding, "OccLLaMa: An occupancy-language-action generative world model for autonomous driving," arXiv 2409.03272, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[483] J. Wei, S. Yuan, P. Li, Q. Hu, Z. Gan, 和 W. Ding, "OccLLaMa: 用于自主驾驶的占用-语言-动作生成世界模型," arXiv 2409.03272, 2024.</p></div><p>[484] L. Wang, W. Zheng, Y. Ren, H. Jiang, Z. Cui, H. Yu, and J. Lu, "OccSora: 4D occupancy generation models as world simulators for autonomous driving," arXiv 2405.20337, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[484] L. Wang, W. Zheng, Y. Ren, H. Jiang, Z. Cui, H. Yu, 和 J. Lu, "OccSora: 作为自主驾驶世界模拟器的4D占用生成模型," arXiv 2405.20337, 2024.</p></div><p>[485] J. Tang, Z. Li, Z. Hao, X. Liu, G. Zeng, M. Liu, and Q. Zhang, "EdgeRunner: Auto-regressive auto-encoder for artistic mesh generation," in CVPR, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[485] J. Tang, Z. Li, Z. Hao, X. Liu, G. Zeng, M. Liu, 和 Q. Zhang, "EdgeRunner: 用于艺术网格生成的自回归自编码器," 在 CVPR, 2024.</p></div><p>[486] M. Wu, H. Dai, K. Yao, T. Tuytelaars, and J. Yu, "BG-Triangle: Bézier gaussian triangle for \(3\mathrm{D}\) vectorization and rendering," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[486] M. Wu, H. Dai, K. Yao, T. Tuytelaars, 和 J. Yu, "BG-Triangle: 用于\(3\mathrm{D}\)矢量化和渲染的贝塞尔高斯三角形," 在 CVPR, 2025.</p></div><p>[487] M. Guo, B. Wang, K. He, and W. Matusik, "TetSphere Splatting: Representing high-quality geometry with lagrangian volumetric meshes," in ICLR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[487] M. Guo, B. Wang, K. He, 和 W. Matusik, "TetSphere Splatting: 用拉格朗日体积网格表示高质量几何," 在 ICLR, 2025.</p></div><p>[488] S. Duggal, Y. Hu, O. Michel, A. Kembhavi, W. T. Freeman, N. A. Smith, R. Krishna, A. Torralba, A. Farhadi, and W.-C. \(\mathrm{{Ma}},\) "Eval3D: Interpretable and fine-grained evaluation for 3D generation," in CVPR, 2025.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[488] S. Duggal, Y. Hu, O. Michel, A. Kembhavi, W. T. Freeman, N. A. Smith, R. Krishna, A. Torralba, A. Farhadi, 和 W.-C. \(\mathrm{{Ma}},\) "Eval3D: 可解释的细粒度3D生成评估," 发表在CVPR, 2025。</p></div><p>[489] Y. Hu, L. Anderson, T. Li, Q. Sun, N. Carr, J. Ragan-Kelley, and F. Durand, "DiffTaichi: Differentiable programming for physical simulation," in ICLR, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[489] Y. Hu, L. Anderson, T. Li, Q. Sun, N. Carr, J. Ragan-Kelley, 和 F. Durand, "DiffTaichi: 用于物理仿真的可微编程," 在 ICLR, 2020.</p></div><!-- Media --><img src="https://cdn.noedgeai.com/bo_d163kf3ef24c73d1lc90_26.jpg?x=138&#x26;y=135&#x26;w=217&#x26;h=271&#x26;r=0"><!-- Media --><p>Beichen Wen received the B.Eng. degree in computer science and technology from Sun Yat-sen University, China, in 2024. He is currently a master student at Nanyang Technological University, supervised by Prof. Ziwei Liu. His research interests include computer graphics and 3D vision.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>文北辰于2024年获得中国中山大学计算机科学与技术专业的工学学士学位。目前，他是南洋理工大学的硕士研究生，导师为刘子伟教授。他的研究兴趣包括计算机图形学和三维视觉。</p></div><!-- Media --><img src="https://cdn.noedgeai.com/bo_d163kf3ef24c73d1lc90_26.jpg?x=140&#x26;y=509&#x26;w=211&#x26;h=270&#x26;r=0"><!-- Media --><p>Haozhe Xie received his Ph.D. from the Harbin Institute of Technology, in 2021. He is currently a research fellow at MMLab@NTU, Nanyang Technological University, Singapore. Previously, he served as a senior research scientist at Ten-cent AI Lab from 2021 to 2023. His research interests include computer vision with a focus on 3D generation and reconstruction. He has published several papers in CVPR, ICCV, ECCV, ICLR, and IJCV, and serves as a reviewer for</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>谢浩哲于2021年获得哈尔滨工业大学的博士学位。目前，他是新加坡南洋理工大学MMLab@NTU的研究员。此前，他在2021年至2023年期间担任腾讯人工智能实验室的高级研究科学家。他的研究兴趣包括计算机视觉，重点关注3D生成和重建。他在CVPR、ICCV、ECCV、ICLR和IJCV等会议上发表了多篇论文，并担任审稿人。</p></div><p>these journals and conferences.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这些期刊和会议。</p></div><!-- Media --><img src="https://cdn.noedgeai.com/bo_d163kf3ef24c73d1lc90_26.jpg?x=144&#x26;y=908&#x26;w=200&#x26;h=273&#x26;r=0"><!-- Media --><p>Zhaoxi Chen received the bachelor's de-</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>陈兆熙获得了学士学位</p></div><p>gree from Tsinghua University, in 2021. He is currently a Ph.D. student at MMLab@NTU, Nanyang Technological University, supervised by Prof. Ziwei Liu. He received the AISG PhD Fellowship in 2021. His research interests include inverse rendering and 3D generative models. He has published several papers in CVPR, ICCV, ECCV, ICLR, NeurlPS, TOG, and TPAMI. He also served as a reviewer for CVPR, ICCV, NeurIPS, TOG, and IJCV.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>他于2021年获得清华大学的学位。目前，他是南洋理工大学MMLab@NTU的博士生，导师是刘子维教授。他在2021年获得了AISG博士奖学金。他的研究兴趣包括逆向渲染和3D生成模型。他在CVPR、ICCV、ECCV、ICLR、NeurIPS、TOG和TPAMI等会议上发表了多篇论文。他还担任了CVPR、ICCV、NeurIPS、TOG和IJCV的审稿人。</p></div><p>Fangzhou Hong received Ph.D. degree from MMLab at Nanyang Technological University, supervised by Prof. Ziwei Liu. He received a B.Eng. degree in software engineering from Tsinghua University, China, in 2020. His research interests include computer vision and deep learning. Particularly, he is interested in 3D representation learning.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>洪方舟在南洋理工大学的MMLab获得博士学位，导师是刘子维教授。他于2020年在中国清华大学获得软件工程的工学学士学位。他的研究兴趣包括计算机视觉和深度学习，特别是对3D表示学习感兴趣。</p></div><!-- Media --><img src="https://cdn.noedgeai.com/bo_d163kf3ef24c73d1lc90_26.jpg?x=145&#x26;y=1317&#x26;w=214&#x26;h=258&#x26;r=0"><!-- Media --><p>Ziwei Liu is currently an associate professor at Nanyang Technological University, Singapore. His research revolves around computer vision, machine learning, and computer graphics. He has published extensively on top-tier conferences and journals in relevant fields, including CVPR, ICCV, ECCV, NeurlPS, ICLR, ICML, TPAMI, TOG, and Nature Machine Intelligence. He is the recipient of the Microsoft Young Fellowship, Hong Kong PhD Fellowship, ICCV Young Researcher Award, HKSTP Best Paper Award and WAIC Yunfan Award. He serves as an Area Chair of CVPR, ICCV,</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>刘子维目前是新加坡南洋理工大学的副教授。他的研究主要集中在计算机视觉、机器学习和计算机图形学领域。他在相关领域的顶级会议和期刊上发表了大量论文，包括CVPR（计算机视觉与模式识别会议）、ICCV（国际计算机视觉会议）、ECCV（欧洲计算机视觉会议）、NeurIPS（神经信息处理系统大会）、ICLR（国际学习表征会议）、ICML（国际机器学习会议）、TPAMI（IEEE模式分析与机器智能期刊）、TOG（ACM图形学年会）和《自然机器智能》。他曾获得微软青年奖学金、香港博士生奖学金、ICCV青年研究者奖、HKSTP最佳论文奖和WAIC云帆奖。他担任CVPR、ICCV的区域主席。</p></div><p>NeurIPS, and ICLR, as well as an Associate Editor of IJCV.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>NeurIPS和ICLR，以及IJCV的副编辑。</p></div>
      </body>
    </html>
  