
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px 350px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <h1>Survey on Fundamental Deep Learning 3D Reconstruction Techniques</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>基于深度学习的基础3D重建技术调查</h1></div><p>Yonge Bai \({}^{1}\) , LikHang Wong \({}^{2}\) , TszYin Twan \({}^{2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>杨白 \({}^{1}\) , 黄立行 \({}^{2}\) , 段子尹 \({}^{2}\)</p></div><p>\({}^{1}\) McMaster University</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{1}\) 麦克马斯特大学</p></div><p>\({}^{2}\) City University of Hong Kong</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{2}\) 香港城市大学</p></div><p><a href="mailto:baiy58@mcmaster.ca">baiy58@mcmaster.ca</a>, <a href="mailto:klhwong3-c@my.cityu.edu.hk">klhwong3-c@my.cityu.edu.hk</a>, <a href="mailto:tytwan2-c@my.cityu.edu.hk">tytwan2-c@my.cityu.edu.hk</a></p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p><a href="mailto:baiy58@mcmaster.ca">baiy58@mcmaster.ca</a>, <a href="mailto:klhwong3-c@my.cityu.edu.hk">klhwong3-c@my.cityu.edu.hk</a>, <a href="mailto:tytwan2-c@my.cityu.edu.hk">tytwan2-c@my.cityu.edu.hk</a></p></div><p>July 10, 2024</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>2024年7月10日</p></div><h2>Abstract</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>摘要</h2></div><p>This survey aims to investigate fundamental deep learning (DL) based 3D reconstruction techniques that produce photo-realistic 3D models and scenes, highlighting Neural Radiance Fields (NeRFs), Latent Diffusion Models (LDM), and 3D Gaussian Splatting. We dissect the underlying algorithms, evaluate their strengths and tradeoffs, and project future research trajectories in this rapidly evolving field. We provide a comprehensive overview of the fundamental in DL-driven 3D scene reconstruction, offering insights into their potential applications and limitations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>本调查旨在研究基于深度学习（DL）的基础3D重建技术，这些技术能够生成照片级真实感的3D模型和场景，重点介绍神经辐射场（NeRF）、潜在扩散模型（LDM）和3D高斯点云。我们剖析了其基础算法，评估了它们的优缺点，并预测了这一快速发展的领域的未来研究方向。我们提供了关于DL驱动的3D场景重建的全面概述，提供了对其潜在应用和局限性的见解。</p></div><h2>Background</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>背景</h2></div><p>3D reconstruction is a process aimed at creating volumetric surfaces from image and/or video data. This area of research has gained immense traction in recent months and finds applications in numerous domains, including virtual reality, augmented reality, autonomous driving, and robotics. Deep learning has emerged to the forefront of \(3\mathrm{D}\) reconstruction techniques and has demonstrated impressive results enhancing realism and accuracy.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D重建是一个旨在从图像和/或视频数据创建体积表面的过程。近年来，这一研究领域获得了巨大的关注，并在虚拟现实、增强现实、自动驾驶和机器人等多个领域找到了应用。深度学习已成为3D重建技术的前沿，并在增强真实感和准确性方面展示了令人印象深刻的成果。</p></div><h2>Neural Radiance Fields</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>神经辐射场</h2></div><p>Neural Radiance Field (NeRF) is a method for novel view synthesis of complex scenes using a set of input perspectives and optimizes a model to approximate a continuous volumetric scene or surface[9]. The method represents the volume using a multilayer preceptron (MLP) whose input is a 5D vector \(\left( {x,y,z,\theta ,\phi }\right) .\left( {x,y,z}\right)\) representing the spatial location and \(\left( {\theta ,\phi }\right)\) representing the viewing direction,with an output of a 4D vector \(\left( {R,G,B,\sigma }\right)\) representing the RGB color and a volume density. NeRFs achieved SOTA results on quantitative benchmarks as well as qualitative tests on neural rendering and view synthesis.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>神经辐射场（NeRF）是一种使用一组输入视角进行复杂场景的新视图合成的方法，并优化模型以近似连续的体积场景或表面[9]。该方法使用多层感知器（MLP）表示体积，其输入是一个5D向量\(\left( {x,y,z,\theta ,\phi }\right) .\left( {x,y,z}\right)\)，表示空间位置，\(\left( {\theta ,\phi }\right)\)表示视角方向，输出是一个4D向量\(\left( {R,G,B,\sigma }\right)\)，表示RGB颜色和体积密度。NeRF在定量基准测试和神经渲染及视图合成的定性测试中取得了最先进的结果。</p></div><h2>Prior Work</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>先前工作</h2></div><p>NeRFs build upon prior work in RGB-alpha volume rendering for view-synthesis and the use of neural networks (NN) as implicit continuous shape representations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>NeRF建立在RGB-alpha体积渲染和使用神经网络（NN）作为隐式连续形状表示的先前工作基础上。</p></div><p>Volume Rendering for View-Synthesis This process involves, using a set of images to learn a 3D discrete volume representation, the model estimate the volume density and emitted color at each point in the 3D space, which is then used to synthesize images from various viewpoints. Prior methods include Soft 3D, which implements a soft 3D representation of the scene by using traditional stereo methods, this representation is used directly to model ray visibility and occlusion during view-synthesis [12]. Along with deep learning methods such as Neural Volumes which uses a an encoder-decoder network that transforms the input images into a 3D voxel grid, used to generate new views [6]. While these volumetric representation are easy to optimize by being trained on how well they render the ground truth views, but as the resolution or complexity of the scene increases the compute and memory needed to store these discretized representations become unpractical.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>视图合成的体积渲染该过程涉及使用一组图像学习3D离散体积表示，模型估计3D空间中每个点的体积密度和发射颜色，然后用于从不同视角合成图像。先前的方法包括Soft 3D，它通过使用传统立体方法实现场景的软3D表示，该表示直接用于在视图合成过程中建模光线可见性和遮挡[12]。以及深度学习方法，如神经体积，它使用编码器-解码器网络将输入图像转换为3D体素网格，用于生成新视图[6]。虽然这些体积表示易于通过训练来优化其渲染真实视图的效果，但随着场景的分辨率或复杂性增加，存储这些离散表示所需的计算和内存变得不切实际。</p></div><p>Neural Networks as Shape Representations This field of study aims to implicitly represent the \(3\mathrm{D}\) surface with a NN's weights. In contrast to the volumetric approach this representation encodes a description of a 3D surface at infinite resolution without excessive memory footprint as described here[8]. The NN encodes the 3D surface by learning to map a point in space to a property of that point in the 3D space, for example occupancy [8] or signed distance fields [11]. While this approach saves significant memory it is harder to optimize, leading to poor synthetic views compared to the discrete representations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>神经网络作为形状表示该研究领域旨在通过神经网络的权重隐式表示\(3\mathrm{D}\)表面。与体积方法相比，该表示以无限分辨率编码3D表面的描述，而不会占用过多内存，如此处所述[8]。神经网络通过学习将空间中的一个点映射到该点在3D空间中的属性来编码3D表面，例如占用[8]或有符号距离场[11]。虽然这种方法节省了大量内存，但优化起来更困难，导致与离散表示相比合成视图较差。</p></div><h2>Approach: NeRF</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>方法：NeRF</h2></div><p>NeRFs combine these two approaches by representing the scene in the weights of an MLP but view synthesis is trained using the techniques in traditional volume rendering.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>NeRF结合了这两种方法，通过在MLP的权重中表示场景，但视图合成是使用传统体积渲染中的技术进行训练的。</p></div><h2>Neural Radiance Field Scene Representation</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>神经辐射场场景表示</h2></div><p>The scene is represented by \(5\mathrm{D}\) vector comprised of \(\mathbf{x} =\) (x,y,z)and \(\mathbf{d} = \left( {\theta ,\phi }\right)\) . This continuous 5D scene representation is approximated by a MLP network \({F}_{\Theta } : \left( {\mathbf{x},\mathbf{d}}\right)  \rightarrow\) \(\left( {\mathbf{c},\sigma }\right)\) ,whose weights \(\Theta\) are optimized to predict each \(5\mathrm{D}\) input’s \(\mathbf{c} = \left( {R,G,B}\right)\) representing RGB color and \(\sigma\) representing density. Density can be thought of as occlusion, points with a high occlusion having a higher \(\sigma\) value than points with lower occlusion.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>场景由\(5\mathrm{D}\)向量表示，该向量由\(\mathbf{x} =\)（x,y,z）和\(\mathbf{d} = \left( {\theta ,\phi }\right)\)组成。这个连续的5D场景表示由MLP网络\({F}_{\Theta } : \left( {\mathbf{x},\mathbf{d}}\right)  \rightarrow\) \(\left( {\mathbf{c},\sigma }\right)\)近似，其权重\(\Theta\)被优化以预测每个\(5\mathrm{D}\)输入的\(\mathbf{c} = \left( {R,G,B}\right)\)，表示RGB颜色，\(\sigma\)表示密度。密度可以被视为遮挡，具有高遮挡的点的\(\sigma\)值高于低遮挡的点。</p></div><p>The implicit representation is held consist by forcing the network to predict \(\sigma\) only as a function of \(\mathbf{x}\) ,as density should not change as a result of viewing angle. While \(\mathbf{c}\) is trained as a function of both \(\mathbf{x}\) and \(\mathbf{d}\) . The MLP \({F}_{\Theta }\) has 9 fully-connected layers using ReLU activation functions and 256 channels per layer for the first 8 layers and 128 channels</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>隐式表示通过强制网络仅根据\(\mathbf{x}\)预测\(\sigma\)来保持一致，因为密度不应因视角而变化。而\(\mathbf{c}\)则作为\(\mathbf{x}\)和\(\mathbf{d}\)的函数进行训练。MLP \({F}_{\Theta }\)具有9个全连接层，使用ReLU激活函数，前8层每层256个通道，最后一层128个通道。</p></div><!-- Media --><!-- figureText: 5D Input Output Volume Rendering Color + Density Rendering Loss (b) (c) (d) Position + Direction \( \rightarrow  \left( {RGB\sigma }\right) \) (a) --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_1.jpg?x=151&#x26;y=147&#x26;w=717&#x26;h=226&#x26;r=0"><p>Figure 1: An overview of the neural radiance field scene representation and differentiable rendering procedure. Synthesize images by sampling \(5\mathrm{D}\) coordinates (location and viewing direction) along camera rays (a), feeding those locations into an MLP to predict a color and volume density (b), and using volume rendering techniques to composite these values into an image (c). This rendering function is differentiable, so we can optimize our scene representation by minimizing the residual between synthesized color and ground truth of the actual color(d).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图1：神经辐射场场景表示和可微渲染过程的概述。通过沿相机光线采样\(5\mathrm{D}\)坐标（位置和视角）合成图像（a），将这些位置输入到MLP中以预测颜色和体积密度（b），并使用体积渲染技术将这些值合成到图像中（c）。这个渲染函数是可微的，因此我们可以通过最小化合成颜色与实际颜色的残差来优化我们的场景表示（d）。</p></div><!-- figureText: \( \left( {x,y,z}\right)  \rightarrow \) \( \left( {R,G,B,\sigma }\right) \) 1 layer \( \left( {\theta ,\phi }\right) \) 128 channels RELU \( {F}_{\Theta } \) 8 layer 256 channels ReLU --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_1.jpg?x=186&#x26;y=785&#x26;w=625&#x26;h=275&#x26;r=0"><p>Figure 2: An overview of the NeRF model. \(\mathbf{x}\) is passed into the first 8 layers,which output \(\mathbf{v}\) and \(\sigma\) a). \(\mathbf{v}\) is concatenated with \(\mathbf{d}\) and passed into the last layer,which outputs \(\mathbf{c}\) b).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2：NeRF模型的概述。\(\mathbf{x}\)被传递到前8层，输出\(\mathbf{v}\)和\(\sigma\)（a）。\(\mathbf{v}\)与\(\mathbf{d}\)连接并传递到最后一层，输出\(\mathbf{c}\)（b）。</p></div><!-- Media --><p>for the last layer. \({F}_{\Theta }\) first processes \(\mathbf{x}\) with the first 8 layers outputting \(\sigma\) and a 256-dimensional feature vector \(\mathbf{v}.\mathbf{v}\) is then concatenated with \(\mathbf{d}\) and passed into the final layer that outputs c. This process is shown in Figure 2.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>对于最后一层。\({F}_{\Theta }\)首先处理\(\mathbf{x}\)，前8层输出\(\sigma\)，然后一个256维特征向量\(\mathbf{v}.\mathbf{v}\)与\(\mathbf{d}\)连接并传递到最终层，输出c。这个过程在图2中展示。</p></div><h2>Volume Rendering with Radiance Fields</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>使用辐射场的体积渲染</h2></div><p>The color of any ray passing through the scene is rendered using principles from classical volume rendering.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>通过经典体积渲染的原理渲染穿过场景的任何光线的颜色。</p></div><p></p>\[\widehat{C}\left( \mathbf{r}\right)  = \mathop{\sum }\limits_{{i = 1}}^{N}{w}_{i}{c}_{i}\text{,where}{w}_{i} = {T}_{i}{\alpha }_{i} \tag{1}\]<p></p><p>Equation (1) can be explained as the color \({c}_{i}\) of each point being weighted by \({w}_{i}.\;{w}_{i}\) is made up of \({T}_{i} =\) \(\exp \left( {-\mathop{\sum }\limits_{{j = 1}}^{{i - 1}}{\sigma }_{i}{\delta }_{i}}\right)\) where \({\sigma }_{i}\) is the density and \({\delta }_{i}\) is the distance between adjacently sampled points. \({T}_{i}\) denotes the accumulated transmittance until point \(i\) which can be thought of as the amount of light blocked earlier along the ray, and \({\alpha }_{i} = 1 - \exp \left( {-{\sigma }_{i}{\delta }_{i}}\right)\) denoting the opacity at point \(i\) . Thus the color predicted at point with higher transmittance and opacity (the beginning of surfaces) contribute more to final predicted color of ray \(\mathbf{r}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>方程（1）可以解释为每个点的颜色\({c}_{i}\)被\({w}_{i}.\;{w}_{i}\)加权，由\({T}_{i} =\) \(\exp \left( {-\mathop{\sum }\limits_{{j = 1}}^{{i - 1}}{\sigma }_{i}{\delta }_{i}}\right)\)组成，其中\({\sigma }_{i}\)是密度，\({\delta }_{i}\)是相邻采样点之间的距离。\({T}_{i}\)表示直到点\(i\)的累积透射率，可以视为沿光线早期阻挡的光量，\({\alpha }_{i} = 1 - \exp \left( {-{\sigma }_{i}{\delta }_{i}}\right)\)表示在点\(i\)的不透明度。因此，在透射率和不透明度较高的点（表面的开始）预测的颜色对光线\(\mathbf{r}\)的最终预测颜色贡献更大。</p></div><!-- Media --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_1.jpg?x=942&#x26;y=174&#x26;w=689&#x26;h=202&#x26;r=0"><p>Figure 3: Visualizing how the model improves the positional encoding. Without it the model is unable to represent high variation geometries and textures resulting in an over smoothed, blurred appearance. Also how removing view dependency affect the models ability to render lighting and reflections.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3：可视化模型如何改善位置编码。没有它，模型无法表示高变化的几何形状和纹理，导致过于平滑、模糊的外观。同时，去除视角依赖性如何影响模型渲染光照和反射的能力。</p></div><!-- Media --><h2>Optimizing a NeRF</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>优化NeRF</h2></div><p>The previous sections covered the core components to NeRFs but the original paper had two more techniques to achieve SOTA quality-positional encoding and hierarchical volume sampling.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>前面的部分涵盖了NeRF的核心组件，但原始论文还有两种技术以实现SOTA质量——位置编码和分层体积采样。</p></div><p>Positional Encoding The authors found that directly feeding in \(\left( {x,y,z,\theta ,\phi }\right)\) to \({F}_{\Theta }\) resulted in poor performance. As a result, they chose to map the inputs to a higher dimensional space using high frequency functions, this enabled the model to better fit data with high variations. Thus \({F}_{\Theta }\) is reformulated as a composition of two functions \({F}_{\Theta } = {F}_{\Theta }^{\prime } \circ  \gamma .{F}_{\Theta }^{\prime }\) being the original MLP and \(\gamma\) defined as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>位置编码 作者发现直接将\(\left( {x,y,z,\theta ,\phi }\right)\)输入到\({F}_{\Theta }\)会导致性能不佳。因此，他们选择使用高频函数将输入映射到更高维空间，这使得模型能够更好地拟合具有高变化的数据。因此，\({F}_{\Theta }\)被重新表述为两个函数\({F}_{\Theta } = {F}_{\Theta }^{\prime } \circ  \gamma .{F}_{\Theta }^{\prime }\)的组合，其中\({F}_{\Theta } = {F}_{\Theta }^{\prime } \circ  \gamma .{F}_{\Theta }^{\prime }\)是原始的MLP，\(\gamma\)定义为：</p></div><p></p>\[\gamma \left( x\right)  = \left( \begin{matrix} \sin \left( {{2}^{0}{\pi x}}\right) , &#x26; \cos \left( {{2}^{0}{\pi x}}\right) \\  \vdots &#x26; \vdots \\  \sin \left( {{2}^{L - 1}{\pi x}}\right) , &#x26; \cos \left( {{2}^{L - 1}{\pi x}}\right)  \end{matrix}\right)  \tag{2}\]<p></p><p>\(\gamma \left( \cdot \right)\) is applied to(x,y,z)in \(\mathbf{x}\) with \(L = {10}\) and \(\left( {\theta ,\phi }\right)\) with \(L = 4\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\(\gamma \left( \cdot \right)\)应用于\(\mathbf{x}\)中的(x,y,z)，与\(L = {10}\)和\(\left( {\theta ,\phi }\right)\)以及\(L = 4\)一起。</p></div><p>\(\gamma\) is a mapping from \(\mathbb{R}\) to \({\mathbb{R}}^{2L}\) that significantly improves performance (Figure 3).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\(\gamma\)是从\(\mathbb{R}\)到\({\mathbb{R}}^{2L}\)的映射，显著提高了性能（图3）。</p></div><p>Hierarchical Volume Sampling Free space and occluded region contribute much less to the quality of the NeRF compared to areas at the beginning of a surface, but with uniform sampling, are sampled at the same rate. So the authors proposed a hierarchical representation that increases rendering efficiency and quality by allocating samples proportional to their expected effect shown in 4. For example, if the object in question was a ball, there would be less samples taken in the open space in front of the ball and inside of the ball verses samples directly on the ball's surface.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>分层体积采样 空间和遮挡区域对NeRF的质量贡献远低于表面起始区域，但在均匀采样下，它们的采样率相同。因此，作者提出了一种分层表示，通过按预期效果分配样本来提高渲染效率和质量，如图4所示。例如，如果所讨论的对象是一个球，则在球前的开放空间和球内部的样本会少于直接在球表面上的样本。</p></div><p>This is done by optimizing two networks. One "coarse" and one "fine". The course network samples points uniformly along the ray, while the fine network is biases toward the relevant part of the volume by normalizing the per sample weights \({w}_{i}\) described in equation (1),this allows one to treat the weight of each point as a probability distribution which is sampled to train the fine network. This procedure allocates more samples to regions expected to contain visible content.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这是通过优化两个网络来完成的。一个是“粗略”的，另一个是“精细”的。粗略网络沿光线均匀采样点，而精细网络则通过归一化每个样本权重\({w}_{i}\)（如公式（1）所述）偏向于体积的相关部分，这使得可以将每个点的权重视为概率分布，从而进行采样以训练精细网络。该过程将更多样本分配给预期包含可见内容的区域。</p></div><!-- Media --><!-- figureText: Camera Ray 3D volume --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_2.jpg?x=151&#x26;y=156&#x26;w=674&#x26;h=568&#x26;r=0"><p>Figure 4: Illustrating hierarchical sampling, where samples are proportional to their contribution to the final volume render.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图4：说明分层采样，其中样本与其对最终体积渲染的贡献成比例。</p></div><!-- Media --><h2>Limitations</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>限制</h2></div><p>While having groundbreaking abilities to render photorealis-tic 3D volumes from 2D images, the original NeRF methodology suffered from several limitations. These limitations include:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>尽管具有从2D图像渲染出逼真3D体积的突破性能力，但原始NeRF方法存在几个限制。这些限制包括：</p></div><p>Computational Efficiency The optimization of a single scene took \({100} - {300}\mathrm{k}\) iterations to converge of a single NVIDIA V100 GPU which corresponded to 1-2 days [9]. This poor computational efficiency is a product of dense sampling of rays for rendering. This dense sampling approach helped in capturing fine details and accurately representing complex scenes, but it significantly increases the computational load.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>计算效率 优化单个场景需要\({100} - {300}\mathrm{k}\)次迭代才能收敛，使用单个NVIDIA V100 GPU，这对应于1-2天[9]。这种较差的计算效率是由于渲染时对光线的密集采样造成的。这种密集采样方法有助于捕捉细节并准确表示复杂场景，但显著增加了计算负担。</p></div><p>Lack of Generalizability NeRFs are inheritably inflexible due to the as models overfit to one scene. A NeRF cannot be adapted for novel scenes without complete retraining.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>缺乏通用性 NeRF固有地不灵活，因为模型会过拟合到一个场景。NeRF无法在不完全重新训练的情况下适应新场景。</p></div><p>Difficulty of Editing Modifying content in NeRFs such as moving or removing object is very difficult. Since the model represents the scene as a continuous function and does not store geometric information.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>编辑困难 在NeRF中修改内容（如移动或删除对象）非常困难。因为模型将场景表示为连续函数，并不存储几何信息。</p></div><p>Data requirements NeRFs require a lot of data to produce high quality results show in the original paper. The synthetic 3D models as lego bulldozer and pirate ship took about 100 image and the real life scenes such as the flower and conference room each requiring around 60 [9].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>数据要求 NeRF需要大量数据才能产生高质量结果，如原始论文所示。合成的3D模型如乐高推土机和海盗船大约需要100张图像，而现实场景如花朵和会议室各需约60张[9]。</p></div><h2>Transient Artifacts</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>瞬态伪影</h2></div><p>The original NeRFs assume that the world is geometrically, materially, and photometrically static. Therefore requiring that any two photographs taken at the same position and orientation must be identical [7] they do not have a way to adjust for transient occlusions or variable appearance which result in artifacts and noise when this assumption fails such as with real world images. This is clearly show in 5 .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>原始NeRF假设世界在几何、材料和光度上是静态的。因此，要求在相同位置和方向拍摄的任何两张照片必须相同[7]，它们没有调整瞬态遮挡或可变外观的方式，这在这种假设失败时会导致伪影和噪声，例如在现实世界图像中。这在图5中清楚地显示出来。</p></div><!-- Media --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_2.jpg?x=933&#x26;y=148&#x26;w=715&#x26;h=279&#x26;r=0"><p>Figure 5: Comparison made in the paper NeRF in the Wild [7], where the original NeRF (left) noisy artifacts compared to NeRF-W (right).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图5：论文《NeRF in the Wild》[7]中的比较，原始NeRF（左）与NeRF-W（右）相比，存在噪声伪影。</p></div><!-- Media --><h2>Instant-NGP</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>Instant-NGP</h2></div><h2>Overview</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>概述</h2></div><p>Instant-NGP[10], proposed by Nvlabs, is a method that significantly reduce the computation demand of original NeRFs. It leverages multi-resolution hash grids to improve memory usage and optimizes \(3\mathrm{D}\) reconstruction performance.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Instant-NGP（即时神经图形原语）[10]是Nvlabs提出的一种方法，显著降低了原始NeRF的计算需求。它利用多分辨率哈希网格来改善内存使用，并优化\(3\mathrm{D}\)重建性能。</p></div><h2>Prior work</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>先前工作</h2></div><p>Learnable positional encoding Learnable positional encoding refers to positional encodings that are parameterized for specific positions in a continuous 3D space. The positional encoding for a point \(p\) in 3D space can be represented as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>可学习的位置编码可学习的位置编码是指为连续3D空间中特定位置参数化的位置编码。3D空间中点\(p\)的位置编码可以表示为：</p></div><p></p>\[\mathbf{{pe}}\left( p\right)  = \sigma \left( {\mathbf{{Wp}} + \mathbf{b}}\right)\]<p></p><p>where \(\mathbf{p} = {\left( x,y,z\right) }^{T}\) represents the coordinates of the position in 3D space, \(\mathbf{W}\) is a learnable weight matrix, \(\mathbf{b}\) is a bias vector,and \(\sigma\) denotes a non-linear activation function.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(\mathbf{p} = {\left( x,y,z\right) }^{T}\)表示3D空间中位置的坐标，\(\mathbf{W}\)是可学习的权重矩阵，\(\mathbf{b}\)是偏置向量，\(\sigma\)表示非线性激活函数。</p></div><p>These positional encodings can then be integrated into a neural network model to facilitate the learning of spatial relationships.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这些位置编码可以集成到神经网络模型中，以促进空间关系的学习。</p></div><!-- Media --><!-- figureText: (a) No encoding (b) Frequency (c) Dense grid (d) Dense grid (e) Hash table (ours) (f) Hash table (ours) Multi resolution \( T = {2}^{14} \) \( T = {2}^{14} \) \( {10}\mathrm{k} + {16.3}\mathrm{M} \) \( {10k} + {494k} \) \( {10}\mathrm{\;k} + {12.6}\mathrm{M} \) 1:26 / PSNR 23.62 1:48 / PSNR 22.61 1:47 / PSNR 24.58 [Mildenhall et al. 2020 Single resolution \( {411}\mathrm{\;k} + 0 \) parameters \( {438}\mathrm{\;k} + 0 \) \( {10}\mathrm{k} + {33.6}\mathrm{M} \) 11:28 (mm:ss) / PSNR 18.56 12:45 / PSNR 22.90 1:09 / PSNR 22.35 --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_2.jpg?x=931&#x26;y=1524&#x26;w=712&#x26;h=225&#x26;r=0"><p>Figure 6: Experiment performed in the original paper. as the number of parameters used for learning the positional encoding increases, the image becomes clearer and sharper.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6：原始论文中进行的实验。随着用于学习位置编码的参数数量增加，图像变得更加清晰和锐利。</p></div><!-- Media --><h2>Algorithm</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>算法</h2></div><p>Multi-Resolution Hash Encoding One of the key components of Instant-NGP (Instant Neural Graphics Primitives) is the Multi-Resolution Hash Encoding. Instead of learning the positional encoding for the entire 3D space, the 3D space is first scaled to fit within a normalized range of 0 to 1 . This normalized space is then replicated across multiple resolutions and each subdivided into grids of varying densities. This captures both coarse and fine details in the scene. Each level focuses on learning the positional encodings at the vertices of the grids. Mathematically, this can be expressed as follows:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>多分辨率哈希编码Instant-NGP（即时神经图形原语）的一个关键组成部分是多分辨率哈希编码。3D空间首先被缩放以适应0到1的标准化范围，而不是为整个3D空间学习位置编码。然后，这个标准化空间在多个分辨率上复制，并且每个分辨率被细分为不同密度的网格。这捕捉了场景中的粗细细节。每个级别专注于学习网格顶点的位置信息编码。从数学上讲，可以表示为：</p></div><!-- Media --><!-- figureText: \( L = 2,b = {1.5} \) \( 1/{N}_{0} \) \( m\left( {y;\Phi }\right) \) O O O O O O O O O O (3) Linear interpolation (4) Concatenation (5) Neural network (1) Hashing of voxel vertices --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_3.jpg?x=155&#x26;y=152&#x26;w=681&#x26;h=257&#x26;r=0"><p>Figure 7: Illustration of the multiresolution hash encoding represented in \(2\mathrm{D}\) in the original paper.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图7：原始论文中表示的多分辨率哈希编码的示意图\(2\mathrm{D}\)。</p></div><!-- Media --><p>\({\mathbf{p}}_{\text{scaled }} = \mathbf{p} \cdot  \mathbf{s}\)</p><p>where \(\mathbf{p}\) represents the original coordinates in \(3\mathrm{D}\) space, and \(\mathbf{s}\) is a scaling factor that normalizes the space to the \(\left\lbrack  {0,1}\right\rbrack\) range. Following the scaling, the coordinates are hashed into a multi-resolution structure using a spatial hash function:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(\mathbf{p}\)表示\(3\mathrm{D}\)空间中的原始坐标，\(\mathbf{s}\)是将空间标准化到\(\left\lbrack  {0,1}\right\rbrack\)范围的缩放因子。缩放后，坐标通过空间哈希函数哈希到多分辨率结构中：</p></div><p></p>\[h\left( \mathbf{x}\right)  = \left( {{\bigoplus }_{i = 1}^{d}\left( {{x}_{i} \cdot  {\pi }_{i}}\right) }\right) \;{\;\operatorname{mod}\;T}\]<p></p><p>Here, \(d\) is the dimensionality of the space (e.g.,3 for \(3\mathrm{D}\) coordinates), \(\mathbf{x} = \left( {{x}_{1},{x}_{2},\ldots ,{x}_{d}}\right)\) represents the scaled coordinates, \(\bigoplus\) denotes the bit-wise XOR operation, \({\pi }_{i}\) are large prime numbers unique to each dimension,and \(T\) is the size of the hash table. This function maps spatial coordinates to indices in the hash table, where the neural network's parameters are stored or retrieved, linking specific spatial locations to neural network parameters.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这里，\(d\)是空间的维度（例如，3表示\(3\mathrm{D}\)坐标），\(\mathbf{x} = \left( {{x}_{1},{x}_{2},\ldots ,{x}_{d}}\right)\)表示缩放后的坐标，\(\bigoplus\)表示按位异或操作，\({\pi }_{i}\)是每个维度唯一的大质数，\(T\)是哈希表的大小。该函数将空间坐标映射到哈希表中的索引，其中存储或检索神经网络的参数，将特定空间位置与神经网络参数链接。</p></div><p>In a nutshell, a hash table is assigned to each level of resolution. For each resolution, each vertex is mapped to a entry in the resolution's hash table. Higher-resolution have larger hash tables compared to lower-resolutions. Every resolution's hash table map each of it's vertices to an individual set of parameters that learn their positional encodings.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>简而言之，每个分辨率级别分配一个哈希表。对于每个分辨率，每个顶点映射到该分辨率哈希表中的一个条目。高分辨率的哈希表比低分辨率的哈希表大。每个分辨率的哈希表将其每个顶点映射到一组学习其位置编码的参数。</p></div><p>Learning positional encoding During training, when the model is exposed to images from different viewpoints, the NN adjusts the parameters stored in the hash table to minimize the difference between the rendered images and the actual training images. The loss \(L\) can be expressed as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>学习位置编码在训练期间，当模型接触到来自不同视角的图像时，神经网络调整存储在哈希表中的参数，以最小化渲染图像与实际训练图像之间的差异。损失\(L\)可以表示为：</p></div><p></p>\[L\left( \theta \right)  = \frac{1}{2}\mathop{\sum }\limits_{{i = 1}}^{m}{\left( R\left( {x}_{i},\theta \right)  - {y}_{i}\right) }^{2}\]<p></p><p>where \(R\left( {{x}_{i},\theta }\right)\) is the rendered image based on parameters \(\theta\) and view point \({x}_{i},{y}_{i}\) is the corresponding actual image, and \(m\) is the number of pixels or data points considered. We can then learn these parameters using different optimization techniques like gradient descent.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(R\left( {{x}_{i},\theta }\right)\)是基于参数\(\theta\)和视点\({x}_{i},{y}_{i}\)的渲染图像，\(m\)是考虑的像素或数据点的数量。然后，我们可以使用不同的优化技术（如梯度下降）来学习这些参数。</p></div><p>Hash Collisions Hash collisions are avoided by assigning a hash table at each resolution that long enough to ensure one-to-one mapping from entries to positional encodings.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>哈希冲突通过在每个分辨率分配一个足够长的哈希表来避免，以确保条目与位置编码之间的一对一映射。</p></div><h2>Performance</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>性能</h2></div><p>As shown in figure 8, Instant-NGP achieved a notable 20- \({60} \times\) speed improvement compared to compared to the original NeRFs while maintaining it's quality.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>如图8所示，Instant-NGP在保持质量的同时，相较于原始的NeRFs实现了显著的20-\({60} \times\)速度提升。</p></div><!-- Media --><table><tbody><tr><td></td><td>Mic</td><td>Ficus</td><td>Chair</td><td>Horboc</td><td>Materials</td><td>Drums</td><td>SHIP</td><td>LEGO</td><td>avg.</td></tr><tr><td>Ours: Hash (1 s)</td><td>26.09</td><td>21.30</td><td>21.55</td><td>21.63</td><td>22.07</td><td>17.76</td><td>20.38</td><td>18.83</td><td>21.202</td></tr><tr><td>Ours: Hash (5 s)</td><td>32.60</td><td>30.35</td><td>30.77</td><td>33.42</td><td>26.60</td><td>23.84</td><td>26.38</td><td>30.13</td><td>29.261</td></tr><tr><td>Ours: Hash (15 s)</td><td>34.76</td><td>32.26</td><td>32.95</td><td>35.56</td><td>28.25</td><td>25.23</td><td>28.56</td><td>33.68</td><td>31.407</td></tr><tr><td>Ours: Hash (1 min)</td><td>35.92 ●</td><td>33.05 ●</td><td>34.34 ●</td><td>36.78</td><td>29.33</td><td>25.82</td><td>30.20 ●</td><td>35.63 ●</td><td>\( {32.635} \bullet \)</td></tr><tr><td>Ours: Hash (5 min)</td><td>36.22</td><td>33.51 %</td><td>\( {35.00}^{ \pm  } \)</td><td>37.40 *</td><td>29.78 ●</td><td>\( {26.02} * \)</td><td>31.10 *</td><td>36.39 #</td><td>33.176 %</td></tr><tr><td>mip-NeRF (~hours)</td><td>36.51 (</td><td>33.29</td><td>35.14 (</td><td>37.48 %</td><td>30.71 %</td><td>25.48 *</td><td>30.411</td><td>35.70</td><td>33.090</td></tr><tr><td>NSVF (~hours)</td><td>34.27</td><td>31.23</td><td>33.19</td><td>37.14 ●</td><td>32.68 %</td><td>25.18</td><td>27.93</td><td>32.29</td><td>31.739</td></tr><tr><td>NeRF (~hours)</td><td>32.91</td><td>30.13</td><td>33.00</td><td>36.18</td><td>29.62</td><td>25.01</td><td>28.65</td><td>32.54</td><td>31.005</td></tr><tr><td>Ours: Frequency (5 min)</td><td>31.89</td><td>28.74</td><td>31.02</td><td>34.86</td><td>28.93</td><td>24.18</td><td>28.06</td><td>32.77</td><td>30.056</td></tr><tr><td>Ours: Frequency (1min)</td><td>26.62</td><td>24.72</td><td>28.51</td><td>32.61</td><td>26.36</td><td>21.33</td><td>24.32</td><td>28.88</td><td>26.669</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td></td><td>麦克</td><td>榕树</td><td>椅子</td><td>霍尔博克</td><td>材料</td><td>鼓</td><td>船</td><td>乐高</td><td>平均</td></tr><tr><td>我们的：哈希（1秒）</td><td>26.09</td><td>21.30</td><td>21.55</td><td>21.63</td><td>22.07</td><td>17.76</td><td>20.38</td><td>18.83</td><td>21.202</td></tr><tr><td>我们的：哈希（5秒）</td><td>32.60</td><td>30.35</td><td>30.77</td><td>33.42</td><td>26.60</td><td>23.84</td><td>26.38</td><td>30.13</td><td>29.261</td></tr><tr><td>我们的：哈希（15秒）</td><td>34.76</td><td>32.26</td><td>32.95</td><td>35.56</td><td>28.25</td><td>25.23</td><td>28.56</td><td>33.68</td><td>31.407</td></tr><tr><td>我们的：哈希（1分钟）</td><td>35.92 ●</td><td>33.05 ●</td><td>34.34 ●</td><td>36.78</td><td>29.33</td><td>25.82</td><td>30.20 ●</td><td>35.63 ●</td><td>\( {32.635} \bullet \)</td></tr><tr><td>我们的：哈希（5分钟）</td><td>36.22</td><td>33.51 %</td><td>\( {35.00}^{ \pm  } \)</td><td>37.40 *</td><td>29.78 ●</td><td>\( {26.02} * \)</td><td>31.10 *</td><td>36.39 #</td><td>33.176 %</td></tr><tr><td>mip-NeRF（约小时）</td><td>36.51 (</td><td>33.29</td><td>35.14 (</td><td>37.48 %</td><td>30.71 %</td><td>25.48 *</td><td>30.411</td><td>35.70</td><td>33.090</td></tr><tr><td>NSVF（约小时）</td><td>34.27</td><td>31.23</td><td>33.19</td><td>37.14 ●</td><td>32.68 %</td><td>25.18</td><td>27.93</td><td>32.29</td><td>31.739</td></tr><tr><td>NeRF（约小时）</td><td>32.91</td><td>30.13</td><td>33.00</td><td>36.18</td><td>29.62</td><td>25.01</td><td>28.65</td><td>32.54</td><td>31.005</td></tr><tr><td>我们的：频率（5分钟）</td><td>31.89</td><td>28.74</td><td>31.02</td><td>34.86</td><td>28.93</td><td>24.18</td><td>28.06</td><td>32.77</td><td>30.056</td></tr><tr><td>我们的：频率（1分钟）</td><td>26.62</td><td>24.72</td><td>28.51</td><td>32.61</td><td>26.36</td><td>21.33</td><td>24.32</td><td>28.88</td><td>26.669</td></tr></tbody></table></div><p>Figure 8: The figure adapted from the original paper compares the Peak Signal to Noise Ratio (PSNR) performance of various NeRF implementations, including the author's multi-resolution hash encoding method, against other models that require hours of training. First row is the name of the object constructed.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图8：该图改编自原始论文，比较了各种NeRF实现的峰值信噪比（PSNR）性能，包括作者的多分辨率哈希编码方法，与其他需要数小时训练的模型。第一行是构建对象的名称。</p></div><!-- Media --><h2>Limitations</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>限制</h2></div><p>Instant-NGP focuses on speeding up the computation and training processes of NeRFs. However, it still suffers from many of the same issues such as generalability different datasets or unseen scenarios. In the next section, we introduce LDM based techniques for \(3\mathrm{D}\) reconstruction to address the issue of generalizable.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Instant-NGP专注于加速NeRF的计算和训练过程。然而，它仍然面临许多相同的问题，例如在不同数据集或未见场景中的泛化能力。在下一节中，我们介绍基于LDM的技术用于\(3\mathrm{D}\)重建，以解决可泛化性的问题。</p></div><h2>Latent-Diffusion-Model based 3D reconstruction</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>基于潜在扩散模型的3D重建</h2></div><h2>Background</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>背景</h2></div><p>Traditional 3D-Reconstruction algorithms rely heavily on the training data to capture all aspects of the volume. Humans,however,are able to estimate a \(3\mathrm{D}\) surface from a single image. This concept is the foundation of the Zero- 1-to-3[5] framework developed out of Columbia University which introduces a diffusion-based 3D reconstruction method. Zero-1-to-3 utilizes a LDM, originally designed for text-conditioned image generation, to generate new perspectives of an image based on a camera's extrinsic parameters like rotation and translation. Zero-1-to-3 leverages the geometric priors learned by large-scale LDMs, allowing the generation of novel views from a single image. Zero-1-to- 3 demonstrates strong zero-shot generalization capabilities, outperforming prior models in both single-view 3D reconstruction and novel view synthesis tasks. See Figure 9.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>传统的3D重建算法在很大程度上依赖于训练数据来捕捉体积的所有方面。然而，人类能够从单张图像中估计\(3\mathrm{D}\)表面。这个概念是哥伦比亚大学开发的Zero-1-to-3[5]框架的基础，该框架引入了一种基于扩散的3D重建方法。Zero-1-to-3利用了最初为文本条件图像生成设计的LDM，根据相机的外部参数（如旋转和平移）生成图像的新视角。Zero-1-to-3利用大规模LDM学习的几何先验，允许从单张图像生成新视图。Zero-1-to-3展示了强大的零样本泛化能力，在单视图3D重建和新视图合成任务中超越了先前的模型。见图9。</p></div><!-- Media --><!-- figureText: Input View (RGB) (R,T) Output View (RGB) Zero-1-to-3 Latent Diffusion Model Gaussian Noise --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_4.jpg?x=151&#x26;y=149&#x26;w=701&#x26;h=457&#x26;r=0"><p>Figure 9: High Level Picture of Zero-1-to-3 from the original paper</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图9：原始论文中的Zero-1-to-3高层次图示</p></div><!-- Media --><h2>Prior Work: Denoising Diffusion Probabilistic Models</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>先前工作：去噪扩散概率模型</h2></div><p>Denoising Diffusion Probabilistic Models (DDPMs)[1] are a class of generative models that transform data by gradually adding noise over a sequence of steps, then learning to reverse this process to generate new samples from noise.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>去噪扩散概率模型（DDPMs）[1]是一类生成模型，通过在一系列步骤中逐渐添加噪声来转换数据，然后学习逆转这一过程以从噪声中生成新样本。</p></div><p>Forward Process The forward process in DDPM is a Markov chain that gradually adds Gaussian noise to the data over \(T\) timesteps. The process can be mathematically described as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>前向过程DDPM中的前向过程是一个马尔可夫链，逐渐在\(T\)时间步上向数据添加高斯噪声。该过程可以用数学描述为：</p></div><p></p>\[{x}_{t} = \sqrt{{\alpha }_{t}}{x}_{t - 1} + \sqrt{1 - {\alpha }_{t}}\epsilon ,\;\epsilon  \sim  \mathcal{N}\left( {0,I}\right)\]<p></p><p>where \({x}_{0}\) is the original data, \({x}_{t}\) is the data at timestep \(t,\epsilon\) is the noise,and \({\alpha }_{t}\) is the variance schedule parameters that determine how much noise is added at each step.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\({x}_{0}\)是原始数据，\({x}_{t}\)是时间步\(t,\epsilon\)的数据，\({\alpha }_{t}\)是噪声，而\({\alpha }_{t}\)是决定每一步添加多少噪声的方差调度参数。</p></div><!-- Media --><!-- figureText: \( \sqrt{{\bar{\alpha }}_{t}} \) <icon/> ground input truth \( {x}_{0} \) --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_4.jpg?x=157&#x26;y=1361&#x26;w=702&#x26;h=197&#x26;r=0"><p>Figure 10: Forward Process of DDPM. Adapted from [4].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图10：DDPM的前向过程。改编自[4]。</p></div><!-- Media --><p>Reverse Process The reverse process aims to reconstruct the original data from the noise by learning a parameterized model \({p}_{\theta }\) . The reverse process is also a Markov chain,described as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>反向过程反向过程旨在通过学习一个参数化模型\({p}_{\theta }\)从噪声中重建原始数据。反向过程也是一个马尔可夫链，描述为：</p></div><p></p>\[{x}_{t - 1} = \frac{1}{\sqrt{{\alpha }_{t}}}\left( {{x}_{t} - \frac{1 - {\alpha }_{t}}{\sqrt{1 - {\bar{\alpha }}_{t}}}{\epsilon }_{\theta }\left( {{x}_{t},t}\right) }\right)  + {\sigma }_{t}z,\;z \sim  \mathcal{N} \tag{0,I}\]<p></p><p>where \({\epsilon }_{\theta }\left( {{x}_{t},t}\right)\) is a neural network predicting the noise, \({\sigma }_{t}\) is the standard deviation of the reverse process noise,and \({\bar{\alpha }}_{t} = \mathop{\prod }\limits_{{s = 1}}^{t}{\alpha }_{s}\) is the cumulative product of the \({\alpha }_{t}\) values.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\({\epsilon }_{\theta }\left( {{x}_{t},t}\right)\)是一个预测噪声的神经网络，\({\sigma }_{t}\)是反向过程噪声的标准差，\({\bar{\alpha }}_{t} = \mathop{\prod }\limits_{{s = 1}}^{t}{\alpha }_{s}\)是\({\alpha }_{t}\)值的累积乘积。</p></div><p>Training The training of DDPMs involves optimizing the parameters \(\theta\) of the neural network to minimize the difference between the noise predicted by the model and the actual noise added during the forward process. The loss function is typically the mean squared error between these two noise terms:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>训练DDPM的训练涉及优化神经网络的参数\(\theta\)，以最小化模型预测的噪声与在前向过程中添加的实际噪声之间的差异。损失函数通常是这两个噪声项之间的均方误差：</p></div><p></p>\[L\left( \theta \right)  = {\mathbb{E}}_{t,{x}_{0},\epsilon }\left\lbrack  {\begin{Vmatrix}\epsilon  - {\epsilon }_{\theta }\left( {x}_{t},t\right) \end{Vmatrix}}_{2}^{2}\right\rbrack\]<p></p><p>where \({x}_{t}\) is computed during the forward process and \(\epsilon\) is the Gaussian noise added at each step.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\({x}_{t}\)是在前向过程中计算的，\(\epsilon\)是在每一步添加的高斯噪声。</p></div><p>Sampling To generate new samples, the reverse process is initialized with pure noise \({x}_{T} \sim  \mathcal{N}\left( {0,I}\right)\) and iteratively applies the reverse steps to produce samples approximating the distribution of the original data \({x}_{0}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>采样 为了生成新样本，反向过程以纯噪声\({x}_{T} \sim  \mathcal{N}\left( {0,I}\right)\)初始化，并迭代应用反向步骤以生成近似原始数据分布的样本\({x}_{0}\)。</p></div><!-- Media --><p>Algorithm 1 DDPM Sampling Algorithm</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>算法 1 DDPM采样算法</p></div><hr><p>procedure DDPMSAMPLING \(\left( {\theta ,T,\left\{  {\alpha }_{t}\right\}  }\right)\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>过程 DDPMSAMPLING \(\left( {\theta ,T,\left\{  {\alpha }_{t}\right\}  }\right)\)</p></div><pre><code>Input: Trained model parameters \( \theta \) ,total timesteps
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>输入：训练模型参数\( \theta \)，总时间步
</code></pre></div><p>\(T\) ,noise schedule \(\left\{  {\alpha }_{t}\right\}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\(T\)，噪声调度\(\left\{  {\alpha }_{t}\right\}\)</p></div><pre><code>Output: A sample approximating the data distribu-
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>输出：一个近似数据分布的样本
</code></pre></div><p>tion</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"></div><pre><code>Initialize: Draw \( {x}_{T} \sim  \mathcal{N}\left( {0,I}\right) \) Start with pure
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>初始化：绘制\( {x}_{T} \sim  \mathcal{N}\left( {0,I}\right) \) 从纯噪声开始
</code></pre></div><p>noise</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>噪声</p></div><pre><code>for \( t = T \) down to 1 do
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>对于\( t = T \)从1到1进行
</code></pre></div><pre><code>	Calculate \( {\bar{\alpha }}_{t} = \mathop{\prod }\limits_{{s = 1}}^{t}{\alpha }_{s} \)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	计算\( {\bar{\alpha }}_{t} = \mathop{\prod }\limits_{{s = 1}}^{t}{\alpha }_{s} \)
</code></pre></div><pre><code>	Calculate \( {\sigma }_{t}^{2} = \frac{1 - {\bar{\alpha }}_{t - 1}}{1 - {\bar{\alpha }}_{t}} \cdot  \left( {1 - {\alpha }_{t}}\right) \)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	计算\( {\sigma }_{t}^{2} = \frac{1 - {\bar{\alpha }}_{t - 1}}{1 - {\bar{\alpha }}_{t}} \cdot  \left( {1 - {\alpha }_{t}}\right) \)
</code></pre></div><pre><code>	Predict noise \( {\epsilon }_{t} = {\epsilon }_{\theta }\left( {{x}_{t},t}\right) \)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	预测噪声\( {\epsilon }_{t} = {\epsilon }_{\theta }\left( {{x}_{t},t}\right) \)
</code></pre></div><pre><code>	if \( t > 1 \) then
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	如果\( t > 1 \)则
</code></pre></div><pre><code>		\( {x}_{t - 1} = \frac{1}{\sqrt{{\alpha }_{t}}}\left( {{x}_{t} - \frac{1 - {\alpha }_{t}}{\sqrt{1 - {\bar{\alpha }}_{t}}}{\epsilon }_{t}}\right)  + {\sigma }_{t}z,\;z \sim \)
</code></pre><p>\(\mathcal{N}\left( {0,I}\right)\)</p><pre><code>	else
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	否则
</code></pre></div><pre><code>		\( {x}_{0} = \frac{1}{\sqrt{{\alpha }_{t}}}\left( {{x}_{t} - \frac{1 - {\alpha }_{t}}{\sqrt{1 - {\bar{\alpha }}_{t}}}{\epsilon }_{t}}\right) \) Final denoising
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>		\( {x}_{0} = \frac{1}{\sqrt{{\alpha }_{t}}}\left( {{x}_{t} - \frac{1 - {\alpha }_{t}}{\sqrt{1 - {\bar{\alpha }}_{t}}}{\epsilon }_{t}}\right) \) 最终去噪
</code></pre></div><p>step</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>步骤</p></div><pre><code>	end if
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	结束如果
</code></pre></div><pre><code>end for
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>结束循环
</code></pre></div><pre><code>Return \( {x}_{0} \)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>返回 \( {x}_{0} \)
</code></pre></div><p>end procedure</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>结束过程</p></div><hr><!-- figureText: \( {x}_{t} \) \( {x}_{t - 1} \) Noise Predicter --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_4.jpg?x=933&#x26;y=1517&#x26;w=712&#x26;h=230&#x26;r=0"><p>Figure 11: Sampling Process. Adapted from [4].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图11：采样过程。改编自[4]。</p></div><!-- Media --><h2>Latent Diffusion Model in Zero-1-to-3</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>零-1到3的潜在扩散模型</h2></div><p>Latent Diffusion Models[13] proposed in 2021 are a type of generative model that combines the strengths of diffusion models and Variationsal Autoencoders(VAEs). Traditional DDPMs operates in the image pixel space, which requires more computation. LDMs compress the full image data space into a latent space before the diffusion and denosing process, improving efficiency and scalability in generating high-quality images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>2021年提出的潜在扩散模型（Latent Diffusion Models）[13]是一种生成模型，结合了扩散模型和变分自编码器（Variational Autoencoders, VAEs）的优点。传统的DDPM在图像像素空间中操作，这需要更多的计算。LDM在扩散和去噪过程之前将完整的图像数据空间压缩到潜在空间，从而提高生成高质量图像的效率和可扩展性。</p></div><!-- Media --><!-- figureText: Latent Space Conditioning Semantic Map Denoising U-Net \( {\epsilon }_{\theta } \) Text Repres entations Q Images \( K,V \) \( x \) Diffusion Process \( K,V \) \( {K}_{ \bullet  }V \) \( {z}_{T - 1} \) Pixel Space 17 --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_5.jpg?x=158&#x26;y=292&#x26;w=702&#x26;h=352&#x26;r=0"><p>Figure 12: The architecture of Latent Diffusion Model in the original paper.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图12：原始论文中潜在扩散模型的架构。</p></div><!-- Media --><p>Training Latent Diffusion Models \({\epsilon }_{\theta }\) LDM is trained in two main stages. First, a VAE is used to learn an encoding function \(E\left( x\right)\) and a decoding function \(D\left( z\right)\) ,where \(x\) represents the high-resolution image and \(z\) is it’s latent representation. The encoder compresses \(x\) to \(z\) ,and the decoder attempts to reconstruct \(x\) from \(z\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>训练潜在扩散模型\({\epsilon }_{\theta }\) LDM的训练分为两个主要阶段。首先，使用VAE学习编码函数\(E\left( x\right)\)和解码函数\(D\left( z\right)\)，其中\(x\)表示高分辨率图像，\(z\)是其潜在表示。编码器将\(x\)压缩为\(z\)，解码器则尝试从\(z\)重建\(x\)。</p></div><p>Training VAE The VAE optimizes the parameters \(\phi\) (encoder) and \(\psi\) (decoder) by minimizing the reconstruction loss combined with the KL-divergence loss:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>训练VAE VAE通过最小化重建损失与KL散度损失的组合来优化参数\(\phi\)（编码器）和\(\psi\)（解码器）：</p></div><p></p>\[{\mathcal{L}}_{VAE}\left( {\phi ,\psi }\right)  = {\mathbb{E}}_{{q}_{\phi }\left( {z \mid  x}\right) }\left\lbrack  {\log {p}_{\psi }\left( {x \mid  z}\right) }\right\rbrack   - {D}_{KL}\left( {{q}_{\phi }\left( {z \mid  x}\right) \parallel p\left( z\right) }\right)\]<p></p><p>(3)</p><!-- Media --><!-- figureText: \( \mathcal{N}\left( {0,1}\right) \) VAE Decoder Backpropagation VAE Encoder \( \mu \) \( \mu  + {\sigma \epsilon } \) 0 ELBO Loss --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_5.jpg?x=165&#x26;y=1129&#x26;w=680&#x26;h=308&#x26;r=0"><p>Figure 13: Training VAE. Figure from Lightning AI</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图13：训练VAE。图来自Lightning AI</p></div><!-- Media --><p>Training Attention-U-Net Denoiser In the second stage, an Attention-U-Net is trained as the denoising model in the latent space. This model learns a sequence of denois-ing steps that transform a sample from a noise distribution \(p\left( {z}_{T}\right)\) to the data distribution \(p\left( {z}_{0}\right)\) over \(\mathrm{T}\) timesteps. The U-Net model parameter \(\theta\) are optimized by minimizing the expected reverse KL-divergence between the true data distribution and the model distribution as follows:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>训练注意力U-Net去噪器在第二阶段，训练一个注意力U-Net作为潜在空间中的去噪模型。该模型学习一系列去噪步骤，将来自噪声分布\(p\left( {z}_{T}\right)\)的样本转换为数据分布\(p\left( {z}_{0}\right)\)，共\(\mathrm{T}\)个时间步。U-Net模型参数\(\theta\)通过最小化真实数据分布与模型分布之间的期望反向KL散度来优化，如下所示：</p></div><p></p>\[\mathcal{L}\left( \theta \right)  = {\mathbb{E}}_{{z}_{0},\epsilon  \sim  \mathcal{N}\left( {0,I}\right) ,t}\left\lbrack  {\begin{Vmatrix}\epsilon  - {\epsilon }_{\theta }\left( {z}_{t},t\right) \end{Vmatrix}}^{2}\right\rbrack   \tag{4}\]<p></p><p>where \({z}_{t} = \sqrt{{\bar{\alpha }}_{t}}{z}_{0} + \sqrt{1 - {\bar{\alpha }}_{t}}\epsilon\) ,and \({\bar{\alpha }}_{t}\) is the variance schedule. We use KL-divergence to calculate "how different" the true data distribution and the model distribution are.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\({z}_{t} = \sqrt{{\bar{\alpha }}_{t}}{z}_{0} + \sqrt{1 - {\bar{\alpha }}_{t}}\epsilon\)，\({\bar{\alpha }}_{t}\)是方差调度。我们使用KL散度来计算真实数据分布与模型分布之间的“差异”。</p></div><p>We aim to make the latent space distribution of the model similar to that of the real world. This is the key to generating realistic images. Figure 14 shows example of minimizing two distributions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的目标是使模型的潜在空间分布与现实世界的分布相似。这是生成真实图像的关键。图14展示了最小化两个分布的示例。</p></div><!-- Media --><!-- figureText: 0.10 \( P\left( X\right) \) 0.08 0.06 0.04 -15 -10 5 20 0.04 0.02 -10 10 --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_5.jpg?x=934&#x26;y=318&#x26;w=711&#x26;h=247&#x26;r=0"><p>Figure 14: On the left (bad) example, the difference between \(Q\left( x\right)\) distribution and \(P\left( x\right)\) is not minimized. On the right (good) example,the difference between \(Q\left( x\right)\) distribution and \(P\left( x\right)\) is minimized. (from [3])</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图14：左侧（差）示例中，\(Q\left( x\right)\)分布与\(P\left( x\right)\)之间的差异未被最小化。右侧（好）示例中，\(Q\left( x\right)\)分布与\(P\left( x\right)\)之间的差异被最小化。（来自[3]）</p></div><!-- figureText: Input Image \( {F}_{1} \times  {H}_{1} \times  {W}_{1} \times  {D}_{1} \) \( {F}_{1} \times  {H}_{2} \times  {W}_{2} \times  {D}_{2} \) \( {F}_{2} \times  {H}_{2} \times  {W}_{2} \times  {D}_{2} \) \( {F}_{3} \times  {H}_{3} \times  {W}_{3} \times  {D}_{3} \) \( {F}_{2} \times  {H}_{2} \times  {W}_{2} \times  {D}_{2} \) \( {F}_{1} \times  {H}_{2} \times  {W}_{2} \times  {D}_{2} \) \( {V}_{c} \times  {H}_{1} \times  {W}_{1} \times  {D}_{1} \) (Conv \( 3 \times  3 \times  3 + \) ReLU) \( ( \times  2 \) Upsampling (by 2) Skip Connection Gating Signal (Query) Concatenatior Attention Gate --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_5.jpg?x=938&#x26;y=774&#x26;w=701&#x26;h=328&#x26;r=0"><p>Figure 15: Attention-U-Net Architecture in the original paper</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图15：原始论文中的Attention-U-Net架构</p></div><!-- figureText: VAE Latent Space Estimate \( {t}_{i} \) Condition Condition Condition Embeddinc Attention UNet Backpropagation \( {t}_{i} \) Generate Gaussian Noise Loss --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_5.jpg?x=933&#x26;y=1245&#x26;w=709&#x26;h=415&#x26;r=0"><p>Figure 16: Training Attention-U-Net Denoiser. Figure from Lightning AI</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图16：训练Attention-U-Net去噪器。来自Lightning AI的图</p></div><!-- Media --><p>Conditioning on Camera Parameters The third stage in the Zero-1-to-3 framework focuses on the conditioning of the LDM based on camera extrinsic parameters such as rotation(R)and translation(t). This conditioning is critical for generating novel views of the object, which are essential for effective \(3\mathrm{D}\) reconstruction from a single image.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于相机参数的条件化 零-1到3框架的第三阶段专注于基于相机外部参数（如旋转(R)和位移(t)）对LDM进行条件化。这种条件化对于生成物体的新视角至关重要，这对于从单张图像有效重建\(3\mathrm{D}\)是必不可少的。</p></div><p>Mechanism of Conditioning In this stage, the previously trained latent representations are manipulated according to the desired camera transformations to simulate new perspectives. This process involves:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>条件化机制 在这一阶段，之前训练的潜在表示根据所需的相机变换进行调整，以模拟新的视角。这个过程包括：</p></div><ul>
<li>Camera Transformations: Adjusting the latent variables \(z\) to reflect changes in viewpoint due to different rotations \(R\) and translations \(t\) .</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>相机变换：调整潜在变量\(z\)以反映由于不同旋转\(R\)和位移\(t\)而导致的视点变化。</li>
</ul></div><ul>
<li>Transformation Implementation: This could be achieved either through a learned transformation model within the LDM framework or by applying predefined transformation matrices directly to the latent vectors.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>变换实现：这可以通过在LDM框架内学习的变换模型实现，或者通过将预定义的变换矩阵直接应用于潜在向量。</li>
</ul></div><p>Training the Model for Conditional Output The model is further trained to handle conditional outputs effectively, which involves:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为条件输出训练模型 该模型进一步训练以有效处理条件输出，这涉及：</p></div><ul>
<li>Data Preparation: The official code used the RTMV dataset[15] where objects are captured from multiple viewpoints to pair latent representations with corresponding camera parameters.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>数据准备：官方代码使用RTMV数据集[15]，该数据集从多个视点捕获物体，以将潜在表示与相应的相机参数配对。</li>
</ul></div><ul>
<li>Model Adaptation: Extending the latent diffusion model training to not only generate images from the latent representation \(z\) but also new perspectives of the images from it’s transformed latent representation \({z}^{\prime }\) .</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>模型适应：扩展潜在扩散模型的训练，不仅生成来自潜在表示\(z\)的图像，还生成来自其变换潜在表示\({z}^{\prime }\)的新视角图像。</li>
</ul></div><ul>
<li>MSE Loss: We compute the MSE between the output image and real image with respect to \(R\) and \(t\) . A "near-view consistency loss" that calculate the MSE between the image rendered from a view and the image rendered from a nearby view is also used to maintain the consistency in 3D reconstruction.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>MSE损失：我们计算输出图像与真实图像之间关于\(R\)和\(t\)的均方误差(MSE)。还使用“近视一致性损失”，计算从一个视角渲染的图像与从附近视角渲染的图像之间的MSE，以保持3D重建的一致性。</li>
</ul></div><p>Novel View generation To generate a novel view, the following transformation is applied to the latent space:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>新视角生成 为了生成新视角，以下变换应用于潜在空间：</p></div><p></p>\[{z}^{\prime } = f\left( {z,R,t}\right)  \tag{5}\]<p></p><p>Where \(f\) is the transformation function that modifies \(z\) based on \(R\) and \(t\) . The Zero-1-to-3 model generates the novel view as follows:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(f\)是根据\(R\)和\(t\)修改\(z\)的变换函数。零-1到3模型生成新视角如下：</p></div><p></p>\[{x}^{\prime } = D\left( {{\epsilon }_{\theta }\left( {z}^{\prime }\right) }\right)  \tag{6}\]<p></p><p>Where \({x}^{\prime }\) represents the image generated from the new perspective,and \(D\) is the decoder part of the LDM that synthesizes the final image output from the transformed latent representation \({z}^{\prime }\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\({x}^{\prime }\)表示从新视角生成的图像，而\(D\)是LDM的解码器部分，从变换后的潜在表示\({z}^{\prime }\)合成最终图像输出。</p></div><p>3D reconstruction \({\epsilon }_{\theta }\) The 3D reconstruction is performed by first generating multiple views of the object using the above method for various \(R\) and \(t\) matrices. Each generated image \({x}^{\prime }\) provides a different perspective of the object. These images are then used to reconstruct the 3D model:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D重建\({\epsilon }_{\theta }\) 3D重建通过首先使用上述方法为各种\(R\)和\(t\)矩阵生成物体的多个视角来执行。每个生成的图像\({x}^{\prime }\)提供物体的不同视角。这些图像随后用于重建3D模型：</p></div><p></p>\[\text{3D Model} = \text{Integrate}\left( \left\{  {x}^{\prime }\right\}  \right)  \tag{7}\]<p></p><p>The integration process typically involves techniques like volumetric fusion or multi-view stereo algorithms, which consolidate the information from different images to create a detailed \(3\mathrm{D}\) representation of the object,as shown in figure 17.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>集成过程通常涉及体积融合或多视图立体算法等技术，这些技术整合来自不同图像的信息，以创建物体的详细\(3\mathrm{D}\)表示，如图17所示。</p></div><!-- Media --><!-- figureText: (SAM: Segment Anything Rotation \( \left( {R}_{1}\right) , \) Translation \( \left( {T}_{1}\right) \) Rotation \( \left( {R}_{2}\right) , \) Translation \( \left( {T}_{2}\right) \) Zero-1-to-3 Rotation \( \left( {\mathrm{R}}_{3}\right) \) ,Translation \( \left( {\mathrm{T}}_{3}\right) \) Rotation \( \left( {\mathrm{R}}_{4}\right) \) ,Translation \( \left( {\mathrm{T}}_{4}\right) \) with neural deferred shading 3D mesh (3D shape formed by vertices and edges) --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_6.jpg?x=981&#x26;y=143&#x26;w=668&#x26;h=363&#x26;r=0"><p>Figure 17: Demo of 3D reconstruction from Zero-1-to-3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图17：从零-1到3的3D重建演示</p></div><!-- Media --><h2>Limitations for diffusion-based and NeRF-based 3D reconstruction</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>基于扩散和NeRF的3D重建的局限性</h2></div><ul>
<li>Flexibility and Real-Time 3D scene Rendering: Training the Zero-1-to-3 model for 3D scenes reconstruction typically require iterative denoising processes during sampling, which can be computationally intensive and slow. This makes them less suitable for applications requiring real-time rendering.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>灵活性和实时3D场景渲染：训练Zero-1-to-3模型进行3D场景重建通常需要在采样过程中进行迭代去噪，这可能会消耗大量计算资源并且速度较慢。这使得它们不太适合需要实时渲染的应用。</li>
</ul></div><ul>
<li>Implicit Representation Ambiguity: Both NeRF and Diffusion models represent the 3D object implicitly; they do not explicitly construct the 3D space. NeRFs utilize the weights of an MLP to represent a 3D scene and LDMs use the latent space for new perspectives generation for 3D reconstruction. While implicit representation saves significant space it may lead to ambiguities in interpreting the model.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>隐式表示歧义：NeRF（神经辐射场）和扩散模型都隐式地表示三维物体；它们并不明确构建三维空间。NeRF利用多层感知器（MLP）的权重来表示三维场景，而LDM（潜在扩散模型）则使用潜在空间生成新视角以进行三维重建。虽然隐式表示节省了大量空间，但可能导致对模型的解释产生歧义。</li>
</ul></div><h2>Approach: 3D Gaussian Splatting</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>方法：3D高斯点云渲染</h2></div><h2>Background</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>背景</h2></div><p>Throughout the evolution of \(3\mathrm{D}\) scene reconstruction,explicit representations such as meshes and point clouds have always been preferred by developers and researchers due to their clearly defined structure, fast rendering, and ease of editing. NeRF-based methods have shifted towards continuous representation of \(3\mathrm{D}\) scenes. While the continuous nature of these methods helps optimization, the stochastic sampling required for rendering is costly and can result in noise [2]. On top of that the implicit representation's lack of geometric information does not lend itself well to editing.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在\(3\mathrm{D}\)场景重建的发展过程中，开发者和研究人员始终偏好显式表示，如网格和点云，因为它们具有明确的结构、快速的渲染和易于编辑的特点。基于NeRF的方法已转向对\(3\mathrm{D}\)场景的连续表示。尽管这些方法的连续特性有助于优化，但渲染所需的随机采样成本高昂，并可能导致噪声[2]。此外，隐式表示缺乏几何信息，不利于编辑。</p></div><h2>Overview</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>概述</h2></div><p>3D Gaussian Splatting provides a high-quality novel view with real-time rendering for \(3\mathrm{D}\) scenes,these are achieved with the utilization of the Gaussian function to present a smooth and accurate texture using captured photos of a scene.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D高斯点云渲染为\(3\mathrm{D}\)场景提供了高质量的新视角和实时渲染，这通过利用高斯函数来呈现平滑且准确的纹理，使用捕获的场景照片实现。</p></div><p>To reconstruct a 3D model using 3D Gaussian splatting, first, a video of the object is taken from different angles, then converted into frames representing static scenes at different camera angles. Structure from Motion (SfM) with feature detection and matching techniques such as SIFT is then applied to these images to produce a sparse point cloud. The \(3\mathrm{D}\) data points of the point cloud are then represented by \(3\mathrm{D}\) Gaussians. With the optimization process, adaptive density control of Gaussians, and high-efficiency algorithm design, realistic views of the 3D model can be reconstructed with a high frame rate.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>要使用3D高斯点云重建3D模型，首先需要从不同角度拍摄物体的视频，然后将其转换为表示不同相机角度的静态场景帧。接着，应用结构光束法（Structure from Motion, SfM）及特征检测和匹配技术，如SIFT（尺度不变特征变换），对这些图像进行处理，以生成稀疏点云。点云的\(3\mathrm{D}\)数据点随后由\(3\mathrm{D}\)高斯分布表示。通过优化过程、自适应高斯密度控制和高效算法设计，可以以高帧率重建出3D模型的真实视图。</p></div><!-- Media --><!-- figureText: Initialization Projection Differentiable Tile Rasterizer Adaptive \( \rightarrow \) Operation Flow \( \rightarrow \) Gradient Flow --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_7.jpg?x=152&#x26;y=151&#x26;w=715&#x26;h=145&#x26;r=0"><p>Figure 18: Overview of 3D Gaussian Splatting process.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图18：3D高斯溅射过程概述。</p></div><!-- Media --><p>The algorithm of 3D Gaussian Splatting is demonstrated below, where it can be slit into 3 parts: initialization, optimization, and density control of Gaussians.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>下面展示了3D高斯溅射（3D Gaussian Splatting）算法，它可以分为三个部分：初始化、优化和高斯密度控制。</p></div><h2>Algorithm</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>算法</h2></div><!-- Media --><p>Algorithm 2 Optimization and Densification</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>算法 2 优化与稠密化</p></div><p>\(w,h\) : width and height of the training images</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\(w,h\) : 训练图像的宽度和高度</p></div><hr><p>\(M \leftarrow\) SfM Points > Positions</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\(M \leftarrow\) SfM 点 > 位置</p></div><p>\(i \leftarrow  0\) \(\vartriangleright\) Iteration Count</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\(i \leftarrow  0\) \(\vartriangleright\) 迭代次数</p></div><p>while not converged do</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>当未收敛时</p></div><pre><code>\( V,\widehat{I} \leftarrow \) SampleTrainingView(   ) \( \; \vartriangleright \) Camera \( V \) and
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>\( V,\widehat{I} \leftarrow \) 样本训练视图(   ) \( \; \vartriangleright \) 相机 \( V \) 和
</code></pre></div><p>Image</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图像</p></div><pre><code>\( I \leftarrow \) Rasterize \( \left( {M,S,C,A,V}\right) \; \vartriangleright \) Alg. 3
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>\( I \leftarrow \) 光栅化 \( \left( {M,S,C,A,V}\right) \; \vartriangleright \) 算法 3
</code></pre></div><pre><code>\( L \leftarrow  \operatorname{Loss}\left( {I,\widehat{I}}\right) \; \vartriangleright \) Loss
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>\( L \leftarrow  \operatorname{Loss}\left( {I,\widehat{I}}\right) \; \vartriangleright \) 损失
</code></pre></div><pre><code>\( M,S,C,A \leftarrow  \operatorname{Adam}\left( {\nabla L}\right) \; \vartriangleright \) Backprop &#x26;Step
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>\( M,S,C,A \leftarrow  \operatorname{Adam}\left( {\nabla L}\right) \; \vartriangleright \) 反向传播与步骤
</code></pre></div><pre><code>if IsRefinementIteration(i)then
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>如果是细化迭代(i)则
</code></pre></div><pre><code>	for all Gaussians \( \left( {\mu ,\sum ,c,\alpha }\right) \) in(M,S,C,A)do
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	对于所有高斯 \( \left( {\mu ,\sum ,c,\alpha }\right) \) 在(M,S,C,A)中
</code></pre></div><pre><code>		if \( \alpha  &#x3C; \epsilon \) or IsTooLarge \( \left( {\mu ,\sum }\right) \) then \( \vartriangleright \) Pruning
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>		如果 \( \alpha  &#x3C; \epsilon \) 或者过大 \( \left( {\mu ,\sum }\right) \) 则 \( \vartriangleright \) 剪枝
</code></pre></div><pre><code>			RemoveGaussian(   )
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>			移除高斯(   )
</code></pre></div><pre><code>		end if
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>		结束如果
</code></pre></div><pre><code>		if \( {\nabla }_{p}L > {\tau }_{p} \) then \( \; \vartriangleright \) Densification
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>		如果 \( {\nabla }_{p}L > {\tau }_{p} \) 则 \( \; \vartriangleright \) 密集化
</code></pre></div><pre><code>			if \( \parallel S\parallel  > {\tau }_{S} \) then \( \vartriangleright \) Over-reconstruction
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>			如果 \( \parallel S\parallel  > {\tau }_{S} \) 则 \( \vartriangleright \) 过度重建
</code></pre></div><pre><code>				SplitGaussian \( \left( {\mu ,\sum ,c,\alpha }\right) \)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>				分裂高斯 \( \left( {\mu ,\sum ,c,\alpha }\right) \)
</code></pre></div><pre><code>			else - Under-reconstruction
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>			否则 - 不足重建
</code></pre></div><pre><code>				CloneGaussian \( \left( {\mu ,\sum ,c,\alpha }\right) \)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>				克隆高斯 \( \left( {\mu ,\sum ,c,\alpha }\right) \)
</code></pre></div><pre><code>			end if
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>			结束如果
</code></pre></div><pre><code>		end if
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>		结束如果
</code></pre></div><pre><code>	end for
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	结束循环
</code></pre></div><pre><code>end if
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>结束如果
</code></pre></div><pre><code>\( i \leftarrow  i + 1 \)
</code></pre><p>end while</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>结束当</p></div><hr><!-- Media --><p>Initialization Points in the sparse \(3\mathrm{D}\) data point cloud generated by SfM, are initialized to 3D Gaussians. The Gaussians are defined by the following variables: position \(p\) , world space 3D covariance matrix \(\sum\) ,opacity \(\alpha\) ,and spherical harmonics coefficient c (representation of RBG color), given formula:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>由SfM生成的稀疏\(3\mathrm{D}\)数据点云中的初始化点被初始化为3D高斯分布。高斯分布由以下变量定义：位置\(p\)、世界空间3D协方差矩阵\(\sum\)、不透明度\(\alpha\)和球谐系数c（RGB颜色的表示），给定公式：</p></div><p></p>\[G\left( x\right)  = {e}^{-\frac{1}{2}{\left( x\right) }^{T}{\sum }^{-1}\left( x\right) } \tag{8}\]<p></p><p>Optimization Initially, the Gaussians are sparse and not representative, but they are gradually optimized to better represent the scene. To do this,a random camera view \(V\) with it’s corresponding image \(\widehat{I}\) is chosen from the training set. A rasterized Gaussian image \(I\) is generated by passing the Gaussian means, \(\sum ,c,\alpha\) ,and \(V\) to a differentiable ras-terizer function Rasterize(   ).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>优化最初，高斯分布是稀疏且不具代表性的，但它们逐渐被优化以更好地表示场景。为此，从训练集中选择一个随机相机视图\(V\)及其对应的图像\(\widehat{I}\)。通过将高斯均值\(\sum ,c,\alpha\)和\(V\)传递给可微分光栅化函数Rasterize( )生成光栅化的高斯图像\(I\)。</p></div><p>A loss function, shown in equation (9), is used to compute the gradients of the two images \(\widehat{I}\) and \(I\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>损失函数，如公式（9）所示，用于计算两幅图像\(\widehat{I}\)和\(I\)的梯度。</p></div><p></p>\[\mathcal{L} = \left( {1 - \lambda }\right) {\mathcal{L}}_{1} + \lambda {\mathcal{L}}_{\text{D-SSIM }} \tag{9}\]<p></p><p>Here, \({\mathcal{L}}_{1}\) is the Mean Absolute Error of \(\widehat{I}\) and \(I\) , \({\mathcal{L}}_{D - {SSMI}}\) is the Difference Structural Similarity Index based loss,and \(\lambda\) is a pre-defined weight that adjusts the contribution of \({\mathcal{L}}_{1}\) and \({\mathcal{L}}_{D - {SSMI}}\) to the final loss \(\mathcal{L}\) . The parameters of the Gaussians are adjusted accordingly with the Adam optimizer [2].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这里，\({\mathcal{L}}_{1}\)是\(\widehat{I}\)和\(I\)的平均绝对误差，\({\mathcal{L}}_{D - {SSMI}}\)是基于差异结构相似性指数的损失，\(\lambda\)是一个预定义的权重，用于调整\({\mathcal{L}}_{1}\)和\({\mathcal{L}}_{D - {SSMI}}\)对最终损失\(\mathcal{L}\)的贡献。高斯分布的参数相应地使用Adam优化器进行调整[2]。</p></div><p>Adaptive control of Gaussians After initialization, an adaptive approach is used to control the number and density of Guassians. Adaptive control refers to automatically adjusting the size and number of Gaussians to optimize the representation of the static \(3\mathrm{D}\) scene. The adaptive density control follows the following behaviors (see Fig. 12):</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>高斯的自适应控制初始化后，采用自适应方法控制高斯的数量和密度。自适应控制是指自动调整高斯的大小和数量，以优化静态\(3\mathrm{D}\)场景的表示。自适应密度控制遵循以下行为（见图12）：</p></div><ul>
<li>Gaussian Removal: For every 100 iterations, if the Gaussians are too large in the 3D space or have opacity under a defined threshold of opacity \({\epsilon }_{\alpha }\) (essentially transparent), they are removed.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>高斯移除：每100次迭代，如果高斯在3D空间中过大或不透明度低于定义的不透明度阈值\({\epsilon }_{\alpha }\)（基本上是透明的），则将其移除。</li>
</ul></div><ul>
<li>Gaussian Duplication: For regions filled by gaussians that are greater than the defined threshold but is too small, the Gaussians are cloned and moved along their direction to cover the empty space. This behavior adaptively and gradually increases the number and volume of the Gaus-sians until the area is well-fitted.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>高斯复制：对于填充的高斯区域，如果大于定义的阈值但过小，则克隆高斯并沿其方向移动以覆盖空白区域。这种行为自适应且逐渐增加高斯的数量和体积，直到区域适配良好。</li>
</ul></div><ul>
<li>Gaussian Split: For regions that are over-reconstructed by large Gaussians (variance is too high), they are split into smaller Gaussians by a factor \(\phi\) ,the original paper used \(\phi  = {1.6}\) .</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>高斯分裂：对于由大高斯（方差过高）过度重建的区域，将其按因子\(\phi\)分裂为更小的高斯，原始论文使用了\(\phi  = {1.6}\)。</li>
</ul></div><!-- Media --><!-- figureText: Under- Reconstruction Clone Optimization Continues Optimization Continues \( \mathbf{{Over} - } \) Reconstruction Split --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_7.jpg?x=941&#x26;y=1345&#x26;w=670&#x26;h=392&#x26;r=0"><p>Figure 19: Gaussians split in over-reconstructed areas while clone in under-reconstructed areas.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图19：在过度重建区域中高斯分裂，而在重建不足区域中克隆。</p></div><!-- Media --><p>Due to the splitting and duplicating, the number of Gaus-sians increases. To address this,every Gaussians’ opacity \(\alpha\) close to zero every \(N = {3000}\) iteration,after the optimization step then increases the \(\alpha\) values for the Guassian where this is needed while allowing the unused ones to be removed.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>由于分裂和复制，高斯的数量增加。为了解决这个问题，每个高斯的不透明度\(\alpha\)在每\(N = {3000}\)次迭代中接近零，优化步骤后再增加需要的高斯的\(\alpha\)值，同时允许未使用的高斯被移除。</p></div><h2>Gradient computation of \(\sum\) and \({\sum }^{\prime }\)</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>\(\sum\)和\({\sum }^{\prime }\)的梯度计算</h2></div><p>Each Gaussian is represented as an ellipsoid in the 3D space, modelled by a covariance matrices \(\sum\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>每个高斯分布在三维空间中表示为一个椭球体，由协方差矩阵 \(\sum\) 建模。</p></div><p></p>\[\sum  = {RS}{S}^{T}{R}^{T} \tag{10}\]<p></p><p>Where \(\mathrm{S}\) is the scaling matrix and \(\mathrm{R}\) is the rotation matrix.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(\mathrm{S}\) 是缩放矩阵，\(\mathrm{R}\) 是旋转矩阵。</p></div><p>For each camera angle, 3D Gaussians are rasterized to the \(2\mathrm{D}\) screen. The \(3\mathrm{D}\) covariance matrix \({\sum }^{\prime }\) is derived using the viewing transformation matrix \(W\) and Jacobian \(J\) ,which approximates the projective transformation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>对于每个相机角度，三维高斯分布被光栅化到 \(2\mathrm{D}\) 屏幕上。协方差矩阵 \(3\mathrm{D}\) \({\sum }^{\prime }\) 是通过视图变换矩阵 \(W\) 和雅可比矩阵 \(J\) 推导而来的，后者近似于投影变换。</p></div><p></p>\[{\sum }^{\prime } = {JW\sum }{W}^{T}{J}^{T} \tag{11}\]<p></p><p>\({\sum }^{\prime }\) ,a 3x3 matrix,can be simplified by ignoring the elements in the third row and column while retaining the properties of their corresponding planar points representation [19].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({\sum }^{\prime }\)，一个 3x3 矩阵，可以通过忽略第三行和第三列的元素，同时保留其对应平面点表示的属性来简化。</p></div><p>To compute the gradient of the 3D space covariance, the chain rule is applied to \(\sum\) and \({\sum }^{\prime }\) with reference to their rotation \(q\) and scaling \(s\) . This results in the expressions \(\frac{d{\sum }^{\prime }}{ds} = \frac{d{\sum }^{\prime }}{d\sum }\frac{d\sum }{ds}\) and \(\frac{d{\sum }^{\prime }}{dq} = \frac{d{\sum }^{\prime }}{d\sum }\frac{d\sum }{dq}\) . By substituting \(U = {JW}\) into the equation for \({\sum }^{\prime }\) ,we get \({\sum }^{\prime } = {U\sum }{U}^{T}\) . This equation allows the partial derivative of each element in \(\frac{d{\sum }^{\prime }}{d\sum }\) to be represented in terms of \(U\) :</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了计算三维空间协方差的梯度，链式法则应用于 \(\sum\) 和 \({\sum }^{\prime }\)，参考它们的旋转 \(q\) 和缩放 \(s\)。这导致了表达式 \(\frac{d{\sum }^{\prime }}{ds} = \frac{d{\sum }^{\prime }}{d\sum }\frac{d\sum }{ds}\) 和 \(\frac{d{\sum }^{\prime }}{dq} = \frac{d{\sum }^{\prime }}{d\sum }\frac{d\sum }{dq}\)。通过将 \(U = {JW}\) 代入 \({\sum }^{\prime }\) 的方程，我们得到 \({\sum }^{\prime } = {U\sum }{U}^{T}\)。这个方程允许将 \(\frac{d{\sum }^{\prime }}{d\sum }\) 中每个元素的偏导数表示为 \(U\)：</p></div><p></p>\[\frac{\partial {\sum }^{\prime }}{\partial {\sum }_{ij}} = \left( \begin{array}{ll} {U}_{1,i}{U}_{1,j} &#x26; {U}_{1,i}{U}_{2,j} \\  {U}_{1,j}{U}_{2,i} &#x26; {U}_{2,i}{U}_{2,j} \end{array}\right)\]<p></p><p>By substituting \(M = {RS}\) into equation (10),it can then be rewritten as \(\sum  = M{M}^{T}\) . Using the chain rule,we can derive \(\frac{d\sum }{ds} = \frac{d\sum }{dM}\frac{dM}{ds}\) and \(\frac{d\sum }{dq} = \frac{d\sum }{dM}\frac{dM}{dq}\) . This allows us to calculate the scaling gradient at position(i,j)of the covariance matrix with:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>通过将 \(M = {RS}\) 代入方程 (10)，它可以被重写为 \(\sum  = M{M}^{T}\)。使用链式法则，我们可以推导出 \(\frac{d\sum }{ds} = \frac{d\sum }{dM}\frac{dM}{ds}\) 和 \(\frac{d\sum }{dq} = \frac{d\sum }{dM}\frac{dM}{dq}\)。这使我们能够计算协方差矩阵在位置 (i,j) 的缩放梯度：</p></div><p></p>\[\frac{\partial {M}_{i,j}}{\partial {s}_{k}} = \left\{  \begin{matrix} {R}_{i,k} &#x26; \text{ if }j = k \\  0 &#x26; \text{ otherwise } \end{matrix}\right.\]<p></p><p>For defining derivatives of \(M\) with respect to rotation matrix \(R\) in terms of quaternion \(q\) components,the following formula demonstrating how quaternion components affect \(R\) is involved:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了定义关于旋转矩阵 \(R\) 的 \(M\) 的导数，以四元数 \(q\) 组件为基础，涉及以下公式，展示了四元数组件如何影响 \(R\)：</p></div><p></p>\[R\left( q\right)  = 2\left( \begin{array}{lll} \frac{1}{2} - \left( {{q}_{j}^{2} + {q}_{k}^{2}}\right) &#x26; \left( {{q}_{i}{q}_{j} - {q}_{r}{q}_{k}}\right) &#x26; \left( {{q}_{i}{q}_{k} + {q}_{r}{q}_{j}}\right) \\  \left( {{q}_{i}{q}_{j} + {q}_{r}{q}_{k}}\right) &#x26; \frac{1}{2} - \left( {{q}_{i}^{2} + {q}_{k}^{2}}\right) &#x26; \left( {{q}_{j}{q}_{k} - {q}_{r}{q}_{i}}\right) \\  \left( {{q}_{i}{q}_{k} - {q}_{r}{q}_{j}}\right) &#x26; \left( {{q}_{j}{q}_{k} + {q}_{r}{q}_{i}}\right) &#x26; \frac{1}{2} - \left( {{q}_{i}^{2} + {q}_{j}^{2}}\right)  \end{array}\right)  \tag{12}\]<p></p><p>And therefore the gradient \(\frac{\partial M}{\partial {q}_{x}}\) for 4 components of quaternion \(r,i,j,k\) can be calculated as follow:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>因此，四元数 \(r,i,j,k\) 的 4 个组件的梯度 \(\frac{\partial M}{\partial {q}_{x}}\) 可以按如下方式计算：</p></div><p></p>\[\frac{\partial M}{\partial {q}_{r}} = 2\left( \begin{matrix} 0 &#x26;  - {s}_{y}{q}_{k} &#x26; {s}_{z}{q}_{j} \\  {s}_{x}{q}_{k} &#x26; 0 &#x26;  - {s}_{z}{q}_{i} \\   - {s}_{x}{q}_{j} &#x26; {s}_{y}{q}_{i} &#x26; 0 \end{matrix}\right) ,\;\frac{\partial M}{\partial {q}_{i}} = 2\left( \begin{matrix} 0 &#x26; {s}_{y}{q}_{j} &#x26; {s}_{z}{q}_{k} \\  {s}_{x}{q}_{j} &#x26;  - 2{s}_{y}{q}_{i} &#x26;  - {s}_{z}{q}_{r} \\  {s}_{x}{q}_{k} &#x26; {s}_{y}{q}_{r} &#x26;  - 2{s}_{z}{q}_{i} \end{matrix}\right)\]<p></p><p></p>\[\frac{\partial M}{\partial {q}_{j}} = 2\left( \begin{matrix}  - 2{s}_{x}{q}_{j} &#x26; {s}_{y}{q}_{i} &#x26; {s}_{z}{q}_{r} \\  {s}_{x}{q}_{i} &#x26; 0 &#x26; {s}_{z}{q}_{k} \\   - {s}_{x}{q}_{r} &#x26; {s}_{y}{q}_{k} &#x26;  - 2{s}_{z}{q}_{j} \end{matrix}\right) ,\;\frac{\partial M}{\partial {q}_{k}} = 2\left( \begin{matrix}  - 2{s}_{x}{q}_{k} &#x26;  - {s}_{y}{q}_{r} &#x26; {s}_{z}{q}_{i} \\  {s}_{x}{q}_{r} &#x26;  - 2{s}_{y}{q}_{k} &#x26; {s}_{z}{q}_{j} \\  {s}_{x}{q}_{i} &#x26; {s}_{y}{q}_{j} &#x26; 0 \end{matrix}\right)\]<p></p><p>(13)</p><h2>Tile-based rasterizer for real-time rendering</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>基于瓦片的光栅化器用于实时渲染</h2></div><p>A technique called Tile-based Rasterizer is used to quickly render the 3D model constructed by the Gaussians (see Fig 13). This approach first uses a given view angle \(V\) of the camera and its position \(p\) to filter out the Gaussians that are not contributing to the view frustum. In this way only the useful Gaussians are involved in the \(\alpha\) -blending,improving the rendering efficiency.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>一种称为基于瓦片的光栅化器的技术被用来快速渲染由高斯分布构建的三维模型（见图 13）。这种方法首先使用相机的给定视角 \(V\) 和其位置 \(p\) 来过滤掉不贡献于视锥体的高斯分布。通过这种方式，只有有用的高斯分布参与 \(\alpha\) 混合，从而提高渲染效率。</p></div><!-- Media --><!-- figureText: tiles GPU \( \begin{array}{llll} {t1} & {t2} & {t3} & {t4} \end{array} \) 3D gaussians \( \{ {p1},{p2},{p5},{p4},{p3}\} \) sorting --><img src="https://cdn.noedgeai.com/bo_d163lkref24c73d1lceg_8.jpg?x=935&#x26;y=174&#x26;w=707&#x26;h=235&#x26;r=0"><p>Figure 20: Overview of Tile-based rasterize areas, where green Gaussians are contributing to the view frustum, and red Gaussians are not,given camera angle \(V\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图 20：基于瓦片的光栅化区域概述，其中绿色高斯分布对视锥体有贡献，红色高斯分布则没有，给定相机角度 \(V\)。</p></div><!-- Media --><p>Instead of sorting all the Gaussian individually for per-pixel \(\alpha\) -blending,tiles are created to divide the 2D screen into smaller 16x16 sections. An instance key is assigned to each of the Gaussians using their corresponding view-space depth and the tiles they reside with a tile ID. The Gaussians are then sorted according to their instance key using a GPU Radix sort. The Radix sort is capable of handling large sets of data in parallel with GPU threads. The result of the Gaussian sorting can also demonstrate the depth level of tiles.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>与其对每个像素进行单独排序所有高斯分布以进行 \(\alpha\) 混合，不如创建瓦片将二维屏幕划分为更小的 16x16 区域。每个高斯分布使用其对应的视空间深度和它们所在的瓦片 ID 被分配一个实例键。然后，高斯分布根据其实例键使用 GPU 基数排序进行排序。基数排序能够与 GPU 线程并行处理大量数据集。高斯分布排序的结果还可以展示瓦片的深度级别。</p></div><p>After sorting, a thread block is assigned to each tile, and the Gaussians are loaded into the corresponding memory. \(\alpha\) - blending is then performed from front to back on the sorted list of Gaussians onto the \(2\mathrm{D}\) scene,using the cumulative color and opacity \(\alpha\) of the Gaussians until each pixel reaches a target alpha saturation. This design maximizes computation efficiency by enabling parallelism of both tile rendering and Gaussian loading in the shared memory space.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>排序后，为每个瓦片分配一个线程块，并将高斯分布加载到相应的内存中。然后，从前到后对排序后的高斯分布进行 \(\alpha\) 混合到 \(2\mathrm{D}\) 场景中，使用高斯分布的累积颜色和不透明度 \(\alpha\)，直到每个像素达到目标 alpha 饱和度。该设计通过在共享内存空间中实现瓦片渲染和高斯加载的并行性，最大化计算效率。</p></div><!-- Media --><p>Algorithm 3 GPU software rasterization of 3D Gaussians</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>算法 3 GPU 软件光栅化三维高斯分布</p></div><p>\(w,h\) : width and height of the image to rasterize</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\(w,h\)：要光栅化的图像的宽度和高度</p></div><p>\(M,S\) : Gaussian means and covariances in world space</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\(M,S\) : 世界空间中的高斯均值和协方差</p></div><p>\(C,A\) : Gaussian colors and opacities</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\(C,A\) : 高斯颜色和不透明度</p></div><p>\(V\) : view configuration of current camera</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\(V\) : 当前相机的视图配置</p></div><hr><p>function RASTERIZE(w,h,M,S,C,A,V)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>函数 RASTERIZE(w,h,M,S,C,A,V)</p></div><pre><code>	CullGaussian \( \left( {p,V}\right) \; \vartriangleright \) Frustum Culling
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	CullGaussian \( \left( {p,V}\right) \; \vartriangleright \) 视锥剔除
</code></pre></div><pre><code>	\( {M}^{\prime },{S}^{\prime } \leftarrow \) ScreenspaceGaussians \( \left( {M,S,V}\right) \; \vartriangleright \)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	\( {M}^{\prime },{S}^{\prime } \leftarrow \) 屏幕空间高斯 \( \left( {M,S,V}\right) \; \vartriangleright \)
</code></pre></div><p>Transform</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>变换</p></div><pre><code>	\( T \leftarrow \) CreateTiles(w,h)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	\( T \leftarrow \) 创建瓦片(w,h)
</code></pre></div><pre><code>	\( L,K \leftarrow \) DuplicateWithKeys \( \left( {{M}^{\prime },T}\right) \; \vartriangleright \) Indices and
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	\( L,K \leftarrow \) 通过键复制 \( \left( {{M}^{\prime },T}\right) \; \vartriangleright \) 索引和
</code></pre></div><p>Keys</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>键</p></div><pre><code>	SortByKeys \( \left( {K,L}\right) \; \vartriangleright \) Globally Sort
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	按键排序 \( \left( {K,L}\right) \; \vartriangleright \) 全局排序
</code></pre></div><pre><code>	\( R \leftarrow \) IdentifyTileRanges(T,K)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	\( R \leftarrow \) 确定瓦片范围(T,K)
</code></pre></div><pre><code>	\( I \leftarrow  \mathbf{0}\; \vartriangleright \) Init Canvas
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	\( I \leftarrow  \mathbf{0}\; \vartriangleright \) 初始化画布
</code></pre></div><pre><code>	for all Tiles \( t \) in \( I \) do
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	对于所有瓦片 \( t \) 在 \( I \) 中执行
</code></pre></div><pre><code>		for all Pixels \( i \) in \( t \) do
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>		对于所有像素 \( i \) 在 \( t \) 中执行
</code></pre></div><pre><code>				\( r \leftarrow \) GetTileRange(R,t)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>				\( r \leftarrow \) 获取瓦片范围(R,t)
</code></pre></div><pre><code>				\( I\left\lbrack  i\right\rbrack   \leftarrow \) BlendInOrder \( (i,L,r,K,{M}^{\prime },{S}^{\prime },C \) ,
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>				\( I\left\lbrack  i\right\rbrack   \leftarrow \) 混合顺序 \( (i,L,r,K,{M}^{\prime },{S}^{\prime },C \) ,
</code></pre></div><p>\(A)\)</p><pre><code>		end for
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>		结束循环
</code></pre></div><pre><code>	end for
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	结束循环
</code></pre></div><pre><code>	return \( I \)
</code></pre><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><pre><code>	返回 \( I \)
</code></pre></div><p>end function</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>结束函数</p></div><hr><!-- Media --><h2>Limitations</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>限制</h2></div><p>Large memory bandwidth To achieve real-time rendering with high frames per second, a parallel computing approach is used in the Rasterize(   ) function. This involves a large amount of dynamic data loading occurring in the shared memory of each tile during the \(\alpha\) -blending process. Therefore, a large GPU memory bandwidth is required to support the data traffic.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>大内存带宽 为了实现高帧率的实时渲染，在Rasterize(   )函数中采用了并行计算方法。这涉及到在每个瓦片的共享内存中进行大量动态数据加载，发生在\(\alpha\) -混合过程中。因此，需要大容量的GPU内存带宽来支持数据流量。</p></div><p>Robustness in Sparse Viewpoints Gaussians are optimized by taking the gradient compared with the true camera view. However, viewpoints with few or no data points have less data to optimize the Guassians in their region resulting in artifacts and distortions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>稀疏视点的鲁棒性 通过与真实相机视图比较梯度来优化高斯分布。然而，数据点较少或没有数据点的视点在其区域内优化高斯分布的数据较少，导致伪影和失真。</p></div><h2>Future Trends</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>未来趋势</h2></div><p>Semantic-Driven 3D Reconstruction Many 3D reconstruction techniques focus on generating \(3\mathrm{D}\) models from images. Yet, the integration of text prompts as a guiding factor presents an exciting avenue for future research. For example, the method outlined in the paper "Semantic-Driven 3D Reconstruction from Single Images"[18] demonstrates how textual cues can significantly enhance both the precision and contextual relevance of reconstructed models. While, "LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation" [14] demonstrate impressive zero-shot 3D generations from only a text prompt.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>语义驱动的3D重建 许多3D重建技术专注于从图像生成\(3\mathrm{D}\)模型。然而，将文本提示作为指导因素的整合为未来的研究提供了一个令人兴奋的方向。例如，论文“从单幅图像进行语义驱动的3D重建”[18]展示了文本线索如何显著提高重建模型的精度和上下文相关性。而“LGM：用于高分辨率3D内容创建的大型多视角高斯模型”[14]展示了仅通过文本提示实现令人印象深刻的零-shot 3D生成。</p></div><p>Dynamic 3D scene reconstruction The previously mentioned approaches can only use captured information in static scenes to reproduce static 3D models, where any structure change of scene during information capturing will result in misinformation that leads to under-reconstruction in specific areas. To achieve dynamic 3D scene reconstruction, 4D Gaussian Splatting utilizes a set of canonical 3D Gaussians and transforming them through a deformation field at different times, resulting in producing dynamically changing 3D models that can represent the motion of objects over time [16].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>动态3D场景重建 前面提到的方法只能利用静态场景中捕获的信息来重现静态3D模型，在信息捕获过程中场景的任何结构变化都会导致错误信息，从而在特定区域导致重建不足。为了实现动态3D场景重建，4D高斯点云利用一组典型的3D高斯分布，并通过不同时间的变形场对其进行变换，从而生成能够表示物体随时间变化的动态3D模型[16]。</p></div><p>Single View 3D reconstruction Building on the methodology introduced in Zero 1-to-3, an area that has gained significant traction is Single View 3D reconstruction. Leveraging diffusion models to generate \(3\mathrm{D}\) objects from a single image, [14] and [17] have demonstrated promising work in this domain. References</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>单视图3D重建 基于在Zero 1-to-3中介绍的方法，单视图3D重建是一个获得显著关注的领域。利用扩散模型从单幅图像生成\(3\mathrm{D}\)对象，[14]和[17]在该领域展示了有前景的工作。参考文献</p></div><p>[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[1] Jonathan Ho, Ajay Jain, 和 Pieter Abbeel. 去噪扩散概率模型, 2020.</p></div><p>[2] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[2] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, 和 George Drettakis. 实时辐射场渲染的3D高斯点云, 2023.</p></div><p>[3] Agustinus Kristiadi. Kl divergence: Forward vs reverse? <a href="https://agustinus.kristia.de/techblog/2016/12/">https://agustinus.kristia.de/techblog/2016/12/</a> 21/forward-reverse-kl/, 2016. Accessed: 2024-04-22.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[3] Agustinus Kristiadi. KL散度：前向与反向？ <a href="https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/">https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/</a>, 2016. 访问时间：2024-04-22.</p></div><p>[4] Hung-yi Lee. Forward process of ddpm, April 2023. <a href="https://www.youtube.com/watch?v=ifCDXFdeaaM&#x26;t=608">https://www.youtube.com/watch?v=ifCDXFdeaaM&#x26;t=608</a>.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[4] Hung-yi Lee. ddpm的前向过程, 2023年4月. <a href="https://www.youtube.com/watch?v=ifCDXFdeaaM&#x26;t=608">https://www.youtube.com/watch?v=ifCDXFdeaaM&#x26;t=608</a>.</p></div><p>[5] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-makov, Sergey Zakharov, and Carl Vondrick. Zero-1- to-3: Zero-shot one image to 3d object, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[5] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-makov, Sergey Zakharov, 和 Carl Vondrick. Zero-1-到-3: 零样本一图像到3D对象, 2023.</p></div><p>[6] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. ACM Trans. Graph., 38(4):65:1-65:14, July 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[6] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, 和 Yaser Sheikh. 神经体积: 从图像中学习动态可渲染体积. ACM Trans. Graph., 38(4):65:1-65:14, 2019年7月.</p></div><p>[7] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections, 2021.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[7] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, 和 Daniel Duckworth. 野外的Nerf: 用于无约束照片集合的神经辐射场, 2021.</p></div><p>[8] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning \(3\mathrm{\;d}\) reconstruction in function space, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[8] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, 和 Andreas Geiger. 占用网络: 在函数空间中学习\(3\mathrm{\;d}\)重建, 2019.</p></div><p>[9] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis, 2020.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[9] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, 和 Ren Ng. Nerf: 将场景表示为神经辐射场以进行视图合成, 2020.</p></div><p>[10] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics, 41(4):1-15, July 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[10] Thomas Müller, Alex Evans, Christoph Schied, 和 Alexander Keller. 具有多分辨率哈希编码的即时神经图形原语. ACM Transactions on Graphics, 41(4):1-15, 2022年7月.</p></div><p>[11] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[11] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, 和 Steven Lovegrove. Deepsdf: 学习用于形状表示的连续有符号距离函数, 2019.</p></div><p>[12] Eric Penner and Li Zhang. Soft 3d reconstruction for view synthesis. 36(6), 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[12] Eric Penner 和 Li Zhang. 用于视图合成的软3D重建. 36(6), 2017.</p></div><p>[13] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[13] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, 和 Björn Ommer. 使用潜在扩散模型的高分辨率图像合成, 2022.</p></div><p>[14] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multiview gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[14] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, 和 Ziwei Liu. Lgm: 用于高分辨率3D内容创建的大型多视角高斯模型. arXiv预印本 arXiv:2402.05054, 2024.</p></div><p>[15] Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Thomas Müller, Charles Loop, Nathan Morrical, Koki Nagano, Towaki Takikawa, and Stan Birchfield. Rtmv: A ray-traced multi-view synthetic dataset for novel view synthesis, 2022.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[15] Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Thomas Müller, Charles Loop, Nathan Morrical, Koki Nagano, Towaki Takikawa, 和 Stan Birchfield. Rtmv: 用于新视图合成的光线追踪多视角合成数据集, 2022.</p></div><p>[16] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xi-aopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[16] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xi-aopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, 和 Xinggang Wang. 4D高斯溅射用于实时动态场景渲染, 2023.</p></div><p>[17] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient \(3\mathrm{\;d}\) mesh generation from a single image with sparse-view large reconstruction models, 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[17] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, 和 Ying Shan. Instantmesh: 从单幅图像中使用稀疏视图大型重建模型高效生成\(3\mathrm{\;d}\)网格, 2024.</p></div><p>[18] Hao Zhang, Yanbo Xu, Tianyuan Dai, Yu-Wing Tai, and Chi-Keung Tang. Facednerf: Semantics-driven face reconstruction, prompt editing and relighting with diffusion models, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[18] Hao Zhang, Yanbo Xu, Tianyuan Dai, Yu-Wing Tai, 和 Chi-Keung Tang. Facednerf: 基于语义的面部重建、提示编辑和使用扩散模型的重光照, 2023.</p></div><p>[19] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and Felix Heide. Differentiable point-based radiance fields for efficient view synthesis, 2023.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[19] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, 和 Felix Heide. 可微分的基于点的辐射场用于高效视图合成, 2023.</p></div>
      </body>
    </html>
  