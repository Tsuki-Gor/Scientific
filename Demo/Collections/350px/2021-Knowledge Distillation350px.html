
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>2021-Knowledge Distillation</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px 350px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      </head>
      <body>
        <div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h1><div><div><div class="locator-translate" data-positiontag-0="136,301">Knowledge Distillation: A Survey<div style="background-color: #d6d6d6;margin: 12px 0;">知识蒸馏：综述</div></div></div></div></h1></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="140,402">Jianping Gou <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="431" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msup><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> Baosheng Yu <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="432" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msup><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> Stephen J. Maybank <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="433" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>3</mn></mrow></msup><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> Dacheng Tao <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="434" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msup><mo>⋅</mo></math></mjx-assistive-mml></mjx-container><div style="background-color: #d6d6d6;margin: 12px 0;">郭建平 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="435" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msup><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> 于宝生 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="436" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msup><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> Stephen J. Maybank <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="437" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>3</mn></mrow></msup><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> 滕达成 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="438" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msup><mo>⋅</mo></math></mjx-assistive-mml></mjx-container></div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="142,493">Received: 29 June 2020 / Accepted: 3 March 2021<div style="background-color: #d6d6d6;margin: 12px 0;">收稿日期：2020年6月29日 / 接受日期：2021年3月3日</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="142,527">(C) Crown 2021<div style="background-color: #d6d6d6;margin: 12px 0;">(C) 皇冠 2021</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-0="142,607">Abstract<div style="background-color: #d6d6d6;margin: 12px 0;">摘要</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="141,644">In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.<div style="background-color: #d6d6d6;margin: 12px 0;">近年来，深度神经网络在工业界和学术界都取得了成功，特别是在计算机视觉任务中。深度学习的巨大成功主要归因于其能够编码大规模数据并操纵数十亿模型参数的可扩展性。然而，将这些庞大的深度模型部署在资源有限的设备上，例如移动电话和嵌入式设备，是一个挑战，这不仅因为高计算复杂度，还因为大存储需求。为此，已经开发出了各种模型压缩和加速技术。作为模型压缩和加速的代表性类型，知识蒸馏有效地从大型教师模型中学习一个小型学生模型。它已经引起了社区的快速关注。本文从知识类别、训练方案、教师-学生架构、蒸馏算法、性能比较和应用的角度，对知识蒸馏进行了全面的综述。此外，本文简要回顾了知识蒸馏中的挑战，并对未来研究的评论进行了讨论和展望。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="141,1047">Keywords Deep neural networks <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="439" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> Model compression <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="440" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> Knowledge distillation <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="441" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> Knowledge transfer <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="442" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> Teacher–student architecture<div style="background-color: #d6d6d6;margin: 12px 0;">关键词 深度神经网络 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="443" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> 模型压缩 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="444" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> 知识蒸馏 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="445" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> 知识转移 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="446" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⋅</mo></math></mjx-assistive-mml></mjx-container> 教师-学生架构</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-0="141,1177">1 Introduction<div style="background-color: #d6d6d6;margin: 12px 0;">1 引言</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="141,1261">During the last few years, deep learning has been the basis of many successes in artificial intelligence, including a variety of applications in computer vision (Krizhevsky et al. 2012), reinforcement learning (Silver et al. 2016; Ashok et al. 2018; Lai et al. 2020), and natural language processing (Devlin et al. 2019). With the help of many recent techniques, including residual connections (He et al. 2016, 2020b) and batch normalization (Ioffe and Szegedy 2015), it is easy to train very deep models with thousands of layers on powerful GPU or TPU clusters. For example, it takes less than ten minutes to train a ResNet model on a popular image recognition benchmark with millions of images (Deng et al. 2009; Sun et al. 2019); It takes no more than one and a half hours to train a powerful BERT model for language understanding (Devlin et al. 2019; You et al. 2019). The large-scale deep models have achieved overwhelming successes, however the huge computational complexity and massive storage requirements make it a great challenge to deploy them in real-time applications, especially on devices with limited resources, such as video surveillance and autonomous driving cars.<div style="background-color: #d6d6d6;margin: 12px 0;">在过去几年里，深度学习一直是人工智能领域许多成功的基石，包括计算机视觉（Krizhevsky等人，2012年）、强化学习（Silver等人，2016年；Ashok等人，2018年；Lai等人，2020年）以及自然语言处理（Devlin等人，2019年）的各种应用。借助许多最近的技术，包括残差连接（He等人，2016年，2020b）和批量归一化（Ioffe和Szegedy，2015年），可以很容易地在强大的GPU或TPU集群上训练具有数千层级的非常深的模型。例如，在拥有数百万图像的流行图像识别基准上训练一个ResNet模型不到十分钟（Deng等人，2009年；Sun等人，2019年）；训练一个强大的BERT语言理解模型不超过一个半小时（Devlin等人，2019年；You等人，2019年）。大规模深度模型取得了巨大的成功，然而巨大的计算复杂性和大量的存储需求使得它们在实时应用部署中面临巨大挑战，特别是在资源有限的设备上，如视频监控和自动驾驶汽车。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="926,1740">To develop efficient deep models, recent works usually focus on 1) efficient building blocks for deep models, including depthwise separable convolution, as in MobileNets (Howard et al. 2017; Sandler et al. 2018) and ShuffleNets (Zhang et al. 2018a; Ma et al. 2018); and 2) model compression and acceleration techniques, in the following categories (Cheng et al. 2018).<div style="background-color: #d6d6d6;margin: 12px 0;">为了开发高效的深度模型，最近的研究通常集中在1）深度模型的高效构建块上，包括MobileNets中的深度可分离卷积（Howard等人，2017年；Sandler等人，2018年）和ShuffleNets（Zhang等人，2018a；Ma等人，2018年）；以及2）模型压缩和加速技术，以下是一些类别（Cheng等人，2018年）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-0="908,2034"><ul><li>Parameter pruning and sharing: These methods focus on removing inessential parameters from deep neural networks without any significant effect on the performance. This category is further divided into model quantization (Wu et al. 2016), model binarization (Courbariaux et al. 2015), structural matrices (Sindhwani et al. 2015) and parameter sharing (Han et al. 2015; Wang et al. 2019f).<div style="background-color: #d6d6d6;margin: 12px 0;">- 参数剪枝与共享：这些方法专注于从深度神经网络中移除不必要的参数，同时不对性能产生显著影响。这一类别进一步细分为模型量化（Wu et al. 2016）、模型二值化（Courbariaux et al. 2015）、结构矩阵（Sindhwani et al. 2015）以及参数共享（Han et al. 2015; Wang et al. 2019f）。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><hr></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="142,1473">Communicated by Minsu Cho.<div style="background-color: #d6d6d6;margin: 12px 0;">由Minsu Cho传达。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="141,1535"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="447" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c22A0 TEX-A"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>⊠</mo></math></mjx-assistive-mml></mjx-container> Dacheng Tao</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="184,1568">dacheng.tao@sydney.edu.au</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="184,1609">Jianping Gou</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="185,1641">cherish.gjp@gmail.com</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="184,1682">Baosheng Yu</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="184,1715">baosheng.yu@sydney.edu.au</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="183,1755">Stephen J. Maybank</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="184,1788">sjmaybank@dcs.bbk.ac.uk</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="143,1842">1 School of Computer Science and Communication Engineering and Jiangsu Key Laboratory of Security Technology for Industrial Cyberspace, Jiangsu University, Zhenjiang 212013, China<div style="background-color: #d6d6d6;margin: 12px 0;">1 计算机科学与通信工程学院及江苏省工业网络安全技术重点实验室，江苏大学，镇江 212013，中国</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="141,1975">2 School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW 2008, Australia<div style="background-color: #d6d6d6;margin: 12px 0;">2 计算机科学学院，工程学部，悉尼大学，Darlington，NSW 2008，澳大利亚</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-0="146,2047">3 Department of Computer Science and Information Systems, Birkbeck College, University of London, London, UK<div style="background-color: #d6d6d6;margin: 12px 0;">3 计算机科学与信息系统系，Birkbeck学院，伦敦大学，伦敦，英国</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-ac38f554-33f0-41f3-b8f6-e3414252f43f" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><hr></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-97ea012c-1260-4ef5-9200-83df2da622fa" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-1="141,151"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_1.jpg?x=141&amp;y=151&amp;w=1463&amp;h=618"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-97ea012c-1260-4ef5-9200-83df2da622fa" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-1="141,793">Fig. 1 The generic teacher-student framework for knowledge distillation<div style="background-color: #d6d6d6;margin: 12px 0;">图 1 知识蒸馏的通用教师-学生框架</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-97ea012c-1260-4ef5-9200-83df2da622fa" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-1="160,1070"><ul><li>Low-rank factorization: These methods identify redundant parameters of deep neural networks by employing the matrix and tensor decomposition (Yu et al. 2017; Denton et al. 2014).<div style="background-color: #d6d6d6;margin: 12px 0;">- 低秩分解：这些方法通过使用矩阵和张量分解（Yu et al. 2017; Denton et al. 2014）来识别深度神经网络中的冗余参数。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-97ea012c-1260-4ef5-9200-83df2da622fa" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-1="157,1216"><ul><li>Transferred compact convolutional filters: These methods remove inessential parameters by transferring or compressing the convolutional filters (Zhai et al. 2016).<div style="background-color: #d6d6d6;margin: 12px 0;">- 转移紧凑卷积滤波器：这些方法通过转移或压缩卷积滤波器来移除不必要的参数（Zhai et al. 2016）。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-97ea012c-1260-4ef5-9200-83df2da622fa" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-1="157,1327"><ul><li>Knowledge distillation (KD): These methods distill the knowledge from a larger deep neural network into a small network (Hinton et al. 2015).<div style="background-color: #d6d6d6;margin: 12px 0;">- 知识蒸馏（KD）：这些方法将较大深度神经网络中的知识蒸馏到一个小网络中（Hinton et al. 2015）。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-97ea012c-1260-4ef5-9200-83df2da622fa" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-1="142,1484">A comprehensive review on model compression and acceleration is outside the scope of this paper. The focus of this paper is knowledge distillation, which has received increasing attention from the research community in recent years. Large deep neural networks have achieved remarkable success with good performance, especially in the real-world scenarios with large-scale data, because the over parameterization improves the generalization performance when new data is considered (Zhang et al. 2018; Brutzkus and Glober-son 2019; Allen-Zhu et al. 2019; Arora et al. 2018; Tu et al. 2020). However, the deployment of deep models in mobile devices and embedded systems is a great challenge, due to the limited computational capacity and memory of the devices. To address this issue, Bucilua et al. (2006) first proposed model compression to transfer the information from a large model or an ensemble of models into training a small model without a significant drop in accuracy. The learning of a small model from a large model is later formally popularized as knowledge distillation (Hinton et al. 2015). In knowledge distillation, a small student model is generally supervised by a large teacher model (Bucilua et al. 2006; Ba and Caru-ana 2014; Hinton et al. 2015; Urban et al. 2017). The main idea is that the student model mimics the teacher model in order to obtain a competitive or even a superior performance. The key problem is how to transfer the knowledge from a large teacher model to a small student model. Basically, a knowledge distillation system is composed of three key components: knowledge, distillation algorithm, and teacher-student architecture. A general teacher-student framework for knowledge distillation is shown in Fig. 1.<div style="background-color: #d6d6d6;margin: 12px 0;">本文不涉及对模型压缩和加速的全面回顾。本文的重点是知识蒸馏，近年来这一领域已经引起了研究界的广泛关注。大型深度神经网络在性能上取得了显著的成功，特别是在具有大规模数据的现实世界场景中，因为过度参数化提高了在考虑新数据时的泛化性能（Zhang et al. 2018；Brutzkus 和 Glober-son 2019；Allen-Zhu et al. 2019；Arora et al. 2018；Tu et al. 2020）。然而，在移动设备和嵌入式系统中部署深度模型是一个巨大的挑战，因为设备的计算能力和内存有限。为了解决这个问题，Bucilua 等人（2006）首次提出了模型压缩，即将大型模型或模型集合的信息转移至训练一个小型模型，而不会在准确性上出现显著下降。从大型模型学习小型模型的方法后来正式被推广为知识蒸馏（Hinton et al. 2015）。在知识蒸馏中，一个小型学生模型通常由一个大型教师模型进行监督（Bucilua et al. 2006；Ba 和 Caruana 2014；Hinton et al. 2015；Urban et al. 2017）。主要思想是学生模型模仿教师模型，以获得有竞争力的甚至更优的性能。关键问题是怎样将大型教师模型的知识转移至小型学生模型。基本上，知识蒸馏系统由三个关键部分组成：知识、蒸馏算法和教师-学生架构。知识蒸馏的一般教师-学生框架如图1所示。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-97ea012c-1260-4ef5-9200-83df2da622fa" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-1="924,1363">Although the great success in practice, there are not too many works on either the theoretical or empirical understanding of knowledge distillation (Cheng et al. 2020; Phuong and Lampert 2019a; Cho and Hariharan 2019). Specifically, to understand the working mechanisms of knowledge distillation, Phuong &amp; Lampert obtained a theoretical justification for a generalization bound with fast convergence of learning distilled student networks in the scenario of deep linear classifiers (Phuong and Lampert 2019a). This justification answers what and how fast the student learns and reveals the factors of determining the success of distillation. Successful distillation relies on data geometry, optimization bias of distillation objective and strong monotonicity of the student classifier. Cheng et al. quantified the extraction of visual concepts from the intermediate layers of a deep neural network, to explain knowledge distillation (Cheng et al. 2020). Ji and Zhu (2020) theoretically explained knowledge distillation on a wide neural network from the respective of risk bound, data efficiency and imperfect teacher. Cho &amp; Hariharan empirically analyzed in detail the efficacy of knowledge distillation<div style="background-color: #d6d6d6;margin: 12px 0;">尽管在实际中取得了巨大成功，但在理论或实证上关于知识蒸馏的研究却并不多（Cheng et al. 2020；Phuong 和 Lampert 2019a；Cho 和 Hariharan 2019）。具体来说，为了理解知识蒸馏的工作机制，Phuong &amp; Lampert 在深度线性分类器的场景中为学习蒸馏学生网络的泛化界和快速收敛提供了理论依据（Phuong 和 Lampert 2019a）。这种解释回答了学生学到什么以及学习速度有多快，并揭示了决定蒸馏成功因素的因素。成功的蒸馏依赖于数据几何结构、蒸馏目标的优化偏差和学生分类器的强烈单调性。Cheng 等人量化了从深度神经网络的中间层提取视觉概念，以解释知识蒸馏（Cheng et al. 2020）。Ji 和 Zhu（2020）从风险边界、数据效率和教师不完美的角度理论解释了宽神经网络的知识蒸馏。Cho 和 Hariharan 实证详细分析了知识蒸馏的有效性</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-bccf3bfd-7146-4d09-96f6-32eab7548a90" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-2="143,148"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_2.jpg?x=143&amp;y=148&amp;w=1459&amp;h=692"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-bccf3bfd-7146-4d09-96f6-32eab7548a90" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-2="143,1067">(Cho and Hariharan 2019). Empirical results show that a larger model may not be a better teacher because of model capacity gap (Mirzadeh et al. 2020). Experiments also show that distillation adversely affects the student learning. The empirical evaluation of different forms of knowledge distillation about knowledge, distillation and mutual affection between teacher and student is not covered by Cho and Hari-haran (2019). Knowledge distillation has also been explored for label smoothing, for assessing the accuracy of the teacher and for obtaining a prior for the optimal output layer geometry (Tang et al. 2020).<div style="background-color: #d6d6d6;margin: 12px 0;">（Cho 和 Hariharan 2019）。实证结果表明，由于模型容量差距，更大的模型可能不是一个更好的教师（Mirzadeh et al. 2020）。实验还表明，蒸馏对学生学习有负面影响。关于知识、蒸馏以及教师和学生之间相互影响的不同形式的知识蒸馏的实证评估并未被 Cho 和 Hari-haran（2019）所涵盖。知识蒸馏还已被应用于标签平滑，用于评估教师的准确性，以及为确定最优输出层结构获取先验知识（Tang et al. 2020）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-bccf3bfd-7146-4d09-96f6-32eab7548a90" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-2="143,860">Fig. 2 The schematic structure of knowledge distillation and the relationship between the adjacent sections. The body of this survey mainly contains the fundamentals of knowledge distillation, knowledge types, distillation schemes, teacher-student architecture, distillation algorithms, performance comparison, applications, discussions, challenges, and future directions. Note that 'Section' is abbreviated as 'Sec.' in this figure<div style="background-color: #d6d6d6;margin: 12px 0;">图 2 知识蒸馏的示意图结构以及相邻部分之间的关系。本调查的主体主要包括知识蒸馏的基础、知识类型、蒸馏方案、师徒架构、蒸馏算法、性能比较、应用、讨论、挑战以及未来方向。注意，在此图中 'Section' 缩写为 'Sec.'</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-bccf3bfd-7146-4d09-96f6-32eab7548a90" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-2="175,1472">Knowledge distillation for model compression is similar to the way in which human beings learn. Inspired by this, recent knowledge distillation methods have extended to teacher-student learning (Hinton et al. 2015), mutual learning (Zhang et al. 2018b), assistant teaching (Mirzadeh et al. 2020), lifelong learning (Zhai et al. 2019), and self-learning (Yuan et al. 2020). Most of the extensions of knowledge distillation concentrate on compressing deep neural networks. The resulting lightweight student networks can be easily deployed in applications such as visual recognition, speech recognition, and natural language processing (NLP). Furthermore, the knowledge transfer from one model to another in knowledge distillation can be extended to other tasks, such as adversarial attacks (Papernot et al. 2016), data augmentation (Lee et al. 2019a; Gordon and Duh 2019), data privacy and security (Wang et al. 2019a). Motivated by knowledge distillation for model compression, the idea of knowledge transfer has been further applied in compressing the training data, i.e., dataset distillation, which transfers the knowledge from a large dataset into a small dataset to reduce the training loads of deep models (Wang et al. 2018c; Bohdal et al. 2020).<div style="background-color: #d6d6d6;margin: 12px 0;">模型压缩中的知识蒸馏与人类学习的方式相似。受此启发，最近的知识蒸馏方法已经扩展到师徒学习（Hinton et al. 2015）、互学习（Zhang et al. 2018b）、辅助教学（Mirzadeh et al. 2020）、终身学习（Zhai et al. 2019）以及自学（Yuan et al. 2020）。知识蒸馏的大部分扩展集中在压缩深度神经网络。由此产生的轻量级学生网络可以轻松部署在视觉识别、语音识别以及自然语言处理（NLP）等应用中。此外，知识蒸馏中一个模型到另一个模型的知识转移可以扩展到其他任务，例如对抗性攻击（Papernot et al. 2016）、数据增强（Lee et al. 2019a; Gordon 和 Duh 2019）、数据隐私和安全（Wang et al. 2019a）。受模型压缩中知识蒸馏的启发，知识转移的思想已经被进一步应用于压缩训练数据，即数据集蒸馏，它将知识从大型数据集转移到小型数据集，以减少深度模型的训练负载（Wang et al. 2018c; Bohdal et al. 2020）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-bccf3bfd-7146-4d09-96f6-32eab7548a90" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-2="924,1216">In this paper, we present a comprehensive survey on knowledge distillation. The main objectives of this survey are to 1) provide an overview on knowledge distillation, including several typical knowledge, distillation and architectures; 2) review the recent progress of knowledge distillation, including algorithms and applications to different real-world scenarios; and 3) address some hurdles and provide insights to knowledge distillation based on different perspectives of knowledge transfer, including different types of knowledge, training schemes, distillation algorithms and structures, and applications. Recently, there is also a similar survey on knowledge distillation (Wang and Yoon. 2020), which presents the comprehensive progress from different perspective of teacher-student learning for vision and its challenges. Different from Wang and Yoon. (2020), our survey mainly focuses on knowledge distillation from a wide perspective of knowledge types, distillation schemes, distillation algorithms, performance comparison and different application areas.<div style="background-color: #d6d6d6;margin: 12px 0;">在本文中，我们对知识蒸馏进行了全面的调研。本次调研的主要目标是：1）提供关于知识蒸馏的概述，包括几种典型的知识、蒸馏架构；2）回顾知识蒸馏的近期进展，包括算法及其在不同现实场景中的应用；3）从知识转移的不同视角出发，探讨知识蒸馏的一些障碍，并提供见解，包括不同类型的知识、训练方案、蒸馏算法和结构以及应用。近期，还有一份关于知识蒸馏的类似调研（Wang和Yoon，2020），该调研从教师-学生学习的不同视角展示了视觉领域的全面进展及其挑战。与Wang和Yoon（2020）不同，我们的调研主要从知识类型、蒸馏方案、蒸馏算法、性能比较以及不同应用领域的广泛视角聚焦于知识蒸馏。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-bccf3bfd-7146-4d09-96f6-32eab7548a90" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-2="923,1912">The organization of this paper is shown in Fig. 2. The different kinds of knowledge and of distillation are summarized in Sects. 2 and 3, respectively. The existing studies about the teacher-student structures in knowledge distillation are illustrated in Sect. 4. The latest knowledge distillation approaches are comprehensively summarized in Sect. 5. The performance comparison of knowledge distillation is reported in Sect. 6. The many applications of knowledge distillation are illustrated in Sect. 7. Challenging problems and future directions in knowledge distillation are discussed and conclusion is given in Sect. 8.<div style="background-color: #d6d6d6;margin: 12px 0;">本文的组织结构如图2所示。不同类型的知识和蒸馏分别在第2节和第3节进行了总结。第4节说明了关于知识蒸馏中教师-学生结构现有研究的示例。第5节全面总结了最新的知识蒸馏方法。第6节报告了知识蒸馏的性能比较。第7节展示了知识蒸馏的许多应用。在第8节中，讨论了知识蒸馏的挑战性问题及未来方向，并给出了结论。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-fd6ada4b-5b8c-43ef-abac-742b826c0bda" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-3="142,157">Fig. 3 The schematic<div style="background-color: #d6d6d6;margin: 12px 0;">图3 示意图</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-fd6ada4b-5b8c-43ef-abac-742b826c0bda" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-3="521,151"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_3.jpg?x=521&amp;y=151&amp;w=1080&amp;h=746"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-fd6ada4b-5b8c-43ef-abac-742b826c0bda" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-3="142,189">illustrations of sources of response-based knowledge, feature-based knowledge and relation-based knowledge in a deep teacher network<div style="background-color: #d6d6d6;margin: 12px 0;">深度教师网络中基于响应的知识、基于特征的知识和基于关系的知识的来源说明</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-fd6ada4b-5b8c-43ef-abac-742b826c0bda" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-3="138,1290">2 Knowledge<div style="background-color: #d6d6d6;margin: 12px 0;">2 知识</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-fd6ada4b-5b8c-43ef-abac-742b826c0bda" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-3="143,1375">In knowledge distillation, knowledge types, distillation strategies and the teacher-student architectures play the crucial role in the student learning. In this section, we focus on different categories of knowledge for knowledge distillation. A vanilla knowledge distillation uses the logits of a large deep model as the teacher knowledge (Hinton et al. 2015; Kim et al. 2018; Ba and Caruana 2014; Mirzadeh et al. 2020). The activations, neurons or features of intermediate layers also can be used as the knowledge to guide the learning of the student model (Romero et al. 2015; Huang and Wang 2017; Ahn et al. 2019; Heo et al. 2019c; Zagoruyko and Komodakis 2017). The relationships between different activations, neurons or pairs of samples contain rich information learned by the teacher model (Yim et al. 2017; Lee and Song 2019; Liu et al. 2019g; Tung and Mori 2019; Yu et al. 2019). Furthermore, the parameters of the teacher model (or the connections between layers) also contain another knowledge (Liu et al. 2019c). We discuss different forms of knowledge in the following categories: response-based knowledge, feature-based knowledge, and relation-based knowledge. An intuitive example of different categories of knowledge within a teacher model is shown in Fig. 3.<div style="background-color: #d6d6d6;margin: 12px 0;">在知识蒸馏中，知识类型、蒸馏策略以及教师-学生架构在学生的学习中扮演着关键角色。在本节中，我们关注知识蒸馏的不同类别。传统的知识蒸馏使用大型深度模型的日志几率作为教师知识（Hinton et al. 2015; Kim et al. 2018; Ba 和 Caruana 2014; Mirzadeh et al. 2020）。中间层的激活、神经元或特征也可以作为知识来指导学生模型的学习（Romero et al. 2015; Huang 和 Wang 2017; Ahn et al. 2019; Heo et al. 2019c; Zagoruyko 和 Komodakis 2017）。不同激活、神经元或样本对之间的关系包含了教师模型学习到的丰富信息（Yim et al. 2017; Lee 和 Song 2019; Liu et al. 2019g; Tung 和 Mori 2019; Yu et al. 2019）。此外，教师模型的参数（或层之间的连接）也包含另一种知识（Liu et al. 2019c）。我们在以下类别中讨论不同形式的知识：基于响应的知识、基于特征的知识和基于关系的知识。图3展示了教师模型内部不同类别知识的直观示例。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-fd6ada4b-5b8c-43ef-abac-742b826c0bda" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-3="898,938"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_3.jpg?x=898&amp;y=938&amp;w=699&amp;h=294"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-fd6ada4b-5b8c-43ef-abac-742b826c0bda" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-3="892,1249">Fig. 4 The generic response-based knowledge distillation<div style="background-color: #d6d6d6;margin: 12px 0;">图4 泛型基于响应的知识蒸馏</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-fd6ada4b-5b8c-43ef-abac-742b826c0bda" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-3="889,1439">2.1 Response-Based Knowledge<div style="background-color: #d6d6d6;margin: 12px 0;">2.1 基于响应的知识</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-fd6ada4b-5b8c-43ef-abac-742b826c0bda" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-3="925,1524">Response-based knowledge usually refers to the neural response of the last output layer of the teacher model. The main idea is to directly mimic the final prediction of the teacher model. The response-based knowledge distillation is simple yet effective for model compression, and has been widely used in different tasks and applications. Given a vector of logits <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="448" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi></math></mjx-assistive-mml></mjx-container> as the outputs of the last fully connected layer of a deep model, the distillation loss for response-based knowledge can be formulated as<div style="background-color: #d6d6d6;margin: 12px 0;">基于响应的知识通常指的是教师模型最后一层输出层的神经响应。其主要思想是直接模仿教师模型的最终预测。基于响应的知识蒸馏简单而有效，用于模型压缩，并在不同的任务和应用中得到了广泛的应用。给定一个作为深度模型最后全连接层输出的日志概率向量 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="449" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi></math></mjx-assistive-mml></mjx-container>，基于响应的知识蒸馏损失可以表示为</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-fd6ada4b-5b8c-43ef-abac-742b826c0bda" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><paragraphpositioning data-position-3="892,1889"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" width="full" tabindex="0" ctxtmenu_counter="450" style="font-size: 122.8%; min-width: 15.491em; position: relative;"><mjx-math width="full" display="true" class="MJX-TEX" aria-hidden="true"><mjx-mtable width="full" side="right" style="min-width: 15.491em;"><mjx-table style="width: auto; min-width: 11.335em; margin: 0px 2.078em;"><mjx-itable width="full"><mjx-mlabeledtr><mjx-mtd><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mlabeledtr></mjx-itable></mjx-table><mjx-labels style="width: 15.491em;"><mjx-itable align="right" style="right: 0px;"><mjx-mtr style="height: 1em;"><mjx-mtd id="mjx-eqn:1"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c28"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c29"></mjx-c></mjx-mtext><mjx-tstrut style="height: 1em; vertical-align: -0.25em;"></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-labels></mjx-mtable></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd><mtext>(1)</mtext></mtd><mtd><msub><mrow data-mjx-texclass="ORD"><mi>L</mi></mrow><mrow data-mjx-texclass="ORD"><mi>R</mi><mi>e</mi><mi>s</mi><mi>D</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>=</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>R</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mtd></mlabeledtr></mtable></math></mjx-assistive-mml></mjx-container></paragraphpositioning></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-fd6ada4b-5b8c-43ef-abac-742b826c0bda" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-3="893,1961">where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="451" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>R</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi><mi>n</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>f</mi><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mi>s</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>n</mi><mi>d</mi></mrow><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="452" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> are logits of teacher and student,respectively. A typical response-based KD model is shown in Fig. 4. The response-based knowledge can be used for different types of model predictions. For example, the response in object detection task may contain the logits together with the offset of a bounding box (Chen et al. 2017). In semantic landmark localization tasks, e.g., human pose estimation, the response of the teacher model may include a heatmap for each landmark (Zhang et al. 2019a). Recently, response-based knowledge has been further explored to address the information of ground-truth label as the conditional targets (Meng et al. 2019).<div style="background-color: #d6d6d6;margin: 12px 0;">其中 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="453" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>R</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi><mi>n</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>f</mi><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mi>s</mi></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>n</mi><mi>d</mi></mrow><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="454" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> 分别是教师和学生模型的日志概率。一个典型的基于响应的知识蒸馏模型如图4所示。基于响应的知识可以用于不同类型的模型预测。例如，在目标检测任务中，响应可能包含日志概率和边界框偏移（Chen et al. 2017）。在语义地标定位任务中，例如人体姿态估计，教师模型的响应可能包括每个地标的热图（Zhang et al. 2019a）。最近，基于响应的知识进一步被探索，以处理作为条件目标的真实标签信息（Meng et al. 2019）。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-4="141,141"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_4.jpg?x=141&amp;y=141&amp;w=1430&amp;h=343"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-4="141,510">Fig. 5 The specific architecture of the benchmark knowledge distillation (Hinton et al. 2015)<div style="background-color: #d6d6d6;margin: 12px 0;">图5 基准知识蒸馏的具体架构（Hinton et al. 2015）</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-4="177,934">The most popular response-based knowledge for image classification is known as soft targets (Hinton et al. 2015; Ba and Caruana 2014). Specifically, soft targets are the probabilities that the input belongs to the classes and can be estimated by a softmax function as<div style="background-color: #d6d6d6;margin: 12px 0;">图像分类中最为流行的基于响应的知识被称为软目标（Hinton et al. 2015; Ba 和 Caruana 2014）。具体来说，软目标是输入属于各个类别的概率，可以通过softmax函数来估计为</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><paragraphpositioning data-position-4="140,1142"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" width="full" tabindex="0" ctxtmenu_counter="455" style="font-size: 122.8%; min-width: 15.335em; position: relative;"><mjx-math width="full" display="true" class="MJX-TEX" aria-hidden="true"><mjx-mtable width="full" side="right" style="min-width: 15.335em;"><mjx-table style="width: auto; min-width: 11.179em; margin: 0px 2.078em;"><mjx-itable width="full"><mjx-mlabeledtr><mjx-mtd><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-n"><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c78"></mjx-c><mjx-c class="mjx-c70"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-mrow><mjx-munder><mjx-row><mjx-base><mjx-texatom texclass="OP"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2211 TEX-S1"></mjx-c></mjx-mo></mjx-texatom></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em; padding-left: 0.382em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-under></mjx-row></mjx-munder><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-n" space="2"><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c78"></mjx-c><mjx-c class="mjx-c70"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mlabeledtr></mjx-itable></mjx-table><mjx-labels style="width: 15.335em;"><mjx-itable align="right" style="right: 0px;"><mjx-mtr style="height: 3.299em;"><mjx-mtd id="mjx-eqn:2"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c28"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c29"></mjx-c></mjx-mtext><mjx-tstrut style="height: 3.299em; vertical-align: -1.839em;"></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-labels></mjx-mtable></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd><mtext>(2)</mtext></mtd><mtd><mi>p</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub><mo>,</mo><mi>T</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>exp</mi><mo data-mjx-texclass="NONE">⁡</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>T</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mrow><munder><mrow data-mjx-texclass="OP"><mo data-mjx-texclass="OP" movablelimits="false">∑</mo></mrow><mrow data-mjx-texclass="ORD"><mi>j</mi></mrow></munder><mo data-mjx-texclass="NONE">⁡</mo><mi>exp</mi><mo data-mjx-texclass="NONE">⁡</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>j</mi></mrow></msub><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>T</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow></mfrac><mo>,</mo></mtd></mlabeledtr></mtable></math></mjx-assistive-mml></mjx-container></paragraphpositioning></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-4="142,1262">where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="456" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> is the logit for the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="457" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi></math></mjx-assistive-mml></mjx-container> -th class,and a temperature factor <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="458" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container> is introduced to control the importance of each soft target. As stated in Hinton et al. (2015), soft targets contain the informative dark knowledge from the teacher model. Accordingly, the distillation loss for soft logits can be rewritten as<div style="background-color: #d6d6d6;margin: 12px 0;">其中 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="459" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> 是第 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="460" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi></math></mjx-assistive-mml></mjx-container> 类的日志概率，引入了一个温度因子 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="461" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container> 来控制每个软目标的重要性。正如Hinton et al. (2015) 中所述，软目标包含了教师模型的信息丰富暗知识。因此，软日志的概率蒸馏损失可以重写为</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><paragraphpositioning data-position-4="141,1516"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" width="full" tabindex="0" ctxtmenu_counter="462" style="font-size: 122.8%; min-width: 26.154em; position: relative;"><mjx-math width="full" display="true" class="MJX-TEX" aria-hidden="true"><mjx-mtable width="full" side="right" style="min-width: 26.154em;"><mjx-table style="width: auto; min-width: 21.998em; margin: 0px 2.078em;"><mjx-itable width="full"><mjx-mlabeledtr><mjx-mtd><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mlabeledtr></mjx-itable></mjx-table><mjx-labels style="width: 26.154em;"><mjx-itable align="right" style="right: 0px;"><mjx-mtr style="height: 1em;"><mjx-mtd id="mjx-eqn:3"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c28"></mjx-c><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c29"></mjx-c></mjx-mtext><mjx-tstrut style="height: 1em; vertical-align: -0.25em;"></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-labels></mjx-mtable></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd><mtext>(3)</mtext></mtd><mtd><msub><mrow data-mjx-texclass="ORD"><mi>L</mi></mrow><mrow data-mjx-texclass="ORD"><mi>R</mi><mi>e</mi><mi>s</mi><mi>D</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>p</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mo>,</mo><mi>T</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><mi>p</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mo>,</mo><mi>T</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>=</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>R</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>p</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mo>,</mo><mi>T</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><mi>p</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mo>,</mo><mi>T</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>.</mo></mtd></mlabeledtr></mtable></math></mjx-assistive-mml></mjx-container></paragraphpositioning></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-4="142,1594">Generally, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="463" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>R</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>p</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mo>,</mo><mi>T</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><mi>p</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mo>,</mo><mi>T</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> often employs Kullback-Leibler divergence loss. Clearly, optimizing Eq. (1) or (3) can make the logits <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="464" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> of student match the ones <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="465" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> of teacher. To easily understand the response-based knowledge distillation, the benchmark model of a vanilla knowledge distillation, which is the joint of the distillation and student losses, is given in Fig. 5. Note that the student loss is always defined as the cross-entropy loss <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="466" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>C</mi><mi>E</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>y</mi><mo>,</mo><mi>p</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mo>,</mo><mi>T</mi><mo>=</mo><mn>1</mn></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> between the ground truth label and the soft logits of the student model.<div style="background-color: #d6d6d6;margin: 12px 0;">通常，<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="467" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>R</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>p</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mo>,</mo><mi>T</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><mi>p</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mo>,</mo><mi>T</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container>经常使用Kullback-Leibler散度损失。显然，优化等式（1）或（3）可以使学生的logits <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="468" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>与教师的logits <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="469" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>相匹配。为了更容易理解基于响应的知识蒸馏，图5给出了一个经典知识蒸馏的基准模型，即蒸馏和学生损失的联合。注意，学生损失总是定义为真实标签和学生模型软logits之间的交叉熵损失 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="470" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>C</mi><mi>E</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>y</mi><mo>,</mo><mi>p</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>z</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mo>,</mo><mi>T</mi><mo>=</mo><mn>1</mn></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container>。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-4="175,1928">The idea of the response-based knowledge is straightforward and easy to understand, especially in the context of "dark knowledge". From another perspective, the effectiveness of the soft targets is analogous to label smoothing (Kim and Kim 2017) or regularizers (Muller et al. 2019; Ding et al. 2019). However, the response-based knowledge usually relies on the output of the last layer, e.g., soft targets, and thus fails to address the intermediate-level supervision from the teacher model, which turns out to be very important for representation learning using very deep neural networks (Romero et al. 2015). Since the soft logits are in fact the class probability distribution, the response-based knowledge distillation is also limited to the supervised learning.<div style="background-color: #d6d6d6;margin: 12px 0;">基于响应的知识概念简单且易于理解，特别是在“暗知识”的背景下。从另一个角度看，软目标的有效性与标签平滑（Kim和Kim 2017）或正则化器（Muller等人2019；Ding等人2019）相似。然而，基于响应的知识通常依赖于最后一层的输出，例如软目标，因此无法解决来自教师模型的中间级别监督，这对于使用非常深的神经网络进行表示学习非常重要（Romero等人2015）。由于软logits实际上是类概率分布，因此基于响应的知识蒸馏也仅限于监督学习。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-4="902,588"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_4.jpg?x=902&amp;y=588&amp;w=694&amp;h=321"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-4="891,927">Fig. 6 The generic feature-based knowledge distillation<div style="background-color: #d6d6d6;margin: 12px 0;">图6 通用基于特征的知识蒸馏</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-4="892,1337">2.2 Feature-Based Knowledge<div style="background-color: #d6d6d6;margin: 12px 0;">2.2 基于特征的知识</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-4="893,1422">Deep neural networks are good at learning multiple levels of feature representation with increasing abstraction. This is known as representation learning (Bengio et al. 2013). Therefore, both the output of the last layer and the output of intermediate layers, i.e., feature maps, can be used as the knowledge to supervise the training of the student model. Specifically, feature-based knowledge from the intermediate layers is a good extension of response-based knowledge, especially for the training of thinner and deeper networks.<div style="background-color: #d6d6d6;margin: 12px 0;">深度神经网络擅长学习逐层抽象的特征表示。这被称为表示学习（Bengio等人2013）。因此，最后一层的输出和中间层的输出，即特征图，都可以用作监督学生模型训练的知识。具体来说，来自中间层的基于特征的知识是基于响应知识的一个很好的扩展，特别是在训练更薄、更深的网络时。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-4="924,1755">The intermediate representations were first introduced in Fitnets (Romero et al. 2015),to provide hints <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="471" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> to improve the training of the student model. The main idea is to directly match the feature activations of the teacher and the student. Inspired by this, a variety of other methods have been proposed to match the features indirectly (Zagoruyko and Komodakis 2017; Kim et al. 2018; Heo et al. 2019c; Passban et al. 2021; Chen et al. 2021; Wang et al. 2020b). To be specific, Zagoruyko and Komodakis (2017) derived an "attention map" from the original feature maps to express knowledge. The attention map was generalized by Huang and Wang (2017) using neuron selectivity transfer. Passalis and Tefas (2018) transferred knowledge by matching the probability distribution in feature space. To make it easier to transfer the teacher knowledge, Kim et al. (2018) introduced so called "factors" as a more understandable form of intermediate representations. To reduce the performance gap between teacher and student, Jin et al. (2019) proposed route constrained hint learning, which supervises student by outputs of hint layers of teacher. Recently, Heo et al. (2019c) proposed to use the activation boundary of the hidden neurons for knowledge transfer. Interestingly, the parameter sharing of intermediate layers of the teacher model together with response-based knowledge is also used as the teacher knowledge (Zhou et al. 2018). To match the semantics between teacher and student, Chen et al. (2021) proposed cross-layer knowledge distillation, which adaptively assigns proper teacher layers for each student layer via attention allocation.<div style="background-color: #d6d6d6;margin: 12px 0;">中间表示最早在Fitnets（Romero等人，2015年）中提出，用以向学生模型训练提供提示 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="472" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container>。主要思想是直接匹配教师模型和学生模型的特征激活。受此启发，已经提出了各种其他方法来间接匹配特征（Zagoruyko和Komodakis 2017；Kim等人 2018；Heo等人 2019c；Passban等人 2021；Chen等人 2021；Wang等人 2020b）。具体来说，Zagoruyko和Komodakis（2017）从原始特征图派生出一个“注意力图”来表达知识。Huang和Wang（2017）使用神经元选择性转移方法推广了注意力图。Passalis和Tefas（2018）通过匹配特征空间中的概率分布来转移知识。为了更容易地转移教师知识，Kim等人（2018）引入了所谓的“因子”，作为中间表示的更易理解的形式。为了减少教师和学生之间的性能差距，Jin等人（2019）提出了路由约束提示学习，通过教师提示层的输出来监督学生。最近，Heo等人（2019c）提出使用隐藏神经元的激活边界来进行知识转移。有趣的是，教师模型中间层的参数共享以及基于响应的知识也被用作教师知识（Zhou等人 2018）。为了匹配教师和学生之间的语义，Chen等人（2021）提出了跨层知识蒸馏，通过注意力分配自适应地为每个学生层指派适当的教师层。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><hr></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-4="894,2041"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="473" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> A hint means the output of a teacher’s hidden layer that supervises the student's learning.<div style="background-color: #d6d6d6;margin: 12px 0;">提示意味着教师隐藏层的输出，该输出监督学生的学习。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-0524f229-3337-409a-bbf5-e00346d2fe08" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><hr></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-ea5a9f07-7bb1-4bc9-be70-e38d370e510a" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-5="140,166">Table 1 A summay of feature-based knowledge<div style="background-color: #d6d6d6;margin: 12px 0;">表1 基于特征的知识总结</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-ea5a9f07-7bb1-4bc9-be70-e38d370e510a" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-5="139,208"><div class="table-container"><table class="fixed-table"><thead><tr><th>Methods</th><th>Knowledge types</th><th>Knowledge sources</th><th>Distillation losses</th></tr></thead><tbody><tr><td>Fitnet (Romero et al. 2015)</td><td>Feature representation</td><td>Hint layer</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="474" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>NST (Huang and Wang 2017)</td><td>Neuron selectivity patterns</td><td>Hint layer</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="475" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>M</mi><mi>M</mi><mi>D</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>AT (Zagoruyko and Komodakis 2017)</td><td>Attention maps</td><td>Multi-layer group</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="476" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>FT (Kim et al. 2018)</td><td>Paraphraser</td><td>Multi-layer group</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="477" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>Rocket Launching (Zhou et al. 2018)</td><td>Sharing parameters</td><td>Hint layer</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="478" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>KR (Liu et al. 2019c)</td><td>Parameters distribution</td><td>Multi-layer group</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="479" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>C</mi><mi>E</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>AB (Heo et al. 2019c)</td><td>Activation boundaries</td><td>Pre-ReLU</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="480" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>Shen et al. (2019a</td><td>Knowledge amalgamation</td><td>Hint layer</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="481" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>Heo et al. (2019a)</td><td>Margin ReLU</td><td>Pre-ReLU</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="482" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>FN (Xu et al. 2020b)</td><td>Feature representation</td><td>Fully-connected layer</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="483" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>C</mi><mi>E</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>DFA (Guan et al. 2020)</td><td>Feature aggregation</td><td>Hint layer</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="484" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>AdaIN (Yang et al. 2020a)</td><td>Feature statistics</td><td>Hint layer</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="485" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>FN (Xu et al. 2020b)</td><td>Feature representation</td><td>Penultimate layer</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="486" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>C</mi><mi>E</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>EC-KD (Wang et al. 2020b)</td><td>Feature representation</td><td>Hint layer</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="487" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>ALP-KD (Passban et al. 2021)</td><td>Attention-based layer projection</td><td>Hint layer</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="488" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>SemCKD (Chen et al. 2021)</td><td>Feature maps</td><td>Hint layer</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="489" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr></tbody></table></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-ea5a9f07-7bb1-4bc9-be70-e38d370e510a" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-5="176,1728">Generally, the distillation loss for feature-based knowledge transfer can be formulated as<div style="background-color: #d6d6d6;margin: 12px 0;">通常，基于特征的知识迁移中的蒸馏损失可以表示为</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-ea5a9f07-7bb1-4bc9-be70-e38d370e510a" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><paragraphpositioning data-position-5="140,1837"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" width="full" tabindex="0" ctxtmenu_counter="490" style="font-size: 122.8%; min-width: 26.078em; position: relative;"><mjx-math width="full" display="true" class="MJX-TEX" aria-hidden="true"><mjx-mtable width="full" side="right" style="min-width: 26.078em;"><mjx-table style="width: auto; min-width: 21.922em; margin: 0px 2.078em;"><mjx-itable width="full"><mjx-mlabeledtr><mjx-mtd><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c46"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c44"></mjx-c><mjx-c class="mjx-cA0"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A6"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A6"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mlabeledtr></mjx-itable></mjx-table><mjx-labels style="width: 26.078em;"><mjx-itable align="right" style="right: 0px;"><mjx-mtr style="height: 1em;"><mjx-mtd id="mjx-eqn:4"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c28"></mjx-c><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c29"></mjx-c></mjx-mtext><mjx-tstrut style="height: 1em; vertical-align: -0.25em;"></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-labels></mjx-mtable></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd><mtext>(4)</mtext></mtd><mtd><msub><mrow data-mjx-texclass="ORD"><mi>L</mi></mrow><mrow data-mjx-texclass="ORD"><mtext>FeaD&nbsp;</mtext></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>=</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Φ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Φ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo></mtd></mlabeledtr></mtable></math></mjx-assistive-mml></mjx-container></paragraphpositioning></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-ea5a9f07-7bb1-4bc9-be70-e38d370e510a" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-5="141,1925">where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="491" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="492" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> are the feature maps of the intermediate layers of teacher and student models, respectively. The transformation functions, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="493" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A6"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Φ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="494" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A6"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Φ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> ,are usually applied when the feature maps of teacher and student models are not in the same shape. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="495" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi><mi>n</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>m</mi><mi>i</mi></mrow><mo>−</mo></math></mjx-assistive-mml></mjx-container> larity function used to match the feature maps of teacher and student models. A general feature-based KD model is shown in Fig. 6. We also summarize different types of feature-based knowledge in Table 1 from the perspective of feature types, source layers,and distillation loss. Specifically, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="496" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo></math></mjx-assistive-mml></mjx-container> <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="497" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>C</mi><mi>E</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>n</mi><mi>d</mi></mrow><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>M</mi><mi>M</mi><mi>D</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi><mi>n</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow><msub><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub></math></mjx-assistive-mml></mjx-container> -norm distance, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="498" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub></math></mjx-assistive-mml></mjx-container> -norm distance, cross-entropy loss and maximum mean discrepancy loss, respectively. Though feature-based knowledge transfer provides favorable information for the learning of the student model, how to effectively choose the hint layers from the teacher model and the guided layers from the student model remains to be further investigated (Romero et al. 2015). Due to the significant differences between sizes of hint and guided layers, how to properly match feature representations of teacher and student also needs to be explored.<div style="background-color: #d6d6d6;margin: 12px 0;">其中 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="499" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="500" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 分别是教师模型和学生模型中间层的特征图。当教师模型和学生模型的特征图形状不同时，通常应用转换函数 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="501" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A6"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Φ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="502" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A6"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Φ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container>。<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="503" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi><mi>n</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>m</mi><mi>i</mi></mrow><mo>−</mo></math></mjx-assistive-mml></mjx-container> 是用于匹配教师模型和学生模型特征图的一致性函数。一个通用的基于特征的知识蒸馏模型如图6所示。我们还从特征类型、源层和蒸馏损失的角度总结了表1中的不同类型基于特征的知识。具体来说，<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="504" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo></math></mjx-assistive-mml></mjx-container> <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="505" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>C</mi><mi>E</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>n</mi><mi>d</mi></mrow><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>M</mi><mi>M</mi><mi>D</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi><mi>n</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow><msub><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub></math></mjx-assistive-mml></mjx-container> -范数距离、<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="506" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msub></math></mjx-assistive-mml></mjx-container> -范数距离、交叉熵损失和最大均值差异损失分别代表。尽管基于特征的知识迁移为学生模型的学习提供了有利信息，但如何有效选择教师模型中的提示层和学生模型中的引导层仍然是进一步研究的课题（Romero等人，2015年）。由于提示层和引导层的大小存在显著差异，如何恰当地匹配教师模型和学生模型的特征表示也需要进行探索。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-ea5a9f07-7bb1-4bc9-be70-e38d370e510a" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-5="892,1547">2.3 Relation-Based Knowledge<div style="background-color: #d6d6d6;margin: 12px 0;">2.3 基于关系的知识</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-ea5a9f07-7bb1-4bc9-be70-e38d370e510a" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-5="893,1631">Both response-based and feature-based knowledge use the outputs of specific layers in the teacher model. Relation-based knowledge further explores the relationships between different layers or data samples.<div style="background-color: #d6d6d6;margin: 12px 0;">响应型知识和特征型知识都使用教师模型中特定层的输出。基于关系的知识进一步探索不同层或数据样本之间的关系。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-ea5a9f07-7bb1-4bc9-be70-e38d370e510a" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-5="922,1781">To explore the relationships between different feature maps, Yim et al. (2017) proposed a flow of solution process (FSP), which is defined by the Gram matrix between two layers. The FSP matrix summarizes the relations between pairs of feature maps. It is calculated using the inner products between features from two layers. Using the correlations between feature maps as the distilled knowledge, knowledge distillation via singular value decomposition was proposed to extract key information in the feature maps (Lee et al. 2018). To use the knowledge from multiple teachers, Zhang and Peng (2018) formed two graph by respectively using the logits and features of each teacher model as the nodes. Specifically, the importance and relationships of the different teachers are modeled by the logits and representation graphs before the knowledge transfer (Zhang and Peng 2018). Multi-head graph-based knowledge distillation was proposed by Lee and Song (2019). The graph knowledge is the intra-data relations between any two feature maps via multi-head attention network. To explore the pairwise hint information, the student model also mimics the mutual information flow from pairs of hint layers of the teacher model (Passalis et al. 2020b). In general, the distillation loss of relation-based knowledge based on the relations of feature maps can be formulated as<div style="background-color: #d6d6d6;margin: 12px 0;">为了探索不同特征图之间的关系，Yim等人（2017年）提出了解决方案流程（FSP），该流程由两个层之间的Gram矩阵定义。FSP矩阵总结了特征图对之间的关系。它是通过计算两个层中特征的内积来计算的。利用特征图之间的相关性作为提炼的知识，通过奇异值分解进行知识蒸馏被提出用于提取特征图中的关键信息（Lee等人，2018年）。为了使用来自多个教师的知识，Zhang和Peng（2018年）分别使用每个教师模型的logits和特征作为节点形成了两个图。具体来说，不同教师的重要性和关系是通过在知识转移之前的logits和表示图来建模的（Zhang和Peng 2018年）。Lee和Song（2019年）提出了基于多头的图知识蒸馏。图知识是通过多头注意力网络在任意两个特征图之间的内部数据关系。为了探索成对提示信息，学生模型还模仿了教师模型提示层对的相互信息流（Passalis等人，2020b年）。总的来说，基于特征图关系的关联知识蒸馏损失可以表示为</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-9df891c3-9e6d-448b-b222-324ce36faf35" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><paragraphpositioning data-position-6="141,740"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" width="full" tabindex="0" ctxtmenu_counter="507" style="font-size: 122.8%; min-width: 24.125em; position: relative;"><mjx-math width="full" display="true" class="MJX-TEX" aria-hidden="true"><mjx-mtable width="full" side="right" style="min-width: 24.125em;"><mjx-table style="width: auto; min-width: 19.969em; margin: 0px 2.078em;"><mjx-itable width="full"><mjx-mlabeledtr><mjx-mtd><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.184em;"><mjx-texatom size="s" texclass="ORD"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.289em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A8"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.457em; margin-bottom: -0.551em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2C6 TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.457em; margin-bottom: -0.513em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-c2C7"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A8"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.457em; margin-bottom: -0.551em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2C6 TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.457em; margin-bottom: -0.513em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-c2C7"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mlabeledtr></mjx-itable></mjx-table><mjx-labels style="width: 24.125em;"><mjx-itable align="right" style="right: 0px;"><mjx-mtr style="height: 1.799em;"><mjx-mtd id="mjx-eqn:5"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c28"></mjx-c><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c29"></mjx-c></mjx-mtext><mjx-tstrut style="height: 1.799em; vertical-align: -0.649em;"></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-labels></mjx-mtable></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd><mtext>(5)</mtext></mtd><mtd><msub><mrow data-mjx-texclass="ORD"><mi>L</mi></mrow><mrow data-mjx-texclass="ORD"><mi>R</mi><mi>e</mi><mi>l</mi><mi>D</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>=</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><msup><mrow data-mjx-texclass="ORD"><mi>R</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msup></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Ψ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mover><mi>f</mi><mo>^</mo></mover></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mover><mi>f</mi><mo stretchy="false">ˇ</mo></mover></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Ψ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mover><mi>f</mi><mo>^</mo></mover></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mover><mi>f</mi><mo stretchy="false">ˇ</mo></mover></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo></mtd></mlabeledtr></mtable></math></mjx-assistive-mml></mjx-container></paragraphpositioning></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-9df891c3-9e6d-448b-b222-324ce36faf35" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-6="142,861">where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="508" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="509" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> are the feature maps of teacher and student models, respectively. Pairs of feature maps are chosen from the teacher model, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="510" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.457em; margin-bottom: -0.551em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2C6 TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mover><mi>f</mi><mo>^</mo></mover></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="511" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.457em; margin-bottom: -0.513em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-c2C7"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mover><mi>f</mi><mo stretchy="false">ˇ</mo></mover></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> ,and from the student model, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="512" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.457em; margin-bottom: -0.551em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2C6 TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mover><mi>f</mi><mo>^</mo></mover></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="513" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.457em; margin-bottom: -0.513em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-c2C7"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A8"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A8"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mover><mi>f</mi><mo stretchy="false">ˇ</mo></mover></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mo>.</mo><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Ψ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>n</mi><mi>d</mi></mrow><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Ψ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>r</mi><mi>e</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mi>f</mi><mi>u</mi><mi>n</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>p</mi><mi>a</mi><mi>i</mi><mi>r</mi><mi>s</mi></mrow></math></mjx-assistive-mml></mjx-container> of feature maps from the teacher and student models. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="514" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.184em;"><mjx-texatom size="s" texclass="ORD"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.289em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><msup><mrow data-mjx-texclass="ORD"><mi>R</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msup></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> indicates the correlation function between the teacher and student feature maps.<div style="background-color: #d6d6d6;margin: 12px 0;">其中 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="515" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="516" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>f</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> 分别是教师模型和学生模型的特征图。从教师模型中选择特征图对 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="517" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.457em; margin-bottom: -0.551em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2C6 TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mover><mi>f</mi><mo>^</mo></mover></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="518" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.457em; margin-bottom: -0.513em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-c2C7"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mover><mi>f</mi><mo stretchy="false">ˇ</mo></mover></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>，以及从学生模型中选择特征图对 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="519" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.457em; margin-bottom: -0.551em;"><mjx-mo class="mjx-sop" style="width: 0px; margin-left: -0.278em;"><mjx-c class="mjx-c2C6 TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mover><mi>f</mi><mo>^</mo></mover></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="520" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.105em; padding-left: 0.457em; margin-bottom: -0.513em;"><mjx-mo class="mjx-n" style="width: 0px; margin-left: -0.25em;"><mjx-c class="mjx-c2C7"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A8"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A8"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mover><mi>f</mi><mo stretchy="false">ˇ</mo></mover></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mo>.</mo><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Ψ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>n</mi><mi>d</mi></mrow><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Ψ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>r</mi><mi>e</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mi>f</mi><mi>u</mi><mi>n</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>p</mi><mi>a</mi><mi>i</mi><mi>r</mi><mi>s</mi></mrow></math></mjx-assistive-mml></mjx-container>。<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="521" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.184em;"><mjx-texatom size="s" texclass="ORD"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.289em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><msup><mrow data-mjx-texclass="ORD"><mi>R</mi></mrow><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow></msup></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 表示教师模型和学生特征图之间的相关函数。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-9df891c3-9e6d-448b-b222-324ce36faf35" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-6="174,1120">Traditional knowledge transfer methods often involve individual knowledge distillation. The individual soft targets of a teacher are directly distilled into student. In fact, the distilled knowledge contains not only feature information but also mutual relations of data samples (You et al. 2017; Park et al. 2019). Specifically, Liu et al. (2019g) proposed a robust and effective knowledge distillation method via instance relationship graph. The transferred knowledge in instance relationship graph contains instance features, instance relationships and the feature space transformation cross layers. Park et al. (2019) proposed a relational knowledge distillation, which transfers the knowledge from instance relations. Based on idea of manifold learning, the student network is learned by feature embedding, which preserves the feature similarities of samples in the intermediate layers of the teacher networks (Chen et al. 2021). The relations between data samples are modelled as probabilistic distribution using feature representations of data (Passalis and Tefas 2018; Passalis et al. 2020a). The probabilistic distributions of teacher and student are matched by knowledge transfer. (Tung and Mori 2019) proposed a similarity-preserving knowledge distillation method. In particular, similarity-preserving knowledge, which arises from the similar activations of input pairs in the teacher networks, is transferred into the student network, with the pairwise similarities preserved. Peng et al. (2019a) proposed a knowledge distillation method based on correlation congruence, in which the distilled knowledge contains both the instance-level information and the correlations between instances. Using the correlation congruence for distillation, the student network can learn the correlation between instances.<div style="background-color: #d6d6d6;margin: 12px 0;">传统的知识传递方法通常涉及个体知识蒸馏。教师的个体软目标直接蒸馏到学生中。实际上，蒸馏的知识不仅包含特征信息，还包含数据样本之间的相互关系（You等人，2017；Park等人，2019）。具体来说，Liu等人（2019g）提出了一种通过实例关系图进行稳健有效的知识蒸馏方法。实例关系图中的迁移知识包含实例特征、实例关系以及跨层的特征空间转换。Park等人（2019）提出了一种关系知识蒸馏，它通过实例关系迁移知识。基于流形学习的思想，学生网络通过特征嵌入进行学习，保持了样本在教师网络中间层中的特征相似性（Chen等人，2021）。数据样本之间的关系使用数据的特征表示建模为概率分布（Passalis和Tefas 2018；Passalis等人，2020a）。通过知识传递匹配教师和学生的概率分布。（Tung和Mori 2019）提出了一种保持相似性的知识蒸馏方法。特别是，保持相似性的知识，源自教师网络中输入对的相似激活，被迁移到学生网络中，同时保持成对相似性。Peng等人（2019a）提出了一种基于相关一致性的知识蒸馏方法，其中蒸馏的知识包含实例级信息以及实例之间的相关性。使用相关一致性进行蒸馏，学生网络可以学习实例之间的相关性。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-9df891c3-9e6d-448b-b222-324ce36faf35" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-6="951,154"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_6.jpg?x=951&amp;y=154&amp;w=597&amp;h=312"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-9df891c3-9e6d-448b-b222-324ce36faf35" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-6="894,480">Fig. 7 The generic instance relation-based knowledge distillation<div style="background-color: #d6d6d6;margin: 12px 0;">图7 通用实例关系基于的知识蒸馏</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-9df891c3-9e6d-448b-b222-324ce36faf35" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-6="927,733">As described above, the distillation loss of relation-based knowledge based on the instance relations can be formulated as<div style="background-color: #d6d6d6;margin: 12px 0;">如上所述，基于实例关系的关系知识蒸馏的损失可以表示为</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-9df891c3-9e6d-448b-b222-324ce36faf35" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><paragraphpositioning data-position-6="893,891"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" width="full" tabindex="0" ctxtmenu_counter="522" style="font-size: 122.8%; min-width: 22.526em; position: relative;"><mjx-math width="full" display="true" class="MJX-TEX" aria-hidden="true"><mjx-mtable width="full" side="right" style="min-width: 22.526em;"><mjx-table style="width: auto; min-width: 18.37em; margin: 0px 2.078em;"><mjx-itable width="full"><mjx-mlabeledtr><mjx-mtd><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.184em;"><mjx-texatom size="s" texclass="ORD"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.289em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D713 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D713 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mlabeledtr></mjx-itable></mjx-table><mjx-labels style="width: 22.526em;"><mjx-itable align="right" style="right: 0px;"><mjx-mtr style="height: 1.044em;"><mjx-mtd id="mjx-eqn:6"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c28"></mjx-c><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c29"></mjx-c></mjx-mtext><mjx-tstrut style="height: 1.044em; vertical-align: -0.294em;"></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-labels></mjx-mtable></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd><mtext>(6)</mtext></mtd><mtd><msub><mrow data-mjx-texclass="ORD"><mi>L</mi></mrow><mrow data-mjx-texclass="ORD"><mi>R</mi><mi>e</mi><mi>l</mi><mi>D</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>=</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><msup><mrow data-mjx-texclass="ORD"><mi>R</mi></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msup></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>ψ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow><mrow data-mjx-texclass="ORD"><mi>j</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>ψ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow><mrow data-mjx-texclass="ORD"><mi>j</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo></mtd></mlabeledtr></mtable></math></mjx-assistive-mml></mjx-container></paragraphpositioning></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-9df891c3-9e6d-448b-b222-324ce36faf35" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-6="893,989">where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="523" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow><mrow data-mjx-texclass="ORD"><mi>j</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>∈</mo><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="524" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow><mrow data-mjx-texclass="ORD"><mi>j</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>∈</mo><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> ,and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="525" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="526" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> are the sets of feature representations from the teacher and student models,respectively. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="527" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D713 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D713 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>ψ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>n</mi><mi>d</mi></mrow><msub><mrow data-mjx-texclass="ORD"><mi>ψ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>r</mi><mi>e</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mi>f</mi><mi>u</mi><mi>n</mi><mi>c</mi></mrow><mo>−</mo></math></mjx-assistive-mml></mjx-container> tions of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="528" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow><mrow data-mjx-texclass="ORD"><mi>j</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="529" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.184em;"><mjx-texatom size="s" texclass="ORD"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.289em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow><mrow data-mjx-texclass="ORD"><mi>j</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>.</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><msup><mrow data-mjx-texclass="ORD"><mi>R</mi></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msup></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi><mi>s</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>l</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>f</mi><mi>u</mi><mi>n</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></math></mjx-assistive-mml></mjx-container> between the teacher and student feature representations. A typical instance relation-based KD model is shown in Fig. 7.<div style="background-color: #d6d6d6;margin: 12px 0;">其中 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="530" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow><mrow data-mjx-texclass="ORD"><mi>j</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>∈</mo><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="531" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow><mrow data-mjx-texclass="ORD"><mi>j</mi></mrow></msub></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>∈</mo><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> 分别是教师模型和学生模型的特征表示集合，<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="532" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> 和 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="533" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> 是教师模型和学生特征表示之间的 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="534" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D713 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D713 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>ψ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>n</mi><mi>d</mi></mrow><msub><mrow data-mjx-texclass="ORD"><mi>ψ</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>r</mi><mi>e</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mi>f</mi><mi>u</mi><mi>n</mi><mi>c</mi></mrow><mo>−</mo></math></mjx-assistive-mml></mjx-container> 关系。一个典型的基于实例关系的知识蒸馏模型如图 7 所示。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-9df891c3-9e6d-448b-b222-324ce36faf35" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-6="924,1211">Distilled knowledge can be categorized from different perspectives, such as structured knowledge of the data (Liu et al. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="535" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mstyle><mjx-mspace style="width: 0.278em;"></mjx-mspace></mjx-mstyle><mjx-mi class="mjx-n"><mjx-c class="mjx-c67"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>2019</mn></mrow><mrow data-mjx-texclass="ORD"><mstyle scriptlevel="0"><mspace width="0.278em"></mspace></mstyle><mi mathvariant="normal">g</mi></mrow></math></mjx-assistive-mml></mjx-container> ; Chen et al. 2021; Peng et al. 2019a; Tung and Mori 2019; Tian et al. 2020), privileged information about input features (Lopez-Paz et al. 2016; Vapnik and Izmailov 2015). A summary of differnet categories of relation-based knowledge is shown in Table 2. Specifically, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="536" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.153em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>M</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>H</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>W</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>n</mi><mi>d</mi></mrow><mo>∥</mo><mo>.</mo><msub><mrow data-mjx-texclass="ORD"><mo>∥</mo></mrow><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow></msub><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>r</mi><mi>e</mi><mi>E</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>h</mi><mi>M</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi></mrow><mo>,</mo></math></mjx-assistive-mml></mjx-container> Huber loss, Angle-wise loss and Frobenius norm, respectively. Although some types of relation-based knowledge are provided recently, how to model the relation information from feature maps or data samples as knowledge still deserves further study.<div style="background-color: #d6d6d6;margin: 12px 0;">知识蒸馏可以从不同的角度进行分类，例如数据结构化的知识（Liu et al. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="537" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mstyle><mjx-mspace style="width: 0.278em;"></mjx-mspace></mjx-mstyle><mjx-mi class="mjx-n"><mjx-c class="mjx-c67"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>2019</mn></mrow><mrow data-mjx-texclass="ORD"><mstyle scriptlevel="0"><mspace width="0.278em"></mspace></mstyle><mi mathvariant="normal">g</mi></mrow></math></mjx-assistive-mml></mjx-container> ; Chen et al. 2021; Peng et al. 2019a; Tung 和 Mori 2019; Tian et al. 2020），输入特征的特权信息（Lopez-Paz et al. 2016; Vapnik 和 Izmailov 2015）。不同类别的关系型知识的总结如表 2 所示。具体来说，<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="538" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.153em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>M</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>H</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>W</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>n</mi><mi>d</mi></mrow><mo>∥</mo><mo>.</mo><msub><mrow data-mjx-texclass="ORD"><mo>∥</mo></mrow><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow></msub><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>r</mi><mi>e</mi><mi>E</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>h</mi><mi>M</mi><mi>o</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi></mrow><mo>,</mo></math></mjx-assistive-mml></mjx-container> 分别是 Huber 损失、角度损失和弗罗贝尼乌斯范数。尽管最近提供了某些类型的关系型知识，但如何将特征图或数据样本中的关系信息建模为知识仍值得进一步研究。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-9df891c3-9e6d-448b-b222-324ce36faf35" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-6="890,1763">3 Distillation Schemes<div style="background-color: #d6d6d6;margin: 12px 0;">3 知识蒸馏方案</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-9df891c3-9e6d-448b-b222-324ce36faf35" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-6="892,1850">In this section, we discuss the distillation schemes (i.e. training schemes) for both teacher and student models. According to whether the teacher model is updated simultaneously with the student model or not, the learning schemes of knowledge distillation can be directly divided into three main categories: offline distillation, online distillation and self-distillation, as shown in Fig. 8.<div style="background-color: #d6d6d6;margin: 12px 0;">在本节中，我们讨论教师模型和学生模型的蒸馏方案（即训练方案）。根据教师模型是否与学生模型同时更新，知识蒸馏的学习方案可以直接分为三大类：离线蒸馏、在线蒸馏和自蒸馏，如图 8 所示。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-720073b4-da88-4061-8ad3-ceda9d39aac2" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-7="139,166">Table 2 A summary of relation-based knowledge<div style="background-color: #d6d6d6;margin: 12px 0;">表 2 基于关系知识的总结</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-720073b4-da88-4061-8ad3-ceda9d39aac2" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-7="136,208"><div class="table-container"><table class="fixed-table"><thead><tr><th>Methods</th><th>Knowledge types</th><th>Knowledge sources</th><th>Distillation losses</th></tr></thead><tbody><tr><td>FSP (Yim et al. 2017)</td><td>FSP matrix</td><td>End of multi-layer group</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="539" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>You et al. (2017)</td><td>Instance relation</td><td>Hint layers</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="540" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>Zhang and Peng (2018)</td><td>Logits graph, Representation graph</td><td>Softmax layers, Hint layers</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="541" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>M</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>M</mi><mi>M</mi><mi>D</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>DarkRank (Chen et al. 2018c)</td><td>Similarity DarkRank</td><td>Fully-connected layers</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="542" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>K</mi><mi>L</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>MHGD (Lee and Song 2019)</td><td>Multi-head graph</td><td>Hint layers</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="543" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>K</mi><mi>L</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>RKD (Park et al. 2019)</td><td>Instance relation</td><td>Fully-connected layers</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="544" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.153em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>H</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>W</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>IRG (Liu et al. 2019g)</td><td>Instance relationship graph</td><td>Hint layers</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="545" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>SP (Tung and Mori 2019)</td><td>Similarity matrix</td><td>Hint layers</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="546" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-msub><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∥</mo><mo>⋅</mo><msub><mrow data-mjx-texclass="ORD"><mo>∥</mo></mrow><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow></msub></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>CCKD (Peng et al. 2019a)</td><td>Instance relation</td><td>Hint layers</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="547" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>MLKD (Yu et al. 2019)</td><td>Instance relation</td><td>Hint layers</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="548" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-msub><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2225"></mjx-c></mjx-mo></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∥</mo><mo>⋅</mo><msub><mrow data-mjx-texclass="ORD"><mo>∥</mo></mrow><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow></msub></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>PKT(Passalis et al. 2020a)</td><td>Similarity probability distribution</td><td>Fully-connected layers</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="549" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>K</mi><mi>L</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>Passalis et al. (2020b)</td><td>Mutual information flow</td><td>Hint layers</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="550" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>K</mi><mi>L</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>LP (Chen et al. 2021)</td><td>Instance relation</td><td>Hint layers</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="551" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr></tbody></table></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-720073b4-da88-4061-8ad3-ceda9d39aac2" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-7="193,835"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_7.jpg?x=193&amp;y=835&amp;w=619&amp;h=564"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-720073b4-da88-4061-8ad3-ceda9d39aac2" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-7="142,1418">Fig. 8 Different distillations. The red color for "pre-trained" means networks are learned before distillation and the yellow color for "to be trained" means networks are learned during distillation<div style="background-color: #d6d6d6;margin: 12px 0;">图 8 不同的蒸馏方法。红色表示“预训练”意味着网络在蒸馏前学习，黄色表示“待训练”意味着网络在蒸馏过程中学习</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-720073b4-da88-4061-8ad3-ceda9d39aac2" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-7="138,1582">3.1 Offline Distillation<div style="background-color: #d6d6d6;margin: 12px 0;">3.1 离线蒸馏</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-720073b4-da88-4061-8ad3-ceda9d39aac2" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-7="142,1669">Most of previous knowledge distillation methods work offline. In vanilla knowledge distillation (Hinton et al. 2015), the knowledge is transferred from a pre-trained teacher model into a student model. Therefore, the whole training process has two stages, namely: 1) the large teacher model is first trained on a set of training samples before distillation; and 2) the teacher model is used to extract the knowledge in the forms of logits or the intermediate features, which are then used to guide the training of the student model during distillation.<div style="background-color: #d6d6d6;margin: 12px 0;">大多数先前的知识蒸馏方法都是离线工作的。在传统的知识蒸馏中（Hinton等人，2015年），知识是从预训练的教师模型转移到学生模型中。因此，整个训练过程分为两个阶段，即：1）在蒸馏之前，先对大型教师模型进行训练，训练集是一组训练样本；2）使用教师模型提取知识，这些知识以 logits 或中间特征的形式存在，然后用于在蒸馏过程中指导学生模型的训练。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-720073b4-da88-4061-8ad3-ceda9d39aac2" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-7="175,2036">The first stage in offline distillation is usually not discussed as part of knowledge distillation, i.e., it is assumed that the teacher model is pre-defined. Little attention is paid to the teacher model structure and its relationship with the student model. Therefore, the offline methods mainly focus on improving different parts of the knowledge transfer, including the design of knowledge (Hinton et al. 2015; Romero et al. 2015) and the loss functions for matching features or distributions matching (Huang and Wang 2017; Passalis and Tefas 2018; Zagoruyko and Komodakis 2017; Mirzadeh et al. 2020; Li et al. 2020d; Heo et al. 2019b; Asif et al. 2020). The main advantage of offline methods is that they are simple and easy to be implemented. For example, the teacher model may contain a set of models trained using different software packages, possibly located on different machines. The knowledge can be extracted and stored in a cache.<div style="background-color: #d6d6d6;margin: 12px 0;">在离线蒸馏的第一阶段通常不作为知识蒸馏的一部分来讨论，即假定教师模型是预先定义的。人们对教师模型的结构及其与学生模型的关系关注较少。因此，离线方法主要关注改进知识转移的不同部分，包括知识的设计（Hinton等人，2015年；Romero等人，2015年）以及匹配特征或分布匹配的损失函数（Huang和Wang，2017年；Passalis和Tefas，2018年；Zagoruyko和Komodakis，2017年；Mirzadeh等人，2020年；Li等人，2020d；Heo等人，2019b；Asif等人，2020年）。离线方法的主要优点是简单且易于实施。例如，教师模型可能包含使用不同软件包训练的一组模型，这些模型可能位于不同的计算机上。知识可以被提取并存储在缓存中。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-720073b4-da88-4061-8ad3-ceda9d39aac2" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-7="922,1357">The offline distillation methods usually employ one-way knowledge transfer and two-phase training procedure. However, the complex high-capacity teacher model with huge training time can not be avoided, while the training of the student model in offline distillation is usually efficient under the guidance of the teacher model. Moreover, the capacity gap between large teacher and small student always exists, and student often largely relies on teacher.<div style="background-color: #d6d6d6;margin: 12px 0;">离线蒸馏方法通常采用单向知识传递和两阶段训练过程。然而，复杂的高容量教师模型及其巨大的训练时间是无法避免的，而在离线蒸馏中，学生模型在教师模型的指导下通常训练效率较高。此外，大容量教师与小容量学生之间的能力差距总是存在，学生往往严重依赖教师。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-720073b4-da88-4061-8ad3-ceda9d39aac2" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-7="890,1693">3.2 Online Distillation<div style="background-color: #d6d6d6;margin: 12px 0;">3.2 在线蒸馏</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-720073b4-da88-4061-8ad3-ceda9d39aac2" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-7="893,1779">Although offline distillation methods are simple and effective, some issues in offline distillation have attracted increasing attention from the research community (Mirzadeh et al. 2020). To overcome the limitation of offline distillation, online distillation is proposed to further improve the performance of the student model, especially when a large-capacity high performance teacher model is not available (Zhang et al. 2018b; Chen et al. 2020a). In online distillation, both the teacher model and the student model are updated simultaneously, and the whole knowledge distillation framework is end-to-end trainable.<div style="background-color: #d6d6d6;margin: 12px 0;">尽管离线蒸馏方法简单有效，但离线蒸馏中的一些问题已经引起了研究界越来越多的关注（Mirzadeh等人，2020年）。为了克服离线蒸馏的限制，提出了在线蒸馏，以进一步改善学生模型的性能，尤其是在没有大容量高性能教师模型可用的情况下（Zhang等人，2018b；Chen等人，2020a）。在在线蒸馏中，教师模型和学生模型同时更新，整个知识蒸馏框架是端到端可训练的。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-26f5c3cc-02e4-4f5a-8bbe-db1a9098a1fc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-8="174,239">A variety of online knowledge distillation methods have been proposed, especially in the last few years (Zhang et al. 2018b; Chen et al. 2020a; Xie et al. 2019; Anil et al. 2018; Kim et al. 2019b; Zhou et al. 2018; Walawalkar et al. 2020; Wu and Gong 2021; Zhang et al. 2021a). Specifically, in deep mutual learning (Zhang et al. 2018b), multiple neural networks work in a collaborative way. Any one network can be the student model and other models can be the teacher during the training process. To improve generalization ability, deep mutual learning is extended by using ensemble of soft logits (Guo et al. 2020). Chen et al. (2020a) further introduced auxiliary peers and a group leader into deep mutual learning to form a diverse set of peer models. To reduce the computational cost, Zhu and Gong (2018) proposed a multi-branch architecture, in which each branch indicates a student model and different branches share the same backbone network. Rather than using the ensemble of logits, Kim et al. (2019b) introduced a feature fusion module to construct the teacher classifier. Xie et al. (2019) replaced the convolution layer with cheap convolution operations to form the student model. Anil et al. (2018) employed online distillation to train large-scale distributed neural network, and proposed a variant of online distillation called co-distillation. Co-distillation in parallel trains multiple models with the same architectures and any one model is trained by transferring the knowledge from the other models. Recently, an online adversarial knowledge distillation method is proposed to simultaneously train multiple networks by the discriminators using knowledge from both the class probabilities and a feature map (Chung et al. 2020). Adversarial co-distillation is lately devised by using GAN to generate divergent examples (Zhang et al. 2021a).<div style="background-color: #d6d6d6;margin: 12px 0;">近年来，提出了多种在线知识蒸馏方法（Zhang et al. 2018b; Chen et al. 2020a; Xie et al. 2019; Anil et al. 2018; Kim et al. 2019b; Zhou et al. 2018; Walawalkar et al. 2020; Wu and Gong 2021; Zhang et al. 2021a）。特别是，在深度互学习（Zhang et al. 2018b）中，多个神经网络以协作方式进行工作。任何一个网络都可以作为学生模型，其他模型在训练过程中可以作为教师模型。为了提高泛化能力，通过使用软标签的集成来扩展深度互学习（Guo et al. 2020）。Chen et al. (2020a)进一步在深度互学习中引入了辅助对等模型和组长，以形成一个多样化的对等模型集合。为了降低计算成本，Zhu和Gong（2018）提出了一种多分支架构，其中每个分支代表一个学生模型，不同分支共享相同的主干网络。Kim et al. (2019b)没有使用标签的集成，而是引入了一个特征融合模块来构建教师分类器。Xie et al. (2019)用廉价的卷积操作替代了卷积层，以形成学生模型。Anil et al. (2018)采用在线蒸馏来训练大规模分布式神经网络，并提出了在线蒸馏的一个变体，称为共蒸馏。共蒸馏并行训练具有相同架构的多个模型，任何一个模型都是通过从其他模型转移知识来训练的。最近，提出了一种在线对抗性知识蒸馏方法，通过判别器同时使用来自类别概率和特征图的知识来训练多个网络（Chung et al. 2020）。利用GAN生成不同示例的对抗性共蒸馏是最近设计的（Zhang et al. 2021a）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-26f5c3cc-02e4-4f5a-8bbe-db1a9098a1fc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-8="175,1412">Online distillation is a one-phase end-to-end training scheme with efficient parallel computing. However, existing online methods (e.g., mutual learning) usually fails to address the high-capacity teacher in online settings, making it an interesting topic to further explore the relationships between the teacher and student model in online settings.<div style="background-color: #d6d6d6;margin: 12px 0;">在线蒸馏是一种具有高效并行计算的一阶段端到端训练方案。然而，现有的在线方法（例如，互学习）通常无法解决在线设置中的高容量教师模型问题，这使得进一步探索在线设置中教师模型与学生模型之间的关系成为一个有趣的研究课题。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-26f5c3cc-02e4-4f5a-8bbe-db1a9098a1fc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-8="140,1659">3.3 Self-Distillation<div style="background-color: #d6d6d6;margin: 12px 0;">3.3 自蒸馏</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-26f5c3cc-02e4-4f5a-8bbe-db1a9098a1fc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-8="141,1741">In self-distillation, the same networks are used for the teacher and the student models (Zhang et al. 2019b; Hou et al. 2019; Zhang and Sabuncu 2020; Yang et al. 2019b; Lee et al. 2019a; Phuong and Lampert 2019b; Lan et al. 2018; Xu and Liu 2019; Mobahi et al. 2020). This can be regarded as a special case of online distillation. Specifically, Zhang et al. (2019b) proposed a new self-distillation method, in which knowledge from the deeper sections of the network is distilled into its shallow sections. Similar to the self-distillation in Zhang et al. (2019b), a self-attention distillation method was proposed for lane detection (Hou et al. 2019). The network utilizes the attention maps of its own layers as distillation targets for its lower layers. Snapshot distillation (Yang et al. 2019b) is a special variant of self-distillation, in which knowledge in the earlier epochs of the network (teacher) is transferred into its later epochs (student) to support a supervised training process within the same network. To further reduce the inference time via the early exit, Phuong and Lampert (2019b) proposed distillation-based training scheme, in which the early exit layer tries to mimic the output of later exit layer during the training. Recently, self-distillation has been theoretically analyzed in Mobahi et al. (2020), and its improved performance experimentally demonstrated in Zhang and Sabuncu (2020).<div style="background-color: #d6d6d6;margin: 12px 0;">在自蒸馏中，教师模型和学生模型使用相同的网络（Zhang et al. 2019b; Hou et al. 2019; Zhang and Sabuncu 2020; Yang et al. 2019b; Lee et al. 2019a; Phuong and Lampert 2019b; Lan et al. 2018; Xu and Liu 2019; Mobahi et al. 2020）。这可以看作是在线蒸馏的一种特殊情况。具体来说，Zhang et al. (2019b) 提出了一种新的自蒸馏方法，其中将网络的深层部分的知识蒸馏到其浅层部分。与 Zhang et al. (2019b) 中的自蒸馏类似，为车道检测提出了一种自注意力蒸馏方法（Hou et al. 2019）。该网络使用其自身层的注意力图作为其较低层的蒸馏目标。快照蒸馏（Yang et al. 2019b）是自蒸馏的一种特殊变体，在这种变体中，网络（教师）早期迭代周期的知识被转移到其后期迭代周期（学生）中，以支持在同一网络内的监督训练过程。为了通过早期退出进一步减少推理时间，Phuong 和 Lampert (2019b) 提出了一个基于蒸馏的训练方案，在该方案中，早期退出层在训练期间尝试模仿后期退出层的输出。最近，Mobahi et al. (2020) 从理论上分析了自蒸馏，Zhang 和 Sabuncu (2020) 实验证明了其改进的性能。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-26f5c3cc-02e4-4f5a-8bbe-db1a9098a1fc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-8="924,679">Furthermore, some interesting self-distillation methods are recently proposed (Yuan et al. 2020; Yun et al. 2020; Hahn and Choi 2019). To be specific, Yuan et al. proposed teacher-free knowledge distillation methods based on the analysis of label smoothing regularization (Yuan et al. 2020). Hahn and Choi proposed a novel self-knowledge distillation method, in which the self-knowledge consists of the predicted probabilities instead of traditional soft probabilities (Hahn and Choi 2019). These predicted probabilities are defined by the feature representations of the training model. They reflect the similarities of data in feature embedding space. Yun et al. proposed class-wise self-knowledge distillation to match the output distributions of the training model between intra-class samples and augmented samples within the same source with the same model (Yun et al. 2020). In addition, the self-distillation proposed by Lee et al. (2019a) is adopted for data augmentation and the self-knowledge of augmentation is distilled into the model itself. Self distillation is also adopted to optimize deep models (the teacher or student networks) with the same architecture one by one (Furlanello et al. 2018; Bagherinezhad et al. 2018). Each network distills the knowledge of the previous network using a teacher-student optimization.<div style="background-color: #d6d6d6;margin: 12px 0;">此外，最近提出了一些有趣的自我蒸馏方法（Yuan et al. 2020; Yun et al. 2020; Hahn and Choi 2019）。具体来说，Yuan等人提出了基于标签平滑正则化分析的无教师知识蒸馏方法（Yuan et al. 2020）。Hahn和Choi提出了一种新颖的自我知识蒸馏方法，其中的自我知识由预测概率组成，而不是传统的软概率（Hahn and Choi 2019）。这些预测概率由训练模型的特征表示定义。它们反映了特征嵌入空间中数据的相似性。Yun等人提出了类-wise自我知识蒸馏，以匹配训练模型在同类样本和同一源中的增强样本之间的输出分布（Yun et al. 2020）。此外，Lee等人（2019a）提出的自我蒸馏被用于数据增强，增强的自我知识被蒸馏到模型本身。自我蒸馏也被用于优化具有相同架构的深度模型（教师或学生网络）一个接一个（Furlanello et al. 2018; Bagherinezhad et al. 2018）。每个网络使用教师-学生优化来蒸馏前一个网络的知识。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-26f5c3cc-02e4-4f5a-8bbe-db1a9098a1fc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-8="923,1523">Besides, offline, online and self distillation can also be intuitively understood from the perspective of human beings teacher-student learning. Offline distillation means the knowledgeable teacher teaches a student knowledge; online distillation means both teacher and student study together with each other; self-distillation means student learn knowledge by oneself. Moreover, just like the human beings learning, these three kinds of distillation can be combined to complement each other due to their own advantages. For example, both self-distillation and online distillation are properly integrated via the multiple knowledge transfer framework (Sun et al. 2021).<div style="background-color: #d6d6d6;margin: 12px 0;">此外，从人类师徒学习的角度出发，也可以直观地理解离线、在线和自蒸馏。离线蒸馏意味着知识渊博的教师教授学生知识；在线蒸馏意味着教师和学生相互学习；自蒸馏意味着学生自学知识。而且，就像人类学习一样，这三种蒸馏方式可以相互结合，互补各自的优点。例如，通过多知识传递框架（Sun等人，2021年），适当地整合了自蒸馏和在线蒸馏。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-fe98b815-bcf4-47ba-a7fe-5ac787369526" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-9="189,155"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_9.jpg?x=189&amp;y=155&amp;w=613&amp;h=407"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-fe98b815-bcf4-47ba-a7fe-5ac787369526" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-9="142,579">Fig. 9 Relationship of the teacher and student models<div style="background-color: #d6d6d6;margin: 12px 0;">图 9 教师模型和学生模型的关系</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-fe98b815-bcf4-47ba-a7fe-5ac787369526" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-9="141,664">4 Teacher-Student Architecture<div style="background-color: #d6d6d6;margin: 12px 0;">4 教师与学生架构</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-fe98b815-bcf4-47ba-a7fe-5ac787369526" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-9="141,751">In knowledge distillation, the teacher-student architecture is a generic carrier to form the knowledge transfer. In other words, the quality of knowledge acquisition and distillation from teacher to student is also determined by how to design the teacher and student networks. In terms of the habits of human beings learning, we hope that a student can find a right teacher. Thus, to well finish capturing and distilling knowledge in knowledge distillation, how to select or design proper structures of teacher and student is very important but difficult problem. Recently, the model setups of teacher and student are almost pre-fixed with unvaried sizes and structures during distillation, so as to easily cause the model capacity gap. However, how to particulary design the architectures of teacher and student and why their architectures are determined by these model setups are nearly missing. In this section, we discuss the relationship between the structures of the teacher model and the student model as illustrated in Fig. 9.<div style="background-color: #d6d6d6;margin: 12px 0;">在知识蒸馏中，教师-学生架构是一种通用载体，用于形成知识传递。换句话说，从教师到学生的知识获取和蒸馏质量也取决于如何设计教师和学生网络。从人类学习习惯的角度来看，我们希望学生能找到一个合适的老师。因此，为了在知识蒸馏中很好地捕捉和提炼知识，如何选择或设计适当的教学结构和学生结构是一个非常重要但困难的课题。最近，教师和学生的模型设置在蒸馏过程中几乎被预设为大小和结构不变，从而容易导致模型容量差距。然而，如何特别设计教师和学生的架构，以及为什么他们的架构由这些模型设置决定，几乎未被探讨。在本节中，我们讨论了如图 9 所示的教师模型结构和学生模型结构之间的关系。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-fe98b815-bcf4-47ba-a7fe-5ac787369526" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-9="176,1413">Knowledge distillation was previously designed to compress an ensemble of deep neural networks in Hinton et al. (2015). The complexity of deep neural networks mainly comes from two dimensions: depth and width. It is usually required to transfer knowledge from deeper and wider neural networks to shallower and thinner neural networks (Romero et al. 2015). The student network is usually chosen to be: (1) a simplified version of a teacher network with fewer layers and fewer channels in each layer (Wang et al. 2018a; Zhu and Gong 2018; Li et al. 2020d); or (2) a quantized version of a teacher network in which the structure of the network is preserved (Polino et al. 2018; Mishra and Marr 2018; Wei et al. 2018; Shin et al. 2019); or (3) a small network with efficient basic operations (Howard et al. 2017; Zhang et al. 2018a; Huang et al. 2017); or (4) a small network with optimized global network structure (Liu et al. 2019i; Xie et al. 2020; Gu and Tresp 2020); or (5) the same network as teacher (Zhang et al. 2018b; Furlanello et al. 2018; Tarvainen and Valpola 2017).<div style="background-color: #d6d6d6;margin: 12px 0;">知识蒸馏最初被设计用于压缩深度神经网络集合，这在 Hinton 等人（2015）的工作中有所描述。深度神经网络的复杂性主要来自两个维度：深度和宽度。通常需要将知识从更深更宽的神经网络转移到更浅更薄的神经网络（Romero 等人 2015）。学生网络通常选择为：（1）教师网络的简化版本，每层具有更少的层和更少的通道（Wang 等人 2018a; Zhu 和 Gong 2018; Li 等人 2020d）；（2）教师网络的量化版本，其中网络的结构被保留（Polino 等人 2018; Mishra 和 Marr 2018; Wei 等人 2018; Shin 等人 2019）；（3）具有高效基本操作的小型网络（Howard 等人 2017; Zhang 等人 2018a; Huang 等人 2017）；（4）具有优化全局网络结构的小型网络（Liu 等人 2019i; Xie 等人 2020; Gu 和 Tresp 2020）；或（5）与教师网络相同的网络（Zhang 等人 2018b; Furlanello 等人 2018; Tarvainen 和 Valpola 2017）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-fe98b815-bcf4-47ba-a7fe-5ac787369526" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-9="923,165">The model capacity gap between the large deep neural network and a small student neural network can degrade knowledge transfer (Mirzadeh et al. 2020; Gao et al. 2021). To effectively transfer knowledge to student networks, a variety of methods have been proposed for a controlled reduction of the model complexity (Zhang et al. 2018b; Nowak and Corso 2018; Crowley et al. 2018; Liu et al. 2019a, i; Wang et al. 2018a; Gu and Tresp 2020). Specifically, Mirzadeh et al. (2020) introduced a teacher assistant to mitigate the training gap between teacher model and student model. The gap is further reduced by residual learning, i.e., the assistant structure is used to learn the residual error (Gao et al. 2021). On the other hand, several recent methods also focus on minimizing the difference in structure of the student model and the teacher model. For example, Polino et al. (2018) combined network quantization with knowledge distillation, i.e., the student model is small and quantized version of the teacher model. Nowak and Corso (2018) proposed a structure compression method which involves transferring the knowledge learned by multiple layers to a single layer. Wang et al. (2018a) progressively performed block-wise knowledge transfer from teacher networks to student networks while preserving the receptive field. In online setting, the teacher networks are usually ensembles of student networks, in which the student models share similar structure (or the same structure) with each other (Zhang et al. 2018b; Zhu and Gong 2018; Furlanello et al. 2018; Chen et al. 2020a).<div style="background-color: #d6d6d6;margin: 12px 0;">大型深度神经网络与小型学生神经网络之间的模型容量差距可能会降低知识迁移效果（Mirzadeh等人，2020；Gao等人，2021）。为了有效地将知识迁移到学生网络，已经提出了多种方法来控制降低模型复杂性（Zhang等人，2018b；Nowak和Corso，2018；Crowley等人，2018；Liu等人，2019a, i；Wang等人，2018a；Gu和Tresp，2020）。特别是，Mirzadeh等人（2020）引入了一个教师助手来减轻教师模型和学生模型之间的训练差距。通过残差学习进一步减少这个差距，即使用助手结构来学习残差误差（Gao等人，2021）。另一方面，一些最近的方法也专注于最小化学生模型和教师模型之间的结构差异。例如，Polino等人（2018）将网络量化与知识蒸馏相结合，即学生模型是教师模型的小型量化版本。Nowak和Corso（2018）提出了一种结构压缩方法，该方法涉及将多层学习到的知识转移到单一层。Wang等人（2018a）在保持感受野的同时，逐步执行从教师网络到学生网络的块状知识迁移。在在线设置中，教师网络通常是学生网络的集合，这些学生模型之间共享相似的结构（或相同的结构）（Zhang等人，2018b；Zhu和Gong，2018；Furlanello等人，2018；Chen等人，2020a）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-fe98b815-bcf4-47ba-a7fe-5ac787369526" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-9="922,1157">Recently, depth-wise separable convolution has been widely used to design efficient neural networks for mobile or embedded devices (Chollet 2017; Howard et al. 2017; Sandler et al. 2018; Zhang et al. 2018a; Ma et al. 2018). Inspired by the success of neural architecture search (or NAS), the performances of small neural networks have been further improved by searching for a global structure based on efficient meta operations or blocks (Wu et al. 2019; Tan et al. 2019; Tan and Le 2019; Radosavovic et al. 2020). Furthermore, the idea of dynamically searching for a knowledge transfer regime also appears in knowledge distillation, e.g., automatically removing redundant layers in a data-driven way using reinforcement learning (Ashok et al. 2018), and searching for optimal student networks given the teacher networks (Liu et al. 2019i; Xie et al. 2020; Gu and Tresp 2020).<div style="background-color: #d6d6d6;margin: 12px 0;">最近，深度可分卷积被广泛用于为移动或嵌入式设备设计高效的神经网络（Chollet 2017；Howard等 2017；Sandler等 2018；Zhang等 2018a；Ma等 2018）。受到神经网络架构搜索（或NAS）成功的启发，通过搜索基于高效元操作或模块的全局结构，小型神经网络的性能得到了进一步提高（Wu等 2019；Tan等 2019；Tan和Le 2019；Radosavovic等 2020）。此外，动态搜索知识迁移机制的想法也出现在知识蒸馏中，例如，使用强化学习以数据驱动的方式自动移除冗余层（Ashok等 2018），以及在给定教师网络的情况下搜索最优学生网络（Liu等 2019i；Xie等 2020；Gu和Tresp 2020）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-fe98b815-bcf4-47ba-a7fe-5ac787369526" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-9="922,1707">Most previous works focus on designing either the structures of teacher and student models or the knowledge transfer scheme between them. To make a small student model well match a large teacher model for improving knowledge distillation performance, the adaptive teacher-student learning architecture is necessary. Recently, the idea of a neural architecture search in knowledge distillation, i.e., a joint search of student structure and knowledge transfer under the guidance of the teacher model, will be an interesting subject of future study.<div style="background-color: #d6d6d6;margin: 12px 0;">大多数先前的工作集中在设计教师和学生模型的结构，或者它们之间的知识迁移方案。为了使小型学生模型能够很好地匹配大型教师模型以提高知识蒸馏性能，需要自适应的教师-学生学习架构。最近，在知识蒸馏中进行神经网络架构搜索的想法，即在教学模型的指导下联合搜索学生结构和知识迁移，将是未来研究的有趣主题。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-10="144,154"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_10.jpg?x=144&amp;y=154&amp;w=702&amp;h=396"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-10="143,570">Fig. 10 The different categories of the main adversarial distillation methods. a Generator in GAN produces training data to improve KD performance; the teacher may be used as discriminator. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="552" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D41B TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">b</mi></mrow></math></mjx-assistive-mml></mjx-container> Discriminator in GAN ensures that the student (also as generator) mimics the teacher. c Teacher and student form a generator; online knowledge distillation is enhanced by the discriminator<div style="background-color: #d6d6d6;margin: 12px 0;">图10 主要对抗性蒸馏方法的分类。a GAN中的生成器产生训练数据以提高KD性能；教师可能被用作判别器。 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="553" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D41B TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">b</mi></mrow></math></mjx-assistive-mml></mjx-container> GAN中的判别器确保学生（也作为生成器）模仿教师。c 教师和学生形成一个生成器；在线知识蒸馏通过判别器增强</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-10="139,813">5 Distillation Algorithms<div style="background-color: #d6d6d6;margin: 12px 0;">5 蒸馏算法</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-10="143,902">A simple yet very effective idea for knowledge transfer is to directly match the response-based knowledge, feature-based knowledge (Romero et al. 2015; Hinton et al. 2015) or the representation distributions in feature space (Passalis and Tefas 2018) between the teacher model and the student model. Many different algorithms have been proposed to improve the process of transferring knowledge in more complex settings. In this section, we review recently proposed typical types of distillation methods for knowledge transfer within the field of knowledge distillation.<div style="background-color: #d6d6d6;margin: 12px 0;">一个简单但非常有效的知识迁移想法是直接匹配基于响应的知识、基于特征的知识（Romero等人2015；Hinton等人2015）或者在特征空间中的表示分布（Passalis和Tefas 2018）在教师模型和学生模型之间。已经提出了许多不同的算法来改进在更复杂场景下转移知识的过程。在本节中，我们回顾了最近提出的知识蒸馏领域内知识迁移的典型蒸馏方法类型。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-10="141,1327">5.1 Adversarial Distillation<div style="background-color: #d6d6d6;margin: 12px 0;">5.1 对抗性蒸馏</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-10="142,1411">In knowledge distillation, it is difficult for the teacher model to perfectly learn from the true data distribution. Simultaneously, the student model has only a small capacity and so cannot mimic the teacher model accurately (Mirzadeh et al. 2020). Are there other ways of training the student model in order to mimic the teacher model? Recently, adversarial learning has received a great deal of attention due to its great success in generative networks, i.e., generative adversarial networks or GANs (Goodfellow et al. 2014). Specifically, the discriminator in a GAN estimates the probability that a sample comes from the training data distribution while the generator tries to fool the discriminator using generated data samples. Inspired by this, many adversarial knowledge distillation methods have been proposed to enable the teacher and student networks to have a better understanding of the true data distribution (Wang et al. 2018e; Xu et al. 2018a; Micaelli and Storkey 2019; Xu et al. 2018b; Liu et al. 2018; Wang et al. 2018f; Chen et al. 2019a; Shen et al. 2019d; Shu et al. 2019; Liu et al. 2020a; Belagiannis et al. 2018).<div style="background-color: #d6d6d6;margin: 12px 0;">在知识蒸馏中，教师模型很难从真实数据分布中完美学习。同时，学生模型容量有限，因此无法准确模仿教师模型（Mirzadeh等人，2020年）。有没有其他训练学生模型以模仿教师模型的方法？最近，对抗学习因其在对生成网络，即生成对抗网络或GANs（Goodfellow等人，2014年）中的巨大成功而受到了极大的关注。具体来说，GAN中的判别器估计一个样本来自训练数据分布的概率，而生成器则试图使用生成的数据样本欺骗判别器。受到这一启发，许多对抗性知识蒸馏方法被提出，以使教师和学生网络更好地理解真实数据分布（Wang等人，2018e；Xu等人，2018a；Micaelli和Storkey，2019；Xu等人，2018b；Liu等人，2018；Wang等人，2018f；Chen等人，2019a；Shen等人，2019d；Shu等人，2019；Liu等人，2020a；Belagiannis等人，2018）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-10="924,165">As shown in Fig. 10, adversarial learning-based distillation methods, especially those methods using GANs, can be divided into three main categories as follows. In the first category, an adversarial generator is trained to generate synthetic data, which is either directly used as the training dataset (Chen et al. 2019a; Ye et al. 2020) or used to augment the training dataset (Liu et al. 2018), shown in Fig. 10a. Furthermore, Micaelli and Storkey (2019) utilized an adversarial generator to generate hard examples for knowledge transfer. Generally, the distillation loss used in this GAN-based KD category can be formulated as<div style="background-color: #d6d6d6;margin: 12px 0;">如图10所示，基于对抗学习的蒸馏方法，特别是使用GANs的方法，可以主要分为以下三大类。第一类中，训练一个对抗生成器来生成合成数据，这些数据要么直接用作训练数据集（Chen et al. 2019a; Ye et al. 2020），要么用于增强训练数据集（Liu et al. 2018），如图10a所示。此外，Micaelli和Storkey（2019）利用对抗生成器为知识迁移生成难样本。通常，这种基于GAN的KD类别中使用的蒸馏损失可以表示为</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><paragraphpositioning data-position-10="892,607"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" width="full" tabindex="0" ctxtmenu_counter="554" style="font-size: 122.8%; min-width: 18.783em; position: relative;"><mjx-math width="full" display="true" class="MJX-TEX" aria-hidden="true"><mjx-mtable width="full" side="right" style="min-width: 18.783em;"><mjx-table style="width: auto; min-width: 14.627em; margin: 0px 2.078em;"><mjx-itable width="full"><mjx-mlabeledtr><mjx-mtd><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mlabeledtr></mjx-itable></mjx-table><mjx-labels style="width: 18.783em;"><mjx-itable align="right" style="right: 0px;"><mjx-mtr style="height: 1em;"><mjx-mtd id="mjx-eqn:7"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c28"></mjx-c><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c29"></mjx-c></mjx-mtext><mjx-tstrut style="height: 1em; vertical-align: -0.25em;"></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-labels></mjx-mtable></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd><mtext>(7)</mtext></mtd><mtd><msub><mrow data-mjx-texclass="ORD"><mi>L</mi></mrow><mrow data-mjx-texclass="ORD"><mi>K</mi><mi>D</mi></mrow></msub><mo>=</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>G</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>G</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>z</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>G</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>z</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo></mtd></mlabeledtr></mtable></math></mjx-assistive-mml></mjx-container></paragraphpositioning></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-10="893,693">where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="555" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>n</mi><mi>d</mi></mrow><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>r</mi><mi>e</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi></mrow></math></mjx-assistive-mml></mjx-container> student models,respectively. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="556" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>z</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> indicates the training samples generated by the generator <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="557" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></mjx-assistive-mml></mjx-container> given the random input vector <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="558" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi></math></mjx-assistive-mml></mjx-container> ,and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="559" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>G</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> is a distillation loss to force the match between the predicted and the ground-truth probability distributions, e.g., the cross entropy loss or the Kullback-Leibler (KL) divergence loss.<div style="background-color: #d6d6d6;margin: 12px 0;">其中 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="560" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>n</mi><mi>d</mi></mrow><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>a</mi><mi>r</mi><mi>e</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi></mrow></math></mjx-assistive-mml></mjx-container> 分别代表学生模型。 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="561" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>z</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> 表示生成器 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="562" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></mjx-assistive-mml></mjx-container> 在给定随机输入向量 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="563" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D467 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>z</mi></math></mjx-assistive-mml></mjx-container> 时生成的训练样本，<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="564" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>G</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> 是一种蒸馏损失，用于强制预测概率分布与真实概率分布之间的匹配，例如交叉熵损失或Kullback-Leibler（KL）散度损失。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-10="927,948">To make student well match teacher, a discriminator in the second category is introduced to distinguish the samples from the student and the teacher models by using either the logits (Xu et al. 2018a, b) or the features (Wang et al. 2018f), shown in Fig. 10b. Specifically, Belagiannis et al. (2018) used unlabeled data samples to form the knowledge transfer. Multiple discriminators were used by Shen et al. (2019d). Furthermore, an effective intermediate supervision, i.e., the squeezed knowledge, was used by Shu et al. (2019) to mitigate the capacity gap between the teacher and the student. A representative model proposed by Wang et al. (2018f) falls into this category, which can be formulated as<div style="background-color: #d6d6d6;margin: 12px 0;">为了使学生模型与教师模型很好地匹配，第二类别中引入了一个判别器，用来通过使用 logits（Xu et al. 2018a, b）或特征（Wang et al. 2018f）来区分学生模型和教师模型的样本，如图10b所示。具体来说，Belagiannis et al.（2018）使用未标记的数据样本进行知识迁移。Shen et al.（2019d）使用了多个判别器。此外，Shu et al.（2019）使用了一种有效的中间监督，即压缩知识，来减轻教师和学生之间的容量差距。Wang et al.（2018f）提出的一个代表性模型属于此类，可以表示为</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><paragraphpositioning data-position-10="894,1364"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" width="full" tabindex="0" ctxtmenu_counter="565" style="font-size: 122.8%; min-width: 29.154em; position: relative;"><mjx-math width="full" display="true" class="MJX-TEX" aria-hidden="true"><mjx-mtable width="full" side="right" style="min-width: 29.154em;"><mjx-table style="width: auto; min-width: 24.998em; margin: 0px 2.078em;"><mjx-itable width="full"><mjx-mlabeledtr><mjx-mtd><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.153em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D6FC TEX-I"></mjx-c></mjx-mi><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mlabeledtr></mjx-itable></mjx-table><mjx-labels style="width: 29.154em;"><mjx-itable align="right" style="right: 0px;"><mjx-mtr style="height: 1em;"><mjx-mtd id="mjx-eqn:8"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c28"></mjx-c><mjx-c class="mjx-c38"></mjx-c><mjx-c class="mjx-c29"></mjx-c></mjx-mtext><mjx-tstrut style="height: 1em; vertical-align: -0.25em;"></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-labels></mjx-mtable></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd><mtext>(8)</mtext></mtd><mtd><msub><mrow data-mjx-texclass="ORD"><mi>L</mi></mrow><mrow data-mjx-texclass="ORD"><mi>G</mi><mi>A</mi><mi>N</mi><mi>K</mi><mi>D</mi></mrow></msub><mo>=</mo><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>C</mi><mi>E</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>G</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><mi>y</mi></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>+</mo><mi>α</mi><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>K</mi><mi>L</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mi>G</mi><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></mtd></mlabeledtr></mtable></math></mjx-assistive-mml></mjx-container></paragraphpositioning></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="566" style="font-size: 122.8%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.153em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em; margin-left: -0.106em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mo>+</mo><mi>β</mi><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>G</mi><mi>A</mi><mi>N</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><msub><mrow data-mjx-texclass="ORD"><mi>F</mi></mrow><mrow data-mjx-texclass="ORD"><mi>t</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>x</mi><mo data-mjx-texclass="CLOSE">)</mo></mrow></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo></math></mjx-assistive-mml></mjx-container></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-10="893,1484">where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="567" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></mjx-assistive-mml></mjx-container> is a student network and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="568" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.153em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>G</mi><mi>A</mi><mi>N</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi><mi>n</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>s</mi><mi>a</mi><mi>t</mi><mi>y</mi><mi>p</mi></mrow><mo>−</mo></math></mjx-assistive-mml></mjx-container> ical loss function used in generative adversarial network to make the outputs between student and teacher as similar as possible.<div style="background-color: #d6d6d6;margin: 12px 0;">其中 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="569" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></mjx-assistive-mml></mjx-container> 是学生网络，<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="570" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4C TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-script style="vertical-align: -0.153em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>G</mi><mi>A</mi><mi>N</mi></mrow></msub><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mtext>.</mtext><mo data-mjx-texclass="CLOSE">)</mo></mrow><mrow data-mjx-texclass="ORD"><mi>i</mi><mi>n</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>s</mi><mi>a</mi><mi>t</mi><mi>y</mi><mi>p</mi></mrow><mo>−</mo></math></mjx-assistive-mml></mjx-container> 是生成对抗网络中使用的ical损失函数，旨在使学生和教师的输出尽可能相似。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-10="923,1634">In the third category, adversarial knowledge distillation is carried out in an online manner, i.e., the teacher and the student are jointly optimized in each iteration (Wang et al. 2018e; Chung et al. 2020), shown in Fig. 10c. Besides, using knowledge distillation to compress GANs, a learned small GAN student network mimics a larger GAN teacher network via knowledge transfer (Aguinaldo et al. 2019; Li et al. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="571" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c63"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>2020</mn></mrow><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">c</mi></mrow></math></mjx-assistive-mml></mjx-container> ).<div style="background-color: #d6d6d6;margin: 12px 0;">在第三类中，对抗性知识蒸馏以在线方式进行，即教师和学生在每个迭代中共同优化（Wang et al. 2018e; Chung et al. 2020），如图10c所示。此外，使用知识蒸馏压缩GANs，学习到的较小的GAN学生网络通过知识传递模仿较大的GAN教师网络（Aguinaldo et al. 2019; Li et al. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="572" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c63"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>2020</mn></mrow><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">c</mi></mrow></math></mjx-assistive-mml></mjx-container>）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-1da42e83-0d07-41a4-97ac-c1d791242728" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-10="925,1927">In summary, three main points can be concluded from the adversarial distillation methods above as follows: GAN is an effective tool to enhance the power of student learning via the teacher knowledge transfer; joint GAN and KD can generate the valuable data for improving the KD performance and overcoming the limitations of unusable and unaccessible data; KD can be used to compress GANs.<div style="background-color: #d6d6d6;margin: 12px 0;">总结来说，从上述对抗性蒸馏方法中可以得出三个主要结论：GAN 是一种有效的工具，通过教师知识传递增强学生的学习能力；联合GAN和KD可以生成改进KD性能和克服不可用和不可访问数据限制的有价值数据；KD可以用于压缩GANs。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-885fa531-fb5f-4fe0-be28-ec1acf662e59" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-11="143,155"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_11.jpg?x=143&amp;y=155&amp;w=702&amp;h=355"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-885fa531-fb5f-4fe0-be28-ec1acf662e59" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-11="142,527">Fig. 11 The generic framework for multi-teacher distillation<div style="background-color: #d6d6d6;margin: 12px 0;">图11 多教师蒸馏的通用框架</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-885fa531-fb5f-4fe0-be28-ec1acf662e59" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-11="141,739">5.2 Multi-teacher Distillation<div style="background-color: #d6d6d6;margin: 12px 0;">5.2 多教师蒸馏</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-885fa531-fb5f-4fe0-be28-ec1acf662e59" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-11="142,825">Different teacher architectures can provide their own useful knowledge for a student network. The multiple teacher networks can be individually and integrally used for distillation during the period of training a student network. In a typical teacher-student framework, the teacher usually has a large model or an ensemble of large models. To transfer knowledge from multiple teachers, the simplest way is to use the averaged response from all teachers as the supervision signal (Hinton et al. 2015). Several multi-teacher knowledge distillation methods have recently been proposed (Sau and Balasubramanian 2016; You et al. 2017; Chen et al. 2019b; Furlanello et al. 2018; Yang et al. 2019a; Zhang et al. 2018b; Lee et al. 2019c; Park and Kwak 2020; Papernot et al. 2017; Fukuda et al. 2017; Ruder et al. 2017; Wu et al. 2019a; Yang et al. 2020c; Vongkulbhisal et al. 2019; Zhao et al. 2020a; Yuan et al. 2021). A generic framework for multi-teacher distillation is shown in Fig. 11.<div style="background-color: #d6d6d6;margin: 12px 0;">不同的教师架构可以为学生网络提供各自有用的知识。在训练学生网络的期间，多个教师网络可以分别和整体地用于蒸馏。在典型的教师-学生框架中，教师通常拥有大型模型或大型模型的集合。为了从多个教师那里转移知识，最简单的方法是使用所有教师的平均响应作为监督信号（Hinton et al. 2015）。最近已经提出了几种多教师知识蒸馏方法（Sau and Balasubramanian 2016; You et al. 2017; Chen et al. 2019b; Furlanello et al. 2018; Yang et al. 2019a; Zhang et al. 2018b; Lee et al. 2019c; Park and Kwak 2020; Papernot et al. 2017; Fukuda et al. 2017; Ruder et al. 2017; Wu et al. 2019a; Yang et al. 2020c; Vongkulbhisal et al. 2019; Zhao et al. 2020a; Yuan et al. 2021）。多教师蒸馏的通用框架如图11所示。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-885fa531-fb5f-4fe0-be28-ec1acf662e59" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-11="174,1451">Multiple teacher networks have turned out to be effective for training student model usually using logits and feature representation as the knowledge. In addition to the averaged logits from all teachers, You et al. (2017) further incorporated features from the intermediate layers in order to encourage the dissimilarity among different training samples. To utilize both logits and intermediate features, Chen et al. (2019b) used two teacher networks, in which one teacher transfers response-based knowledge to the student and the other teacher transfers feature-based knowledge to the student. Fukuda et al. (2017) randomly selected one teacher from the pool of teacher networks at each iteration. To transfer feature-based knowledge from multiple teachers, additional teacher branches are added to the student networks to mimic the intermediate features of teachers (Park and Kwak 2020; Asif et al. 2020). Born again networks address multiple teachers in a step-by-step manner,i.e.,the student at the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="573" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>t</mi></math></mjx-assistive-mml></mjx-container> step is used as the teacher of the student at the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="574" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>t</mi><mo>+</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> step (Furlanello et al. 2018), and similar ideas can be found in Yang et al. (2019a). To efficiently perform knowledge transfer and explore the power of multiple teachers, several alternative methods have been proposed to simulate multiple teachers by adding different types of noise to a given teacher (Sau and Balasubramanian 2016) or by using stochastic blocks and skip connections (Lee et al. 2019c). Using multiple teacher models with feature ensembles, knowledge amalgamation is designed in (Shen et al. 2019a; Luo et al. 2019; Shen et al. 2019b; Luo et al. 2020). Through knowledge amalgamation, many public available trained deep models as teachers can be reused. More interestingly, due to the special characteristics of multi-teacher distillation, its extensions are used for domain adaptation via knowledge adaptation (Ruder et al. 2017), and to protect the privacy and security of data (Vongkulbhisal et al. 2019; Papernot et al. 2017).<div style="background-color: #d6d6d6;margin: 12px 0;">多个教师网络已被证明在训练学生模型时有效，通常使用日志概率和特征表示作为知识。除了所有教师的平均日志概率之外，You等人（2017年）进一步合并了中间层的特征，以促进不同训练样本之间的差异性。为了同时利用日志概率和中间特征，Chen等人（2019b）使用了两个教师网络，其中一个教师将基于响应的知识传递给学生，另一个教师将基于特征的知识传递给学生。Fukuda等人（2017年）在每次迭代中从教师网络池中随机选择一个教师。为了从多个教师那里传递基于特征的知识，学生网络中添加了额外的教师分支来模拟教师的中间特征（Park和Kwak 2020；Asif等人 2020）。再生网络以逐步方式处理多个教师，即<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="575" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>t</mi></math></mjx-assistive-mml></mjx-container>步骤中的学生被用作<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="576" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>t</mi><mo>+</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container>步骤中学生的教师（Furlanello等人 2018年），类似的想法在Yang等人（2019a）中也可以找到。为了有效地执行知识转移并探索多个教师的力量，已经提出了几种替代方法，通过向给定的教师添加不同类型的噪声来模拟多个教师（Sau和Balasubramanian 2016），或者通过使用随机块和跳过连接（Lee等人 2019c）。使用带有特征组合的多个教师模型，知识融合被设计在（Shen等人 2019a；Luo等人 2019；Shen等人 2019b；Luo等人 2020）。通过知识融合，许多公开可用的已训练深度模型作为教师可以被重用。更有趣的是，由于多教师蒸馏的特殊特性，其扩展被用于通过知识适应进行域自适应（Ruder等人 2017），以及保护数据的隐私和安全（Vongkulbhisal等人 2019；Papernot等人 2017）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-885fa531-fb5f-4fe0-be28-ec1acf662e59" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-11="891,168">Table 3 A summary of multi-teacher distillation using different types of knowledge and distillation schemes. The response-based knowledge, feature-based knowledge and relation-based knowledge are abbreviated as 'ResK', 'FeaK' and 'RelK', respectively<div style="background-color: #d6d6d6;margin: 12px 0;">表3 多教师蒸馏使用不同类型知识和蒸馏方案的总结。基于响应的知识、基于特征的知识和基于关系的知识分别简称为 'ResK'、'FeaK' 和 'RelK'</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-885fa531-fb5f-4fe0-be28-ec1acf662e59" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-11="884,293"><div class="table-container"><table class="fixed-table"><thead><tr><th>Methods</th><th>ResK</th><th>FeaK</th><th>RelK</th></tr></thead><tbody><tr><td colspan="4">Offline Distillation</td></tr><tr><td>You et al. (2017)</td><td>v</td><td>x</td><td>1</td></tr><tr><td>Fukuda et al. (2017)</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="577" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c2713 TEX-A"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>✓</mi></math></mjx-assistive-mml></mjx-container></td><td>x</td><td>x</td></tr><tr><td>Shen et al. (2019b)</td><td>、</td><td>v</td><td>x</td></tr><tr><td>Wu et al. (2019a)</td><td>x</td><td>x</td><td>3</td></tr><tr><td>Park and Kwak (2020)</td><td>x</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="578" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c2713 TEX-A"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>✓</mi></math></mjx-assistive-mml></mjx-container></td><td>x</td></tr><tr><td>Yang et al. (2020c)</td><td>、</td><td>x</td><td>x</td></tr><tr><td>Luo et al. (2020)</td><td>、</td><td>V</td><td>x</td></tr><tr><td>Kwon et al. (2020)</td><td>、</td><td>x</td><td>x</td></tr><tr><td>Liu et al. (2020c)</td><td>、</td><td>3</td><td>x</td></tr><tr><td>Zhao et al. (2020a)</td><td>V</td><td>v</td><td>x</td></tr><tr><td>Yuan et al. (2021)</td><td>、</td><td>x</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="579" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math></mjx-assistive-mml></mjx-container></td></tr><tr><td colspan="4">Online Distillation</td></tr><tr><td>Papernot et al. (2017)</td><td>1</td><td>x</td><td>x</td></tr><tr><td>Furlanello et al. (2018)</td><td>V</td><td>x</td><td>x</td></tr><tr><td>Zhang et al. (2018b)</td><td>、</td><td>x</td><td>x</td></tr><tr><td>Yang et al. (2019a)</td><td>、</td><td>x</td><td>x</td></tr><tr><td>Lee et al. (2019c)</td><td>v</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="580" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c2713 TEX-A"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>✓</mi></math></mjx-assistive-mml></mjx-container></td><td>x</td></tr></tbody></table></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-885fa531-fb5f-4fe0-be28-ec1acf662e59" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-11="923,1817">A summary of typical multi-teacher distillation methods using different types of knowledge and distillation schemes is shown in Table 3. Generally, multi-teacher knowledge distillation can provide rich knowledge and tailor a versatile student model because of the diverse knowledge from different teachers. However, how to effectively integrate different types of knowledge from multiple teachers needs to be further studied.<div style="background-color: #d6d6d6;margin: 12px 0;">表3展示了使用不同类型知识和蒸馏方案的典型多教师蒸馏方法总结。通常，多教师知识蒸馏能够提供丰富的知识，并因来自不同教师的知识多样性而定制出多功能的student模型。然而，如何有效地整合来自多个教师的不同类型知识需要进一步研究。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-83382fb0-5aef-4cab-9531-758252ee1ef3" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-12="143,151"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_12.jpg?x=143&amp;y=151&amp;w=708&amp;h=334"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-83382fb0-5aef-4cab-9531-758252ee1ef3" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-12="142,502">Fig. 12 The generic framework for cross-modal distillation. For simplicity, only two modalities are shown<div style="background-color: #d6d6d6;margin: 12px 0;">图12 跨模态蒸馏的通用框架。为简化起见，只展示了两种模态。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-83382fb0-5aef-4cab-9531-758252ee1ef3" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-12="141,610">5.3 Cross-Modal Distillation<div style="background-color: #d6d6d6;margin: 12px 0;">5.3 跨模态蒸馏</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-83382fb0-5aef-4cab-9531-758252ee1ef3" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-12="142,694">The data or labels for some modalities might not be available during training or testing (Gupta et al. 2016; Garcia et al. 2018; Zhao et al. 2018; Roheda et al. 2018; Zhao et al. 2020b). For this reason it is important to transfer knowledge between different modalities. Several typical scenarios using cross-modal knowledge transfer are reviewed as follows.<div style="background-color: #d6d6d6;margin: 12px 0;">在训练或测试期间，某些模态的数据或标签可能不可用（Gupta等人，2016；Garcia等人，2018；Zhao等人，2018；Roheda等人，2018；Zhao等人，2020b）。因此，在不同模态之间转移知识是很重要的。以下回顾了使用跨模态知识转移的几个典型场景。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-83382fb0-5aef-4cab-9531-758252ee1ef3" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-12="175,917">Given a teacher model pretrained on one modality (e.g., RGB images) with a large number of well-annotated data samples, Gupta et al. (2016) transferred the knowledge from the teacher model to the student model with a new unlabeled input modality, such as a depth image and optical flow. Specifically, the proposed method relies on unlabeled paired samples involving both modalities, i.e., both RGB and depth images. The features obtained from RGB images by the teacher are then used for the supervised training of the student (Gupta et al. 2016). The idea behind the paired samples is to transfer the annotation or label information via pair-wise sample registration and has been widely used for cross-modal applications (Albanie et al. 2018; Zhao et al. 2018; Thoker and Gall 2019). To perform human pose estimation through walls or with occluded images, Zhao et al. (2018) used synchronized radio signals and camera images. Knowledge is transferred across modalities for radio-based human pose estimation. Thoker and Gall (2019) obtained paired samples from two modalities: RGB videos and skeleton sequence. The pairs are used to transfer the knowledge learned on RGB videos to a skeleton-based human action recognition model. To improve the action recognition performance using only RGB images, Garcia et al. (2018) performed cross-modality distillation on an additional modality, i.e., depth image, to generate a hallucination stream for RGB image modality. Tian et al. (2020) introduced a contrastive loss to transfer pair-wise relationship across different modalities. To improve target detection, Roheda et al. (2018) proposed cross-modality distillation among the missing and available modelities using GANs. The generic framework of cross-modal distillation is shown in Fig. 12.<div style="background-color: #d6d6d6;margin: 12px 0;">给定一个在一种模态（例如，RGB图像）上预训练的教师模型，该模型拥有大量标注良好的数据样本，Gupta等人（2016年）将教师模型的知识迁移到了一个新的未标注输入模态的学生模型上，比如深度图像和光流。具体来说，提出的方法依赖于涉及两种模态的未标注成对样本，即RGB和深度图像。教师从RGB图像中获取的特征随后用于学生模型的监督训练（Gupta等人 2016年）。成对样本背后的思想是通过样本对的注册来迁移标注或标签信息，并且这种方法已经被广泛用于跨模态应用（Albanie等人 2018年；Zhao等人 2018年；Thoker和Gall 2019年）。为了通过墙壁或遮挡图像进行人体姿态估计，Zhao等人（2018年）使用了同步的无线电信号和摄像机图像。知识在模态之间迁移，用于基于无线电的人体姿态估计。Thoker和Gall（2019年）从两种模态中获取成对样本：RGB视频和骨架序列。这些对被用来将RGB视频上学到的知识迁移到一个基于骨架的人体动作识别模型。为了仅使用RGB图像提高动作识别性能，Garcia等人（2018年）在额外的模态上进行了跨模态蒸馏，即深度图像，以生成用于RGB图像模态的幻觉流。Tian等人（2020年）引入了对比损失来迁移不同模态间的成对关系。为了提高目标检测，Roheda等人（2018年）提出在缺失和可用模型之间使用GANs进行跨模态蒸馏。跨模态蒸馏的通用框架如图12所示。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-83382fb0-5aef-4cab-9531-758252ee1ef3" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-12="922,165">Moreover, Do et al. (2019) proposed a knowledge distillation-based visual question answering method, in which knowledge from trilinear interaction teacher model with image-question-answer as inputs is distilled into the learning of a bilinear interaction student model with image-question as inputs. The probabilistic knowledge distillation proposed by Passalis and Tefas (2018) is also used for knowledge transfer from the textual modality into the visual modality. Hoffman et al. (2016) proposed a modality hallucination architecture based on cross-modality distillation to improve detection performance. Besides, these cross-model distillation methods also transfer the knowledge among multiple domains (Kundu et al. 2019; Chen et al. 2019c; Su and Maji 2017).<div style="background-color: #d6d6d6;margin: 12px 0;">此外，Do等人（2019年）提出了一种基于知识蒸馏的视觉问答方法，其中将图像-问题-答案作为输入的三线性交互教师模型的 knowledge 通过蒸馏转移到以图像-问题作为输入的双线性交互学生模型的学习中。Passalis和Tefas（2018年）提出的概率知识蒸馏也用于将文本模态的知识转移到视觉模态。Hoffman等人（2016年）提出了一种基于跨模态蒸馏的模态幻觉架构，以提高检测性能。此外，这些跨模态蒸馏方法还在多个域之间传递知识（Kundu等人2019年；Chen等人2019c；Su和Maji 2017年）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-83382fb0-5aef-4cab-9531-758252ee1ef3" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-12="923,643">A summary of cross-modal distillation with different modalities, types of knowledge and distillation schemes is shown in Table 4. Specifically, it can be seen that knowledge distillation performs well in visual recognition tasks in the cross-modal scenarios. However, cross-modal knowledge transfer is a challenging study when there is a modality gap, e.g., lacking of the paired samples between different modalities.<div style="background-color: #d6d6d6;margin: 12px 0;">表4展示了不同模态、知识类型和蒸馏方案的跨模态蒸馏概览。具体来说，可以看出在跨模态场景中，知识蒸馏在视觉识别任务上表现良好。然而，当存在模态差距时，例如不同模态之间缺乏配对样本，跨模态知识转移是一项具有挑战性的研究。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-83382fb0-5aef-4cab-9531-758252ee1ef3" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-12="892,960">5.4 Graph-Based Distillation<div style="background-color: #d6d6d6;margin: 12px 0;">5.4 基于图的蒸馏</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-83382fb0-5aef-4cab-9531-758252ee1ef3" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-12="892,1044">Most of knowledge distillation algorithms focus on transferring individual instance knowledge from the teacher to the student, while some recent methods have been proposed to explore the intra-data relationships using graphs (Chen et al. 2021; Zhang and Peng 2018; Lee and Song 2019; Park et al. 2019; Yao et al. 2020; Ma and Mei 2019; Hou et al. 2020). The main ideas of these graph-based distillation methods are 1) to use the graph as the carrier of teacher knowledge; or 2) to use the graph to control the message passing of the teacher knowledge. A generic framework for graph-based distillation is shown in Fig. 13. As described in Sect. 2.3, the graph-based knowledge falls in line of relation-based knowledge. In this section, we introduce typical definitions of the graph-based knowledge and the graph-based message passing distillation algorithms.<div style="background-color: #d6d6d6;margin: 12px 0;">大多数知识蒸馏算法专注于将单个实例知识从教师模型转移到学生模型，而近期一些方法被提出以探索数据间的内在关系，使用图来表示（Chen et al. 2021; Zhang and Peng 2018; Lee and Song 2019; Park et al. 2019; Yao et al. 2020; Ma and Mei 2019; Hou et al. 2020）。这些基于图的蒸馏方法的主要思想是：1）使用图作为教师知识的载体；或者2）使用图来控制教师知识的消息传递。图13展示了一个基于图的蒸馏的通用框架。如第2.3节所述，基于图的知识与基于关系知识相吻合。在本节中，我们介绍基于图的知识和基于图的消息传递蒸馏算法的典型定义。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-83382fb0-5aef-4cab-9531-758252ee1ef3" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-12="923,1597">Specifically, in Zhang and Peng (2018), each vertex represents a self-supervised teacher. Two graphs are then constructed using logits and intermediate features, i.e., the logits graph and representation graph, to transfer knowledge from multiple self-supervised teachers to the student. In Chen et al. (2021), the graph is used to maintain the relationship between samples in the high-dimensional space. Knowledge transfer is then carried out using a proposed locality preserving loss function. Lee and Song (2019) analysed intra-data relations using a multi-head graph, in which the vertices are the features from different layers in CNNs. Park et al. (2019) directly transferred the mutual relations of data samples, i.e., to match edges between a teacher graph and a student graph. Tung and Mori (2019) used the similarity matrix to represent the mutual relations of the activations of the input pairs in teacher and student models. The similarity matrix of student matches that of teacher. Furthermore, Peng et al. (2019a) not only matched the response-based and feature-based knowledge, but also used the graph-based knowledge. In Liu et al. (2019g), the instance features and instance relationships are modeled as vertexes and edges of the graph, respectively.<div style="background-color: #d6d6d6;margin: 12px 0;">具体来说，在 Zhang 和 Peng (2018) 的研究中，每个顶点代表一个自监督教师。接着使用 logits 和中间特征构建了两个图，即 logits 图和表示图，以将从多个自监督教师那里获得的知识迁移给学生。在 Chen 等人 (2021) 的研究中，图被用来维持样本在高维空间中的关系。然后通过一个提出的局部保持损失函数进行知识迁移。Lee 和 Song (2019) 使用多头图分析了数据内的关系，其中顶点是来自卷积神经网络不同层的特征。Park 等人 (2019) 直接迁移了数据样本的相互关系，即匹配教师图和学生图之间的边。Tung 和 Mori (2019) 使用相似度矩阵来表示输入对在教师和学生模型中的激活的相互关系。学生的相似度矩阵与教师的匹配。此外，Peng 等人 (2019a) 不仅匹配了基于响应和基于特征的知识，还使用了基于图的知识。在 Liu 等人 (2019g) 的研究中，实例特征和实例关系分别被建模为图的顶点和边。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-0e670e1c-9e03-4db3-99c5-4b35aab99310" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-13="139,166">Table 4 A summary of cross-modal distillation with modalities, types of knowledge and distillation<div style="background-color: #d6d6d6;margin: 12px 0;">表4 跨模态蒸馏的总结，包括模态、知识类型和蒸馏方法</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-0e670e1c-9e03-4db3-99c5-4b35aab99310" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-13="135,206"><div class="table-container"><table class="fixed-table"><thead><tr><th>Methods</th><th>Modality for teacher</th><th>Modality for student</th><th>Knowledge</th><th>Distillation</th></tr></thead><tbody><tr><td>Hoffman et al. (2016)</td><td>RGB images</td><td>Depth images</td><td>FeaK</td><td>Offline</td></tr><tr><td>Gupta et al. (2016)</td><td>RGB images</td><td>Depth images</td><td>ResK</td><td>Offline</td></tr><tr><td>Passalis and Tefas (2018)</td><td>Textual modality</td><td>Visual modality</td><td>RelK</td><td>Offline</td></tr><tr><td>Garcia et al. (2018)</td><td>Depth and RGB videos</td><td>RGB videos</td><td>ResK, FeaK</td><td>Offline</td></tr><tr><td>Zhao et al. (2018)</td><td>RGB frames</td><td>Radio frequency heatmaps</td><td>ResK</td><td>Offline</td></tr><tr><td>Roheda et al. (2018)</td><td>Temporal data</td><td>Spatial data</td><td>FeaK</td><td>Online</td></tr><tr><td>Albanie et al. (2018)</td><td>Vision</td><td>Sound</td><td>ResK</td><td>Offline</td></tr><tr><td>Thoker and Gall (2019)</td><td>RGB videos</td><td>Skeleton data</td><td>ResK</td><td>Offline</td></tr><tr><td>Do et al. (2019)</td><td>Images, question, answer information</td><td>Image-questions</td><td>ResK</td><td>Offline</td></tr><tr><td>Tian et al. (2020)</td><td>RGB images</td><td>Depth images</td><td>ResK</td><td>Offline</td></tr><tr><td>Gao et al. (2020)</td><td>Multi-modal images</td><td>Single-mode images</td><td>ResK, FeaK</td><td>Offline</td></tr></tbody></table></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-0e670e1c-9e03-4db3-99c5-4b35aab99310" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-13="144,757"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_13.jpg?x=144&amp;y=757&amp;w=709&amp;h=363"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-0e670e1c-9e03-4db3-99c5-4b35aab99310" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-13="143,1138">Fig. 13 A generic framework for graph-based distillation<div style="background-color: #d6d6d6;margin: 12px 0;">图13 基于图的蒸馏的通用框架</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-0e670e1c-9e03-4db3-99c5-4b35aab99310" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-13="175,1525">Rather than using the graph-based knowledge, several methods control knowledge transfer using a graph. Specifically, Luo et al. (2018) considered the modality discrepancy to incorporate privileged information from the source domain. A directed graph, referred to as a distillation graph is introduced to explore the relationship between different modalities. Each vertex represent a modality and the edges indicate the connection strength between one modality and another. Minami et al. (2019) proposed a bidirectional graph-based diverse collaborative learning to explore diverse knowledge transfer patterns. Yao et al. (2020) introduced GNNs to deal with the knowledge transfer for graph-based knowledge. Besides, using knowledge distillation, the topological semantics of a graph convolutional teacher network as the topology-aware knowledge are transferred into the graph convolutional student network (Yang et al. 2020b)<div style="background-color: #d6d6d6;margin: 12px 0;">与使用基于图的知识相比，几种方法通过图来控制知识迁移。具体来说，Luo等人（2018年）考虑了模态差异，将源域的特权信息纳入其中。引入了一个有向图，称为蒸馏图，用于探索不同模态之间的关系。每个顶点代表一个模态，边表示一个模态与另一个模态之间的连接强度。Minami等人（2019年）提出了一种基于图的双向多样化协作学习，用于探索多样化的知识迁移模式。Yao等人（2020年）引入了GNN来处理基于图的知识迁移。此外，通过知识蒸馏，图卷积教师网络的拓扑语义作为拓扑感知知识，被迁移到图卷积学生网络中（Yang等人2020b）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-0e670e1c-9e03-4db3-99c5-4b35aab99310" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-13="926,765">Graph-based distillation can transfer the informative structure knowledge of data. However, how to properly construct graph to model the structure knowledge of data is a still challenging study.<div style="background-color: #d6d6d6;margin: 12px 0;">基于图的反蒸馏可以迁移数据的信息结构知识。然而，如何恰当地构建图来模拟数据结构知识仍然是一个具有挑战性的研究课题。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-0e670e1c-9e03-4db3-99c5-4b35aab99310" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-13="892,966">5.5 Attention-Based Distillation<div style="background-color: #d6d6d6;margin: 12px 0;">5.5 基于注意力的蒸馏</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-0e670e1c-9e03-4db3-99c5-4b35aab99310" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-13="891,1052">Since attention can well reflect the neuron activations of convolutional neural network, some attention mechanisms are used in knowledge distillation to improve the performance of the student network (Zagoruyko and Komodakis 2017; Huang and Wang 2017; Srinivas and Fleuret 2018; Crowley et al. 2018; Song et al. 2018). Among these attention-based KD methods (Crowley et al. 2018; Huang and Wang 2017; Srinivas and Fleuret 2018; Zagoruyko and Komodakis 2017), different attention transfer mechanisms are defined for distilling knowledge from the teacher network to the student network. The core of attention transfer is to define the attention maps for feature embedding in the layers of a neural network. That is to say, knowledge about feature embedding is transferred using attention map functions. Unlike the attention maps, a different attentive knowledge distillation method was proposed by Song et al. (2018). An attention mechanism is used to assign different confidence rules (Song et al. 2018).<div style="background-color: #d6d6d6;margin: 12px 0;">由于注意力可以很好地反映卷积神经网络的神经元激活，因此在知识蒸馏中使用了某些注意力机制来提高学生网络的性能（Zagoruyko和Komodakis 2017；Huang和Wang 2017；Srinivas和Fleuret 2018；Crowley等 2018；Song等 2018）。在这些基于注意力的KD方法（Crowley等 2018；Huang和Wang 2017；Srinivas和Fleuret 2018；Zagoruyko和Komodakis 2017）中，为将从教师网络到学生网络的知识蒸馏定义了不同的注意力转移机制。注意力转移的核心是为神经网络层的特征嵌入定义注意力图。也就是说，特征嵌入的知识是通过注意力图函数进行转移的。与注意力图不同，Song等（2018）提出了一种不同的注意力知识蒸馏方法。该方法使用注意力机制为分配不同的置信规则（Song等 2018）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-0e670e1c-9e03-4db3-99c5-4b35aab99310" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-13="891,1730">5.6 Data-Free Distillation<div style="background-color: #d6d6d6;margin: 12px 0;">5.6 无数据蒸馏</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-0e670e1c-9e03-4db3-99c5-4b35aab99310" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-13="892,1814">Some data-free KD methods have been proposed to overcome problems with unavailable data arising from privacy, legality, security and confidentiality concerns (Chen et al. 2019a; Lopes et al. 2017; Nayak et al. 2019; Micaelli and Storkey 2019; Haroush et al. 2020; Ye et al. 2020; Nayak et al. 2021; Chawla et al. 2021). Just as "data free" implies, there is no training data. Instead, the data is newly or synthetically generated.<div style="background-color: #d6d6d6;margin: 12px 0;">已经提出了一些无数据KD方法，以克服由于隐私、合法性、安全和保密问题导致的数据不可用的问题（Chen等 2019a；Lopes等 2017；Nayak等 2019；Micaelli和Storkey 2019；Haroush等 2020；Ye等 2020；Nayak等 2021；Chawla等 2021）。正如“无数据”所暗示的，没有训练数据。相反，数据是新生成或合成的。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-60a05b8f-301f-438d-b9df-7cbb0f01bda8" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-14="193,150"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_14.jpg?x=193&amp;y=150&amp;w=610&amp;h=456"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-60a05b8f-301f-438d-b9df-7cbb0f01bda8" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-14="143,626">Fig. 14 A generic framework for data-free distillation<div style="background-color: #d6d6d6;margin: 12px 0;">图14 无数据蒸馏的通用框架</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-60a05b8f-301f-438d-b9df-7cbb0f01bda8" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-14="176,729">Specifically, in (Chen et al. 2019a; Ye et al. 2020; Micaelli and Storkey 2019; Yoo et al. 2019; Hu et al. 2020), the transfer data is generated by a GAN. In the proposed data-free knowledge distillation method (Lopes et al. 2017), the transfer data to train the student network is reconstructed by using the layer activations or layer spectral activations of the teacher network. Yin et al. (2020) proposed DeepInversion, which uses knowledge distillation to generate synthesized images for data-free knowledge transfer. Nayak et al. (2019) proposed zero-shot knowledge distillation that does not use existing data. The transfer data is produced by modelling the softmax space using the parameters of the teacher network. In fact, the target data in (Micaelli and Storkey 2019; Nayak et al. 2019) is generated by using the information from the feature representations of teacher networks. Similar to zero-shot learning, a knowledge distillation method with few-shot learning is designed by distilling knowledge from a teacher model into a student neural network (Kimura et al. 2018; Shen et al. 2021). The teacher uses limited labelled data. Besides, there is a new type of distillation called data distillation, which is similar to data-free distillation (Radosavovic et al. 2018; Liu et al. 2019d; Zhang et al. 2020d). In data distillation, new training annotations of unlabeled data generated from the teacher model are employed to train a student model.<div style="background-color: #d6d6d6;margin: 12px 0;">具体来说，在 (Chen et al. 2019a; Ye et al. 2020; Micaelli and Storkey 2019; Yoo et al. 2019; Hu et al. 2020) 中，迁移数据是由 GAN 生成的。在提出的数据无关知识蒸馏方法中 (Lopes et al. 2017)，用于训练学生网络的迁移数据是通过使用教师网络的层激活或层频谱激活重建的。Yin et al. (2020) 提出了 DeepInversion，它使用知识蒸馏来生成用于数据无关知识迁移的合成图像。Nayak et al. (2019) 提出了零样本知识蒸馏，该方法不使用现有数据。迁移数据是通过使用教师网络的参数来模拟 softmax 空间产生的。实际上，(Micaelli and Storkey 2019; Nayak et al. 2019) 中的目标数据是通过使用教师网络的特征表示信息生成的。类似于零样本学习，一种带有少量样本学习知识蒸馏方法被设计出来，该方法将教师模型的知识蒸馏到学生神经网络中 (Kimura et al. 2018; Shen et al. 2021)。教师使用有限的标记数据。此外，还有一种新的蒸馏类型，称为数据蒸馏，它与数据无关蒸馏相似 (Radosavovic et al. 2018; Liu et al. 2019d; Zhang et al. 2020d)。在数据蒸馏中，使用从教师模型生成的新训练注释来训练学生模型的未标记数据。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-60a05b8f-301f-438d-b9df-7cbb0f01bda8" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-14="175,1648">In summary, the synthesis data in data-free distillation is usually generated from the feature representations from the pre-trained teacher model, as shown in Fig. 14. Although the data-free distillation has shown a great potential under the condition of unavailable data, it remains a very challenging task, i.e., how to generate high quality diverse training data to improve the model generalizability.<div style="background-color: #d6d6d6;margin: 12px 0;">总结来说，在无数据蒸馏中，合成数据通常是由如图14所示的预训练教师模型的特征表示生成的。尽管无数据蒸馏在数据不可用的情况下显示出巨大的潜力，但它仍然是一个非常具有挑战性的任务，即如何生成高质量的多样化训练数据以提高模型的泛化性。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-60a05b8f-301f-438d-b9df-7cbb0f01bda8" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-14="141,1951">5.7 Quantized Distillation<div style="background-color: #d6d6d6;margin: 12px 0;">5.7 量化蒸馏</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-60a05b8f-301f-438d-b9df-7cbb0f01bda8" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-14="141,2034">Network quantization reduces the computation complexity of neural networks by converting high-precision networks (e.g., 32-bit floating point) into low-precision networks (e.g., 2-bit and 8-bit). Meanwhile, knowledge distillation aims to train a small model to yield a performance comparable to that of a complex model. Some KD methods have been proposed using the quantization process in the teacher-student framework (Polino et al. 2018; Mishra and Marr 2018; Wei et al. 2018; Shin et al. 2019; Kim et al. 2019a). A framework for quantized distillation methods is shown in Fig. 15.<div style="background-color: #d6d6d6;margin: 12px 0;">网络量化通过将高精度网络（例如，32位浮点数）转换为低精度网络（例如，2位和8位）来降低神经网络的计算复杂度。同时，知识蒸馏的目标是训练一个小型模型，使其性能与复杂模型相当。已经提出了一些在教师-学生框架中使用量化过程的知识蒸馏方法（Polino等人2018年；Mishra和Marr 2018年；Wei等人2018年；Shin等人2019年；Kim等人2019a）。量化蒸馏方法的框架如图15所示。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-60a05b8f-301f-438d-b9df-7cbb0f01bda8" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-14="945,150"><div class="text-center"><img class="max-w-[100%]" draggable="false" src="https://cdn.noedgeai.com/01924370-8ae2-7305-9578-8e1f0c96a483_14.jpg?x=945&amp;y=150&amp;w=601&amp;h=342"></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-60a05b8f-301f-438d-b9df-7cbb0f01bda8" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-14="892,510">Fig. 15 A generic framework for quantized distillation<div style="background-color: #d6d6d6;margin: 12px 0;">图15 量化蒸馏的通用框架</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-60a05b8f-301f-438d-b9df-7cbb0f01bda8" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-14="925,899">Specifically, Polino et al. (2018) proposed a quantized distillation method to transfer the knowledge to a weight-quantized student network. In Mishra and Marr (2018), the proposed quantized KD is called the "apprentice". A high precision teacher network transfers knowledge to a small low-precision student network. To ensure that a small student network accurately mimics a large teacher network, the full-precision teacher network is first quantized on the feature maps, and then the knowledge is transferred from the quantized teacher to a quantized student network (Wei et al. 2018). Kim et al. (2019a) proposed quantization-aware knowledge distillation, which is based on self-study of a quantized student network and on the co-studying of teacher and student networks with knowledge transfer. Furthermore, Shin et al. (2019) carried out empirical analysis of deep neural networks using both distillation and quantization, taking into account the hyper-parameters for knowledge distillation, such as the size of teacher networks and the distillation temperature. Recently, unlike the quantized distillation methods above, a self-distillation training schemes is designed to improve the performance of quantized deep models, where teacher shares model parameters of student (Boo et al. 2021).<div style="background-color: #d6d6d6;margin: 12px 0;">具体来说，Polino等人（2018年）提出了一种量化蒸馏方法，用以将知识迁移到权重量化的学生网络。在Mishra和Marr（2018年）的研究中，所提出的量化KD被称为“学徒”。一个高精度教师网络将知识迁移到一个小型低精度学生网络。为了确保小型学生网络能够准确地模仿大型教师网络，首先在特征图上对全精度教师网络进行量化，然后将知识从量化后的教师网络迁移到量化后的学生网络（Wei等人，2018年）。Kim等人（2019a）提出了量化感知的知识蒸馏，该方法基于量化学生网络的自我学习和教师与学生网络的共同学习，并进行知识迁移。此外，Shin等人（2019年）对深度神经网络进行了实证分析，同时考虑了知识蒸馏的多个超参数，例如教师网络的大小和蒸馏温度。最近，与上述量化蒸馏方法不同，设计了一种自蒸馏训练方案，旨在提高量化深度模型的性能，在该方案中教师共享学生模型的参数（Boo等人，2021年）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-60a05b8f-301f-438d-b9df-7cbb0f01bda8" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-14="892,1731">5.8 Lifelong Distillation<div style="background-color: #d6d6d6;margin: 12px 0;">5.8 终身蒸馏</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-60a05b8f-301f-438d-b9df-7cbb0f01bda8" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-14="893,1814">Lifelong learning, including continual learning, continuous learning and meta-learning, aims to learn in a similar way to human. It accumulates the previously learned knowledge and also transfers the learned knowledge into future learning (Chen and Liu 2018). Knowledge distillation provides an effective way to preserve and transfer learned knowledge without catastrophic forgetting. Recently, an increasing number of KD variants, which are based on lifelong learning, have been developed (Jang et al. 2019; Flennerhag et al. 2019; Peng et al. 2019b; Liu et al. 2019e; Lee et al. 2019b; Zhai et al. 2019; Zhou et al. 2020; Shmelkov et al. 2017; Li and Hoiem 2017; Caccia et al. 2020). The methods proposed in (Jang et al. 2019; Peng et al. 2019b; Liu et al. 2019e; Flen-nerhag et al. 2019) adopt meta-learning. Jang et al. (2019) designed meta-transfer networks that can determine what and where to transfer in the teacher-student architecture. Flennerhag et al. (2019) proposed a light-weight framework called Leap for meta-learning over task manifolds by transferring knowledge from one learning process to another. Peng et al. (2019b) designed a new knowledge transfer network architecture for few-shot image recognition. The architecture simultaneously incorporates visual information from images and prior knowledge. Liu et al. (2019e) proposed the semantic-aware knowledge preservation method for image retrieval. The teacher knowledge obtained from the image modalities and semantic information are preserved and transferred.<div style="background-color: #d6d6d6;margin: 12px 0;">终身学习，包括持续学习、连续学习和元学习，旨在以类似于人类的方式学习。它累积之前学到的知识，并将学到的知识转移到未来的学习中（Chen和Liu 2018）。知识蒸馏提供了一种有效的保留和转移学习知识的方法，而不会发生灾难性遗忘。最近，越来越多的基于终身学习的知识蒸馏变体（KD变体）已经被开发出来（Jang等人2019；Flennerhag等人2019；Peng等人2019b；Liu等人2019e；Lee等人2019b；Zhai等人2019；Zhou等人2020；Shmelkov等人2017；Li和Hoiem 2017；Caccia等人2020）。在（Jang等人2019；Peng等人2019b；Liu等人2019e；Flennerhag等人2019）中提出的方法采用了元学习。Jang等人（2019）设计了元转移网络，可以在教师-学生架构中确定转移什么和转移到哪里。Flennerhag等人（2019）提出了一种轻量级框架Leap，用于通过将知识从一个学习过程转移到另一个学习过程来进行任务流形的元学习。Peng等人（2019b）为小样本图像识别设计了一种新的知识转移网络架构。该架构同时结合了图像的视觉信息和先验知识。Liu等人（2019e）提出了用于图像检索的语义感知知识保留方法。从图像模态获得的教师知识和语义信息得到保留和转移。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-15="174,862">Moreover, to address the problem of catastrophic forgetting in lifelong learning, global distillation (Lee et al. 2019b), knowledge distillation-based lifelong GAN (Zhai et al. 2019), multi-model distillation (Zhou et al. 2020) and the other KD-based methods (Li and Hoiem 2017; Shmelkov et al. 2017) have been developed to extract the learned knowledge and teach the student network on new tasks.<div style="background-color: #d6d6d6;margin: 12px 0;">此外，为了解决终身学习中灾难性遗忘的问题，已经开发出了全局蒸馏（Lee et al. 2019b）、基于知识蒸馏的终身GAN（Zhai et al. 2019）、多模型蒸馏（Zhou et al. 2020）以及其他基于KD的方法（Li和Hoiem 2017；Shmelkov et al. 2017），以提取学习到的知识并指导学生网络在新任务上的学习。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-15="141,1143">5.9 NAS-Based Distillation<div style="background-color: #d6d6d6;margin: 12px 0;">5.9 基于NAS的蒸馏</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-15="141,1228">Neural architecture search (NAS), which is one of the most popular auto machine learning (or AutoML) techniques, aims to automatically identify deep neural models and adaptively learn appropriate deep neural structures. In knowledge distillation, the success of knowledge transfer depends on not only the knowledge from the teacher but also the architecture of the student. However, there might be a capacity gap between the large teacher model and the small student model, making it difficult for the student to learn well from the teacher. To address this issue, neural architecture search has been adopted to find the appropriate student architecture in oracle-based (Kang et al. 2020) and architecture-aware knowledge distillation (Liu et al. 2019i). Furthermore, knowledge distillation is employed to improve the efficiency of neural architecture search, such as AdaNAS (Macko et al. 2019), NAS with distilled architecture knowledge (Li et al. 2020a), teacher guided search for architectures or TGSA (Bashivan et al. 2019), and one-shot NAS (Peng et al. 2020). In TGSA, each architecture search step is guided to mimic the intermediate feature representations of the teacher network. The possible structures for the student are efficiently searched and the feature transfer is effectively supervised by the teacher.<div style="background-color: #d6d6d6;margin: 12px 0;">神经架构搜索（NAS），作为最受欢迎的自动机器学习（或AutoML）技术之一，旨在自动识别深度神经网络模型并自适应地学习适当的深度神经网络结构。在知识蒸馏中，知识转移的成功不仅取决于教师的知识，还取决于学生的架构。然而，大型教师模型和小型学生模型之间可能存在容量差距，使得学生难以从教师那里学好。为了解决这个问题，神经架构搜索已被用于在基于Oracle的（Kang et al. 2020）和架构感知的知识蒸馏（Liu et al. 2019i）中找到适当的学生架构。此外，知识蒸馏还被用于提高神经架构搜索的效率，例如AdaNAS（Macko et al. 2019）、具有蒸馏架构知识的NAS（Li et al. 2020a）、教师指导的架构搜索或TGSA（Bashivan et al. 2019）以及单次NAS（Peng et al. 2020）。在TGSA中，每个架构搜索步骤都被指导去模仿教师网络的中间特征表示。学生可能的结构被有效地搜索，特征传递也通过教师有效地进行监督。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-15="891,153">6 Performance Comparison<div style="background-color: #d6d6d6;margin: 12px 0;">6 性能比较</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-15="891,237">Knowledge distillation is an excellent technique for model compression. Through capturing the teacher knowledge and using distillation strategies with teacher-student learning, it provides effective performance of the lightweight student model. Recently, many knowledge distillation methods focus on improving the performance, especially in image classification tasks. In this section, to clearly demonstrate the effectiveness of knowledge distillation, we summarize the classification performance of some typical KD methods on two popular image classification datasets.<div style="background-color: #d6d6d6;margin: 12px 0;">知识蒸馏是一种优秀的模型压缩技术。通过捕获教师模型的知识并使用教师-学生学习的蒸馏策略，它提供了轻量级学生模型的有效性能。最近，许多知识蒸馏方法专注于提高性能，特别是在图像分类任务上。在本节中，为了清楚地展示知识蒸馏的有效性，我们总结了一些典型KD方法在两个流行的图像分类数据集上的分类性能。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-15="923,606">The two datasets are CIFAR10 and CIFAR100 (Krizhevsky and Hinton 2009) that are composed of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="581" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>32</mn></mrow><mo>×</mo><mrow data-mjx-texclass="ORD"><mn>32</mn></mrow></math></mjx-assistive-mml></mjx-container> RGB images taken from 10 and 100 classes, respectively. Both have 50000 training images and 10000 testing images, and each class has the same numbers of training and testing images. For fair comparison, the experimental classification accuracy results (%) of the KD methods are directly derived from the corresponding original papers, as shown in Table 5 for CIFAR10 and Table 6 for CIFAR100. We report the performance of different methods when using different types of knowledge, distillation schemes, and structures of teacher/student models. Specifically, the accuracies in parentheses are the classification results of the teacher and student models, which are trained individually. It should be noted that the pairs of accuracies of DML (Zhang et al. 2018b), DCM (Yao and Sun 2020) and KDCL (Guo et al. 2020) are the performance of teacher and student after online distillation.<div style="background-color: #d6d6d6;margin: 12px 0;">这两个数据集是CIFAR10和CIFAR100（Krizhevsky和Hinton 2009），分别由来自10个和100个类别的<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="582" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>32</mn></mrow><mo>×</mo><mrow data-mjx-texclass="ORD"><mn>32</mn></mrow></math></mjx-assistive-mml></mjx-container> RGB图像组成。两个数据集都有50000张训练图像和10000张测试图像，每个类别都有相同数量的训练和测试图像。为了公平比较，KD方法的实验分类准确度结果（%）直接来源于相应的原始论文，如表5所示为CIFAR10的结果，表6所示为CIFAR100的结果。我们报告了在使用不同类型的知识、蒸馏方案以及教师/学生模型结构时不同方法的性能。具体来说，括号中的准确度是单独训练的教师和学生模型的分类结果。需要注意的是，DML（Zhang et al. 2018b）、DCM（Yao和Sun 2020）和KDCL（Guo et al. 2020）的准确度对是经过在线蒸馏的教师和学生模型的性能。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-15="924,1229">From the performance comparison in Table 5 and Table 6, several observations can be summarized as<div style="background-color: #d6d6d6;margin: 12px 0;">从表5和表6的性能比较中，可以总结出以下几点观察结果</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-15="905,1337"><ul><li>Knowledge distillation can be realized on different deep models.<div style="background-color: #d6d6d6;margin: 12px 0;">- 知识蒸馏可以在不同的深度模型上实现。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-15="905,1412"><ul><li>Model compression of different deep models can be achieved by knowledge distillation.<div style="background-color: #d6d6d6;margin: 12px 0;">- 通过知识蒸馏可以实现不同深度模型的压缩。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-15="905,1487"><ul><li>The online knowledge distillation through collaborative learning (Zhang et al. 2018b; Yao and Sun 2020) can significantly improve the performance of the deep models.<div style="background-color: #d6d6d6;margin: 12px 0;">- 通过协作学习的在线知识蒸馏（Zhang et al. 2018b; Yao和Sun 2020）可以显著提高深度模型的性能。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-15="908,1597"><ul><li>The self-knowledge distillation (Yang et al. 2019b; Yuan et al. 2020; Xu and Liu 2019; Yun et al. 2020) can well improve the performance of the deep models.<div style="background-color: #d6d6d6;margin: 12px 0;">- 自知蒸馏（Yang等人2019b；Yuan等人2020；Xu和Liu 2019；Yun等人2020）可以很好地提高深度模型的性能。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-15="909,1707"><ul><li>The offline and online distillation methods often transfer feature-based knowledge and response-based knowledge, respectively.<div style="background-color: #d6d6d6;margin: 12px 0;">- 离线和在线蒸馏方法通常分别转移基于特征的知识和基于响应的知识。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-15="908,1817"><ul><li>The performance of the lightweight deep models (student) can be improved by the knowledge transfer from the high-capacity teacher models.<div style="background-color: #d6d6d6;margin: 12px 0;">- 通过从高容量教师模型的知识迁移，轻量级深度模型（学生）的性能可以得到提高。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-f464cdd2-0e19-4e1e-90cb-a72e82e6c124" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-15="892,1961">Through the performance comparison of different knowledge distillation methods, it can be easily concluded that knowledge distillation is an effective and efficient technique of compressing deep models.<div style="background-color: #d6d6d6;margin: 12px 0;">通过比较不同的知识蒸馏方法的性能，可以很容易地得出结论，知识蒸馏是一种有效且高效的技术，用于压缩深度模型。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="17" id="mark-045cb006-87da-4d69-b0ae-f41d08873dc5" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-16="139,166">Table 5 Performance comparison of different knowledge distillation methods on CIFAR10. Note that <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="583" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> indicates the performance improvement of the student network learned by each method comparing with the corresponding baseline model<div style="background-color: #d6d6d6;margin: 12px 0;">表5 不同知识蒸馏方法在CIFAR10上的性能比较。注意，<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="584" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container>表示通过每种方法学习到的学生网络与相应基线模型相比的性能提升。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="17" id="mark-045cb006-87da-4d69-b0ae-f41d08873dc5" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-16="133,233"><div class="table-container"><table class="fixed-table"><thead><tr><th>Methods</th><th>Knowledge</th><th>Teacher (baseline)</th><th>Student (baseline)</th><th>Accuracies</th></tr></thead><tbody><tr><td colspan="5">Offline distillation</td></tr><tr><td>FSP (Yim et al. 2017)</td><td>RelK</td><td>ResNet26 (91.91)</td><td>ResNet8 (87.91)</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="585" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c38"></mjx-c><mjx-c class="mjx-c38"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>88.70</mn></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mn>0.79</mn></mrow><mo stretchy="false">↑</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>FT (Kim et al. 2018)</td><td>FeaK</td><td>ResNet56 (93.61)</td><td>ResNet20 (92.22)</td><td>93.15 (0.93 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="586" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>IRG (Liu et al. 2019g)</td><td>RelK</td><td>ResNet20 (91.45)</td><td>ResNet20-x0.5 (88.36)</td><td>90.69 (2.33 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="587" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>SP (Tung and Mori 2019)</td><td>RelK</td><td>WRN-40-1 (93.49)</td><td>WRN-16-1 (91.26)</td><td>91.87 (0.61 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="588" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>SP (Tung and Mori 2019)</td><td>RelK</td><td>WRN-40-2 (95.76)</td><td>WRN-16-8 (94.82)</td><td>95.45 (0.63 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="589" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>FN (Xu et al. 2020b)</td><td>FeaK</td><td>ResNet110 (94.29)</td><td>ResNet56 (93.63)</td><td>94.14 (0.51 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="590" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>FN (Xu et al. 2020b)</td><td>FeaK</td><td>ResNet56 (93.63)</td><td>ResNet20 (92.11)</td><td>92.67 (0.56 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="591" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>AdaIN (Yang et al. 2020a)</td><td>FeaK</td><td>ResNet26 (93.58)</td><td>ResNet8 (87.78)</td><td>89.02 (1.24 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="592" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>AdaIN (Yang et al. 2020a)</td><td>FeaK</td><td>WRN-40-2 (95.07)</td><td>WRN-16-2 (93.98)</td><td>94.67 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="593" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mn>0.69</mn></mrow><mo stretchy="false">↑</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>AE-KD (Du et al. 2020)</td><td>FeaK</td><td>ResNet56 (–)</td><td>MobileNetV2 (75.97)</td><td>77.07 (1.10 ↑)</td></tr><tr><td>JointRD (Li et al. 2020b)</td><td>FeaK</td><td>ResNet34 (95.39)</td><td>plain-CNN 34 (93.73)</td><td>94.78 (1.05 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="594" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>TOFD (Zhang et al. 2020a)</td><td>FeaK</td><td>ResNet152 (–)</td><td>ResNeXt50-4 (94.49)</td><td>97.09 (2.60 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="595" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>TOFD (Zhang et al. 2020a)</td><td>FeaK</td><td>ResNet152 (–)</td><td>MobileNetV2 (90.43)</td><td>93.34 (2.91 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="596" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>CTKD (Zhao et al. 2020a)</td><td>RelK, FeaK</td><td>WRN-40-1 (93.43)</td><td>WRN-16-1 (91.28)</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="597" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c39"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>92.50</mn></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mn>1.22</mn></mrow><mo stretchy="false">↑</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>CTKD (Zhao et al. 2020a)</td><td>RelK, FeaK</td><td>WRN-40-2 (94.70)</td><td>WRN-16-2 (93.68)</td><td>94.42 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="598" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mn>0.74</mn></mrow><mo stretchy="false">↑</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td colspan="5">Online distillation</td></tr><tr><td>Rocket-KD (Zhou et al. 2018)</td><td>FeaK</td><td>WRN-40-1 (93.42)</td><td>WRN-16-1 (91.23)</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="599" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c39"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>92.48</mn></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mn>1.25</mn></mrow><mo stretchy="false">↑</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>DML (Zhang et al. 2018b)</td><td>ResK</td><td>WRN-28-10 (95.01)</td><td>ResNet32 (92.47)</td><td>95.75, 93.18 (0.71 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="600" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>DML (Zhang et al. 2018b)</td><td>ResK</td><td>MobileNet (93.59)</td><td>ResNet32 (92.47)</td><td>94.24, 93.32 (0.85 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="601" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>DML (Zhang et al. 2018b)</td><td>ResK</td><td>ResNet32 (92.47)</td><td>ResNet32 (92.47)</td><td>92.68, 92.80 (0.33 ↑)</td></tr><tr><td>ONE (Zhu and Gong 2018</td><td>ResK</td><td>ResNet32+ONE</td><td>ResNet32 (93.07)</td><td>94.01 (0.84 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="602" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>ONE (Zhu and Gong 2018)</td><td>ResK</td><td>ResNet110+ONE</td><td>ResNet110 (94.44)</td><td>94.83 (0.39 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="603" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>PCL (Wu and Gong 2021)</td><td>ResK</td><td>Student ensemble</td><td>ResNet110 (94.91)</td><td>95.53 (0.62 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="604" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>PCL (Wu and Gong 2021)</td><td>ResK</td><td>Student ensemble</td><td>DenseNet-40-12 (93.19)</td><td>94.13 (0.94 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="605" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>PCL (Wu and Gong 2021)</td><td>ResK</td><td>Student ensemble</td><td>VGG16 (93.96)</td><td>94.74 (0.78 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="606" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>ACNs (Zhang et al. 2021a)</td><td>ResK</td><td>ResNet14 (90.66)</td><td>ResNet14 (90.66)</td><td>92.09 (1.43 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="607" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>ACNs (Zhang et al. 2021a)</td><td>ResK</td><td>VGG11 (91.25)</td><td>VGG11 (91.25)</td><td>92.65 (1.40 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="608" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>ACNs (Zhang et al. 2021a)</td><td>ResK</td><td>AlexNet (73.24)</td><td>AlexNet (73.24)</td><td>78.57 (5.33 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="609" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td colspan="5">Self-Distillation</td></tr><tr><td>Xu and Liu (2019)</td><td>ResK, FeaK</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="610" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>−</mo></math></mjx-assistive-mml></mjx-container></td><td>ResNet32 (92.78)</td><td>93.68 (0.90↑)</td></tr><tr><td>Xu and Liu (2019)</td><td>ResK, FeaK</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="611" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>−</mo></math></mjx-assistive-mml></mjx-container></td><td>DenseNe40(94.53)</td><td>94.80 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="612" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c37"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mn>0.27</mn></mrow><mo stretchy="false">↑</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr></tbody></table></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="17" id="mark-045cb006-87da-4d69-b0ae-f41d08873dc5" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-16="138,1583">7 Applications<div style="background-color: #d6d6d6;margin: 12px 0;">7 应用</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="17" id="mark-045cb006-87da-4d69-b0ae-f41d08873dc5" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-16="141,1669">As an effective technique for the compression and acceleration of deep neural networks, knowledge distillation has been widely used in different fields of artificial intelligence, including visual recognition, speech recognition, natural language processing (NLP), and recommendation systems. Furthermore, knowledge distillation also can be used for other purposes, such as the data privacy and as a defense against adversarial attacks. This section briefly reviews applications of knowledge distillation.<div style="background-color: #d6d6d6;margin: 12px 0;">作为一种有效的深度神经网络压缩和加速技术，知识蒸馏已经在人工智能的不同领域得到了广泛应用，包括视觉识别、语音识别、自然语言处理（NLP）和推荐系统。此外，知识蒸馏还可以用于其他目的，例如数据隐私保护和对抗攻击的防御。本节简要回顾了知识蒸馏的应用。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="17" id="mark-045cb006-87da-4d69-b0ae-f41d08873dc5" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-16="889,1583">7.1 KD in Visual Recognition<div style="background-color: #d6d6d6;margin: 12px 0;">7.1 视觉识别中的KD</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="17" id="mark-045cb006-87da-4d69-b0ae-f41d08873dc5" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-16="889,1669">In last few years, a variety of knowledge distillation methods have been widely used for model compression in different visual recognition applications. Specifically, most of the knowledge distillation methods were previously developed for image classification (Li and Hoiem 2017; Peng et al. 2019b; Bagherinezhad et al. 2018; Chen et al. 2018a; Wang et al. 2019b; Mukherjee et al. 2019; Zhu et al. 2019) and then extended to other visual recognition applications, including face recognition (Luo et al. 2016; Kong et al. 2019; Yan et al. 2019; Ge et al. 2018; Wang et al. 2018b, 2019c; Duong et al. 2019; Wu et al. 2020; Wang et al. 2017; Zhang et al. 2020b; Wang et al. 2020b), image/video segmentation (He et al. 2019; Mullapudi et al. 2019; Dou et al. 2020; Liu et al. 2019h; Siam et al. 2019; Hou et al. 2020; Bergmann et al. 2020), action recognition (Luo et al. 2018; Hao and Zhang 2019; Thoker and Gall 2019; Garcia et al. 2018; Wang et al. 2019e; Wu et al. 2019b; Zhang et al. 2020c; Cui et al. 2020), object detection (Li et al. 2017; Shmelkov et al. 2017; Cun and Pun 2020; Wang et al. 2019d; Huang et al. 2020; Wei et al. 2018; Hong and Yu 2019; Chawla et al. 2021), lane detection (Hou et al. 2019), person re-identification (Wu et al. 2019a), pedestrian detection (Shen et al. 2016), facial landmark detection (Dong and Yang 2019), pose estimation (Nie et al. 2019; Zhang et al. 2019a; Zhao et al. 2018), video captioning (Pan et al. 2020; Zhang et al. 2020f), person search (Munjal et al. 2019; Zhang et al. 2021c), image retrieval (Liu et al. 2019e), shadow detection (Chen et al. 2020c), saliency estimation (Li et al. 2019), depth estimation (Pilzer et al. 2019; Ye et al. 2019), visual odometry (Saputra et al. 2019), text-to-image synthesis (Yuan and Peng 2020; Tan et al. 2021), video classification (Zhang and Peng 2018; Bhard-wajet al. 2019), visual question answering (Mun et al. 2018; Aditya et al. 2019) and anomaly detection (Bergmann et al. 2020). Since knowledge distillation in classification task is fundamental for other tasks, we briefly review knowledge distillation in challenging image classification settings, such as face recognition and action recognition.<div style="background-color: #d6d6d6;margin: 12px 0;">在过去几年里，各种知识蒸馏方法被广泛应用于不同视觉识别应用中的模型压缩。具体来说，大多数知识蒸馏方法之前是为了图像分类而开发的（Li和Hoiem 2017；Peng等 2019b；Bagherinezhad等 2018；Chen等 2018a；Wang等 2019b；Mukherjee等 2019；Zhu等 2019），然后扩展到其他视觉识别应用，包括人脸识别（Luo等 2016；Kong等 2019；Yan等 2019；Ge等 2018；Wang等 2018b, 2019c；Duong等 2019；Wu等 2020；Wang等 2017；Zhang等 2020b；Wang等 2020b），图像/视频分割（He等 2019；Mullapudi等 2019；Dou等 2020；Liu等 2019h；Siam等 2019；Hou等 2020；Bergmann等 2020），动作识别（Luo等 2018；Hao和Zhang 2019；Thoker和Gall 2019；Garcia等 2018；Wang等 2019e；Wu等 2019b；Zhang等 2020c；Cui等 2020），目标检测（Li等 2017；Shmelkov等 2017；Cun和Pun 2020；Wang等 2019d；Huang等 2020；Wei等 2018；Hong和Yu 2019；Chawla等 2021），车道检测（Hou等 2019），行人重识别（Wu等 2019a），行人检测（Shen等 2016），面部标记检测（Dong和Yang 2019），姿态估计（Nie等 2019；Zhang等 2019a；Zhao等 2018），视频字幕（Pan等 2020；Zhang等 2020f），行人搜索（Munjal等 2019；Zhang等 2021c），图像检索（Liu等 2019e），阴影检测（Chen等 2020c），显著性估计（Li等 2019），深度估计（Pilzer等 2019；Ye等 2019），视觉里程计（Saputra等 2019），文本到图像合成（Yuan和Peng 2020；Tan等 2021），视频分类（Zhang和Peng 2018；Bhard-wajet al. 2019），视觉问答（Mun等 2018；Aditya等 2019）以及异常检测（Bergmann等 2020）。由于分类任务中的知识蒸馏是其他任务的基础，我们简要回顾了在具有挑战性的图像分类设置中的知识蒸馏，例如人脸识别和动作识别。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="18" id="mark-f815f708-2ae2-4f19-8af5-4deb155493d4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-17="139,166">Table 6 Performance comparison of different knowledge distillation methods on CIFAR 100. Note that <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="613" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> indicates the performance improvement of the student network learned by each method comparing with the corresponding baseline model<div style="background-color: #d6d6d6;margin: 12px 0;">表6 不同知识蒸馏方法在CIFAR 100上的性能比较。注意 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="614" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> 表示每种方法学习到的学生网络相较于相应基线模型的性能提升。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="18" id="mark-f815f708-2ae2-4f19-8af5-4deb155493d4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-17="130,229"><div class="table-container"><table class="fixed-table"><thead><tr><th>Methods</th><th>Knowledge</th><th>Teacher (baseline)</th><th>Student (baseline)</th><th>Accuracies</th></tr></thead><tbody><tr><td colspan="5">Offline distillation</td></tr><tr><td>FSP (Yim et al. 2017)</td><td>RelK</td><td>ResNet32 (64.06)</td><td>ResNet14 (58.65)</td><td>63.33 (4.68 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="615" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>FT (Kim et al. 2018)</td><td>FeaK</td><td>ResNet110 (73.09)</td><td>ResNet56 (71.96)</td><td>74.48 (2.52 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="616" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>RKD (Park et al. 2019)</td><td>RelK, FeaK</td><td>ResNet50 (77.76)</td><td>VGG11 (71.26)</td><td>74.66 (3.40 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="617" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>IRG (Liu et al. 2019g)</td><td>RelK</td><td>ResNet20 (78.40)</td><td>ResNet20-x0.5 (72.51)</td><td>74.64 (2.13 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="618" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>CCKD (Peng et al. 2019a)</td><td>RelK, ResK</td><td>ResNet110 (–)</td><td>ResNet20 (68.40)</td><td>72.40 (4.00 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="619" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>KR (Liu et al. 2019c)</td><td>FeaK</td><td>ResNet32 (64.06)</td><td>ResNet14 (58,65)</td><td>63.95 (5.30 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="620" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>LKD (Li et al. 2020e)</td><td>RelK</td><td>ResNet110 (75.76)</td><td>ResNet20 (69.47)</td><td>72.63 (3.16 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="621" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>LKD (Li et al. 2020e)</td><td>RelK</td><td>WRN-40-2 (75.61)</td><td>WRN-16-2 (73.10)</td><td>75.44 (2.34 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="622" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>SSKD (Xu et al. 2020a)</td><td>RelK, ResK</td><td>VGG13 (75.38)</td><td>MobileNetV2 (65.79)</td><td>71.53 (5.74 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="623" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>SSKD (Xu et al. 2020a)</td><td>RelK, ResK</td><td>ResNet50 (79.10)</td><td>MobileNetV2 (65.79)</td><td>72.57 (6.78 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="624" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>FN (Xu et al. 2020b)</td><td>FeaK</td><td>ResNet110 (82.01)</td><td>ResNet56 (81.73)</td><td>82.23 (0.50 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="625" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>AdaIN (Yang et al. 2020a)</td><td>FeaK</td><td>WRN-40-4 (78.31)</td><td>WRN-16-4 (75.68)</td><td>78.25 (2.57 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="626" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>AdaIN (Yang et al. 2020a)</td><td>FeaK</td><td>ResNet34 (77.26)</td><td>MobileNetV2 (68.36)</td><td>70.66 (2.30 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="627" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>PAD- <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="628" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi>L</mi></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msub></math></mjx-assistive-mml></mjx-container> (Zhang et al. 2020e)</td><td>FeaK</td><td>ResNet18 (75.86)</td><td>MobileNetV2 (68.16)</td><td>74.06 (5.90 ↑)</td></tr><tr><td>MGD (Yue et al. 2020)</td><td>FeaK</td><td>WRN-28-4 (78.91)</td><td>WRN-28-2 (75.12)</td><td>78.82 (3.70 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="629" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>AE-KD (Du et al. 2020)</td><td>FeaK</td><td>ResNet56 (–)</td><td>ResNet20 (69.06)</td><td>70.55 (1.49 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="630" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>JointRD (Li et al. 2020b)</td><td>FeaK</td><td>ResNet18 (77.92)</td><td>plain-CNN 18 (77.44)</td><td>78.24 (0.80 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="631" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>TOFD (Zhang et al. 2020a)</td><td>FeaK</td><td>ResNet152 (–)</td><td>ResNet50 (77.42)</td><td>84.74 (7.32 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="632" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>TOFD (Zhang et al. 2020a)</td><td>FeaK</td><td>ResNet152 (–)</td><td>ShuffleNetV2 (72.38)</td><td>76.68 (4.30 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="633" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>CTKD (Zhao et al. 2020a)</td><td>RelK, FeaK</td><td>ResNet110 (72.65)</td><td>ResNet20 (68.33)</td><td>70.75 (2.42 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="634" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>CTKD (Zhao et al. 2020a)</td><td>RelK, FeaK</td><td>WRN-40-2 (75.42)</td><td>WRN-16-2 (72.27)</td><td>74.70 (2.43 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="635" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>SemCKD (Chen et al. 2021)</td><td>FeaK</td><td>ResNet-32x4 (79.42)</td><td>VGG13 (74.82)</td><td>79.43 (4.61 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="636" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>SemCKD (Chen et al. 2021)</td><td>FeaK</td><td>WRN-40-2 (75.61)</td><td>MobileNetV2 (65.43)</td><td>69.61 (4.18 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="637" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>SemCKD (Chen et al. 2021)</td><td>FeaK</td><td>VGG13 (74.64)</td><td>ShuffleNetV2 (72.60)</td><td>76.39 (3.79 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="638" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>RKD (Gao et al. 2021)</td><td>FeaK</td><td>ResNet34 (73.05)</td><td>ResNet18 (68.06)</td><td>72.82 (4.76 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="639" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td colspan="5">Online distillation</td></tr><tr><td>Rocket-KD (Zhou et al. 2018)</td><td>FeaK</td><td>WRN-40-1 (–)</td><td>WRN-16-1 (56.30)</td><td>67.00 (10.07 ↑)</td></tr><tr><td>DML (Zhang et al. 2018b)</td><td>ResK</td><td>WRN-28-10 (78.69)</td><td>MobileNet (73.65)</td><td>80.28, 77.39 (3.74 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="640" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>DML (Zhang et al. 2018b)</td><td>ResK</td><td>MobileNet (73.65)</td><td>ResNet32 (68.99)</td><td>76.13, 71.10 (8.11 ↑)</td></tr><tr><td>ONE (Zhu and Gong 2018)</td><td>ResK</td><td>ResNet32+ONE</td><td>ResNet32 (68.82)</td><td>73.39 (4.57 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="641" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>ONE (Zhu and Gong 2018)</td><td>ResK</td><td>ResNet110+ONE</td><td>ResNet110 (74.67)</td><td>78.38 (3.71 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="642" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>DCM (Yao and Sun 2020)</td><td>ResK</td><td>WRN-28-10 (81.28)</td><td>ResNet110 (73.45)</td><td>82.18, 77.01 (3.56 ↑)</td></tr><tr><td>DCM (Yao and Sun 2020)</td><td>ResK</td><td>WRN-28-10 (81.28</td><td>MobileNet (73.70)</td><td>83.17, 78.57 (4.87 ↑)</td></tr><tr><td>KDCL (Guo et al. 2020)</td><td>ResK</td><td>WRN-16-2 (72.20)</td><td>ResNet32 (69.90)</td><td>75.50, 74.30 (4.40 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="643" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>PCL (Wu and Gong 2021)</td><td>ResK</td><td>Student ensemble</td><td>ResNet110 (76.21)</td><td>79.98 (3.77 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="644" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>PCL (Wu and Gong 2021)</td><td>ResK</td><td>Student ensemble</td><td>DenseNet-40-12 (71.03)</td><td>73.09 (2.06 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="645" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>ACNs (Zhang et al. 2021a)</td><td>ResK</td><td>ResNet14 (66.88)</td><td>ResNet14 (66.88)</td><td>68.40 (1.52 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="646" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td>ACNs (Zhang et al. 2021a)</td><td>ResK</td><td>VGG11 (67.38)</td><td>VGG11 (67.38)</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="647" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>70.11</mn></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mn>2.73</mn></mrow><mo stretchy="false">↑</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>ACNs (Zhang et al. 2021a)</td><td>ResK</td><td>AlexNet (39.45)</td><td>AlexNet (39.45)</td><td>46.27 (6.82 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="648" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↑</mo></math></mjx-assistive-mml></mjx-container> )</td></tr><tr><td colspan="5">Self-distillation</td></tr><tr><td>Xu and Liu (2019)</td><td>ResK, FeaK</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="649" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>−</mo></math></mjx-assistive-mml></mjx-container></td><td>DenseNet (74.80)</td><td>76.32 (1.52↑)</td></tr><tr><td>SD (Yang et al. 2019b)</td><td>ResK</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="650" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>−</mo></math></mjx-assistive-mml></mjx-container></td><td>ResNet32 (68.39)</td><td>71.29 (2.90↑)</td></tr><tr><td>Tf-KD (Yuan et al. 2020)</td><td>ResK</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="651" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>−</mo></math></mjx-assistive-mml></mjx-container></td><td>ResNet18 (75.87)</td><td>77.10 (1.23↑)</td></tr><tr><td>Tf-KD (Yuan et al. 2020)</td><td>ResK</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="652" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>−</mo></math></mjx-assistive-mml></mjx-container></td><td>ShuffleNetV2 (70.34)</td><td>72.23 (1.89↑)</td></tr><tr><td>Tf-KD (Yuan et al. 2020)</td><td>ResK</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="653" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>−</mo></math></mjx-assistive-mml></mjx-container></td><td>ResNeXt29 (81.03)</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="654" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c38"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2191"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>82.08</mn></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mn>1.05</mn></mrow><mo stretchy="false">↑</mo></mrow><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container></td></tr><tr><td>CS-KD (Yun et al. 2020)</td><td>ResK</td><td><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="655" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>−</mo></math></mjx-assistive-mml></mjx-container></td><td>ResNet18 (75.29)</td><td>78.01 (2.72↑)</td></tr></tbody></table></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="19" id="mark-207ac4e4-a61c-4ab2-a125-cca6fc1ab2cf" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-18="172,1119">Existing KD-based face recognition methods focus on not only efficient deployment but also competitive recognition accuracy (Luo et al. 2016; Kong et al. 2019; Yan et al. 2019; Ge et al. 2018; Wang et al. 2018b, 2019c; Duong et al. 2019; Wang et al. 2017, 2020b; Zhang et al. 2020b). Specifically, in Luo et al. (2016), the knowledge from the chosen informative neurons of top hint layer of the teacher network is transferred into the student network. A teacher weighting strategy with the loss of feature representations from hint layers was designed for knowledge transfer to avoid the incorrect supervision by the teacher (Wang et al. 2018b). A recursive knowledge distillation method was designed by using a previous student network to initialize the next one (Yan et al. 2019). Since most face recognition methods perform the open-set recognition, i.e., the classes/identities on test set are unknown to the training set, the face recognition criteria are usually distance metrics between feature representations of positive and negtive samples, e.g., the angular loss in Duong et al. (2019) and the correlated embedding loss in Wu et al. (2020).<div style="background-color: #d6d6d6;margin: 12px 0;">现有的基于KD的面部识别方法不仅关注高效的部署，还关注具有竞争力的识别精度（Luo et al. 2016; Kong et al. 2019; Yan et al. 2019; Ge et al. 2018; Wang et al. 2018b, 2019c; Duong et al. 2019; Wang et al. 2017, 2020b; Zhang et al. 2020b）。具体来说，在Luo et al. (2016)中，将教师网络顶层提示层的选定信息性神经元的知识转移到了学生网络。为了防止教师错误的监督，设计了一种教师加权策略，该策略与提示层的特征表示损失相关（Wang et al. 2018b）。Yan et al. (2019)设计了一种递归知识蒸馏方法，使用前一个学生网络来初始化下一个网络。由于大多数面部识别方法执行的是开放集识别，即测试集上的类/身份对训练集是未知的，因此面部识别准则通常是正样本和负样本特征表示之间的距离度量，例如Duong et al. (2019)中的角度损失和Wu et al. (2020)中的相关嵌入损失。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="19" id="mark-207ac4e4-a61c-4ab2-a125-cca6fc1ab2cf" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-18="173,1854">To improve low-resolution face recognition accuracy, the knowledge distillation framework is developed by using architectures between high-resolution face teacher and low-resolution face student for model acceleration and improved classification performance (Ge et al. 2018; Wang et al. 2019c; Kong et al. 2019; Ge et al. 2020). Specifically, Ge et al. (2018) proposed a selective knowledge distillation method, in which the teacher network for high-resolution face recognition selectively transfers its informative facial features into the student network for low-resolution face recognition through sparse graph optimization. In Kong et al. (2019), cross-resolution face recognition was realized by designing a resolution invariant model unifying both face hallucination and heterogeneous recognition sub-nets. To get efficient and effective low resolution face recognition model, the multi-kernel maximum mean discrepancy between student and teacher networks was adopted as the feature loss (Wang et al. 2019c). In addition, the KD-based face recognition can be extended to face alignment and verification by changing the losses in knowledge distillation (Wang et al. 2017).<div style="background-color: #d6d6d6;margin: 12px 0;">为了提高低分辨率人脸识别的准确性，通过使用高分辨率人脸教师模型与低分辨率人脸学生模型之间的架构，开发了知识蒸馏框架，以加速模型并提高分类性能（Ge et al. 2018; Wang et al. 2019c; Kong et al. 2019; Ge et al. 2020）。具体来说，Ge et al. (2018) 提出了一种选择性知识蒸馏方法，其中高分辨率人脸识别的教师网络通过稀疏图优化选择性将其信息性面部特征传递到用于低分辨率人脸识别的学生网络。在 Kong et al. (2019) 中，通过设计一个统一人脸超分辨率和异质识别子网的分辨率不变模型，实现了跨分辨率人脸识别。为了获得高效且有效的低分辨率人脸识别模型，采用学生网络与教师网络之间的多核最大均值差异作为特征损失（Wang et al. 2019c）。此外，通过改变知识蒸馏中的损失，基于KD的人脸识别可以扩展到人脸对齐和验证（Wang et al. 2017）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="19" id="mark-207ac4e4-a61c-4ab2-a125-cca6fc1ab2cf" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-18="920,643">Recently, knowledge distillation has been used successfully for solving the complex image classification problems (Zhu et al. 2019; Bagherinezhad et al. 2018; Peng et al. 2019b; Li and Hoiem 2017; Chen et al. 2018a; Wang et al. 2019b; Mukherjee et al. 2019). For incomplete, ambiguous and redundant image labels, the label refinery model through self-distillation and label progression is proposed to learn soft, informative, collective and dynamic labels for complex image classification (Bagherinezhad et al. 2018). To address catastrophic forgetting with CNN in a variety of image classification tasks, a learning without forgetting method for CNN, including both knowledge distillation and lifelong learning is proposed to recognize a new image task and to preserve the original tasks (Li and Hoiem 2017). For improving image classification accuracy, Chen et al. (2018a) proposed the feature maps-based knowledge distillation method with GAN. It transfers knowledge from feature maps to a student. Using knowledge distillation, a visual interpretation and diagnosis framework that unifies the teacher-student models for interpretation and a deep generative model for diagnosis is designed for image classifiers (Wang et al. 2019b). Similar to the KD-based low-resolution face recognition, Zhu et al. (2019) proposed deep feature distillation for the low-resolution image classification, in which the output features of a student match that of teacher.<div style="background-color: #d6d6d6;margin: 12px 0;">最近，知识蒸馏已成功用于解决复杂的图像分类问题（Zhu et al. 2019; Bagherinezhad et al. 2018; Peng et al. 2019b; Li 和 Hoiem 2017; Chen et al. 2018a; Wang et al. 2019b; Mukherjee et al. 2019）。针对不完整、模糊和冗余的图像标签，提出了通过自蒸馏和标签演化的标签精炼模型，以学习复杂图像分类的柔和、信息丰富、集体和动态标签（Bagherinezhad et al. 2018）。为了解决在多种图像分类任务中CNN面临的灾难性遗忘问题，提出了一种结合知识蒸馏和终身学习的CNN无遗忘学习方法，以识别新的图像任务并保留原始任务（Li 和 Hoiem 2017）。为了提高图像分类精度，Chen et al. (2018a) 提出了基于特征图的知识蒸馏方法，结合了GAN。它将知识从特征图传递给学生。利用知识蒸馏，设计了一个视觉解释和诊断框架，该框架统一了用于解释的教师-学生模型和用于诊断的深度生成模型，用于图像分类器（Wang et al. 2019b）。类似于基于KD的低分辨率人脸识别，Zhu et al. (2019) 提出了深度特征蒸馏用于低分辨率图像分类，其中学生的输出特征与教师相匹配。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="19" id="mark-207ac4e4-a61c-4ab2-a125-cca6fc1ab2cf" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-18="924,1559">As argued in Sect. 5.3, knowledge distillation with the teacher-student structure can transfer and preserve the cross-modality knowledge. Efficient and effective action recognition under its cross-modal task scenarios can be successfully realized (Thoker and Gall 2019; Luo et al. 2018; Garcia et al. 2018; Hao and Zhang 2019; Wu et al. 2019b; Zhang et al. 2020c). These methods are the examples of spatiotemporal modality distillation with a different knowledge transfer for action recognition. Examples include mutual teacher-student networks (Thoker and Gall 2019), multiple stream networks (Garcia et al. 2018), spatiotemporal distilled dense-connectivity network (Hao and Zhang 2019), graph distillation (Luo et al. 2018) and multi-teacher to multi-student networks (Wu et al. 2019b; Zhang et al. 2020c). Among these methods, the light-weight student can distill and share the knowledge information from multiple modalities stored in the teacher.<div style="background-color: #d6d6d6;margin: 12px 0;">如第5.3节所述，基于教师-学生结构的知识蒸馏可以转移并保留跨模态知识。在其跨模态任务场景下，可以成功实现高效有效的动作识别（Thoker和Gall 2019；Luo等 2018；Garcia等 2018；Hao和Zhang 2019；Wu等 2019b；Zhang等 2020c）。这些方法是在动作识别中应用时空模态蒸馏，进行不同知识转移的例子。包括相互教师-学生网络（Thoker和Gall 2019）、多流网络（Garcia等 2018）、时空蒸馏密集连接网络（Hao和Zhang 2019）、图蒸馏（Luo等 2018）以及多教师到多学生网络（Wu等 2019b；Zhang等 2020c）。在这些方法中，轻量级学生可以从教师那里蒸馏并共享存储在多个模态中的知识信息。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="20" id="mark-a5168668-48cb-4a6f-a927-9a1bb5fb55b5" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-19="175,239">We summarize two main observations of distillation-based visual recognition applications, as follows.<div style="background-color: #d6d6d6;margin: 12px 0;">我们总结了基于蒸馏的视觉识别应用的两个主要观察结果，如下所示。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="20" id="mark-a5168668-48cb-4a6f-a927-9a1bb5fb55b5" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-19="157,367"><ul><li>Knowledge distillation provides efficient and effective teacher-student learning for a variety of different visual recognition tasks, because a lightweight student network can be easily trained under the guidance of the high-capacity teacher networks.<div style="background-color: #d6d6d6;margin: 12px 0;">- 知识蒸馏为各种不同的视觉识别任务提供了高效有效的教师-学生学习方式，因为轻量级学生网络可以在高容量教师网络的指导下轻松训练。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="20" id="mark-a5168668-48cb-4a6f-a927-9a1bb5fb55b5" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-19="160,551"><ul><li>Knowledge distillation can make full use of the different types of knowledge in complex data sources, such as cross-modality data, multi-domain data and multi-task data and low-resolution data, because of flexible teacher-student architectures and knowledge transfer.<div style="background-color: #d6d6d6;margin: 12px 0;">- 由于灵活的教师-学生架构和知识转移，知识蒸馏可以充分利用复杂数据源中不同类型的知识，例如跨模态数据、多域数据和多任务数据以及低分辨率数据。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="20" id="mark-a5168668-48cb-4a6f-a927-9a1bb5fb55b5" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-19="138,780">7.2 KD in NLP<div style="background-color: #d6d6d6;margin: 12px 0;">7.2 KD在NLP中的应用</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="20" id="mark-a5168668-48cb-4a6f-a927-9a1bb5fb55b5" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-19="141,861">Conventional language models such as BERT are very time consuming and resource consuming with complex cumbersome structures. Knowledge distillation is extensively studied in the field of natural language processing (NLP), in order to obtain the lightweight, efficient and effective language models. More and more KD methods are proposed for solving the numerous NLP tasks (Liu et al. 2019b; Gordon and Duh 2019; Haidar and Rezagholizadeh 2019; Yang et al. 2020c; Tang et al. 2019; Hu et al. 2018; Sun et al. 2019; Nakashole and Flauger 2017; Jiao et al. 2020; Wang et al. 2018d; Zhou et al. 2019a; Sanh et al. 2019; Turc et al. 2019; Arora et al. 2019; Clark et al. 2019; Kim and Rush 2016; Mou et al. 2016; Liu et al. 2019f; Hahn and Choi 2019; Tan et al. 2019; Kuncoro et al. 2016; Cui et al. 2017; Wei et al. 2019; Freitag et al. 2017; Shakeri et al. 2019; Aguilar et al. 2020; Fu et al. 2021; Zhang et al. 2021b; Chen et al. 2020b; Wang and Du 2021). The existing NLP tasks using KD contain neural machine translation (NMT) (Hahn and Choi 2019; Zhou et al. 2019a; Li et al. 2021; Kim and Rush 2016; Gordon and Duh 2019; Tan et al. 2019; Wei et al. 2019; Freitag et al. 2017; Zhang et al. 2021b), text generation (Chen et al. 2020b; Haidar and Rezagholizadeh 2019), question answering system (Hu et al. 2018; Wang et al. 2018d; Arora et al. 2019; Yang et al. 2020c), event detection (Liu et al. 2019b), document retrieval (Shakeri et al. 2019), text recognition (Wang and Du 2021) and so on. Among these KD-based NLP methods, most of them belong to natural language understanding (NLU), and many of these KD methods for NLU are designed as the task-specific distillation (Tang et al. 2019; Turc et al. 2019; Mou et al. 2016) and multi-task distillation (Liu et al. 2019f; Yang et al. 2020c; Sanh et al. 2019; Clark et al. 2019). In what follows, we describe KD research works for neural machine translation and then for extending a typical multilingual representation model entitled bidirectional encoder representations from transformers ( or BERT) (Devlin et al. 2019) in NLU.<div style="background-color: #d6d6d6;margin: 12px 0;">传统的语言模型如BERT由于结构复杂繁琐，非常耗时且资源消耗大。知识蒸馏在自然语言处理（NLP）领域被广泛研究，目的是为了获得轻量级、高效且有效的语言模型。越来越多的KD方法被提出用于解决众多的NLP任务（Liu et al. 2019b; Gordon and Duh 2019; Haidar and Rezagholizadeh 2019; Yang et al. 2020c; Tang et al. 2019; Hu et al. 2018; Sun et al. 2019; Nakashole and Flauger 2017; Jiao et al. 2020; Wang et al. 2018d; Zhou et al. 2019a; Sanh et al. 2019; Turc et al. 2019; Arora et al. 2019; Clark et al. 2019; Kim and Rush 2016; Mou et al. 2016; Liu et al. 2019f; Hahn and Choi 2019; Tan et al. 2019; Kuncoro et al. 2016; Cui et al. 2017; Wei et al. 2019; Freitag et al. 2017; Shakeri et al. 2019; Aguilar et al. 2020; Fu et al. 2021; Zhang et al. 2021b; Chen et al. 2020b; Wang and Du 2021）。现有的使用KD的NLP任务包括神经机器翻译（NMT）（Hahn and Choi 2019; Zhou et al. 2019a; Li et al. 2021; Kim and Rush 2016; Gordon and Duh 2019; Tan et al. 2019; Wei et al. 2019; Freitag et al. 2017; Zhang et al. 2021b）、文本生成（Chen et al. 2020b; Haidar and Rezagholizadeh 2019）、问答系统（Hu et al. 2018; Wang et al. 2018d; Arora et al. 2019; Yang et al. 2020c）、事件检测（Liu et al. 2019b）、文档检索（Shakeri et al. 2019）、文本识别（Wang and Du 2021）等等。在这些基于KD的NLP方法中，大部分属于自然语言理解（NLU），而且许多为NLU设计的KD方法被设计为特定任务的蒸馏（Tang et al. 2019; Turc et al. 2019; Mou et al. 2016）和多任务蒸馏（Liu et al. 2019f; Yang et al. 2020c; Sanh et al. 2019; Clark et al. 2019）。接下来，我们描述KD在神经机器翻译方面的研究工作，然后是扩展一个典型的多语种表示模型，即双向编码器表示来自变换器（或BERT）（Devlin et al. 2019）在NLU中的应用。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="20" id="mark-a5168668-48cb-4a6f-a927-9a1bb5fb55b5" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-19="926,239">In natural language processing, neural machine translation is the hottest application. However, the existing NMT models with competitive performance is very large. To obtain lightweight NMT, there are many extended knowledge distillation methods for neural machine translation (Hahn and Choi 2019; Zhou et al. 2019a; Kim and Rush 2016; Gordon and Duh 2019; Wei et al. 2019; Freitag et al. 2017; Tan et al. 2019). Recently, Zhou et al. (2019a) empirically proved the better performance of the KD-based non-autoregressive machine translation (NAT) model largely relies on its capacity and the distilled data via knowledge transfer. Gordon and Duh (2019) explained the good performance of sequence-level knowledge distillation from the perspective of data augmentation and regularization. In (Kim and Rush 2016), the effective word-level knowledge distillation is extended to the sequence-level one in the sequence generation scenario of NMT. The sequence generation student model mimics the sequence distribution of the teacher. To overcome the multilingual diversity, Tan et al. (2019) proposed multi-teacher distillation, in which multiple individual models for handling bilingual pairs are teacher and a multilingual model is student. To improve the translation quality, an ensemble of mutiple NMT models as teacher supervise the student model with a data filtering method Freitag et al. (2017). To improve the performance of machine translation and machine reading tasks, (Wei et al. 2019) proposed a novel online knowledge distillation method, which addresses the unstableness of the training process and the decreasing performance on each validation set. In this online KD, the best evaluated model during training is chosen as teacher and updated by any subsequent better model. If the next model had the poor performance, the current teacher model would guide it.<div style="background-color: #d6d6d6;margin: 12px 0;">在自然语言处理中，神经机器翻译是最热门的应用。然而，现有性能领先的NMT模型体积非常大。为了获得轻量级的NMT，有许多针对神经机器翻译的扩展知识蒸馏方法（Hahn和Choi 2019；Zhou等 2019a；Kim和Rush 2016；Gordon和Duh 2019；Wei等 2019；Freitag等 2017；Tan等 2019）。最近，Zhou等（2019a）通过实验证明了基于KD的非自回归机器翻译（NAT）模型的更好性能很大程度上依赖于其容量和通过知识传递得到的蒸馏数据。Gordon和Duh（2019）从数据增强和正则化的角度解释了序列级知识蒸馏的良好性能。在（Kim和Rush 2016）中，有效的单词级知识蒸馏被扩展到序列级，应用于NMT的序列生成场景。序列生成学生模型模拟教师的序列分布。为了克服多语种的多样性，Tan等（2019）提出了多教师蒸馏，其中多个处理双语对的个体模型作为教师，而一个多语种模型作为学生。为了提高翻译质量，Freitag等（2017）提出了一种多个NMT模型组合作为教师，通过数据过滤方法监督学生模型。为了提高机器翻译和机器阅读任务的性能，Wei等（2019）提出了一种新颖的在线知识蒸馏方法，该方法解决了训练过程的不稳定性以及每个验证集上性能下降的问题。在这种在线KD中，训练期间评估最佳模型被选为教师，并通过任何后续的更好模型进行更新。如果下一个模型的性能较差，当前教师模型将会指导它。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="20" id="mark-a5168668-48cb-4a6f-a927-9a1bb5fb55b5" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-19="924,1412">As a multilingual representation model, BERT has attracted attention in natural language understanding (Devlin et al. 2019), but it is also a cumbersome deep model that is not easy to be deployed. To address this problem, several lightweight variations of BERT (called BERT model compression) using knowledge distillation are proposed (Sun et al. 2019; Jiao et al. 2020; Tang et al. 2019; Sanh et al. 2019; Wang et al. 2020a; Liu et al. 2020b; Fu et al. 2021). Sun et al. (2019) proposed patient knowledge distillation for BERT model compression (BERT-PKD), which is used for sentiment classification, paraphrase similarity matching, natural language inference, and machine reading comprehension. In the patient KD method, the feature representations of the [CLS] token from the hint layers of teacher are transferred to the student. To accelerate language inference, Jiao et al. (2020) proposed TinyBERT that is two-stage transformer knowledge distillation. It contains general-domain and task-specific knowledge distillation. For sentence classification and matching, Tang et al. (2019) proposed task-specific knowledge distillation from the BERT teacher model into a bidirectional long short-term memory network (BiLSTM). In (Sanh et al. 2019), a lightweight student model called Distil-BERT with the same generic structure as BERT is designed and learned on a variety of tasks of NLP. In Aguilar et al. (2020), a simplified student BERT is proposed by using the internal representations of a large teacher BERT via internal distillation.<div style="background-color: #d6d6d6;margin: 12px 0;">作为一种多语言表示模型，BERT 在自然语言理解领域（Devlin et al. 2019）引起了关注，但它也是一个庞大的深度模型，不易部署。为了解决这个问题，提出了几种使用知识蒸馏的BERT轻量级变体（称为BERT模型压缩）（Sun et al. 2019; Jiao et al. 2020; Tang et al. 2019; Sanh et al. 2019; Wang et al. 2020a; Liu et al. 2020b; Fu et al. 2021）。Sun et al. (2019) 提出了用于BERT模型压缩的患者知识蒸馏方法（BERT-PKD），该方法用于情感分类、释义相似度匹配、自然语言推理和机器阅读理解。在患者KD方法中，将教师模型的提示层的 [CLS] 标记的特征表示转移到学生模型。为了加速语言推理，Jiao et al. (2020) 提出了两阶段转换器知识蒸馏的TinyBERT。它包含通用领域和特定任务的知识蒸馏。对于句子分类和匹配，Tang et al. (2019) 提出了从BERT教师模型到双向长短期记忆网络（BiLSTM）的特定任务知识蒸馏。在 (Sanh et al. 2019) 中，设计了一个与BERT具有相同通用结构的轻量级学生模型Distil-BERT，并在多种NLP任务上进行了学习。在Aguilar et al. (2020) 中，通过内部蒸馏使用大型教师BERT的内部表示，提出了一个简化的学生BERT。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-277a3e48-80df-4c81-99c8-5c5fce937b14" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-20="175,458">Furthermore, some typical KD methods for NLP with different perspectives are represented below. For question answering, to improve the efficiency and robustness of machine reading comprehension, Hu et al. (2018) proposed an attention-guided answer distillation method, which fuses generic distillation and answer distillation to avoid confusing answers. For a task-specific distillation (Turc et al. 2019), the performance of knowledge distillation with the interactions among pre-training, distillation and fine-tuning for the compact student model is studied. The proposed pre-trained distillation performs well in sentiment classification, natural language inference, textual entailment. For a multi-task distillation in the context of natural language understanding, Clark et al. (2019) proposed the single-multi born-again distillation, which is based on born-again neural networks (Furlanello et al. 2018). Single-task teachers teach a multi-task student. For multilingual representations, knowledge distillation transfers knowledge among the multilingual word embeddings for bilingual dictionary induction (Nakashole and Flauger 2017). For low-resource languages, knowledge transfer is effective across ensembles of multilingual models (Cui et al. 2017).<div style="background-color: #d6d6d6;margin: 12px 0;">此外，以下是一些从不同视角出发的针对自然语言处理的典型知识蒸馏（KD）方法。对于问答任务，为了提高机器阅读理解的效率和鲁棒性，胡等人（2018年）提出了一种注意力引导的答案蒸馏方法，该方法融合了通用蒸馏和答案蒸馏，以避免混淆答案。对于特定任务的蒸馏（Turc等人，2019年），研究了在紧凑的学生模型中，知识蒸馏在预训练、蒸馏和微调之间的交互作用的表现。所提出的预训练蒸馏在情感分类、自然语言推理、文本蕴含等任务中表现良好。在自然语言理解的多任务蒸馏背景下，Clark等人（2019年）提出了基于新生神经网络（Furlanello等人，2018年）的单多重生蒸馏方法，其中单任务教师教导多任务学生。对于多语种表示，知识蒸馏在多语种词向量之间转移知识，用于双语文本词典的诱导（Nakashole和Flauger，2017年）。对于低资源语言，知识传递在多语种模型的集成之间是有效的（Cui等人，2017年）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-277a3e48-80df-4c81-99c8-5c5fce937b14" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-20="174,1267">Several observations about knowledge distillation for natural language processing are summarized as follows.<div style="background-color: #d6d6d6;margin: 12px 0;">关于自然语言处理的知识蒸馏有以下几点观察总结。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-277a3e48-80df-4c81-99c8-5c5fce937b14" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-20="157,1374"><ul><li>Knowledge distillation provides efficient and effective lightweight language deep models. The large-capacity teacher model can transfer the rich knowledge from a large number of different kinds of language data to train a small student model, so that the student can quickly complete many language tasks with effective performance.<div style="background-color: #d6d6d6;margin: 12px 0;">- 知识蒸馏为轻量级语言深度模型提供了高效且有效的方法。大容量的教师模型能够将从大量不同种类的语言数据中获取的丰富知识转移给小型的学生模型，使学生模型能够快速完成许多语言任务，并表现出有效的性能。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-277a3e48-80df-4c81-99c8-5c5fce937b14" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-20="155,1597"><ul><li>The teacher-student knowledge transfer can easily and effectively solve many multilingual tasks, considering that knowledge from multilingual models can be transferred and shared by each other.<div style="background-color: #d6d6d6;margin: 12px 0;">- 教师到学生的知识传递可以轻松且有效地解决许多多语言任务，考虑到多语言模型之间的知识可以相互转移和共享。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-277a3e48-80df-4c81-99c8-5c5fce937b14" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-20="157,1743"><ul><li>In deep language models, the sequence knowledge can be effectively transferred from large networks into small networks.<div style="background-color: #d6d6d6;margin: 12px 0;">- 在深度语言模型中，序列知识可以从大型网络有效地转移到小型网络。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-277a3e48-80df-4c81-99c8-5c5fce937b14" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-20="140,1877">7.3 KD in Speech Recognition<div style="background-color: #d6d6d6;margin: 12px 0;">7.3 语音识别中的KD</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-277a3e48-80df-4c81-99c8-5c5fce937b14" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-20="141,1961">In the field of speech recognition, deep neural acoustic models have attracted attention and interest due to their powerful performance. However, more and more real-time speech recognition systems are deployed in embedded platforms with limited computational resources and fast response time. The state-of-the-art deep complex models cannot satisfy the requirement of such speech recognition scenarios. To satisfy these requirements, knowledge distillation is widely studied and applied in many speech recognition tasks. There are many knowledge distillation systems for designing lightweight deep acoustic models for speech recognition (Chebotar and Waters 2016; Wong and Gales 2016; Chan et al. 2015; Price et al. 2016; Fukuda et al. 2017; Bai et al. 2019; Ng et al. 2018; Albanie et al. 2018; Lu et al. 2017; Shi et al. 2019a; Roheda et al. 2018; Shi et al. 2019b; Gao et al. 2019; Ghorbani et al. 2018; Takashima et al. 2018; Watanabe et al. 2017; Shi et al. 2019c; Asami et al. 2017; Huang et al. 2018; Shen et al. 2018; Perez et al. 2020; Shen et al. 2019c; Oord et al. 2018; Kwon et al. 2020; Shen et al. 2020). In particular, these KD-based speech recognition applications have spoken language identification (Shen et al. 2018, 2019c, 2020), audio classification (Gao et al. 2019; Perez et al. 2020), text-independent speaker recognition (Ng et al. 2018), speech enhancement (Watanabe et al. 2017), acoustic event detection (Price et al. 2016; Shi et al. 2019a, b), speech synthesis (Oord et al. 2018) and so on.<div style="background-color: #d6d6d6;margin: 12px 0;">在语音识别领域，深度神经声学模型因其强大的性能而吸引了关注和兴趣。然而，越来越多的实时语音识别系统被部署在计算资源有限且需要快速响应的嵌入式平台上。最先进的深度复杂模型无法满足此类语音识别场景的要求。为了满足这些要求，知识蒸馏在许多语音识别任务中得到了广泛的研究和应用。有许多知识蒸馏系统被设计用于为语音识别构建轻量级深度声学模型（Chebotar和Waters 2016；Wong和Gales 2016；Chan等 2015；Price等 2016；Fukuda等 2017；Bai等 2019；Ng等 2018；Albanie等 2018；Lu等 2017；Shi等 2019a；Roheda等 2018；Shi等 2019b；Gao等 2019；Ghorbani等 2018；Takashima等 2018；Watanabe等 2017；Shi等 2019c；Asami等 2017；Huang等 2018；Shen等 2018；Perez等 2020；Shen等 2019c；Oord等 2018；Kwon等 2020；Shen等 2020）。特别是，这些基于KD的语音识别应用包括语言识别（Shen等 2018, 2019c, 2020）、音频分类（Gao等 2019；Perez等 2020）、文本无关说话人识别（Ng等 2018）、语音增强（Watanabe等 2017）、声学事件检测（Price等 2016；Shi等 2019a, b）、语音合成（Oord等 2018）等等。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-277a3e48-80df-4c81-99c8-5c5fce937b14" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-20="923,973">Most existing knowledge distillation methods for speech recognition, use teacher-student architectures to improve the efficiency and recognition accuracy of acoustic models (Chan et al. 2015; Watanabe et al. 2017; Chebotar and Waters 2016; Shen et al. 2019c; Lu et al. 2017; Shen et al. 2018, 2020; Gao et al. 2019; Shi et al. 2019c, a; Perez et al. 2020). Using a recurrent neural network (RNN) for holding the temporal information from speech sequences, the knowledge from the teacher RNN acoustic model is transferred into a small student DNN model (Chan et al. 2015). Better speech recognition accuracy is obtained by combining multiple acoustic modes. The ensembles of different RNNs with different individual training criteria are designed to train a student model through knowledge transfer (Chebotar and Waters 2016). The learned student model performs well on 2,000-hour large vocabulary continuous speech recognition (LVCSR) tasks in 5 languages. To strengthen the generalization of the spoken language identification (LID) model on short utterances, the knowledge of feature representations of the long utterance-based teacher network is transferred into the short utterance-based student network that can discriminate short utterances and perform well on the short duration utterance-based LID tasks (Shen et al. 2018). To further improve the performance of short utterance-based LID, an interactive teacher-student online distillation learning is proposed to enhance the performance of the feature representations of short utterances (Shen et al. 2019c). The LID performance on short utterances is also improved by distilling internal representation knowledge of teacher on longer utterances into the one of student on short utterances (Shen et al. 2020).<div style="background-color: #d6d6d6;margin: 12px 0;">大多数现有的语音识别知识蒸馏方法，如教师-学生架构，用于提高声学模型的效率和识别精度（Chan et al. 2015; Watanabe et al. 2017; Chebotar and Waters 2016; Shen et al. 2019c; Lu et al. 2017; Shen et al. 2018, 2020; Gao et al. 2019; Shi et al. 2019c, a; Perez et al. 2020）。使用循环神经网络（RNN）来保持语音序列的时序信息，将教师RNN声学模型的知识转移到一个小型学生深度神经网络（DNN）模型中（Chan et al. 2015）。通过组合多个声学模型可以获得更好的语音识别精度。设计不同个体训练标准的多个RNN的组合，通过知识转移来训练学生模型（Chebotar and Waters 2016）。学习到的学生模型在5种语言的2,000小时大规模词汇连续语音识别（LVCSR）任务上表现良好。为了加强口语识别（LID）模型在短语上的泛化能力，将基于长句的教师网络的特征表示知识转移到基于短句的学生网络中，该网络能够区分短句并在基于短时语音的LID任务上表现良好（Shen et al. 2018）。为了进一步提高基于短句的LID性能，提出了一种交互式教师-学生在线蒸馏学习，以增强短句的特征表示性能（Shen et al. 2019c）。通过将教师对较长语句的内部表示知识蒸馏到学生对短语句的内部表示中，也提高了短语句的LID性能（Shen et al. 2020）。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-5f5af9ea-5f85-4caa-942f-ecd3dc6e314d" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-21="173,165">Meanwhile, for audio classification, a multi-level feature distillation method is developed and an adversarial learning strategy is adopted to optimize the knowledge transfer (Gao et al. 2019). To improve noise robust speech recognition, knowledge distillation is employed as the tool of speech enhancement (Watanabe et al. 2017). In Perez et al. (2020), a audio-visual multi-modal knowledge distillation method is proposed. knowledge is transferred from the teacher models on visual and acoustic data into a student model on audio data. In essence, this distillation shares the cross-modal knowledge among the teachers and students (Perez et al. 2020; Albanie et al. 2018; Roheda et al. 2018). For efficient acoustic event detection, a quantized distillation method is proposed by using both knowledge distillation and quantization (Shi et al. 2019a). The quantized distillation transfers knowledge from a large CNN teacher model with better detection accuracy into a quantized RNN student model.<div style="background-color: #d6d6d6;margin: 12px 0;">同时，针对音频分类，开发了一种多层次特征蒸馏方法，并采用对抗学习策略来优化知识迁移（Gao et al. 2019）。为了提高噪声鲁棒性语音识别，将知识蒸馏作为语音增强的工具（Watanabe et al. 2017）。在Perez et al. (2020)中，提出了一种音频-视觉多模态知识蒸馏方法。知识从视觉和声学数据的教师模型转移到音频数据的学生模型中。本质上，这种蒸馏在教师和学生之间共享了跨模态知识（Perez et al. 2020; Albanie et al. 2018; Roheda et al. 2018）。为了高效地进行声学事件检测，提出了一种量化蒸馏方法，该方法同时使用了知识蒸馏和量化（Shi et al. 2019a）。量化蒸馏将具有更好检测精度的大型CNN教师模型的知识转移到量化RNN学生模型中。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-5f5af9ea-5f85-4caa-942f-ecd3dc6e314d" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-21="176,789">Unlike most existing traditional frame-level KD methods, sequence-level KD can perform better in some sequence models for speech recognition, such as connectionist temporal classification (CTC) (Wong and Gales 2016; Takashima et al. 2018; Huang et al. 2018). In (Huang et al. 2018), sequence-level KD is introduced into connectionist temporal classification, in order to match an output label sequence used in the training of teacher model and the input speech frames used in distillation. In Wong and Gales (2016), the effect of speech recognition performance on frame-level and sequence-level student-teacher training is studied and a new sequence-level student-teacher training method is proposed. The teacher ensemble is constructed by using sequence-level combination instead of frame-level combination. To improve the performance of unidirectional RNN-based CTC for real-time speech recognition, the knowledge of a bidirectional LSTM-based CTC teacher model is transferred into a unidirectional LSTM-based CTC student model via frame-level KD and sequence-level KD (Takashima et al. 2018).<div style="background-color: #d6d6d6;margin: 12px 0;">与大多数现有的传统帧级知识蒸馏（KD）方法不同，序列级KD在语音识别的某些序列模型中表现更好，例如连接主义时间分类（CTC）（Wong和Gales 2016；Takashima等人2018；Huang等人2018）。在（Huang等人2018）中，序列级KD被引入到连接主义时间分类中，以匹配教师模型训练中使用的输出标签序列和蒸馏中使用的输入语音帧。在Wong和Gales（2016）中，研究了帧级和序列级学生-教师训练对语音识别性能的影响，并提出了一种新的序列级学生-教师训练方法。教师集成是通过使用序列级组合而不是帧级组合构建的。为了提高基于单向RNN的CTC在实时语音识别中的性能，通过帧级KD和序列级KD将基于双向LSTM的CTC教师模型的知识转移到了基于单向LSTM的CTC学生模型中（Takashima等人2018）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-5f5af9ea-5f85-4caa-942f-ecd3dc6e314d" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-21="173,1487">Moreover, knowledge distillation can be used to solve some special issues in speech recognition (Bai et al. 2019; Asami et al. 2017; Ghorbani et al. 2018). To overcome over-fitting issue of DNN acoustic models when data are scarce, knowledge distillation is employed as a regularization way to train adapted model with the supervision of the source model (Asami et al. 2017). The final adapted model achieves better performance on three real acoustic domains. To overcome the degradation of the performance of non-native speech recognition, an advanced multi-accent student model is trained by distilling knowledge from the multiple accent-specific RNN-CTC models (Ghorbani et al. 2018). In essence, knowledge distillation in (Asami et al. 2017; Ghorbani et al. 2018) realizes the cross-domain knowledge transfer. To solve the complexity of fusing the external language model (LM) into sequence-to-sequence model (Seq2seq) for speech recognition, knowledge distillation is employed as an effective tool to integrate a LM (teacher) into Seq2seq model (student) (Bai et al. 2019). The trained Seq2seq model can reduce character error rates in sequence-to-sequence speech recognition.<div style="background-color: #d6d6d6;margin: 12px 0;">此外，知识蒸馏可以用于解决语音识别中的一些特殊问题（Bai et al. 2019; Asami et al. 2017; Ghorbani et al. 2018）。为了克服在数据稀缺的情况下深度神经网络（DNN）声学模型的过拟合问题，知识蒸馏被用作一种正则化方式，在源模型的监督下训练适应模型（Asami et al. 2017）。最终的适应模型在三个真实声学域上取得了更好的性能。为了克服非母语语音识别性能的退化，通过从多个特定口音的RNN-CTC模型中蒸馏知识，训练了一个先进的多元口音学生模型（Ghorbani et al. 2018）。实质上，(Asami et al. 2017; Ghorbani et al. 2018)中的知识蒸馏实现了跨领域的知识迁移。为了解决将外部语言模型（LM）融合到语音识别的序列到序列模型（Seq2seq）中的复杂性，知识蒸馏被用作一种有效工具，将LM（教师）集成到Seq2seq模型（学生）中（Bai et al. 2019）。训练后的Seq2seq模型可以降低序列到序列语音识别中的字符错误率。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-5f5af9ea-5f85-4caa-942f-ecd3dc6e314d" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-21="924,277">In summary, several observations on knowledge distillation-based speech recognition can be concluded as follows.<div style="background-color: #d6d6d6;margin: 12px 0;">总结来说，基于知识蒸馏的语音识别可以得出以下几点观察结果。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-5f5af9ea-5f85-4caa-942f-ecd3dc6e314d" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-21="908,384"><ul><li>The lightweight student model can satisfy the practical requirements of speech recognition, such as real-time responses, use of limited resources and high recognition accuracy.<div style="background-color: #d6d6d6;margin: 12px 0;">- 轻量级的学生模型可以满足语音识别的实际要求，例如实时响应、使用有限资源和高识别精度。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-5f5af9ea-5f85-4caa-942f-ecd3dc6e314d" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-21="909,533"><ul><li>Many teacher-student architectures are built on RNN models because of the temporal property of speech sequences. In general, the RNN models are chosen as the teacher, which can well preserve and transfer the temporal knowledge from real acoustic data to a student model.<div style="background-color: #d6d6d6;margin: 12px 0;">- 许多教师-学生架构是基于RNN模型构建的，因为语音序列具有时间属性。一般来说，选择RNN模型作为教师，可以很好地保持并从真实声学数据向学生模型转移时间知识。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-5f5af9ea-5f85-4caa-942f-ecd3dc6e314d" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-21="909,717"><ul><li>Sequence-level knowledge distillation can be well applied to sequence models with good performance. In fact, the frame-level KD always uses the response-based knowledge, but sequence-level KD usually transfers the feature-based knowledge from hint layers of teacher models.<div style="background-color: #d6d6d6;margin: 12px 0;">- 序列级别的知识蒸馏可以很好地应用于性能良好的序列模型。实际上，帧级别的KD总是使用基于响应的知识，但序列级别的KD通常从教师模型的提示层转移基于特征的知识。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-5f5af9ea-5f85-4caa-942f-ecd3dc6e314d" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div class="locator-translate" data-positiontag-21="909,936"><ul><li>Knowledge distillation using teacher-student knowledge transfer can easily solve the cross-domain or cross-modal speech recognition in applications such as multi-accent and multilingual speech recognition.<div style="background-color: #d6d6d6;margin: 12px 0;">- 使用教师-学生知识迁移的知识蒸馏可以轻松解决跨领域或跨模态的语音识别问题，例如多口音和多语种语音识别。</div></li></ul></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-5f5af9ea-5f85-4caa-942f-ecd3dc6e314d" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-21="891,1106">7.4 KD in Other Applications<div style="background-color: #d6d6d6;margin: 12px 0;">7.4 KD在其他应用中的使用</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-5f5af9ea-5f85-4caa-942f-ecd3dc6e314d" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-21="891,1191">The full and correct leverages of external knowledge, such as in a user review or in images, play a very important role in the effectiveness of deep recommendation models. Reducing the complexity and improving the efficiency of deep recommendation models is also very necessary. Recently, knowledge distillation has been successfully applied in recommender systems for deep model compression and acceleration (Chen et al. 2018b; Tang and Wang 2018; Pan et al. 2019). In (Tang and Wang 2018), knowledge distillation is first introduced into the recommender systems and called ranking distillation because the recommendation is expressed as a ranking problem. Chen et al. (2018b) proposed an adversarial knowledge distillation method for efficient recommendation. A teacher as the right review predication network supervises the student as user-item prediction network (generator). The student learning is adjusted by adversarial adaption between teacher and student networks. Unlike distillation in Chen et al. (2018b), Tang and Wang (2018), Pan et al. (2019) designed a enhanced collaborative denoising autoencoder (ECAE) model for recommender systems via knowledge distillation to capture useful knowledge from user feedbacks and to reduce noise. The unified ECAE framework contains a generation network, a retraining network and a distillation layer that transfers knowledge and reduces noise from the generation network.<div style="background-color: #d6d6d6;margin: 12px 0;">充分且正确地利用外部知识，如用户评论或图片，在深度推荐模型的有效性中起着非常重要的作用。降低深度推荐模型的复杂性和提高效率也是非常必要的。最近，知识蒸馏已成功应用于推荐系统，用于深度模型的压缩和加速（Chen et al. 2018b; Tang and Wang 2018; Pan et al. 2019）。在（Tang and Wang 2018）中，知识蒸馏首次被引入推荐系统，并被称为排名蒸馏，因为推荐被表达为一个排名问题。Chen et al. (2018b) 提出了一种用于高效推荐的对抗性知识蒸馏方法。一个作为右侧评论预测网络的教学网络监督着一个作为用户-物品预测网络（生成器）的学生网络。学生学习通过教师和学生网络之间的对抗性适应进行调整。与Chen et al. (2018b) 中的蒸馏不同，Tang and Wang (2018), Pan et al. (2019) 设计了一个通过知识蒸馏捕获用户反馈中的有用知识并减少噪声的增强协同降噪自动编码器（ECAE）模型。统一的ECAE框架包含一个生成网络、一个重训练网络和一个蒸馏层，该层从生成网络转移知识并减少噪声。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="23" id="mark-40a20fa9-b4e8-45a3-a867-e6b7a77ecf92" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-22="174,164">Using the natural characteristic of knowledge distillation with teacher-student architectures, knowledge distillation is used as an effective strategy to solve adversarial attacks or perturbations of deep models (Papernot et al. 2016; Ross and Doshi-Velez 2018; Goldblum et al. 2020; Gil et al. 2019) and the issue of the unavailable data due to the privacy, confidentiality and security concerns (Lopes et al. 2017; Papernot et al. 2017; Wang et al. 2019a; Bai et al. 2020; Vongkulb-hisal et al. 2019). To be specific, the perturbations of the adversarial samples can be overcome by the robust outputs of the teacher networks via distillation (Ross and Doshi-Velez 2018; Papernot et al. 2016). To avoid exposing the private data, multiple teachers access subsets of the sensitive or unlabelled data and supervise the student (Papernot et al. 2017; Vongkulbhisal et al. 2019). To address the issue of privacy and security, the data to train the student network is generated by using the layer activations or layer spectral activations of the teacher network via data-free distillation (Lopes et al. 2017). To protect data privacy and prevent intellectual piracy, Wang et al. (2019a) proposed a private model compression framework via knowledge distillation. The student model is applied to public data while the teacher model is applied to both sensitive and public data. This private knowledge distillation adopts privacy loss and batch loss to further improve privacy. To consider the compromise between privacy and performance, Bai et al. (2020) developed a few shot network compression method via a novel layer-wise knowledge distillation with few samples per class. Of course, there are other special interesting applications of knowledge distillation, such as neural architecture search (Macko et al. 2019; Bashivan et al. 2019), interpretability of deep neural networks (Liu et al. 2018b), and federated learning (Bistritz et al. 2020; Lin et al. 2020; Seo et al. 2020; He et al. 2020a).<div style="background-color: #d6d6d6;margin: 12px 0;">利用知识蒸馏与师生架构的自然特性，知识蒸馏被用作一种有效策略来解决深度模型的对抗攻击或扰动（Papernot et al. 2016；Ross和Doshi-Velez 2018；Goldblum et al. 2020；Gil et al. 2019），以及由于隐私、保密和安全问题导致的数据不可用问题（Lopes et al. 2017；Papernot et al. 2017；Wang et al. 2019a；Bai et al. 2020；Vongkulb-hisal et al. 2019）。具体来说，对抗样本的扰动可以通过蒸馏过程中教师网络的鲁棒输出克服（Ross和Doshi-Velez 2018；Papernot et al. 2016）。为了避免暴露私有数据，多个教师访问敏感或未标记数据的子集并指导学生（Papernot et al. 2017；Vongkulbhisal et al. 2019）。为了解决隐私和安全问题，训练学生网络的数据是通过使用教师网络的层激活或层谱激活通过无需数据蒸馏生成的（Lopes et al. 2017）。为了保护数据隐私并防止知识产权盗版，Wang et al. (2019a) 提出了一个通过知识蒸馏的私有模型压缩框架。学生模型应用于公共数据，而教师模型同时应用于敏感和公共数据。这种私有知识蒸馏采用隐私损失和批次损失以进一步改善隐私。为了考虑隐私和性能之间的妥协，Bai et al. (2020) 开发了一种通过新颖的逐层知识蒸馏用每类少量样本的网络压缩方法。当然，知识蒸馏还有其他一些特别有趣的应用，例如神经架构搜索（Macko et al. 2019；Bashivan et al. 2019），深度神经网络的解释性（Liu et al. 2018b），以及联邦学习（Bistritz et al. 2020；Lin et al. 2020；Seo et al. 2020；He et al. 2020a）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="23" id="mark-40a20fa9-b4e8-45a3-a867-e6b7a77ecf92" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-22="139,1432">8 Conclusion and Discussion<div style="background-color: #d6d6d6;margin: 12px 0;">8 结论与讨论</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="23" id="mark-40a20fa9-b4e8-45a3-a867-e6b7a77ecf92" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-22="142,1521">Knowledge distillation and its applications have aroused considerable attention in recent few years. In this paper, we present a comprehensive review on knowledge distillation, from the perspectives of knowledge, distillation schemes, teacher-student architectures, distillation algorithms, performance comparison and applications. Below, we discuss the challenges of knowledge distillation and provide some insights on the future research of knowledge distillation.<div style="background-color: #d6d6d6;margin: 12px 0;">知识蒸馏及其应用在近几年来引起了相当大的关注。在本文中，我们从知识、蒸馏方案、师-生架构、蒸馏算法、性能比较和应用的角度，对知识蒸馏进行了全面的综述。下面，我们讨论知识蒸馏的挑战，并对知识蒸馏的未来研究提供一些见解。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="23" id="mark-40a20fa9-b4e8-45a3-a867-e6b7a77ecf92" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-22="140,1844">8.1 Challenges<div style="background-color: #d6d6d6;margin: 12px 0;">8.1 挑战</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="23" id="mark-40a20fa9-b4e8-45a3-a867-e6b7a77ecf92" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-22="141,1925">For knowledge distillation, the key is to 1) extract rich knowledge from the teacher and 2) to transfer the knowledge from the teacher to guide the training of the student. Therefore, we discuss the challenges in knowledge distillation from the followings aspects: the quality of knowledge, the types of distillation, the design of the teacher-student architectures, and the theory behind knowledge distillation.<div style="background-color: #d6d6d6;margin: 12px 0;">对于知识蒸馏来说，关键在于 1) 从教师模型中提取丰富的知识，2) 将知识从教师模型转移，以指导学生模型的训练。因此，我们从以下方面讨论知识蒸馏的挑战：知识的质量、蒸馏的类型、师-生架构的设计以及知识蒸馏背后的理论。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="23" id="mark-40a20fa9-b4e8-45a3-a867-e6b7a77ecf92" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-22="922,240">Most KD methods leverage a combination of different kinds of knowledge, including response-based, feature-based, and relation-based knowledge. Therefore, it is important to know the influence of each individual type of knowledge and to know how different kinds of knowledge help each other in a complementary manner. For example, the response-based knowledge has a similar motivation to label smoothing and the model regularization (Kim and Kim 2017; Muller et al. 2019; Ding et al. 2019); The featured-based knowledge is often used to mimic the intermediate process of the teacher and the relation-based knowledge is used to capture the relationships across different samples. To this end, it is still challenging to model different types of knowledge in a unified and complementary framework. For example, the knowledge from different hint layers may have different influences on the training of the student model: 1) response-based knowledge is from the last layer; 2) feature-based knowledge from the deeper hint/guided layers may suffer from over-regularization (Romero et al. 2015).<div style="background-color: #d6d6d6;margin: 12px 0;">大多数KD方法利用不同种类知识的组合，包括基于响应的知识、基于特征的知识和基于关系的知识。因此，了解每种单独知识的 influence 以及不同种类的知识如何以互补的方式相互帮助是很重要的。例如，基于响应的知识与标签平滑和模型正则化（Kim和Kim 2017；Muller等人2019；Ding等人2019）的动机相似；基于特征的知识通常用于模仿教师的中间过程，而基于关系的知识用于捕捉不同样本之间的关系。为此，在一个统一和互补的框架中建模不同类型的知识仍然是一个挑战。例如，来自不同提示层的知识可能对学生模型的训练有不同的影响：1) 基于响应的知识来自最后一层；2) 来自更深提示/引导层的基于特征的知识可能会遭受过正则化（Romero等人2015）。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="23" id="mark-40a20fa9-b4e8-45a3-a867-e6b7a77ecf92" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-22="922,937">How to transfer the rich knowledge from the teacher to a student is a key step in knowledge distillation. Generally, the existing distillation methods can be categorized into offline distillation, online distillation and self distillation. Offline distillation is usually used to transfer knowledge from a complex teacher model, while the teacher model and the student model are comparable in the settings of online distillation and self distillation. To improve the efficacy of knowledge transfer, the relationships between the model complexity and existing distillation schemes or other novel distillation schemes (Sun et al. 2021) should be further investigated.<div style="background-color: #d6d6d6;margin: 12px 0;">如何将教师丰富的知识传递给学生是知识蒸馏的关键步骤。通常，现有的蒸馏方法可以分为离线蒸馏、在线蒸馏和自蒸馏。离线蒸馏通常用于从复杂的教师模型中传递知识，而在在线蒸馏和自蒸馏的设置中，教师模型和学生模型是可比的。为了提高知识传递的效度，应进一步研究模型复杂性与现有蒸馏方案或其他新颖蒸馏方案（Sun et al. 2021）之间的关系。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="23" id="mark-40a20fa9-b4e8-45a3-a867-e6b7a77ecf92" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-22="925,1339">Currently, most KD methods focus on new types of knowledge or distillation loss functions, leaving the design of the teacher-student architectures poorly investigated (Nowak and Corso 2018; Crowley et al. 2018; Kang et al. 2020; Liu et al. 2019i; Ashok et al. 2018; Liu et al. 2019a). In fact, apart from the knowledge and distillation algorithms, the relationship between the structures of the teacher and the student also significantly influences the performance of knowledge distillation. For example, on one hand, some recent works find that the student model can learn little from some teacher models due to the model capacity gap between the teacher model and the student model (Zhang et al. 2019b; Kang et al. 2020); On the other hand, from some early theoretical analysis on the capacity of neural networks, shallow networks are capable of learning the same representation as deep neural networks (Ba and Caruana 2014). Therefore, the design of an effective student model or construction of a proper teacher model are still challenging problems in knowledge distillation.<div style="background-color: #d6d6d6;margin: 12px 0;">目前，大多数知识蒸馏（KD）方法专注于新型知识或蒸馏损失函数，而对教师-学生架构的设计研究不足（Nowak和Corso 2018；Crowley et al. 2018；Kang et al. 2020；Liu et al. 2019i；Ashok et al. 2018；Liu et al. 2019a）。实际上，除了知识和蒸馏算法之外，教师和学生结构之间的关系也显著影响知识蒸馏的性能。例如，一方面，一些近期的工作发现，由于教师模型和学生模型之间的模型容量差距（Zhang et al. 2019b；Kang et al. 2020），学生模型从某些教师模型中学到的知识很少；另一方面，从对神经网络容量的早期理论分析来看，浅层网络能够学习到与深层神经网络相同的表示（Ba和Caruana 2014）。因此，设计有效的学生模型或构建适当的教学模型仍然是知识蒸馏中的挑战性问题。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="23" id="mark-40a20fa9-b4e8-45a3-a867-e6b7a77ecf92" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-22="926,2001">Despite a huge number of the knowledge distillation methods and applications, the understanding of knowledge distillation including theoretical explanations and empirical evaluations remains insufficient (Lopez-Paz et al. 2016; Phuong and Lampert 2019a; Cho and Hariharan 2019). For example, distillation can be viewed as a form of learning with privileged information (Lopez-Paz et al. 2016). The assumption of linear teacher and student models enables the study of the theoretical explanations of characteristics of the student learning via distillation (Phuong and Lampert 2019a). Furthermore, some empirical evaluations and analysis on the efficacy of knowledge distillation were performed by Cho and Hariharan (2019). However, a deep understanding of generalizability of knowledge distillation, especially how to measure the quality of knowledge or the quality of the teacher-student architecture, is still very difficult to attain.<div style="background-color: #d6d6d6;margin: 12px 0;">尽管存在大量的知识蒸馏方法和应用，但对知识蒸馏的理解，包括理论解释和实证评估仍然不足（Lopez-Paz等人2016年；Phuong和Lampert 2019a；Cho和Hariharan 2019年）。例如，蒸馏可以被看作是一种带有特权的知识学习形式（Lopez-Paz等人2016年）。线性教师和学生模型的假设使得可以通过蒸馏研究学生学习的理论解释（Phuong和Lampert 2019a）。此外，Cho和Hariharan（2019年）对知识蒸馏的有效性进行了一些实证评估和分析。然而，要深入理解知识蒸馏的泛化性，尤其是如何衡量知识的质量或教师-学生架构的质量，仍然非常困难。</div></div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-23="140,669">8.2 Future Directions<div style="background-color: #d6d6d6;margin: 12px 0;">8.2 未来方向</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="141,751">In order to improve the performance of knowledge distillation, the most important factors include what kind of teacher-student network architecture, what kind of knowledge is learned from the teacher network, and where is distilled into the student network.<div style="background-color: #d6d6d6;margin: 12px 0;">为了提高知识蒸馏的性能，最重要的因素包括教师-学生网络架构的类型、从教师网络中学到的知识类型以及知识被蒸馏到学生网络的位置。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="174,937">The model compression and acceleration methods for deep neural networks usually fall into four different categories, namely parameter pruning and sharing, low-rank factorization, transferred compact convolutional filters and knowledge distillation (Cheng et al. 2018). In existing knowledge distillation methods, there are only a few related works discussing the combination of knowledge distillation and other kinds of compressing methods. For example, quantized knowledge distillation, which can be seen as a parameter pruning method, integrates network quantization into the teacher-student architectures (Polino et al. 2018; Mishra and Marr 2018; Wei et al. 2018). Therefore, to learn efficient and effective lightweight deep models for the deployment on portable platforms, the hybrid compression methods via both knowledge distillation and other compressing techniques are necessary, since most compressing techniques require a retraining/fine-tuning process. Furthermore, how to decide the proper orders for applying different compressing methods will be an interesting topic for future study.<div style="background-color: #d6d6d6;margin: 12px 0;">深度神经网络的模型压缩和加速方法通常分为四类，即参数剪枝与共享、低秩分解、迁移紧凑卷积滤波器以及知识蒸馏（Cheng et al. 2018）。在现有的知识蒸馏方法中，只有少数相关工作讨论了知识蒸馏与其他压缩方法的结合。例如，量化知识蒸馏，可以被视为一种参数剪枝方法，将网络量化整合到教师-学生架构中（Polino et al. 2018；Mishra 和 Marr 2018；Wei et al. 2018）。因此，为了在便携式平台上部署高效且有效的轻量级深度模型，通过知识蒸馏和其他压缩技术的混合压缩方法是必要的，因为大多数压缩技术都需要一个重新训练/微调过程。此外，如何确定应用不同压缩方法的适当顺序将是未来研究的有趣话题。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="175,1634">Apart from model compression for acceleration for deep neural networks, knowledge distillation also can be used in other problems because of the natural characteristics of knowledge transfer on the teacher-student architecture. Recently, knowledge distillation has been applied to the data privacy and security (Wang et al. 2019a), adversarial attacks of deep models (Papernot et al. 2016), cross-modalities (Gupta et al. 2016), multiple domains (Asami et al. 2017), catastrophic forgetting (Lee et al. 2019b), accelerating learning of deep models (Chen et al. 2016), efficiency of neural architecture search (Bashivan et al. 2019), self-supervision (Noroozi et al. 2018), and data augmentation (Lee et al. 2019a; Gordon and Duh 2019). Another interesting example is that the knowledge transfer from the small teacher networks to a large student network can accelerate the student learning (Chen et al. 2016). This is very quite different from vanilla knowledge distillation. The feature representations learned from unlabelled data by a large model can also supervise the target model via distillation (Noroozi et al. 2018). To this end, the extensions of knowledge distillation for other purposes and applications might be a meaningful future direction.<div style="background-color: #d6d6d6;margin: 12px 0;">除了深度神经网络的加速模型压缩外，由于知识传递在教师-学生架构上的自然特性，知识蒸馏也可以用于其他问题。近期，知识蒸馏已被应用于数据隐私和安全（Wang et al. 2019a）、深度模型的对抗攻击（Papernot et al. 2016）、跨模态（Gupta et al. 2016）、多个领域（Asami et al. 2017）、灾难性遗忘（Lee et al. 2019b）、加速深度模型的学习（Chen et al. 2016）、神经架构搜索的效率（Bashivan et al. 2019）、自监督（Noroozi et al. 2018）以及数据增强（Lee et al. 2019a; Gordon 和 Duh 2019）。另一个有趣的例子是，从小型教师网络到大型学生网络的知识传递可以加速学生的学习（Chen et al. 2016）。这与传统的知识蒸馏非常不同。大型模型从未标记数据中学习到的特征表示也可以通过蒸馏来监督目标模型（Noroozi et al. 2018）。为此，知识蒸馏在其他目的和应用方面的扩展可能是一个有意义的未来方向。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="922,496">The learning of knowledge distillation is similar to the human beings learning. It can be practicable to popularize the knowledge transfer to the classic and traditional machine learning methods (Zhou et al. 2019b; Gong et al. 2018; You et al. 2018; Gong et al. 2017). For example, traditional two-stage classification is felicitous cast to a single teacher single student problem based on the idea of knowledge distillation (Zhou et al. 2019b). Furthermore, knowledge distillation can be flexibly deployed to various excellent learning schemes, such as the adversarial learning (Liu et al. 2018), auto machine learning (Macko et al. 2019; Fakoor et al. 2020), label noise filtering learning (Xia et al. 2018), lifelong learning (Zhai et al. 2019), and reinforcement learning (Ashok et al. 2018; Xu et al. 2020c; Zhao and Hospedales 2020). Therefore, it will be useful to integrate knowledge distillation with other learning schemes for practical challenges in the future.<div style="background-color: #d6d6d6;margin: 12px 0;">知识蒸馏的学习过程类似于人类的学习。将知识迁移推广到经典和传统的机器学习方法是可行的（Zhou et al. 2019b; Gong et al. 2018; You et al. 2018; Gong et al. 2017）。例如，传统的两阶段分类可以基于知识蒸馏的思想转化为一个单一教师单一学生问题（Zhou et al. 2019b）。此外，知识蒸馏可以灵活地部署到各种优秀的学习方案中，如对抗学习（Liu et al. 2018）、自动机器学习（Macko et al. 2019; Fakoor et al. 2020）、标签噪声过滤学习（Xia et al. 2018）、终身学习（Zhai et al. 2019）和强化学习（Ashok et al. 2018; Xu et al. 2020c; Zhao and Hospedales 2020）。因此，将知识蒸馏与其他学习方案相结合，对于未来实际挑战将是很有用的。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="891,1150">Acknowledgements The work was in part supported by Australian Research Council Projects FL-170100117, IH-180100002, IC-190100 031, and National Natural Science Foundation of China 61976107.<div style="background-color: #d6d6d6;margin: 12px 0;">致谢 本工作部分得到了澳大利亚研究理事会项目 FL-170100117、IH-180100002、IC-190100 031 以及中国国家自然科学基金 61976107 的支持。</div></div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><h2><div><div><div class="locator-translate" data-positiontag-23="886,1349">References<div style="background-color: #d6d6d6;margin: 12px 0;">参考文献</div></div></div></div></h2></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="894,1429">Aditya, S., Saha, R., Yang, Y., &amp; Baral, C. (2019). Spatial knowledge distillation to aid visual reasoning. In WACV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="893,1492">Aguilar, G., Ling, Y., Zhang, Y., Yao, B., Fan, X., &amp; Guo, E. (2020). Knowledge distillation from internal representations. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="889,1551">Aguinaldo, A., Chiang, P. Y., Gain, A., Patil, A., Pearson, K., &amp; Feizi, S. (2019). Compressing gans using knowledge distillation. arXiv preprint arXiv:1902.00159.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="893,1639">Ahn, S., Hu, S., Damianou, A., Lawrence, N. D., &amp; Dai, Z. (2019). Variational information distillation for knowledge transfer. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="891,1698">Albanie, S., Nagrani, A., Vedaldi, A., &amp; Zisserman, A. (2018). Emotion recognition in speech using cross-modal transfer in the wild. In ACM MM.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="893,1784">Allen-Zhu, Z., Li, Y., &amp; Liang, Y. (2019). Learning and generalization in overparameterized neural networks, going beyond two layers. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="894,1872">Anil, R., Pereyra, G., Passos, A., Ormandi, R., Dahl, G. E., &amp; Hinton, G. E. (2018). Large scale distributed neural network training through online distillation. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="891,1961">Arora, S., Cohen, N., &amp; Hazan, E. (2018). On the optimization of deep networks: Implicit acceleration by overparameterization. In ICML.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="24" id="mark-01913058-d55e-4608-a625-f311de5dd3cc" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-23="891,2021">Arora, S., Khapra, M. M., &amp; Ramaswamy, H. G. (2019). On knowledge distillation from complex networks for response prediction. In NAACL-HLT.</div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="142,168">Asami, T., Masumura, R., Yamaguchi, Y., Masataki, H., &amp; Aono, Y. (2017). Domain adaptation of dnn acoustic models using knowledge distillation. In ICASSP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="141,259">Ashok, A., Rhinehart, N., Beainy, F., &amp; Kitani, K. M. (2018). N2N learning: Network to network compression via policy gradient reinforcement learning. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="141,347">Asif, U., Tang, J. &amp; Harrer, S. (2020). Ensemble knowledge distillation for learning improved and efficient networks. In ECAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="141,406">Ba, J., &amp; Caruana, R. (2014). Do deep nets really need to be deep? In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="141,463">Bagherinezhad, H., Horton, M., Rastegari, M., &amp; Farhadi, A. (2018). Label refinery: Improving imagenet classification through label progression. arXiv preprint arXiv:1805.02641.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="140,553">Bai, H., Wu, J., King, I., &amp; Lyu, M. (2020). Few shot network compression via cross distillation. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="143,611">Bai, Y., Yi, J., Tao, J., Tian, Z., &amp; Wen, Z. (2019). Learn spelling from teachers: transferring knowledge from language models to sequence-to-sequence speech recognition. In Interspeech.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="142,700">Bashivan, P., Tensen, M., &amp; DiCarlo, J. J. (2019). Teacher guided architecture search. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="656" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>I</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="145,757">Belagiannis, V., Farshad, A., &amp; Galasso, F. (2018). Adversarial network compression. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="142,816">Bengio, Y., Courville, A., &amp; Vincent, P. (2013). Representation learning: A review and new perspectives. IEEE TPAMI, 35(8), 1798-1828.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="143,875">Bergmann, P., Fauser, M., Sattlegger, D., &amp; Steger, C. (2020). Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="142,963">Bhardwaj, S., Srinivasan, M., &amp; Khapra, M. M. (2019). Efficient video classification using fewer frames. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="143,1022">Bistritz, I., Mann, A., &amp; Bambos, N. (2020). Distributed Distillation for On-Device Learning. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="143,1080">Bohdal, O., Yang, Y., &amp; Hospedales, T. (2020). Flexible Dataset Distillation: Learn Labels Instead of Images. arXiv preprint arXiv:2006.08572.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="142,1168">Boo, Y., Shin, S., Choi, J., &amp; Sung, W. (2021). Stochastic precision ensemble: self-knowledge distillation for quantized deep neural networks. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="657" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>A</mi><mi>A</mi><mi>I</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="143,1256">Brutzkus, A., &amp; Globerson, A. (2019). Why do Larger Models Generalize Better? A Theoretical Perspective via the XOR Problem. In ICML.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="144,1343">Bucilua, C., Caruana, R. &amp; Niculescu-Mizil, A. (2006). Model compression. In SIGKDD.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="141,1403">Caccia, M., Rodriguez, P., Ostapenko, O., Normandin, F., Lin, M., Caccia, L., Laradji, I., Rish, I., Lacoste, A., Vazquez D., &amp; Charlin, L. (2020). Online Fast Adaptation and Knowledge Accumulation (OSAKA): a New Approach to Continual Learning. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="143,1521">Chan, W., Ke, N. R., &amp; Lane, I. (2015). Transferring knowledge from a RNN to a DNN. arXiv preprint arXiv:1504.01483.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="148,1579">Chawla, A., Yin, H., Molchanov, P., &amp; Alvarez, J. (2021). Data-Free Knowledge Distillation for Object Detection. In WACV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="151,1638">Chebotar, Y. &amp; Waters, A. (2016). Distilling knowledge from ensembles of neural networks for speech recognition. In Interspeech.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="143,1697">Chen, D., Mei, J. P., Wang, C., Feng, Y. &amp; Chen, C. (2020a). Online knowledge distillation with diverse peers. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="141,1756">Chen, D., Mei, J. P., Zhang, Y., Wang, C., Wang, Z., Feng, Y., &amp; Chen, C. (2021). Cross-layer distillation with semantic calibration. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="143,1842">Chen, G., Choi, W., Yu, X., Han, T., &amp; Chandraker, M. (2017). Learning efficient object detection models with knowledge distillation. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="141,1930">Chen, H., Wang, Y., Xu, C., Yang, Z., Liu, C., Shi, B., Xu, C., Xu, C., &amp;Tian, Q. (2019a). Data-free learning of student networks. In ICCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="143,2018">Chen, H., Wang, Y., Xu, C., Xu, C., &amp; Tao, D. (2021). Learning student networks via feature embedding. IEEE TNNLS, 32(1), 25-35.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="892,168">Chen, T., Goodfellow, I. &amp; Shlens, J. (2016). Net2net: Accelerating learning via knowledge transfer. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="893,230">Chen, W. C., Chang, C. C. &amp; Lee, C. R. (2018a). Knowledge distillation with feature maps for image classification. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="658" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="889,289">Chen, X., Zhang, Y., Xu, H., Qin, Z., &amp; Zha, H. (2018b). Adversarial distillation for efficient recommendation with external knowledge. ACM TOIS, 37(1), 1-28.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="892,376">Chen, X., Su, J., &amp; Zhang, J. (2019b). A two-teacher tramework for knowledge distillation. In ISNN.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="893,435">Chen, Y., Wang, N., &amp; Zhang, Z. (2018c). Darkrank: Accelerating deep metric learning via cross sample similarities transfer. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="896,494">Chen, Y. C., Gan, Z., Cheng, Y., Liu, J., &amp; Liu, J. (2020b). Distilling knowledge learned in BERT for text generation. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="659" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>C</mi><mi>L</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="891,553">Chen, Y. C., Lin, Y. Y., Yang, M. H., Huang, J. B. (2019c). Crdoco: Pixel-level domain transfer with cross-domain consistency. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="891,612">Chen, Z., &amp; Liu, B. (2018). Lifelong machine learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 12(3), <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="660" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c37"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mo>−</mo><mrow data-mjx-texclass="ORD"><mn>207</mn></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="893,698">Chen, Z., Zhu, L., Wan, L., Wang, S., Feng, W., &amp; Heng, P. A. (2020c). A multi-task mean teacher for semi-supervised shadow detection. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="661" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>C</mi><mi>V</mi><mi>P</mi><mi>R</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="890,786">Cheng, Y., Wang, D., Zhou, P., &amp; Zhang, T. (2018). Model compression and acceleration for deep neural networks: The principles, progress, and challenges. IEEE Signal Processing Magazine, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="662" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom><mjx-mrow space="2"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>35</mn></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mn>1</mn><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>126</mn></mrow><mo>−</mo><mrow data-mjx-texclass="ORD"><mn>136</mn></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="892,904">Cheng, X., Rao, Z., Chen, Y., &amp; Zhang, Q. (2020). Explaining knowledge distillation by quantifying the knowledge. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="891,964">Cho, J. H. &amp; Hariharan, B. (2019). On the efficacy of knowledge distillation. In ICCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="892,1021">Chollet, F. (2017). Xception: Deep learning with depthwise separable convolutions. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="891,1080">Chung, I., Park, S., Kim, J. &amp; Kwak, N. (2020). Feature-map-level online adversarial knowledge distillation. In ICML.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="891,1139">Clark, K., Luong, M. T., Khandelwal, U., Manning, C. D. &amp; Le, Q. V. (2019). Bam! born-again multi-task networks for natural language understanding. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="663" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>C</mi><mi>L</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="892,1227">Courbariaux, M., Bengio, Y. &amp; David, J. P. (2015). Binaryconnect: Training deep neural networks with binary weights during propagations. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="891,1315">Crowley, E. J., Gray, G. &amp; Storkey, A. J. (2018). Moonshine: Distilling with cheap convolutions. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="891,1373">Cui, J., Kingsbury, B., Ramabhadran, B., Saon, G., Sercu, T., Audhkhasi, K., et al. (2017). Knowledge distillation across ensembles of multilingual models for low-resource languages. In ICASSP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="893,1490">Cui, Z., Song, T., Wang, Y., &amp; Ji, Q. (2020). Knowledge augmented deep neural networks for joint facial expression and action unit recognition. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="893,1578">Cun, X., &amp; Pun, C. M. (2020). Defocus blur detection via depth distillation. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="893,1637">Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., &amp; Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="891,1697">Denton, E. L., Zaremba, W., Bruna, J., LeCun, Y. &amp; Fergus, R. (2014). Exploiting linear structure within convolutional networks for efficient evaluation. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="890,1784">Devlin, J., Chang, M. W., Lee, K. &amp; Toutanova, K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="899,1872">Ding, Q., Wu, S., Sun, H., Guo, J. &amp; Xia, S. T. (2019). Adaptive regularization of labels. arXiv preprint arXiv:1908.05474.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="893,1932">Do, T., Do, T. T., Tran, H., Tjiputra, E. &amp; Tran, Q. D. (2019). Compact trilinear interaction for visual question answering. In ICCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="25" id="mark-9a47ba09-698d-4c61-ac74-614b5948f7da" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-24="893,1991">Dong, X. &amp; Yang, Y. (2019). Teacher supervises students how to learn from partially labeled images for facial landmark detection. In ICCV.</div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="142,168">Dou, Q., Liu, Q., Heng, P. A., &amp; Glocker, B. (2020). Unpaired multimodal segmentation via knowledge distillation. IEEE TMI, 39(7), 2415-2425.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="143,258">Du, S., You, S., Li, X., Wu, J., Wang, F., Qian, C., &amp; Zhang, C. (2020). Agree to disagree: Adaptive ensemble knowledge distillation in gradient space. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="143,347">Duong, C. N., Luu, K., Quach, K. G. &amp; Le, N. (2019.) ShrinkTeaNet: Million-scale lightweight face recognition via shrinking teacher-student networks. arXiv preprint arXiv:1905.10620.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="142,435">Fakoor, R., Mueller, J. W., Erickson, N., Chaudhari, P., &amp; Smola, A. J. (2020). Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="142,523">Flennerhag, S., Moreno, P. G., Lawrence, N. D. &amp; Damianou, A. (2019). Transferring knowledge across learning processes. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="141,582">Freitag, M., Al-Onaizan, Y. &amp; Sankaran, B. (2017). Ensemble distillation for neural machine translation. arXiv preprint arXiv:1702.01802.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="141,669">Fu, H., Zhou, S., Yang, Q., Tang, J., Liu, G., Liu, K., &amp; Li, X. (2021). LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="141,758">Fukuda, T., Suzuki, M., Kurata, G., Thomas, S., Cui, J. &amp; Ramabhadran, B. (2017). Efficient knowledge distillation from an ensemble of teachers. In Interspeech.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="139,846">Furlanello, T., Lipton, Z., Tschannen, M., Itti, L. &amp; Anandkumar, A. (2018). Born again neural networks. In ICML.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="140,905">Gao, L., Mi, H., Zhu, B., Feng, D., Li, Y., &amp; Peng, Y. (2019). An adversarial feature distillation method for audio classification. IEEE Access, 7, 105319-105330.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="143,992">Gao, M., Wang, Y., &amp; Wan, L. (2021). Residual error based knowledge distillation. Neurocomputing, 433, 154-161.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="138,1052">Gao, Z., Chung, J., Abdelrazek, M., Leung, S., Hau, W. K., Xian, Z., et al. (2020). Privileged modality distillation for vessel border detection in intracoronary imaging. IEEE TMI, 39(5), 1524-1534.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="143,1139">Garcia, N. C., Morerio, P. &amp; Murino, V. (2018). Modality distillation with multiple stream networks for action recognition. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="140,1198">Ge, S., Zhao, S., Li, C., &amp; Li, J. (2018). Low-resolution face recognition in the wild via selective knowledge distillation. IEEE TIP, 28(4), 2051–2062.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="141,1284">Ge, S., Zhao, S., Li, C., Zhang, Y., &amp; Li, J. (2020). Efficient low-resolution face recognition via bridge distillation. IEEE TIP, 29, 6898–6908.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="141,1372">Ghorbani, S., Bulut, A. E. &amp; Hansen, J. H. (2018). Advancing multi-accented 1stm-ctc speech recognition using a domain specific student-teacher learning paradigm. In SLTW.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="141,1462">Gil, Y., Chai, Y., Gorodissky, O. &amp; Berant, J. (2019). White-to-black: Efficient distillation of black-box adversarial attacks. In NAACL-HLT.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="143,1549">Goldblum, M., Fowl, L., Feizi, S. &amp; Goldstein, T. (2020). Adversarially robust distillation. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="664" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>A</mi><mi>A</mi><mi>I</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="143,1608">Gong, C., Chang, X., Fang, M. &amp; Yang, J. (2018). Teaching semi-supervised classifier via generalized distillation. In IJCAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="141,1668">Gong, C., Tao, D., Liu, W., Liu, L., &amp; Yang, J. (2017). Label propagation via teaching-to-learn and learning-to-teach. TNNLS, 28(6), 1452- 1465.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="141,1755">Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014). Generative adversarial nets. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="144,1842">Gordon, M. A. &amp; Duh, K. (2019). Explaining sequence-level knowledge distillation as data-augmentation for neural machine translation. arXiv preprint arXiv:1912.03334.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="145,1932">Gu, J., &amp; Tresp, V. (2020). Search for better students to learn distilled knowledge. In ECAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="143,1989">Guan, Y., Zhao, P., Wang, B., Zhang, Y., Yao, C., Bian, K., &amp; Tang, J. (2020). Differentiable feature aggregation search for knowledge distillation. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="893,168">Guo, Q., Wang, X., Wu, Y., Yu, Z., Liang, D., Hu, X., &amp; Luo, P. (2020). Online knowledge distillation via collaborative learning. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="893,230">Gupta, S., Hoffman, J. &amp; Malik, J. (2016). Cross modal distillation for supervision transfer. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="889,288">Hahn, S. &amp; Choi, H. (2019). Self-knowledge distillation in natural language processing. In RANLP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="891,347">Haidar, M. A. &amp; Rezagholizadeh, M. (2019). Textkd-gan: Text generation using knowledge distillation and generative adversarial networks. In Canadian conference on artificial intelligence.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="890,435">Han, S., Pool, J., Tran, J. &amp; Dally, W. (2015). Learning both weights and connections for efficient neural network. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="893,493">Hao, W., &amp; Zhang, Z. (2019). Spatiotemporal distilled dense-connectivity network for video action recognition. Pattern Recognition, 92, 13-24.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="893,581">Haroush, M., Hubara, I., Hoffer, E., &amp; Soudry, D. (2020). The knowledge within: Methods for data-free model compression. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="892,641">He, C., Annavaram, M., &amp; Avestimehr, S. (2020a). Group knowledge transfer: Federated learning of large CNNs at the edge. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="891,699"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="665" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c48"></mjx-c><mjx-c class="mjx-c65"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c46"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">He</mi></mrow></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">F</mi></mrow></math></mjx-assistive-mml></mjx-container> .,Liu,T.,&amp;Tao,D. (2020b). Why resnet works? residuals generalize. IEEE TNNLS, 31(12), 5349-5362.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="893,758">He, K., Zhang, X., Ren, S. &amp; Sun, J. (2016) Deep residual learning for image recognition. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="894,816">He, T., Shen, C., Tian, Z., Gong, D., Sun, C. &amp; Yan, Y. (2019). Knowledge adaptation for efficient semantic segmentation. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="893,875">Heo, B., Kim, J., Yun, S., Park, H., Kwak, N., &amp; Choi, J. Y. (2019a). A comprehensive overhaul of feature distillation. In ICCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="888,934">Heo, B., Lee, M., Yun, S. &amp; Choi, J. Y. (2019b). Knowledge distillation with adversarial samples supporting decision boundary. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="887,993">Heo, B., Lee, M., Yun, S. &amp; Choi, J. Y. (2019c). Knowledge transfer via distillation of activation boundaries formed by hidden neurons. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="896,1079">Hinton, G., Vinyals, O. &amp; Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="895,1139">Hoffman, J., Gupta, S. &amp; Darrell, T. (2016). Learning with side information through modality hallucination. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="893,1198">Hong, W. &amp; Yu, J. (2019). Gan-knowledge distillation for one-stage object detection. arXiv preprint arXiv:1906.08467.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="892,1257">Hou, Y., Ma, Z., Liu, C. &amp; Loy, CC. (2019). Learning lightweight lane detection cnns by self attention distillation. In ICCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="889,1315">Hou, Y., Ma, Z., Liu, C., Hui, T. W., &amp; Loy, C. C. (2020). Inter-Region Affinity Distillation for Road Marking Segmentation. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="891,1374">Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., &amp; Adam, H. (2017). Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="892,1492"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="666" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c48"></mjx-c><mjx-c class="mjx-c75"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c48"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">Hu</mi></mrow></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">H</mi></mrow></math></mjx-assistive-mml></mjx-container> .,Xie,L.,Hong,R.,&amp;Tian,Q. (2020). Creating something from nothing: Unsupervised knowledge distillation for cross-modal hashing. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="892,1578"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="667" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c48"></mjx-c><mjx-c class="mjx-c75"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c4D"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">Hu</mi></mrow></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">M</mi></mrow></math></mjx-assistive-mml></mjx-container> .,Peng,Y.,Wei,F.,Huang,Z.,Li,D.,Yang,N.,et al. (2018). Attention-guided answer distillation for machine reading comprehension. In EMNLP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="893,1666">Huang, G., Liu, Z., Van, Der Maaten, L. &amp; Weinberger, K. Q. (2017). Densely connected convolutional networks. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="892,1726">Huang, M., You, Y., Chen, Z., Qian, Y. &amp; Yu, K. (2018). Knowledge distillation for sequence model. In Interspeech.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="889,1784">Huang, Z. &amp; Wang, N. (2017). Like what you like: Knowledge distill via neuron selectivity transfer. arXiv preprint arXiv:1707.01219.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="889,1844">Huang, Z., Zou, Y., Bhagavatula, V., &amp; Huang, D. (2020). Comprehensive attention self-distillation for weakly-supervised object detection. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="892,1932">Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="26" id="mark-c8592135-a425-48a7-afcf-1d29643fda47" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-25="889,1991">Jang, Y., Lee, H., Hwang, S. J. &amp; Shin, J. (2019). Learning what and where to transfer. In ICML.</div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="141,168">Ji, G., &amp; Zhu, Z. (2020). Knowledge distillation in wide neural networks: Risk bound, data efficiency and imperfect teacher. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="143,258">Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., et al. (2020). Tinybert: Distilling bert for natural language understanding. In EMNLP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="140,346">Jin, X., Peng, B., Wu, Y., Liu, Y., Liu, J., Liang, D., Yan, J., &amp; Hu, X. (2019). Knowledge distillation via route constrained optimization. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="668" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>I</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="143,434">Kang, M., Mun, J. &amp; Han, B. (2020). Towards oracle knowledge distillation with neural architecture search. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="142,494">Kim, J., Park, S. &amp; Kwak, N. (2018). Paraphrasing complex network: Network compression via factor transfer. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="141,552">Kim, J., Bhalgat, Y., Lee, J., Patel, C., &amp; Kwak, N. (2019a). QKD: Quantization-aware Knowledge Distillation. arXiv preprint arXiv:1911.12491.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="142,639">Kim, J., Hyun, M., Chung, I. &amp; Kwak, N. (2019b). Feature fusion for online mutual knowledge distillation. In ICPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="142,699">Kim, S. W. &amp; Kim, H. E. (2017). Transferring knowledge to smaller network with class-distance loss. In ICLRW.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="145,758">Kim, Y., Rush &amp; A. M. (2016). Sequence-level knowledge distillation. In EMNLP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="140,815">Kimura, A., Ghahramani, Z., Takeuchi, K., Iwata, T. &amp; Ueda, N. (2018). Few-shot learning of neural networks from scratch by pseudo example optimization. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="669" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>B</mi><mi>M</mi><mi>V</mi><mi>C</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="139,904">Kwon, K., Na, H., Lee, H., &amp; Kim, N. S. (2020). Adaptive knowledge distillation based on entropy. In ICASSP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="142,963">Kong, H., Zhao, J., Tu, X., Xing, J., Shen, S. &amp; Feng, J. (2019). Cross-resolution face recognition via prior-aided face hallucination and residual knowledge distillation. arXiv preprint arXiv:1905.10777.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="143,1052">Krizhevsky, A., &amp; Hinton, G. (2009). Learning multiple layers of features from tiny images.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="143,1109">Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="141,1169">Kuncoro, A., Ballesteros, M., Kong, L., Dyer, C. &amp; Smith, N. A. (2016). Distilling an ensemble of greedy dependency parsers into one mst parser. In EMNLP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="142,1256">Kundu, J. N., Lakkakula, N. &amp; Babu, R. V. (2019). Um-adapt: Unsupervised multi-task adaptation using adversarial cross-task distillation. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="143,1343">Lai, K. H., Zha, D., Li, Y., &amp; Hu, X. (2020). Dual policy distillation. In IJCAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="143,1402">Lan, X., Zhu, X., &amp; Gong, S. (2018). Self-referenced deep learning. In ACCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="140,1460">Lee, H., Hwang, S. J. &amp; Shin, J. (2019a). Rethinking data augmentation: Self-supervision and self-distillation. arXiv preprint arXiv:1910.05872.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="141,1549">Lee, K., Lee, K., Shin, J. &amp; Lee, H. (2019b). Overcoming catastrophic forgetting with unlabeled data in the wild. In ICCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="140,1609">Lee, K., Nguyen, L. T. &amp; Shim, B. (2019c). Stochasticity and skip connections improve knowledge transfer. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="148,1668">Lee, S. &amp; Song, B. (2019). Graph-based knowledge distillation by multihead attention network. In BMVC.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="140,1725">Lee, S. H., Kim, D. H. &amp; Song, B. C. (2018). Self-supervised knowledge distillation using singular value decomposition. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="140,1784">Li, B., Wang, Z., Liu, H., Du, Q., Xiao, T., Zhang, C., &amp; Zhu, J. (2021). Learning light-weight translation models from deep transformer. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="670" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>A</mi><mi>A</mi><mi>I</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="140,1872">Li, C., Peng, J., Yuan, L., Wang, G., Liang, X., Lin, L., &amp; Chang, X. (2020a). Blockwisely supervised neural architecture search with knowledge distillation. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="139,1961">Li, G., Zhang, J., Wang, Y., Liu, C., Tan, M., Lin, Y., Zhang, W., Feng, J., &amp; Zhang, T. (2020b). Residual distillation: Towards portable deep neural networks without shortcuts. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="890,168">Li, J., Fu, K., Zhao, S., &amp; Ge, S. (2019). Spatiotemporal knowledge distillation for efficient estimation of aerial video saliency. IEEE TIP, 29, 1902-1914.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="891,258">Li, M., Lin, J., Ding, Y., Liu, Z., Zhu, J. Y., &amp; Han, S. (2020c). Gan compression: Efficient architectures for interactive conditional gans. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="893,346">Li, Q., Jin, S. &amp; Yan, J. (2017). Mimicking very efficient network for object detection. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="890,405">Li, T., Li, J., Liu, Z., &amp; Zhang, C. (2020d). Few sample knowledge distillation for efficient network compression. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="891,465">Li, X., Wu, J., Fang, H., Liao, Y., Wang, F., &amp; Qian, C. (2020e). Local correlation consistency for knowledge distillation. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="896,523">Li, Z., &amp; Hoiem, D. (2017). Learning without forgetting. IEEE TPAMI, 40(12), 2935–2947.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="897,581">Lin, T., Kong, L., Stich, S. U., &amp; Jaggi, M. (2020). Ensemble distillation for robust model fusion in federated learning. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="890,640">Liu, I. J., Peng, J. &amp; Schwing, A. G. (2019a). Knowledge flow: Improve upon your teachers. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="891,699">Liu, J., Chen, Y. &amp; Liu, K. (2019b). Exploiting the ground-truth: An adversarial imitation based knowledge distillation approach for event detection. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="891,786">Liu, J., Wen, D., Gao, H., Tao, W., Chen, T. W., Osa, K., et al. (2019c). Knowledge representing: efficient, sparse representation of prior knowledge for knowledge distillation. In CVPRW.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="893,875">Liu, P., King, I., Lyu, M. R., &amp; Xu, J. (2019d). DDFlow: Learning optical flow with unlabeled data distillation. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="890,934">Liu, P., Liu, W., Ma, H., Mei, T. &amp; Seok, M. (2020a). Ktan: knowledge transfer adversarial network. In IJCNN.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="889,992">Liu, Q., Xie, L., Wang, H., Yuille &amp; A. L. (2019e). Semantic-aware knowledge preservation for zero-shot sketch-based image retrieval. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="671" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>I</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="893,1080">Liu, R., Fusi, N. &amp; Mackey, L. (2018). Model compression with generative adversarial networks. arXiv preprint arXiv:1812.02271.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="889,1139">Liu, W., Zhou, P., Zhao, Z., Wang, Z., Deng, H., &amp; Ju, Q. (2020b). FastBERT: a self-distilling BERT with adaptive inference time. In ACL.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="892,1226">Liu, X., Wang, X. &amp; Matwin, S. (2018b). Improving the interpretability of deep neural networks with knowledge distillation. In ICDMW.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="890,1286">Liu, X., He, P., Chen, W. &amp; Gao, J. (2019f). Improving multi-task deep neural networks via knowledge distillation for natural language understanding. arXiv preprint arXiv:1904.09482.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="892,1374">Liu, Y., Cao, J., Li, B., Yuan, C., Hu, W., Li, Y. &amp; Duan, Y. (2019g). Knowledge distillation via instance relationship graph. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="891,1433">Liu, Y., Chen, K., Liu, C., Qin, Z., Luo, Z. &amp; Wang, J. (2019h). Structured knowledge distillation for semantic segmentation. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="892,1492">Liu, Y., Jia, X., Tan, M., Vemulapalli, R., Zhu, Y., Green, B., et al. (2019i). Search to distill: Pearls are everywhere but not the eyes. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="893,1578">Liu, Y., Zhang, W., &amp; Wang, J. (2020c). Adaptive multi-teacher multilevel knowledge distillation. Neurocomputing, 415, 106-113.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="892,1638">Lopes, R. G., Fenu, S. &amp; Starner, T. (2017). Data-free knowledge distillation for deep neural networks. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="901,1697">Lopez-Paz, D., Bottou, L., Schölkopf, B. &amp; Vapnik, V. (2016). Unifying distillation and privileged information. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="891,1755">Lu, L., Guo, M. &amp; Renals, S. (2017). Knowledge distillation for small-footprint highway networks. In ICASSP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="893,1814">Luo, P., Zhu, Z., Liu, Z., Wang, X. &amp; Tang, X. (2016). Face model compression by distilling knowledge from neurons. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="891,1874">Luo, S., Pan, W., Wang, X., Wang, D., Tang, H., &amp; Song, M. (2020). Collaboration by competition: Self-coordinated knowledge amalgamation for multi-talent student learning. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="27" id="mark-f63b6f44-8485-4d71-8cef-1bfb8547462c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-26="889,1961">Luo, S., Wang, X., Fang, G., Hu, Y., Tao, D., &amp; Song, M. (2019). Knowledge amalgamation from heterogeneous networks by common feature learning. In IJCAI.</div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="141,168">Luo, Z., Hsieh, J. T., Jiang, L., Carlos Niebles, J.&amp; Fei-Fei, L. (2018). Graph distillation for action detection with privileged modalities. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="672" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>E</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="141,258">Macko, V., Weill, C., Mazzawi, H. &amp; Gonzalvo, J. (2019). Improving neural architecture search image classifiers via ensemble learning. In NeurIPS workshop.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="141,347">Ma, J., &amp; Mei, Q. (2019). Graph representation learning via multi-task knowledge distillation. arXiv preprint arXiv:1911.05700.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="139,406">Ma, N., Zhang, X., Zheng, H. T., &amp; Sun, J. (2018). Shufflenet v2: Practical guidelines for efficient CNN architecture design. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="143,465">Meng, Z., Li, J., Zhao, Y. &amp; Gong, Y. (2019). Conditional teacher-student learning. In ICASSP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="143,523">Micaelli, P. &amp; Storkey, A. J. (2019). Zero-shot knowledge transfer via adversarial belief matching. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="141,582">Minami, S., Hirakawa, T., Yamashita, T. &amp; Fujiyoshi, H. (2019). Knowledge transfer graph for deep collaborative learning. arXiv preprint arXiv:1909.04286.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="142,669">Mirzadeh, S. I., Farajtabar, M., Li, A. &amp; Ghasemzadeh, H. (2020). Improved knowledge distillation via teacher assistant. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="141,729">Mishra, A. &amp; Marr, D. (2018). Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="152,788">Mobahi, H., Farajtabar, M., &amp; Bartlett, P. L. (2020). Self-distillation amplifies regularization in hilbert space. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="155,846">Mou, L., Jia, R., Xu, Y., Li, G., Zhang, L. &amp; Jin, Z. (2016). Distilling word embeddings: An encoding approach. In CIKM.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="141,905">Mukherjee, P., Das, A., Bhunia, A. K. &amp; Roy, P. P. (2019). Cogni-net: Cognitive feature learning through deep visual perception. In ICIP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="142,964">Mullapudi, R. T., Chen, S., Zhang, K., Ramanan, D. &amp; Fatahalian, K. (2019). Online model distillation for efficient video inference. In ICCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="141,1050">Muller, R., Kornblith, S. &amp; Hinton, G. E. (2019). When does label smoothing help? In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="142,1109">Mun, J., Lee, K., Shin, J. &amp; Han, B. (2018). Learning to specialize with knowledge distillation for visual question answering. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="139,1169">Munjal, B., Galasso, F. &amp; Amin, S. (2019). Knowledge distillation for end-to-end person search. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="673" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>B</mi><mi>M</mi><mi>V</mi><mi>C</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="141,1227">Nakashole, N. &amp; Flauger, R. (2017). Knowledge distillation for bilingual dictionary induction. In EMNLP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="142,1286">Nayak, G. K., Mopuri, K. R., &amp; Chakraborty, A. (2021). Effectiveness of arbitrary transfer sets for data-free knowledge distillation. In WACV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="141,1372">Nayak, G. K., Mopuri, K. R., Shaj, V., Babu, R. V. &amp; Chakraborty, A. (2019). Zero-shot knowledge distillation in deep networks. In ICML.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="140,1460">Ng, R. W., Liu, X. &amp; Swietojanski, P. (2018). Teacher-student training for text-independent speaker recognition. In SLTW.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="143,1520">Nie, X., Li, Y., Luo, L., Zhang, N. &amp; Feng, J. (2019). Dynamic kernel distillation for efficient pose estimation in videos. In ICCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="145,1579">Noroozi, M., Vinjimoor, A., Favaro, P. &amp; Pirsiavash, H. (2018). Boosting self-supervised learning via knowledge transfer. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="141,1638">Nowak, T. S. &amp; Corso, J. J. (2018). Deep net triage: Analyzing the importance of network layers via structural compression. arXiv preprint arXiv:1801.04651.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="140,1725">Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., et al. (2018). Parallel wavenet: Fast high-fidelity speech synthesis. In ICML.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="143,1814">Pan, B., Cai, H., Huang, D. A., Lee, K. H., Gaidon, A., Adeli, E., &amp; Niebles, J. C. (2020). Spatio-temporal graph for video captioning with knowledge distillation. In CVPR</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="141,1902">Pan, Y., He, F., &amp; Yu, H. (2019). A novel enhanced collaborative autoen-coder with knowledge distillation for top-n recommender systems. Neurocomputing, 332, 137-148.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="142,1989">Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I. &amp; Talwar, K. (2017). Semi-supervised knowledge transfer for deep learning from private training data. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="890,168">Papernot, N., McDaniel, P., Wu, X., Jha, S. &amp; Swami, A. (2016). Distillation as a defense to adversarial perturbations against deep neural networks. In IEEE SP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="890,258">Park, S. &amp; Kwak, N. (2020). Feature-level ensemble knowledge distillation for aggregating knowledge from multiple networks. In ECAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="894,346">Park, W., Kim, D., Lu, Y. &amp; Cho, M. (2019). Relational knowledge distillation. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="893,405">Passban, P., Wu, Y., Rezagholizadeh, M., &amp; Liu, Q. (2021). ALP-KD: Attention-based layer projection for knowledge distillation. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="891,493">Passalis, N. &amp; Tefas, A. (2018). Learning deep representations with probabilistic knowledge transfer. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="891,552">Passalis, N., Tzelepi, M., &amp; Tefas, A. (2020a). Probabilistic knowledge transfer for lightweight deep representation learning. TNNLS. https://doi.org/10.1109/TNNLS.2020.2995884.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="892,640">Passalis, N., Tzelepi, M., &amp; Tefas, A. (2020b). Heterogeneous knowledge distillation using information flow modeling. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="892,699">Peng, B., Jin, X., Liu, J., Li, D., Wu, Y., Liu, Y., et al. (2019a). Correlation congruence for knowledge distillation. In ICCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="893,758">Peng, H., Du, H., Yu, H., Li, Q., Liao, J., &amp; Fu, J. (2020). Cream of the crop: Distilling prioritized paths for one-shot neural architecture search. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="891,845">Peng, Z., Li, Z., Zhang, J., Li, Y., Qi, G. J. &amp; Tang, J. (2019b). Few-shot image recognition with knowledge transfer. In ICCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="891,905">Perez, A., Sanguineti, V., Morerio, P. &amp; Murino, V. (2020). Audio-visual model distillation using acoustic images. In WACV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="892,963">Phuong, M., &amp; Lampert, C. H. (2019a). Towards understanding knowledge distillation. In ICML.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="891,1022">Phuong, M., &amp; Lampert, C. H. (2019b). Distillation-based training for multi-exit architectures. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="674" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>I</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="892,1080">Pilzer, A., Lathuiliere, S., Sebe, N. &amp; Ricci, E. (2019). Refine and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="888,1169">Polino, A., Pascanu, R. &amp; Alistarh, D. (2018). Model compression via distillation and quantization. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="889,1227">Price, R., Iso, K., &amp; Shinoda, K. (2016). Wise teachers train better DNN acoustic models. EURASIP Journal on Audio, Speech, and Music Processing, 2016(1), 10.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="890,1315">Radosavovic, I., Dollar, P., Girshick, R., Gkioxari, G., &amp; He, K. (2018). Data distillation: Towards omni-supervised learning. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="890,1374">Radosavovic, I., Kosaraju, R. P., Girshick, R., He, K., &amp; Dollar P. (2020). Designing network design spaces. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="891,1433">Roheda, S., Riggan, B. S., Krim, H. &amp; Dai, L. (2018). Cross-modality distillation: A case for conditional generative adversarial networks. In ICASSP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="892,1520">Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., &amp; Bengio, Y. (2015). Fitnets: Hints for thin deep nets. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="889,1579">Ross, A. S. &amp; Doshi-Velez, F. (2018). Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="675" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>A</mi><mi>A</mi><mi>I</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="893,1667">Ruder, S., Ghaffari, P. &amp; Breslin, J. G. (2017). Knowledge adaptation: Teaching to adapt. arXiv preprint arXiv:1702.02052.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="889,1726">Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., &amp; Chen, L. C. (2018). Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="893,1812">Sanh, V., Debut, L., Chaumond, J. &amp; Wolf, T. (2019). Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="891,1902">Saputra, M. R. U., de Gusmao, P. P., Almalioglu, Y., Markham, A. &amp; Trigoni, N. (2019). Distilling knowledge from a deep pose regressor network. In ICCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="28" id="mark-8a9783ed-e098-4826-ab7a-0dddbcd2573c" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-27="893,1989">Sau, B. B. &amp; Balasubramanian, V. N. (2016). Deep model compression: Distilling knowledge from noisy teachers. arXiv preprint arXiv:1610.09650.</div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="142,168">Seo, H., Park, J., Oh, S., Bennis, M., &amp; Kim, S. L. (2020). Federated Knowledge Distillation. arXiv preprint arXiv:2011.02367.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="144,230">Shakeri, S., Sethy, A. &amp; Cheng, C. (2019). Knowledge distillation in document retrieval. arXiv preprint arXiv:1911.11065.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="141,289">Shen, C., Wang, X., Song, J., Sun, L., &amp; Song, M. (2019a). Amalgamating knowledge towards comprehensive classification. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="140,347">Shen, C., Wang, X., Yin, Y., Song, J., Luo, S., &amp; Song, M. (2021). Progressive network grafting for few-shot knowledge distillation. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="676" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>A</mi><mi>A</mi><mi>I</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="141,434">Shen, C., Xue, M., Wang, X., Song, J., Sun, L., &amp; Song, M. (2019b). Customizing student networks from heterogeneous teachers via adaptive knowledge amalgamation. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="677" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>I</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="141,523">Shen, J., Vesdapunt, N., Boddeti, V. N. &amp; Kitani, K. M. (2016). In teacher we trust: Learning compressed models for pedestrian detection. arXiv preprint arXiv:1612.00478.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="139,611">Shen, P., Lu, X., Li, S. &amp; Kawai, H. (2018). Feature representation of short utterances based on knowledge distillation for spoken language identification. In Interspeech.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="141,699">Shen, P., Lu, X., Li, S., &amp; Kawai, H. (2020). Knowledge distillation-based representation learning for short-utterance spoken language identification. IEEE/ACM Transactions on Audio Speech and Language, 28, 2674-2683.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="142,816">Shen, P., Lu, X., Li, S. &amp; Kawai, H. (2019c). Interactive learning of teacher-student model for short utterance spoken language identification. In ICASSP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="142,903">Shen, Z., He, Z. &amp; Xue, X. (2019d). Meal: Multi-model ensemble via adversarial learning. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="141,963">Shi, B., Sun, M., Kao, C. C., Rozgic, V., Matsoukas, S. &amp; Wang, C. (2019a). Compression of acoustic event detection models with quantized distillation. In Interspeech.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="142,1052">Shi, B., Sun, M., Kao, CC., Rozgic, V., Matsoukas, S. &amp; Wang, C. (2019b). Semi-supervised acoustic event detection based on tri-training. In ICASSP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="143,1138">Shi, Y., Hwang, M. Y., Lei, X., &amp; Sheng, H. (2019c). Knowledge distillation for recurrent neural network language modeling with trust regularization. In ICASSP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="142,1227">Shin, S., Boo, Y. &amp; Sung, W. (2019). Empirical analysis of knowledge distillation technique for optimization of quantized deep neural networks. arXiv preprint arXiv:1909.01688.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="140,1315">Shmelkov, K., Schmid, C., &amp; Alahari, K. (2017). Incremental learning of object detectors without catastrophic forgetting. In ICCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="141,1374">Shu, C., Li, P., Xie, Y., Qu, Y., Dai, L., &amp; Ma, L.(2019). Knowledge squeezed adversarial network compression. arXiv preprint arXiv:1904.05100.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="140,1461">Siam, M., Jiang, C., Lu, S., Petrich, L., Gamal, M., Elhoseiny, M., et al. (2019). Video object segmentation using teacher-student adaptation in a human robot interaction (HRI) setting. In ICRA.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="141,1550">Sindhwani, V., Sainath, T. &amp; Kumar, S. (2015). Structured transforms for small-footprint deep learning. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="139,1609">Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., &amp; Dieleman, S. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), 484-489.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="140,1725">Song, X., Feng, F., Han, X., Yang, X., Liu, W. &amp; Nie, L. (2018). Neural compatibility modeling with attentive knowledge distillation. In SIGIR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="143,1812">Srinivas, S. &amp; Fleuret, F. (2018). Knowledge transfer with jacobian matching. In ICML.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="145,1872">Su, J. C. &amp; Maji, S. (2017). Adapting models to signal degradation using distillation. In BMVC.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="142,1932">Sun, L., Gou, J., Yu, B., Du, L., &amp; Tao, D. (2021) Collaborative teacher-student learning via multiple knowledge transfer. arXiv preprint arXiv:2101.08471.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="143,2019">Sun, S., Cheng, Y., Gan, Z. &amp; Liu, J. (2019). Patient knowledge distillation for bert model compression. In NEMNLP-IJCNLP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="891,168">Sun, P., Feng, W., Han, R., Yan, S., &amp; Wen, Y. (2019). Optimizing network performance for distributed dnn training on gpu clusters: Imagenet/alexnet training in 1.5 minutes. arXiv preprint arXiv:1902.06855.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="893,288">Takashima, R., Li, S. &amp; Kawai, H. (2018). An investigation of a knowledge distillation method for CTC acoustic models. In ICASSP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="892,347">Tan, H., Liu, X., Liu, M., Yin, B., &amp; Li, X. (2021). KT-GAN: Knowledge-transfer generative adversarial network for text-to-image synthesis. IEEE TIP, 30, 1275-1290.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="893,435">Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., &amp; Le, Q. V. (2019). Mnasnet: Platform-aware neural architecture search for mobile. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="890,523">Tan, M., &amp; Le, Q. (2019). EfficientNet: Rethinking model scaling for convolutional neural networks. In ICML.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="890,581">Tan, X., Ren, Y., He, D., Qin, T., Zhao, Z. &amp; Liu, T. Y. (2019). Multilingual neural machine translation with knowledge distillation. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="894,668">Tang, J., Shivanna, R., Zhao, Z., Lin, D., Singh, A., Chi, E. H., &amp; Jain, S. (2020). Understanding and improving knowledge distillation. arXiv preprint arXiv:2002.03532.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="896,758">Tang, J., &amp; Wang, K. (2018). Ranking distillation: Learning compact ranking models with high performance for recommender system. In SIGKDD.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="891,844">Tang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O. &amp; Lin, J. (2019). Distilling task-specific knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="893,934">Tarvainen, A., &amp; Valpola, H. (2017). Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="894,1022">Thoker, F. M. &amp; Gall, J. (2019). Cross-modal knowledge distillation for action recognition. In ICIP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="892,1080">Tian, Y., Krishnan, D. &amp; Isola, P. (2020). Contrastive representation distillation. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="891,1138">Tu, Z., He, F., &amp; Tao, D. (2020). Understanding generalization in recurrent neural networks. In International conference on learning representations. ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="893,1227">Tung, F., &amp; Mori, G. (2019). Similarity-preserving knowledge distillation. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="678" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>I</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="893,1285">Turc, I., Chang, M. W., Lee, K. &amp; Toutanova, K. (2019). Well-read students learn better: The impact of student initialization on knowledge distillation. arXiv preprint arXiv:1908.08962.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="888,1374">Urban, G., Geras, K. J., Kahou, S. E., Aslan, O., Wang, S., Caruana, R., (2017). Do deep convolutional nets really need to be deep and convolutional? In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="893,1461">Vapnik, V., &amp; Izmailov, R. (2015). Learning using privileged information: Similarity control and knowledge transfer. Journal of Machine Learning Research, 16(1), 2023-2049.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="893,1550">Vongkulbhisal, J., Vinayavekhin, P. &amp; Visentini-Scarzanella, M. (2019). Unifying heterogeneous classifiers with distillation. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="894,1609">Walawalkar, D., Shen, Z., &amp; Savvides, M. (2020). Online ensemble model compression using knowledge distillation. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="891,1668">Wang, C., Lan, X. &amp; Zhang, Y. (2017). Model distillation with knowledge transfer from face classification to alignment and verification. arXiv preprint arXiv:1709.02929.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="891,1755">Wang, L., &amp; Yoon, K. J. (2020). Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks. arXiv preprint arXiv:2004.05937.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="894,1842">Wang, H., Zhao, H., Li, X. &amp; Tan, X. (2018a). Progressive blockwise knowledge distillation for neural network acceleration. In IJCAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="893,1902">Wang, J., Bao, W., Sun, L., Zhu, X., Cao, B., &amp; Philip, S. Y. (2019a). Private model compression via knowledge distillation. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="29" id="mark-1498bd0f-dc1a-4d61-ad25-f879bac8a4ac" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-28="891,1961">Wang, J., Gou, L., Zhang, W., Yang, H., &amp; Shen, H. W. (2019b). Deep-vid: Deep visual interpretation and diagnosis for image classifiers via knowledge distillation. TVCG, 25(6), 2168-2180.</div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="143,168">Wang, M., Liu, R., Abe, N., Uchida, H., Matsunami, T., &amp; Yamada, S. (2018b). Discover the effective strategy for face recognition model compression by improved knowledge distillation. In ICIP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="141,259">Wang, M., Liu, R., Hajime, N., Narishige, A., Uchida, H. &amp; Matsunami, T.(2019c). Improved knowledge distillation for training fast low resolution face recognition model. In ICCVW.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="143,347">Wang, T., Yuan, L., Zhang, X. &amp; Feng, J. (2019d). Distilling object detectors with fine-grained feature imitation. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="140,406">Wang, T., Zhu, J. Y., Torralba, A., &amp; Efros, A. A. (2018c). Dataset distillation. arXiv preprint arXiv:1811.10959.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="142,465">Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., &amp; Zhou, M. (2020a). Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="143,552">Wang, W., Zhang, J., Zhang, H., Hwang, M. Y., Zong, C. &amp; Li, Z. (2018d). A teacher-student framework for maintainable dialog manager. In EMNLP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="143,640">Wang, X., Fu, T., Liao, S., Wang, S., Lei, Z., &amp; Mei, T. (2020b). Exclusivity-consistency regularized knowledge distillation for face recognition. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="142,728">Wang, X., Hu, J. F., Lai, J. H., Zhang, J. &amp; Zheng, W. S. (2019e). Progressive teacher-student learning for early action prediction. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="144,815">Wang, X., Zhang, R., Sun, Y. &amp; Qi, J. (2018e) Kdgan: Knowledge distillation with generative adversarial networks. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="141,875">Wang, Y., Xu, C., Xu, C., &amp; Tao, D. (2019f). Packing convolutional neural networks in the frequency domain. IEEE TPAMI, 41(10), 2495–2510.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="143,962">Wang, Y., Xu, C., Xu, C. &amp; Tao, D. (2018f). Adversarial learning of portable student networks. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="144,1022">Wang, Z. R., &amp; Du, J. (2021). Joint architecture and knowledge distillation in CNN for Chinese text recognition. Pattern Recognition, 111, 107722.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="141,1109">Watanabe, S., Hori, T., Le Roux, J. &amp; Hershey, J. R. (2017). Student-teacher network learning with enhanced features. In ICASSP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="139,1169">Wei, H. R., Huang, S., Wang, R., Dai, X. &amp; Chen, J. (2019). Online distilling from checkpoints for neural machine translation. In NAACL-HLT.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="142,1256">Wei, Y., Pan, X., Qin, H., Ouyang, W. &amp; Yan, J. (2018). Quantization mimic: Towards very tiny CNN for object detection. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="140,1315">Wong, J. H. &amp; Gales, M. (2016). Sequence student-teacher training of deep neural networks. In Interspeech.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="141,1374">Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., et al. (2019). Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="142,1461">Wu, A., Zheng, W. S., Guo, X. &amp; Lai, J. H. (2019a). Distilled person re-identification: Towards a more scalable system. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="146,1520">Wu, G., &amp; Gong, S. (2021). Peer collaborative learning for online knowledge distillation. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="141,1578">Wu, J., Leng, C., Wang, Y., Hu, Q. &amp; Cheng, J. (2016). Quantized convolutional neural networks for mobile devices. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="142,1638">Wu, M. C., Chiu, C. T. &amp; Wu, K. H. (2019b). Multi-teacher knowledge distillation for compressed video action recognition on deep neural networks. In ICASSP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="141,1725">Wu, X., He, R., Hu, Y., &amp; Sun, Z. (2020). Learning an evolutionary embedding via massive knowledge distillation. International Journal of Computer Vision, 1-18.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="141,1814">Xia, S., Wang, G., Chen, Z., &amp; Duan, Y. (2018). Complete random forest based class noise filtering learning for improving the generalizability of classifiers. IEEE TKDE, 31(11), 2063-2078.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="141,1902">Xie, J., Lin, S., Zhang, Y. &amp; Luo, L. (2019). Training convolutional neural networks with cheap convolutions and online distillation. arXiv preprint arXiv:1909.13063.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="141,1989">Xie, Q., Hovy, E., Luong, M. T., &amp; Le, Q. V. (2020). Self-training with Noisy Student improves ImageNet classification. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="142,2049">Xu, G., Liu, Z., Li, X., &amp; Loy, C. C. (2020a). Knowledge distillation meets self-supervision. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="892,168"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="679" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c58"></mjx-c><mjx-c class="mjx-c75"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c4B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c52"></mjx-c><mjx-c class="mjx-c75"></mjx-c><mjx-c class="mjx-c69"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c4C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c4C"></mjx-c><mjx-c class="mjx-c69"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c59"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-n" space="2"><mjx-c class="mjx-c26"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c47"></mjx-c><mjx-c class="mjx-c75"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c4C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">Xu</mi></mrow></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">K</mi></mrow><mo>.</mo><mo>,</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">Rui</mi></mrow></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">L</mi></mrow><mo>.</mo><mo>,</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">Li</mi></mrow></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Y</mi></mrow><mo>.</mo><mo>,</mo><mi mathvariant="normal">&amp;</mi><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">Gu</mi></mrow></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">L</mi></mrow></math></mjx-assistive-mml></mjx-container> . (2020b). Feature normalized knowledge distillation for image classification. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="893,229"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="680" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c58"></mjx-c><mjx-c class="mjx-c75"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c5A"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">Xu</mi></mrow></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Z</mi></mrow></math></mjx-assistive-mml></mjx-container> .,Wu,K.,Che,Z.,Tang,J.,&amp;Ye,J. (2020c). Knowledge transfer in multi-task deep reinforcement learning for continuous control. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="891,317"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="681" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c58"></mjx-c><mjx-c class="mjx-c75"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c5A"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">Xu</mi></mrow></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Z</mi></mrow></math></mjx-assistive-mml></mjx-container> .,Hsu,Y. C. &amp;Huang,J. (2018a). Training shallow and thin networks for acceleration via knowledge distillation with conditional adversarial networks. In ICLR workshop.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="893,406"><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="682" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c58"></mjx-c><mjx-c class="mjx-c75"></mjx-c></mjx-mi></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c5A"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi data-mjx-auto-op="false">Xu</mi></mrow></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">Z</mi></mrow></math></mjx-assistive-mml></mjx-container> .,Hsu,Y. C. &amp;Huang,J. (2018b). Training student networks for acceleration with conditional adversarial networks. In BMVC.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="889,464">Xu, T. B., &amp; Liu, C. L. (2019). Data-distortion guided self-distillation for deep neural networks. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="893,523">Yan, M., Zhao, M., Xu, Z., Zhang, Q., Wang, G. &amp; Su, Z. (2019). Vargfacenet: An efficient variable group convolutional neural network for lightweight face recognition. In ICCVW.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="896,611">Yang, C., Xie, L., Qiao, S. &amp; Yuille, A. (2019a). Knowledge distillation in generations: More tolerant teachers educate better students. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="894,698">Yang, C., Xie, L., Su, C. &amp; Yuille, A. L. (2019b). Snapshot distillation: Teacher-student optimization in one generation. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="893,758">Yang, J., Martinez, B., Bulat, A., &amp; Tzimiropoulos, G. (2020a). Knowledge distillation via adaptive instance normalization. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="891,817">Yang, Y., Qiu, J., Song, M., Tao, D. &amp; Wang, X. (2020b). Distilling knowledge from graph convolutional networks. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="892,875">Yang, Z., Shou, L., Gong, M., Lin, W. &amp; Jiang, D. (2020c). Model compression with two-stage multi-teacher knowledge distillation for web question answering system. In WSDM.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="890,963">Yao, A., &amp; Sun, D. (2020). Knowledge transfer via dense cross-layer mutual-distillation. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="893,1021">Yao, H., Zhang, C., Wei, Y., Jiang, M., Wang, S., Huang, J., Chawla, N. V., &amp; Li, Z. (2020). Graph few-shot learning via knowledge transfer. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="892,1109">Ye, J., Ji, Y., Wang, X., Gao, X., &amp; Song, M. (2020). Data-free knowledge amalgamation via group-stack dual-GAN. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="892,1169">Ye, J., Ji, Y., Wang, X., Ou, K., Tao, D. &amp; Song, M. (2019). Student becoming the master: Knowledge amalgamation for joint scene parsing, depth estimation, and more. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="890,1257">Yim, J., Joo, D., Bae, J. &amp; Kim, J. (2017). A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="893,1344">Yin, H., Molchanov, P., Alvarez, J. M., Li, Z., Mallya, A., Hoiem, D., Jha, Niraj K., &amp; Kautz, J. (2020). Dreaming to distill: Data-free knowledge transfer via DeepInversion. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="891,1433">Yoo, J., Cho, M., Kim, T., &amp; Kang, U. (2019). Knowledge extraction with no observable data. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="893,1491">You, S., Xu, C., Xu, C., &amp; Tao, D. (2017). Learning from multiple teacher networks. In SIGKDD.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="892,1549">You, S., Xu, C., Xu, C. &amp; Tao, D. (2018). Learning with single-teacher multi-student. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="893,1608">You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., et al. (2019). Large batch optimization for deep learning: Training bert in 76 minutes. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="893,1697">Yu, L., Yazici, V. O., Liu, X., Weijer, J., Cheng, Y. &amp; Ramisa, A. (2019). Learning metrics from teachers: Compact networks for image embedding. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="894,1784">Yu, X., Liu, T., Wang, X., &amp; Tao, D. (2017). On compressing deep models by low rank and sparse decomposition. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="889,1844">Yuan, F., Shou, L., Pei, J., Lin, W., Gong, M., Fu, Y., &amp; Jiang, D. (2021). Reinforced multi-teacher selection for knowledge distillation. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="894,1930">Yuan, L., Tay, F. E., Li, G., Wang, T. &amp; Feng, J. (2020). Revisit knowledge distillation: a teacher-free framework. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="893,1991">Yuan, M., &amp; Peng, Y. (2020). CKD: Cross-task knowledge distillation for text-to-image synthesis. IEEE TMM, 22(8), 1955-1968.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="30" id="mark-43d9db80-5d5f-4158-821e-6c344501e035" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-29="891,2049">Yue, K., Deng, J., &amp; Zhou, F. (2020). Matching guided distillation. In ECCV.</div></div></div></div></div></span></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="144,168">Yun, S., Park, J., Lee, K. &amp; Shin, J. (2020). Regularizing class-wise predictions via self-knowledge distillation. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="143,230">Zagoruyko, S. &amp; Komodakis, N. (2017). Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="141,317">Zhai, M., Chen, L., Tung, F., He, J., Nawhal, M. &amp; Mori, G. (2019). Lifelong gan: Continual learning for conditional image generation. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="683" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>I</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="140,404">Zhai, S., Cheng, Y., Zhang, Z. M. &amp; Lu, W. (2016). Doubly convolutional neural networks. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="140,464">Zhao, C., &amp; Hospedales, T. (2020). Robust domain randomised reinforcement learning through peer-to-peer distillation. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="141,523">Zhao, H., Sun, X., Dong, J., Chen, C., &amp; Dong, Z. (2020a). Highlight every step: Knowledge distillation via collaborative teaching. IEEE TCYB. https://doi.org/10.1109/TCYB.2020.3007506.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="139,611">Zhao, L., Peng, X., Chen, Y., Kapadia, M., &amp; Metaxas, D. N. (2020b). Knowledge as Priors: Cross-Modal Knowledge Generalization for Datasets without Superior Knowledge. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="140,699">Zhao, M., Li, T., Abu Alsheikh, M., Tian, Y., Zhao, H., Torralba, A. &amp; Katabi, D. (2018). Through-wall human pose estimation using radio signals. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="140,787">Zhang, C. &amp; Peng, Y. (2018). Better and faster: knowledge transfer from multiple self-supervised learning tasks via graph distillation for video classification. In IJCAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="142,875">Zhang, F., Zhu, X. &amp; Ye, M. (2019a). Fast human pose estimation. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="140,932">Zhang, J., Liu, T., &amp; Tao, D. (2018). An information-theoretic view for deep learning. arXiv preprint arXiv:1804.09060.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="142,993">Zhang, H., Hu, Z., Qin, W., Xu, M., &amp; Wang, M. (2021a). Adversarial co-distillation learning for image recognition. Pattern Recognition, 111, 107659.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="141,1080">Zhang, L., Shi, Y., Shi, Z., Ma, K., &amp; Bao, C. (2020a). Task-oriented feature distillation. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="141,1139">Zhang, L., Song, J., Gao, A., Chen, J., Bao, C. &amp; Ma, K. (2019b). Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="684" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>I</mi><mi>C</mi><mi>C</mi><mi>V</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="141,1227">Zhang, M., Song, G., Zhou, H., &amp; Liu, Y. (2020b). Discriminability distillation in group representation learning. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="141,1286">Zhang, S., Feng, Y., &amp; Li, L. (2021b). Future-guided incremental transformer for simultaneous translation. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="889,168">Zhang, S., Guo, S., Wang, L., Huang, W., &amp; Scott, M. R. (2020c). Knowledge integration networks for action recognition. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="893,230">Zhang, W., Miao, X., Shao, Y., Jiang, J., Chen, L., Ruas, O., &amp; Cui, B. (2020d). Reliable data distillation on graph convolutional network. In ACM SIGMOD.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="891,317">Zhang, X., Wang, X., Bian, J. W., Shen, C., &amp; You, M. (2021c). Diverse knowledge distillation for end-to-end person search. In AAAI.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="889,377">Zhang, X., Zhou, X., Lin, M. &amp; Sun, J. (2018a). Shufflenet: An extremely efficient convolutional neural network for mobile devices. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="892,463">Zhang, Y., Lan, Z., Dai, Y., Zeng, F., Bai, Y., Chang, J., &amp; Wei, Y. (2020e). Prime-aware adaptive distillation. In ECCV.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="890,523">Zhang, Y., Xiang, T., Hospedales, T. M. &amp; Lu, H. (2018b). Deep mutual learning. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="893,581">Zhang, Z., &amp; Sabuncu, M. R. (2020). Self-distillation as instance-specific label smoothing. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="889,641">Zhang, Z., Shi, Y., Yuan, C., Li, B., Wang, P., Hu, W., &amp; Zha, Z. J. (2020f). Object relational graph with teacher-recommended learning for video captioning. In CVPR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="891,729">Zhou C, Neubig G, Gu J (2019a) Understanding knowledge distillation in non-autoregressive machine translation. In ICLR.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="889,787">Zhou, G., Fan, Y., Cui, R., Bian, W., Zhu, X. &amp; Gai, K. (2018). Rocket launching: A universal and efficient framework for training well-performing light net. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="685" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>A</mi><mi>A</mi><mi>A</mi><mi>I</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="891,875">Zhou, J., Zeng, S. &amp; Zhang, B. (2019b) Two-stage image classification supervised by a single teacher single student model. In <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="686" style="font-size: 122.8%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D435 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi>B</mi><mi>M</mi><mi>V</mi><mi>C</mi></mrow></math></mjx-assistive-mml></mjx-container> .</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="889,934">Zhou, P., Mai, L., Zhang, J., Xu, N., Wu, Z. &amp; Davis, L. S. (2020). M2KD: Multi-model and multi-level knowledge distillation for incremental learning. In BMVC.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="891,1022">Zhu, M., Han, K., Zhang, C., Lin, J., &amp; Wang, Y. (2019). Low-resolution visual recognition via deep feature distillation. In ICASSP.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="888,1080">Zhu, X., &amp; Gong, S. (2018). Knowledge distillation by on-the-fly native ensemble. In NeurIPS.</div></div></div></div></div></span></div></div></div></div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="31" id="mark-45227be5-2a13-405a-80c9-d99426e4e4c4" class="markdown-parser-view mb-5 relative"><div><span style="display: block;"><div class="cursor-pointer"><div><div><div></div></div></div></div><div class="cursor-pointer"><div><div><div><div class="locator-translate" data-positiontag-30="892,1183">Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.<div style="background-color: #d6d6d6;margin: 12px 0;">出版商注记 Springer Nature 对出版地图上的领土主张和机构从属关系保持中立。</div></div></div></div></div></div></span></div></div></div></div></div>
      </body>
    </html>
  