
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px 350px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <h1>Deep Disentangled Representations for Volumetric Reconstruction</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>深度解耦表示用于体积重建</h1></div><p>Edward Grant \({}^{1\left( \boxtimes \right) }\) ,Pushmeet Kohli \({}^{2}\) ,and Marcel van Gerven \({}^{1}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>爱德华·格兰特 \({}^{1\left( \boxtimes \right) }\) ,普什米特·科利 \({}^{2}\) ,和马塞尔·范·赫尔文 \({}^{1}\)</p></div><p>\({}^{1}\) Radboud University,Nijmegen,The Netherlands</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{1}\) 荷兰奈梅亨拉德布德大学</p></div><p><a href="mailto:edward339@gmail.com">edward339@gmail.com</a>, <a href="mailto:m.vangerven@donders.ru.nl">m.vangerven@donders.ru.nl</a></p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p><a href="mailto:edward339@gmail.com">edward339@gmail.com</a>, <a href="mailto:m.vangerven@donders.ru.nl">m.vangerven@donders.ru.nl</a></p></div><p>\({}^{2}\) Microsoft Research,Cambridge,UK</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{2}\) 微软研究院, 剑桥, 英国</p></div><p><a href="mailto:pkohli@microsoft.com">pkohli@microsoft.com</a></p><p>Abstract. We introduce a convolutional neural network for inferring a compact disentangled graphical description of objects from 2D images that can be used for volumetric reconstruction. The network comprises an encoder and a twin-tailed decoder. The encoder generates a disentangled graphics code. The first decoder generates a volume, and the second decoder reconstructs the input image using a novel training regime that allows the graphics code to learn a separate representation of the 3D object and a description of its lighting and pose conditions. We demonstrate this method by generating volumes and disentangled graphical descriptions from images and videos of faces and chairs.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>摘要. 我们介绍了一种卷积神经网络，用于从2D图像推断出紧凑的解耦图形描述，这可以用于体积重建。该网络由编码器和双尾解码器组成。编码器生成解耦图形代码。第一个解码器生成体积，第二个解码器使用一种新颖的训练机制重建输入图像，使图形代码能够学习3D对象的单独表示以及其光照和姿态条件的描述。我们通过从人脸和椅子的图像和视频生成体积和解耦图形描述来演示该方法。</p></div><h2>1 Introduction</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>1 引言</h2></div><p>Images depicting natural objects are 2D representations of an underlying 3D structure from a specific viewpoint in specific lighting conditions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>描绘自然物体的图像是从特定视角和特定光照条件下的潜在3D结构的2D表示。</p></div><p>This work demonstrates a method for recovering the underlying 3D geometry of an object depicted in a single 2D image or video. To accomplish this we first encode the image as a separate description of the shape and transformation properties of the input such as lighting and pose. The shape description is used to generate a volumetric representation that is interpretable by modern rendering software.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>本工作展示了一种从单个2D图像或视频中恢复物体潜在3D几何形状的方法。为此，我们首先将图像编码为形状和变换属性的单独描述，例如光照和姿态。形状描述用于生成可由现代渲染软件解释的体积表示。</p></div><p>State of the art computer vision models perform recognition by learning hierarchical layers of feature detectors across overlapping sub-regions of the input space. Invariance to small transformations to the input is created by subsampling the image at various stages in the hierarchy.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最先进的计算机视觉模型通过学习输入空间重叠子区域的特征检测器的层次结构来执行识别。通过在层次结构的各个阶段对图像进行子采样，创建对输入的小变换的不变性。</p></div><p>In contrast, computer graphics models represent visual entities in a canonical form that is disentangled with respect to various realistic transformations in \(3\mathrm{D}\) , such as pose, scale and lighting conditions. 2D images can be rendered from the graphics code with the desired transformation properties.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>相比之下，计算机图形模型以规范形式表示视觉实体，该形式在各种现实变换（如姿态、尺度和光照条件）下是解耦的。可以从图形代码渲染出具有所需变换属性的2D图像。</p></div><p>A long standing hypothesis in computer vision is that vision is better accomplished by inferring such a disentangled graphical representation from 2D images. This process is known as 'de-rendering' and the field is known as 'vision as inverse graphics' [1].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>计算机视觉中的一个长期假设是，通过从2D图像推断出这样的解耦图形表示，视觉效果会更好。这个过程被称为“去渲染”，该领域被称为“视觉作为逆图形”[1]。</p></div><hr>
<!-- Footnote --><p>G. Hua and H. Jégou (Eds.): ECCV 2016 Workshops, Part III, LNCS 9915, pp. 266-279, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>G. Hua 和 H. Jégou (编): ECCV 2016 研讨会, 第三部分, LNCS 9915, 第266-279页, 2016。</p></div><!-- Footnote -->
<hr><p>One obstacle to realising this aim is that the de-rendering problem is ill-posed. The same 2D image can be rendered from a variety of 3D objects. This uncertainty means that there is normally no analytical solution to de-rendering. There are however, solutions that are more or less likely, given an object class or the class of all natural objects.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>实现这一目标的一个障碍是去渲染问题是病态的。同一2D图像可以由多种3D对象渲染。这种不确定性意味着通常没有去渲染的解析解。然而，给定一个对象类别或所有自然对象的类别，仍然存在或多或少可能的解决方案。</p></div><p>Recent work in the field of vision as inverse graphics has produced a number of convolutional neural network models that accomplish de-rendering [2-4]. Typically these models follow an encoding/decoding architecture. The encoder predicts a compact \(3\mathrm{D}\) graphical representation of the input. A control signal is applied corresponding with a known transformation to the input and a decoder renders the transformed image. We use a similar architecture. However, rather than rendering an image from the graphics code, we generate a full volumetric representation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在视觉作为逆图形领域的最新研究中，产生了一些实现去渲染的卷积神经网络模型[2-4]。通常这些模型遵循编码/解码架构。编码器预测输入的紧凑\(3\mathrm{D}\)图形表示。应用与已知变换相对应的控制信号，解码器渲染变换后的图像。我们使用类似的架构。然而，与其从图形代码渲染图像，我们生成完整的体积表示。</p></div><p>Unlike the disentangled graphics code generated by existing models, which is only renderable using a custom trained decoder, the volumetric representation generated by our model is easily converted to a polygon mesh or other professional quality 3D graphical format. This allows the object to be rendered at any scale and with other rendering techniques available in modern rendering software.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>与现有模型生成的解耦图形代码不同，该代码只能使用自定义训练的解码器进行渲染，而我们模型生成的体积表示可以轻松转换为多边形网格或其他专业质量的3D图形格式。这使得对象可以以任何比例渲染，并使用现代渲染软件中可用的其他渲染技术。</p></div><h2>2 Related Work</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>2 相关工作</h2></div><p>Several models have been developed that generate an disentangled representation given a 2D input, and output a new image subject to a transformation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>已经开发了几种模型，这些模型在给定2D输入的情况下生成解耦表示，并输出受变换影响的新图像。</p></div><p>Kulkarni et al. proposed the Deep Convolutional Inverse Graphics Network (DC-IGN) trained using Stochastic Gradient Variational Bayes [2]. This model encodes a factored latent representation of the input that is disentangled with respect to changes in azimuth, elevation and light source. A decoder renders the graphics code subject to the desired transformation as a \(2\mathrm{D}\) image. Training is performed with batches in which only a single transformation or the shape of the object are different. The activations of the graphics code layer chosen to represent the static parameters are clamped as the mean of the activations for that batch on the forward pass. On the backward pass the gradients for the corresponding nodes are set to their difference from this mean. The method is demonstrated by generating chairs and face images transformed with respect to azimuth, elevation and light source.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Kulkarni等人提出了深度卷积逆图形网络（DC-IGN），该网络使用随机梯度变分贝叶斯进行训练[2]。该模型对输入进行编码，生成一个与方位角、高度和光源变化解耦的因子潜在表示。解码器根据所需变换渲染图形代码，生成\(2\mathrm{D}\)图像。训练是在仅有单一变换或物体形状不同的批次中进行的。选择用于表示静态参数的图形代码层的激活值在前向传播时被固定为该批次激活值的均值。在反向传播中，相应节点的梯度被设置为与该均值的差值。该方法通过生成与方位角、高度和光源变化的椅子和面部图像来进行演示。</p></div><p>Tatarchenko et al. proposed a similar model that is trained in a fully supervised manner [3]. The encoder takes a 2D image as input and generates a graphics code representing a canonical 3D object form. A signal is added to the code corresponding with a known transformation in \(3\mathrm{D}\) and the decoder renders a new image corresponding with that transformation. This method is also demonstrated by generating rotated images of cars and chairs.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Tatarchenko等人提出了一个类似的模型，该模型以完全监督的方式进行训练[3]。编码器以2D图像作为输入，生成表示规范3D物体形状的图形代码。一个信号被添加到与已知变换对应的代码中，解码器渲染出与该变换对应的新图像。该方法同样通过生成汽车和椅子的旋转图像进行演示。</p></div><p>Yang et al. demonstrated an encoder/decoder model similar to the above but utilize a recurrent structure to account for long-term dependencies in a sequence of transformations, allowing for realistic re-rendering of real face images from different azimuth angles [4].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Yang等人展示了一个与上述类似的编码器/解码器模型，但利用递归结构来考虑一系列变换中的长期依赖关系，从而允许从不同方位角真实地重新渲染真实面部图像[4]。</p></div><p>Spatial Transformer Networks (STN) allow for the spatial manipulation of images and data within a convolutional neural network [5]. The STN first generates a transformation matrix given an input, creates a grid of sampling points based on the transformation and outputs samples from the grid. The module is trained using back-propagation and transforms the input with an input dependent affine transformation. Since the output sample can be of arbitrary size, these modules have been used as an efficient down-sampling method in classification networks. STNs transform existing data by sampling but they are not generative, so cannot make predictions about occluded data, which is necessary when predicting 3D structure.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>空间变换网络（STN）允许在卷积神经网络中对图像和数据进行空间操作[5]。STN首先根据输入生成变换矩阵，基于变换创建采样点网格，并从网格中输出样本。该模块使用反向传播进行训练，并通过输入依赖的仿射变换来变换输入。由于输出样本可以是任意大小，这些模块已被用作分类网络中的高效下采样方法。STN通过采样变换现有数据，但它们不是生成性的，因此无法对被遮挡数据进行预测，而这在预测3D结构时是必要的。</p></div><p>Girdhar et al. and Rezende et al. present methods for volumetric reconstructing from 2D images but do not generate disentangled representations \(\left\lbrack  {6,7}\right\rbrack\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Girdhar等人和Rezende等人提出了从2D图像进行体积重建的方法，但未生成解耦表示\(\left\lbrack  {6,7}\right\rbrack\)。</p></div><p>The contribution of this work is an encoding/decoding model that generates a compact graphics code from 2D images and videos that is disentangled with respect to shape and the transformation parameters of the input, and that can also be used for volumetric reconstruction. To our knowledge this is the first work that generates a disentanlged graphical representation that can be used to reconstruct volumes from 2D images. In addition, we show that Spatial Transformer Networks can be used to replace max-pooling in the encoder as an efficient sampling method. We demonstrate this approach by generating a compact disentangled graphical representation from single 2D images and videos of faces and chairs in a variety of viewpoint and lighting conditions. This code is used to generate volumetric representations which are rendered from a variety of viewpoints to show their 3D structure.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>本工作的贡献是一个编码/解码模型，该模型从2D图像和视频中生成一个与输入的形状和变换参数解耦的紧凑图形代码，并且还可以用于体积重建。据我们所知，这是第一个生成可用于从2D图像重建体积的解耦图形表示的工作。此外，我们展示了空间变换网络可以用作编码器中的高效采样方法，替代最大池化。我们通过从各种视角和光照条件下的面部和椅子的单个2D图像和视频生成紧凑的解耦图形表示来演示这种方法。该代码用于生成体积表示，这些表示从不同视角渲染，以展示其3D结构。</p></div><h2>3 Model</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>3 模型</h2></div><h3>3.1 Architecture</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.1 架构</h3></div><p>As shown in Fig. 1, the network has one encoder, a graphics code layer and two decoders. The graphics code layer is separated into a shape code and a transformation code. The encoder takes as input an \({80} \times  {80}\) pixel color image and generates the graphics code following a series of convolutions, point-wise randomized rectified linear units (RReLU) [8], down-sampling Spatial Transformer Networks and max pooling. Batch normalization layers are used after each convolutional layer to speed up training and avoid problems with exploding and vanishing gradients [9].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>如图1所示，网络有一个编码器、一个图形代码层和两个解码器。图形代码层分为形状代码和变换代码。编码器以\({80} \times  {80}\)像素颜色图像作为输入，并通过一系列卷积、逐点随机修正线性单元（RReLU）[8]、下采样空间变换网络和最大池化生成图形代码。每个卷积层后使用批量归一化层，以加速训练并避免梯度爆炸和消失的问题[9]。</p></div><p>The two decoders are connected to the graphics code by switches so that the message from the graphics code is passed to either one of the decoders. The first decoder is the volume decoder. The volume decoder takes the shape code as input and generates an \({80} \times  {80} \times  {80}\) voxel volumetric prediction of the encoded shape. This is accomplished by a series of volumetric convolutions, point-wise RReLU and volumetric up-sampling. A parametric rectified linear unit (PReLU) [10] is substituted for the RReLU in the output layer. This is done to avoid the saturation problems with rectified linear units early in training but allows for learning an activation threshold later in training, corresponding with the positive-valued output targets.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>两个解码器通过开关与图形代码相连，以便将图形代码中的信息传递给任一解码器。第一个解码器是体积解码器。体积解码器以形状代码作为输入，生成编码形状的\({80} \times  {80} \times  {80}\)体素体积预测。这是通过一系列体积卷积、逐点RReLU和体积上采样实现的。在输出层中，参数化修正线性单元（PReLU）[10]替代了RReLU。这是为了避免在训练早期使用修正线性单元时出现的饱和问题，但允许在训练后期学习激活阈值，以对应于正值输出目标。</p></div><!-- Media --><!-- figureText: Encoder Decoders 40x 40x e,f,h 3(1) e,f,h Output 40x f,i 82 B 80x g,a 3 [1] g,a 4 Output 80x Og Og Og 44 17 44 17 80x 5 80x Graphics Code \( a,h,b,c \) Input 4I (Shape) a,h,d,c 5 [1] 80 Switches CT 17 A 46 (100) CT Graphics Code a. Spatial convolution (Pose, Lighting) Batch normalization d. Spatial max pooling (2x2) e. Volumetric upsampling (2x2 nearest) Volumetric convolution |. Spatial upsampling (2x2 nearest) h. RReLU i. PReLU --><img src="https://cdn.noedgeai.com/bo_d164l2bef24c73d1lk00_3.jpg?x=191&#x26;y=182&#x26;w=1197&#x26;h=899&#x26;r=0"><p>Fig. 1. Network architecture: The network consists of an encoder (A), a volume decoder (B) and an image decoder (C). The encoder takes as input a 2D image and generates a 3D graphics code through a series of spatial convolutions, down-sampling Spatial Transformer Networks and max pooling layers. This code is split into a shape code and a transformation code. The volume decoder takes the shape code as input and generates a prediction of the volumetric contents of the input. The image decoder takes the shape code and the transformation code as input and reconstructs the input image.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图1. 网络架构：该网络由编码器（A）、体积解码器（B）和图像解码器（C）组成。编码器以2D图像为输入，通过一系列空间卷积、下采样空间变换网络和最大池化层生成3D图形代码。该代码被分为形状代码和变换代码。体积解码器以形状代码为输入，生成输入的体积内容预测。图像解码器以形状代码和变换代码为输入，重建输入图像。</p></div><!-- Media --><p>The second decoder reconstructs the input image with the correct pose and lighting, showing that pose and lighting parameters of the input are contained in the graphics code. The image decoder takes as input both the shape code and the transformation code, and generates a reconstruction of the original input image. This is accomplished by a series of spatial convolutions, point-wise RReLU, spatial up-sampling and point-wise PReLU in the final layer. During training, the backward pass from the image decoder to the shape code is blocked (see Fig. 2). This encourages the shape code to only represent shape, as it only receives an error signal from the volume decoder.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>第二个解码器以正确的姿态和光照重建输入图像，表明输入的姿态和光照参数包含在图形代码中。图像解码器同时以形状代码和变换代码为输入，生成原始输入图像的重建。这是通过一系列空间卷积、逐点RReLU、空间上采样和最后一层的逐点PReLU实现的。在训练过程中，图像解码器到形状代码的反向传播被阻断（见图2）。这促使形状代码仅表示形状，因为它只接收来自体积解码器的错误信号。</p></div><p>The volume decoder only requires knowledge about the shape of the input since it generates binary volumes that are invariant to pose and lighting. However, the image decoder must generate a reconstruction of the original image which is not invariant to shape, pose or lighting. Both decoders have access to the shape code but only the image decoder has access to the transformation code. This encourages the network to learn a graphics code that is disentangled with respect to shape and transformations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>体积解码器只需要了解输入的形状，因为它生成对姿态和光照不变的二进制体积。然而，图像解码器必须生成原始图像的重建，而这对形状、姿态或光照并不不变。两个解码器都可以访问形状代码，但只有图像解码器可以访问变换代码。这促使网络学习一个与形状和变换解耦的图形代码。</p></div><!-- Media --><!-- figureText: Forward Backward Z1 Z2 D1 D2 E Z1 Z2 D1 D2 --><img src="https://cdn.noedgeai.com/bo_d164l2bef24c73d1lk00_4.jpg?x=526&#x26;y=172&#x26;w=426&#x26;h=379&#x26;r=0"><p>Fig. 2. Network training: In the forward pass the shape code (Z1) and the transformation code (Z2) receive a signal from the encoder (E). The volume decoder (D1) receives input only from the shape code. The image decoder (D2) receives input from the shape code and the transformation code. On the backward pass the signal from the image decoder to the shape code is suppressed to force it to only represent shape.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2. 网络训练：在前向传播中，形状代码（Z1）和变换代码（Z2）接收来自编码器（E）的信号。体积解码器（D1）仅接收来自形状代码的输入。图像解码器（D2）接收来自形状代码和变换代码的输入。在反向传播中，来自图像解码器到形状代码的信号被抑制，以迫使其仅表示形状。</p></div><!-- Media --><p>The network can be trained differently depending on whether pose and lighting conditions need to be encoded. If the only objective is to generate volumes from the input then the image decoder can be switched off during training. In this case the graphics code will learn to be invariant to viewpoint and lighting. If the volume decoder and image decoder are both used during training the graphics code learns a disentangled representation of shape and transformations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>网络的训练方式可以根据是否需要编码姿态和光照条件而有所不同。如果唯一的目标是从输入生成体积，则在训练期间可以关闭图像解码器。在这种情况下，图形代码将学习对视点和光照不变。如果在训练期间同时使用体积解码器和图像解码器，则图形代码学习到形状和变换的解耦表示。</p></div><h3>3.2 Spatial Transformer Networks</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.2 空间变换网络</h3></div><p>Spatial Transformer Networks (STNs) perform input dependent geometric transformations on images or sets of feature maps [5]. There are two STNs in our model (see Fig. 1).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>空间变换网络（STNs）对图像或特征图集执行依赖于输入的几何变换[5]。我们的模型中有两个STN（见图1）。</p></div><p>Each STN comprises a localisation network, a grid generator and sampling grid. The localisation network takes the activations of the previous layer as input and regresses the parameters of an affine transformation matrix. The grid generator generates a sampling grid of(x,y)coordinates corresponding with the desired height and width of the output. The sampling grid is obtained by multiplying the generated grid with the transformation matrix. In our model this takes the form:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>每个STN由一个定位网络、一个网格生成器和采样网格组成。定位网络以前一层的激活值为输入，回归仿射变换矩阵的参数。网格生成器生成与输出所需高度和宽度相对应的(x,y)坐标的采样网格。采样网格通过将生成的网格与变换矩阵相乘获得。在我们的模型中，这呈现为：</p></div><p></p>\[\left( \begin{array}{l} {x}_{i}^{s} \\  {y}_{i}^{s} \end{array}\right)  = {\mathcal{T}}_{\theta }\left( {G}_{i}\right)  = \left\lbrack  \begin{array}{lll} {\theta }_{11} &#x26; {\theta }_{12} &#x26; {\theta }_{13} \\  {\theta }_{21} &#x26; {\theta }_{22} &#x26; {\theta }_{23} \end{array}\right\rbrack  \left( \begin{matrix} {x}_{i}^{t} \\  {y}_{i}^{t} \\  1 \end{matrix}\right)  \tag{1}\]<p></p><p>where \(\left( {{x}_{i}^{t},{y}_{i}^{t}}\right)\) are the generated grid coordinates and \(\left( {{x}_{i}^{s},{y}_{i}^{s}}\right)\) define the sample points. The transformation matrix \({\mathcal{T}}_{\theta }\) allows for cropping,scale,translation, scale, rotation and skew. Cropping and scale, in particular allow the STN to focus on the most important region in a feature map.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(\left( {{x}_{i}^{t},{y}_{i}^{t}}\right)\)是生成的网格坐标，\(\left( {{x}_{i}^{s},{y}_{i}^{s}}\right)\)定义了采样点。变换矩阵\({\mathcal{T}}_{\theta }\)允许裁剪、缩放、平移、旋转和倾斜。裁剪和缩放，特别是允许STN关注特征图中最重要的区域。</p></div><p>STNs have been shown to improve performance in convolutional network classifiers by modelling attention and transforming feature maps. Our model uses STNs in a generative setting to perform efficient down-sampling and assist the network in learning invariance to pose and lighting.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>STN已被证明通过建模注意力和变换特征图来提高卷积网络分类器的性能。我们的模型在生成设置中使用STN以执行高效的下采样，并帮助网络学习对姿态和光照的不变性。</p></div><p>The first STN in our model is positioned after the first convolutional layer. It uses a convolutional neural network to regress the transformation coefficients. This localisation network consists of four \(5 \times  5\) convolutional layers,each followed by batch normalization and the first three also followed by \(2 \times  2\) max pooling.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们模型中的第一个STN位于第一个卷积层之后。它使用卷积神经网络回归变换系数。该定位网络由四个\(5 \times  5\)卷积层组成，每个卷积层后面都跟有批量归一化，前三个卷积层后面还跟有\(2 \times  2\)最大池化。</p></div><p>The second STN in our model is positioned after the second convolutional layer and regresses the transformation parameters with a convolutional network consisting of two \(5 \times  5\) an one \(6 \times  6\) convolutional layers each followed by batch normalization and the last two also by \(2 \times  2\max\) pooling.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们模型中的第二个STN位于第二个卷积层之后，并使用由两个\(5 \times  5\)和一个\(6 \times  6\)卷积层组成的卷积网络回归变换参数，每个卷积层后面都跟有批量归一化，最后两个卷积层后面也跟有\(2 \times  2\max\)池化。</p></div><h3>3.3 Data</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.3 数据</h3></div><p>The model was trained using 16,000 image-volume pairs generated from the Basel Face Model [11]. Images of size \({80} \times  {80}\) were rendered in RGB from five different azimuth angles and three ambient lighting settings. Volumes of size \({80} \times  {80} \times  {80}\) were created by discretizing the triangular mesh generated by the Basel Face Model.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>该模型使用从巴塞尔面部模型（Basel Face Model）生成的16,000个图像-体积对进行训练[11]。图像大小为\({80} \times  {80}\)，从五个不同的方位角和三个环境光照设置中以RGB格式渲染。体积大小为\({80} \times  {80} \times  {80}\)，通过离散化巴塞尔面部模型生成的三角网格创建。</p></div><h2>4 Experimental Results</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>4 实验结果</h2></div><h3>4.1 Training</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.1 训练</h3></div><p>We evaluated the model’s volume prediction capacity by training it on 16,000 image-volume pairs. Each example pair was shown to the network only once to discourage memorization of the training data.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们通过在16,000个图像-体积对上训练模型来评估其体积预测能力。每个示例对仅向网络展示一次，以防止对训练数据的记忆。</p></div><p>Training was performed using the Torch framework on a single NVIDIA Tesla K80 GPU. Batches of size 10 were given as input to the encoder and forward propagated through the network. The mean-squared error of the predicted and target volumes was calculated and back-propagated using the Adam learning algorithm [12]. The initial learning rate was set to 0.001 .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>训练是在单个NVIDIA Tesla K80 GPU上使用Torch框架进行的。批次大小为10的输入被提供给编码器，并在网络中向前传播。预测体积与目标体积的均方误差被计算并使用Adam学习算法[12]进行反向传播。初始学习率设置为0.001。</p></div><h3>4.2 Volume Predictions from Images of Faces</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.2 从人脸图像生成体积预测</h3></div><p>In this experiment we used the network to generate volumes from a single 2D images. The network was presented with unseen face images as input and generated 3D volume predictions. The image decoder was not used in this experiment.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本实验中，我们使用网络从单个2D图像生成体积。网络以未见过的人脸图像作为输入，生成3D体积预测。此实验中未使用图像解码器。</p></div><p>The predicted volumes were binarized with a threshold of 0.01 . A triangular mesh was generated from the coordinates of active voxels using Delaunay triangulation. The patch was smoothed and the resulting image rendered using OpenGL and Matlab's trimesh function.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>预测的体积通过0.01的阈值进行二值化。使用德劳内三角剖分从活动体素的坐标生成三角网格。该补丁被平滑处理，结果图像使用OpenGL和Matlab的trimesh函数进行渲染。</p></div><!-- Media --><!-- figureText: (a) (c) (b) --><img src="https://cdn.noedgeai.com/bo_d164l2bef24c73d1lk00_6.jpg?x=180&#x26;y=173&#x26;w=1123&#x26;h=1095&#x26;r=0"><p>Fig. 3. Generated volumes: Qualitative results showing the volume predicting capacity of the network on unseen data. (a) First column: network inputs. Columns 2-4 (white): network predictions shown from three viewpoints. Columns 5-7 (black): ground truth from the same viewpoints. Column 8: nearest neighbour image. Columns 9-11 (blue): nearest neighbour image ground truth. (b) Each column is an input/output pair. The inputs are in the first row. Each input is the same face viewed from a different position. The generated volumes in the second row are shown from the same viewpoint for comparison. (c) Each column is an input/output pair. The inputs are in the first row. Each input is the same face in different lighting conditions. (Color figure online)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3. 生成的体积：定性结果显示网络在未见数据上的体积预测能力。(a) 第一列：网络输入。第2-4列（白色）：从三个视点显示的网络预测。第5-7列（黑色）：来自相同视点的真实值。第8列：最近邻图像。第9-11列（蓝色）：最近邻图像的真实值。(b) 每列是一个输入/输出对。输入在第一行。每个输入是从不同位置观察的相同面孔。第二行中生成的体积从相同视点显示以便比较。(c) 每列是一个输入/输出对。输入在第一行。每个输入是在不同光照条件下的相同面孔。(在线彩色图)</p></div><!-- Media --><p>Figure 3(a) shows the input image, network predictions, ground truth, nearest neighour in the input space and the ground truth of the nearest neighour. The nearest neighbour was determined by searching the training images for the image with the smallest pixel-wise distance to the input. The generated volumes are visibly different depending on the shape of the input.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3(a)显示了输入图像、网络预测、真实值、输入空间中的最近邻和最近邻的真实值。最近邻是通过在训练图像中搜索与输入图像像素级距离最小的图像来确定的。生成的体积在形状上明显不同，取决于输入的形状。</p></div><p>Figure 3(b) shows the network output for the same input presented from different viewpoints. The images in the first row are the inputs to the network and the second row contains the volumes generated from each input. These are shown from the same viewpoint for comparison. The generated volumes are visually very similar, showing that the network generated volumes that are invariant to the pose of the input.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3(b)显示了相同输入在不同视点下的网络输出。第一行的图像是网络的输入，第二行包含从每个输入生成的体积。这些体积从相同视点显示以便比较。生成的体积在视觉上非常相似，表明网络生成的体积对输入的姿态是不变的。</p></div><p>Figure 3(c) shows the network output for the same face presented in different lighting conditions. The first row images are the inputs and the second row are the generated volumes also shown from the same viewpoint for comparison. These volumes are also visually very similar to each other showing that the network output appears invariant to lighting conditions in the input.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3(c)显示了相同面孔在不同光照条件下的网络输出。第一行的图像是输入，第二行是生成的体积，也从相同视点显示以便比较。这些体积在视觉上也非常相似，表明网络输出似乎对输入的光照条件是不变的。</p></div><h3>4.3 Nearest Neighbour Comparison</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.3 最近邻比较</h3></div><p>The network's quantitative performance was benchmarked using a nearest neighbour test. A test set of 200 image/volume pairs was generated using the Basel Face Model (ground truth). The nearest neighbour to each test image in the training set was identified by searching for the training set image with the smallest pixel-wise Euclidean distance to the test set image (nearest neighbour). The network generated a volume for each test set input (prediction).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>网络的定量性能通过最近邻测试进行基准测试。使用巴塞尔面部模型（真实值）生成了200个图像/体积对的测试集。通过搜索训练集中与测试集图像像素级欧几里得距离最小的训练集图像（最近邻）来识别每个测试图像的最近邻。网络为每个测试集输入生成了一个体积（预测）。</p></div><p>Nearest neighbour error was determined by measuring the mean voxel-wise Euclidean distance between the ground truth and nearest neighbour volumes. Prediction error was determined by measuring the mean voxel-wise Euclidean distance between the ground truth volumes and the predicted volumes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最近邻误差通过测量真实值和最近邻体积之间的均体素欧几里得距离来确定。预测误差通过测量真实值体积和预测体积之间的均体素欧几里得距离来确定。</p></div><p>A paired-samples t-test was conducted to compare error score in predicted and nearest neighbour volumes. There was a significant difference in the error score for predictions \(\left( {M = {0.0096},{SD} = {0.0013}}\right)\) and nearest neighbours \((M =\) \({0.017},{SD} = {0.0038})\) conditions; \(t\left( {199}\right)  =  - {21.5945},p = {4.7022e} - {54}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>进行了配对样本t检验，以比较预测体积和最近邻体积的误差分数。在预测\(\left( {M = {0.0096},{SD} = {0.0013}}\right)\)和最近邻\((M =\) \({0.017},{SD} = {0.0038})\)条件下，误差分数存在显著差异；\(t\left( {199}\right)  =  - {21.5945},p = {4.7022e} - {54}\)。</p></div><p>These results show that network is better at predicting volumes than using the nearest neighbour.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这些结果表明，网络在预测体积方面优于使用最近邻。</p></div><h3>4.4 Internal Representations</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.4 内部表示</h3></div><p>In this experiment we tested the ability of the encoder to generate a graphics code that can be used to generate a volume that is invariant to pose and lighting. Since the volume encoder doesn't need pose and lighting information we didn't use the image decoder in this experiment.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在这个实验中，我们测试了编码器生成图形代码的能力，该代码可以用于生成对姿态和光照不变的体积。由于体积编码器不需要姿态和光照信息，因此我们在这个实验中没有使用图像解码器。</p></div><p>To test the invariance of the encoder with respect to pose, lighting and shape we re-trained the model without using batch normalization. Three sets of 100 image batches were prepared where two of these parameters were clamped and the target parameter was different. This makes it possible to measure the variance of activations for changes in pose, lighting and shape. The set-wise mean of the mean variance of activations in each batch was compared for all layers in the network.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了测试编码器对姿态、光照和形状的不变性，我们在不使用批量归一化的情况下重新训练了模型。准备了三组100个图像批次，其中两个参数被固定，目标参数不同。这使得可以测量姿态、光照和形状变化的激活方差。比较了网络中所有层的每个批次的激活均值的均值方差的组均值。</p></div><p>Figure 4(a) shows that the network's heightened sensitivity to shape relative to pose and lighting begins in the second convolutional layer. There is a sharp increase in sensitivity to shape in the graphics code, which is much more sensitive to shape than pose or lighting, and more sensitive to pose than lighting. This relative invariance to pose and lighting is retained in the volume decoder.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图4(a)显示，网络对形状的敏感性相对于姿态和光照在第二个卷积层开始增强。图形代码对形状的敏感性急剧增加，远高于对姿态或光照的敏感性，并且对姿态的敏感性高于光照。这种对姿态和光照的相对不变性在体积解码器中得以保留。</p></div><p>Figure 4(b) shows a visual representation of the activations for the same face with different poses. The effect of the first STN can be seen in the second convolutional layer activations which are visibly warped. The difference in the warp depending on the pose of the face suggests that the STNs may be helping to create invariance to pose later in the network. The example input images have a light source which is directed from the left of the camera. The second convolutional layer activations show a dark area on the right side of each face which is less evident in the first convolutional layer, suggesting that shadowing is an important feature for predicting the \(3\mathrm{D}\) shape of the face.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图4(b)显示了同一张面孔在不同姿态下的激活的视觉表示。可以在第二个卷积层的激活中看到第一个STN的效果，这些激活明显变形。面孔姿态不同导致的变形差异表明，STN可能有助于在网络后期创建对姿态的不变性。示例输入图像的光源来自相机的左侧。第二个卷积层的激活在每张面孔的右侧显示出一个阴暗区域，而在第一个卷积层中不太明显，这表明阴影是预测\(3\mathrm{D}\)面孔形状的重要特征。</p></div><!-- Media --><!-- figureText: 0.8 Volume (b) Relative Standard Deviation of Activations 0.7 Lighting Pose 0.6 community 0.5 0.4 continuously 0.3 0.2 Image E1 E3 D1 D2 D3 (a) --><img src="https://cdn.noedgeai.com/bo_d164l2bef24c73d1lk00_8.jpg?x=141&#x26;y=181&#x26;w=1201&#x26;h=497&#x26;r=0"><p>Fig. 4. Invariance to pose and lighting: (a) The relative mean standard deviation (SD) of activations in each network layer is compared for changes in shape, pose and lighting. Image is the input image, E1-E3 are the convolutional encoder layers, Z is the graphics code, D1-D3 are the convolutional decoder layers and Volume is the generated volume. In the input, changes to pose account for the highest SD. By the second convolutional layer the network is more sensitive to changes in shape than pose or lighting. The graphics code is much more sensitive to shape than pose or lighting. (b) The first row is five images of the same face from different viewpoints. Rows 2-4 show sampled encoder activations for the input image at the top of each column. The last row shows sampled graphics code activations reshaped into a square.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图4. 对姿态和光照的不变性：(a) 比较了每个网络层中激活的相对均值标准差(SD)在形状、姿态和光照变化下的变化。图像是输入图像，E1-E3是卷积编码器层，Z是图形代码，D1-D3是卷积解码器层，Volume是生成的体积。在输入中，姿态变化占据了最高的SD。在第二个卷积层，网络对形状变化的敏感性高于姿态或光照。图形代码对形状的敏感性远高于姿态或光照。(b) 第一行是同一张面孔从不同视角拍摄的五张图像。第2-4行显示了每列顶部输入图像的编码器激活的采样。最后一行显示了重塑为正方形的图形代码激活的采样。</p></div><!-- Media --><h3>4.5 Disentangled Representations</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.5 解耦表示</h3></div><p>In this experiment we tested the network’s ability to generate a compact \(3\mathrm{D}\) description of the input that is disentangled with respect to the shape of the object and transformations such as pose and lighting.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在这个实验中，我们测试了网络生成与物体形状和姿态、光照等变换解耦的紧凑\(3\mathrm{D}\)描述的能力。</p></div><p>In order to generate this description we used the same network as in the volume generation experiment but with an additional fully connected RReLU layer of size3,000in the encoder to compensate for the increased difficulty of the task.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了生成这个描述，我们使用了与体积生成实验相同的网络，但在编码器中增加了一个大小为3000的全连接RReLU层，以补偿任务难度的增加。</p></div><p>During training, images were given as input to the encoder which generated an activity vector of 200 scalar values. These were divided in the shape code comprising 185 values and the transformation code comprising 15 values. The network was trained on 16,000 image/volumes pairs with batches of size 10 .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在训练期间，图像作为输入提供给编码器，生成一个包含200个标量值的活动向量。这些值被分为形状代码（包含185个值）和变换代码（包含15个值）。网络在16000对图像/体积的基础上进行训练，批次大小为10。</p></div><p>The switches connecting the encoder to the decoders were adjusted after every three training batches to allow the volume decoder and the image decoder to see the same number of examples. The volume decoder only received the shape code, whereas the image decoder received both the shape code and the transformation code.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>连接编码器和解码器的开关在每三个训练批次后进行调整，以使体积解码器和图像解码器看到相同数量的示例。体积解码器仅接收形状代码，而图像解码器接收形状代码和变换代码。</p></div><p>To test if the shape code and the transformation code learned the desired invariance we measured the mean standard deviation of activations for batches where only one of shape, pose or lighting conditions were changed. The same batches as in the invariance experiment were used.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了测试形状代码和变换代码是否学习到了所需的不变性，我们测量了仅改变形状、姿态或光照条件的批次的激活均值标准差。使用了与不变性实验相同的批次。</p></div><p>Figure 5(a) shows the relative mean standard deviation of activations of each layer in the encoder, graphics code and image decoder. The bifurcation at point \(\mathrm{Z}\) on the plot shows that the two codes learned to respond differently to the same input. The shape code learned to be more sensitive to changes in shape than pose or lighting, and the transformation code learned to be more sensitive to changes in pose and lighting than shape.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图5(a)显示了编码器、图形代码和图像解码器中每层激活的相对均值标准差。图中点\(\mathrm{Z}\)的分叉显示这两个代码对相同输入的响应不同。形状代码对形状变化的敏感性高于姿态或光照，而变换代码对姿态和光照变化的敏感性高于形状。</p></div><p>To make sure the image decoder used the shape code to reconstruct the input we compared the output of the image decoder with input only from the shape code, the transformation code and both together. Figure 5(b) shows the output of the volume decoder and image decoder on a number of unseen images. The first column shows the input to the network. The second column shows the output of the image decoder with input only from the shape code. The third column shows the same for the output of the transformation code. The fourth column shows the combined output of the shape code and the transformation code. The fifth column shows the output of the volume decoder.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了确保图像解码器使用形状编码重建输入，我们将图像解码器的输出与仅来自形状编码、变换编码和两者结合的输入进行了比较。图5(b)显示了体积解码器和图像解码器在若干未见图像上的输出。第一列显示了网络的输入。第二列显示了仅来自形状编码的图像解码器的输出。第三列显示了变换编码的输出。第四列显示了形状编码和变换编码的组合输出。第五列显示了体积解码器的输出。</p></div><!-- Media --><!-- figureText: Relative Standard Deviation of Activations 0.7 Shape Transformation Code Image (b) ... Lighting Shape Code tron Pose 0.6 continu 0.4 0.3 0.1 Image E1 E2 E3 E4 Z D1 D2 D3 (a) --><img src="https://cdn.noedgeai.com/bo_d164l2bef24c73d1lk00_9.jpg?x=195&#x26;y=1182&#x26;w=1188&#x26;h=492&#x26;r=0"><p>Fig. 5. Disentangled representations: (a) The relative mean standard deviation (SD) of activations in the encoder, shape code, transformation code and image decoder is compared for changes in shape, pose and lighting. The shape code is most sensitive to changes in shape. The transformation code is most sensitive to changes in pose and lighting. Error bars show standard deviation. (b) The output of the volume decoder and image decoder on a number of unseen images. The first column is the input image. The second column is the image decoded from the shape code only. The third column is the image decoded from the transformation code only. The fourth column is the image decoded from the shape code and the transformation code. The fifth column is the output of the volume decoder shown from the same viewpoint for comparison.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图5. 解耦表示: (a) 编码器、形状编码、变换编码和图像解码器在形状、姿态和光照变化下的相对均值标准差(SD)进行比较。形状编码对形状变化最敏感。变换编码对姿态和光照变化最敏感。误差条表示标准差。 (b) 体积解码器和图像解码器在若干未见图像上的输出。第一列是输入图像。第二列是仅从形状编码解码的图像。第三列是仅从变换编码解码的图像。第四列是从形状编码和变换编码解码的图像。第五列是体积解码器的输出，从相同视角显示以便比较。</p></div><!-- Media --><h3>4.6 Face Recognition in Novel Pose and Lighting Conditions</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.6 新姿态和光照条件下的人脸识别</h3></div><p>To measure the invariance and representational quality of the shape code we tested it on a face recognition task.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了测量形状编码的不变性和表示质量，我们在一个人脸识别任务上进行了测试。</p></div><p>The point-wise Euclidean distance between the shape code generated by an image was measured for a batch of 150 random images including one image that was the same face with a different pose (target). The random images were ordered from the smallest to greatest distance and the rank of the target was recorded. This was repeated 100 times and an identical experiment was performed for pose. The mean rank for the same face with a different pose was 11.08. The mean rank of the same face with different lighting was 1.02 . This demonstrates that the shape code can be used as a pose and lighting invariant face classifier.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们测量了由图像生成的形状编码之间的逐点欧几里得距离，测试了一批150张随机图像，其中包括一张相同面孔但姿态不同的图像（目标）。随机图像按距离从小到大排序，并记录目标的排名。这一过程重复了100次，并对姿态进行了相同的实验。相同面孔在不同姿态下的平均排名为11.08。相同面孔在不同光照下的平均排名为1.02。这表明形状编码可以用作姿态和光照不变的人脸分类器。</p></div><p>To test if the shape code was more invariant to pose and lighting than the full graphics code we repeated this experiment using the full graphics code. The mean rank for the same face with a different pose was 26.86. The mean rank of the same face with different lighting was 1.14. This shows that the shape code was relatively more invariant to pose and lighting than the full graphics code.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了测试形状编码是否比完整图形编码对姿态和光照更不变，我们使用完整图形编码重复了这个实验。相同面孔在不同姿态下的平均排名为26.86。相同面孔在不同光照下的平均排名为1.14。这表明形状编码对姿态和光照的相对不变性高于完整图形编码。</p></div><h3>4.7 Volume Predictions from Videos of Faces</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.7 从人脸视频中预测体积</h3></div><p>To test if video input improved the quality of the generated volumes we adapted the encoder to take video as input and compared to a single image baseline. 10,000 video/volume pairs of faces were created. Each video consisted of five RGB frames of a face rotating from left facing profile to right facing profile in equidistant degrees of rotation. The same network architecture was used as in experiment 4.5. For the video model the first layer was adapted to take the whole video as input. For the single image baseline model, single images from each video were used as input.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了测试视频输入是否提高了生成体积的质量，我们调整了编码器以接受视频作为输入，并与单张图像基线进行了比较。创建了10,000对人脸视频/体积。每个视频由五帧RGB图像组成，展示了一个面孔从左侧侧面到右侧侧面的旋转，旋转角度均匀分布。使用了与实验4.5相同的网络架构。对于视频模型，第一层被调整为接受整个视频作为输入。对于单张图像基线模型，使用每个视频中的单张图像作为输入。</p></div><p>To test the performance difference between video and single image inputs a test set of 500 video/volume pairs was generated. Error was measured using the mean voxel-wise distance between ground truth and volumes generated by the network. For the video network the entire video was used as input. For the single image baseline each frame of the video was given separately as input to the network and the generated volume with the lowest error was used as the benchmark.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了测试视频和单张图像输入之间的性能差异，生成了一组500对视频/体积的测试集。误差通过生成的体积与真实值之间的平均体素距离来测量。对于视频网络，整个视频作为输入。对于单张图像基线，每帧视频单独作为输入提供给网络，生成误差最低的体积作为基准。</p></div><p>A paired-samples t-test was conducted to compare error score in volumes generated from volumes and single images. There was a significant difference in the error score for video based volume predictions \(\left( {M = {0.0073},{SD} = {0.0009}}\right)\) and single image based predictions \(\left( {M = {0.0089},{SD} = {0.0014}}\right)\) conditions; \(t\left( {199}\right)  =  - {13.7522},{1.0947e} - {30}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>进行了配对样本t检验，以比较从体积和单张图像生成的体积的误差得分。在基于视频的体积预测\(\left( {M = {0.0073},{SD} = {0.0009}}\right)\)和基于单张图像的预测\(\left( {M = {0.0089},{SD} = {0.0014}}\right)\)条件下，误差得分存在显著差异；\(t\left( {199}\right)  =  - {13.7522},{1.0947e} - {30}\)。</p></div><p>These results show that video input results in superior volume reconstruction performance compared with single images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这些结果表明，与单张图像相比，视频输入在体积重建性能上表现更优。</p></div><h3>4.8 Volume Predictions from Images of Chairs</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.8 从椅子图像中预测体积</h3></div><p>In this experiment we tested the capacity of the network to generate volume predictions from objects with more variable geometry. 5000 Volume/image pairs of chairs were created from the ModelNet dataset [13]. The images were \({80} \times  {80}\) RGB images and the volumes were \({30} \times  {30} \times  {30}\) binary volumes. The predicted volumes were binarized with a threshold of 0.2 . Both decoders were used in this experiment. The shape code consisted of 599 activations and the transformation code consisted of one activation. The shape code was used to reconstruct the volumes. Both the shape code and transformation code were used to reconstruct the input.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在这个实验中，我们测试了网络从几何形状更为多变的物体生成体积预测的能力。我们从ModelNet数据集中创建了5000对椅子的体积/图像。图像是\({80} \times  {80}\) RGB图像，体积是\({30} \times  {30} \times  {30}\) 二进制体积。预测的体积经过0.2的阈值二值化。两个解码器在这个实验中都被使用。形状编码由599个激活组成，变换编码由一个激活组成。形状编码用于重建体积。形状编码和变换编码都用于重建输入。</p></div><p>Figure 6 demonstrates the network's capacity to generate volumetric predictions of chairs from novel images.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6展示了网络从新图像生成椅子的体积预测的能力。</p></div><!-- Media --><img src="https://cdn.noedgeai.com/bo_d164l2bef24c73d1lk00_11.jpg?x=216&#x26;y=626&#x26;w=1154&#x26;h=953&#x26;r=0"><p>Fig. 6. Generated chair volumes: Qualitative results showing the volume predicting capacity of the network on unseen data. First column: network inputs. Columns 2-4 (Yellow): network predictions shown from three viewpoints. Columns 5-7 (black): ground truth from the same viewpoints. Column 8: nearest neighbour image in the training set. Columns 9-11 (blue): nearest neighbour image ground truth. (Color figure online)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6. 生成的椅子体积：定性结果显示网络在未见数据上的体积预测能力。第一列：网络输入。第2-4列（黄色）：从三个视角展示的网络预测。第5-7列（黑色）：来自相同视角的真实值。第8列：训练集中最近邻图像。第9-11列（蓝色）：最近邻图像的真实值。（在线彩色图）</p></div><!-- Media --><h3>4.9 Interpolating the Graphics Code</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.9 插值图形编码</h3></div><p>In order to qualitatively demonstrate that the graphics code in experiment 4.8 was disentangled with respect to shape and pose, we swapped the shape code and transformation code of a number of images and generated new images from the interpolated code using the image decoder. Figure 7 shows the output of the image decoder using the interpolated code. The shape of the chairs in the generated images is most similar to the shape of the chairs in the images used to generate the shape code. The pose of each chair is most similar to the pose of the chairs in the images used to generate the transformation code. This demonstrates that the graphics code is disentangled with respect to shape and pose.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了定性展示实验4.8中的图形编码在形状和姿态方面是解耦的，我们交换了多个图像的形状编码和变换编码，并使用图像解码器从插值编码生成新图像。图7展示了使用插值编码的图像解码器的输出。生成图像中椅子的形状与用于生成形状编码的图像中的椅子形状最为相似。每把椅子的姿态与用于生成变换编码的图像中的椅子姿态最为相似。这表明图形编码在形状和姿态方面是解耦的。</p></div><!-- Media --><img src="https://cdn.noedgeai.com/bo_d164l2bef24c73d1lk00_12.jpg?x=221&#x26;y=508&#x26;w=1050&#x26;h=406&#x26;r=0"><p>Fig. 7. Interpolated code: Qualitative results combining the shape code and transformation code from different images. First row: images used to generate the shape code. Second row: images used to generate the transformation code. Last row: Image decoder output.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图7. 插值编码：定性结果结合了来自不同图像的形状编码和变换编码。第一行：用于生成形状编码的图像。第二行：用于生成变换编码的图像。最后一行：图像解码器输出。</p></div><!-- Media --><h2>5 Discussion</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>5 讨论</h2></div><p>We have shown that a convolutional neural network can learn to generate a compact graphical representation that is disentangled with respect to shape, and transformations such as lighting and pose. This representation can be used to generate a full volumetric prediction of the contents of the input image.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们已经展示了卷积神经网络可以学习生成一个与形状和光照、姿态等变换解耦的紧凑图形表示。这个表示可以用来生成输入图像内容的完整体积预测。</p></div><p>By comparing the activations of batches corresponding with a specific transformation or the shape of the image, we showed that the network can learn to represent a shape code that is relatively invariant to pose and lighting conditions. By adding an additional decoder to the network that reconstructs the input image, the network can learn to represent a transformation code that represents the pose and lighting conditions of the input.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>通过比较与特定变换或图像形状对应的批次激活，我们展示了网络可以学习表示一个对姿态和光照条件相对不变的形状编码。通过向网络添加一个额外的解码器来重建输入图像，网络可以学习表示输入的姿态和光照条件的变换编码。</p></div><p>Extending the approach to real world scenes requires consideration of the viewpoint of the generated volume. Although the volume is invariant in the sense that it contains all the information necessary to render the generated object from any viewpoint, a canonical viewpoint was used for all volumes so that they were generated from a frontal perspective. Natural scenes do not always have a canonical viewpoint for reference. One possible solution is to generate a volume from the same viewpoint as the input. Experiments show that this approach is promising but further work is needed.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>将该方法扩展到现实场景需要考虑生成体积的视角。尽管体积在某种意义上是不变的，因为它包含从任何视角渲染生成物体所需的所有信息，但所有体积都使用了一个典型视角，以便从正面视角生成。自然场景并不总是有一个典型的参考视角。一个可能的解决方案是从与输入相同的视角生成体积。实验表明这种方法是有前景的，但仍需进一步研究。</p></div><p>In order to learn, the network requires image-volume pairs. This limits the type of data that can be used as volumetric datasets of sufficient size, or models that generate them are limited in number. A promising avenue for future work is incorporating a professional quality renderer into the decoder structure. This theoretically allows for \(3\mathrm{D}\) graphical representations to be learned,provided that the rendering process is approximately differentiable.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了学习，网络需要图像-体积对。这限制了可以用作足够大体积数据集的数据类型，或者生成它们的模型数量有限。未来工作的一个有前景的方向是将专业质量的渲染器纳入解码器结构。这在理论上允许学习\(3\mathrm{D}\)图形表示，前提是渲染过程大致可微分。</p></div><p>Acknowledgements. Thanks to Thomas Vetter for access to the Basel Face Model.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>致谢。感谢Thomas Vetter提供巴塞尔面部模型的访问权限。</p></div><h2>References</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>参考文献</h2></div><ol>
<li>Yuille, A., Kersten, D.: Vision as Bayesian inference: analysis by synthesis? Trends Cogn. Sci. 10(7), 301-308 (2006)</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol>
<li>Yuille, A., Kersten, D.: 视知觉作为贝叶斯推断：通过合成进行分析？《认知科学趋势》10(7), 301-308 (2006)</li>
</ol></div><ol start="2">
<li>Kulkarni, T.D., Whitney, W.F., Kohli, P., Tenenbaum, J.: Deep convolutional inverse graphics network. In: Advances in Neural Information Processing Systems, pp. 2530-2538 (2015)</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="2">
<li>Kulkarni, T.D., Whitney, W.F., Kohli, P., Tenenbaum, J.: 深度卷积逆图形网络。在：神经信息处理系统进展，pp. 2530-2538 (2015)</li>
</ol></div><ol start="3">
<li>Tatarchenko, M., Dosovitskiy, A., Brox, T.: Single-view to multi-view: reconstructing unseen views with a convolutional network arXiv preprint (2015). arXiv:1511.06702</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="3">
<li>Tatarchenko, M., Dosovitskiy, A., Brox, T.: 从单视图到多视图：使用卷积网络重建未见视图 arXiv 预印本 (2015). arXiv:1511.06702</li>
</ol></div><ol start="4">
<li>Yang, J., Reed, S.E., Yang, M.H., Lee, H.: Weakly-supervised disentangling with recurrent transformations for 3D view synthesis. In: Advances in Neural Information Processing Systems, pp. 1099-1107 (2015)</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="4">
<li>Yang, J., Reed, S.E., Yang, M.H., Lee, H.: 使用递归变换进行弱监督解缠结以实现3D视图合成。在：神经信息处理系统进展，pp. 1099-1107 (2015)</li>
</ol></div><ol start="5">
<li>Jaderberg, M., Simonyan, K., Zisserman, A., et al.: Spatial Transformer Networks. In: Advances in Neural Information Processing Systems, pp. 2008-2016 (2015)</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="5">
<li>Jaderberg, M., Simonyan, K., Zisserman, A., 等：空间变换网络。在：神经信息处理系统进展，pp. 2008-2016 (2015)</li>
</ol></div><ol start="6">
<li>Girdhar, R., Fouhey, D.F., Rodriguez, M., Gupta, A.: Learning a predictable and generative vector representation for objects arXiv preprint (2016). arXiv:1603.08637</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="6">
<li>Girdhar, R., Fouhey, D.F., Rodriguez, M., Gupta, A.: 学习可预测和生成的对象向量表示 arXiv 预印本 (2016). arXiv:1603.08637</li>
</ol></div><ol start="7">
<li>Rezende, D.J., Eslami, S., Mohamed, S., Battaglia, P., Jaderberg, M., Heess, N.: Unsupervised learning of \(3\mathrm{\;d}\) structure from images arXiv preprint (2016). arXiv:1607.00662</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="7">
<li>Rezende, D.J., Eslami, S., Mohamed, S., Battaglia, P., Jaderberg, M., Heess, N.: 从图像中无监督学习\(3\mathrm{\;d}\)结构 arXiv 预印本 (2016). arXiv:1607.00662</li>
</ol></div><ol start="8">
<li>Xu, B., Wang, N., Chen, T., Li, M.: Empirical evaluation of rectified activations in convolutional network arXiv preprint (2015). arXiv:1505.00853</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="8">
<li>Xu, B., Wang, N., Chen, T., Li, M.: 对卷积网络中修正激活的实证评估 arXiv 预印本 (2015). arXiv:1505.00853</li>
</ol></div><ol start="9">
<li>Ioffe, S., Szegedy, C.: Batch normalization: accelerating deep network training by reducing internal covariate shift. In: Proceedings of The 32nd International Conference on Machine Learning, pp. 448-456 (2015)</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="9">
<li>Ioffe, S., Szegedy, C.: 批量归一化：通过减少内部协变量偏移加速深度网络训练。在：第32届国际机器学习会议论文集，pp. 448-456 (2015)</li>
</ol></div><ol start="10">
<li>He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: surpassing human-level performance on ImageNet classification. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 1026-1034 (2015)</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="10">
<li>He, K., Zhang, X., Ren, S., Sun, J.: 深入研究修正器：在ImageNet分类中超越人类水平的表现。在：IEEE国际计算机视觉会议论文集，pp. 1026-1034 (2015)</li>
</ol></div><ol start="11">
<li>Paysan, P., Knothe, R., Amberg, B., Romdhani, S., Vetter, T.: A 3D face model for pose and illumination invariant face recognition. In: Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance AVSS 2009, pp. 296-301. IEEE (2009)</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="11">
<li>Paysan, P., Knothe, R., Amberg, B., Romdhani, S., Vetter, T.: 一种用于姿态和光照不变人脸识别的3D人脸模型。在：第六届IEEE国际先进视频和信号监控会议AVSS 2009，pp. 296-301. IEEE (2009)</li>
</ol></div><ol start="12">
<li>Kingma, D., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint (2014). arXiv:1412.6980</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="12">
<li>Kingma, D., Ba, J.: Adam：一种随机优化方法。arXiv 预印本 (2014). arXiv:1412.6980</li>
</ol></div><ol start="13">
<li>Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3D shapenets: a deep representation for volumetric shapes. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1912-1920 (2015)</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="13">
<li>Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3D形状网络：体积形状的深度表示。在：IEEE计算机视觉与模式识别会议论文集，pp. 1912-1920 (2015)</li>
</ol></div>
      </body>
    </html>
  