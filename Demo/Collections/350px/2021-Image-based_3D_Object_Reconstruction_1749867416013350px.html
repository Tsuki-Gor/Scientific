
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px 350px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <h1>Image-based 3D Object Reconstruction: State-of-the-Art and Trends in the Deep Learning Era</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>基于图像的3D物体重建：深度学习时代的最新进展与趋势</h1></div><p>Xian-Feng Han*, Hamid Laga*, Mohammed Bennamoun Senior Member, IEEE</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>韩先锋*, 哈米德·拉加*, 穆罕默德·本纳蒙，IEEE高级会员</p></div><p>Abstract-3D reconstruction is a longstanding ill-posed problem, which has been explored for decades by the computer vision, computer graphics, and machine learning communities. Since 2015, image-based 3D reconstruction using convolutional neural networks (CNN) has attracted increasing interest and demonstrated an impressive performance. Given this new era of rapid evolution, this article provides a comprehensive survey of the recent developments in this field. We focus on the works which use deep learning techniques to estimate the 3D shape of generic objects either from a single or multiple RGB images. We organize the literature based on the shape representations, the network architectures, and the training mechanisms they use. While this survey is intended for methods which reconstruct generic objects, we also review some of the recent works which focus on specific object classes such as human body shapes and faces. We provide an analysis and comparison of the performance of some key papers, summarize some of the open problems in this field, and discuss promising directions for future research.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>摘要-3D重建是一个长期存在的病态问题，计算机视觉、计算机图形学和机器学习领域的研究者们已经探索了数十年。自2015年以来，基于图像的3D重建利用卷积神经网络（CNN）引起了越来越多的关注，并展示了令人印象深刻的性能。鉴于这一快速发展的新时期，本文提供了该领域近期发展的全面综述。我们重点关注使用深度学习技术从单个或多个RGB图像估计通用物体的3D形状的研究。我们根据形状表示、网络架构和训练机制对文献进行了组织。虽然本综述旨在介绍重建通用物体的方法，但我们也回顾了一些关注特定物体类别（如人体形状和面部）的近期研究。我们提供了一些关键论文的性能分析与比较，总结了该领域的一些开放问题，并讨论了未来研究的有希望方向。</p></div><p>Index Terms—3D Reconstruction, Depth Estimation, SLAM, SfM, CNN, Deep Learning, LSTM, 3D face, 3D Human Body, 3D Video.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>索引词—3D重建、深度估计、SLAM、SfM、CNN、深度学习、LSTM、3D面部、3D人体、3D视频。</p></div><h2>1 INTRODUCTION</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>1 引言</h2></div><p>The goal of image-based 3D reconstruction is to infer the 3D geometry and structure of objects and scenes from one or multiple 2D images. This long standing ill-posed problem is fundamental to many applications such as robot navigation, object recognition and scene understanding, 3D modeling and animation, industrial control, and medical diagnosis.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于图像的3D重建的目标是从一张或多张2D图像推断物体和场景的3D几何形状和结构。这个长期存在的病态问题对许多应用至关重要，如机器人导航、物体识别和场景理解、3D建模与动画、工业控制和医学诊断。</p></div><p>Recovering the lost dimension from just 2D images has been the goal of classic multiview stereo and shape-from- \(\mathrm{X}\) methods,which have been extensively investigated for many decades. The first generation of methods approached the problem from the geometric perspective; they focused on understanding and formalizing, mathematically, the 3D to 2D projection process, with the aim to devise mathematical or algorithmic solutions to the ill-posed inverse problem. Effective solutions typically require multiple images, captured using accurately calibrated cameras. Stereo-based techniques [1], for example, require matching features across images captured from slightly different viewing angles, and then use the triangulation principle to recover the 3D coordinates of the image pixels. Shape-from-silhouette, or shape-by-space-carving, methods [2] require accurately segmented 2D silhouettes. These methods, which have led to reasonable quality 3D reconstructions, require multiple images of the same object captured by well-calibrated cameras. This, however, may not be practical or feasible in many situations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>从仅有的2D图像中恢复丢失的维度一直是经典多视图立体和从\(\mathrm{X}\)形状方法的目标，这些方法已经被广泛研究了数十年。第一代方法从几何角度入手，专注于理解和形式化3D到2D的投影过程，旨在制定数学或算法解决方案来应对病态逆问题。有效的解决方案通常需要多张图像，这些图像是通过精确校准的相机捕获的。例如，基于立体的技术[1]需要在从略微不同的视角捕获的图像之间匹配特征，然后利用三角测量原理恢复图像像素的3D坐标。基于轮廓的形状或空间雕刻方法[2]需要准确分割的2D轮廓。这些方法虽然能够产生合理质量的3D重建，但需要通过良好校准的相机捕获同一物体的多张图像。然而，在许多情况下，这可能并不实用或可行。</p></div><p>Interestingly, humans are good at solving such ill-posed inverse problems by leveraging prior knowledge. They can infer the approximate size and rough geometry of objects using only one eye. They can even guess what it would look like from another viewpoint. We can do this because all the previously seen objects and scenes have enabled us to build prior knowledge and develop mental models of what objects look like. The second generation of 3D reconstruction methods tried to leverage this prior knowledge by formulating the \(3\mathrm{D}\) reconstruction problem as a recognition problem. The avenue of deep learning techniques, and more importantly, the increasing availability of large training data sets, have led to a new generation of methods that are able to recover the \(3\mathrm{D}\) geometry and structure of objects from one or multiple RGB images without the complex camera calibration process. Despite being recent, these methods have demonstrated exciting and promising results on various tasks related to computer vision and graphics.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>有趣的是，人类擅长通过利用先验知识来解决这种病态逆问题。他们可以仅用一只眼睛推断物体的大致大小和粗略几何形状。他们甚至可以猜测从另一个视角看起来会是什么样子。我们之所以能够做到这一点，是因为之前看到的所有物体和场景使我们能够建立先验知识并发展出物体外观的心理模型。第二代3D重建方法试图通过将\(3\mathrm{D}\)重建问题表述为识别问题来利用这种先验知识。深度学习技术的出现，更重要的是，大型训练数据集的日益可用，催生了一代新方法，这些方法能够从一张或多张RGB图像中恢复\(3\mathrm{D}\)物体的几何形状和结构，而无需复杂的相机校准过程。尽管这些方法相对较新，但在与计算机视觉和图形相关的各种任务中展示了令人兴奋和有希望的结果。</p></div><p>In this article, we provide a comprehensive and structured review of the recent advances in 3D object reconstruction using deep learning techniques. We first focus on generic shapes and then discuss specific cases, such as human body shapes faces reconstruction, and 3D scene parsing. We have gathered 149 papers, which appeared since 2015 in leading computer vision, computer graphics, and machine learning conferences and journals \({}^{1}\) . The goal is to help the reader navigate in this emerging field, which gained a significant momentum in the past few years. Compared to the existing literature, the main contributions of this article are as follows;</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本文中，我们提供了基于深度学习技术的3D物体重建近期进展的全面和结构化的综述。我们首先关注通用形状，然后讨论特定案例，如人体形状、面部重建和3D场景解析。我们收集了自2015年以来在领先的计算机视觉、计算机图形学和机器学习会议及期刊上发表的149篇论文\({}^{1}\)。我们的目标是帮助读者在这一新兴领域中导航，该领域在过去几年中获得了显著的动力。与现有文献相比，本文的主要贡献如下：</p></div><hr>
<!-- Footnote --><ul>
<li>(* Joint first author) Xian-Feng Han is with College of Computer and Information Science, Southwest University, Chongqing 400715, China, with Tianjin University, Tianjin, 300350, China and with the University of Western Australia, Perth, WA 6009, Australia.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>(* 联合第一作者) 韩先锋来自中国重庆西南大学计算机与信息科学学院，天津大学（中国天津300350）和澳大利亚西澳大学（Perth, WA 6009）。</li>
</ul></div><ul>
<li>(* Joint first author) Hamid Laga is with the Information Technology, Mathematics and Statistics Discipline, Murdoch University (Australia), and with the Phenomics and Bioinformatics Research Centre, University of South Australia. E-mail: <a href="mailto:H.Laga@murdoch.edu.au">H.Laga@murdoch.edu.au</a></li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>(* 联合第一作者) 哈米德·拉加来自澳大利亚默多克大学信息技术、数学与统计学科，以及南澳大利亚大学的表型学与生物信息学研究中心。电子邮件：<a href="mailto:H.Laga@murdoch.edu.au">H.Laga@murdoch.edu.au</a></li>
</ul></div><ul>
<li>Mohammed Bennamoun is with the University of Western Australia, Perth, WA 6009, Australia. Email: <a href="mailto:mohammed.bennamoun@uwa.edu.au">mohammed.bennamoun@uwa.edu.au</a></li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>Mohammed Bennamoun 在澳大利亚西澳大利亚大学，珀斯，邮政编码 6009，澳大利亚。电子邮件：<a href="mailto:mohammed.bennamoun@uwa.edu.au">mohammed.bennamoun@uwa.edu.au</a></li>
</ul></div><p>Manuscript received April 19, 2005; revised December 27, 2012.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>手稿于 2005 年 4 月 19 日收到；2012 年 12 月 27 日修订。</p></div><ol>
<li>This continuously and rapidly increasing number, even at the time we are finalising this article, does not include many of the CVPR2019 and the upcoming ICCV2019 papers.</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol>
<li>即使在我们完成本文时，这个不断快速增长的数字也不包括许多 CVPR2019 和即将到来的 ICCV2019 论文。</li>
</ol></div><!-- Footnote -->
<hr><ol>
<li>To the best of our knowledge, this is the first survey paper in the literature which focuses on image-based 3D object reconstruction using deep learning.</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol>
<li>据我们所知，这是文献中第一篇专注于基于图像的 3D 物体重建的深度学习调查论文。</li>
</ol></div><ol start="2">
<li>We cover the contemporary literature with respect to this area. We present a comprehensive review of 149 methods, which appeared since 2015.</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="2">
<li>我们涵盖了该领域的当代文献。我们对自 2015 年以来出现的 149 种方法进行了全面回顾。</li>
</ol></div><ol start="3">
<li>We provide a comprehensive review and an insightful analysis on all aspects of \(3\mathrm{D}\) reconstruction using deep learning, including the training data, the choice of network architectures and their effect on the \(3\mathrm{D}\) reconstruction results,the training strategies, and the application scenarios.</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="3">
<li>我们提供了对使用深度学习进行 \(3\mathrm{D}\) 重建的各个方面的全面回顾和深刻分析，包括训练数据、网络架构的选择及其对 \(3\mathrm{D}\) 重建结果的影响、训练策略和应用场景。</li>
</ol></div><ol start="4">
<li>We provide a comparative summary of the properties and performance of the reviewed methods for generic 3D object reconstruction. We cover 88 algorithms for generic 3D object reconstruction, 11 methods related to \(3\mathrm{D}\) face reconstruction,and 6 methods for \(3\mathrm{D}\) human body shape reconstruction.</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="4">
<li>我们提供了对所评审方法在通用 3D 物体重建中的属性和性能的比较总结。我们涵盖了 88 种通用 3D 物体重建算法、11 种与 \(3\mathrm{D}\) 面部重建相关的方法，以及 6 种用于 \(3\mathrm{D}\) 人体形状重建的方法。</li>
</ol></div><ol start="5">
<li>We provide a comparative summary of the methods in a tabular form.</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="5">
<li>我们以表格形式提供了方法的比较总结。</li>
</ol></div><p>The rest of this article is organized as follows; Section 2 fo-mulates the problem and lays down the taxonomy. Section 3 reviews the latent spaces and the input encoding mechanisms. Section 4 surveys the volumetric reconstruction techniques, while Section 5 focuses on surface-based techniques. Section 6 shows how some of the state-of-the-art techniques use additional cues to boost the performance of \(3\mathrm{D}\) reconstruction. Section 7 discusses the training procedures. Section 8 focuses on specific objects such as human body shapes and faces. Section 9 summarizes the most commonly used datasets to train, test, and evaluate the performance of various deep learning-based 3D reconstruction algorithms. Section 10 compares and discusses the performance of some key methods. Finally, Section 11 discusses potential future research directions while Section 12 concludes the paper with some important remarks.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>本文的其余部分组织如下；第 2 节阐述了问题并建立了分类法。第 3 节回顾了潜在空间和输入编码机制。第 4 节调查了体积重建技术，而第 5 节则专注于基于表面的技术。第 6 节展示了一些最先进技术如何利用额外线索来提升 \(3\mathrm{D}\) 重建的性能。第 7 节讨论了训练过程。第 8 节专注于特定物体，如人体形状和面部。第 9 节总结了用于训练、测试和评估各种基于深度学习的 3D 重建算法性能的最常用数据集。第 10 节比较并讨论了一些关键方法的性能。最后，第 11 节讨论了潜在的未来研究方向，而第 12 节则以一些重要的评论结束了论文。</p></div><h2>2 Problem STATEMENT AND TAXONOMY</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>2 问题陈述与分类法</h2></div><p>Let \(\mathbf{I} = \left\{  {{I}_{k},k = 1,\ldots ,n}\right\}\) be a set of \(n \geq  1\) RGB images of one or multiple objects \(X\) . 3D reconstruction can be summarized as the process of learning a predictor \({f}_{\theta }\) that can infer a shape \(\widehat{X}\) that is as close as possible to the unknown shape \(X\) . In other words,the function \({f}_{\theta }\) is the minimizer of a reconstruction objective \(\mathcal{L}\left( \mathbf{I}\right)  = d\left( {{f}_{\theta }\left( \mathbf{I}\right) ,X}\right)\) . Here, \(\theta\) is the set of parameters of \(f\) and \(d\left( {\cdot , \cdot  }\right)\) is a certain measure of distance between the target shape \(X\) and the reconstructed shape \(f\left( \mathbf{I}\right)\) . The reconstruction objective \(\mathcal{L}\) is also known as the loss function in the deep learning literature.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>设 \(\mathbf{I} = \left\{  {{I}_{k},k = 1,\ldots ,n}\right\}\) 为一组 \(n \geq  1\) RGB 图像，包含一个或多个物体 \(X\) 。3D 重建可以总结为学习一个预测器 \({f}_{\theta }\) 的过程，该预测器可以推断出尽可能接近未知形状 \(X\) 的形状 \(\widehat{X}\) 。换句话说，函数 \({f}_{\theta }\) 是重建目标 \(\mathcal{L}\left( \mathbf{I}\right)  = d\left( {{f}_{\theta }\left( \mathbf{I}\right) ,X}\right)\) 的最小化器。在这里，\(\theta\) 是 \(f\) 的参数集，而 \(d\left( {\cdot , \cdot  }\right)\) 是目标形状 \(X\) 和重建形状 \(f\left( \mathbf{I}\right)\) 之间的某种距离度量。重建目标 \(\mathcal{L}\) 在深度学习文献中也被称为损失函数。</p></div><p>This survey discusses and categorizes the state-of-the-art based on the nature of the input \(\mathbf{I}\) ,the representation of the output, the deep neural network architectures used during training and testing to approximate the predictor \(f\) ,the training procedures they use,and their degree of supervision, see Table 1 for a visual summary. In particular, the input \(\mathbf{I}\) can be (1) a single image,(2) multiple images captured using RGB cameras whose intrinsic and extrinsic parameters can be known or unknown, or (3) a video stream, i.e., a sequence of images with temporal correlation. The first case is very challenging because of the ambiguities in the 3D reconstruction. When the input is a video stream, one can exploit the temporal correlation to facilitate the 3D reconstruction while ensuring that the reconstruction is smooth and consistent across all the frames of the video stream. Also, the input can be depicting one or multiple 3D objects belonging to known or unknown shape categories. It can also include additional information such as silhouettes, segmentation masks, and semantic labels as priors to guide the reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>本调查根据输入 \(\mathbf{I}\) 的性质、输出的表示、在训练和测试中使用的深度神经网络架构、它们使用的训练程序以及它们的监督程度进行分类和讨论，见表 1 的视觉总结。特别是，输入 \(\mathbf{I}\) 可以是 (1) 单个图像，(2) 使用 RGB 相机捕获的多张图像，其内在和外在参数可以是已知或未知，或 (3) 视频流，即具有时间相关性的图像序列。第一种情况由于 3D 重建中的歧义而非常具有挑战性。当输入是视频流时，可以利用时间相关性来促进 3D 重建，同时确保重建在视频流的所有帧中都是平滑和一致的。此外，输入可以描绘一个或多个属于已知或未知形状类别的 3D 物体。它还可以包括额外信息，如轮廓、分割掩码和语义标签，作为指导重建的先验信息。</p></div><!-- Media --><p>TABLE 1: Taxonomy of the state-of-the-art image-based 3D object reconstruction using deep learning.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表1：基于深度学习的最先进图像3D物体重建的分类。</p></div><table><tbody><tr><td rowspan="2">Input</td><td>Training</td><td>1 vs. muli RGB, 3D ground truth, Segmentation.</td><td rowspan="2">One vs. multiple objects, Uniform vs. cluttered background.</td></tr><tr><td>Testing</td><td>1 vs. muli RGB, Segmentation</td></tr><tr><td rowspan="3">Output</td><td>Volumetric</td><td colspan="2">High vs. low resolution</td></tr><tr><td>Surface</td><td colspan="2">Parameterization, template deformation, Point cloud.</td></tr><tr><td colspan="3">Direct vs. intermediating</td></tr><tr><td rowspan="3">Network architec- ture</td><td colspan="2">Architecture at training</td><td>Architecture at testing</td></tr><tr><td colspan="2">Encoder - Decoder TL-Net (Conditional) GAN</td><td>Encoder - Decoder</td></tr><tr><td colspan="2">3D-VAE-GAN</td><td>3D-VAE</td></tr><tr><td rowspan="3">Training</td><td rowspan="2">Degree of supervision</td><td colspan="2">2D vs. 3D supervision. Weak supervision.</td></tr><tr><td colspan="2">Loss functions.</td></tr><tr><td>Training procedure</td><td colspan="2">Adversarial training. Joint 2D-3D embedding. Joint training with other tasks.</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">输入</td><td>训练</td><td>单个与多个RGB，3D真实数据，分割。</td><td rowspan="2">单个与多个物体，均匀与杂乱背景。</td></tr><tr><td>测试</td><td>单个与多个RGB，分割</td></tr><tr><td rowspan="3">输出</td><td>体积</td><td colspan="2">高分辨率与低分辨率</td></tr><tr><td>表面</td><td colspan="2">参数化，模板变形，点云。</td></tr><tr><td colspan="3">直接与中介</td></tr><tr><td rowspan="3">网络架构</td><td colspan="2">训练时的架构</td><td>测试时的架构</td></tr><tr><td colspan="2">编码器-解码器TL-Net（条件）GAN</td><td>编码器-解码器</td></tr><tr><td colspan="2">3D-VAE-GAN</td><td>3D-VAE</td></tr><tr><td rowspan="3">训练</td><td rowspan="2">监督程度</td><td colspan="2">2D与3D监督。弱监督。</td></tr><tr><td colspan="2">损失函数。</td></tr><tr><td>训练过程</td><td colspan="2">对抗训练。联合2D-3D嵌入。与其他任务的联合训练。</td></tr></tbody></table></div><!-- Media --><p>The representation of the output is crucial to the choice of the network architecture. It also impacts the computational efficiency and quality of the reconstruction. In particular,</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>输出的表示对于网络架构的选择至关重要。它还影响计算效率和重建质量。特别是，</p></div><ul>
<li>Volumetric representations, which have been extensively adopted in early deep leaning-based 3D reconstruction techniques, allow the parametrization of 3D shapes using regular voxel grids. As such, 2D convolutions used in image analysis can be easily extended to 3D. They are, however, very expensive in terms of memory requirements, and only a few techniques can achieve sub-voxel accuracy.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>体积表示法，这在早期基于深度学习的3D重建技术中被广泛采用，允许使用规则的体素网格对3D形状进行参数化。因此，图像分析中使用的2D卷积可以很容易地扩展到3D。然而，它们在内存需求方面非常昂贵，只有少数技术能够实现亚体素精度。</li>
</ul></div><ul>
<li>Surface-based representations: Other papers explored surface-based representations such as meshes and point clouds. While being memory-efficient, such representations are not regular structures and thus, they do not easily fit into deep learning architectures.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>基于表面的表示法：其他论文探讨了基于表面的表示法，如网格和点云。虽然这些表示法在内存上高效，但它们不是规则结构，因此不容易适应深度学习架构。</li>
</ul></div><ul>
<li>Intermediation: While some 3D reconstruction algorithms predict the 3D geometry of an object from RGB images directly, others decompose the problem into sequential steps, each step predicts an intermediate representation.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>中介：虽然一些3D重建算法直接从RGB图像预测物体的3D几何形状，但其他算法将问题分解为顺序步骤，每个步骤预测一个中间表示。</li>
</ul></div><p>A variety of network architectures have been utilized to implement the predictor \(f\) . The backbone architecture,which can be different during training and testing, is composed of an encoder \(h\) followed by a decoder \(g\) ,i.e., \(f = g \circ  h\) . The encoder maps the input into a latent variable \(\mathbf{x}\) ,referred to as a feature vector or a code, using a sequence of convolutions and pooling operations, followed by fully connected layers of neurons. The decoder, also called the generator, decodes the feature vector into the desired output by using either fully connected layers or a deconvolution network (a sequence of convolution and upsampling operations, also referred to as upconvolutions). The former is suitable for unstructured output, e.g., 3D point clouds, while the latter is used to reconstruct volumetric grids or parametrized surfaces. Since the introduction of this vanilla architecture, several extensions have been proposed by varying the architecture (e.g., ConvNet vs. ResNet, Convolutional Neural Networks (CNN) vs. Generative Adversarial Networks (GAN), CNN vs. Variational Auto-Encoders, and 2D vs. 3D convolutions), and by cascading multiple blocks each one achieving a specific task.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>已经利用多种网络架构来实现预测器\(f\)。主干架构在训练和测试期间可能不同，由编码器\(h\)和解码器\(g\)组成，即\(f = g \circ  h\)。编码器将输入映射到潜在变量\(\mathbf{x}\)，称为特征向量或代码，使用一系列卷积和池化操作，后接全连接神经元层。解码器，也称为生成器，通过使用全连接层或反卷积网络（卷积和上采样操作的序列，也称为上卷积）将特征向量解码为所需输出。前者适用于非结构化输出，例如3D点云，而后者用于重建体积网格或参数化表面。自从引入这种基础架构以来，通过改变架构（例如，ConvNet与ResNet，卷积神经网络（CNN）与生成对抗网络（GAN），CNN与变分自编码器，以及2D与3D卷积）和级联多个每个实现特定任务的模块，提出了几种扩展。</p></div><p>While the architecture of the network and its building blocks are important, the performance depends highly on the way it is trained. In this survey, we will look at:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>虽然网络的架构及其构建模块很重要，但性能在很大程度上取决于训练方式。在本次调查中，我们将关注：</p></div><ul>
<li>Datasets: There are various datasets that are currently available for training and evaluating deep learning-based 3D reconstruction. Some of them use real data, other are CG-generated.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>数据集：目前有多种数据集可用于训练和评估基于深度学习的3D重建。其中一些使用真实数据，其他则是计算机生成的。</li>
</ul></div><ul>
<li>Loss functions: The choice of the loss function can significantly impact on the reconstruction quality. It also defines the degree of supervision.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>损失函数：损失函数的选择会显著影响重建质量。它还定义了监督的程度。</li>
</ul></div><ul>
<li>Training procedure sand degree of supervision: Some methods require real images annotated with their corresponding 3D models, which are very expensive to obtain. Other methods rely on a combination of real and synthetic data. Others avoid completely 3D supervision by using loss functions that exploit supervisory signals that are easy to obtain.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>训练过程和监督程度：一些方法需要带有相应3D模型的真实图像标注，这些图像获取成本非常高。其他方法依赖于真实数据和合成数据的组合。还有一些方法通过使用易于获取的监督信号的损失函数完全避免3D监督。</li>
</ul></div><p>The following sections review in detail these aspects.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>以下部分将详细回顾这些方面。</p></div><h2>3 THE ENCODING STAGE</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>3 编码阶段</h2></div><p>Deep learning-based 3D reconstruction algorithms encode the input \(\mathbf{I}\) into a feature vector \(\mathbf{x} = h\left( \mathbf{I}\right)  \in  \mathcal{X}\) where \(\mathcal{X}\) is the latent space. A good mapping function \(h\) should satisfy the following properties:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于深度学习的3D重建算法将输入\(\mathbf{I}\)编码为特征向量\(\mathbf{x} = h\left( \mathbf{I}\right)  \in  \mathcal{X}\)，其中\(\mathcal{X}\)是潜在空间。一个好的映射函数\(h\)应满足以下属性：</p></div><ul>
<li>Two inputs \({\mathbf{I}}_{1}\) and \({\mathbf{I}}_{2}\) that represent similar 3D objects should be mapped into \({\mathbf{x}}_{1}\) and \({\mathbf{x}}_{2} \in  \mathcal{X}\) that are close to each other in the latent space.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>两个输入\({\mathbf{I}}_{1}\)和\({\mathbf{I}}_{2}\)表示相似的3D对象，应映射到潜在空间中彼此接近的\({\mathbf{x}}_{1}\)和\({\mathbf{x}}_{2} \in  \mathcal{X}\)。</li>
</ul></div><ul>
<li>A small perturbation \(\partial \mathbf{x}\) of \(\mathbf{x}\) should correspond to a small perturbation of the shape of the input.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>输入\(\mathbf{x}\)的小扰动\(\partial \mathbf{x}\)应对应于输入形状的小扰动。</li>
</ul></div><ul>
<li>The latent representation induced by \(h\) should be invariant to extrinsic factors such as the camera pose.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>由\(h\)引起的潜在表示应对外部因素（如相机姿态）不变。</li>
</ul></div><ul>
<li>A 3D model and its corresponding 2D images should be mapped onto the same point in the latent space. This will ensure that the representation is not ambiguous and thus facilitate the reconstruction.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>3D模型及其对应的2D图像应映射到潜在空间中的同一点。这将确保表示不模糊，从而促进重建。</li>
</ul></div><p>The first two conditions have been addressed by using encoders that map the input onto discrete (Section 3.1) or continuous (Section 3.2) latent spaces. These can be flat or hierarchical (Section 3.3). The third one has been addressed by using disentangled representations (Section 3.4). The latter has been addressed by using TL-architectures during the training phase. This is covered in Section 7.3.1 as one of the many training mechanisms that have been used in the literature. Table 2 summarizes this taxonomy.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>前两个条件通过使用将输入映射到离散（第3.1节）或连续（第3.2节）潜在空间的编码器得以解决。这些空间可以是平坦的或层次化的（第3.3节）。第三个条件通过使用解耦表示（第3.4节）得以解决。后者在训练阶段通过使用TL架构得以解决。这在第7.3.1节中作为文献中使用的众多训练机制之一进行了介绍。表2总结了这一分类。</p></div><h3>3.1 Discrete latent spaces</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.1 离散潜在空间</h3></div><p>Wu et al. in their seminal work [3] introduced 3D ShapeNet, an encoding network which maps a \(3\mathrm{D}\) shape,represented as a discretized volumetric grid of size \({30}^{3}\) ,into a latent representation of size \({4000} \times  1\) . Its core network is composed of \({n}_{\text{conv }} = 3\) convolutional layers (each one using 3D convolution filters),followed by \({n}_{fc} = 3\) fully connected layers. This standard vanilla architecture has been used for \(3\mathrm{D}\) shape classification and retrieval \(\left\lbrack  3\right\rbrack\) ,and for \(3\mathrm{D}\) reconstruction from depth maps represented as voxel grids [3]. It has also been used in the \(3\mathrm{D}\) encoding branch of the TL architectures during the training of \(3\mathrm{D}\) reconstruction networks, see Section 7.3.1.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>吴等人在他们的开创性工作[3]中引入了3D ShapeNet，这是一种编码网络，将一个\(3\mathrm{D}\)形状（表示为大小为\({30}^{3}\)的离散体积网格）映射到大小为\({4000} \times  1\)的潜在表示。其核心网络由\({n}_{\text{conv }} = 3\)个卷积层（每个层使用3D卷积滤波器）组成，后面跟着\({n}_{fc} = 3\)个全连接层。这种标准的基础架构已被用于\(3\mathrm{D}\)形状分类和检索\(\left\lbrack  3\right\rbrack\)，以及从表示为体素网格的深度图中进行\(3\mathrm{D}\)重建[3]。它还被用于TL架构的\(3\mathrm{D}\)编码分支，在\(3\mathrm{D}\)重建网络的训练中，见第7.3.1节。</p></div><!-- Media --><p>TABLE 2: Taxonomy of the encoding stage. FC: fully-connected layers. VAE: Variational Auto-Encoder.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表2：编码阶段的分类。FC：全连接层。VAE：变分自编码器。</p></div><table><tbody><tr><td>Latent spaces</td><td>Architectures</td></tr><tr><td>Discrete (3.1) vs. continuous 3.2 Flat vs. hierarchical (3.3 Disentangled representation3.4</td><td>ConvNet, ResNet, FC, 3D-VAE</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>潜在空间</td><td>架构</td></tr><tr><td>离散 (3.1) 与连续 3.2 平面与层次 (3.3 解耦表示 3.4</td><td>卷积网络, 残差网络, 全连接网络, 3D变分自编码器</td></tr></tbody></table></div><!-- Media --><p>2D encoding networks that map input images into a latent space follow the same architecture as 3D ShapeNet [3] but use 2D convolutions [4], [5], [6], [7], [8], [9], [10], [11]. Early works differ in the type and number of layers they use. For instance,Yan et al. [4] use \({n}_{\text{conv }} = 3\) convolutional layers with 64,128,ad 256 channels,respectively,and \({n}_{fc} = 3\) fully-connected layers with 1024, 1024, and 512 neurons, respectively. Wiles and Zisserman [10] use \({n}_{\text{conv }} = 6\) convolutional layers of \(3,{64},{128},{256},\overline{128}\) ,and 160 channels, respectively. Other works add pooling layers [7], [12], and leaky Rectified Linear Units (ReLU) [7], [12], [13]. For example, Wiles and Zisserman [10] use max pooling layers between each pair of convolutional layers, except after the first layer and before the last layer. ReLU layers improve learning since the gradient during the back propagation is never zero.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>将输入图像映射到潜在空间的2D编码网络遵循与3D ShapeNet（3D形状网络）相同的架构，但使用2D卷积。早期的工作在使用的层类型和数量上有所不同。例如，Yan等人使用64、128和256通道的卷积层，以及1024、1024和512个神经元的全连接层。Wiles和Zisserman使用的卷积层分别为160通道。其他工作添加了池化层和泄漏整流线性单元（ReLU）。例如，Wiles和Zisserman在每对卷积层之间使用最大池化层，除了第一层之后和最后一层之前。ReLU层改善了学习，因为反向传播过程中的梯度从不为零。</p></div><p>Both 3D shape and 2D image encoding networks can be implemented using deep residual networks (ResNet) [14], which add residual connections between the convolutional layers, see for example [6], [7], [9]. Compared to conventional networks such as VGGNet [15], ResNets improve and speed up the learning process for very deep networks.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D形状和2D图像编码网络都可以使用深度残差网络（ResNet）实现，这在卷积层之间添加了残差连接。与传统网络如VGGNet相比，ResNet改善并加速了非常深层网络的学习过程。</p></div><h3>3.2 Continuous latent spaces</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.2 连续潜在空间</h3></div><p>Using the encoders presented in the previous section, the latent space \(\mathcal{X}\) may not be continuous and thus it does not allow easy interpolation. In other words,if \({\mathbf{x}}_{1} = h\left( {\mathbf{I}}_{1}\right)\) and \({\mathbf{x}}_{2} = h\left( {\mathbf{I}}_{2}\right)\) ,then there is no guarantee that \(\frac{1}{2}\left( {{\mathbf{x}}_{1} + {\mathbf{x}}_{2}}\right)\) can be decoded into a valid 3D shape. Also, small perturbations of \({\mathbf{x}}_{1}\) do not necessarily correspond to small perturbations of the input. Variational Autoencoders (VAE) [16] and their 3D extension (3D-VAE) [17] have one fundamentally unique property that makes them suitable for generative modeling: their latent spaces are, by design, continuous, allowing easy sampling and interpolation. The key idea is that instead of mapping the input into a feature vector, it is mapped into a mean vector \(\mathbf{\mu }\) and a vector of standard deviations \(\mathbf{\sigma }\) of a multivariate Gaussian distribution. A sampling layer then takes these two vectors, and generates, by random sampling from the Gaussian distribution,a feature vector \(\mathbf{x}\) ,which will serve as input to the subsequent decoding stages.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>使用前一节中介绍的编码器，潜在空间可能不是连续的，因此不允许轻松插值。换句话说，如果存在某些条件，则不能保证可以解码为有效的3D形状。此外，潜在空间的小扰动不一定对应于输入的小扰动。变分自编码器（VAE）及其3D扩展（3D-VAE）具有一个根本独特的特性，使其适合生成建模：它们的潜在空间在设计上是连续的，允许轻松采样和插值。关键思想是将输入映射到均值向量和多元高斯分布的标准差向量。然后，采样层从这两个向量中生成一个特征向量，作为后续解码阶段的输入。</p></div><p>This architecture has been used to learn continuous latent spaces for volumetric [17], [18], depth-based [19], surface-based [20], and point-based [21], [22] 3D reconstruction. In Wu et al. [17], for example, the image encoder takes a \({256} \times  {256}\) RGB image and outputs two 200-dimensional vectors representing, respectively, the mean and the standard deviation of a Gaussian distribution in the 200-dimensional space. Compared to standard encoders, 3D-VAE can be used to randomly sample from the latent space, to generate variations of an input, and to reconstruct multiple plausible 3D shapes from an input image [21], [22]. It generalizes well to images that have not been seen during the training.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>该架构已被用于学习体积、基于深度、基于表面和基于点的3D重建的连续潜在空间。例如，在Wu等人的研究中，图像编码器接收一张RGB图像，并输出两个200维向量，分别表示200维空间中高斯分布的均值和标准差。与标准编码器相比，3D-VAE可以随机从潜在空间中采样，以生成输入的变体，并从输入图像重建多个合理的3D形状。它对训练期间未见过的图像具有良好的泛化能力。</p></div><h3>3.3 Hierarchical latent spaces</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.3 层次潜在空间</h3></div><p>Liu et al. [18] showed that encoders that map the input into a single latent representation cannot extract rich structures and thus may lead to blurry reconstructions. To improve the quality of the reconstruction, Liu et al. [18] introduced a more complex internal variable structure, with the specific goal of encouraging the learning of a hierarchical arrangement of latent feature detectors. The approach starts with a global latent variable layer that is hardwired to a set of local latent variable layers, each tasked with representing one level of feature abstraction. The skip-connections tie together the latent codes in a top-down directed fashion: local codes closer to the input will tend to represent lower-level features while local codes farther away from the input will tend towards representing higher-level features. Finally, the local latent codes are concatenated to a flattened structure when fed into the task-specific models such as 3D reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>刘等人[18]表明，将输入映射到单一潜在表示的编码器无法提取丰富的结构，因此可能导致模糊的重建。为了提高重建的质量，刘等人[18]引入了一种更复杂的内部变量结构，具体目标是鼓励学习潜在特征检测器的层次排列。该方法从一个全局潜在变量层开始，该层与一组局部潜在变量层硬连接，每个局部层负责表示一个特征抽象级别。跳跃连接以自上而下的方式将潜在代码联系在一起：靠近输入的局部代码往往表示较低级别的特征，而远离输入的局部代码则倾向于表示较高级别的特征。最后，当局部潜在代码被输入到特定任务模型（如3D重建）时，它们被连接成一个扁平结构。</p></div><h3>3.4 Disentangled representation</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.4 解耦表示</h3></div><p>The appearance of an object in an image is affected by multiple factors such as the object's shape, the camera pose, and the lighting conditions. Standard encoders represent all these variabilities in the learned code \(\mathbf{x}\) . This is not desirable in applications such as recognition and classification, which should be invariant to extrinsic factors such as pose and lighting [23]. 3D reconstruction can also benefit from disentangled representations where shape, pose, and lighting are represented with different codes. To this end, Grant et al. [5] proposed an encoder, which maps an RGB image into a shape code and a transformation code. The former is decoded into a 3D shape. The latter, which encodes lighting conditions and pose,is decoded into (1) another \({80} \times  {80}\mathrm{{RGB}}\) image with correct lighting, using upconvolutional layers, and (2) camera pose using fully-connected layers (FC). To enable a disentangled representation, the network is trained in such a way that in the forward pass, the image decoder receives input from the shape code and the transformation code. In the backward pass, the signal from the image decoder to the shape code is suppressed to force it to only represent shape.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图像中物体的外观受多个因素的影响，例如物体的形状、相机姿态和光照条件。标准编码器在学习的代码\(\mathbf{x}\)中表示所有这些变异性。这在识别和分类等应用中并不理想，这些应用应该对姿态和光照等外部因素保持不变[23]。3D重建也可以从解耦表示中受益，其中形状、姿态和光照用不同的代码表示。为此，Grant等人[5]提出了一种编码器，将RGB图像映射到形状代码和变换代码。前者被解码为3D形状。后者编码光照条件和姿态，解码为(1) 使用上卷积层生成的具有正确光照的另一个\({80} \times  {80}\mathrm{{RGB}}\)图像，以及(2) 使用全连接层(FC)生成的相机姿态。为了实现解耦表示，网络以这样的方式进行训练：在前向传播中，图像解码器接收来自形状代码和变换代码的输入。在反向传播中，来自图像解码器到形状代码的信号被抑制，以强制其仅表示形状。</p></div><p>Zhu et al. [24] followed the same idea by decoupling the 6DOF pose parameters and shape. The network reconstructs from the 2D input the 3D shape but in a canonical pose. At the same time, a pose regressor estimates the 6DOF pose parameters, which are then applied to the reconstructed canonical shape. Decoupling pose and shape reduces the number of free parameters in the network, which results in improved efficiency.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>朱等人[24]遵循了相同的思路，通过解耦6自由度姿态参数和形状。网络从2D输入重建3D形状，但在一个规范姿态下。同时，姿态回归器估计6自由度姿态参数，然后将其应用于重建的规范形状。解耦姿态和形状减少了网络中的自由参数数量，从而提高了效率。</p></div><h2>4 VOLUMETRIC DECODING</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>4 体积解码</h2></div><p>Volumetric representations discritize the space around a 3D object into a 3D voxel grid \(V\) . The finer the discretization is, the more accurate the representation will be. The goal is then to recover a grid \(\widehat{V} = {f}_{\theta }\left( \mathbf{I}\right)\) such that the 3D shape \(\widehat{X}\) it represents is as close as possible to the unknown real 3D shape \(X\) . The main advantage of using volumetric grids is that many of the existing deep learning architectures that have been designed for \(2\mathrm{D}\) image analysis can be easily extended to \(3\mathrm{D}\) data by replacing the \(2\mathrm{D}\) pixel array with its \(3\mathrm{D}\) analogue and then processing the grid using \(3\mathrm{D}\) convolution and pooling operations. This section looks at the different volumetric representations (Section 4.1) and reviews the decoder architectures for low-resolution (Section 4.2) and high-resolution (Section 4.3) 3D reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>体积表示将3D物体周围的空间离散化为3D体素网格\(V\)。离散化越精细，表示就越准确。目标是恢复一个网格\(\widehat{V} = {f}_{\theta }\left( \mathbf{I}\right)\)，使其表示的3D形状\(\widehat{X}\)尽可能接近未知的真实3D形状\(X\)。使用体积网格的主要优点是，许多为\(2\mathrm{D}\)图像分析设计的现有深度学习架构可以通过将\(2\mathrm{D}\)像素数组替换为其\(3\mathrm{D}\)对应物，轻松扩展到\(3\mathrm{D}\)数据，然后使用\(3\mathrm{D}\)卷积和池化操作处理该网格。本节将探讨不同的体积表示（第4.1节），并回顾低分辨率（第4.2节）和高分辨率（第4.3节）3D重建的解码器架构。</p></div><h3>4.1 Volumetric representations of 3D shapes</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.1 3D形状的体积表示</h3></div><p>There are four main volumetric representations that have been used in the literature:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>文献中使用了四种主要的体积表示：</p></div><ul>
<li>Binary occupancy grid. In this representation, a voxel is set to one if it belongs to the objects of interest, whereas background voxels are set to zero.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>二进制占用网格。在这种表示中，如果一个体素属于感兴趣的物体，则将其设置为1，而背景体素则设置为0。</li>
</ul></div><ul>
<li>Probabilistic occupancy grid. Each voxel in a probabilistic occupancy grid encodes its probability of belonging to the objects of interest.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>概率占用网格。概率占用网格中的每个体素编码其属于感兴趣物体的概率。</li>
</ul></div><ul>
<li>The Signed Distance Function (SDF). Each voxel encodes its signed distance to the closest surface point. It is negative if the voxel is located inside the object and positive otherwise.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>符号距离函数(SDF)。每个体素编码其到最近表面点的符号距离。如果体素位于物体内部，则为负值，否则为正值。</li>
</ul></div><ul>
<li>Truncated Signed Distance Function (TSDF). Introduced by Curless and Levoy [37], TSDF is computed by first estimating distances along the lines of sight of a range sensor, forming a projective signed distance field, and then truncating the field at small negative and positive values.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>截断符号距离函数(TSDF)。由Curless和Levoy[37]提出，TSDF通过首先沿着范围传感器的视线估计距离，形成投影符号距离场，然后在小的负值和正值处截断该场来计算。</li>
</ul></div><p>Probabilistic occupancy grids are particularly suitable for machine learning algorithms which output likelihoods. SDFs provide an unambiguous estimate of surface positions and normal directions. However, they are not trivial to construct from partial data such as depth maps. TSDFs sacrifice the full signed distance field that extends indefinitely away from the surface geometry, but allow for local updates of the field based on partial observations. They are suitable for reconstructing \(3\mathrm{D}\) volumes from a set of depth maps [26], |31|, |35|, |38|.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>概率占用网格特别适合输出可能性的机器学习算法。SDF（签名距离场）提供了表面位置和法线方向的明确估计。然而，从深度图等部分数据构建它们并不简单。TSDF（截断签名距离场）牺牲了从表面几何体无限延伸的完整签名距离场，但允许根据部分观察对场进行局部更新。它们适合从一组深度图重建\(3\mathrm{D}\)体积[26]、|31|、|35|、|38|。</p></div><p>In general, volumetric representations are created by regular sampling of the volume around the objects. Knyaz et al. [30] introduced a representation method called Frustum Voxel Model or Fruxel, which combines the depth representation with voxel grids. It uses the slices of the camera's 3D frustum to build the voxel space, and thus provides precise alignment of voxel slices with the contours in the input image.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>一般来说，体积表示是通过对物体周围的体积进行规则采样来创建的。Knyaz等人[30]提出了一种称为视锥体体素模型（Frustum Voxel Model）或Fruxel的表示方法，它将深度表示与体素网格结合起来。它使用相机3D视锥体的切片来构建体素空间，从而提供体素切片与输入图像轮廓的精确对齐。</p></div><p>Also, common SDF and TSDF representations are discre-tised into a regular grid. Recently, however, Park et al. [39] proposed Deep SDF (deepSDF), a generative deep learning model that produces a continuous SDF field from an input point cloud. Unlike the traditional SDF representation, DeepSDF can handle noisy and incomplete data. It can also represent an entire class of shape</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>此外，常见的SDF和TSDF表示被离散化为规则网格。然而，最近，Park等人[39]提出了深度SDF（Deep SDF），这是一种生成性深度学习模型，可以从输入点云生成连续的SDF场。与传统的SDF表示不同，深度SDF可以处理噪声和不完整数据。它还可以表示一整类形状。</p></div><!-- Media --><p>TABLE 3: Taxonomy of the various volumetric decoders used in the literature. Number in parentheses are the corresponding section numbers. MDN: Mixture Density Network. BBX: Bounding Box primitives. Part.: partitioning.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表3：文献中使用的各种体积解码器的分类。括号中的数字是相应的章节编号。MDN：混合密度网络。BBX：边界框原语。Part.：分区。</p></div><table><tbody><tr><td rowspan="5"></td><td colspan="2">Representation (4.1)</td><td colspan="6">Resolution</td><td colspan="2">Architecture</td></tr><tr><td rowspan="3">Sampling</td><td rowspan="3">Content</td><td rowspan="4">Low res: \( {32}^{3} \) . \( {64}^{3} \) 4.2)</td><td colspan="5">High resolution (4.3)</td><td rowspan="3">Network</td><td rowspan="3">Intermediation (6.1)</td></tr><tr><td colspan="2">Space part. (4.3.1)</td><td rowspan="2">Shape part. (4.3.3)</td><td>Subspace</td><td rowspan="2">Refinement (4.3.5)</td></tr><tr><td>Fixed Octree</td><td>Learned Octree</td><td>param. (4.3.4)</td></tr><tr><td>Regular, Fruxel, Adaptive</td><td>Occupancy, SDF, TSDF</td><td>Normal, O-CNN, OctNet</td><td>HSP, OGN, Patch-guide</td><td>Parts, Patches</td><td>PCA, DCT</td><td>Upsampling, Volume slicing, Patch synthesis, Patch refinement</td><td>FC, UpConv.</td><td>(1) image \( \rightarrow \) voxels, (2) image \( \rightarrow \) (2.5D, silh.) \( \rightarrow \) voxels</td></tr><tr><td>7</td><td>Regular</td><td>Occupancy</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>LSTM + UpConv</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>25</td><td>Regular</td><td>Occupancy</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>17</td><td>Regular</td><td>Occupancy</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>4</td><td>Regular</td><td>Occupancy</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>6</td><td>Regular</td><td>Occupancy</td><td>✓</td><td>\( {128}^{3} \)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>(2)</td></tr><tr><td>26</td><td>Regular</td><td>SDF</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>patch synthesis</td><td>UpConv</td><td>scans \( \rightarrow \) voxels</td></tr><tr><td>27</td><td>Regular</td><td>Occupancy</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>12</td><td>Regular</td><td>Occupancy</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>DCT</td><td>-</td><td>IDCT</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>18</td><td>Regular</td><td>Occupancy</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>28</td><td>Regular</td><td>Occupancy</td><td>-</td><td>\( {128}^{3} \)</td><td>-</td><td>-</td><td>-</td><td>Volume slicing</td><td>\( \mathrm{{CNN}} \rightarrow  \mathrm{{LSTM}} \rightarrow  \mathrm{{CNN}} \)</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>24</td><td>Regular</td><td>Occupancy</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>29</td><td>Regular</td><td>TSDF</td><td>-</td><td>-</td><td>-</td><td>Parts</td><td>-</td><td>-</td><td>LSTM + MDN</td><td>depth \( \rightarrow \) BBX</td></tr><tr><td>30</td><td>Fruxel</td><td>Occupancy</td><td>-</td><td>\( {128}^{3} \)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>31</td><td>Regular</td><td>TSDF</td><td>-</td><td>-</td><td>-</td><td></td><td>PCA</td><td>-</td><td>FC</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>32</td><td>Adaptive</td><td>-</td><td>-</td><td>O-CNN</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>33</td><td>Adaptive</td><td>-</td><td>-</td><td></td><td>OGN</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>8</td><td>Regular</td><td>Occupancy</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>9</td><td>Regular</td><td>Occupancy</td><td>-</td><td>\( {128}^{3} \)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>(2)</td></tr><tr><td>34</td><td>Adaptive</td><td>Occupancy</td><td>-</td><td>O-CNN</td><td>patch-guided</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>13</td><td>Regular</td><td>Occupancy</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>35</td><td>Regular</td><td>TSDF</td><td>-</td><td>OctNet</td><td>-</td><td>-</td><td></td><td>Global to local</td><td>UpConv</td><td>scans \( \rightarrow \) voxels</td></tr><tr><td>11</td><td>Regular</td><td>Occupancy</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>image \( \rightarrow \) voxels</td></tr><tr><td>36</td><td>Adaptive</td><td>Occupancy</td><td>-</td><td>-</td><td>HSP</td><td>-</td><td>-</td><td>-</td><td>UpConv nets</td><td>image \( \rightarrow \) voxels</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="5"></td><td colspan="2">表示 (4.1)</td><td colspan="6">分辨率</td><td colspan="2">架构</td></tr><tr><td rowspan="3">采样</td><td rowspan="3">内容</td><td rowspan="4">低分辨率: \( {32}^{3} \) . \( {64}^{3} \) 4.2)</td><td colspan="5">高分辨率 (4.3)</td><td rowspan="3">网络</td><td rowspan="3">中介 (6.1)</td></tr><tr><td colspan="2">空间部分 (4.3.1)</td><td rowspan="2">形状部分 (4.3.3)</td><td>子空间</td><td rowspan="2">细化 (4.3.5)</td></tr><tr><td>固定八叉树</td><td>学习八叉树</td><td>参数 (4.3.4)</td></tr><tr><td>规则、Fruxel、自适应</td><td>占用、SDF、TSDF</td><td>法线、O-CNN、OctNet</td><td>HSP、OGN、补丁引导</td><td>部分、补丁</td><td>PCA、DCT</td><td>上采样、体积切片、补丁合成、补丁细化</td><td>FC、UpConv。</td><td>(1) 图像 \( \rightarrow \) 体素, (2) 图像 \( \rightarrow \) (2.5D, 轮廓) \( \rightarrow \) 体素</td></tr><tr><td>7</td><td>规则</td><td>占用</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>LSTM + UpConv</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>25</td><td>规则</td><td>占用</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>17</td><td>规则</td><td>占用</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>4</td><td>规则</td><td>占用</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>6</td><td>规则</td><td>占用</td><td>✓</td><td>\( {128}^{3} \)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>(2)</td></tr><tr><td>26</td><td>规则</td><td>SDF</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>补丁合成</td><td>UpConv</td><td>扫描 \( \rightarrow \) 体素</td></tr><tr><td>27</td><td>规则</td><td>占用</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>12</td><td>规则</td><td>占用</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>DCT</td><td>-</td><td>IDCT</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>18</td><td>规则</td><td>占用</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>28</td><td>规则</td><td>占用</td><td>-</td><td>\( {128}^{3} \)</td><td>-</td><td>-</td><td>-</td><td>体积切片</td><td>\( \mathrm{{CNN}} \rightarrow  \mathrm{{LSTM}} \rightarrow  \mathrm{{CNN}} \)</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>24</td><td>规则</td><td>占用</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>29</td><td>规则</td><td>TSDF</td><td>-</td><td>-</td><td>-</td><td>部分</td><td>-</td><td>-</td><td>LSTM + MDN</td><td>深度 \( \rightarrow \) BBX</td></tr><tr><td>30</td><td>Fruxel</td><td>占用</td><td>-</td><td>\( {128}^{3} \)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>31</td><td>规则</td><td>TSDF</td><td>-</td><td>-</td><td>-</td><td></td><td>PCA</td><td>-</td><td>FC</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>32</td><td>自适应</td><td>-</td><td>-</td><td>O-CNN</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>33</td><td>自适应</td><td>-</td><td>-</td><td></td><td>OGN</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>8</td><td>规则</td><td>占用</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>9</td><td>规则</td><td>占用</td><td>-</td><td>\( {128}^{3} \)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>(2)</td></tr><tr><td>34</td><td>自适应</td><td>占用</td><td>-</td><td>O-CNN</td><td>补丁引导</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>13</td><td>规则</td><td>占用</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>35</td><td>规则</td><td>TSDF</td><td>-</td><td>OctNet</td><td>-</td><td>-</td><td></td><td>从全局到局部</td><td>UpConv</td><td>扫描 \( \rightarrow \) 体素</td></tr><tr><td>11</td><td>规则</td><td>占用</td><td>✓</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>UpConv</td><td>图像 \( \rightarrow \) 体素</td></tr><tr><td>36</td><td>自适应</td><td>占用</td><td>-</td><td>-</td><td>HSP</td><td>-</td><td>-</td><td>-</td><td>UpConv 网络</td><td>图像 \( \rightarrow \) 体素</td></tr></tbody></table></div><!-- Media --><h3>4.2 Low resolution 3D volume reconstruction</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.2 低分辨率三维体积重建</h3></div><p>Once a compact vector representation of the input is learned using an encoder, the next step is to learn the decoding function \(g\) ,known as the generator or the generative model,which maps the vector representation into a volumetric voxel grid. The standard approach uses a convolutional decoder, called also up-convolutional network, which mirrors the convolutional encoder. Wu et al. [3] were among the first to propose this methodology to reconstruct 3D volumes from depth maps. Wu et al. [6] proposed a two-stage reconstruction network called MarrNet. The first stage uses an encoder-decoder architecture to reconstruct, from an input image, the depth map, the normal map, and the silhouette map. These three maps, referred to as 2.5 sketches, are then used as input to another encoder-decoder architecture, which regresses a volumetric \(3\mathrm{D}\) shape. The network has been later extended by Sun et al. [9] to also regress the pose of the input. The main advantage of this two-stage approach is that, compared to full 3D models, depth maps, normal maps, and silhouette maps are much easier to recover from 2D images. Likewise, 3D models are much easier to recover from these three modalities than from 2D images alone. This method, however, fails to reconstruct complex, thin structures.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>一旦使用编码器学习了输入的紧凑向量表示，下一步就是学习解码函数\(g\)，即生成器或生成模型，它将向量表示映射到体素网格。标准方法使用卷积解码器，也称为上卷积网络，镜像卷积编码器。吴等人[3]是最早提出这种方法以从深度图重建三维体积的研究者之一。吴等人[6]提出了一种称为MarrNet的两阶段重建网络。第一阶段使用编码器-解码器架构，从输入图像重建深度图、法线图和轮廓图。这三幅图被称为2.5草图，随后作为输入用于另一个编码器-解码器架构，该架构回归出一个体积\(3\mathrm{D}\)形状。该网络后来被孙等人[9]扩展，以回归输入的姿态。这种两阶段方法的主要优点是，与完整的三维模型相比，深度图、法线图和轮廓图从二维图像中恢复要容易得多。同样，从这三种模态恢复三维模型也比仅从二维图像恢复要容易得多。然而，该方法在重建复杂、细薄结构时失败。</p></div><p>Wu et al.'s work [3] has led to several extensions [7], [8], [17], [27], [40]. In particular, recent works tried to directly regress the 3D voxel grid [8], [11], [13], [18] without intermediation. Tulsiani et al. [8], and later in [11], used a decoder composed of \(3\mathrm{D}\) upconvolution layers to predict the voxel occupancy probabilities. Liu et al. [18] used a 3D up-convolutional neural network, followed by an element-wise logistic sigmoid, to decode the learned latent features into a 3D occupancy probability grid. These methods have been successful in performing \(3\mathrm{D}\) reconstruction from a single or a collection of images captured with uncalibrated cameras. Their main advantage is that the deep learning architectures proposed for the analysis of \(2\mathrm{D}\) images can be easily adapted to \(3\mathrm{D}\) models by replacing the \(2\mathrm{D}\) up-convolutions in the decoder with 3D up-convolutions, which also can be efficiently implemented on the GPU. However, given the computational complexity and memory requirements, these methods produce low resolution grids,usually of size \({32}^{3}\) or \({64}^{3}\) . As such,they fail to recover fine details.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>吴等人的工作[3]导致了几项扩展[7]，[8]，[17]，[27]，[40]。特别是，最近的研究尝试直接回归三维体素网格[8]，[11]，[13]，[18]，而不进行中介。Tulsiani等人[8]，以及后来的[11]，使用由\(3\mathrm{D}\)上卷积层组成的解码器来预测体素占用概率。刘等人[18]使用了一个三维上卷积神经网络，后接逐元素逻辑 sigmoid，以将学习到的潜在特征解码为三维占用概率网格。这些方法在从单个或一组使用未校准相机拍摄的图像中执行\(3\mathrm{D}\)重建方面取得了成功。它们的主要优点是，针对\(2\mathrm{D}\)图像分析提出的深度学习架构可以通过将解码器中的\(2\mathrm{D}\)上卷积替换为三维上卷积，轻松适应\(3\mathrm{D}\)模型，这也可以在GPU上高效实现。然而，考虑到计算复杂性和内存要求，这些方法产生的网格分辨率较低，通常为\({32}^{3}\)或\({64}^{3}\)。因此，它们无法恢复细节。</p></div><h3>4.3 High resolution 3D volume reconstruction</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.3 高分辨率三维体积重建</h3></div><p>There have been attempts to upscale the deep learning architectures for high resolution volumetric reconstruction. For instance, Wu et al. [6] were able to reconstruct voxel grids of size \({128}^{3}\) by simply expanding the network. Volumetric grids, however, are very expensive in terms of memory requirements, which grow cubically with the grid resolution. This section reviews some of the techniques that have been used to infer high resolution volumetric grids, while keeping the computational and memory requirements tractable. We classify these methods into four categories based on whether they use space partitioning, shape partitioning, subspace parameterization, or coarse-to-fine refinement strategies.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>已经有尝试对深度学习架构进行升级，以实现高分辨率的体积重建。例如，吴等人[6]通过简单扩展网络能够重建大小为\({128}^{3}\)的体素网格。然而，体积网格在内存要求方面非常昂贵，内存需求随着网格分辨率的立方增长。本节回顾了一些用于推断高分辨率体积网格的技术，同时保持计算和内存要求在可控范围内。我们根据这些方法是否使用空间划分、形状划分、子空间参数化或粗到细的细化策略，将其分为四类。</p></div><h4>4.3.1 Space partitioning</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.3.1 空间划分</h4></div><p>While regular volumetric grids facilitate convolutional operations, they are very sparse since surface elements are contained in few voxels. Several papers have exploited this sparsity to address the resolution problem [32], [33], [41], [42]. They were able to reconstruct 3D volumetric grids of size \({256}^{3}\) to \({512}^{3}\) by using space partitioning techniques such as octrees. There are, however, two main challenging issues when using octree structures for deep-learning based reconstruction. The first one is computational since convolutional operations are easier to implement (especially on GPUs) when operating on regular grids. For this purpose, Wang et al. [32] designed O-CNN, a novel octree data structure, to efficiently store the octant information and CNN features into the graphics memory and execute the entire training and evaluation on the GPU. O-CNN supports various CNN structures and works with 3D shapes of different representations. By restraining the computations on the octants occupied by \(3\mathrm{D}\) surfaces,the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>虽然常规体积网格便于卷积操作，但它们非常稀疏，因为表面元素仅包含在少数体素中。几篇论文利用这种稀疏性来解决分辨率问题 [32]，[33]，[41]，[42]。他们能够通过使用空间划分技术（如八叉树）重建大小为\({256}^{3}\)到\({512}^{3}\)的3D体积网格。然而，在使用八叉树结构进行基于深度学习的重建时，有两个主要的挑战问题。第一个是计算上的，因为在常规网格上进行卷积操作更容易实现（特别是在GPU上）。为此，Wang等人 [32] 设计了O-CNN，这是一种新型的八叉树数据结构，能够高效地将八分体信息和CNN特征存储到图形内存中，并在GPU上执行整个训练和评估。O-CNN支持各种CNN结构，并与不同表示的3D形状一起工作。通过限制对被\(3\mathrm{D}\)表面占据的八分体的计算，O-CNN的内存和计算成本随着八叉树深度的增加而呈平方增长，这使得3D CNN在高分辨率3D模型中变得可行。</p></div><!-- Media --><!-- figureText: Input image Octree Octree Octree Level 1 Level 2 (c) Octree Generative Network (OGN) [33] Hierarchical (a) Octree Network (OctNet) [41]. (b) Hierarchical Space Partionning (HSP) [36 --><img src="https://cdn.noedgeai.com/bo_d163t43ef24c73d1le4g_5.jpg?x=134&#x26;y=130&#x26;w=1528&#x26;h=300&#x26;r=0"><p>Fig. 1: Space partitioning. OctNet [41] is a hybrid grid-octree, which enables deep and high-resolution 3D CNNs. High-resolution octrees can also be generated, progressively, in a depth-first [36] or breadth-first [33] manner.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图1：空间划分。OctNet [41] 是一种混合网格-八叉树，能够实现深度和高分辨率的3D CNN。高分辨率的八叉树也可以以深度优先 [36] 或广度优先 [33] 的方式逐步生成。</p></div><!-- Media --><p>The second challenge stems from the fact that the octree structure is object-dependent. Thus, ideally, the deep neural network needs to learn how to infer both the structure of the octree and its content. In this section, we will discuss how these challenges have been addressed in the literature.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>第二个挑战源于八叉树结构依赖于对象。因此，理想情况下，深度神经网络需要学习如何推断八叉树的结构及其内容。在本节中，我们将讨论文献中如何解决这些挑战。</p></div><p>4.3.1.1 Using pre-defined octree structures: The simplest approach is to assume that, at runtime, the structure of the octree is known. This is fine for applications such as semantic segmentation where the structure of the output octree can be set to be identical to that of the input. However, in many important scenarios, e.g., 3D reconstruction, shape modeling, and RGB-D fusion, the structure of the octree is not known in advance and must be predicted. To this end, Riegler et al. [41] proposed a hybrid grid-octree structure called OctNet (Fig. 1-(a)). The key idea is to restrict the maximal depth of an octree to a small number, e.g., three, and place several such shallow octrees on a regular grid. This representation enables 3D convolutional networks that are both deep and of high resolution. However, at test time, Riegler et al. [41] assume that the structure of the individual octrees is known. Thus, although the method is able to reconstruct \(3\mathrm{D}\) volumes at a resolution of \({256}^{3}\) ,it lacks flexibility since different types of objects may require different training.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>4.3.1.1 使用预定义的八叉树结构：最简单的方法是假设在运行时，八叉树的结构是已知的。这对于语义分割等应用是可以的，因为输出八叉树的结构可以设置为与输入相同。然而，在许多重要场景中，例如3D重建、形状建模和RGB-D融合，八叉树的结构是事先未知的，必须进行预测。为此，Riegler等人 [41] 提出了一个名为OctNet的混合网格-八叉树结构（图1-(a)）。关键思想是将八叉树的最大深度限制为一个小数字，例如三，并在常规网格上放置几个这样的浅八叉树。这种表示使得3D卷积网络既深又高分辨率。然而，在测试时，Riegler等人 [41] 假设单个八叉树的结构是已知的。因此，尽管该方法能够以\(3\mathrm{D}\)的分辨率重建体积，但由于不同类型的对象可能需要不同的训练，因此缺乏灵活性。</p></div><p>4.3.1.2 Learning the octree structure : Ideally, the octree structure and its content should be simultaneously estimated. This can be done as follows;</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>4.3.1.2 学习八叉树结构：理想情况下，八叉树结构及其内容应该同时估计。这可以通过以下方式完成；</p></div><ul>
<li>First, the input is encoded into a compact feature vector using a convolutional encoder (Section 3).</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>首先，使用卷积编码器将输入编码为紧凑的特征向量（第3节）。</li>
</ul></div><ul>
<li>Next, the feature vector is decoded using a standard up-convolutional network. This results in a coarse volumetric reconstruction of the input, usually of resolution \({32}^{3}\) (Section 4.2).</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>接下来，使用标准的上卷积网络解码特征向量。这会导致输入的粗略体积重建，通常分辨率为\({32}^{3}\)（第4.2节）。</li>
</ul></div><ul>
<li>The reconstructed volume, which forms the root of the octree, is subdivided into 8 octants. Octants with boundary voxels are upsampled and further processed, using an up-convolutional network, to refine the reconstruction of the regions in that octant.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>重建的体积形成八叉树的根，分为8个八分体。具有边界体素的八分体被上采样并进一步处理，使用上卷积网络来细化该八分体中区域的重建。</li>
</ul></div><ul>
<li>The octants are processed recursively until the desired resolution is reached.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>八分体被递归处理，直到达到所需的分辨率。</li>
</ul></div><p>Häne et al. [36] introduced the Hierarchical Surface Prediction (HSP), see Fig. 1-(b), which used the approach described above to reconstruct volumetric grids of resolution up to \({256}^{3}\) . In this approach,the octree is explored in depth-first manner. Tatarchenko et al. [33], on the other hand, proposed the Octree Generating Networks (OGN), which follows the same idea but the octree is explored in breadth-first manner, see Fig. 1-(c). As such, OGN produces a hierarchical reconstruction of the 3D shape. The approach was able to reconstruct volumetric grids of size \({512}^{3}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Häne等人 [36] 引入了层次表面预测（HSP），见图1-(b)，该方法使用上述方法重建分辨率高达\({256}^{3}\)的体积网格。在这种方法中，八叉树以深度优先的方式进行探索。另一方面，Tatarchenko等人 [33] 提出了八叉树生成网络（OGN），该方法遵循相同的思路，但八叉树以广度优先的方式进行探索，见图1-(c)。因此，OGN生成了3D形状的层次重建。该方法能够重建大小为\({512}^{3}\)的体积网格。</p></div><p>Wang et al. [34] introduced a patch-guided partitioning strategy. The core idea is to represent a 3D shape with an octree where each of its leaf nodes approximates a planar surface. To infer such structure from a latent representation, Wang et al. [34] used a cascade of decoders, one per octree level. At each octree level, a decoder predicts the planar patch within each cell, and a predictor (composed of fully connected layers) predicts the patch approximation status for each octant, i.e., whether the cell is "empty", "surface well approximated" with a plane, and "surface poorly approximated". Cells of poorly approximated surface patches are further subdivided and processed by the next level. This approach reduces the memory requirements from 6.4GB for volumetric grids of size \({256}^{3}\left\lbrack  {32}\right\rbrack\) to \({1.7}\mathrm{{GB}}\) ,and the computation time from 1.39s to 0.30s, while maintaining the same level of accuracy. Its main limitation is that adjacent patches are not seamlessly reconstructed. Also, since a plane is fitted to each octree cell, it does not approximate well curved surfaces.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Wang等人[34]提出了一种基于补丁引导的分区策略。其核心思想是用八叉树表示三维形状，其中每个叶节点近似一个平面表面。为了从潜在表示中推断出这种结构，Wang等人[34]使用了一系列解码器，每个八叉树层一个。在每个八叉树层，解码器预测每个单元内的平面补丁，而预测器（由全连接层组成）预测每个八分体的补丁近似状态，即单元是“空的”、“表面良好近似”的平面和“表面近似差”。近似差的表面补丁的单元进一步细分，并由下一个层处理。这种方法将体素网格的内存需求从6.4GB减少到\({1.7}\mathrm{{GB}}\)，计算时间从1.39秒减少到0.30秒，同时保持相同的准确性。其主要限制是相邻补丁无法无缝重建。此外，由于每个八叉树单元都拟合一个平面，因此对曲面近似效果不佳。</p></div><h4>4.3.2 Occupancy networks</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.3.2 占用网络</h4></div><p>While it is possible to reduce the memory footprint by using various space partitionning techniques, these approaches lead to complex implementations and existing data-adaptive algorithms are still limited to relatively small voxel grids \(\left( {256}^{3}\right.\) to \(\left. {512}^{2}\right)\) . Recently,several papers proposed to learn implicit representations of \(3\mathrm{D}\) shapes using deep neural networks. For instance, Chen and Zhang [43] proposed a decoder that takes the latent representation of a shape and a 3D point,and returns a value indicating whether the point is outside or inside the shape. The network can be used to reconstruct high resolution 3D volumetric representations. However, when retrieving generated shapes, volumetric CNNs only need one shot to obtain the voxel model, while this method needs to pass every point in the voxel grid to the network to obtain its value. Thus, the time required to generate a sample depends on the sampling resolution.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>虽然可以通过使用各种空间分区技术来减少内存占用，但这些方法导致实现复杂，现有的数据自适应算法仍然仅限于相对较小的体素网格\(\left( {256}^{3}\right.\)到\(\left. {512}^{2}\right)\)。最近，几篇论文提出使用深度神经网络学习\(3\mathrm{D}\)形状的隐式表示。例如，Chen和Zhang[43]提出了一种解码器，该解码器接受形状的潜在表示和一个三维点，并返回一个值，指示该点是在形状外部还是内部。该网络可用于重建高分辨率的三维体积表示。然而，在检索生成的形状时，体积CNN只需一次即可获得体素模型，而该方法需要将体素网格中的每个点传递给网络以获取其值。因此，生成样本所需的时间取决于采样分辨率。</p></div><p>Tatarchenko et al. [44] introduced occupancy networks that implicitly represent the \(3\mathrm{D}\) surface of an object as the continuous decision boundary of a deep neural network classifier. Instead of predicting a voxelized representation at a fixed resolution, the approach predicts the complete occupancy function with a neural network that can be evaluated at any arbitrary resolution. This drastically reduces the memory footprint during training. At inference time, a mesh can be extracted from the learned model using a simple multi-resolution isosurface extraction algorithm.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Tatarchenko等人[44]引入了占用网络，该网络隐式表示对象的\(3\mathrm{D}\)表面作为深度神经网络分类器的连续决策边界。该方法不是在固定分辨率下预测体素化表示，而是使用神经网络预测完整的占用函数，该函数可以在任何任意分辨率下进行评估。这大大减少了训练期间的内存占用。在推理时，可以使用简单的多分辨率等值面提取算法从学习到的模型中提取网格。</p></div><h4>4.3.3 Shape partitioning</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.3.3 形状分区</h4></div><p>Instead of partitionning the volumetric space in which the 3D shapes are embedded, an alternative approach is to consider the shape as an arrangement of geometric parts, reconstruct the individual parts independently from each other, and then stitch the parts together to form the complete \(3\mathrm{D}\) shape. There has been a few works which attempted this approach. For instance, Li et al. [42] only generate voxel representations at the part level. They proposed a Generative Recursive Autoencoder for Shape Structure (GRASS). The idea is to split the problem into two steps. The first step uses a Recursive Neural Nets (RvNN) encoder-decoder architecture coupled with a Generative Adversarial Network to learn how to best organize a shape structure into a symmetry hierarchy and how to synthesize the part arrangements. The second step learns, using another generative model, how to synthesize the geometry of each part, represented as a voxel grid of size \({32}^{3}\) . Thus,although the part generator network synthesizes the \(3\mathrm{D}\) geometry of parts at only \({32}^{3}\) resolution,the fact that individual parts are treated separately enables the reconstruction of 3D shapes at high resolution.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>与其在三维形状嵌入的体积空间中进行分区，不如考虑将形状视为几何部分的排列，独立重建各个部分，然后将部分拼接在一起形成完整的\(3\mathrm{D}\)形状。已经有一些工作尝试这种方法。例如，Li等人[42]仅在部分级别生成体素表示。他们提出了一种用于形状结构的生成递归自编码器（GRASS）。其思想是将问题分为两个步骤。第一步使用递归神经网络（RvNN）编码器-解码器架构与生成对抗网络相结合，学习如何最好地将形状结构组织成对称层次，以及如何合成部分排列。第二步使用另一种生成模型学习如何合成每个部分的几何形状，表示为大小为\({32}^{3}\)的体素网格。因此，尽管部分生成网络仅在\({32}^{3}\)分辨率下合成\(3\mathrm{D}\)部分的几何形状，但由于各个部分被单独处理，使得能够以高分辨率重建三维形状。</p></div><p>Zou et al. [29] reconstruct a 3D object as a collection of primitives using a generative recurrent neural network called 3D-PRNN. The architecture transforms the input into a feature vector of size 32 via an encoder network. Then, a recurrent generator composed of stacks of Long Short-Term Memory (LSTM) and a Mixture Density Network (MDN) sequentially predicts from the feature vector the different parts of the shape. At each time step, the network predicts a set of primitives conditioned on both the feature vector and the previously estimated single primitive. The predicted parts are then combined together to form the reconstruction result. This approach predicts only an abstracted representation in the form of cuboids. Coupling it with volumetric-based reconstruction techniques, which would focus on individual cuboids, could lead to a refined \(3\mathrm{D}\) reconstruction at the part level.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Zou等人[29]使用一种称为3D-PRNN的生成递归神经网络重建3D对象，作为一组原始体的集合。该架构通过编码器网络将输入转换为大小为32的特征向量。然后，由堆叠的长短期记忆（LSTM）和混合密度网络（MDN）组成的递归生成器顺序地从特征向量预测形状的不同部分。在每个时间步，网络根据特征向量和先前估计的单个原始体预测一组原始体。然后将预测的部分组合在一起形成重建结果。这种方法仅预测以立方体形式表示的抽象表示。将其与基于体积的重建技术结合，后者将重点放在单个立方体上，可能会导致在部分级别上更精细的\(3\mathrm{D}\)重建。</p></div><h4>4.3.4 Subspace parameterization</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.3.4 子空间参数化</h4></div><p>The space of all possible shapes can be parameterized using a set of orthogonal basis \(\mathbf{B} = \left\{  {{\mathbf{b}}_{1},\ldots ,{\mathbf{b}}_{n}}\right\}\) . Every shape \(X\) can then be represented as a linear combination of the bases,i.e., \(X = \mathop{\sum }\limits_{{i = 1}}^{n}{\alpha }_{i}{\mathbf{b}}_{i}\) ,with \({\alpha }_{i} \in  \mathbb{R}\) . This formulation simplifies the reconstruction problem; instead of trying to learn how to reconstruct the volumetric grid \(V\) ,one can design a decoder composed of fully connected layers to estimate the coefficients \({\alpha }_{i},i = 1,\ldots ,n\) from the latent representation,and then recover the complete \(3\mathrm{D}\) volume. Johnston et al. [12] used the Discrete Cosine Transform-II (DCT-II) to define \(\mathbf{B}\) . They then proposed a convolutional encoder to predict the low frequency DCT-II coefficients \({\alpha }_{i}\) . These coefficients are then converted by a simple Inverse DCT (IDCT) linear transform, which replaces the decoding network, to a solid 3D volume. This had a profound impact on the computational cost of training and inference: using \(n = {20}^{3}\) DCT coefficients,the network is able to reconstruct surfaces at volumetric grids of size \({128}^{3}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>所有可能形状的空间可以使用一组正交基\(\mathbf{B} = \left\{  {{\mathbf{b}}_{1},\ldots ,{\mathbf{b}}_{n}}\right\}\)进行参数化。每个形状\(X\)可以表示为基的线性组合，即\(X = \mathop{\sum }\limits_{{i = 1}}^{n}{\alpha }_{i}{\mathbf{b}}_{i}\)，具有\({\alpha }_{i} \in  \mathbb{R}\)。这种公式简化了重建问题；与其尝试学习如何重建体积网格\(V\)，不如设计一个由全连接层组成的解码器，从潜在表示中估计系数\({\alpha }_{i},i = 1,\ldots ,n\)，然后恢复完整的\(3\mathrm{D}\)体积。Johnston等人[12]使用离散余弦变换-II（DCT-II）来定义\(\mathbf{B}\)。然后，他们提出了一个卷积编码器来预测低频DCT-II系数\({\alpha }_{i}\)。这些系数通过简单的逆DCT（IDCT）线性变换转换，替代了解码网络，形成一个实心的3D体积。这对训练和推理的计算成本产生了深远的影响：使用\(n = {20}^{3}\) DCT系数，网络能够在大小为\({128}^{3}\)的体积网格上重建表面。</p></div><p>The main issue when using generic bases such as the DCT bases is that, in general, one requires a large number of basis elements to accurately represent complex \(3\mathrm{D}\) objects. In practice, we usually deal with objects of known categories, e.g., human faces and 3D human bodies, and usually, training data is available, see Section 8. As such, one can use Principal Component (PCA) bases, learned from the training data, to parameterize the space of shapes [31]. This would require a significantly smaller number of bases (in the order of 10 compared to the number of generic basis, which is in the order of thousands.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>使用通用基（如DCT基）时的主要问题是，通常需要大量基元素才能准确表示复杂\(3\mathrm{D}\)对象。在实践中，我们通常处理已知类别的对象，例如人脸和3D人体，并且通常有可用的训练数据，见第8节。因此，可以使用从训练数据中学习的主成分（PCA）基来参数化形状的空间[31]。这将需要显著更少的基（大约10个），而通用基的数量通常在数千个的数量级。</p></div><h4>4.3.5 Coarse-to-fine refinement</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.3.5 粗到细的细化</h4></div><p>Another way to improve the resolution of volumetric techniques is by using multi-staged approaches [26], [28], [35], [45], [46]. The first stage recovers a low resolution voxel grid,say \({32}^{3}\) ,using an encoder-decoder architecture. The subsequent stages, which function as upsampling networks, refine the reconstruction by focusing on local regions. Yang et al. [46] used an up-sampling module which simply consists of two up-convolutional layers. This simple up-sampling module upgrades the output 3D shape to a higher resolution of \({256}^{3}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>提高体积技术分辨率的另一种方法是使用多阶段方法[26]，[28]，[35]，[45]，[46]。第一阶段使用编码器-解码器架构恢复低分辨率体素网格，例如\({32}^{3}\)。后续阶段作为上采样网络，通过关注局部区域来细化重建。Yang等人[46]使用了一个简单的上采样模块，该模块仅由两个上卷积层组成。这个简单的上采样模块将输出的3D形状升级到更高的分辨率\({256}^{3}\)。</p></div><p>Wang et al. [28] treat the reconstructed coarse voxel grid as a sequence of images (or slices). The 3D object is then reconstructed slice by slice at high resolution. While this approach allows efficient refinement using 2D up-convolutions, the 3D shapes used for training should be consistently aligned so that the volumes can be sliced along the first principal direction. Also, reconstructing individual slices independently from each other may result in discontinuities and incoherences in the final volume. To capture the dependencies between the slices, Wang et al. [28] use a Long term Recurrent Convolutional Network (LRCN) [47] composed of a 3D encoder, an LSTM unit, and a 2D decoder. At each time,the \(3\mathrm{D}\) encoder processes five consecutive slices to produce a fixed-length vector representation as input to the LSTM. The output of the LSTM is passed to the 2D convolutional decoder to produce a high resolution image. The concatenation of the high-resolution 2D images forms the high-resolution output 3D volume.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>王等人 [28] 将重建的粗糙体素网格视为一系列图像（或切片）。然后以高分辨率逐切片重建三维物体。虽然这种方法允许使用二维上卷积进行高效的细化，但用于训练的三维形状应保持一致对齐，以便可以沿着第一主方向切片。此外，独立重建各个切片可能导致最终体积中的不连续性和不一致性。为了捕捉切片之间的依赖关系，王等人 [28] 使用了一个长短期记忆卷积网络（LRCN） [47]，该网络由一个三维编码器、一个LSTM单元和一个二维解码器组成。在每个时间点，\(3\mathrm{D}\) 编码器处理五个连续切片，以生成固定长度的向量表示作为LSTM的输入。LSTM的输出传递给二维卷积解码器，以生成高分辨率图像。高分辨率二维图像的连接形成高分辨率输出三维体积。</p></div><p>Instead of using volume slicing, other papers used additional CNN modules, which focus on regions that require refinement. For example, Dai et al. [26] firstly predict a coarse but complete shape volume of size \({32}^{3}\) and then refine it into a \({128}^{3}\) grid via an iterative volumetric patch synthesis process, which copy-pastes voxels from the k-nearest-neighbors retrieved from a database of 3D models. Han et al. [45] extend Dai et al.'s approach by introducing a local 3D CNN to perform patch-level surface refinement. Cao et al. [35], which recover in the first stage a volumetric grid of size \({128}^{3}\) ,take volumetric blocks of size \({16}^{3}\) and predict whether they require further refinement. Blocks that require refinement are resampled into \({512}^{3}\) and fed into another encoder-decoder for refinement, along with the initial coarse prediction to guide the refinement. Both subnetworks adopt the U-net architecture [48] while substituting convolution and pooling layers with the corresponding operations from OctNet [41].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>与体积切片不同，其他论文使用了额外的CNN模块，专注于需要细化的区域。例如，戴等人 [26] 首先预测一个粗糙但完整的形状体积，大小为 \({32}^{3}\)，然后通过迭代体积补丁合成过程将其细化为 \({128}^{3}\) 网格，该过程从3D模型数据库中复制粘贴来自k近邻的体素。韩等人 [45] 通过引入局部3D CNN来扩展戴等人的方法，以进行补丁级表面细化。曹等人 [35] 在第一阶段恢复大小为 \({128}^{3}\) 的体积网格，采用大小为 \({16}^{3}\) 的体积块，并预测它们是否需要进一步细化。需要细化的块被重新采样为 \({512}^{3}\)，并与初始粗略预测一起输入到另一个编码器-解码器中进行细化，以指导细化。两个子网络都采用U-net架构 [48]，同时用OctNet [41] 的相应操作替代卷积和池化层。</p></div><p>Note that these methods need separate and sometimes time-consuming steps before local inference. For example, Dai et al. [26] require nearest neighbor searches from a 3D database. Han et al. [45] require 3D boundary detection while Cao et al. [35] require assessing whether a block requires further refinement or not.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>请注意，这些方法在局部推理之前需要单独且有时耗时的步骤。例如，戴等人 [26] 需要从3D数据库中进行最近邻搜索。韩等人 [45] 需要进行3D边界检测，而曹等人 [35] 需要评估一个块是否需要进一步细化。</p></div><h3>4.4 Deep marching cubes</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.4 深度行进立方体</h3></div><p>While volumetric representations can handle 3D shapes of arbitrary topologies, they require a post processing step, e.g., marching cubes [49], to retrieve the actual 3D surface mesh, which is the quantity of interest in 3D reconstruction. As such, the whole pipeline cannot be trained end-to-end. To overcome this limitation, Liao et al. [50] introduced the Deep Marching Cubes, an end-to-end trainable network, which predicts explicit surface representations of arbitrary topology. They use a modified differentiable representation, which separates the mesh topology from the geometry. The network is composed of an encoder and a two-branch decoder. Instead of predicting signed distance values, the first branch predicts the probability of occupancy for each voxel. The mesh topology is then implicitly (and probabilistically) defined by the state of the occupancy variables at its corners. The second branch of the decoder predicts a vertex location for every edge of each cell. The combination of both implicitly-defined topology and vertex location defines a distribution over meshes that is differentiable and can be used for back propagation. While the approach is trainable end-to-end,it is limited to low resolution grids of size \({32}^{3}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>虽然体积表示可以处理任意拓扑的三维形状，但它们需要一个后处理步骤，例如行进立方体 [49]，以检索实际的三维表面网格，这是三维重建中关注的量。因此，整个流程无法端到端训练。为了克服这一限制，廖等人 [50] 引入了深度行进立方体，这是一种可端到端训练的网络，能够预测任意拓扑的显式表面表示。他们使用了一种修改过的可微分表示，将网格拓扑与几何分离。该网络由一个编码器和一个双分支解码器组成。第一个分支预测每个体素的占用概率，而不是预测带符号的距离值。网格拓扑则通过占用变量在其角落的状态隐式（且概率性）定义。解码器的第二个分支为每个单元的每条边预测一个顶点位置。两者隐式定义的拓扑和顶点位置的组合定义了一个可微分的网格分布，可以用于反向传播。尽管该方法可以端到端训练，但仅限于大小为 \({32}^{3}\) 的低分辨率网格。</p></div><p>Instead of directly estimating high resolution volumetric grids, some methods produce multiview depth maps, which are fused into an output volume. The main advantage is that, in the decoding stage, one can use 2D convolutions, which are more efficient, in terms of computation and memory storage, than 3D convolutions. Their main limitation, however, is that depth maps only encode the external surface. To capture internal structures, Richter et al. [51] introduced Matryoshka Networks,which use \(L\) nested depth layers; the shape is recursively reconstructed by first fusing the depth maps in the first layer, then subtracting shapes in even layers, and adding shapes in odd layers. The method is able to reconstruct volumetric grids of size \({256}^{3}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>一些方法并不是直接估计高分辨率体积网格，而是生成多视角深度图，这些深度图被融合成输出体积。其主要优点在于，在解码阶段，可以使用二维卷积，这在计算和内存存储方面比三维卷积更高效。然而，它们的主要限制在于深度图仅编码外部表面。为了捕捉内部结构，里希特等人 [51] 引入了套娃网络，使用 \(L\) 嵌套深度层；该形状通过首先融合第一层的深度图，然后在偶数层中减去形状，在奇数层中添加形状来递归重建。该方法能够重建大小为 \({256}^{3}\) 的体积网格。</p></div><h2>5 3D SURFACE DECODING</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>5 三维表面解码</h2></div><p>Volumetric representation-based methods are computationally very wasteful since information is rich only on or near the surfaces of \(3\mathrm{D}\) shapes. The main challenge when working directly with surfaces is that common representations such as meshes or point clouds are not regularly structured and thus, they do not easily fit into deep learning architectures, especially those using CNNs. This section reviews the techniques used to address this problem. We classify the state-of-the-art into three main categories: parameterization-based (Section 5.1), template deformation-based (Section 5.2), and point-based methods (Section 5.3).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于体积表示的方法在计算上非常浪费，因为信息仅在\(3\mathrm{D}\)形状的表面或附近丰富。直接处理表面的主要挑战是，常见的表示方法如网格或点云并不是规则结构，因此它们不容易适应深度学习架构，尤其是使用卷积神经网络（CNN）的架构。本节回顾了解决此问题的技术。我们将最先进的技术分为三大类：基于参数化的方法（第5.1节）、基于模板变形的方法（第5.2节）和基于点的方法（第5.3节）。</p></div><!-- Media --><p>TABLE 4: Taxonomy of mesh decoders. GCNN: graph CNN. MLP: Multilayer Perceptron. Param.: parameterization.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表4：网格解码器的分类。GCNN：图卷积神经网络。MLP：多层感知器。Param.：参数化。</p></div><table><tbody><tr><td rowspan="3"></td><td rowspan="2">Param.-based</td><td colspan="2">Deformation-based</td><td rowspan="2">Decoder architecture</td></tr><tr><td>Defo. model</td><td>Template</td></tr><tr><td>Geometry Images Spherical maps Patch-based</td><td>Vertex defo. Morphable FFD</td><td>Sphere / ellipse (k-)NN Learned (PCA) Learned (CNN)</td><td>FC layers UpConv</td></tr><tr><td>52</td><td>Geometry Image</td><td>-</td><td>-</td><td>UpConv</td></tr><tr><td>53</td><td>Geometry Image</td><td>-</td><td>-</td><td>ResNet blocks + 2 Conv layers</td></tr><tr><td>54</td><td>Patch-based</td><td>-</td><td>-</td><td>MLP</td></tr><tr><td>55</td><td>Mesh</td><td>vertex defo.</td><td>sphere</td><td>FC</td></tr><tr><td>56</td><td>Mesh</td><td>vertex defo.</td><td>ellipse</td><td>GCNN blocks</td></tr><tr><td>20</td><td>Mesh</td><td>vertex</td><td>cube</td><td>UpConv</td></tr><tr><td>57</td><td>Mesh</td><td>vertex defo.</td><td>Learned (CNN)</td><td>FC layer</td></tr><tr><td>58</td><td>Mesh</td><td>FFD</td><td>\( k \) -NN</td><td>FC</td></tr><tr><td>59</td><td>Mesh</td><td>FFD</td><td>NN</td><td>UpConv</td></tr><tr><td>60</td><td>Mesh</td><td>FFD</td><td>\( k \) -NN</td><td>Feed-forward</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="3"></td><td rowspan="2">基于参数</td><td colspan="2">基于变形</td><td rowspan="2">解码器架构</td></tr><tr><td>变形模型</td><td>模板</td></tr><tr><td>几何图像 球面图 Patch-based</td><td>顶点变形 可变形FFD</td><td>球体/椭圆 (k-)NN 学习的 (PCA) 学习的 (CNN)</td><td>全连接层 上采样卷积</td></tr><tr><td>52</td><td>几何图像</td><td>-</td><td>-</td><td>上采样卷积</td></tr><tr><td>53</td><td>几何图像</td><td>-</td><td>-</td><td>ResNet块 + 2个卷积层</td></tr><tr><td>54</td><td>基于补丁</td><td>-</td><td>-</td><td>多层感知器</td></tr><tr><td>55</td><td>网格</td><td>顶点变形</td><td>球体</td><td>全连接</td></tr><tr><td>56</td><td>网格</td><td>顶点变形</td><td>椭圆</td><td>GCNN块</td></tr><tr><td>20</td><td>网格</td><td>顶点</td><td>立方体</td><td>上采样卷积</td></tr><tr><td>57</td><td>网格</td><td>顶点变形</td><td>学习的 (CNN)</td><td>全连接层</td></tr><tr><td>58</td><td>网格</td><td>FFD</td><td>\( k \) -NN</td><td>全连接</td></tr><tr><td>59</td><td>网格</td><td>FFD</td><td>NN</td><td>上采样卷积</td></tr><tr><td>60</td><td>网格</td><td>FFD</td><td>\( k \) -NN</td><td>前馈</td></tr></tbody></table></div><!-- Media --><h3>5.1 Parameterization-based 3D reconstruction</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.1 基于参数化的三维重建</h3></div><p>Instead of working directly with triangular meshes, we can represent the surface of a 3D shape \(X\) as a mapping \(\zeta  : \mathcal{D} \rightarrow  {\mathbb{R}}^{3}\) where \(\mathcal{D}\) is a regular parameterization domain. The goal of the 3D reconstruction process is then to recover the shape function \(\zeta\) from an input I. When \(\mathcal{D}\) is a 3D domain then the methods in this class fall within the volumetric techniques described in Section 4 Here, we focus on the case where \(\mathcal{D}\) is a regular 2D domain,which can be a subset of the two dimensional plane,e.g., \(\mathcal{D} = {\left\lbrack  0,1\right\rbrack  }^{2}\) , or the unit sphere,i.e., \(\mathcal{D} = {S}^{2}\) . In the first case,one can implement encoder-decoder architectures using standard \(2\mathrm{D}\) convolution operations. In the latter case,one has to use spherical convolutions [61] since the domain is spherical.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们可以将三维形状的表面 \(X\) 表示为一个映射 \(\zeta  : \mathcal{D} \rightarrow  {\mathbb{R}}^{3}\)，其中 \(\mathcal{D}\) 是一个规则的参数化域，而不是直接处理三角网格。三维重建过程的目标是从输入 I 中恢复形状函数 \(\zeta\)。当 \(\mathcal{D}\) 是三维域时，这类方法属于第4节中描述的体积技术。在这里，我们关注的情况是 \(\mathcal{D}\) 是一个规则的二维域，可以是二维平面的一个子集，例如 \(\mathcal{D} = {\left\lbrack  0,1\right\rbrack  }^{2}\)，或单位球面，即 \(\mathcal{D} = {S}^{2}\)。在第一种情况下，可以使用标准的 \(2\mathrm{D}\) 卷积操作实现编码器-解码器架构。在后者的情况下，由于域是球形的，因此必须使用球面卷积 [61]。</p></div><p>Spherical parameterizations and geometry images [62], [63], [64] are the most commonly used parameterizations. They are, however, suitable only for genus-0 and disk-like surfaces. Surfaces of arbitrary topology need to be cut into disk-like patches, and then unfolded into a regular 2D domain. Finding the optimal cut for a given surface, and more importantly, findings cuts that are consistent across shapes within the same category is challenging. In fact, naively creating independent geometry images for a shape category and feeding them into deep neural networks would fail to generate coherent 3D shape surfaces [52].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>球面参数化和几何图像 [62]，[63]，[64] 是最常用的参数化方法。然而，它们仅适用于 genus-0 和盘状表面。任意拓扑的表面需要被切割成盘状补丁，然后展开成一个规则的二维域。为给定表面找到最佳切割，更重要的是，找到在同一类别内形状一致的切割是具有挑战性的。事实上，天真地为一个形状类别创建独立的几何图像并将其输入深度神经网络将无法生成一致的三维形状表面 [52]。</p></div><p>To create, for genus-0 surfaces, robust geometry images that are consistent across a shape category, the 3D objects within the category should be first put in correspondence [65], [66], [67]. Sinha et al. [52] proposed a cut-invariant procedure, which solves a large-scale correspondence problem, and an extension of deep residual nets to automatically generate geometry images encoding the \(x,y,z\) surface coordinates. The approach uses three separate encoder-decoder networks, which learn, respectively, the \(x,y\) and \(z\) geometry images. The three networks are composed of standard convolutions, up-residual, and down-residual blocks. They take as input a depth image or a RGB image, and learn the 3D reconstruction by minimizing a shape-aware \({L}_{2}\) loss function.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了为 genus-0 表面创建在形状类别中一致的稳健几何图像，首先应将该类别内的三维对象进行对应 [65]，[66]，[67]。Sinha 等人 [52] 提出了一个切割不变的程序，解决了大规模的对应问题，并扩展了深度残差网络以自动生成编码 \(x,y,z\) 表面坐标的几何图像。该方法使用三个独立的编码器-解码器网络，分别学习 \(x,y\) 和 \(z\) 几何图像。这三个网络由标准卷积、上残差和下残差块组成。它们以深度图像或 RGB 图像为输入，通过最小化形状感知的 \({L}_{2}\) 损失函数来学习三维重建。</p></div><p>Pumarola et al. [53] reconstruct the shape of a deformable surface using a network which has two branches: a detection branch and a depth estimation branch, which operate in parallel, and a third shape branch, which merges the detection mask and the depth map into a parameterized surface. Groueix et al. [54] decompose the surface of a 3D object into \(m\) patches,each patch \(i\) is defined as a mapping \({\zeta }_{i} : \mathcal{D} = {\left\lbrack  0,1\right\rbrack  }^{2} \mapsto  {\mathbb{R}}^{3}\) . They have then designed a decoder which is composed of \(m\) branches. Each branch \(i\) reconstructs the \(i\) -th patch by estimating the function \({\zeta }_{i}\) . At the end, the reconstructed patches are merged together to form the entire surface. Although this approach can handle surfaces of high genus, it is still not general enough to handle surfaces of arbitrary genus. In fact, the optimal number of patches depends on the genus of the surface ( \(n = 1\) for genus-0, \(n = 2\) for genus-1,etc.). Also,the patches are not guaranteed to be connected, although in practice one can still post-process the result and fill in the gaps between disconnected patches.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Pumarola 等人 [53] 使用一个具有两个分支的网络重建可变形表面的形状：一个检测分支和一个深度估计分支，它们并行操作，还有一个第三个形状分支，将检测掩码和深度图合并为一个参数化表面。Groueix 等人 [54] 将三维对象的表面分解为 \(m\) 补丁，每个补丁 \(i\) 被定义为一个映射 \({\zeta }_{i} : \mathcal{D} = {\left\lbrack  0,1\right\rbrack  }^{2} \mapsto  {\mathbb{R}}^{3}\)。然后，他们设计了一个由 \(m\) 分支组成的解码器。每个分支 \(i\) 通过估计函数 \({\zeta }_{i}\) 来重建第 \(i\) 个补丁。最后，重建的补丁被合并在一起形成整个表面。尽管这种方法可以处理高 genus 的表面，但仍然不足以处理任意 genus 的表面。实际上，补丁的最佳数量取决于表面的 genus（对于 genus-0 为 \(n = 1\)，对于 genus-1 为 \(n = 2\)，等等）。此外，补丁不保证是连接的，尽管在实践中仍然可以对结果进行后处理，并填补不连接补丁之间的空隙。</p></div><p>In summary, parameterization methods are limited to low-genus surfaces. As such, they are suitable for the reconstruction of objects that belong to a given shape category, e.g., human faces and bodies.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>总之，参数化方法仅限于低 genus 表面。因此，它们适合重建属于特定形状类别的对象，例如人脸和身体。</p></div><h3>5.2 Deformation-based 3D reconstruction</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.2 基于变形的三维重建</h3></div><p>Methods in this class take an input \(\mathbf{I}\) and estimate a deformation field \(\Delta\) ,which,when applied to a template \(3\mathrm{D}\) shape,results in the reconstructed \(3\mathrm{D}\) model \(X\) . Existing techniques differ in the type of deformation models they use (Section 5.2.1), the way the template is defined (Section 5.2.2), and in the network architecture used to estimate the deformation field \(\Delta\) (Section 5.2.3). In what follows,we assume that a 3D shape \(X = \left( {\mathcal{V},\mathcal{F}}\right)\) is represented with \(n\) vertices \(\mathcal{V} = \left\{  {{\mathbf{v}}_{1},\ldots ,{\mathbf{v}}_{n}}\right\}\) and faces \(\mathcal{F}\) . Let \(\widetilde{X} = \left( {\widetilde{\mathcal{V}},\mathcal{F}}\right)\) denote a template shape.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>本类方法接受一个输入\(\mathbf{I}\)并估计一个变形场\(\Delta\)，该变形场应用于模板\(3\mathrm{D}\)形状时，生成重建的\(3\mathrm{D}\)模型\(X\)。现有技术在使用的变形模型类型（第5.2.1节）、模板定义方式（第5.2.2节）以及用于估计变形场\(\Delta\)的网络架构（第5.2.3节）上有所不同。接下来，我们假设一个3D形状\(X = \left( {\mathcal{V},\mathcal{F}}\right)\)由\(n\)个顶点\(\mathcal{V} = \left\{  {{\mathbf{v}}_{1},\ldots ,{\mathbf{v}}_{n}}\right\}\)和面\(\mathcal{F}\)表示。设\(\widetilde{X} = \left( {\widetilde{\mathcal{V}},\mathcal{F}}\right)\)为模板形状。</p></div><h4>5.2.1 Deformation models</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>5.2.1 变形模型</h4></div><p>(1) Vertex deformation. This model assumes that a 3D shape \(X\) can be written in terms of linear displacements of the individual vertices of the template,i.e., \(\forall {\mathbf{v}}_{i} \in  \mathcal{V},{\mathbf{v}}_{i} = {\widetilde{\mathbf{v}}}_{i} + {\delta }_{i}\) , where \({\delta }_{i} \in  {\mathbb{R}}^{3}\) . The deformation field is defined as \(\Delta  =\) \(\left( {{\delta }_{1},\ldots ,{\delta }_{n}}\right)\) . This deformation model,illustrated in Fig. 2 (top), has been used in) [55], [56], [57]. It assumes that (1) there is a one-to-one correspondence between the vertices of the shape \(X\) and those of the template \(\widetilde{X}\) ,and (2) the shape \(X\) has the same topology as the template \(\widetilde{X}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(1) 顶点变形。该模型假设3D形状\(X\)可以用模板的各个顶点的线性位移来表示，即\(\forall {\mathbf{v}}_{i} \in  \mathcal{V},{\mathbf{v}}_{i} = {\widetilde{\mathbf{v}}}_{i} + {\delta }_{i}\)，其中\({\delta }_{i} \in  {\mathbb{R}}^{3}\)。变形场定义为\(\Delta  =\)\(\left( {{\delta }_{1},\ldots ,{\delta }_{n}}\right)\)。该变形模型如图2（上）所示，已在文献中使用[55]，[56]，[57]。它假设（1）形状\(X\)的顶点与模板\(\widetilde{X}\)的顶点之间存在一一对应关系，以及（2）形状\(X\)与模板\(\widetilde{X}\)具有相同的拓扑结构。</p></div><p>(2) Morphable models. Instead of using a generic template, one can use learned morphable models [68] to parameterize a 3D mesh. Let \(\widetilde{\mathcal{V}}\) be the mean shape and \({\Lambda }_{1},\ldots ,{\Lambda }_{K}\) be a set of orthonormal basis. Any shape \(\mathcal{V}\) can be written in the form:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(2) 可变形模型。可以使用学习到的可变形模型[68]来参数化3D网格，而不是使用通用模板。设\(\widetilde{\mathcal{V}}\)为平均形状，\({\Lambda }_{1},\ldots ,{\Lambda }_{K}\)为一组正交基。任何形状\(\mathcal{V}\)都可以写成以下形式：</p></div><p></p>\[\mathcal{V} = \widetilde{\mathcal{V}} + \mathop{\sum }\limits_{{i = 1}}^{K}{\alpha }_{i}{\Lambda }_{i},{\alpha }_{i} \in  \mathbb{R}. \tag{1}\]<p></p><p>The second term of Equation (1) can be seen as a deformation field, \(\Delta  = \mathop{\sum }\limits_{{i = 1}}^{K}{\alpha }_{i}{\Lambda }_{i}\) ,applied to the vertices \(\widetilde{\mathcal{V}}\) of the mean shape. By setting \({\Lambda }_{0} = \widetilde{\mathcal{V}}\) and \({\alpha }_{0} = 1\) ,Equation 1, can be written as \(\mathcal{V} = \mathop{\sum }\limits_{{i = 0}}^{K}{\alpha }_{i}{\Lambda }_{i}\) . In this case,the mean \(\mathcal{V}\) is treated as a bias term.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>方程（1）的第二项可以看作是施加于平均形状顶点\(\widetilde{\mathcal{V}}\)的变形场\(\Delta  = \mathop{\sum }\limits_{{i = 1}}^{K}{\alpha }_{i}{\Lambda }_{i}\)。通过设定\({\Lambda }_{0} = \widetilde{\mathcal{V}}\)和\({\alpha }_{0} = 1\)，方程1可以写成\(\mathcal{V} = \mathop{\sum }\limits_{{i = 0}}^{K}{\alpha }_{i}{\Lambda }_{i}\)。在这种情况下，平均\(\mathcal{V}\)被视为偏置项。</p></div><!-- Media --><!-- figureText: Conv pooling Conv pooling Conv pooling Conv Fine reconstruction Input image Template mesh Coarse reconstruction --><img src="https://cdn.noedgeai.com/bo_d163t43ef24c73d1le4g_8.jpg?x=909&#x26;y=120&#x26;w=769&#x26;h=510&#x26;r=0"><p>Fig. 2: Template deformation (top) [56] vs. domain deformation (bottom) [60].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2：模板变形（上）[56]与领域变形（下）[60]。</p></div><!-- Media --><p>One approach to learning a morphable model is by using Principal Component Analysis (PCA) on a collection of clean 3D mesh exemplars [68]. Recent techniques showed that,with only \(2\mathrm{D}\) annotations,it is possible to build category-specific 3D morphable models from 2D silhouettes or 2D images [69], [70]. These methods require efficient detection and segmentation of the objects, and camera pose estimation, which can also be done using CNN-based techniques.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>学习可变形模型的一种方法是对一组干净的3D网格样本使用主成分分析（PCA）[68]。最近的技术表明，仅凭\(2\mathrm{D}\)注释，可以从2D轮廓或2D图像构建特定类别的3D可变形模型[69]，[70]。这些方法需要高效的对象检测和分割，以及相机姿态估计，这也可以通过基于CNN的技术来完成。</p></div><p>(3) Free-Form Deformation (FFD). Instead of directly deforming the vertices of the template \(\widetilde{X}\) ,one can deform the space around it, see Fig. 2-(bottom). This can be done by defining around \(\widetilde{X}\) a set \(P \in  {\mathbb{R}}^{m \times  3}\) of \(m\) control points, called deformation handles. When the deformation field \(\Delta  = \left( {{\delta }_{1},\ldots ,{\delta }_{m}}\right) ,m \ll  n\) ,is applied to these control points, they deform the entire space around the shape and thus, they also deform the vertices \(\mathcal{V}\) of the shape according to the following equation:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(3) 自由形状变形 (FFD)。与其直接变形模板 \(\widetilde{X}\) 的顶点，不如变形其周围的空间，见图 2-(底部)。这可以通过在 \(\widetilde{X}\) 周围定义一组 \(P \in  {\mathbb{R}}^{m \times  3}\) 控制点来实现，这些控制点称为变形手柄。当变形场 \(\Delta  = \left( {{\delta }_{1},\ldots ,{\delta }_{m}}\right) ,m \ll  n\) 应用于这些控制点时，它们会变形整个形状周围的空间，从而根据以下方程变形形状的顶点 \(\mathcal{V}\)：</p></div><p></p>\[{\mathcal{V}}^{\top } = {B\Phi }{\left( P + \Delta \right) }^{\top }, \tag{2}\]<p></p><p>where the deformation matrix \(B \in  {\mathbb{R}}^{n \times  m}\) is a set of polynomial basis,e.g.,the Bernstein polynomials \(\overline{60}\rbrack .\Phi\) is a \(m \times  m\) matrix used to impose symmetry in the FFD field, see [71], and \(\Delta\) is the displacements.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中变形矩阵 \(B \in  {\mathbb{R}}^{n \times  m}\) 是一组多项式基，例如，Bernstein 多项式 \(\overline{60}\rbrack .\Phi\) 是一个 \(m \times  m\) 矩阵，用于在 FFD 场中施加对称性，见 [71]，\(\Delta\) 是位移。</p></div><p>This approach has been used by Kuryenkov et al. [59], Pontes et al. [60], and Jack et al. [58]. The main advantage of free-form deformation is that it does not require one-toone correspondence between the shapes and the template. However, the shapes that can be approximated by the FFD of the template are only those that have the same topology as the template.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这种方法已被 Kuryenkov 等人 [59]、Pontes 等人 [60] 和 Jack 等人 [58] 使用。自由形状变形的主要优点是它不需要形状与模板之间的一一对应关系。然而，能够通过模板的 FFD 进行近似的形状仅限于那些与模板具有相同拓扑结构的形状。</p></div><h4>5.2.2 Defining the template</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>5.2.2 定义模板</h4></div><p>Kato et al. [55] used a sphere as a template. Wang et al. [56] used an ellipse. Henderson et al. [20] defined two types of templates: a complex shape abstracted into cuboidal primitives, and a cube subdivided into multiple vertices. While the former is suitable for man-made shapes that have multiple components, the latter is suitable for representing genus-0 shapes and does not offer advantage compared to using a sphere or an ellipsoid.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Kato 等人 [55] 使用了一个球体作为模板。Wang 等人 [56] 使用了一个椭圆。Henderson 等人 [20] 定义了两种类型的模板：一种是抽象为立方体原语的复杂形状，另一种是细分为多个顶点的立方体。前者适合具有多个组件的人造形状，而后者适合表示 genus-0 形状，并且与使用球体或椭球体相比没有优势。</p></div><p>To speed up the convergence, Kuryenkov et al. [59] introduced DeformNet, which takes an image as input, searches the nearest shape from a database, and then deforms, using the FFD model of Equation (2), the retrieved model to match the query image. This method allows detail-preserving 3D reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了加快收敛速度，Kuryenkov 等人 [59] 引入了 DeformNet，它以图像作为输入，从数据库中搜索最近的形状，然后使用方程 (2) 的 FFD 模型对检索到的模型进行变形，以匹配查询图像。该方法允许保留细节的 3D 重建。</p></div><p>Pontes et al. [60] used an approach that is similar to DeformNet [59]. However, once the FFD field is estimated and applied to the template, the result is further refined by adding a residual defined as a weighted sum of some 3D models retrieved from a dictionary. The role of the deep neural network is to learn how to estimate the deformation field \(\Delta\) and the weights used in computing the refinement residual. Jack et al. [58], on the other hand, deform, using FFD, multiple templates and select the one that provides the best fitting accuracy.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Pontes 等人 [60] 使用了一种与 DeformNet [59] 类似的方法。然而，一旦估计并应用了 FFD 场，结果会通过添加一个定义为从字典中检索的一些 3D 模型的加权和的残差进一步精细化。深度神经网络的作用是学习如何估计变形场 \(\Delta\) 和用于计算精细化残差的权重。另一方面，Jack 等人 [58] 使用 FFD 变形多个模板，并选择提供最佳拟合精度的模板。</p></div><p>Another approach is to learn the template, either separately using statistical shape analysis techniques, e.g., PCA, on a set of training data, or jointly with the deformation field using deep learning techniques. For instance, Tulsiani et al. [70] use the mean shape of each category of 3D models as a class-specific template. The deep neural network estimates both the class of the input shape, which is used to select the class-specific mean shape, and the deformation field that needs to be applied to the class-specific mean shape. Kanazawa et al. [57] learn, at the same time, the mean shape and the deformation field. Thus, the approach does not require a separate \(3\mathrm{D}\) training set to learn the morphable model. In both cases, the reconstruction results lack details and are limited to popular categories such as cars and birds.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>另一种方法是学习模板，或者使用统计形状分析技术（例如 PCA）在一组训练数据上单独学习，或者与变形场一起使用深度学习技术共同学习。例如，Tulsiani 等人 [70] 使用每类 3D 模型的平均形状作为类特定模板。深度神经网络同时估计输入形状的类别，用于选择类特定的平均形状，以及需要应用于类特定平均形状的变形场。Kanazawa 等人 [57] 同时学习平均形状和变形场。因此，该方法不需要单独的 \(3\mathrm{D}\) 训练集来学习可变形模型。在这两种情况下，重建结果缺乏细节，并且仅限于汽车和鸟类等流行类别。</p></div><h4>5.2.3 Network architectures</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>5.2.3 网络架构</h4></div><p>Deformation-based methods also use encoder-decoder architectures. The encoder maps the input into a latent variable \(\mathbf{x}\) using successive convolutional operations. The latent space can be discrete or continuous as in [20], which used a variational auto-encoder (see Section 3). The decoder is, in general, composed of fully-connected layers. Kato et al. [55], for example, used two fully connected layers to estimate the deformation field to apply to a sphere to match the input's silhouette.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于变形的方法也使用编码器-解码器架构。编码器使用连续的卷积操作将输入映射到潜在变量 \(\mathbf{x}\)。潜在空间可以是离散的或连续的，如 [20] 中所用的变分自编码器（见第 3 节）。解码器通常由全连接层组成。例如，Kato 等人 [55] 使用两个全连接层来估计应用于球体的变形场，以匹配输入的轮廓。</p></div><p>Instead of deforming a sphere or an ellipse, Kuryenkov et al. [59] retrieve from a database the 3D model that is most similar to the input \(\mathbf{I}\) and then estimate the FFD needed to deform it to match the input. The retrieved template is first voxelized and encoded, using a 3D CNN, into another latent variable \({\mathbf{x}}_{t}\) . The latent representation of the input image and the latent representation of the retrieved template are then concatenated and decoded, using an up-convolutional network, into an FFD field defined on the vertices of a voxel grid.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Kuryenkov等人[59]没有对球体或椭球体进行变形，而是从数据库中检索与输入\(\mathbf{I}\)最相似的3D模型，然后估计所需的FFD以使其与输入匹配。检索到的模板首先被体素化并编码，使用3D CNN，转化为另一个潜在变量\({\mathbf{x}}_{t}\)。输入图像的潜在表示和检索到的模板的潜在表示随后被连接并解码，使用上卷积网络，转化为定义在体素网格顶点上的FFD场。</p></div><p>Pontes et al. [60] used a similar approach, but the latent variable \(\mathbf{x}\) is used as input into a classifier which finds, from a database, the closest model to the input. At the same time, the latent variable is decoded, using a feed-forward network,into a deformation field \(\Delta\) and weights \({\alpha }_{i},i = 1,\ldots ,K\) . The retrieved template is then deformed using \(\Delta\) and a weighted combination of a dictionary of CAD models,using the weights \({\alpha }_{i}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Pontes等人[60]采用了类似的方法，但潜在变量\(\mathbf{x}\)作为输入进入分类器，该分类器从数据库中找到与输入最接近的模型。同时，潜在变量被解码，使用前馈网络，转化为变形场\(\Delta\)和权重\({\alpha }_{i},i = 1,\ldots ,K\)。然后，使用\(\Delta\)和CAD模型字典的加权组合，使用权重\({\alpha }_{i}\)对检索到的模板进行变形。</p></div><p>Note that, one can design several variants to these approaches. For instance, instead of using a 3D model retrieved from a database as a template, one can use a class-specific mean shape. In this case,the latent variable \(\mathbf{x}\) can be used to classify the input into one of the shape categories, and then pick the learned mean shape of this category as a template [70]. Also, instead of learning separately the mean shape, e.g., using morphable models, Kanazawa et al. [57] treated the mean shape as a bias term, which can then be predicted by the network, along with the deformation field \(\Delta\) . Finally,Wang et al. [56] adopted a coarse to fine strategy, which makes the procedure more stable. They proposed a deformation network composed of three deformation blocks, each block is a graph-based CNN (GCNN), intersected by two graph unpooling layers. The deformation blocks update the location of the vertices while the graph unpoolling layers increase the number of vertices.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>请注意，可以设计多种变体来实现这些方法。例如，可以使用特定类别的均值形状，而不是从数据库中检索的3D模型作为模板。在这种情况下，潜在变量\(\mathbf{x}\)可以用于将输入分类为某个形状类别，然后选择该类别的学习均值形状作为模板[70]。此外，Kanazawa等人[57]没有单独学习均值形状，例如，使用可变形模型，而是将均值形状视为偏置项，然后可以由网络预测，连同变形场\(\Delta\)。最后，Wang等人[56]采用了粗到细的策略，使得过程更加稳定。他们提出了一个由三个变形块组成的变形网络，每个块是一个基于图的CNN（GCNN），由两个图反池化层交叉。变形块更新顶点的位置，而图反池化层增加顶点的数量。</p></div><p>Parameterization and deformation-based techniques can only reconstruct surfaces of fixed topology. The former is limited to surfaces of low genus while the latter is limited to the topology of the template.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>参数化和基于变形的技术只能重建固定拓扑的表面。前者限于低属的表面，而后者限于模板的拓扑。</p></div><h3>5.3 Point-based techniques</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>5.3 基于点的技术</h3></div><p>A 3D shape can be represented using an unordered set \(S = {\left\{  \left( {x}_{i},{y}_{i},{z}_{i}\right) \right\}  }_{i = 1}^{N}\) of \(N\) points. Such point-based representation is simple but efficient in terms of memory requirements. It is well suited for objects with intriguing parts and fine details. As such, an increasing number of papers, at least one in 2017 [72], more than 12 in 2018 [21], [21], [22], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], and a few others in 2019 [81], explored their usage for deep learning-based reconstruction. This section discusses the state-of-the-art point-based representations and their corresponding network architectures.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D形状可以使用一个无序集合\(S = {\left\{  \left( {x}_{i},{y}_{i},{z}_{i}\right) \right\}  }_{i = 1}^{N}\)的\(N\)个点来表示。这种基于点的表示简单但在内存需求方面高效。它非常适合具有引人注目的部分和细节的物体。因此，越来越多的论文，至少在2017年有一篇[72]，2018年有超过12篇[21]，[21]，[22]，[73]，[74]，[75]，[76]，[77]，[78]，[79]，[80]，[81]，[82]，以及2019年的几篇[81]，探讨了它们在基于深度学习的重建中的应用。本节讨论了最先进的基于点的表示及其相应的网络架构。</p></div><h4>5.3.1 Representations</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>5.3.1 表示</h4></div><p>The main challenge with point clouds is that they are not regular structures and do not easily fit into the convolutional architectures that exploit the spatial regularity. Three representations have been proposed to overcome this limitation:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>点云的主要挑战在于它们不是规则结构，无法轻易适应利用空间规则性的卷积架构。为克服这一限制，提出了三种表示方法：</p></div><ul>
<li>Point set representation treats a point cloud as a matrix of size \(N \times  3\) [21], [22], [72], [75], [77], [81].</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>点集表示将点云视为大小为\(N \times  3\)的矩阵[21]，[22]，[72]，[75]，[77]，[81]。</li>
</ul></div><ul>
<li>One or multiple 3-channel grids of size \(H \times  W \times\) 3 [72], [73], [82]. Each pixel in a grid encodes the (x,y,z)coordinates of a 3D point.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>一个或多个大小为\(H \times  W \times\)的3通道网格[72]，[73]，[82]。网格中的每个像素编码一个3D点的(x,y,z)坐标。</li>
</ul></div><ul>
<li>Depth maps from multiple viewpoints [78], [83].</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>来自多个视点的深度图[78]，[83]。</li>
</ul></div><p>The last two representations, hereinafter referred to as grid representations, are well suited for convolutional networks. They are also computationally efficient as they can be inferred using only \(2\mathrm{D}\) convolutions. Note that depth map-based methods require an additional fusion step to infer the entire 3D shape of an object. This can be done in a straightforward manner if the camera parameters are known. Otherwise, the fusion can be done using point cloud registration techniques [84], [85] or fusion networks [86]. Also, point set representations require fixing in advance the number of points \(N\) while in methods that use grid representations, the number of points can vary based on the nature of the object but it is always bounded by the grid resolution.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最后两种表示，以下简称为网格表示，非常适合卷积网络。它们在计算上也很高效，因为只需使用\(2\mathrm{D}\)卷积即可推断。请注意，基于深度图的方法需要额外的融合步骤来推断物体的整个3D形状。如果已知相机参数，这可以以简单的方式完成。否则，可以使用点云配准技术[84]、[85]或融合网络[86]进行融合。此外，点集表示需要提前固定点的数量\(N\)，而使用网格表示的方法中，点的数量可以根据物体的性质变化，但始终受到网格分辨率的限制。</p></div><!-- Media --><!-- figureText: (x,y,z)grids or Fan et al. 2017 Tatarchenko eta al. 2016, Wang et al. 2018, Wang et al. GDU Deformed depth Point Cloud Deformed depth Point Cloud Point Cloud PointNet (High res) Refined and Mandikal et al. Point ... Cloud Conv Conv 1D, ReLU N viewpoints (a) Fan et al. Tatarchenko et al Li et al. 2018 deformation Deformation field Pre- - deformation depth Deformation field N viewpoints (b) Li et al. [7] Mandikal et al. 2018, Insafutnitov et al. 2018 FC layers Global PointNet Point Cloud (Low res) Local PointNet > Pose Insafutnitov et al. 2018 (c) Mandikal et al. Insafutdinov Gadelha et al. 2018 FC layers (d) Gadelha et al. 22 --><img src="https://cdn.noedgeai.com/bo_d163t43ef24c73d1le4g_10.jpg?x=123&#x26;y=113&#x26;w=761&#x26;h=1372&#x26;r=0"><p>Fig. 3: The different network architectures used in point-based 3D reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3：用于基于点的3D重建的不同网络架构。</p></div><!-- Media --><h4>5.3.2 Network architectures</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>5.3.2 网络架构</h4></div><p>Similar to volumetric and surface-based representations, techniques that use point-based representations follow the encoder-decoder model. While they all use the same architecture for the encoder, they differ in the type and architecture of their decoder, see Fig. 3. In general, grid representations use up-convolutional networks to decode the latent variable [72], [73], [78], [82], see Fig. 3-(a) and (b). Point set representations (Fig. 3-(c)) use fully connected layers [21], [72], [74], [77], [81] since point clouds are unordered.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>与体积和基于表面的表示类似，使用基于点的表示的技术遵循编码器-解码器模型。虽然它们都使用相同的编码器架构，但在解码器的类型和架构上有所不同，见图3。一般来说，网格表示使用上卷积网络来解码潜在变量[72]、[73]、[78]、[82]，见图3-(a)和(b)。点集表示（图3-(c)）使用全连接层[21]、[72]、[74]、[77]、[81]，因为点云是无序的。</p></div><p>The main advantage of fully-connected layers is that they capture the global information. However, compared to convolutional operations, they are computationally expensive. To benefit from the efficiency of convolutional operations, Gadelha et al. [22] order, spatially, the point cloud using a space-partitionning tree such as KD-tree and then process them using 1D convolutional operations, see Fig. 3-(d). With a conventional CNN, each convolutional operation has a restricted receptive field and is not able to leverage both global and local information effectively. Gadelha et al. [22] resolve this issue by maintaining three different resolutions. That is, the latent variable is decoded into three different resolutions, which are then concatenated and further processed with 1D convolutional layers to generate a point cloud of size \(4\mathrm{\;K}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>全连接层的主要优点是能够捕捉全局信息。然而，与卷积操作相比，它们在计算上是昂贵的。为了利用卷积操作的效率，Gadelha等[22]使用空间划分树（如KD树）对点云进行空间排序，然后使用1D卷积操作进行处理，见图3-(d)。在传统的CNN中，每个卷积操作的感受野有限，无法有效利用全局和局部信息。Gadelha等[22]通过保持三种不同的分辨率来解决这个问题。也就是说，潜在变量被解码为三种不同的分辨率，然后连接并进一步处理，以生成大小为\(4\mathrm{\;K}\)的点云。</p></div><p>Fan et al. [72] proposed a generative deep network that combines both the point set representation and the grid representation (Fig. 3-(a)). The network is composed of a cascade of encoder-decoder blocks:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Fan等[72]提出了一种生成深度网络，结合了点集表示和网格表示（图3-(a)）。该网络由一系列编码器-解码器块组成：</p></div><ul>
<li>The first block takes the input image and maps it into a latent representation, which is then decoded into a 3-channel image of size \(H \times  W\) . The three values at each pixel are the coordinates of a point.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>第一个块接收输入图像并将其映射到潜在表示，然后解码为大小为\(H \times  W\)的3通道图像。每个像素的三个值是一个点的坐标。</li>
</ul></div><ul>
<li>Each of the subsequent blocks takes the output of its previous block and further encodes and decodes it into a 3-channel image of size \(H \times  W\) .</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>每个后续块接收其前一个块的输出，并进一步编码和解码为大小为\(H \times  W\)的3通道图像。</li>
</ul></div><ul>
<li>The last block is an encoder, of the same type as the previous ones, followed by a predictor composed of two branches. The first branch is a decoder which predicts a 3-channel image of size \(H \times  W({32} \times  {24}\) in this case), of which the three values at each pixel are the coordinates of a point. The second branch is a fully-connected network, which predicts a matrix of size \(N \times  3\) ,each row is a \(3\mathrm{D}\) point \(\left( {N = {256}}\right)\) .</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>最后一个块是一个编码器，类型与前面的相同，后面跟着一个由两个分支组成的预测器。第一个分支是一个解码器，预测一个大小为\(H \times  W({32} \times  {24}\)的3通道图像，其中每个像素的三个值是一个点的坐标。第二个分支是一个全连接网络，预测一个大小为\(N \times  3\)的矩阵，每行是一个\(3\mathrm{D}\)点\(\left( {N = {256}}\right)\)。</li>
</ul></div><ul>
<li>The predictions of the two branches are merged using set union to produce a 3D point set of size 1024.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>两个分支的预测通过集合并集合并，以生成大小为1024的3D点集。</li>
</ul></div><p>This approach has been also used by Jiang et al. [74]. The main difference between the two is in the training procedure, which we will discuss in Section 7.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这种方法也被Jiang等[74]使用。两者之间的主要区别在于训练过程，我们将在第7节中讨论。</p></div><p>Tatarchenki et al. [83], Wang et al. [82], and Lin et al. [73] followed the same idea but their decoder regresses \(N\) grids, see Fig. 3-(a). Each grid encodes the depth map [83] or the (x,y,z)coordinates \(\left\lbrack  {73}\right\rbrack  ,\left\lbrack  {82}\right\rbrack\) of the visible surface from that view point. The viewpoint, encoded with a sequence of fully connected layers, is provided as input to the decoder along with the latent representation of the input image. Li et al. [78], on the other hand, used a multi-branch decoder, one for each viewpoint, see Fig. 3-(b). Unlike [83], each branch regresses a canonical depth map from a given view point and a deformation field, which deforms the estimated canonical depth map to match the input, using Grid Deformation Units (GDUs). The reconstructed grids are then lifted to \(3\mathrm{D}\) and merged together.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Tatarchenki 等人 [83]、Wang 等人 [82] 和 Lin 等人 [73] 遵循了相同的思路，但他们的解码器回归 \(N\) 网格，见图 3-(a)。每个网格编码了深度图 [83] 或从该视点可见表面的 (x,y,z) 坐标 \(\left\lbrack  {73}\right\rbrack  ,\left\lbrack  {82}\right\rbrack\)。视点通过一系列全连接层进行编码，并与输入图像的潜在表示一起作为输入提供给解码器。另一方面，Li 等人 [78] 使用了一个多分支解码器，每个视点一个，见图 3-(b)。与 [83] 不同，每个分支从给定视点回归一个标准深度图和一个变形场，该变形场使用网格变形单元 (GDUs) 将估计的标准深度图变形以匹配输入。然后重建的网格被提升到 \(3\mathrm{D}\) 并合并在一起。</p></div><p>Similar to volumetric techniques, the vanilla architecture for point-based 3D reconstruction only recovers low resolution geometry. For high-resolution reconstruction, Mandikal et al. [81], see Fig. 3-(c), use a cascade of multiple networks. The first network predicts a low resolution point cloud. Each subsequent block takes the previously predicted point cloud, computes global features, using a multi-layer perceptron architecture (MLP) similar to PointNet [87] or Pointnet++ [88], and local features by applying MLPs in balls around each point. Local and global features are then aggregated and fed to another MLP, which predicts a dense point cloud. The process can be repeated recursively until the desired resolution is reached.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>与体积技术类似，基于点的 3D 重建的基本架构仅恢复低分辨率几何形状。为了进行高分辨率重建，Mandikal 等人 [81]，见图 3-(c)，使用了多个网络的级联。第一个网络预测低分辨率点云。每个后续块采用先前预测的点云，使用类似于 PointNet [87] 或 Pointnet++ [88] 的多层感知器架构 (MLP) 计算全局特征，并通过在每个点周围的球体中应用 MLP 计算局部特征。然后将局部和全局特征聚合并输入到另一个 MLP 中，该 MLP 预测密集点云。该过程可以递归重复，直到达到所需的分辨率。</p></div><!-- Media --><!-- figureText: Blockwise training End-to-end training Blockwise training Encoder 3D Decoder 3D shape Normal decoder Image Encoder decoder Sihouette decoder silhouette Intermediate representation estimation --><img src="https://cdn.noedgeai.com/bo_d163t43ef24c73d1le4g_11.jpg?x=147&#x26;y=115&#x26;w=717&#x26;h=355&#x26;r=0"><p>Fig. 4: Intermediating via 2.5D sketches (depth, normals, and silhouettes).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图 4：通过 2.5D 草图（深度、法线和轮廓）进行中介。</p></div><!-- Media --><p>Mandikal et al. [21] combine TL-embedding with a variational auto-encoder (Fig. 3-(c)). The former allows mapping a 3D point cloud and its corresponding views onto the same location in the latent space. The latter enables the reconstruction of multiple plausible point clouds from the input image(s).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Mandikal 等人 [21] 将 TL 嵌入与变分自编码器结合（图 3-(c)）。前者允许将 3D 点云及其对应视图映射到潜在空间中的同一位置。后者使得能够从输入图像重建多个合理的点云。</p></div><p>Finally, point-based representations can handle 3D shapes of arbitrary topologies. However, they require a post processing step, e.g., Poisson surface reconstruction [89] or SSD [90], to retrieve the 3D surface mesh, which is the quantity of interest. The pipeline, from the input until the final mesh is obtained, cannot be trained end-to-end. Thus, these methods only optimise an auxiliary loss defined on an intermediate representation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最后，基于点的表示可以处理任意拓扑的 3D 形状。然而，它们需要一个后处理步骤，例如，泊松表面重建 [89] 或 SSD [90]，以检索 3D 表面网格，这是关注的数量。从输入到最终网格获得的整个流程无法进行端到端训练。因此，这些方法仅优化在中间表示上定义的辅助损失。</p></div><h2>6 LEVERAGING OTHER CUES</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>6 利用其他线索</h2></div><p>The previous sections discussed methods that directly reconstruct 3D objects from their 2D observations. This section shows how additional cues such as intermediate representations (Section 6.1) and temporal correlations (Section 6.2) can be used to boost 3D reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>前面的章节讨论了直接从 2D 观测重建 3D 物体的方法。本节展示了如何利用中间表示（第 6.1 节）和时间相关性（第 6.2 节）等额外线索来提升 3D 重建。</p></div><h3>6.1 Intermediating</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.1 中介</h3></div><p>Many of the deep learning-based 3D reconstruction algorithms directly predict the \(3\mathrm{D}\) geometry of an object from RGB images. Some techniques, however, decompose the problem into sequential steps,which estimate \({2.5}\mathrm{D}\) information such as depth maps, normal maps, and/or segmentation masks, see Fig. 4. The last step, which can be implemented using traditional techniques such as space carving or 3D back-projection followed by filtering and registration, recovers the full 3D geometry and the pose of the input.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>许多基于深度学习的 3D 重建算法直接从 RGB 图像预测 \(3\mathrm{D}\) 物体的几何形状。然而，一些技术将问题分解为顺序步骤，估计 \({2.5}\mathrm{D}\) 信息，例如深度图、法线图和/或分割掩码，见图 4。最后一步可以使用传统技术实现，例如空间雕刻或 3D 反投影，随后进行过滤和配准，恢复输入的完整 3D 几何形状和姿态。</p></div><p>While early methods train separately the different modules, recent works proposed end-to-end solutions [6], [9], [38], [53], [80], [91], [92]. For instance, Wu et al. [6] and later Sun et al. [9] used two blocks. The first block is an encoder followed by a three-branch decoder, which estimates the depth map, the normal map, and the segmentation mask (called 2.5D sketches). These are then concatenated and fed into another encoder-decoder, which regresses a full 3D volumetric grid \(\left\lbrack  6\right\rbrack  ,\left\lbrack  9\right\rbrack  ,\left\lbrack  {91}\right\rbrack\) ,and a set of fully-connected layers, which regress the camera pose [9]. The entire network is trained end-to-end.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>虽然早期的方法分别训练不同的模块，但最近的工作提出了端到端的解决方案 [6]、[9]、[38]、[53]、[80]、[91]、[92]。例如，Wu 等人 [6] 和后来的 Sun 等人 [9] 使用了两个块。第一个块是一个编码器，后面跟着一个三分支解码器，估计深度图、法线图和分割掩码（称为 2.5D 草图）。这些然后被连接并输入到另一个编码器-解码器中，该解码器回归一个完整的 3D 体积网格 \(\left\lbrack  6\right\rbrack  ,\left\lbrack  9\right\rbrack  ,\left\lbrack  {91}\right\rbrack\)，以及一组全连接层，回归相机姿态 [9]。整个网络是端到端训练的。</p></div><p>Other techniques convert the intermediate depth map into (1) a 3D occupancy grid [46] or a truncated signed distance function volume [38], which is then processed using a 3D encoder-decoder network for completion and refinement, or (2) a partial point cloud, which is further processed using a point-cloud completion module [80]. Zhang et al. [92] convert the inferred depth map into a spherical map and unpaint it, to fill in holes, using another encoder-decoder. The unpainted spherical depth map is then back-projected to \(3\mathrm{D}\) and refined using a voxel refinement network,which estimates a voxel occupancy grid of size \({128}^{3}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其他技术将中间深度图转换为 (1) 3D 占用网格 [46] 或截断有符号距离函数体积 [38]，然后使用 3D 编码器-解码器网络进行补全和精炼，或 (2) 部分点云，进一步使用点云补全模块 [80] 进行处理。Zhang 等 [92] 将推断的深度图转换为球面图并进行去涂色，以填补空洞，使用另一个编码器-解码器。去涂色的球面深度图随后被反投影到 \(3\mathrm{D}\) 并使用体素精炼网络进行精炼，该网络估计大小为 \({128}^{3}\) 的体素占用网格。</p></div><p>Other techniques estimate multiple depth maps from pre-defined or arbitrary viewpoints. Tatarchenko et al. [83] proposed a network, which takes as input an RGB image and a target viewpoint \(v\) ,and infers the depth map of the object as seen from the viewpoint \(v\) . By varying the viewpoint, the network is able to estimate multiple depths, which can then be merged into a complete \(3\mathrm{D}\) model. The approach uses a standard encoder-decoder and an additional network composed of three fully-connected layers to encode the viewpoint. Soltani et al. [19] and Lin et al. [73] followed the same approach but predict the depth maps, along with their binary masks, from pre-defined view points. In both methods, the merging is performed in a post-processing step. Smith et al. [93] first estimate a low resolution voxel grid. They then take the depth maps, of the low resolution voxel grid, computed from the six axis-aligned views and refine them using a silhouette and depth refinement network. The refined depth maps are finally combined into a volumetric grid of size \({256}^{3}\) using space carving techniques.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其他技术从预定义或任意视点估计多个深度图。Tatarchenko 等 [83] 提出了一个网络，该网络以 RGB 图像和目标视点 \(v\) 作为输入，并推断从视点 \(v\) 看到的物体的深度图。通过改变视点，网络能够估计多个深度，然后将其合并为一个完整的 \(3\mathrm{D}\) 模型。该方法使用标准的编码器-解码器和一个由三个全连接层组成的附加网络来编码视点。Soltani 等 [19] 和 Lin 等 [73] 采用相同的方法，但从预定义视点预测深度图及其二进制掩码。在这两种方法中，合并是在后处理步骤中进行的。Smith 等 [93] 首先估计低分辨率体素网格。然后，他们使用从六个轴对齐视图计算的低分辨率体素网格的深度图，并使用轮廓和深度精炼网络对其进行精炼。最后，精炼后的深度图使用空间雕刻技术合并为大小为 \({256}^{3}\) 的体积网格。</p></div><p>Tatarchenko et al. [83], Lin et al. [73], and Sun et al. [9] also estimate the binary/silhouette masks, along with the depth maps. The binary masks have been used to filter out points that are not back-projected to the surface in 3D space. The side effect of these depth mask-based approaches is that it is a huge computation waste as a large number of points are discarded, especially for objects with thin structures. Li et al. [78] overcome this problem by deforming a regular depth map using a learned deformation field. Instead of directly inferring depth maps that best fit the input, Li et al. [78] infer a set of \(2\mathrm{D}\) pre-deformation depth maps and their corresponding deformation fields at pre-defined canonical viewpoints. These are each passed to a Grid Deformation Unit (GDU) that transforms the regular grid of the depth map to a deformed depth map. Finally, the deformed depth maps are transformed into a common coordinate frame for fusion into a dense point cloud.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Tatarchenko 等 [83]、Lin 等 [73] 和 Sun 等 [9] 还估计二进制/轮廓掩码以及深度图。二进制掩码用于过滤掉未在 3D 空间中反投影到表面的点。这些基于深度掩码的方法的副作用是，计算浪费巨大，因为大量点被丢弃，尤其是对于具有细结构的物体。Li 等 [78] 通过使用学习到的变形场对常规深度图进行变形来克服这个问题。Li 等 [78] 并不是直接推断最适合输入的深度图，而是推断一组 \(2\mathrm{D}\) 预变形深度图及其在预定义标准视点下的相应变形场。这些每个都传递给一个网格变形单元 (GDU)，将深度图的常规网格转换为变形深度图。最后，变形深度图被转换为一个共同的坐标框架，以便融合成一个密集点云。</p></div><p>The main advantage of multi-staged approaches is that depth, normal, and silhouette maps are much easier to recover from 2D images. Likewise, 3D models are much easier to recover from these three modalities than from 2D images alone.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>多阶段方法的主要优点是，从 2D 图像中恢复深度、法线和轮廓图要容易得多。同样，从这三种模态中恢复 3D 模型也比仅从 2D 图像中恢复要容易得多。</p></div><h3>6.2 Exploiting spatio-temporal correlations</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>6.2 利用时空相关性</h3></div><p>There are many situations where multiple spatially distributed images of the same object(s) are acquired over an extended period of time. Single image-based reconstruction techniques can be used to reconstruct the 3D shapes by processing individual frames independently from each other, and then merging the reconstruction using registration techniques. Ideally, we would like to leverage on the spatio-temporal correlations that exist between the frames to resolve ambiguities especially in the presence of occlusions and highly cluttered scenes. In particular, the network at time \(t\) should remember what has been reconstructed up to time \(t - 1\) ,and use it,in addition to the new input, to reconstruct the scene or objects at time \(t\) . This problem of processing sequential data has been addressed by using Recurrent Neural Networks (RNN) and Long-Short Term Memory (LSTM) networks, which enable networks to remember their inputs over a period of time.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在许多情况下，多个空间分布的同一物体的图像在较长时间内被获取。基于单图像的重建技术可以通过独立处理每个帧来重建 3D 形状，然后使用配准技术合并重建。理想情况下，我们希望利用帧之间存在的时空相关性来解决模糊，特别是在存在遮挡和高度杂乱的场景中。特别是，时间 \(t\) 的网络应该记住到时间 \(t - 1\) 为止已重建的内容，并将其与新输入结合使用，以重建时间 \(t\) 的场景或物体。处理序列数据的问题已经通过使用递归神经网络 (RNN) 和长短期记忆 (LSTM) 网络来解决，这使得网络能够在一段时间内记住其输入。</p></div><p>Choy et al. [7] proposed an architecture called 3D Recurrent Reconstruction Network (3D-R2N2), which allows the network to adaptively and consistently learn a suitable 3D representation of an object as (potentially conflicting) information from different viewpoints becomes available. The network can perform incremental refinement every time a new view becomes available. It is composed of two parts; a standard convolution encoder-decoder and a set of \(3\mathrm{D}\) Convolutional Long-Short Term Memory (3D-LSTM) units placed at the start of the convolutional decoder. These take the output of the encoder, and then either selectively update their cell states or retain the states by closing the input gate. The decoder then decodes the hidden states of the LSTM units and generates a probabilistic reconstruction in the form of a voxel occupancy map.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Choy 等人 [7] 提出了一个名为 3D 循环重建网络 (3D-R2N2) 的架构，该网络能够自适应且一致地学习物体的合适 3D 表示，随着来自不同视角的信息（可能存在冲突）变得可用。每当有新视角可用时，网络可以进行增量细化。它由两个部分组成；一个标准的卷积编码器-解码器和一组放置在卷积解码器开头的 \(3\mathrm{D}\) 卷积长短期记忆 (3D-LSTM) 单元。这些单元接收编码器的输出，然后选择性地更新其单元状态或通过关闭输入门保留状态。解码器随后解码 LSTM 单元的隐藏状态，并生成以体素占用图形式的概率重建。</p></div><p>The 3D-LSTM allows the network to retain what it has seen and update its memory when it sees a new image. It is able to effectively handle object self-occlusions when multiple views are fed to the network. At each time step, it selectively updates the memory cells that correspond to parts that became visible while retaining the states of the other parts.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D-LSTM 允许网络保留其所见内容，并在看到新图像时更新其记忆。当多个视角输入到网络时，它能够有效处理物体自遮挡。在每个时间步，它选择性地更新与变得可见的部分对应的记忆单元，同时保留其他部分的状态。</p></div><p>LSTM and RNNs are time consuming since the input images are processed sequentially without parallelization. Also, when given the same set of images with different orders, RNNs are unable to estimate the 3D shape of an object consistently due to permutation variance. To overcome these limitations, Xie et al. [86, introduced Pix2Vox, which is composed of multiple encoder-decoder blocks, running in parallel, each one predicts a coarse volumetric grid from its input frame. This eliminates the effect of the order of input images and accelerates the computation. Then, a context-aware fusion module selects high-quality reconstructions from the coarse 3D volumes and generates a fused 3D volume, which fully exploits information of all input images without long-term memory loss.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>LSTM 和 RNN 的处理时间较长，因为输入图像是顺序处理的，没有并行化。此外，当给定相同的一组图像但顺序不同时，RNN 由于排列方差无法一致地估计物体的 3D 形状。为了克服这些限制，Xie 等人 [86] 提出了 Pix2Vox，该方法由多个并行运行的编码器-解码器块组成，每个块从其输入帧预测一个粗略的体积网格。这消除了输入图像顺序的影响，并加速了计算。然后，一个上下文感知融合模块从粗糙的 3D 体积中选择高质量的重建，并生成一个融合的 3D 体积，充分利用所有输入图像的信息而不丢失长期记忆。</p></div><h2>7 TRAINING</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>7 训练</h2></div><p>In addition to their architectures, the performance of deep learning networks depends on the way they are trained. This section discusses the various supervisory modes (Section 7.1 and training procedures that have been used in the literature (Section 7.3).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>除了它们的架构，深度学习网络的性能还取决于它们的训练方式。本节讨论了文献中使用的各种监督模式（第 7.1 节）和训练程序（第 7.3 节）。</p></div><h3>7.1 Degree of supervision</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>7.1 监督程度</h3></div><p>Early methods rely on 3D supervision (Section 7.1.1). However,obtaining ground-truth 3D data,either manually or using traditional 3D reconstruction techniques, is extremely difficult and expensive. As such, recent techniques try to minimize the amount of \(3\mathrm{D}\) supervision by exploiting other supervisory signals such consistency across views (Section 7.1.2).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>早期方法依赖于 3D 监督（第 7.1.1 节）。然而，获取真实的 3D 数据，无论是手动还是使用传统的 3D 重建技术，都是极其困难和昂贵的。因此，最近的技术试图通过利用其他监督信号（如视角一致性，第 7.1.2 节）来最小化 \(3\mathrm{D}\) 监督的数量。</p></div><h4>7.1.1 Training with 3D supervision</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>7.1.1 使用 3D 监督进行训练</h4></div><p>Supervised methods require training using images paired with their corresponding ground-truth 3D shapes. The training process then minimizes a loss function that measures the discrepancy between the reconstructed \(3\mathrm{D}\) shape and the corresponding ground-truth 3D model. The discrepancy is measured using loss functions, which are required to be differentiable so that gradients can be computed. Examples of such functions include:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>监督方法需要使用与其对应的真实 3D 形状配对的图像进行训练。训练过程最小化一个损失函数，该函数测量重建的 \(3\mathrm{D}\) 形状与对应的真实 3D 模型之间的差异。差异通过损失函数进行测量，这些函数需要可微分，以便计算梯度。这类函数的例子包括：</p></div><p>(1) Volumetric loss. It is defined as the distance between the reconstructed and the ground-truth volumes;</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(1) 体积损失。它被定义为重建体积与真实体积之间的距离；</p></div><p></p>\[{\mathcal{L}}_{\text{vol }}\left( \mathbf{I}\right)  = d\left( {f\left( \mathbf{I}\right) ,X}\right) . \tag{3}\]<p></p><p>Here, \(d\left( {\cdot , \cdot  }\right)\) can be the \({L}_{2}\) distance between the two volumes or the negative Intersection over Union (IoU) \({\mathcal{L}}_{\text{IoU }}\) (see Equation (16)). Both metrics are suitable for binary occupancy grids and TSDF representations. For probabilistic occupancy grids, the cross-entropy loss is the most commonly used [25]:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这里，\(d\left( {\cdot , \cdot  }\right)\) 可以是两个体积之间的 \({L}_{2}\) 距离或负的交并比 (IoU) \({\mathcal{L}}_{\text{IoU }}\)（见公式 (16)）。这两种度量适用于二进制占用网格和 TSDF 表示。对于概率占用网格，交叉熵损失是最常用的 [25]：</p></div><p></p>\[{\mathcal{L}}_{CE} =  - \frac{1}{N}\mathop{\sum }\limits_{{i = 1}}^{N}\left\{  {{p}_{i}\log {\widehat{p}}_{i} + \left( {1 - {p}_{i}}\right) \log \left( {1 - {\widehat{p}}_{i}}\right. }\right\}  . \tag{4}\]<p></p><p>Here, \({p}_{i}\) is the ground-truth probability of voxel \(i\) being occupied, \({\widehat{p}}_{i}\) is the estimated probability,and \(N\) is the number of voxels.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这里，\({p}_{i}\) 是体素 \(i\) 被占用的真实概率，\({\widehat{p}}_{i}\) 是估计概率，\(N\) 是体素的数量。</p></div><p>(2) Point set loss. When using point-based representations, the reconstruction loss can be measured using the Earth Mover's Distance (EMD) [59], [72] or the Chamfer Distance (CD) [59], [72]. The EMD is defined as the minimum of the sum of distances between a point in one set and a point in another set over all possible permutations of the correspondences. More formally, given two sets of points \({S}_{gt}\) and \({S}_{rec}\) ,the EMD is defined as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(2) 点集损失。当使用基于点的表示时，重建损失可以使用地球移动者距离 (EMD) [59]，[72] 或者 Chamfer 距离 (CD) [59]，[72] 来测量。EMD 被定义为在所有可能的对应排列中，一个集合中的一个点与另一个集合中的一个点之间距离的最小和。更正式地说，给定两个点集 \({S}_{gt}\) 和 \({S}_{rec}\)，EMD 被定义为：</p></div><p></p>\[{\mathcal{L}}_{EMD} = \mathop{\min }\limits_{{{S}_{gt} \rightarrow  {S}_{rec}}}\mathop{\sum }\limits_{{p \in  {S}_{gt}}}\parallel p - \phi \left( p\right) \parallel . \tag{5}\]<p></p><p>Here, \(\phi \left( p\right)  \in  {S}_{\text{rec }}\) is the closest point on \({S}_{\text{rec }}\) to \(p \in  {S}_{gt}\) . The CD loss, on the other hand, is defined as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这里，\(\phi \left( p\right)  \in  {S}_{\text{rec }}\) 是 \({S}_{\text{rec }}\) 中离 \(p \in  {S}_{gt}\) 最近的点。另一方面，CD 损失被定义为：</p></div><p></p>\[{\mathcal{L}}_{CD} = \frac{1}{{N}_{gt}}\mathop{\min }\limits_{{p \in  {S}_{gt}}}\parallel p - q{\parallel }^{2} + \frac{1}{{N}_{rec}}\mathop{\min }\limits_{{q \in  {S}_{rec}}}\parallel p - q{\parallel }^{2}. \tag{6}\]<p></p><p>\({N}_{gt}\) and \({N}_{rec}\) are,respectively,the size of \({S}_{gt}\) and \({S}_{rec}\) . The CD is computationally easier than EMD since it uses suboptimal matching to determine the pairwise relations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({N}_{gt}\) 和 \({N}_{rec}\) 分别是 \({S}_{gt}\) 和 \({S}_{rec}\) 的大小。CD 在计算上比 EMD 更简单，因为它使用次优匹配来确定成对关系。</p></div><p>(3) Learning to generate multiple plausible reconstructions. 3D reconstruction from a single image is an ill-posed problem, thus for a given input there might be multiple plausible reconstructions. Fan et al. [72] proposed the Min-of- \(\mathrm{N}\left( \mathrm{{MoN}}\right)\) loss to train neural networks to generate distributional output. The idea is to use a random vector \(r\) drawn from a certain distribution to perturb the input. The network learns to generate a plausible \(3\mathrm{D}\) shape from each perturbation of the input. It is trained using a loss defined as follows;</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(3) 学习生成多个合理的重建。从单张图像进行 3D 重建是一个不适定问题，因此对于给定的输入，可能会有多个合理的重建。Fan 等人 [72] 提出了 Min-of-\(\mathrm{N}\left( \mathrm{{MoN}}\right)\) 损失来训练神经网络生成分布输出。这个想法是使用从某个分布中抽取的随机向量 \(r\) 来扰动输入。网络学习从每个输入的扰动中生成一个合理的 \(3\mathrm{D}\) 形状。它的训练使用如下定义的损失；</p></div><p></p>\[{\mathcal{L}}_{MoN} = \mathop{\sum }\limits_{i}\mathop{\min }\limits_{{r \sim  \mathbb{N}\left( {0,\mathrm{I}}\right) }}\left\{  {d\left( {f\left( {\mathbf{I},r}\right) ,{S}_{gt}}\right) }\right\}  . \tag{7}\]<p></p><p>Here, \(f\left( {\mathbf{I},r}\right)\) is the reconstructed \(3\mathrm{D}\) point cloud after perturbing the input with the random vector \(r\) sampled from the multivariate normal distribution \(\mathbb{N}\left( {0,\mathrm{I}}\right) ,{S}_{gt}\) is the ground-truth point cloud,and \(d\left( {\cdot , \cdot  }\right)\) is a reconstruction loss, which can be any of the loss functions defined above. At runtime, various plausible reconstructions can be generated from a given input by sampling different random vectors \(r\) from \(\mathbb{N}\left( {0,\mathrm{I}}\right)\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这里，\(f\left( {\mathbf{I},r}\right)\) 是在用从多元正态分布中抽样的随机向量 \(r\) 扰动输入后重建的 \(3\mathrm{D}\) 点云，\(\mathbb{N}\left( {0,\mathrm{I}}\right) ,{S}_{gt}\) 是真实点云，\(d\left( {\cdot , \cdot  }\right)\) 是重建损失，可以是上述定义的任何损失函数。在运行时，可以通过从 \(\mathbb{N}\left( {0,\mathrm{I}}\right)\) 中抽样不同的随机向量 \(r\) 来生成给定输入的各种合理重建。</p></div><h4>7.1.2 Training with 2D supervision</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>7.1.2 使用 2D 监督进行训练</h4></div><p>Obtaining 3D ground-truth data for supervision is an expensive and tedious process even for a small scale training. However,obtaining multiview 2D or \({2.5}\mathrm{D}\) images for training is relatively easy. Methods in the category use the fact that if the estimated \(3\mathrm{D}\) shape is as close as possible to the ground truth then the discrepancy between views of the 3D model and the projection of the reconstructed 3D model onto any of these views is also minimized. Implementing this idea requires defining a projection operator, which renders the reconstructed \(3\mathrm{D}\) model from a given viewpoint (Section 7.1.2.1), and a loss function that measures the reprojection error (Section 7.1.2.2).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>获取用于监督的 3D 真实数据是一个昂贵且繁琐的过程，即使对于小规模训练也是如此。然而，获取多视角 2D 或 \({2.5}\mathrm{D}\) 图像进行训练相对容易。该类别的方法利用了这样一个事实：如果估计的 \(3\mathrm{D}\) 形状尽可能接近真实值，则 3D 模型的视图与重建的 3D 模型在任何这些视图上的投影之间的差异也会被最小化。实现这个想法需要定义一个投影算子，该算子从给定视点渲染重建的 \(3\mathrm{D}\) 模型（第 7.1.2.1 节），以及一个测量重投影误差的损失函数（第 7.1.2.2 节）。</p></div><p>7.1.2.1 Projection operators: Techniques from projective geometry can be used to render views of a 3D object. However, to enable end-to-end training without gradient approximation [55], the projection operator should be differentiable. Gadelha et al. [27] introduced a differentiable projection operator \(P\) defined as \(P\left( {\left( {i,j}\right) ,V}\right)  =\) \(1 - {e}^{-\mathop{\sum }\limits_{k}V\left( {i,j,k}\right) }\) ,where \(V\) is the \(3\mathrm{D}\) voxel grid. This operator sums up the voxel occupancy values along each line of sight. However, it assumes an orthographic projection. Loper and Black [94] introduced OpenDR, an approximate differentiable renderer, which is suitable for orthographic and perspective projections.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>7.1.2.1 投影算子：可以使用射影几何中的技术来渲染 3D 物体的视图。然而，为了实现端到端训练而不进行梯度近似 [55]，投影算子应该是可微的。Gadelha 等人 [27] 引入了一种可微的投影算子 \(P\)，定义为 \(P\left( {\left( {i,j}\right) ,V}\right)  =\) \(1 - {e}^{-\mathop{\sum }\limits_{k}V\left( {i,j,k}\right) }\)，其中 \(V\) 是 \(3\mathrm{D}\) 体素网格。该算子沿每条视线汇总体素占用值。然而，它假设为正交投影。Loper 和 Black [94] 引入了 OpenDR，一种适用于正交和透视投影的近似可微渲染器。</p></div><p>Petersen et al. [95] introduced a novel \({C}^{\infty }\) smooth differentiable renderer for image-to-geometry reconstruction. The idea is that instead of taking a discrete decision of which triangle is the visible from a pixel, the approach softly blends their visibility. Taking the weighted SoftMin of the \(z\) - positions in the camera space constitutes a smooth z-buffer, which leads to a \({C}^{\infty }\) smooth renderer,where the \(z\) -positions of triangles are differentiable with respect to occlusions. In previous renderers,only the \({xy}\) -coordinates were locally differentiable with respect to occlusions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Petersen 等人 [95] 引入了一种新颖的 \({C}^{\infty }\) 平滑可微渲染器，用于图像到几何体的重建。其想法是，不是对哪个三角形从像素中可见做出离散决策，而是该方法柔和地混合它们的可见性。在相机空间中对 \(z\) - 位置进行加权 SoftMin 形成一个平滑的 z 缓冲区，这导致一个 \({C}^{\infty }\) 平滑渲染器，其中三角形的 \(z\) - 位置相对于遮挡是可微的。在之前的渲染器中，只有 \({xy}\) - 坐标相对于遮挡是局部可微的。</p></div><p>Finally, instead of using fixed renderers, Rezende et al. [96] proposed a learned projection operator, or a learnable camera, which is built by first applying an affine transformation to the reconstructed volume, followed by a combination of 3D and 2D convolutional layers, which map the 3D volume onto a 2D image.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最后，Rezende 等人 [96] 提出了一个学习的投影算子，或可学习的相机，该相机首先通过对重建体积应用仿射变换，然后结合 3D 和 2D 卷积层，将 3D 体积映射到 2D 图像。</p></div><p>7.1.2.2 Re-projection loss functions: There are several loss functions that have been proposed for \(3\mathrm{D}\) reconstruction using 2D supervision. We classify them into two main categories; (1) silhouette-based and (2) normal and depth-based loss functions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>7.1.2.2 重新投影损失函数：已经提出了几种用于\(3\mathrm{D}\)重建的损失函数，采用2D监督。我们将它们分为两大类；（1）基于轮廓的和（2）基于法线和深度的损失函数。</p></div><p>(1) Silhouette-based loss functions. The idea is that a \(2\mathrm{D}\) silhouette projected from the reconstructed volume, under certain camera intrinsic and extrinsic parameters, should match the ground truth 2D silhouette of the input image. The discrepancy, which is inspired by space carving, is then:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>（1）基于轮廓的损失函数。其思想是，从重建体积投影出的\(2\mathrm{D}\)轮廓，在特定的相机内参和外参下，应与输入图像的真实2D轮廓相匹配。由空间雕刻启发的差异为：</p></div><p></p>\[{\mathcal{L}}_{\text{proj }}\left( \mathbf{I}\right)  = \frac{1}{n}\mathop{\sum }\limits_{{j = 1}}^{n}d\left( {P\left( {f\left( \mathbf{I}\right) ;{\alpha }^{\left( j\right) }}\right) ,{S}^{\left( j\right) }}\right) , \tag{8}\]<p></p><p>where \({S}^{\left( j\right) }\) is the \(j\) -th ground truth 2D silhouette of the original 3D object \(X,n\) is the number of silhouettes or views used for each \(3\mathrm{D}\) model, \(P\left( \cdot \right)\) is a \(3\mathrm{D}\) to \(2\mathrm{D}\) projection function,and \({\alpha }^{\left( j\right) }\) are the camera parameters of the \(j\) -th silhouette. The distance metric \(d\left( {\cdot , \cdot  }\right)\) can be the standard \({L}_{2}\) metric [77],the negative Intersection over Union (IoU) between the true and reconstructed silhouettes [55], or the binary cross-entropy loss [4], [24].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\({S}^{\left( j\right) }\)是原始3D对象的\(j\) -th真实2D轮廓，\(X,n\)是用于每个\(3\mathrm{D}\)模型的轮廓或视图数量，\(P\left( \cdot \right)\)是从\(3\mathrm{D}\)到\(2\mathrm{D}\)的投影函数，\({\alpha }^{\left( j\right) }\)是\(j\) -th轮廓的相机参数。距离度量\(d\left( {\cdot , \cdot  }\right)\)可以是标准\({L}_{2}\)度量[77]，真实和重建轮廓之间的负交并比（IoU）[55]，或二元交叉熵损失[4]，[24]。</p></div><p>Kundu et al. [31] introduced the render-and-compare loss, which is defined in terms of the IoU between the ground-truth silhouette \({G}_{s}\) and the rendered silhouette \({R}_{s}\) , and the \({L}_{2}\) distance between the ground-truth depth \({G}_{d}\) and the rendered depth \({R}_{d}\) ,i.e.,</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Kundu等人[31]提出了渲染与比较损失，其定义为真实轮廓\({G}_{s}\)与渲染轮廓\({R}_{s}\)之间的IoU，以及真实深度\({G}_{d}\)与渲染深度\({R}_{d}\)之间的\({L}_{2}\)距离，即：</p></div><p></p>\[{\mathcal{L}}_{r} = 1 - \operatorname{IoU}\left( {{R}_{s},{G}_{s};{I}_{s}}\right)  + {d}_{{L}_{2}}\left( {{R}_{d},{G}_{d};{I}_{d}}\right) . \tag{9}\]<p></p><p>Here, \({I}_{s}\) and \({I}_{d}\) are binary ignore masks that have value of one at pixels which do not contribute to the loss. Since this loss is not differentiable, Kundu et al. [31] used finite difference to approximate its gradients.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这里，\({I}_{s}\)和\({I}_{d}\)是二元忽略掩码，在不贡献于损失的像素上取值为1。由于该损失不可微，Kundu等人[31]使用有限差分来近似其梯度。</p></div><p>Silhouette-based loss functions cannot distinguish between some views, e.g., front and back. To alleviate this issue, Insafutdinov and Dosovitskiy [77] use multiple pose regressors during training, each one using silhouette loss. The overall network is trained with the min of the individual losses. The predictor with minimum loss is used at test time.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于轮廓的损失函数无法区分某些视图，例如前视和后视。为了解决这个问题，Insafutdinov和Dosovitskiy[77]在训练期间使用多个姿态回归器，每个回归器使用轮廓损失。整个网络通过各个损失的最小值进行训练。在测试时使用最小损失的预测器。</p></div><p>Gwak et al. [97] minimize the reprojection error subject to the reconstructed shape being a valid member of a certain class, e.g., chairs. To constrain the reconstruction to remain in the manifold of the shape class, the approach defines a barrier function \(\phi\) ,which is set to be 1 if the shape is in the manifold and 0 otherwise. The loss function is then:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Gwak等人[97]最小化重投影误差，前提是重建的形状是某个类别的有效成员，例如椅子。为了约束重建保持在形状类别的流形中，该方法定义了一个障碍函数\(\phi\)，如果形状在流形中则设置为1，否则为0。损失函数为：</p></div><p></p>\[\mathcal{L} = {\mathcal{L}}_{\text{reprojection }} - \frac{1}{t}\log \phi \left( \widehat{X}\right) . \tag{10}\]<p></p><p>The barrier function is learned as the discriminative function of a GAN, see Section 7.3.2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>障碍函数作为GAN的判别函数进行学习，见第7.3.2节</p></div><p>Finally, Tulsiani et al. [8] define the re-projection loss using a differentiable ray consistency loss for volumetric reconstruction. First,it assumes that the estimated shape \(\widehat{X}\) is defined in terms of the probability occupancy grid. Let (O,C)be an observation-camera pair. Let also \(\mathcal{R}\) be a set of rays where each ray \(r \in  \mathcal{R}\) has the camera center as origin and is casted through the image plane of the camera \(C\) . The ray consistency loss is then defined as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最后，Tulsiani等人[8]使用可微分的光线一致性损失定义重新投影损失，用于体积重建。首先，它假设估计的形状\(\widehat{X}\)是通过概率占用网格定义的。设（O,C）为观察-相机对。设\(\mathcal{R}\)为一组光线，其中每条光线\(r \in  \mathcal{R}\)的相机中心为原点，并通过相机\(C\)的图像平面投射。光线一致性损失定义为：</p></div><p></p>\[{\mathcal{L}}_{\text{ray_cons }}\left( {\widehat{X};\left( {O,C}\right) }\right)  = \mathop{\sum }\limits_{{r \in  \mathcal{R}}}{\mathcal{L}}_{r}\left( \widehat{X}\right) , \tag{11}\]<p></p><p>where \({\mathcal{L}}_{r}\left( \widehat{X}\right)\) captures if the inferred 3D model \(\widehat{X}\) correctly explains the observations associated with the specific ray \(r\) . If the observation \(O\) is a ground-truth foreground mask taking values 0 at foreground pixels and 1 elsewhere,then \({\mathcal{L}}_{r}\) is the probability that the ray \(r\) hits a surface voxel weighted by the mask value at the pixel associated with the ray \(r\) . This loss is differentiable with respect to the network predictions. Note that when using foreground masks as observations, this loss, which requires known camera parameters, is similar to the approaches designed to specifically use mask supervision where a learned [25] or a fixed [4] reprojection function is used. Also, the binary cross-entropy loss used in \(\left\lbrack  4\right\rbrack  ,\left\lbrack  {24}\right\rbrack\) can be thought of as an approximation of the one derived using ray consistency.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\({\mathcal{L}}_{r}\left( \widehat{X}\right)\)捕捉到推断的3D模型\(\widehat{X}\)是否正确解释了与特定光线\(r\)相关的观察。如果观察\(O\)是一个真实的前景掩码，在前景像素处取值为0，在其他地方取值为1，则\({\mathcal{L}}_{r}\)是光线\(r\)击中表面体素的概率，按与光线\(r\)相关的像素的掩码值加权。这个损失对网络预测是可微分的。请注意，当使用前景掩码作为观察时，这个损失需要已知的相机参数，类似于专门使用掩码监督的方法，其中使用了学习到的[25]或固定的[4]重投影函数。此外，在\(\left\lbrack  4\right\rbrack  ,\left\lbrack  {24}\right\rbrack\)中使用的二元交叉熵损失可以被视为使用光线一致性推导的损失的近似。</p></div><p>(2) Surface normal and depth-based loss. Additional cues such as surface normals and depth values can be used to guide the training process. Let \({n}_{x,y} = \left( {{n}_{a},{n}_{b},{n}_{c}}\right)\) be a normal vector to a surface at a point(x,y,z). The vectors \({n}_{x} = \left( {0, - {n}_{c},{n}_{b}}\right)\) and \(\left( {-{n}_{c},\overline{0},{n}_{a}}\right)\) are orthogonal to \({n}_{x,y}\) . By normalizing them,we obtain two vectors \({n}_{x}^{\prime } = \left( {0, - 1,{n}_{b}/{n}_{c}}\right)\) and \({n}_{y}^{\gamma } = \left( {-1,0,{n}_{a}/{n}_{c}}\right)\) . The normal loss tries to guarantee that the voxels at \(\left( {x,y,z}\right)  \pm  {n}_{x}^{\prime }\) and \(\left( {x,y,z}\right)  \pm  {n}_{y}^{r}\) should be 1 to match the estimated surface normals. This constraint only applies when the target voxels are inside the estimated silhouette. The projected surface normal loss is then:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(2) 基于表面法线和深度的损失。可以使用额外的线索，如表面法线和深度值，来指导训练过程。设\({n}_{x,y} = \left( {{n}_{a},{n}_{b},{n}_{c}}\right)\)为点(x,y,z)处表面的法向量。向量\({n}_{x} = \left( {0, - {n}_{c},{n}_{b}}\right)\)和\(\left( {-{n}_{c},\overline{0},{n}_{a}}\right)\)与\({n}_{x,y}\)正交。通过对它们进行归一化，我们得到两个向量\({n}_{x}^{\prime } = \left( {0, - 1,{n}_{b}/{n}_{c}}\right)\)和\({n}_{y}^{\gamma } = \left( {-1,0,{n}_{a}/{n}_{c}}\right)\)。法线损失试图保证在\(\left( {x,y,z}\right)  \pm  {n}_{x}^{\prime }\)和\(\left( {x,y,z}\right)  \pm  {n}_{y}^{r}\)处的体素应该为1，以匹配估计的表面法线。这个约束仅适用于目标体素在估计的轮廓内时。投影表面法线损失为：</p></div><p></p>\[{\mathcal{L}}_{\text{normal }} = {\left( 1 - {v}_{x,y - 1,z + \frac{{n}_{b}}{{n}_{c}}}\right) }^{2} + {\left( 1 - {v}_{x,y + 1,z - \frac{{n}_{b}}{{n}_{c}}}\right) }^{2} +\]<p></p><p></p>\[{\left( 1 - {v}_{x - 1,y,z + \frac{{n}_{a}}{{n}_{c}}}\right) }^{2} + {\left( 1 - {v}_{x + 1,y,z - \frac{{n}_{a}}{{n}_{c}}}\right) }^{2}. \tag{12}\]<p></p><p>This loss has been used by Wu et al. [6], which includes, in addition to the normal loss, the projected depth loss. The idea is that the voxel with depth \({v}_{x,y,{d}_{x,y}}\) should be 1,and all voxels in front of it should be 0 . The depth loss is then defined as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这个损失已被吴等人[6]使用，除了法线损失外，还包括投影深度损失。其思想是深度为\({v}_{x,y,{d}_{x,y}}\)的体素应该为1，而它前面的所有体素应该为0。深度损失定义为：</p></div><p></p>\[{\mathcal{L}}_{\text{depth }}\left( {x,y,z}\right)  = \left\{  \begin{array}{ll} {v}_{x,y,z}^{2} &#x26; \text{ if }z &#x3C; {d}_{x,y}, \\  {\left( 1 - {v}_{x,y,z}\right) }^{2} &#x26; \text{ if }z = {d}_{x,y}, \\  0 &#x26; \text{ otherwise. } \end{array}\right.  \tag{13}\]<p></p><p>This ensures the estimated 3D shape matches the estimated depth values.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这确保了估计的三维形状与估计的深度值相匹配。</p></div><p>(3) Combining multiple losses. One can also combine 2D and 3D losses. This is particularly useful when some ground-truth 3D data is available. One can for example train first the network using 3D supervision, and then fine-tune it using 2D supervision. Yan et al. [4], on the other hand, take the weighted sum of a 2D and a 3D loss.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(3) 结合多种损失。还可以结合二维和三维损失。当有一些真实的三维数据可用时，这尤其有用。例如，可以先使用三维监督训练网络，然后再使用二维监督进行微调。另一方面，Yan等人[4]则取二维和三维损失的加权和。</p></div><p>In addition to the reconstruction loss, one can impose additional constraints to the solution. For instance, Kato et al. [55] used a weighted sum of silhouette loss, defined as the negative intersection over union (IoU) between the true and reconstructed silhouettes, and a smoothness loss. For surfaces, the smoothness loss ensures that the angles between adjacent faces is close to \({180}^{o}\) ,encouraging flatness.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>除了重建损失外，还可以对解决方案施加额外的约束。例如，Kato等人[55]使用了轮廓损失的加权和，该损失定义为真实轮廓和重建轮廓之间的负交并比（IoU），以及平滑损失。对于表面，平滑损失确保相邻面的角度接近\({180}^{o}\)，鼓励平坦性。</p></div><p>7.1.2.3 Camera parameters and viewpoint estimation: Reprojection-based loss functions use the camera parameters to render the estimated \(3\mathrm{D}\) shape onto image planes. Some methods assume the availability of one or multiple observation-camera pairs [4], [8], [10]. Here, the observation can be an RGB image, a silhouette/foreground mask or a depth map of the target 3D shape. Other methods optimize at the same time for the camera parameters and the 3D reconstruction that best describe the input [27], [77].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>7.1.2.3 相机参数和视点估计：基于重投影的损失函数使用相机参数将估计的\(3\mathrm{D}\)形状渲染到图像平面上。一些方法假设存在一个或多个观察-相机对[4]，[8]，[10]。在这里，观察可以是RGB图像、轮廓/前景掩码或目标三维形状的深度图。其他方法则同时优化相机参数和最佳描述输入的三维重建[27]，[77]。</p></div><p>Gadelha et al. [27] encode an input image into a latent representation and a pose code using fully-connected layers. The pose code is then used as input to the 2D projection module,which renders the estimated \(3\mathrm{D}\) volume onto the view of the input. Insafutdinov and Dosovitskiy [77], on the other hand,take two views of the same object, and predict the corresponding shape (represented as a point cloud) from the first view, and the camera pose (represented as a quaternion) from the second one. The approach then uses a differentiable projection module to generate the view of the predicted shape from the predicted camera pose. The shape and pose predictor is implemented as a convolutional network with two branches. The network starts with a convolutional encoder with a total of 7 layers followed by 2 shared fully connected layers, after which the network splits into two branches for shape and pose prediction. The pose branch is implemented as a multi-layer perceptron.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Gadelha等人[27]将输入图像编码为潜在表示和姿态代码，使用全连接层。然后将姿态代码作为输入传递给二维投影模块，该模块将估计的\(3\mathrm{D}\)体积渲染到输入的视图上。另一方面，Insafutdinov和Dosovitskiy[77]则获取同一物体的两个视图，并从第一个视图预测相应的形状（表示为点云），从第二个视图预测相机姿态（表示为四元数）。该方法随后使用可微分投影模块从预测的相机姿态生成预测形状的视图。形状和姿态预测器实现为具有两个分支的卷积网络。网络以一个总共7层的卷积编码器开始，后面跟着2个共享的全连接层，然后网络分为两个分支进行形状和姿态预测。姿态分支实现为多层感知器。</p></div><p>There has been a few papers that only estimate the camera pose [70], [98], [99]. Unlike techniques that do simultaneously reconstruction, these approaches are trained with only pose annotations. For instance, Kendall et al. [98] introduced PoseNet, a convolutional neural network which estimates the camera pose from a single image. The network, which represents the camera pose using its location vector and orientation quaternion,is trained to minimize the \({L}_{2}\) loss between the ground-truth and the estimate pose. Su et al. [99] found that CNNs trained for viewpoint estimation of one class do not perform well on another class, possibly due to the huge geometric variation between the classes. As such, they proposed a network architecture where the lower layers (both convolutional layers and fully connected layers) are shared by all classes, while class-dependent fully-connected layers are stacked over them.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>有一些论文仅估计相机姿态[70]，[98]，[99]。与同时进行重建的技术不同，这些方法仅使用姿态注释进行训练。例如，Kendall等人[98]引入了PoseNet，这是一种从单张图像估计相机姿态的卷积神经网络。该网络使用其位置向量和方向四元数表示相机姿态，旨在最小化真实姿态与估计姿态之间的\({L}_{2}\)损失。Su等人[99]发现，针对一种类别的视点估计训练的CNN在另一类别上表现不佳，可能是由于类别之间巨大的几何变化。因此，他们提出了一种网络架构，其中较低层（卷积层和全连接层）由所有类别共享，而类别依赖的全连接层则堆叠在其上。</p></div><h3>7.2 Training with video supervision</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>7.2 使用视频监督进行训练</h3></div><p>Another approach to significantly lower the level of supervision required to learn the \(3\mathrm{D}\) geometry of objects is by replacing 3D supervision with motion. To this end, Novotni et al. [100] used Structure-from Motion (SfM) to generate a supervisory signal from videos. That is, at training, the approach takes a video sequences, generates a partial point cloud and the relative camera parameters using SfM [101]. Each RGB frame is then processed with a network that estimates a depth map, an uncertainty map, and the camera parameters. The different depth estimates are fused, using the estimated camera parameters, into a partial point cloud, which is further processed for completion using the point cloud completion network PointNet [87]. The network is trained using the estimates of the SfM as supervisory signals. That is, the loss functions measure the discrepancy between the depth maps estimated by the network and the depth maps estimated by SfM, and between the camera parameters estimated by the network and those estimated by SfM. At test time, the network is able to recover a full 3D geometry from a single RGB image.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>另一种显著降低学习\(3\mathrm{D}\)物体几何形状所需监督水平的方法是用运动替代三维监督。为此，Novotni等人[100]使用运动结构（SfM）从视频生成监督信号。也就是说，在训练时，该方法采用视频序列，使用SfM生成部分点云和相对相机参数[101]。然后，每个RGB帧通过一个网络进行处理，该网络估计深度图、不确定性图和相机参数。不同的深度估计使用估计的相机参数融合成一个部分点云，随后使用点云补全网络PointNet[87]进行补全。该网络使用SfM的估计作为监督信号进行训练。也就是说，损失函数测量网络估计的深度图与SfM估计的深度图之间的差异，以及网络估计的相机参数与SfM估计的相机参数之间的差异。在测试时，该网络能够从单张RGB图像恢复完整的三维几何形状。</p></div><!-- Media --><!-- figureText: (a) Joint 2D-3D embedding. Testing Image 3D Encoder Generator 3D model (b) TL-network. Discriminator Testing Image 3D Encoder Generator 3D Encoder A 3D model (c) 3D-VAE-GAN architecture. --><img src="https://cdn.noedgeai.com/bo_d163t43ef24c73d1le4g_15.jpg?x=141&#x26;y=119&#x26;w=746&#x26;h=670&#x26;r=0"><p>Fig. 5: At test time, the 3D encoder and the discriminator are removed and only the highlighted modules are kept.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图5：在测试时，3D编码器和鉴别器被移除，仅保留高亮模块。</p></div><!-- Media --><h3>7.3 Training procedure</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>7.3 训练过程</h3></div><p>In addition to the datasets, loss functions, and degree of supervision, there are a few practical aspects that one needs to consider when training deep learning architectures for \(3\mathrm{D}\) reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>除了数据集、损失函数和监督程度外，在训练深度学习架构进行\(3\mathrm{D}\)重建时，还需要考虑一些实际方面。</p></div><h4>7.3.1 Joint 2D-3D embedding</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>7.3.1 联合2D-3D嵌入</h4></div><p>Most of the state-of-the-art works map the input (e.g., RGB images) into a latent representation, and then decode the latent representation into a 3D model. A good latent representation should be (1) generative in 3D, i.e., we should be able to reconstruct objects in 3D from it, and (2) it must be predictable from \(2\mathrm{D}\) ,i.e.,we should be able to easily infer this representation from images [25]. Achieving these two goals has been addressed by using TL-embedding networks during the training phase, see Fig. 5-(a) and (b). It is composed of two jointly trained encoding branches: the \(2\mathrm{D}\) encoder and the \(3\mathrm{D}\) encoder. They map,respectively,a \(2\mathrm{D}\) image and its corresponding \(3\mathrm{D}\) annotation into the same point in the latent space [24], [25].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>大多数最先进的工作将输入（例如，RGB图像）映射到潜在表示中，然后将潜在表示解码为3D模型。一个好的潜在表示应该是（1）在3D中是生成的，即我们应该能够从中重建3D物体，以及（2）必须能够从\(2\mathrm{D}\)中预测，即我们应该能够轻松地从图像中推断出该表示[25]。通过在训练阶段使用TL-嵌入网络来实现这两个目标，见图5-(a)和(b)。它由两个联合训练的编码分支组成：\(2\mathrm{D}\)编码器和\(3\mathrm{D}\)编码器。它们分别将\(2\mathrm{D}\)图像及其对应的\(3\mathrm{D}\)注释映射到潜在空间中的同一点[24]，[25]。</p></div><p>Gidhar et al. [25], which use the TL-embedding network to reconstruct volumetric shapes from RGB images, train the network using batches of (image, voxel) pairs. The images are generated by rendering the \(3\mathrm{D}\) model and the network is then trained in a three stage procedure.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Gidhar等人[25]使用TL-嵌入网络从RGB图像重建体积形状，使用（图像，体素）对的批次训练网络。图像是通过渲染\(3\mathrm{D}\)模型生成的，然后以三阶段程序训练网络。</p></div><ul>
<li>In the first stage,the \(3\mathrm{D}\) encoder part of the network and its decoder are initialized at random. They are then trained, end-to-end with the sigmoid cross-entropy loss, independently of the 2D encoder.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>在第一阶段，网络的\(3\mathrm{D}\)编码器部分及其解码器随机初始化。然后，它们以端到端的方式使用sigmoid交叉熵损失进行训练，与2D编码器独立。</li>
</ul></div><ul>
<li>In the second stage,the \(2\mathrm{D}\) encoder is trained to regress the latent representation. The encoder generates the embedding for the voxel, and the image network is trained to regress the embedding.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>在第二阶段，训练\(2\mathrm{D}\)编码器以回归潜在表示。编码器为体素生成嵌入，图像网络被训练以回归该嵌入。</li>
</ul></div><ul>
<li>The final stage jointly fine-tunes the entire network.</li>
</ul><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ul>
<li>最后阶段联合微调整个网络。</li>
</ul></div><p>This approach has been extended by Li et al. [79] and Mandikal et al. [21] for point cloud-based 3D reconstruction by replacing the volume encoder by a point cloud auto-encoder.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Li等人[79]和Mandikal等人[21]通过用点云自编码器替换体积编码器，扩展了这种方法以进行基于点云的3D重建。</p></div><h4>7.3.2 Adversarial training</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>7.3.2 对抗训练</h4></div><p>In general, a good reconstruction model should be able to go beyond what has been seen during training. Networks trained with standard procedures may not generalize well to unseen data. Also, Yang et al. [46] noted that the results of standard techniques tend to be grainy and lack fine details. To overcome these issues, several recent papers train their networks with adversarial loss by using Generative Adversarial Networks (GAN). GANs generate a signal from a given random vector [102]. Conditional GANs, on the other hand, conditions the generated signal on the input image(s),see Fig. 5-(c). It consists of a generator \(g\) ,which mirrors the encoder \(h\) ,and a discriminator \(D\) ,which mirrors the generator.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>一般来说，一个好的重建模型应该能够超越训练期间所见的内容。使用标准程序训练的网络可能无法很好地泛化到未见数据。此外，Yang等人[46]指出，标准技术的结果往往颗粒感强，缺乏细节。为了解决这些问题，最近几篇论文通过使用生成对抗网络（GAN）以对抗损失训练网络。GAN从给定的随机向量生成信号[102]。条件GAN则根据输入图像条件生成信号，见图5-(c)。它由一个生成器\(g\)组成，镜像编码器\(h\)，以及一个镜像生成器的鉴别器\(D\)。</p></div><p>In the case of \(3\mathrm{D}\) reconstruction,the encoder can be a ConvNet/ResNet [46], [103] or a variational auto-encoder (VAE) [17]. The generator decodes the latent vector \(\mathbf{x}\) into a 3D shape \(X = g\left( \mathbf{x}\right)\) . The discriminator,which is only used during training, evaluates the authenticity of the decoded data. It outputs a confidence \(C\left( X\right)\) between 0 and 1 of whether the 3D object \(X\) is real or synthetic,i.e.,coming from the generator. The goal is to jointly train the generator and the discriminator to make the reconstructed shape as close as possible to the ground truth.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在\(3\mathrm{D}\)重建的情况下，编码器可以是ConvNet/ResNet[46]，[103]或变分自编码器（VAE）[17]。生成器将潜在向量\(\mathbf{x}\)解码为3D形状\(X = g\left( \mathbf{x}\right)\)。鉴别器仅在训练期间使用，评估解码数据的真实性。它输出一个介于0和1之间的置信度\(C\left( X\right)\)，表示3D物体\(X\)是真实的还是合成的，即来自生成器。目标是联合训练生成器和鉴别器，使重建的形状尽可能接近真实值。</p></div><p>Central to GAN is the adversarial loss function used to jointly train the discriminator and the generator. Following Goodfellow et al. [102], Wu et al. [17] use the binary cross entropy as the classification loss. The overall adversarial loss function is defined as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>GAN的核心是用于联合训练鉴别器和生成器的对抗损失函数。根据Goodfellow等人[102]，Wu等人[17]使用二元交叉熵作为分类损失。整体对抗损失函数定义为：</p></div><p></p>\[{\mathcal{L}}_{{3D} - {GAN}} = \log \left( {D\left( X\right) }\right)  + \log \left( {1 - D\left( {g\left( \mathbf{x}\right) }\right) }\right) . \tag{14}\]<p></p><p>Here \(\mathbf{x} = h\left( \mathbf{I}\right)\) where \(\mathbf{I}\) is the \(2\mathrm{D}\) images(s)of the training shape \(X\) . Yang et al. [46],[103] observed that the original GAN loss function presents an overall loss for both real and fake input. They then proposed to use the WGAN-GP loss [104], [105], which separately represents the loss for generating fake reconstruction pairs and the loss for discriminating fake and real reconstruction pairs, see [104], 105 for the details.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这里\(\mathbf{x} = h\left( \mathbf{I}\right)\)是训练形状的\(2\mathrm{D}\)图像\(X\)。杨等人[46],[103]观察到，原始GAN损失函数对真实和虚假输入都呈现出整体损失。然后他们提出使用WGAN-GP损失[104],[105]，该损失分别表示生成虚假重建对的损失和区分虚假与真实重建对的损失，详细信息见[104],[105]。</p></div><p>To jointly train the three components of the network, i.e., the encoder, the generator, and the discriminator, the overall loss is defined as the sum of the reconstruction loss, see Section 7.1, and the GAN loss. When the network uses a variational auto-encoder, e.g., the 3D VAE-GAN [17], then an additional term is added to the overall loss in order to push the variational distribution towards the prior distribution. For example, Wu et al. [17] used a KL-divergence metric, and a multivariate Gaussian distribution with zero-mean and unit variance as a prior distribution.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了联合训练网络的三个组件，即编码器、生成器和判别器，整体损失被定义为重建损失的总和，见第7.1节，以及GAN损失。当网络使用变分自编码器时，例如3D VAE-GAN[17]，则在整体损失中添加一个额外项，以推动变分分布朝向先验分布。例如，吴等人[17]使用KL散度度量，并以零均值和单位方差的多元高斯分布作为先验分布。</p></div><p>The potential of GANs is huge, because they can learn to mimic any distribution of data. They are also very suitable for single-view 3D shape reconstruction. They have been used for volumetric [13], [17], [30], [40], [46], [103] and point cloud [74], [75] reconstruction. They have been used with 3D supervision [17], [30], [40], [46], [103] and with 2D supervision as in [13], [27], [97], see Section 7.1.2. The latter methods train a single discriminator with \(\overline{2D}\) silhouette images. However, among plausible shapes, there are still multiple shapes that fit the \(2\mathrm{D}\) image equally well. To address this ambiguity, Wu et al. [91] used the discriminator of the GAN to penalize the \(3\mathrm{D}\) estimator if the predicted 3D shape is unnatural. Li et al. [106], on the other hand, use multiple discriminators, one for each view, resulting in a better generation quality.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>GAN的潜力巨大，因为它们可以学习模仿任何数据分布。它们也非常适合单视图3D形状重建。它们已被用于体积[13],[17],[30],[40],[46],[103]和点云[74],[75]重建。它们已与3D监督[17],[30],[40],[46],[103]和2D监督一起使用，如[13],[27],[97]所示，见第7.1.2节。后者方法使用\(\overline{2D}\)轮廓图像训练单个判别器。然而，在合理的形状中，仍然存在多个形状与\(2\mathrm{D}\)图像同样匹配。为了解决这种模糊性，吴等人[91]使用GAN的判别器来惩罚\(3\mathrm{D}\)估计器，如果预测的3D形状不自然。另一方面，李等人[106]使用多个判别器，每个视图一个，从而提高生成质量。</p></div><p>GANs are hard to train, especially for the complex joint data distribution over \(3\mathrm{D}\) objects of many categories and orientations. They also become unstable for high-resolution shapes. In fact, one must carefully balance the learning of the generator and the discriminator, otherwise the gradients can vanish, which will prevent improvement [40]. To address this issue, Smith and Meger [40] and later Wu et al. [91] used as a training objective the Wasserstein distance normalized with the gradient penalization.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>GAN的训练很困难，尤其是对于许多类别和方向的\(3\mathrm{D}\)对象的复杂联合数据分布。对于高分辨率形状，它们也会变得不稳定。事实上，必须仔细平衡生成器和判别器的学习，否则梯度可能会消失，这将阻止改进[40]。为了解决这个问题，史密斯和梅格尔[40]以及后来的吴等人[91]使用归一化的Wasserstein距离作为训练目标，并进行梯度惩罚。</p></div><h4>7.3.3 Joint training with other tasks</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>7.3.3 与其他任务的联合训练</h4></div><p>Jointly training for reconstruction and segmentation leads to improved performance in both tasks, when compared to training for each task individually. Mandikal et al. [107] proposed an approach, which generates a part-segmented 3D point cloud from one RGB image. The idea is to enable propagating information between the two tasks so as to generate more faithful part reconstructions while also improving segmentation accuracy. This is done using a weighted sum of a reconstruction loss, defined using the Chamfer distance, and a segmentation loss, defined using the symmetric softmax cross-entropy loss.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>与重建和分割的联合训练相比，单独训练每个任务时，两个任务的性能都有所提高。Mandikal等人[107]提出了一种方法，从一张RGB图像生成部分分割的3D点云。其想法是使两个任务之间的信息传播，从而生成更真实的部分重建，同时提高分割精度。这是通过使用重建损失的加权和来实现的，该损失使用Chamfer距离定义，以及使用对称softmax交叉熵损失定义的分割损失。</p></div><h2>8 APPLICATIONS AND SPECIAL CASES</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>8 应用与特殊案例</h2></div><p>Image-based 3D reconstruction is an important problem and a building block to many applications ranging from robotics and autonomous navigation to graphics and entertainment. While some of these applications deal with generic objects, many of them deal with objects that belong to specific classes such as human bodies or body parts (e.g., faces and hands), animals in the wild, and cars. The techniques described above can be applied to these specific classes of shapes. However, the quality of the reconstruction can be significantly improved by designing customised methods that leverage the prior knowledge of the shape class. In this section, we will briefly summarize recent developments in the image-based 3D reconstruction of human body shapes (Section 8.1), and body parts such as faces (Section 8.2). We will also discuss in Section 8.3 methods that deal with the parsing entire \(3\mathrm{D}\) scenes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于图像的3D重建是一个重要问题，也是许多应用的基础，从机器人技术和自主导航到图形和娱乐。虽然其中一些应用处理通用对象，但许多应用处理属于特定类别的对象，如人体或身体部位（例如，面部和手）、野生动物和汽车。上述技术可以应用于这些特定类别的形状。然而，通过设计利用形状类别先验知识的定制方法，可以显著提高重建质量。在本节中，我们将简要总结人体形状的基于图像的3D重建的最新进展（第8.1节），以及面部等身体部位（第8.2节）。我们还将在第8.3节讨论处理整个\(3\mathrm{D}\)场景解析的方法。</p></div><h3>8.1 3D human body reconstruction</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>8.1 3D人体重建</h3></div><p>3D static and dynamic digital humans are essential for a variety of applications ranging from gaming, visual effects to free-viewpoint videos. However, high-end 3D capture solutions use a large number of cameras and active sensors, and are restricted to professionals as they operate under controlled lighting conditions and studio settings. With the avenue of deep learning techniques, several papers have explored more lightweight solutions that are able to recover the 3D human shape and pose from a few RGB images. We can distinguish two classes of methods; (1) volumetric methods (Section 4), and (2) template or parameteric-based methods (Section 5.2). Some methods in both categories reconstruct naked 3D human body shapes [108], [109], while others recover also cloths and garments [110], [111].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D静态和动态数字人类对于从游戏、视觉效果到自由视角视频等多种应用至关重要。然而，高端3D捕捉解决方案使用大量相机和主动传感器，并且仅限于专业人士，因为它们在受控的照明条件和工作室环境下操作。随着深度学习技术的发展，一些论文探索了更轻量级的解决方案，这些方案能够从少量RGB图像中恢复3D人类形状和姿态。我们可以区分两类方法；（1）体积方法（第4节），和（2）模板或参数化方法（第5.2节）。这两类中的一些方法重建裸露的3D人类身体形状[108]，[109]，而其他方法则同时恢复衣物和服装[110]，[111]。</p></div><h4>8.1.1 Parametric methods</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>8.1.1 参数化方法</h4></div><p>Parametric methods regularize the problem using statistical models such as morphable models [112], SCAPE [113], and SMPL [114]. The problem of 3D human body shape reconstruction then boils down to estimating the parameters of the model.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>参数化方法使用统计模型（如可变形模型[112]、SCAPE[113]和SMPL[114]）来规范化问题。3D人类身体形状重建的问题归结为估计模型的参数。</p></div><p>Dibra et al. [108] used an encoder followed by three fully connected layers to regress the SCAPE parameters from one or multiple silhouette images. Later, Dibra et al. [109] first learn a common embedding of \(2\mathrm{D}\) silhouettes and 3D human body shapes (see Section 7.3.1). The latter are represented using their Heat Kernel Signatures [115]. Both methods can only predict naked body shapes in nearly neutral poses.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Dibra等[108]使用编码器和三个全连接层从一个或多个轮廓图像中回归SCAPE参数。后来，Dibra等[109]首先学习一个共同的嵌入\(2\mathrm{D}\)轮廓和3D人类身体形状（见第7.3.1节）。后者使用其热核签名[115]表示。两种方法只能预测几乎中性姿态下的裸露身体形状。</p></div><p>SMPL has the advantage of encoding in a disentangled manner both the shape, the pose, and the pose specific details, and thus it has been extensively used in deep learning-based human body shape reconstruction [110], [116], [117], [118]. Bogo et al. [116] proposed SMPLify, the first 3D human pose and shape reconstruction from one image. They first used a CNN-based architecture, DeepCut [119], to estimate the 2D joint locations. They then fit an SMPL model to the predicted \(2\mathrm{D}\) joints giving the estimation of \(3\mathrm{D}\) human body pose and shape. The training procedure minimizes an objective function of five terms: a joint-based data term, three pose priors, and a shape prior. Experimental results show that this method is effective in 3D human body reconstruction from arbitrary poses.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>SMPL的优势在于以解耦的方式编码形状、姿态和姿态特定细节，因此它在基于深度学习的人体形状重建中得到了广泛应用[110]，[116]，[117]，[118]。Bogo等[116]提出了SMPLify，这是从一张图像重建3D人类姿态和形状的第一个方法。他们首先使用基于CNN的架构DeepCut[119]来估计2D关节位置。然后，他们将SMPL模型拟合到预测的\(2\mathrm{D}\)关节上，从而给出\(3\mathrm{D}\)人类身体姿态和形状的估计。训练过程最小化一个包含五个项的目标函数：一个基于关节的数据项、三个姿态先验和一个形状先验。实验结果表明，该方法在从任意姿态重建3D人类身体方面是有效的。</p></div><p>Kanazawa et al. [120], on the other hand, argue that such a stepwise approach is not optimal and propose an end-to-end solution to learn a direct mapping from image pixels to model parameters. This approach addresses two important challenges: (1) the lack of large scale ground truth 3D annotations for in-the-wild images, and (2) the inherent ambiguities in single 2D-view-to-3D mapping of human body shapes. An example is depth ambiguity where multiple 3D body configurations can explain the same 2D projections [116]. To address the first challenge, Kanazawa et al. observe that there are large-scale 2D keypoint annotations of in-the-wild images and a separate large-scale dataset of \(3\mathrm{D}\) meshes of people with various poses and shapes. They then take advantage of these unpaired 2D keypoint annotations and 3D scans in a conditional generative adversarial manner. They propose a network that infers the SMPL [114] parameters of a 3D mesh and the camera parameters such that the 3D keypoints match the annotated 2D keypoints after projection. To deal with ambiguities, these parameters are sent to a discriminator whose task is to determine if the 3D parameters correspond to bodies of real humans or not. Hence, the network is encouraged to output parameters on the human manifold. The discriminator acts as a weak supervisor.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>另一方面，Kanazawa等[120]认为这种逐步方法并不是最优的，并提出了一种端到端的解决方案，以学习从图像像素到模型参数的直接映射。这种方法解决了两个重要挑战：（1）缺乏大规模的真实3D注释用于野外图像，以及（2）单个2D视图到3D人类身体形状映射的固有模糊性。一个例子是深度模糊性，其中多个3D身体配置可以解释相同的2D投影[116]。为了解决第一个挑战，Kanazawa等观察到，野外图像有大规模的2D关键点注释，以及一个单独的大规模数据集，包含各种姿态和形状的\(3\mathrm{D}\)网格。他们利用这些未配对的2D关键点注释和3D扫描，以条件生成对抗的方式进行处理。他们提出了一个网络，推断3D网格的SMPL[114]参数和相机参数，使得3D关键点在投影后与注释的2D关键点匹配。为了处理模糊性，这些参数被发送到一个判别器，判别器的任务是确定3D参数是否对应于真实人类的身体。因此，网络被鼓励输出位于人类流形上的参数。判别器充当弱监督者。</p></div><p>These approaches can handle complex poses from images with complex backgrounds, but are limited to a single person per image and does not handle clothes. Also, these approaches do not capture details such as hair and clothing with garment wrinkles, as well has details on the body parts. To capture these details, Alldieck et al. [118] train an encoder-decoder to predict normals and displacements, which can then be applied to the reconstructed SMPL model.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这些方法可以处理来自复杂背景图像的复杂姿态，但每张图像仅限于一个人，并且不处理衣物。此外，这些方法无法捕捉到诸如头发和衣物褶皱等细节，以及身体部位的细节。为了捕捉这些细节，Alldieck等[118]训练了一个编码器-解码器来预测法线和位移，这些可以应用于重建的SMPL模型。</p></div><h4>8.1.2 Volumetric methods</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>8.1.2 体积方法</h4></div><p>Volumetric techniques for \(3\mathrm{D}\) human body reconstruction do not use statistical models. Instead, they directly infer 3D occupancy grids. As such, all the methods reviewed in Section 4 can be used for 3D human body shape reconstruction. An example is the approach of Huang et al. [121], which takes multiple RGB views and their corresponding camera calibration parameters as input, and predicts a dense 3D field that encodes for each voxel its probability of being inside or outside the human body shape. The surface geometry can then be faithfully reconstructed from the \(3\mathrm{D}\) probability field using marching cubes. The approach uses a multi-branch encoder, one for each image, followed by a multi-layer perceptron which aggregates the features that correspond to the same 3D point into a probability value. The approach is able to recover detailed geometry even on human bodies with cloth but it is limited to simple backgrounds.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>体积技术用于\(3\mathrm{D}\)人类身体重建不使用统计模型。相反，它们直接推断3D占用网格。因此，第4节中回顾的所有方法都可以用于3D人类身体形状重建。一个例子是Huang等人[121]的方法，该方法以多个RGB视图及其相应的相机校准参数作为输入，并预测一个密集的3D场，该场为每个体素编码其在人体形状内部或外部的概率。然后，可以使用行进立方体从\(3\mathrm{D}\)概率场忠实地重建表面几何。该方法使用多分支编码器，每个图像一个，后面是一个多层感知器，将对应于同一3D点的特征聚合为一个概率值。该方法能够恢复即使在穿着衣物的人体上也能获得详细的几何形状，但它限于简单的背景。</p></div><p>To exploit domain-specific knowledge, Varol et al. [122] introduce BodyNet, a volumetric approach for inferring, from a single RGB image, the 3D human body shape, along with its \(2\mathrm{D}\) and \(3\mathrm{D}\) pose,and its partwise segmentation. The approach uses a cascade of four networks; (1) a 2D pose and a 2D segmentation network, which operate in parallel, (2) a 3D pose inference network, which estimates the 3D pose of the human body from the input RGB image and the estimated \(2\mathrm{D}\) pose and \(2\mathrm{D}\) partwise segmentation,and (4) finally, a 3D shape estimation network, which infers a volumetric representation of the human body shape and its partwise segmentation from the input RGB image and the estimates of the previous networks. By dividing the problem into four tasks, the network can benefit from intermediate supervision, which results in an improved performance.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了利用特定领域的知识，Varol等人[122]引入了BodyNet，这是一种体积方法，用于从单个RGB图像推断3D人类身体形状，以及其\(2\mathrm{D}\)和\(3\mathrm{D}\)姿态及其逐部分分割。该方法使用四个网络的级联；（1）一个2D姿态和一个2D分割网络，它们并行操作，（2）一个3D姿态推断网络，它从输入的RGB图像和估计的\(2\mathrm{D}\)姿态及\(2\mathrm{D}\)逐部分分割中估计人类身体的3D姿态，以及（4）最后，一个3D形状估计网络，它从输入的RGB图像和前面网络的估计中推断人类身体形状的体积表示及其逐部分分割。通过将问题分为四个任务，网络可以受益于中间监督，从而提高性能。</p></div><h3>8.2 3D face reconstruction</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>8.2 3D面部重建</h3></div><p>Detailed and dense image-based 3D reconstruction of the human face, which aims to recover shape, pose, expression, skin reflectance, and finer scale surface details, is a longstanding problem in computer vision and computer graphics. Recently, this problem has been formulated as a regression problem and solved using convolutional neural networks.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于图像的详细且密集的3D人脸重建，旨在恢复形状、姿态、表情、皮肤反射率和更细尺度的表面细节，是计算机视觉和计算机图形学中的一个长期问题。最近，这个问题被表述为回归问题，并使用卷积神经网络解决。</p></div><p>In this section, we review some of the representative papers. Most of the recent techniques use parametric representations, which parametrize the manifold of 3D faces. The most commonly used representation is the 3D morphable model (3DMM) of Blanz and Vetter [68], which is an extension of the 2D active appearance model [123] (see also Section 5.2.1). The model captures facial variabily in terms of geometry and texture. Gerig et al. [124] extended the model by including expressions as a separate space. Below, we discuss the various network architectures (Section 8.2.1) and their training procedures (Section 8.2.2). We will also discuss some of the model-free techniques (Section 8.2.3).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本节中，我们回顾一些代表性的论文。大多数最近的技术使用参数化表示，这些表示对3D面部的流形进行参数化。最常用的表示是Blanz和Vetter[68]的3D可变模型（3DMM），这是2D活动外观模型[123]（另见第5.2.1节）的扩展。该模型在几何和纹理方面捕捉面部的变化。Gerig等人[124]通过将表情作为一个单独的空间来扩展该模型。下面，我们讨论各种网络架构（第8.2.1节）及其训练过程（第8.2.2节）。我们还将讨论一些无模型技术（第8.2.3节）。</p></div><h4>8.2.1 Network architectures</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>8.2.1 网络架构</h4></div><p>The backbone architecture is an encoder, which maps the input image into the parametric model parameters. It is composed of convolutional layers followed by fully connected layers. In general, existing techniques use generic networks such as AlexNet, or networks specifically trained on facial images such as VGG-Face [125] or FaceNet [126]. Tran et al. [127] use this architecture to regress the 198 parameters of a 3DMM that encodes facial identity (geometry) and texture. It has been trained with 3D supervision using \({L}_{2}\) asymetric loss, i.e., a loss function that favours 3D reconstructions that are far from the mean.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>主干架构是一个编码器，它将输入图像映射到参数化模型参数。它由卷积层和全连接层组成。一般来说，现有技术使用通用网络，如AlexNet，或专门针对面部图像训练的网络，如VGG-Face[125]或FaceNet[126]。Tran等人[127]使用该架构回归编码面部身份（几何）和纹理的3DMM的198个参数。它已经使用\({L}_{2}\)不对称损失进行3D监督训练，即一种偏向于远离均值的3D重建的损失函数。</p></div><p>Richardson et al. [128] used a similar architecture but perform the reconstruction iteratively. At each iteration, the network takes the previously reconstructed face, but projected onto an image using a frontal camera, with the input image, and regresses the parameters of a 3DMM. The reconstruction is initialized with the average face. Results show that, with three iterations, the approach can successfully handle face reconstruction from images with various expressions and illumination conditions.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Richardson等人[128]使用了类似的架构，但以迭代方式进行重建。在每次迭代中，网络取之前重建的面部，但投影到使用前置相机的图像上，并回归3DMM的参数。重建以平均面部初始化。结果表明，通过三次迭代，该方法能够成功处理来自具有各种表情和光照条件的图像的面部重建。</p></div><p>One of the main issues with 3DMM-based approaches is that they tend to reconstruct smooth facial surfaces, which lack fine details such as wrinkles and dimples. As such, methods in this category use a refinement module to recover the fine details. For instance, Richardson et al. [128] refine the reconstructed face using Shape from Shading (SfS) techniques. Richardson et al. [129], on the other hand, add a second refinement block, FineNet, which takes as input the depth map of the coarse estimation and recovers using an encoder-decoder network a high resolution facial depth map. To enable end-to-end training, the two blocks are connected with a differentiable rendering layer. Unlike traditional SfS, the introduction of FineNet treats the calculation of albedo and lighting coefficients as part of the loss function without explicitly estimating these information. However, lighting is modeled by first-order spherical harmonics, which lead to an inaccurate reconstruction of the facial details.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于3DMM（可变形模型）的方法的主要问题之一是它们往往重建光滑的面部表面，缺乏皱纹和酒窝等细微细节。因此，这一类别的方法使用细化模块来恢复细节。例如，Richardson等人[128]使用形状从阴影（SfS）技术来细化重建的面部。另一方面，Richardson等人[129]添加了第二个细化模块FineNet，该模块以粗略估计的深度图为输入，通过编码器-解码器网络恢复高分辨率的面部深度图。为了实现端到端的训练，这两个模块通过可微渲染层连接。与传统的SfS不同，FineNet的引入将反照率和光照系数的计算视为损失函数的一部分，而不显式估计这些信息。然而，光照是通过一阶球谐函数建模的，这导致面部细节的重建不准确。</p></div><h4>8.2.2 Training and supervision</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>8.2.2 训练与监督</h4></div><p>One of the main challenges is in how to collect enough training images labelled with their corresponding \(3\mathrm{D}\) faces, to feed the network. Richardson et al. [128], [129] generate synthetic training data by drawing random samples from the morphable model and rendering the resulting faces. However, a network trained on purely synthetic data may perform poorly when faced with occlusions, unusual lighting, or ethnicities that are not well represented. Genova et al. [130] address the lack of training data by including randomly generated synthetic faces in each training batch to provide ground truth 3D coordinates, but train the network on real photographs at the same time. Tran et al. [127] use an iterative optimization to fit an expressionless model to a large number of photographs, and treat the results where the optimization converged as ground truth. To generalize to faces with expressions, identity labels and at least one neutral image are required. Thus, the potential size of the training dataset is restricted.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>主要挑战之一是如何收集足够的标记有相应\(3\mathrm{D}\)面孔的训练图像，以供网络使用。Richardson等人[128]，[129]通过从可变形模型中随机抽样并渲染生成的面孔来生成合成训练数据。然而，纯粹在合成数据上训练的网络在面对遮挡、异常光照或未得到良好表示的种族时可能表现不佳。Genova等人[130]通过在每个训练批次中包含随机生成的合成面孔来解决训练数据不足的问题，以提供真实的3D坐标，同时在真实照片上训练网络。Tran等人[127]使用迭代优化将无表情模型拟合到大量照片，并将优化收敛的结果视为真实值。为了推广到有表情的面孔，需要身份标签和至少一张中性图像。因此，训练数据集的潜在规模受到限制。</p></div><p>Tewari et al. [131] train, without 3D supervision, an encoder-decoder network to simultaneously predict the facial shape, expression, texture, pose, and lighting. The encoder is a regression network from images to morphable model coordinates, and the decoder is a fixed, differentiable rendering layer that attempts to reproduce the input photograph. The loss measures the discrepancy between the reproduced photograph and the input one. Since the training loss is based on individual image pixels, the network is vulnerable to confounding variation between related variables. For example, it cannot readily distinguish between dark skin tone and a dim lighting environment.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Tewari等人[131]在没有3D监督的情况下，训练一个编码器-解码器网络，同时预测面部形状、表情、纹理、姿势和光照。编码器是一个从图像到可变形模型坐标的回归网络，解码器是一个固定的、可微的渲染层，试图重现输入的照片。损失度量重现的照片与输入照片之间的差异。由于训练损失是基于单个图像像素，因此网络容易受到相关变量之间混淆变化的影响。例如，它无法轻易区分深色肤色和昏暗的光照环境。</p></div><p>To remove the need for supervised training with 3D data and the reliance on inverse rendering, Genova et al. [130] propose a framework that learns to minimize a loss based on the facial identity features produced by a face recognition network such as VGG-Face [125] or Google's FaceNet [126]. In other words, the face recognition network encodes the input photograph as well as the image rendered from the reconstructed face into feature vectors that are robust to pose, expression, lighting, and even non-photorealistic inputs. The method then applies a loss that measures the discrepancy between these two feature vectors instead of using pixel-wise distance between the rendered image and the input photograph. The 3D facial shape and texture regressor network is trained using only a face recognition network, a morphable face model, and a dataset of unlabelled facial images. The approach does not only improve on the accuracy of previous works but also produces \(3\mathrm{D}\) reconstructions that are often recognizable as the original subjects.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为了消除对3D数据的监督训练的需求以及对逆向渲染的依赖，Genova等人[130]提出了一个框架，该框架学习最小化基于面部识别网络（如VGG-Face [125]或Google的FaceNet [126]）生成的面部身份特征的损失。换句话说，面部识别网络将输入照片以及从重建面孔渲染的图像编码为对姿势、表情、光照甚至非真实感输入具有鲁棒性的特征向量。该方法然后应用一种损失，度量这两个特征向量之间的差异，而不是使用渲染图像与输入照片之间的像素距离。3D面部形状和纹理回归网络仅使用面部识别网络、可变形面部模型和未标记面部图像的数据集进行训练。该方法不仅提高了先前工作的准确性，还产生了\(3\mathrm{D}\)重建，通常可以识别为原始对象。</p></div><h4>8.2.3 Model-free approaches</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>8.2.3 无模型方法</h4></div><p>Morphable model-based techniques are restricted to the modelled subspace. As such, implausible reconstructions are possible outside the span of the training data. Other representations such as volumetric grids, which do not suffer from this problem, have been also explored in the context of 3D face reconstruction. Jackson et al. [132], for example, propose a Volumetric Regression Network (VRN), which takes as input 2D images and predicts their corresponding 3D binary volume instead of a 3DMM. Unlike [127], the approach can deal with a wide range of expressions, poses and occlusions without alignment and correspondences. It, however, fails to recover fine details due to the resolution restriction of volumetric techniques.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于可变形模型的技术受到建模子空间的限制。因此，在训练数据的范围之外可能会出现不合理的重建。其他表示方法，如体积网格，在3D面部重建的背景下也得到了探索，这些方法不受此问题的影响。例如，Jackson等人[132]提出了一种体积回归网络（VRN），该网络以2D图像为输入，预测其对应的3D二进制体积，而不是3DMM。与[127]不同，该方法可以处理广泛的表情、姿势和遮挡，而无需对齐和对应。然而，由于体积技术的分辨率限制，它未能恢复细微细节。</p></div><p>Other techniques use intermediate representations. For example, Sela et al. [133] use an Image-to-Image Translation Network based on U-Net [48] to estimate a depth image and a facial correspondence map. Then, an iterative deformation-based registration is performed followed by a geometric refinement procedure to reconstruct subtle facial details. Unlike 3DMM, this method can handle large geometric variations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其他技术使用中间表示。例如，Sela等人[133]使用基于U-Net [48]的图像到图像转换网络来估计深度图像和面部对应图。然后，进行迭代变形注册，随后进行几何细化程序以重建细微的面部细节。与3DMM不同，该方法可以处理较大的几何变化。</p></div><p>Feng et al. [134] also investigated a model-free method. First, a densely connected CNN framework is designed to regress \(3\mathrm{D}\) facial curves from horizontal and vertical epipolar plane images. Then, these curves are transformed into a 3D point cloud and the grid-fit algorithm [135] is used to fit a facial surface. Experimental results suggest that this approach is robust to varying poses, expressions and illumination.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>冯等人[134]还研究了一种无模型的方法。首先，设计了一个密集连接的卷积神经网络（CNN）框架，从水平和垂直的极平面图像中回归\(3\mathrm{D}\)面部曲线。然后，这些曲线被转换为3D点云，并使用网格拟合算法[135]来拟合面部表面。实验结果表明，该方法对不同的姿势、表情和光照具有鲁棒性。</p></div><h3>8.3 3D scene parsing</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>8.3 3D场景解析</h3></div><p>Methods discussed so far are primarily dedicated to the \(3\mathrm{D}\) reconstruction of objects in isolation. Scenes with multiple objects pose the additional challenges of delineating objects, properly handling occlusions, clutter, and uncertainty in shape and pose, and estimating the scene layout. Solutions to this problem involve 3D object detection and recognition, pose estimation, and 3D reconstruction. Traditionally, many of these tasks have been addressed using hand-crafted features. In the deep learning-based era, several of the blocks of the pipeline have been replaced with CNNs.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>到目前为止讨论的方法主要致力于\(3\mathrm{D}\)孤立物体的重建。多个物体的场景带来了额外的挑战，包括物体的划分、正确处理遮挡、杂乱和形状及姿势的不确定性，以及估计场景布局。解决这个问题的方法涉及3D物体检测与识别、姿势估计和3D重建。传统上，许多这些任务是通过手工设计的特征来解决的。在基于深度学习的时代，管道中的几个模块已被卷积神经网络（CNN）所替代。</p></div><p>For instance, Izadinia et al. [136] proposed an approach that is based on recognizing objects in indoor scenes, inferring room geometry,and optimizing \(3\mathrm{D}\) object poses and sizes in the room to best match synthetic renderings to the input photo. The approach detects object regions, finds from a CAD database the most similar shapes, and then deforms them to fit the input. The room geometry is estimated using a fully convolutional network. Both the detection and retrieval of objects are performed using Faster R-CNN [137]. The deformation and fitting, however, are performed via render and match. Tulsiani et al. [138], on the other hand, proposed an approach that is entirely based on deep learning. The input, which consists of an RGB image and the bounding boxes of the objects, is processed with a four-branch network. The first branch is an encoder-decoder with skip connections, which estimates the disparity of the scene layout. The second branch takes a low resolution image of the entire scene and maps it into a latent space using a CNN followed by three fully-connected layers. The third branch, which has the same architecture as the second one, maps the image at its original resolution to convolutional feature maps, followed by ROI pooling to obtain features for the ROI. The last layer maps the bounding box location through fully connected layers. The three features are then concatenated and further processed with fully-connected layers followed by a decoder, which produces a \({32}^{3}\) voxel grid of the object in the ROI and its pose in the form of position, orientation, and scale. The method has been trained using synthetically-rendered images with their associated ground-truth 3D scene.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>例如，Izadinia等人[136]提出了一种基于识别室内场景中的物体、推断房间几何形状，并优化\(3\mathrm{D}\)物体在房间中的姿势和大小以最佳匹配合成渲染与输入照片的方法。该方法检测物体区域，从CAD数据库中找到最相似的形状，然后对其进行变形以适应输入。房间几何形状是使用全卷积网络估计的。物体的检测和检索是通过Faster R-CNN[137]进行的。然而，变形和拟合是通过渲染和匹配来完成的。另一方面，Tulsiani等人[138]提出了一种完全基于深度学习的方法。输入由RGB图像和物体的边界框组成，使用四分支网络进行处理。第一分支是一个带有跳跃连接的编码器-解码器，用于估计场景布局的视差。第二分支使用整个场景的低分辨率图像，并通过CNN映射到潜在空间，后接三个全连接层。第三分支与第二个分支具有相同的架构，将图像以其原始分辨率映射到卷积特征图，随后进行ROI池化以获取ROI的特征。最后一层通过全连接层映射边界框位置。然后将这三个特征连接在一起，并通过全连接层进一步处理，后接解码器，生成ROI中物体的\({32}^{3}\)体素网格及其姿势，包括位置、方向和尺度。该方法使用合成渲染图像及其相关的真实3D场景进行训练。</p></div><h2>9 DATASETS</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>9 数据集</h2></div><p>Table 5 lists and summarizes the properties of the most commonly used datasets. Unlike traditional techniques, the success of deep learning-based 3D reconstruction algorithms depends on the availability of large training datasets. Supervised techniques require images and their corresponding 3D annotations in the form of (1) full 3D models represented as volumetric grids, triangular meshes, or point clouds, or (2) depth maps, which can be dense or sparse. Weakly supervised and unsupervised techniques, on the other hand, rely on additional supervisory signals such as the extrinsic and intrinsic camera parameters and segmentation masks.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表5列出了最常用数据集的属性并进行了总结。与传统技术不同，基于深度学习的3D重建算法的成功依赖于大规模训练数据集的可用性。监督技术需要图像及其对应的3D注释，形式为（1）作为体积网格、三角网或点云表示的完整3D模型，或（2）深度图，可以是稠密或稀疏的。另一方面，弱监督和无监督技术依赖于额外的监督信号，如外部和内部相机参数以及分割掩码。</p></div><p>The main challenge in collecting training datasets for deep learning-based 3D reconstruction is two-fold. First, while one can easily collect \(2\mathrm{D}\) images,obtaining their corresponding 3D ground truth is challenging. As such, in many datasets, IKEA, PASCAL 3D+, and ObjectNet3D, only a relatively small subset of the images are annotated with their corresponding 3D models. Second, datasets such as ShapeNet and ModelNet, which are the largest 3D datasets currently available, contain 3D CAD models without their corresponding natural images since they have been originally intended to benchmark 3D shape retrieval algorithms.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>收集用于基于深度学习的3D重建的训练数据集的主要挑战有两个方面。首先，虽然可以轻松收集\(2\mathrm{D}\)图像，但获取其对应的3D真实值却很具挑战性。因此，在许多数据集中，如IKEA、PASCAL 3D+和ObjectNet3D，只有相对较小的一部分图像被注释为其对应的3D模型。其次，像ShapeNet和ModelNet这样的数据集是目前可用的最大3D数据集，包含3D CAD模型，但没有其对应的自然图像，因为它们最初是为了基准测试3D形状检索算法而设计的。</p></div><!-- Media --><p>TABLE 5: Some of the datasets that are used to train and evaluate the performance of deep learning-based 3D reconstruction algorithms. "img": image. "obj": object. "Bkg": background. "cats": categories. "GT": ground truth.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表5：一些用于训练和评估基于深度学习的3D重建算法性能的数据集。“img”：图像。“obj”：物体。“Bkg”：背景。“cats”：类别。“GT”：真实值。</p></div><table><tbody><tr><td rowspan="2"></td><td rowspan="2">\( \mathbf{{Year}} \)</td><td colspan="5">Images</td><td colspan="2">Objects</td><td colspan="3">3D ground truth</td><td rowspan="2">Camera params</td></tr><tr><td>No. imgs</td><td>Size</td><td>objs per img</td><td>Type</td><td>Bkg</td><td>Type</td><td>No. cats</td><td>No.</td><td>Type</td><td>Img with 3D GT</td></tr><tr><td>ShapeNet [139]</td><td>2015</td><td>-</td><td>-</td><td>Single</td><td>rendered</td><td>Uniform</td><td>Generic</td><td>55</td><td>51,300</td><td>3D model</td><td>51,300</td><td>Intrinsic</td></tr><tr><td>ModelNet 31</td><td>2015</td><td>-</td><td>-</td><td>Single</td><td>Rendered</td><td>Uniform</td><td>Generic</td><td>662</td><td>127.915</td><td>3D model</td><td>127,915</td><td>Intrinsic</td></tr><tr><td>IKEA [140]</td><td>2013</td><td>759</td><td>Variable</td><td>Single</td><td>Real, indoor</td><td>Cluttered</td><td>Generic</td><td>7</td><td>219</td><td>3D model</td><td>759</td><td>Intrinsic+extrinsic</td></tr><tr><td>\( {}_{\mathrm{{Pix3D}}} \) [9]</td><td>2018</td><td>9,531</td><td>\( {110} \times  {110} \) to \( {3264} \times  {2448} \)</td><td>Single</td><td>Real, indoor</td><td>Cluttered</td><td>Generic</td><td>9</td><td>1015</td><td>3D model</td><td>9,531</td><td>Focal length, extrinsic</td></tr><tr><td>PASCAL 3D+ 141</td><td>2014</td><td>30,899</td><td>Variable</td><td>Multiple</td><td>Real, indoor, outdoor</td><td>Cluttered</td><td>Generic</td><td>12</td><td>36,000</td><td>3D model</td><td>30,809</td><td>Intrinsic+extrinsic</td></tr><tr><td>\( \mathrm{{ObjectNet3D}}\left\lbrack  {142}\right\rbrack \)</td><td>2016</td><td>90,127</td><td>Variable</td><td>Multiple</td><td>Real, indoor, outdoor</td><td>Cluttered</td><td>Generic</td><td>100</td><td>44,147</td><td>3D model</td><td>90,127</td><td>Intrinsic+extrinsic</td></tr><tr><td>KITTI12 [143]</td><td>2012</td><td>41,778</td><td>\( {1240} \times  {376} \)</td><td>Multiple</td><td>Real, outdoor</td><td>Cluttered</td><td>Generic</td><td>2</td><td>40,000</td><td>Point cloud</td><td>12,000</td><td>Intrinsic+extrinsic</td></tr><tr><td>ScanNet [144]</td><td>2017, 2018</td><td>2,492,518</td><td>\( {640} \times  {480}, \) RGB 1296 \( \times  {968} \)</td><td>Multiple</td><td>Real, indoor</td><td>Cluttered</td><td>Generic</td><td>296</td><td>36, 123</td><td>Dense depth</td><td>2,492,518</td><td>Intrinsic+extrinsic</td></tr><tr><td>Stanford Car [145]</td><td>2013</td><td>16, 185</td><td>Variable</td><td>Single</td><td>Real. outdoor</td><td>Cluttered</td><td>Cars</td><td>196</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Caltech-UCSD Birds 200 [146], [147]</td><td>2010, 2011</td><td>6, 033</td><td>Variable</td><td>Single</td><td>Real</td><td>Cluttered</td><td>Birds</td><td>200</td><td>-</td><td>-</td><td>-</td><td>Extrinsic</td></tr><tr><td>SUNCG 148</td><td>2017</td><td>130,269</td><td>-</td><td>Multiple</td><td>Synthetic</td><td>Cluttered</td><td>Generic</td><td>84</td><td>5,697,217</td><td>Depth, voxel grid</td><td>130, 269</td><td>Intrinsic</td></tr><tr><td>Stanford 2D-3D-S \( \left\lbrack  {149}\right\rbrack  ,\left\lbrack  {150}\right\rbrack \)</td><td>2016, 2017</td><td>70,496</td><td>\( {1080} \times  {1080} \)</td><td>Multiple</td><td>Real, indoor</td><td>Cluttered</td><td>Generic</td><td>13</td><td>6,005</td><td>Point cloud, mesh</td><td>70,496</td><td>Intrinsic + extrinsic</td></tr><tr><td>ETHZ CVL RueMonge [151]</td><td>2014</td><td>428</td><td>-</td><td>Multiple</td><td>Real, outdoor</td><td>Cluttered</td><td>Generic</td><td>8</td><td>-</td><td>Point cloud, mesh</td><td>428</td><td>Intrinsic</td></tr><tr><td>NYU V2 [152]</td><td>2012</td><td>1,449</td><td>\( {640} \times  {480} \)</td><td>Multiple</td><td>Real, indoor</td><td>Cluttered</td><td>Generic</td><td>894</td><td>35,064</td><td>Dense depth, 3D points</td><td>1,449</td><td>Intrinsic</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2"></td><td rowspan="2">\( \mathbf{{Year}} \)</td><td colspan="5">图像</td><td colspan="2">物体</td><td colspan="3">3D真实数据</td><td rowspan="2">相机参数</td></tr><tr><td>图像数量</td><td>大小</td><td>每张图像的物体数量</td><td>类型</td><td>背景</td><td>类型</td><td>类别数量</td><td>编号</td><td>类型</td><td>带3D真实数据的图像</td></tr><tr><td>ShapeNet [139]</td><td>2015</td><td>-</td><td>-</td><td>单一</td><td>渲染的</td><td>均匀的</td><td>通用的</td><td>55</td><td>51,300</td><td>3D模型</td><td>51,300</td><td>内在的</td></tr><tr><td>ModelNet 31</td><td>2015</td><td>-</td><td>-</td><td>单一</td><td>渲染的</td><td>均匀的</td><td>通用的</td><td>662</td><td>127.915</td><td>3D模型</td><td>127,915</td><td>内在的</td></tr><tr><td>IKEA [140]</td><td>2013</td><td>759</td><td>可变的</td><td>单一</td><td>真实的，室内</td><td>杂乱的</td><td>通用的</td><td>7</td><td>219</td><td>3D模型</td><td>759</td><td>内在+外在</td></tr><tr><td>\( {}_{\mathrm{{Pix3D}}} \) [9]</td><td>2018</td><td>9,531</td><td>\( {110} \times  {110} \) 到 \( {3264} \times  {2448} \)</td><td>单一</td><td>真实的，室内</td><td>杂乱的</td><td>通用的</td><td>9</td><td>1015</td><td>3D模型</td><td>9,531</td><td>焦距，外在</td></tr><tr><td>PASCAL 3D+ 141</td><td>2014</td><td>30,899</td><td>可变的</td><td>多个</td><td>真实的，室内，室外</td><td>杂乱的</td><td>通用的</td><td>12</td><td>36,000</td><td>3D模型</td><td>30,809</td><td>内在+外在</td></tr><tr><td>\( \mathrm{{ObjectNet3D}}\left\lbrack  {142}\right\rbrack \)</td><td>2016</td><td>90,127</td><td>可变的</td><td>多个</td><td>真实的，室内，室外</td><td>杂乱的</td><td>通用的</td><td>100</td><td>44,147</td><td>3D模型</td><td>90,127</td><td>内在+外在</td></tr><tr><td>KITTI12 [143]</td><td>2012</td><td>41,778</td><td>\( {1240} \times  {376} \)</td><td>多个</td><td>真实的，室外</td><td>杂乱的</td><td>通用的</td><td>2</td><td>40,000</td><td>点云</td><td>12,000</td><td>内在+外在</td></tr><tr><td>ScanNet [144]</td><td>2017, 2018</td><td>2,492,518</td><td>\( {640} \times  {480}, \) RGB 1296 \( \times  {968} \)</td><td>多个</td><td>真实的，室内</td><td>杂乱的</td><td>通用的</td><td>296</td><td>36, 123</td><td>稠密深度</td><td>2,492,518</td><td>内在+外在</td></tr><tr><td>斯坦福汽车 [145]</td><td>2013</td><td>16, 185</td><td>可变的</td><td>单一</td><td>真实的，室外</td><td>杂乱的</td><td>汽车</td><td>196</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>加州理工学院-加州大学圣地亚哥分校鸟类200 [146], [147]</td><td>2010, 2011</td><td>6, 033</td><td>可变的</td><td>单一</td><td>真实的</td><td>杂乱的</td><td>鸟类</td><td>200</td><td>-</td><td>-</td><td>-</td><td>外在的</td></tr><tr><td>SUNCG 148</td><td>2017</td><td>130,269</td><td>-</td><td>多个</td><td>合成的</td><td>杂乱的</td><td>通用的</td><td>84</td><td>5,697,217</td><td>深度，体素网格</td><td>130, 269</td><td>内在的</td></tr><tr><td>斯坦福2D-3D-S \( \left\lbrack  {149}\right\rbrack  ,\left\lbrack  {150}\right\rbrack \)</td><td>2016, 2017</td><td>70,496</td><td>\( {1080} \times  {1080} \)</td><td>多个</td><td>真实的，室内</td><td>杂乱的</td><td>通用的</td><td>13</td><td>6,005</td><td>点云，网格</td><td>70,496</td><td>内在 + 外在</td></tr><tr><td>ETHZ CVL RueMonge [151]</td><td>2014</td><td>428</td><td>-</td><td>多个</td><td>真实的，室外</td><td>杂乱的</td><td>通用的</td><td>8</td><td>-</td><td>点云，网格</td><td>428</td><td>内在的</td></tr><tr><td>NYU V2 [152]</td><td>2012</td><td>1,449</td><td>\( {640} \times  {480} \)</td><td>多个</td><td>真实的，室内</td><td>杂乱的</td><td>通用的</td><td>894</td><td>35,064</td><td>稠密深度，3D点</td><td>1,449</td><td>内在的</td></tr></tbody></table></div><!-- Media --><p>This issue has been addressed in the literature by data augmentation, which is the process of augmenting the original sets with synthetically-generated data. For instance, one can generate new images and new 3D models by applying geometric transformations, e.g., translation, rotation, and scaling, to the existing ones. Note that, although some transformations are similarity-preserving, they still enrich the datasets. One can also synthetically render, from existing 3D models, new 2D and 2.5D (i.e., depth) views from various (random) viewpoints, poses, lighting conditions, and backgrounds. They can also be overlaid with natural images or random textures. This, however, results in the domain shift problem, i.e., the space of synthetic images is different from the space of real images, which often results in a decline in performance when methods are tested on images of a completely different type.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这个问题在文献中通过数据增强得到了处理，数据增强是将原始数据集与合成生成的数据进行扩充的过程。例如，可以通过对现有图像应用几何变换（如平移、旋转和缩放）来生成新的图像和新的3D模型。请注意，尽管某些变换保持相似性，但它们仍然丰富了数据集。还可以从现有的3D模型合成渲染出新的2D和2.5D（即深度）视图，来自不同（随机）视角、姿势、光照条件和背景。它们还可以与自然图像或随机纹理叠加。然而，这导致了领域转移问题，即合成图像的空间与真实图像的空间不同，这通常会导致在完全不同类型的图像上测试方法时性能下降。</p></div><p>Domain shift problem in machine learning has been traditionally addressed using domain adaptation or translation techniques, which are becoming popular in depth estimation [153]. They are, however, not commonly used for 3D reconstruction. An exception is the work of Petersen et al. [95], which observed that a differentiable renderer used in unsupervised techniques may produce images that are different in appearance compared to the input images. This has been alleviated through the use of image domain translation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>机器学习中的领域转移问题传统上通过领域适应或转换技术来解决，这些技术在深度估计中变得越来越流行[153]。然而，它们在3D重建中并不常用。一个例外是Petersen等人的工作[95]，他们观察到在无监督技术中使用的可微渲染器可能会生成与输入图像在外观上不同的图像。这通过使用图像领域转换得到了缓解。</p></div><p>Finally, weakly supervised and unsupervised techniques (Section 7.1.2) minimize the reliance on \(3\mathrm{D}\) annotations. They, however, require (1) segmentation masks, which can be obtained using the recent state-of-the-art object detection and segmentation algorithms [154], and/or (2) camera parameters. Jointly training for \(3\mathrm{D}\) reconstruction,segmentation, and camera parameters estimation can be a promising direction for feature research.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最后，弱监督和无监督技术（第7.1.2节）最小化对\(3\mathrm{D}\)注释的依赖。然而，它们需要（1）分割掩码，这可以通过最近的最先进的目标检测和分割算法[154]获得，和/或（2）相机参数。联合训练\(3\mathrm{D}\)重建、分割和相机参数估计可能是特征研究的一个有前景的方向。</p></div><h2>10 PERFORMANCE COMPARISON</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>10 性能比较</h2></div><p>This section discusses the performance of some key methods. We will present the various performance criteria and metrics (Section 10.1), and discuss and compare the performance of some key methods (Section 10.2).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>本节讨论一些关键方法的性能。我们将介绍各种性能标准和指标（第10.1节），并讨论和比较一些关键方法的性能（第10.2节）。</p></div><h3>10.1 Accuracy metrics and performance criteria</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>10.1 精度指标和性能标准</h3></div><p>Let \(X\) be the ground truth 3D shape and \(\widehat{X}\) the reconstructed one. Below, we discuss some of the accuracy metrics (Section 10.1.1 and performance criteria (Section 10.1.2) used to compare \(\overline{3}\mathrm{D}\) reconstruction algorithms.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>设\(X\)为真实的3D形状，\(\widehat{X}\)为重建的形状。下面，我们讨论一些用于比较\(\overline{3}\mathrm{D}\)重建算法的精度指标（第10.1.1节）和性能标准（第10.1.2节）。</p></div><h4>10.1.1 Accuracy metrics</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>10.1.1 精度指标</h4></div><p>The most commonly used quantitative metrics for evaluating the accuracy of \(3\mathrm{D}\) reconstruction algorithms include:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>用于评估\(3\mathrm{D}\)重建算法精度的最常用定量指标包括：</p></div><p>(1) The Mean Squared Error (MSE) [60]. It is defined as the symmetric surface distance between the reconstructed shape \(\widehat{X}\) and the ground-truth shape \(X\) ,i.e.,</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(1) 均方误差（MSE）[60]。它被定义为重建形状\(\widehat{X}\)与真实形状\(X\)之间的对称表面距离，即：</p></div><p></p>\[d\left( {\widehat{X},X}\right)  = \frac{1}{{n}_{X}}\mathop{\sum }\limits_{{p \in  X}}d\left( {p,\widehat{X}}\right)  + \frac{1}{{n}_{\widehat{X}}}\mathop{\sum }\limits_{{\widehat{p} \in  \widehat{X}}}d\left( {\widehat{p},X}\right) . \tag{15}\]<p></p><p>Here, \({n}_{X}\) and \({n}_{\widehat{X}}\) are,respectively,the number of densely sampled points on \(X\) and \(\widehat{X}\) ,and \(d\left( {p,X}\right)\) is the distance, e.g.,the \({L}_{1}\) or \({L}_{2}\) distance,of \(p\) to \(X\) along the normal direction to \(X\) . The smaller this measure is,the better is the reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>这里，\({n}_{X}\)和\({n}_{\widehat{X}}\)分别是\(X\)和\(\widehat{X}\)上密集采样点的数量，\(d\left( {p,X}\right)\)是距离，例如，\({L}_{1}\)或\({L}_{2}\)距离，\(p\)到\(X\)沿着法线方向的距离。这个度量越小，重建效果越好。</p></div><p>(2) Intersection over Union (IoU). The IoU measures the ratio of the intersection between the volume of the predicted shape and the volume of the ground-truth, to the union of the two volumes, i.e.,</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(2) 交并比（IoU）。IoU测量预测形状的体积与真实体积之间的交集与两个体积的并集的比率，即：</p></div><p></p>\[{Io}{U}_{\epsilon } = \frac{\widehat{V} \cap  V}{\widehat{V} \cup  V} = \frac{\mathop{\sum }\limits_{i}\left\{  {I\left( {{\widehat{V}}_{i} > \epsilon }\right)  * I\left( {V}_{i}\right) }\right\}  }{\mathop{\sum }\limits_{i}\left\{  {I\left( {I\left( {{\widehat{V}}_{i} > \epsilon }\right)  + I\left( {V}_{i}\right) }\right) }\right\}  }, \tag{16}\]<p></p><p>where \(I\left( \cdot \right)\) is the indicator function, \({\widehat{V}}_{i}\) is the predicted value at the \(i\) -th voxel, \({V}_{i}\) is the ground truth,and \(\epsilon\) is a threshold. The higher the IoU value, the better is the reconstruction. This metric is suitable for volumetric reconstructions. Thus, when dealing with surface-based representations, the reconstructed and ground-truth 3D models need to be voxelized.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(I\left( \cdot \right)\)是指示函数，\({\widehat{V}}_{i}\)是第\(i\)个体素的预测值，\({V}_{i}\)是真实值，\(\epsilon\)是一个阈值。IoU值越高，重建效果越好。该指标适用于体积重建。因此，在处理基于表面的表示时，重建的和真实的3D模型需要进行体素化。</p></div><p>(3) Mean of Cross Entropy (CE) loss [103]. It is defined as follows;</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(3) 交叉熵（CE）损失的均值[103]。其定义如下：</p></div><p></p>\[{CE} =  - \frac{1}{N}\mathop{\sum }\limits_{{i = 1}}^{N}\left\{  {{p}_{i}\log {\widehat{p}}_{i} + \left( {1 - {p}_{i}}\right) \log \left( {1 - {\widehat{p}}_{i}}\right\}  .}\right.  \tag{17}\]<p></p><p>where \(N\) is the total number of voxels or points,depending whether using a volumetric or a point-based representation. \(p\) and \(\widehat{p}\) are,respectively,the ground-truth and the predicted value at the \(i\) -voxel or point. The lower the CE value is,the better is the reconstruction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(N\)是体素或点的总数，具体取决于使用体积表示还是基于点的表示。\(p\)和\(\widehat{p}\)分别是真实值和第\(i\)个体素或点的预测值。CE值越低，重建效果越好。</p></div><p>(4) Earth Mover Distance (EMD) and Chamfer Distance (CD). These distances are, respectively, defined in Equations (5) and (6).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(4) 地球移动者距离（EMD）和Chamfer距离（CD）。这些距离分别在公式（5）和（6）中定义。</p></div><h4>10.1.2 Performance criteria</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>10.1.2 性能标准</h4></div><p>In addition to these quantitative metrics, there are several qualitative aspects that are used to evaluate the efficiency of these methods. This includes:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>除了这些定量指标，还有几个定性方面用于评估这些方法的效率。这包括：</p></div><p>(1) Degree of 3D supervision. An important aspect of deep learning-based 3D reconstruction methods is the degree of 3D supervision they require at training. In fact, while obtaining RGB images is easy, obtaining their corresponding ground-truth 3D data is quite challenging. As such, techniques that require minimal or no 3D supervision are usually preferred over those that require ground-truth 3D information during training.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(1) 3D监督的程度。基于深度学习的3D重建方法的一个重要方面是它们在训练时所需的3D监督程度。实际上，获取RGB图像很容易，而获取其对应的真实3D数据则相当具有挑战性。因此，通常更倾向于那些在训练过程中需要最少或不需要3D监督的技术，而不是那些需要真实3D信息的技术。</p></div><p>(2) Computation time. While training can be slow, in general, its is desirable to achieve real-time performance at runtime.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(2) 计算时间。虽然训练可能很慢，但一般来说，期望在运行时实现实时性能。</p></div><p>(3) Memory footprint. Deep neural networks have a large number of parameters. Some of them operate on volumes using 3D convolutions. As such, they usually require a large memory storage, which can affect their performance at runtime and limit their usage.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(3) 内存占用。深度神经网络有大量参数。其中一些使用3D卷积在体积上操作。因此，它们通常需要大量内存存储，这可能会影响它们在运行时的性能并限制其使用。</p></div><h3>10.2 Comparison and discussion</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>10.2 比较与讨论</h3></div><p>We present the improvement in reconstruction accuracy over the past 4 years in Fig. 6, and the performance of some representative methods in Table 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们在图6中展示了过去4年重建精度的提升，以及在表6中一些代表性方法的性能。</p></div><p>The majority of early works resort to voxelized representations [4], [7], [17], [25], [156], which can represent both the surface and the internal details of complex objects of arbitrary topology. With the introduction of space partitioning techniques such as O-CNN [32], OGN [33], and OctNet [41], volumetric techniques can attain relatively high resolutions,e.g., \({512}^{3}\) . This is due to the significant gain in memory efficiency. For instance, the OGN of [33] reduces the memory requirement for the reconstruction of volumetric grids of size \({32}^{3}\) from 4.5GB in [7] and 1.7GB in [12] to just 0.29GB (see Table 6). However, only a few papers, e.g., [35], adopted these techniques due to the complexity of their implementation. To achieve high resolution 3D volumetric reconstruction, many recent papers use intermediation, through multiple depth maps, followed by volumetric [38], [46], [51], [92] or point-based [80] fusion. More recently, several papers start to focus on mechanisms for learning continuous Signed Distance Functions [39], [43] or continuous occupancy grids [157], which are less demanding in terms of memory requirement. Their advantage is that since they learn a continuous field, the reconstructed 3D object can be extracted at the desired resolution.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>大多数早期工作采用体素化表示 [4], [7], [17], [25], [156]，可以表示任意拓扑复杂对象的表面和内部细节。随着空间划分技术的引入，如O-CNN [32]、OGN [33]和OctNet [41]，体积技术可以达到相对较高的分辨率，例如\({512}^{3}\)。这是由于内存效率的显著提升。例如，[33]的OGN将体积网格大小为\({32}^{3}\)的重建内存需求从[7]中的4.5GB和[12]中的1.7GB减少到仅0.29GB（见表6）。然而，由于其实现的复杂性，只有少数论文，例如[35]，采用了这些技术。为了实现高分辨率的3D体积重建，许多近期论文通过多个深度图进行中介，然后进行体积 [38], [46], [51], [92] 或基于点 [80] 的融合。最近，一些论文开始关注学习连续签名距离函数 [39], [43] 或连续占用网格 [157] 的机制，这些机制在内存需求方面要求较低。它们的优势在于，由于学习了连续场，重建的3D对象可以在所需分辨率下提取。</p></div><p>Fig. 6 shows the evolution of the performance over the years, since 2016, using the ShapeNet dataset [3] as a benchmark. On the IoU metric, computed on volumetric grids of size \({32}^{3}\) ,we can see that methods that use multiple views at training and/or at testing outperform those that are based solely on single views. Also, surface-based techniques, which started to emerge in 2017 (both mesh-based [60] and point-based [59], [72]), slightly outperform volumetric methods. Mesh-based techniques, however, are limited to genus-0 surfaces or surfaces with the same topology as the template.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6显示了自2016年以来，使用ShapeNet数据集 [3] 作为基准的性能演变。在IoU指标上，计算在大小为\({32}^{3}\)的体积网格上，我们可以看到，在训练和/或测试中使用多个视图的方法优于仅基于单一视图的方法。此外，自2017年开始出现的基于表面的技术（包括基于网格 [60] 和基于点 [59], [72]）略微优于体积方法。然而，基于网格的技术仅限于属数为0的表面或与模板具有相同拓扑的表面。</p></div><!-- Media --><!-- figureText: 0. Intersection over Union (the higher, the better) Xie et al. (2019, multiple, 2 Xie et al Tulsiani et al. (2019, multiple, single) (2018, multiple, single) Tatarchenko et al. Richter et al (2017, multiple, single) (2018, single, single) ATulsiani et al. (2017, multiple, single) — Gwak et al. (2017, 5, single) J Gwak et al. (2017, single, single) 3D supervision 2D supervision Method / Year (a) IoU of volumetric methods. Intersection over Union (the higher, the better) 3D supervision 2D supervision Jiang et al. (2018, single, single (2018, single, single) Zeng et al. Kato et al. (2019, 20, single) (2018, single, single) Kato et al (2019, single, single) Method / Year (b) IoU of surface-based methods. Chamfer Distance (the lower, the better) Mandikal et al. (2019, single, single) Groueix et al. (2018), single, single Wang et al. (2018, single, single) Kato et al. (2019, single, single Method / Year 0.7 Choy et al. (2016, multiple, 20) Johnston et al. 0.6 (2017, single, single) (2016, multiple, single) IoU @ 32 Choy et al. Gwak et al. – (2016, multiple, single) (2017, 5, single) 0.3 Gwak et al. (2017, single, single) 0.2 0.9 0.8 Soltani et al. (2017, 20, 20) 0.75 IoU @ 32 Soltani et al Fan et al. (2017, single, single) 0.6 0.6 Pontes et al (2017, single, single) 0.55 0.9 3D supervision 2D supervision Mandikal et al. Fan et al. (2018, single, single) Zeng et al (2017, single, single) (c) CD of surface-based methods. --><img src="https://cdn.noedgeai.com/bo_d163t43ef24c73d1le4g_20.jpg?x=909&#x26;y=114&#x26;w=753&#x26;h=1447&#x26;r=0"><p>Fig. 6: Performance of some key methods on the ShapeNet dataset. References highlighted in red are point-based. The IoU is computed on grids of size \({32}^{3}\) . The label next to each circle is encoded as follow: First author et al. (year, \(n\) at training, \(n\) at test),where \(n\) is the number of input images. Table 6 provides a detailed comparison.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6：在ShapeNet数据集上某些关键方法的性能。红色突出显示的参考文献为基于点的方法。IoU是在大小为\({32}^{3}\)的网格上计算的。每个圆旁边的标签编码如下：第一作者等（年份，\(n\)在训练时，\(n\)在测试时），其中\(n\)是输入图像的数量。表6提供了详细的比较。</p></div><!-- Media --><p>Fig. 6 shows that, since their introduction in 2017 by Yan et al. [4], 2D supervision-based methods significantly improved in performance. The IoU curves of Figures 6- (a) and (b), however, show that methods that use 3D su-</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6显示，自2017年Yan等人引入2D监督方法以来，其性能显著提高。然而，图6-(a)和(b)的IoU曲线显示，使用3D监督的方法表现出色。</p></div><!-- Meanless: ロmWi-maxアミスの中のWoCoDang-manOSZのゴcCoTSでション--><!-- Media --><p>TABLE 6: Performance summary of some representative methods. "Obj.": objects. "Time" refers to timing in milliseconds. "U3D": unlabelled 3D. # params: number of the parameters of the network. "mem.": memory requirements. "Resol.": resolution. "Bkg": background.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表6：一些代表性方法的性能总结。“Obj.”：对象。“Time”指以毫秒为单位的时间。“U3D”：未标记的3D。“# params”：网络参数的数量。“mem.”：内存需求。“Resol.”：分辨率。“Bkg”：背景。</p></div><table><tbody><tr><td rowspan="2">Method</td><td colspan="4">Input</td><td colspan="2">Output</td><td colspan="2">Training</td><td colspan="4">Performance@(ShapeNet, Pix3D,Pascal3D+)</td></tr><tr><td>Train</td><td>Test</td><td>Bkg</td><td># objects</td><td>Type</td><td>Resol.</td><td>Supervision</td><td>Network</td><td>IoU</td><td>CD</td><td>Time</td><td>Memory (# params., mem.)</td></tr><tr><td>Xie et al. 86]</td><td>\( n \geq  1 \) RGB,3D GT</td><td>1 RGB</td><td>Clutter</td><td>1</td><td>Volumetric</td><td rowspan="2">\( {32}^{2} \)</td><td>3D</td><td>Encoder (VGG16),</td><td>\( \left( {{0.658},-,{0.669}}\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>9.9</td><td>(114.4M, -)</td></tr><tr><td></td><td></td><td>20 RGB</td><td></td><td></td><td></td><td>3D</td><td>Decoder, Refiner</td><td>\( \left( {{0.705},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Richter et al. [51]</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>Clean</td><td>1</td><td>Volumetric</td><td>\( {512}^{3} \)</td><td>3D</td><td>encoder, 2D decoder</td><td>\( \left( {{0.641},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Tulsiani et al. [11] Taterchenko et al. [33]</td><td>\( n > 1 \) RGB,segmentation \( n > 1\mathrm{{RGB}} + 3\mathrm{{DGT}} \)</td><td>1 RGB 1 RGB</td><td>Clutter Clutter</td><td>1 1</td><td>Volumetric + pose Volumetric</td><td>- \( {512}^{3} \)</td><td>2D (multiview) 3D</td><td>Encoder, decoder, pose CNN Octree Generating</td><td>\( \left( {{0.62},-, - }\right) @{32}^{3} \) (-,-, -)</td><td>(-,-, -) (-,-, -)</td><td>- 2.06s</td><td>- (- , 0.88GB)</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>\( {32}^{3} \)</td><td></td><td>Network</td><td>\( \left( {{0.596}, - ,{0.504}}\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>16</td><td>(12.46M,0.29GB)</td></tr><tr><td>Tulsiani et al. [8]</td><td>\( n > 1 \) RGB,silh.,(depth)</td><td>1 RGB</td><td>Clutter</td><td>1</td><td>Volumetric</td><td>\( {32}^{3} \)</td><td>2D (multiview)</td><td>encoder-decoder</td><td>\( \left( {{0.546},-,{0.536}}\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Wu et al. [6]</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>Clutter</td><td>1</td><td>Volumetric</td><td>\( {128}^{3} \)</td><td>2D and 2.5D</td><td>TL, 3D-VAE + encoder-decoder</td><td>\( \left( {{0.57},-,{0.39}}\right) @{128}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td rowspan="4">Gwak et al. [97]</td><td>1 RGB, silh., pose</td><td>1 RGB</td><td></td><td></td><td></td><td>-</td><td>2D</td><td>GAN</td><td>\( \left( {{0.257},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>1 RGB, silh., pose, U3D</td><td>1 RGB</td><td>Clutter</td><td>1</td><td>Volumetric</td><td></td><td>2D, U3D</td><td>GAN</td><td>\( \left( {{0.403},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td></td><td></td></tr><tr><td>5 RGB, silh., pose</td><td>1 RGB</td><td></td><td></td><td></td><td>-</td><td>2D</td><td>GAN</td><td>\( \left( {{0.444},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>5 RGB, silh., pose, U3D</td><td>1 RGB</td><td></td><td></td><td></td><td>-</td><td>2D, U3D</td><td>GAN</td><td>\( \left( {{0.4849},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td rowspan="2">Johnston et al.</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>Clutter</td><td>1</td><td>Volumetric</td><td>\( {128}^{3} \)</td><td>3D</td><td>Encoder + IDCT</td><td>\( \left( {{0.417}, - .{0.4496}}\right) @{128}^{3} \)</td><td>(-,-, -)</td><td>32</td><td>(-, 2.2GB)</td></tr><tr><td></td><td></td><td></td><td>1</td><td>Volumetric</td><td>\( {32}^{3} \)</td><td>3D</td><td>Encoder + IDCT</td><td>\( \left( {{0.579}, - ,{0.5474}}\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>15</td><td>(-, 1.7GB)</td></tr><tr><td>Yan et al. [4]</td><td>\( n > 1 \) RGB,silh.,pose</td><td>1 RGB</td><td>Clean</td><td>1</td><td>Volumetric</td><td>\( {32}^{3} \)</td><td>2D</td><td>encoder-decoder</td><td>\( \left( {{0.57},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Choy et al. [7]</td><td>\( n > 1 \) RGB,3D GT</td><td>1 RGB</td><td>Clutter</td><td>1</td><td>Volumetric</td><td>\( {32}^{3} \)</td><td>3D</td><td>encoder-LSTM-</td><td>\( \left( {{0.56}. - .{0.537}}\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>73.35</td><td>(35.97M, > 4.5GB)</td></tr><tr><td>Kato et al. 155]</td><td>\( n = 1 \) RGB,silh.,pose</td><td>20 RGB 1 RGB</td><td>Clutter</td><td>1</td><td>Mesh + texture</td><td>\( {32}^{3} \) -</td><td>2D</td><td>decoder Encoder-Decoder</td><td>\( \left( {{0.636},-, - }\right) @{32}^{3} \) \( \left( {{0.513},-, - }\right) @{32}^{3} \)</td><td>(-,-, -) (0.0378,-, -)</td><td>- -</td><td>- -</td></tr><tr><td></td><td>20 RGB, silh., poses</td><td></td><td></td><td></td><td></td><td>-</td><td></td><td></td><td>\( \left( {{0.655},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Mandikal et al. [8]</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>Clean</td><td>1</td><td>Point cloud</td><td>16384</td><td>3D</td><td>Conv + FC lavers. Global to local</td><td>(-,-, -)</td><td>(8.63,-, -)</td><td>-</td><td>(13.3M, -)</td></tr><tr><td>Jiang et al. [74]</td><td>\( n = 1\mathrm{{RGB}} + 3\mathrm{{DGT}} \)</td><td>1 RGB</td><td>Clutter</td><td>1</td><td>Point cloud</td><td>1024</td><td>3D</td><td>\( 2 \times \) (encoder-decoder), GAN</td><td>\( \left( {{0.7116},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Zeng et al. [80]</td><td>1 RGB, silh., pose, 3D</td><td>1 RGB</td><td>Clutter</td><td>1</td><td>Point cloud</td><td>1024</td><td>3D + self</td><td>encoder-decoder + point auto-encoder</td><td>\( \left( {{0.648},-, - }\right) @{32}^{3} \)</td><td>(3.678,-, -)</td><td>-</td><td>-</td></tr><tr><td>Kato et al. 55]</td><td>\( n = 1 \) RGB,silh.,</td><td>1 RGB</td><td>Clutter</td><td>1</td><td>Mesh</td><td>642</td><td>2D</td><td>encoder + FC layers</td><td>\( \left( {{0.602},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Jack et al. [58]</td><td>1 RGB, FFD params</td><td>1 RGB</td><td>Clean</td><td>1</td><td>Mesh</td><td>16384</td><td>3D</td><td>Encoder + FC lavers</td><td>\( \left( {{0.671},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Groueix et al. [54</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>Clean</td><td>1</td><td>Mesh</td><td>1024</td><td>3D</td><td>Multibranch MLP</td><td>(-,-, -)</td><td>(1.51,-, -)</td><td>-</td><td>-</td></tr><tr><td>Wang et al. [56]</td><td>1 RGB, 3D GT</td><td>1 RGB</td><td>Clean</td><td>1</td><td>Mesh</td><td>2466</td><td>3D</td><td>see Fig. 2 (left)</td><td>(-,-, -)</td><td>(0.591,-, -)</td><td>-</td><td>-</td></tr><tr><td>Mandikal et al. [21]</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>Clutter</td><td>1</td><td>Point cloud</td><td>2048</td><td>3D</td><td>3D-VAE. TL-embedding</td><td>(-,-, -)</td><td>(5.4,-, -)</td><td>-</td><td>-</td></tr><tr><td>Soltani et al. [19]</td><td>20 silh., poses</td><td>1 silh</td><td>Clear</td><td>1</td><td>20 depth maps</td><td>-</td><td>2D</td><td>3D-VAE</td><td>\( \left( {{0.835},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td></td><td>1 silh.</td><td>1 silh.</td><td></td><td></td><td>20 depth maps</td><td>-</td><td></td><td></td><td>\( \left( {{0.679},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Fan et al. [72]</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>Clutter</td><td>1</td><td>Point set</td><td>1024</td><td>3D</td><td>see Fig. 3-(a)</td><td>\( \left( {{0.64},-, - }\right) @{32}^{3} \)</td><td>(5.62,-, -)</td><td>-</td><td>-</td></tr><tr><td>Pontes et al. [60]</td><td>\( n = 1 \) RGB,GT FFD</td><td>1 RGB</td><td>Clean</td><td>1</td><td>Mesh</td><td>-</td><td>3D</td><td>Encoder + FC layers</td><td>\( \left( {{0.575}, - ,{0.299}}\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Kurenkov et al. [59]</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>Clean</td><td>1</td><td>Point cloud</td><td>1024</td><td>3D</td><td>2D encoder, 3D encoder, 3D decoder</td><td>(-,-, -)</td><td>(0.3,-, -)</td><td>-</td><td>-</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">方法</td><td colspan="4">输入</td><td colspan="2">输出</td><td colspan="2">训练</td><td colspan="4">性能@(ShapeNet, Pix3D,Pascal3D+)</td></tr><tr><td>训练</td><td>测试</td><td>背景</td><td># 对象</td><td>类型</td><td>分辨率</td><td>监督</td><td>网络</td><td>IoU</td><td>CD</td><td>时间</td><td>内存 (# 参数, 内存)</td></tr><tr><td>Xie et al. 86]</td><td>\( n \geq  1 \) RGB,3D GT</td><td>1 RGB</td><td>杂乱</td><td>1</td><td>体积</td><td rowspan="2">\( {32}^{2} \)</td><td>3D</td><td>编码器 (VGG16),</td><td>\( \left( {{0.658},-,{0.669}}\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>9.9</td><td>(114.4M, -)</td></tr><tr><td></td><td></td><td>20 RGB</td><td></td><td></td><td></td><td>3D</td><td>解码器, 精炼器</td><td>\( \left( {{0.705},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Richter et al. [51]</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>干净</td><td>1</td><td>体积</td><td>\( {512}^{3} \)</td><td>3D</td><td>编码器, 2D 解码器</td><td>\( \left( {{0.641},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Tulsiani et al. [11] Taterchenko et al. [33]</td><td>\( n > 1 \) RGB,分割 \( n > 1\mathrm{{RGB}} + 3\mathrm{{DGT}} \)</td><td>1 RGB 1 RGB</td><td>杂乱 杂乱</td><td>1 1</td><td>体积 + 姿态 体积</td><td>- \( {512}^{3} \)</td><td>2D (多视角) 3D</td><td>编码器, 解码器, 姿态 CNN 八叉树生成</td><td>\( \left( {{0.62},-, - }\right) @{32}^{3} \) (-,-, -)</td><td>(-,-, -) (-,-, -)</td><td>- 2.06s</td><td>- (- , 0.88GB)</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>\( {32}^{3} \)</td><td></td><td>网络</td><td>\( \left( {{0.596}, - ,{0.504}}\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>16</td><td>(12.46M,0.29GB)</td></tr><tr><td>Tulsiani et al. [8]</td><td>\( n > 1 \) RGB,轮廓,(深度)</td><td>1 RGB</td><td>杂乱</td><td>1</td><td>体积</td><td>\( {32}^{3} \)</td><td>2D (多视角)</td><td>编码器-解码器</td><td>\( \left( {{0.546},-,{0.536}}\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Wu et al. [6]</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>杂乱</td><td>1</td><td>体积</td><td>\( {128}^{3} \)</td><td>2D 和 2.5D</td><td>TL, 3D-VAE + 编码器-解码器</td><td>\( \left( {{0.57},-,{0.39}}\right) @{128}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td rowspan="4">Gwak et al. [97]</td><td>1 RGB, 轮廓, 姿态</td><td>1 RGB</td><td></td><td></td><td></td><td>-</td><td>2D</td><td>GAN</td><td>\( \left( {{0.257},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>1 RGB, 轮廓, 姿态, U3D</td><td>1 RGB</td><td>杂乱</td><td>1</td><td>体积</td><td></td><td>2D, U3D</td><td>GAN</td><td>\( \left( {{0.403},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td></td><td></td></tr><tr><td>5 RGB, 轮廓, 姿态</td><td>1 RGB</td><td></td><td></td><td></td><td>-</td><td>2D</td><td>GAN</td><td>\( \left( {{0.444},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>5 RGB, 轮廓, 姿态, U3D</td><td>1 RGB</td><td></td><td></td><td></td><td>-</td><td>2D, U3D</td><td>GAN</td><td>\( \left( {{0.4849},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td rowspan="2">Johnston et al.</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>杂乱</td><td>1</td><td>体积</td><td>\( {128}^{3} \)</td><td>3D</td><td>编码器 + IDCT</td><td>\( \left( {{0.417}, - .{0.4496}}\right) @{128}^{3} \)</td><td>(-,-, -)</td><td>32</td><td>(-, 2.2GB)</td></tr><tr><td></td><td></td><td></td><td>1</td><td>体积</td><td>\( {32}^{3} \)</td><td>3D</td><td>编码器 + IDCT</td><td>\( \left( {{0.579}, - ,{0.5474}}\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>15</td><td>(-, 1.7GB)</td></tr><tr><td>Yan et al. [4]</td><td>\( n > 1 \) RGB,轮廓,姿态</td><td>1 RGB</td><td>干净</td><td>1</td><td>体积</td><td>\( {32}^{3} \)</td><td>2D</td><td>编码器-解码器</td><td>\( \left( {{0.57},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Choy et al. [7]</td><td>\( n > 1 \) RGB,3D GT</td><td>1 RGB</td><td>杂乱</td><td>1</td><td>体积</td><td>\( {32}^{3} \)</td><td>3D</td><td>编码器-LSTM-</td><td>\( \left( {{0.56}. - .{0.537}}\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>73.35</td><td>(35.97M, > 4.5GB)</td></tr><tr><td>Kato et al. 155]</td><td>\( n = 1 \) RGB,轮廓,姿态</td><td>20 RGB 1 RGB</td><td>杂乱</td><td>1</td><td>网格 + 纹理</td><td>\( {32}^{3} \) -</td><td>2D</td><td>解码器 编码器-解码器</td><td>\( \left( {{0.636},-, - }\right) @{32}^{3} \) \( \left( {{0.513},-, - }\right) @{32}^{3} \)</td><td>(-,-, -) (0.0378,-, -)</td><td>- -</td><td>- -</td></tr><tr><td></td><td>20 RGB，轮廓，姿势</td><td></td><td></td><td></td><td></td><td>-</td><td></td><td></td><td>\( \left( {{0.655},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Mandikal 等 [8]</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>干净</td><td>1</td><td>点云</td><td>16384</td><td>3D</td><td>卷积 + 全连接层。全局到局部</td><td>(-,-, -)</td><td>(8.63,-, -)</td><td>-</td><td>(13.3M, -)</td></tr><tr><td>Jiang 等 [74]</td><td>\( n = 1\mathrm{{RGB}} + 3\mathrm{{DGT}} \)</td><td>1 RGB</td><td>杂乱</td><td>1</td><td>点云</td><td>1024</td><td>3D</td><td>\( 2 \times \)（编码器-解码器），GAN</td><td>\( \left( {{0.7116},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Zeng 等 [80]</td><td>1 RGB，轮廓，姿势，3D</td><td>1 RGB</td><td>杂乱</td><td>1</td><td>点云</td><td>1024</td><td>3D + 自我</td><td>编码器-解码器 + 点自编码器</td><td>\( \left( {{0.648},-, - }\right) @{32}^{3} \)</td><td>(3.678,-, -)</td><td>-</td><td>-</td></tr><tr><td>Kato 等 [55]</td><td>\( n = 1 \) RGB，轮廓，</td><td>1 RGB</td><td>杂乱</td><td>1</td><td>网格</td><td>642</td><td>2D</td><td>编码器 + 全连接层</td><td>\( \left( {{0.602},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Jack 等 [58]</td><td>1 RGB，FFD 参数</td><td>1 RGB</td><td>干净</td><td>1</td><td>网格</td><td>16384</td><td>3D</td><td>编码器 + 全连接层</td><td>\( \left( {{0.671},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Groueix 等 [54]</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>干净</td><td>1</td><td>网格</td><td>1024</td><td>3D</td><td>多分支 MLP</td><td>(-,-, -)</td><td>(1.51,-, -)</td><td>-</td><td>-</td></tr><tr><td>Wang 等 [56]</td><td>1 RGB，3D GT</td><td>1 RGB</td><td>干净</td><td>1</td><td>网格</td><td>2466</td><td>3D</td><td>见图 2（左）</td><td>(-,-, -)</td><td>(0.591,-, -)</td><td>-</td><td>-</td></tr><tr><td>Mandikal 等 [21]</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>杂乱</td><td>1</td><td>点云</td><td>2048</td><td>3D</td><td>3D-VAE。TL-嵌入</td><td>(-,-, -)</td><td>(5.4,-, -)</td><td>-</td><td>-</td></tr><tr><td>Soltani 等 [19]</td><td>20 轮廓，姿势</td><td>1 轮廓</td><td>清晰</td><td>1</td><td>20 深度图</td><td>-</td><td>2D</td><td>3D-VAE</td><td>\( \left( {{0.835},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td></td><td>1 轮廓。</td><td>1 轮廓。</td><td></td><td></td><td>20 深度图</td><td>-</td><td></td><td></td><td>\( \left( {{0.679},-, - }\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Fan 等 [72]</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>杂乱</td><td>1</td><td>点集</td><td>1024</td><td>3D</td><td>见图 3-(a)</td><td>\( \left( {{0.64},-, - }\right) @{32}^{3} \)</td><td>(5.62,-, -)</td><td>-</td><td>-</td></tr><tr><td>Pontes 等 [60]</td><td>\( n = 1 \) RGB，GT FFD</td><td>1 RGB</td><td>干净</td><td>1</td><td>网格</td><td>-</td><td>3D</td><td>编码器 + 全连接层</td><td>\( \left( {{0.575}, - ,{0.299}}\right) @{32}^{3} \)</td><td>(-,-, -)</td><td>-</td><td>-</td></tr><tr><td>Kurenkov 等 [59]</td><td>\( n = 1 \) RGB,3D GT</td><td>1 RGB</td><td>干净</td><td>1</td><td>点云</td><td>1024</td><td>3D</td><td>2D 编码器，3D 编码器，3D 解码器</td><td>(-,-, -)</td><td>(0.3,-, -)</td><td>-</td><td>-</td></tr></tbody></table></div><!-- Media --><!-- Meanless: 乙--><p>pervision achieve slightly better performance. This can be attributed to the fact that 2D-based supervision methods use loss functions that are based on \(2\mathrm{D}\) binary masks and silhouettes. However, multiple 3D objects can explain the same 2D projections. This 2D to 3D ambiguity has been addressed either by using multiple binary masks captured from multiple viewpoints [19], which can only reconstruct the visual hull and as such, they are limited in accuracy, or by using adversarial training [91], [97], which constrains the reconstructed \(3\mathrm{D}\) shapes to be within the manifold of valid classes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>监督实现了稍微更好的性能。这可以归因于基于2D的监督方法使用基于\(2\mathrm{D}\)二进制掩模和轮廓的损失函数。然而，多个3D对象可以解释相同的2D投影。这种2D到3D的模糊性已经通过使用从多个视点捕获的多个二进制掩模[19]来解决，这只能重建视觉外壳，因此在准确性上受到限制，或者通过使用对抗训练[91]，[97]来解决，这限制了重建的\(3\mathrm{D}\)形状在有效类别的流形内。</p></div><h2>11 FUTURE RESEARCH DIRECTIONS</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>11 未来研究方向</h2></div><p>In light of the extensive research undertaken in the past five years, image-based 3D reconstruction using deep learning techniques has achieved promising results. The topic, however, is still in its infancy and further developments are yet to be expected. In this section, we present some of the current issues and highlight directions for future research.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>鉴于过去五年进行的大量研究，基于图像的3D重建使用深度学习技术取得了令人鼓舞的成果。然而，这一主题仍处于起步阶段，未来仍有进一步发展的期待。在本节中，我们提出一些当前问题并强调未来研究的方向。</p></div><p>(1) Training data issue. The success of deep learning techniques depends heavily on the availability of training data. Unfortunately, the size of the publicly available datasets that include both images and their 3D annotations is small compared to the training datasets used in tasks such as classification and recognition. 2D supervision techniques have been used to address the lack of \(3\mathrm{D}\) training data. Many of them, however, rely on silhouette-based supervision and thus they can only reconstruct the visual hull. As such, we expect to see in the future more papers proposing new large-scale datasets, new weakly-supervised and unsupervised methods that leverage various visual cues, and new domain adaptation techniques where networks trained with data from a certain domain, e.g., synthetically rendered images, are adapted to a new domain, e.g., in-the-wild images, with minimum retraining and supervision. Research on realistic rendering techniques that are able to close the gap between real images and synthetically rendered images can potentially contribute towards addressing the training data issue.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(1) 训练数据问题。深度学习技术的成功在很大程度上依赖于训练数据的可用性。不幸的是，包含图像及其3D注释的公开可用数据集的规模与用于分类和识别等任务的训练数据集相比很小。已经使用2D监督技术来解决缺乏\(3\mathrm{D}\)训练数据的问题。然而，其中许多依赖于基于轮廓的监督，因此只能重建视觉外壳。因此，我们预计未来会有更多论文提出新的大规模数据集、新的弱监督和无监督方法，利用各种视觉线索，以及新的领域适应技术，其中使用来自某一领域的数据（例如，合成渲染图像）训练的网络适应于新领域（例如，野外图像），并且最小化再训练和监督。关于能够缩小真实图像与合成渲染图像之间差距的真实渲染技术的研究，可能有助于解决训练数据问题。</p></div><p>(2) Generalization to unseen objects. Most of the state-of-the-art papers split a dataset into three subsets for training, validation, and testing, e.g., ShapeNet or Pix3D, then report the performance on the test subsets. However, it is not clear how these methods would perform on a completely unseen object/image categories. In fact, the ultimate goal of 3D reconstruction method is to be able to reconstruct any arbitrary \(3\mathrm{D}\) shape from arbitrary images. Learning-based techniques, however, perform well only on images and objects spanned by the training set. Some recent papers, e.g., Cherabier et al. [38], started to address this issue. An interesting direction for future research, however, would be to combine traditional and learning based techniques to improve the generalization of the latter methods.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(2) 对未见对象的泛化。大多数最先进的论文将数据集分为三个子集进行训练、验证和测试，例如ShapeNet或Pix3D，然后报告测试子集的性能。然而，这些方法在完全未见的对象/图像类别上的表现尚不清楚。事实上，3D重建方法的最终目标是能够从任意图像重建任何任意的\(3\mathrm{D}\)形状。然而，基于学习的技术仅在训练集所覆盖的图像和对象上表现良好。一些最近的论文，例如Cherabier等人[38]，开始解决这个问题。然而，未来研究的一个有趣方向是结合传统和基于学习的技术，以提高后者方法的泛化能力。</p></div><p>(2) Fine-scale 3D reconstruction. Current state-of-the-art techniques are able to recover the coarse 3D structure of shapes. Although recent works have significantly improved the resolution of the reconstruction by using refinement modules, they still fail to recover thin and small parts such as plants, hair, and fur. (3) Reconstruction vs. recognition. 3D reconstruction from images is an ill-posed problem. As such, efficient solutions need to combine low-level image cues, structural knowledge, and high-level object understanding. As outlined in the recent paper of Tatarchenko et al. [44], deep learning-based reconstruction methods are biased towards recognition and retrieval. As such, many of them do not generalize well and fail to recover fine-scale details. Thus, we expect in the future to see more research on how to combine top-down approaches (i.e., recognition, classification, and retrieval) with bottom-up approaches (i.e., pixel-level reconstruction based on geometric and photometric cues). This also has the potential to improve the generalization ability of the methods, see item (2) above.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(2) 精细尺度的3D重建。目前最先进的技术能够恢复形状的粗略3D结构。尽管最近的工作通过使用细化模块显著提高了重建的分辨率，但它们仍然无法恢复薄小的部分，如植物、头发和毛发。(3) 重建与识别。从图像进行3D重建是一个病态问题。因此，高效的解决方案需要结合低级图像线索、结构知识和高级对象理解。正如Tatarchenko等人[44]最近的论文中所述，基于深度学习的重建方法偏向于识别和检索。因此，它们中的许多方法泛化能力较差，无法恢复精细的细节。因此，我们预计未来会有更多研究探讨如何将自上而下的方法（即识别、分类和检索）与自下而上的方法（即基于几何和光度线索的像素级重建）结合起来。这也有可能提高方法的泛化能力，见上文(2)项。</p></div><p>(4) Specialized instance reconstruction. We expect in the future to see more synergy between class-specific knowledge modelling and deep learning-based 3D reconstruction in order to leverage domain-specific knowledge. In fact, there is an increasing interest in reconstruction methods that are specialized in specific classes of objects such as human bodies and body parts (which we have briefly covered in this survey), vehicles, animals [57], trees, and buildings. Specialized methods exploit prior and domain-specific knowledge to optimise the network architecture and its training process. As such, they usually perform better than the general framework. However, similar to deep learning-based 3D reconstruction, modelling prior knowledge, e.g., by using advanced statistical shape models [65], [66], [67], [158], [159], requires 3D annotations, which are not easy to obtain for many classes of shapes, e.g., animals in the wild.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(4) 专门实例重建。我们预计未来会看到类特定知识建模与基于深度学习的3D重建之间更多的协同，以利用领域特定知识。事实上，越来越多的研究关注于专门针对特定对象类别（如人体及其部位（我们在本次调查中简要介绍过）、车辆、动物[57]、树木和建筑物）的重建方法。专门的方法利用先验和领域特定知识来优化网络架构及其训练过程。因此，它们通常比通用框架表现更好。然而，与基于深度学习的3D重建类似，建模先验知识（例如，使用先进的统计形状模型[65]，[66]，[67]，[158]，[159]）需要3D注释，而对于许多形状类别（例如，野外动物）来说，这些注释并不容易获得。</p></div><p>(5) Handling multiple objects in the presence of occlusions and cluttered backgrounds. Most of the state-of-the-art techniques deal with images that contain a single object. In-the-wild images, however, contain multiple objects of different categories. Previous works employ detection followed by reconstruction within regions of interests, e.g., [138]. The detection and then reconstruction modules operate independently from each other. However, these tasks are inter-related and can benefit from each other if solved jointly. Towards this goal, two important issues should be addressed. The first one is the lack of training data for multiple-object reconstruction. Second, designing appropriate CNN architectures, loss functions, and learning methods are important especially for methods that are trained without 3D supervision. These, in general, use silhouette-based loss functions, which require accurate object-level segmentation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(5) 在遮挡和杂乱背景下处理多个物体。大多数最先进的技术处理的是包含单个物体的图像。然而，野外图像包含多个不同类别的物体。之前的工作采用检测后在感兴趣区域内进行重建，例如，[138]。检测和重建模块彼此独立操作。然而，这些任务是相互关联的，如果共同解决，可以相互受益。为此，应该解决两个重要问题。第一个是缺乏多物体重建的训练数据。第二，设计合适的卷积神经网络（CNN）架构、损失函数和学习方法尤其重要，特别是对于没有3D监督的训练方法。这些通常使用基于轮廓的损失函数，这需要准确的物体级分割。</p></div><p>(6) 3D video. This paper focused on 3D reconstruction from one or multiple images, but with no temporal correlation. There is, however, a growing interest in 3D video, i.e., 3D reconstruction of entire video streams where successive frames are temporally correlated. On one hand, the availability of a sequence of frames can improve the reconstruction, since one can exploit the additional information available in subsequent frames to disambiguate and refine the reconstruction at the current frame. On the other hand, the reconstruction should be smooth and consistent across frames.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(6) 3D视频。本文集中于从一张或多张图像进行3D重建，但没有时间相关性。然而，3D视频（即整个视频流的3D重建，其中连续帧在时间上相关）正受到越来越多的关注。一方面，帧序列的可用性可以改善重建，因为可以利用后续帧中可用的额外信息来消歧义并细化当前帧的重建。另一方面，重建应该在帧之间保持平滑和一致。</p></div><p>(7) Towards full 3D scene parsing. Finally, the ultimate goal is to be able to semantically parse a full 3D scene from one or multiple of its images. This requires joint detection, recognition, and reconstruction. It would also require capturing and modeling spatial relationships and interactions between objects and between object parts. While there have been a few attempts in the past to address this problem, they are mostly limited to indoor scenes with strong assumptions about the geometry and locations of the objects that compose the scene.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>(7) 朝着完整的3D场景解析迈进。最终目标是能够从一张或多张图像中对完整的3D场景进行语义解析。这需要联合检测、识别和重建。还需要捕捉和建模物体之间及物体部分之间的空间关系和交互。虽然过去有一些尝试解决这个问题，但大多限于对场景中物体的几何形状和位置有强假设的室内场景。</p></div><h2>12 SUMMARY AND CONCLUDING REMARKS</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>12 总结与结论</h2></div><p>This paper provides a comprehensive survey of the past five years developments in the field of image-based 3D object reconstruction using deep learning techniques. We classified the state-of-the-art into volumetric, surface-based, and point-based techniques. We then discussed methods in each category based on their input, the network architectures, and the training mechanisms they use. We have also discussed and compared the performance of some key methods.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>本文对过去五年在基于图像的3D物体重建领域使用深度学习技术的发展进行了全面的调查。我们将最先进的技术分为体积、基于表面和基于点的技术。然后，我们根据输入、网络架构和使用的训练机制讨论了每个类别中的方法。我们还讨论并比较了一些关键方法的性能。</p></div><p>This survey focused on methods that define \(3\mathrm{D}\) reconstruction as the problem of recovering the 3D geometry of objects from one or multiple RGB images. There are, however, many other related problems that share similar solutions. The closest topics include depth reconstruction from RGB images, which has been recently addressed using deep learning techniques,see the recent survey of Laga [153], 3D shape completion [26], [28], [45], [103], [156], [160], [161], 3D reconstruction from depth images [103], which can be seen as a 3D fusion and completion problem, 3D reconstruction and modelling from hand-drawn 2D sketches [162], [163], novel view synthesis [164], [165], and 3D shape structure recovery [10], [29], [83], [96]. These topics have been extensively investigated in the past five years and require separate survey papers.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>本次调查集中于将\(3\mathrm{D}\)重建定义为从一张或多张RGB图像中恢复物体的3D几何形状的问题。然而，还有许多其他相关问题共享类似的解决方案。最接近的主题包括从RGB图像进行深度重建，最近已使用深度学习技术进行了解决，参见Laga的最新调查[153]，3D形状补全[26]，[28]，[45]，[103]，[156]，[160]，[161]，从深度图像进行3D重建[103]，这可以视为3D融合和补全问题，从手绘2D草图进行3D重建和建模[162]，[163]，新视角合成[164]，[165]，以及3D形状结构恢复[10]，[29]，[83]，[96]。这些主题在过去五年中得到了广泛研究，需要单独的调查论文。</p></div><h2>ACKNOWLEDGEMENTS</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>致谢</h2></div><p>Xian-Feng Han is supported by a China Scholarship Council (CSC) scholarship. This work was supported in part by ARC DP150100294 and DP150104251. REFERENCES</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Xian-Feng Han获得了中国国家留学基金委（CSC）奖学金的支持。此项工作部分得到了ARC DP150100294和DP150104251的支持。参考文献</p></div><p>[1] R. Hartley and A. Zisserman, Multiple view geometry in computer vision. Cambridge university press, 2003.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[1] R. Hartley和A. Zisserman, 计算机视觉中的多视图几何。剑桥大学出版社，2003。</p></div><p>[2] A. Laurentini, "The visual hull concept for silhouette-based image understanding," IEEE TPAMI, vol. 16, no. 2, pp. 150-162, 1994.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[2] A. Laurentini, "基于轮廓的图像理解的视觉外壳概念," IEEE TPAMI, 第16卷，第2期, 第150-162页, 1994。</p></div><p>[3] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, "3D shapenets: A deep representation for volumetric shapes," in IEEE CVPR, 2015, pp. 1912-1920.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[3] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang和J. Xiao, "3D shapenets: 一种体积形状的深度表示," 在IEEE CVPR, 2015, 第1912-1920页。</p></div><p>[4] X. Yan, J. Yang, E. Yumer, Y. Guo, and H. Lee, "Perspective Transformer Nets: Learning single-view 3D object reconstruction without 3D supervision," in NIPS, 2016, pp. 1696-1704.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[4] X. Yan, J. Yang, E. Yumer, Y. Guo和H. Lee, "透视变换网络: 在没有3D监督的情况下学习单视图3D物体重建," 在NIPS, 2016, 第1696-1704页。</p></div><p>[5] E. Grant, P. Kohli, and M. van Gerven, "Deep disentangled representations for volumetric reconstruction," in ECCV, 2016, pp. 266-279.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[5] E. Grant, P. Kohli和M. van Gerven, "用于体积重建的深度解耦表示," 在ECCV, 2016, 第266-279页。</p></div><p>[6] J. Wu, Y. Wang, T. Xue, X. Sun, B. Freeman, and J. Tenenbaum, "MarrNet: 3D shape reconstruction via 2.5D sketches," in NIPS, 2017, pp. 540-550.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[6] J. Wu, Y. Wang, T. Xue, X. Sun, B. Freeman, 和 J. Tenenbaum, "MarrNet: 通过2.5D草图进行3D形状重建," 在NIPS, 2017, 第540-550页.</p></div><p>[7] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese, "3D-R2N2: A unified approach for single and multi-view 3D object reconstruction," in ECCV, 2016, pp. 628-644.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[7] C. B. Choy, D. Xu, J. Gwak, K. Chen, 和 S. Savarese, "3D-R2N2: 单视图和多视图3D物体重建的统一方法," 在ECCV, 2016, 第628-644页.</p></div><p>[8] S. Tulsiani, T. Zhou, A. A. Efros, and J. Malik, "Multi-view supervision for single-view reconstruction via differentiable ray consistency," in IEEE CVPR, vol. 1, no. 2, 2017, p. 3.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[8] S. Tulsiani, T. Zhou, A. A. Efros, 和 J. Malik, "通过可微分光线一致性进行单视图重建的多视图监督," 在IEEE CVPR, 第1卷，第2期, 2017, 第3页.</p></div><p>[9] X. Z. Xingyuan Sun, Jiajun Wu and Z. Zhang, "Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling," in IEEE CVPR, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[9] X. Z. Xingyuan Sun, Jiajun Wu 和 Z. Zhang, "Pix3D: 单图像3D形状建模的数据集和方法," 在IEEE CVPR, 2018.</p></div><p>[10] O. Wiles and A. Zisserman, "SilNet: Single-and Multi-View Reconstruction by Learning from Silhouettes," BMVC, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[10] O. Wiles 和 A. Zisserman, "SilNet: 通过学习轮廓进行单视图和多视图重建," BMVC, 2017.</p></div><p>[11] S. Tulsiani, A. A. Efros, and J. Malik, "Multi-View Consistency as Supervisory Signal for Learning Shape and Pose Prediction," in IEEE CVPR, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[11] S. Tulsiani, A. A. Efros, 和 J. Malik, "多视图一致性作为学习形状和姿态预测的监督信号," 在IEEE CVPR, 2018.</p></div><p>[12] A. Johnston, R. Garg, G. Carneiro, I. Reid, and A. van den Hengel, "Scaling CNNs for High Resolution Volumetric Reconstruction From a Single Image," in IEEE CVPR, 2017, pp. 939-948.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[12] A. Johnston, R. Garg, G. Carneiro, I. Reid, 和 A. van den Hengel, "从单图像进行高分辨率体积重建的CNN扩展," 在IEEE CVPR, 2017, 第939-948页.</p></div><p>[13] G. Yang, Y. Cui, S. Belongie, and B. Hariharan, "Learning single-view 3d reconstruction with limited pose supervision," in ECCV, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[13] G. Yang, Y. Cui, S. Belongie, 和 B. Hariharan, "在有限姿态监督下学习单视图3D重建," 在ECCV, 2018.</p></div><p>[14] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in IEEE CVPR, 2016, pp. 770-778.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[14] K. He, X. Zhang, S. Ren, 和 J. Sun, "用于图像识别的深度残差学习," 在IEEE CVPR, 2016, 第770-778页.</p></div><p>[15] K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition," arXiv preprint arXiv:1409.1556, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[15] K. Simonyan 和 A. Zisserman, "用于大规模图像识别的非常深的卷积网络," arXiv预印本 arXiv:1409.1556, 2014.</p></div><p>[16] D. P. Kingma and M. Welling, "Auto-encoding variational bayes," ICLR, 2014.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[16] D. P. Kingma 和 M. Welling, "自编码变分贝叶斯," ICLR, 2014.</p></div><p>[17] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum, "Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling," in NIPS, 2016, pp. 82-90.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[17] J. Wu, C. Zhang, T. Xue, B. Freeman, 和 J. Tenenbaum, "通过3D生成对抗建模学习物体形状的概率潜在空间," 在NIPS, 2016, 第82-90页.</p></div><p>[18] S. Liu, C. L. Giles, I. Ororbia, and G. Alexander, "Learning a Hierarchical Latent-Variable Model of 3D Shapes," International Conference on 3D Vision, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[18] S. Liu, C. L. Giles, I. Ororbia, 和 G. Alexander, "学习3D形状的层次潜变量模型," 国际3D视觉会议, 2018.</p></div><p>[19] A. A. Soltani, H. Huang, J. Wu, T. D. Kulkarni, and J. B. Tenenbaum, "Synthesizing 3D shapes via modeling multi-view depth maps and silhouettes with deep generative networks," in IEEE CVPR, 2017, pp. 1511-1519.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[19] A. A. Soltani, H. Huang, J. Wu, T. D. Kulkarni, 和 J. B. Tenenbaum, "通过建模多视图深度图和轮廓与深度生成网络合成3D形状," 在IEEE CVPR, 2017, 第1511-1519页.</p></div><p>[20] P. Henderson and V. Ferrari, "Learning to generate and reconstruct 3D meshes with only 2D supervision," BMVC, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[20] P. Henderson 和 V. Ferrari, "学习仅用2D监督生成和重建3D网格," BMVC, 2018.</p></div><p>[21] P. Mandikal, N. Murthy, M. Agarwal, and R. V. Babu, "3D-LMNet: Latent Embedding Matching for Accurate and Diverse 3D Point Cloud Reconstruction from a Single Image," BMVC, pp. 662-674, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[21] P. Mandikal, N. Murthy, M. Agarwal, 和 R. V. Babu, "3D-LMNet: 从单图像进行准确和多样化3D点云重建的潜在嵌入匹配," BMVC, 第662-674页, 2018.</p></div><p>[22] M. Gadelha, R. Wang, and S. Maji, "Multiresolution tree networks for 3D point cloud processing," in ECCV, 2018, pp. 103-118.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[22] M. Gadelha, R. Wang, 和 S. Maji, "用于3D点云处理的多分辨率树网络," 在ECCV, 2018, 第103-118页.</p></div><p>[23] H. Laga, Y. Guo, H. Tabia, R. B. Fisher, and M. Bennamoun, 3D Shape Analysis: Fundamentals, Theory, and Applications. John Wiley &#x26; Sons, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[23] H. Laga, Y. Guo, H. Tabia, R. B. Fisher, 和 M. Bennamoun, 3D形状分析：基础、理论与应用. 约翰·威利父子公司, 2019.</p></div><p>[24] R. Zhu, H. K. Galoogahi, C. Wang, and S. Lucey, "Rethinking re-projection: Closing the loop for pose-aware shape reconstruction from a single image," in IEEE ICCV, 2017, pp. 57-65.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[24] R. Zhu, H. K. Galoogahi, C. Wang, 和 S. Lucey, "重新思考重投影：闭合单幅图像的姿态感知形状重建循环," 在IEEE ICCV, 2017, 第57-65页.</p></div><p>[25] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta, "Learning a predictable and generative vector representation for objects," in ECCV, 2016, pp. 484-499.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[25] R. Girdhar, D. F. Fouhey, M. Rodriguez, 和 A. Gupta, "学习可预测和生成的对象向量表示," 在ECCV, 2016, 第484-499页.</p></div><p>[26] A. Dai, C. Ruizhongtai Qi, and M. Nießner, "Shape completion using 3D-encoder-predictor CNNs and shape synthesis," in IEEE CVPR, 2017, pp. 5868-5877.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[26] A. Dai, C. Ruizhongtai Qi, 和 M. Nießner, "使用3D编码器-预测器CNN和形状合成的形状补全," 在IEEE CVPR, 2017, 第5868-5877页.</p></div><p>[27] M. Gadelha, S. Maji, and R. Wang, "3D shape induction from 2D views of multiple objects," in 3D Vision, 2017, pp. 402-411.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[27] M. Gadelha, S. Maji, 和 R. Wang, "从多个对象的2D视图中诱导3D形状," 在3D视觉, 2017, 第402-411页.</p></div><p>[28] W. Wang, Q. Huang, S. You, C. Yang, and U. Neumann, "Shape inpainting using 3D generative adversarial network and recurrent convolutional networks," ICCV, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[28] W. Wang, Q. Huang, S. You, C. Yang, 和 U. Neumann, "使用3D生成对抗网络和递归卷积网络的形状修复," ICCV, 2017.</p></div><p>[29] C. Zou, E. Yumer, J. Yang, D. Ceylan, and D. Hoiem, "3D-PRNN: Generating shape primitives with recurrent neural networks," in IEEE ICCV, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[29] C. Zou, E. Yumer, J. Yang, D. Ceylan, 和 D. Hoiem, "3D-PRNN：使用递归神经网络生成形状原语," 在IEEE ICCV, 2017.</p></div><p>[30] V. A. Knyaz, V. V. Kniaz, and F. Remondino, "Image-to-Voxel Model Translation with Conditional Adversarial Networks," in ECCV, 2018, pp. 0-0.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[30] V. A. Knyaz, V. V. Kniaz, 和 F. Remondino, "使用条件对抗网络进行图像到体素模型转换," 在ECCV, 2018, 第0-0页.</p></div><p>[31] A. Kundu, Y. Li, and J. M. Rehg, "3D-RCNN: Instance-Level 3D Object Reconstruction via Render-and-Compare," in IEEE CVPR, 2018, pp. 3559-3568.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[31] A. Kundu, Y. Li, 和 J. M. Rehg, "3D-RCNN：通过渲染和比较进行实例级3D对象重建," 在IEEE CVPR, 2018, 第3559-3568页.</p></div><p>[32] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong, "O-CNN: Octree-based convolutional neural networks for 3D shape analysis," ACM TOG, vol. 36, no. 4, p. 72, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[32] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, 和 X. Tong, "O-CNN：基于八叉树的卷积神经网络用于3D形状分析," ACM TOG, 第36卷, 第4期, 第72页, 2017.</p></div><p>[33] M. Tatarchenko, A. Dosovitskiy, and T. Brox, "Octree generating networks: Efficient convolutional architectures for high-resolution 3D outputs," in IEEE CVPR, 2017, pp. 2088-2096.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[33] M. Tatarchenko, A. Dosovitskiy, 和 T. Brox, "八叉树生成网络：高分辨率3D输出的高效卷积架构," 在IEEE CVPR, 2017, 第2088-2096页.</p></div><p>[34] P.-S. Wang, C.-Y. Sun, Y. Liu, and X. Tong, "Adaptive O-CNN: a patch-based deep representation of 3D shapes," ACM ToG, p. 217, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[34] P.-S. Wang, C.-Y. Sun, Y. Liu, 和 X. Tong, "自适应O-CNN：基于补丁的3D形状深度表示," ACM ToG, 第217页, 2018.</p></div><p>[35] Y.-P. Cao, Z.-N. Liu, Z.-F. Kuang, L. Kobbelt, and S.-M. Hu, "Learning to reconstruct high-quality 3D shapes with cascaded fully convolutional networks," in ECCV, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[35] Y.-P. Cao, Z.-N. Liu, Z.-F. Kuang, L. Kobbelt, 和 S.-M. Hu, "学习使用级联全卷积网络重建高质量3D形状," 在ECCV, 2018.</p></div><p>[36] C. Hane, S. Tulsiani, and J. Malik, "Hierarchical Surface Prediction," IEEE PAMI, no. 1, pp. 1-1, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[36] C. Hane, S. Tulsiani, 和 J. Malik, "层次表面预测," IEEE PAMI, 第1期, 第1-1页, 2019.</p></div><p>[37] B. Curless and M. Levoy, "A volumetric method for building complex models from range images," CUMINCAD, 1996.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[37] B. Curless 和 M. Levoy, "一种从范围图像构建复杂模型的体积方法," CUMINCAD, 1996.</p></div><p>[38] I. Cherabier, J. L. Schonberger, M. R. Oswald, M. Pollefeys, and A. Geiger, "Learning Priors for Semantic 3D Reconstruction," in ECCV, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[38] I. Cherabier, J. L. Schonberger, M. R. Oswald, M. Pollefeys, 和 A. Geiger, "学习语义3D重建的先验知识," 在ECCV, 2018.</p></div><p>[39] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove, "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation," in IEEE CVPR, 2019, pp. 165-174.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[39] J. J. Park, P. Florence, J. Straub, R. Newcombe, 和 S. Lovegrove, "DeepSDF: 学习用于形状表示的连续有符号距离函数," 在IEEE CVPR, 2019, 第165-174页.</p></div><p>[40] E. Smith and D. Meger, "Improved Adversarial Systems for 3D Object Generation and Reconstruction," arXiv:1707.09557, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[40] E. Smith 和 D. Meger, "改进的对抗系统用于3D物体生成和重建," arXiv:1707.09557, 2017.</p></div><p>[41] G. Riegler, A. O. Ulusoy, and A. Geiger, "OctNet: Learning deep 3D representations at high resolutions," in IEEE CVPR, vol. 3, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[41] G. Riegler, A. O. Ulusoy, 和 A. Geiger, "OctNet: 在高分辨率下学习深度3D表示," 在IEEE CVPR, 第3卷, 2017.</p></div><p>[42] J. Li, K. Xu, S. Chaudhuri, E. Yumer, H. Zhang, and L. Guibas, "GRASS: Generative Recursive Autoencoders for Shape Structures," ACM TOG, vol. 36, no. 4, p. 52, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[42] J. Li, K. Xu, S. Chaudhuri, E. Yumer, H. Zhang, 和 L. Guibas, "GRASS: 用于形状结构的生成递归自编码器," ACM TOG, 第36卷, 第4期, 第52页, 2017.</p></div><p>[43] Z. Chen and H. Zhang, "Learning implicit fields for generative shape modeling," in IEEE CVPR, 2019, pp. 5939-5948.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[43] Z. Chen 和 H. Zhang, "学习隐式场用于生成形状建模," 在IEEE CVPR, 2019, 第5939-5948页.</p></div><p>[44] M. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, and T. Brox, "What do single-view 3d reconstruction networks learn?" in IEEE CVPR, June 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[44] M. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, 和 T. Brox, "单视图3D重建网络学习了什么?" 在IEEE CVPR, 2019年6月.</p></div><p>[45] X. Han, Z. Li, H. Huang, E. Kalogerakis, and Y. Yu, "High-resolution shape completion using deep neural networks for global structure and local geometry inference," in IEEE CVPR, 2017, pp. 85-93.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[45] X. Han, Z. Li, H. Huang, E. Kalogerakis, 和 Y. Yu, "使用深度神经网络进行高分辨率形状补全以推断全局结构和局部几何," 在IEEE CVPR, 2017, 第85-93页.</p></div><p>[46] B. Yang, S. Rosa, A. Markham, N. Trigoni, and H. Wen, "Dense 3D object reconstruction from a single depth view," IEEE PAMI, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[46] B. Yang, S. Rosa, A. Markham, N. Trigoni, 和 H. Wen, "从单个深度视图进行密集3D物体重建," IEEE PAMI, 2018.</p></div><p>[47] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell, "Long-term recurrent convolutional networks for visual recognition and description," in IEEE CVPR, 2015, pp. 2625-2634.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[47] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, 和 T. Darrell, "用于视觉识别和描述的长期递归卷积网络," 在IEEE CVPR, 2015, 第2625-2634页.</p></div><p>[48] O. Ronneberger, P. Fischer, and T. Brox, "U-net: Convolutional networks for biologically-motivatedge segmentation," in MIC-CAI, 2015, pp. 234-241.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[48] O. Ronneberger, P. Fischer, 和 T. Brox, "U-net: 用于生物学动机的边缘分割的卷积网络," 在MIC-CAI, 2015, 第234-241页.</p></div><p>[49] W. E. Lorensen and H. E. Cline, "Marching cubes: A high resolution 3D surface construction algorithm," vol. 21, no. 4, pp. 163-169, 1987.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[49] W. E. Lorensen 和 H. E. Cline, "Marching cubes: 一种高分辨率3D表面构建算法," 第21卷, 第4期, 第163-169页, 1987.</p></div><p>[50] Y. Liao, S. Donné, and A. Geiger, "Deep Marching Cubes: Learning Explicit Surface Representations," in IEEE CVPR, 2018, pp. 2916-2925.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[50] Y. Liao, S. Donné, 和 A. Geiger, "深度Marching Cubes: 学习显式表面表示," 在IEEE CVPR, 2018, 第2916-2925页.</p></div><p>[51] S. R. Richter and S. Roth, "Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers," in IEEE CVPR, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[51] S. R. Richter 和 S. Roth, "Matryoshka Networks: 通过嵌套形状层预测3D几何," 在IEEE CVPR, 2018.</p></div><p>[52] A. Sinha, A. Unmesh, Q. Huang, and K. Ramani, "SurfNet: Generating 3D shape surfaces using deep residual networks," in IEEE CVPR, vol. 1, no. 2, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[52] A. Sinha, A. Unmesh, Q. Huang, 和 K. Ramani, "SurfNet: 使用深度残差网络生成3D形状表面," 在IEEE CVPR, 第1卷, 第2期, 2017.</p></div><p>[53] A. Pumarola, A. Agudo, L. Porzi, A. Sanfeliu, V. Lepetit, and F. Moreno-Noguer, "Geometry-Aware Network for Non-Rigid Shape Prediction From a Single View," in IEEE CVPR, June 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[53] A. Pumarola, A. Agudo, L. Porzi, A. Sanfeliu, V. Lepetit, 和 F. Moreno-Noguer, "几何感知网络用于从单个视图预测非刚性形状," 在IEEE CVPR, 2018年6月.</p></div><p>[54] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry, "AtlasNet: A papier-Mache Approach to Learning 3D Surface Generation," in IEEE CVPR, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[54] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, 和 M. Aubry, "AtlasNet: 一种纸浆法学习3D表面生成," 在IEEE CVPR, 2018.</p></div><p>[55] H. Kato, Y. Ushiku, and T. Harada, "Neural 3D Mesh Renderer," in IEEE CVPR, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[55] H. Kato, Y. Ushiku, 和 T. Harada, "神经3D网格渲染器," 在IEEE CVPR, 2018.</p></div><p>[56] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang, "Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images," in ECCV, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[56] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, 和 Y.-G. Jiang, "Pixel2Mesh: 从单个RGB图像生成3D网格模型," 在ECCV, 2018.</p></div><p>[57] A. Kanazawa, S. Tulsiani, A. A. Efros, and J. Malik, "Learning Category-Specific Mesh Reconstruction from Image Collections," \({ECCV},{2018}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[57] A. Kanazawa, S. Tulsiani, A. A. Efros, 和 J. Malik, "从图像集合中学习类别特定的网格重建," \({ECCV},{2018}\) .</p></div><p>[58] D. Jack, J. K. Pontes, S. Sridharan, C. Fookes, S. Shirazi, F. Maire, and A. Eriksson, "Learning free-form deformations for 3D object reconstruction," ACCV, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[58] D. Jack, J. K. Pontes, S. Sridharan, C. Fookes, S. Shirazi, F. Maire, 和 A. Eriksson, "学习自由形变用于3D物体重建," ACCV, 2018.</p></div><p>[59] A. Kurenkov, J. Ji, A. Garg, V. Mehta, J. Gwak, C. Choy, and S. Savarese, "DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction from a Single Image," IEEE WACV, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[59] A. Kurenkov, J. Ji, A. Garg, V. Mehta, J. Gwak, C. Choy, 和 S. Savarese, "DeformNet: 从单个图像进行3D形状重建的自由形变网络," IEEE WACV, 2018.</p></div><p>[60] J. K. Pontes, C. Kong, S. Sridharan, S. Lucey, A. Eriksson, and C. Fookes, "Image2Mesh: A Learning Framework for Single Image 3D Reconstruction," ACCV, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[60] J. K. Pontes, C. Kong, S. Sridharan, S. Lucey, A. Eriksson, 和 C. Fookes, "Image2Mesh: 单图像3D重建的学习框架," ACCV, 2018.</p></div><p>[61] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein, "Geometric deep learning on graphs and manifolds using mixture model cnns," in CVPR, vol. 1, no. 2, 2017, p. 3.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[61] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, 和 M. M. Bronstein, "在图和流形上使用混合模型卷积神经网络的几何深度学习," 在CVPR, 第1卷，第2期, 2017, 第3页.</p></div><p>[62] C. Gotsman, X. Gu, and A. Sheffer, "Fundamentals of spherical parameterization for 3d meshes," ACM TOG, vol. 22, no. 3, pp. 358-363, 2003.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[62] C. Gotsman, X. Gu, 和 A. Sheffer, "3D网格的球面参数化基础," ACM TOG, 第22卷，第3期, 第358-363页, 2003.</p></div><p>[63] E. Praun and H. Hoppe, "Spherical parametrization and remesh-ing," ACM TOG, vol. 22, no. 3, pp. 340-349, 2003.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[63] E. Praun 和 H. Hoppe, "球面参数化和重网格化," ACM TOG, 第22卷，第3期, 第340-349页, 2003.</p></div><p>[64] A. Sheffer, E. Praun, K. Rose et al., "Mesh parameterization methods and their applications," Foundations and Trends® in Computer Graphics and Vision, vol. 2, no. 2, pp. 105-171, 2007.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[64] A. Sheffer, E. Praun, K. Rose 等, "网格参数化方法及其应用," 计算机图形学与视觉基础与趋势®, 第2卷，第2期, 第105-171页, 2007.</p></div><p>[65] H. Laga, Q. Xie, I. H. Jermyn, A. Srivastava et al., "Numerical inversion of srnf maps for elastic shape analysis of genus-zero surfaces," IEEE PAMI, vol. 39, no. 12, pp. 2451-2464, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[65] H. Laga, Q. Xie, I. H. Jermyn, A. Srivastava 等, "用于零基数曲面的弹性形状分析的SRNF映射的数值反演," IEEE PAMI, 第39卷，第12期, 第2451-2464页, 2017.</p></div><p>[66] G. Wang, H. Laga, N. Xie, J. Jia, and H. Tabia, "The shape space of 3d botanical tree models," ACM TOG, vol. 37, no. 1, p. 7, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[66] G. Wang, H. Laga, N. Xie, J. Jia, 和 H. Tabia, "3D植物树模型的形状空间," ACM TOG, 第37卷，第1期, 第7页, 2018.</p></div><p>[67] G. Wang, H. Laga, J. Jia, N. Xie, and H. Tabia, "Statistical modeling of the \(3\mathrm{\;d}\) geometry and topology of botanical trees," CGF, vol. 37, no. 5, pp. 185-198, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[67] G. Wang, H. Laga, J. Jia, N. Xie, 和 H. Tabia, "植物树的几何和拓扑的统计建模," CGF, 第37卷，第5期, 第185-198页, 2018.</p></div><p>[68] V. Blanz and T. Vetter, "A morphable model for the synthesis of 3d faces," in Siggraph, 1999, pp. 187-194.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[68] V. Blanz 和 T. Vetter, "用于合成3D人脸的可变形模型," 在Siggraph, 1999, 第187-194页.</p></div><p>[69] S. Vicente, J. Carreira, L. Agapito, and J. Batista, "Reconstructing PASCAL VOC," in IEEE CVPR, 2014, pp. 41-48.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[69] S. Vicente, J. Carreira, L. Agapito, 和 J. Batista, "重建PASCAL VOC," 在IEEE CVPR, 2014, 第41-48页.</p></div><p>[70] S. Tulsiani, A. Kar, J. Carreira, and J. Malik, "Learning category-specific deformable 3D models for object reconstruction," IEEE PAMI, vol. 39, no. 4, pp. 719-731, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[70] S. Tulsiani, A. Kar, J. Carreira, 和 J. Malik, "学习类别特定的可变形3D模型用于物体重建," IEEE PAMI, 第39卷, 第4期, 页719-731, 2017.</p></div><p>[71] J. K. Pontes, C. Kong, A. Eriksson, C. Fookes, S. Sridharan, and S. Lucey, "Compact model representation for 3D reconstruction," 3DV, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[71] J. K. Pontes, C. Kong, A. Eriksson, C. Fookes, S. Sridharan, 和 S. Lucey, "3D重建的紧凑模型表示," 3DV, 2017.</p></div><p>[72] H. Fan, H. Su, and L. Guibas, "A point set generation network for 3D object reconstruction from a single image," in IEEE CVPR, vol. 38, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[72] H. Fan, H. Su, 和 L. Guibas, "用于从单幅图像进行3D物体重建的点集生成网络," 在IEEE CVPR, 第38卷, 2017.</p></div><p>[73] C.-H. Lin, C. Kong, and S. Lucey, "Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction," AAAI, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[73] C.-H. Lin, C. Kong, 和 S. Lucey, "学习高效的点云生成用于密集3D物体重建," AAAI, 2018.</p></div><p>[74] L. Jiang, S. Shi, X. Qi, and J. Jia, "GAL: Geometric Adversarial Loss for Single-View 3D-Object Reconstruction," in ECCV,2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[74] L. Jiang, S. Shi, X. Qi, 和 J. Jia, "GAL: 单视图3D物体重建的几何对抗损失," 在ECCV, 2018.</p></div><p>[75] C.-L. Li, M. Zaheer, Y. Zhang, B. Poczos, and R. Salakhutdinov, "Point cloud GAN," ICLR Workshop on Deep Generative Models for Highly Structured Data, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[75] C.-L. Li, M. Zaheer, Y. Zhang, B. Poczos, 和 R. Salakhutdinov, "点云GAN," ICLR深度生成模型研讨会, 2019.</p></div><p>[76] Y. Sun, Y. Wang, Z. Liu, J. E. Siegel, and S. E. Sarma, "Point-Grow: Autoregressively learned point cloud generation with self-attention," arXiv:1810.05591, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[76] Y. Sun, Y. Wang, Z. Liu, J. E. Siegel, 和 S. E. Sarma, "Point-Grow: 自回归学习的点云生成与自注意力," arXiv:1810.05591, 2018.</p></div><p>[77] E. Insafutdinov and A. Dosovitskiy, "Unsupervised learning of shape and pose with differentiable point clouds," in NIPS, 2018, pp. 2802-2812.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[77] E. Insafutdinov 和 A. Dosovitskiy, "通过可微分点云进行形状和姿态的无监督学习," 在NIPS, 2018, 页2802-2812.</p></div><p>[78] K. Li, T. Pham, H. Zhan, and I. Reid, "Efficient dense point cloud object reconstruction using deformation vector fields," in ECCV, 2018, pp. 497-513.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[78] K. Li, T. Pham, H. Zhan, 和 I. Reid, "使用变形向量场进行高效的密集点云物体重建," 在ECCV, 2018, 页497-513.</p></div><p>[79] K. Li, R. Garg, M. Cai, and I. Reid, "Single-view object shape reconstruction using deep shape prior and silhouette," arXiv:1811.11921, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[79] K. Li, R. Garg, M. Cai, 和 I. Reid, "使用深度形状先验和轮廓进行单视图物体形状重建," arXiv:1811.11921, 2019.</p></div><p>[80] W. Zeng, S. Karaoglu, and T. Gevers, "Inferring Point Clouds from Single Monocular Images by Depth Intermediation," arXiv:1812.01402, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[80] W. Zeng, S. Karaoglu, 和 T. Gevers, "通过深度中介从单幅单目图像推断点云," arXiv:1812.01402, 2018.</p></div><p>[81] P. Mandikal and V. B. Radhakrishnan, "Dense 3D Point Cloud Reconstruction Using a Deep Pyramid Network," in IEEE WACV, 2019, pp. 1052-1060.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[81] P. Mandikal 和 V. B. Radhakrishnan, "使用深度金字塔网络进行密集3D点云重建," 在IEEE WACV, 2019, 页1052-1060.</p></div><p>[82] J. Wang, B. Sun, and Y. Lu, "MVPNet: Multi-View Point Regression Networks for 3D Object Reconstruction from A Single Image," arXiv:1811.09410, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[82] J. Wang, B. Sun, 和 Y. Lu, "MVPNet: 多视角点回归网络用于从单幅图像重建3D物体," arXiv:1811.09410, 2018.</p></div><p>[83] M. Tatarchenko, A. Dosovitskiy, and T. Brox, "Multi-view 3D models from single images with a convolutional network," in ECCV, 2016, pp. 322-337.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[83] M. Tatarchenko, A. Dosovitskiy, 和 T. Brox, "使用卷积网络从单幅图像生成多视角3D模型," 在ECCV, 2016, 第322-337页.</p></div><p>[84] P. J. Besl and N. D. McKay, "Method for registration of 3-d shapes," in Sensor Fusion IV: Control Paradigms and Data Structures, vol. 1611. International Society for Optics and Photonics, 1992, pp. 586-607.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[84] P. J. Besl 和 N. D. McKay, "三维形状配准的方法," 在传感器融合IV: 控制范式和数据结构, 第1611卷. 国际光学与光子学学会, 1992, 第586-607页.</p></div><p>[85] Y. Chen and G. Medioni, "Object modelling by registration of multiple range images," Image and vision computing, vol. 10, no. 3, pp. 145-155, 1992.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[85] Y. Chen 和 G. Medioni, "通过多幅深度图像的配准进行物体建模," 图像与视觉计算, 第10卷, 第3期, 第145-155页, 1992.</p></div><p>[86] H. Xie, H. Yao, X. Sun, S. Zhou, S. Zhang, and X. Tong, "Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view Images," IEEE ICCV, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[86] H. Xie, H. Yao, X. Sun, S. Zhou, S. Zhang, 和 X. Tong, "Pix2Vox: 从单幅和多视角图像进行上下文感知的3D重建," IEEE ICCV, 2019.</p></div><p>[87] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, "PointNet: Deep learning on point sets for \(3\mathrm{\;d}\) classification and segmentation," in IEEE CVPR, 2017, pp. 652-660.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[87] C. R. Qi, H. Su, K. Mo, 和 L. J. Guibas, "PointNet: 基于点集的深度学习用于\(3\mathrm{\;d}\)分类和分割," 在IEEE CVPR, 2017, 第652-660页.</p></div><p>[88] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, "PointNet++: Deep hierarchical feature learning on point sets in a metric space," in NIPS, 2017, pp. 5099-5108.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[88] C. R. Qi, L. Yi, H. Su, 和 L. J. Guibas, "PointNet++: 在度量空间中对点集进行深度层次特征学习," 在NIPS, 2017, 第5099-5108页.</p></div><p>[89] M. Kazhdan and H. Hoppe, "Screened poisson surface reconstruction," ACM TOG, vol. 32, no. 3, p. 29, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[89] M. Kazhdan 和 H. Hoppe, "筛选的泊松表面重建," ACM TOG, 第32卷, 第3期, 第29页, 2013.</p></div><p>[90] F. Calakli and G. Taubin, "Ssd: Smooth signed distance surface reconstruction," CGF, vol. 30, no. 7, pp. 1993-2002, 2011.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[90] F. Calakli 和 G. Taubin, "SSD: 平滑有符号距离表面重建," CGF, 第30卷, 第7期, 第1993-2002页, 2011.</p></div><p>[91] J. Wu, C. Zhang, X. Zhang, Z. Zhang, W. T. Freeman, and J. B. Tenenbaum, "Learning shape priors for single-view 3d completion and reconstruction," in ECCV, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[91] J. Wu, C. Zhang, X. Zhang, Z. Zhang, W. T. Freeman, 和 J. B. Tenenbaum, "为单视图3D补全和重建学习形状先验," 在ECCV, 2018.</p></div><p>[92] X. Zhang, Z. Zhang, C. Zhang, J. Tenenbaum, B. Freeman, and J. Wu, "Learning to reconstruct shapes from unseen classes," in NIPS, 2018, pp. 2257-2268.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[92] X. Zhang, Z. Zhang, C. Zhang, J. Tenenbaum, B. Freeman, 和 J. Wu, "学习从未见过的类别重建形状," 在NIPS, 2018, 第2257-2268页.</p></div><p>[93] E. Smith, S. Fujimoto, and D. Meger, "Multi-view silhouette and depth decomposition for high resolution 3D object representation," in NIPS, 2018, pp. 6478-6488.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[93] E. Smith, S. Fujimoto, 和 D. Meger, "用于高分辨率3D物体表示的多视图轮廓和深度分解," 在NIPS, 2018, 第6478-6488页.</p></div><p>[94] M. M. Loper and M. J. Black, "Opendr: An approximate differentiable renderer," in ECCV, 2014, pp. 154-169.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[94] M. M. Loper 和 M. J. Black, "OpenDR: 一种近似可微渲染器," 在ECCV, 2014, 第154-169页.</p></div><p>[95] F. Petersen, A. H. Bermano, O. Deussen, and D. Cohen-Or, "Pix2Vex: Image-to-Geometry Reconstruction using a Smooth Differentiable Renderer," arXiv:1903.11149, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[95] F. Petersen, A. H. Bermano, O. Deussen, 和 D. Cohen-Or, "Pix2Vex: 使用平滑可微渲染器进行图像到几何体的重建," arXiv:1903.11149, 2019.</p></div><p>[96] D. t. Rezende, S. A. Eslami, S. Mohamed, P. Battaglia, M. Jader-berg, and N. Heess, "Unsupervised learning of 3D structure from images," in NIPS, 2016, pp. 4996-5004.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[96] D. t. Rezende, S. A. Eslami, S. Mohamed, P. Battaglia, M. Jader-berg, 和 N. Heess, "从图像中无监督学习3D结构," 在NIPS, 2016, 第4996-5004页.</p></div><p>[97] J. Gwak, C. B. Choy, A. Garg, M. Chandraker, and S. Savarese, "Weakly Supervised Generative Adversarial Networks for 3D Reconstruction," 3D Vision, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[97] J. Gwak, C. B. Choy, A. Garg, M. Chandraker, 和 S. Savarese, "用于3D重建的弱监督生成对抗网络," 3D视觉, 2017.</p></div><p>[98] A. Kendall, M. Grimes, and R. Cipolla, "PoseNet: A convolutional network for real-time 6-DOF camera relocalization," in IEEE ICCV, 2015, pp. 2938-2946.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[98] A. Kendall, M. Grimes, 和 R. Cipolla, "PoseNet: 一种用于实时6自由度相机重定位的卷积网络," 在IEEE ICCV, 2015, pp. 2938-2946.</p></div><p>[99] H. Su, C. R. Qi, Y. Li, and L. J. Guibas, "Render for CNN: Viewpoint estimation in images using CNNs trained with rendered 3D model views," in IEEE ICCV, 2015, pp. 2686-2694.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[99] H. Su, C. R. Qi, Y. Li, 和 L. J. Guibas, "Render for CNN: 使用经过渲染的3D模型视图训练的CNN进行图像中的视点估计," 在IEEE ICCV, 2015, pp. 2686-2694.</p></div><p>[100] D. Novotny, D. Larlus, and A. Vedaldi, "Capturing the geometry of object categories from video supervision," IEEE PAMI, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[100] D. Novotny, D. Larlus, 和 A. Vedaldi, "从视频监督中捕捉物体类别的几何形状," IEEE PAMI, 2018.</p></div><p>[101] J. L. Schonberger and J.-M. Frahm, "Structure-from-motion revisited," in IEEE CVPR, 2016, pp. 4104-4113.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[101] J. L. Schonberger 和 J.-M. Frahm, "运动重建的结构再探," 在IEEE CVPR, 2016, pp. 4104-4113.</p></div><p>[102] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative Adversarial Nets," in NIPS, 2014, pp. 2672-2680.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[102] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, 和 Y. Bengio, "生成对抗网络," 在NIPS, 2014, pp. 2672-2680.</p></div><p>[103] B. Yang, H. Wen, S. Wang, R. Clark, A. Markham, and N. Trigoni, "3D object reconstruction from a single depth view with adversarial learning," in IEEE ICCV Workshops, 2017, pp. 679-688.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[103] B. Yang, H. Wen, S. Wang, R. Clark, A. Markham, 和 N. Trigoni, "基于对抗学习的单深度视图3D物体重建," 在IEEE ICCV Workshops, 2017, pp. 679-688.</p></div><p>[104] M. Arjovsky, S. Chintala, and L. Bottou, "Wasserstein GAN," ICML, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[104] M. Arjovsky, S. Chintala, 和 L. Bottou, "Wasserstein GAN," ICML, 2017.</p></div><p>[105] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, "Improved training of Wasserstein GANs," in NIPS, 2017, pp. 5767-5777.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[105] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, 和 A. C. Courville, "改进Wasserstein GAN的训练," 在NIPS, 2017, pp. 5767-5777.</p></div><p>[106] X. Li, Y. Dong, P. Peers, and X. Tong, "Synthesizing 3d shapes from silhouette image collections using multi-projection generative adversarial networks," in IEEE CVPR, June 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[106] X. Li, Y. Dong, P. Peers, 和 X. Tong, "使用多投影生成对抗网络从轮廓图像集合合成3D形状," 在IEEE CVPR, 2019年6月.</p></div><p>[107] P. Mandikal, N. KL, and R. Venkatesh Babu, "3D-PSRNet: Part segmented 3D point cloud reconstruction from a single image," in \({ECCV},{2018}\) ,pp. 0-0 .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[107] P. Mandikal, N. KL, 和 R. Venkatesh Babu, "3D-PSRNet: 从单幅图像进行部分分割的3D点云重建," 在\({ECCV},{2018}\), pp. 0-0.</p></div><p>[108] E. Dibra, H. Jain, C. Öztireli, R. Ziegler, and M. Gross, "Hs-nets: Estimating human body shape from silhouettes with convolutional neural networks," in 3D Vision, 2016, pp. 108-117.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[108] E. Dibra, H. Jain, C. Öztireli, R. Ziegler, 和 M. Gross, "Hs-nets: 使用卷积神经网络从轮廓估计人体形状," 在3D Vision, 2016, pp. 108-117.</p></div><p>[109] E. Dibra, H. Jain, C. Oztireli, R. Ziegler, and M. Gross, "Human shape from silhouettes using generative hks descriptors and cross-modal neural networks," in IEEE CVPR (CVPR), Honolulu, HI, USA, vol. 5, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[109] E. Dibra, H. Jain, C. Oztireli, R. Ziegler, 和 M. Gross, "使用生成HKS描述符和跨模态神经网络从轮廓获取人体形状," 在IEEE CVPR (CVPR), 美国夏威夷檀香山, vol. 5, 2017.</p></div><p>[110] T. Alldieck, M. Magnor, B. L. Bhatnagar, C. Theobalt, and G. Pons-Moll, "Learning to Reconstruct People in Clothing from a Single RGB Camera," in IEEE CVPR, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[110] T. Alldieck, M. Magnor, B. L. Bhatnagar, C. Theobalt, 和 G. Pons-Moll, "从单个RGB相机重建穿着衣物的人体," 在IEEE CVPR, 2019.</p></div><p>[111] B. L. Bhatnagar, G. Tiwari, C. Theobalt, and G. Pons-Moll, "Multi-Garment Net: Learning to Dress 3D People from Images," in IEEE ICCV, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[111] B. L. Bhatnagar, G. Tiwari, C. Theobalt, 和 G. Pons-Moll, "Multi-Garment Net: 从图像学习为3D人物穿衣," 在IEEE ICCV, 2019.</p></div><p>[112] B. Allen, B. Curless, and Z. Popović, "The space of human body shapes: reconstruction and parameterization from range scans," ACM TOG, vol. 22, no. 3, pp. 587-594, 2003.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[112] B. Allen, B. Curless, 和 Z. Popović, "人类身体形状的空间: 从范围扫描重建和参数化," ACM TOG, vol. 22, no. 3, pp. 587-594, 2003.</p></div><p>[113] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers, and J. Davis, "Scape: shape completion and animation of people," ACM TOG, vol. 24, no. 3, pp. 408-416, 2005.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[113] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers, 和 J. Davis, "Scape: 人体的形状补全和动画," ACM TOG, vol. 24, no. 3, pp. 408-416, 2005.</p></div><p>[114] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black, "SMPL: A skinned multi-person linear model," ACM TOG, vol. 34, no. 6, p. 248, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[114] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, 和 M. J. Black, "SMPL：一种皮肤化的多人体线性模型," ACM TOG, vol. 34, no. 6, p. 248, 2015.</p></div><p>[115] J. Sun, M. Ovsjanikov, and L. Guibas, "A concise and provably informative multi-scale signature based on heat diffusion," CGF, vol. 28, no. 5, pp. 1383-1392, 2009.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[115] J. Sun, M. Ovsjanikov, 和 L. Guibas, "基于热扩散的简洁且可证明信息丰富的多尺度特征," CGF, vol. 28, no. 5, pp. 1383-1392, 2009.</p></div><p>[116] F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero, and M. J. Black, "Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image," in ECCV, 2016, pp. 561-578.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[116] F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero, 和 M. J. Black, "保持简单：从单张图像自动估计3D人体姿态和形状," 在ECCV, 2016, pp. 561-578.</p></div><p>[117] M. Omran, C. Lassner, G. Pons-Moll, P. Gehler, and B. Schiele, "Neural Body Fitting: Unifying Deep Learning and Model Based Human Pose and Shape Estimation," in 3DV, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[117] M. Omran, C. Lassner, G. Pons-Moll, P. Gehler, 和 B. Schiele, "神经体拟合：统一深度学习与基于模型的人体姿态和形状估计," 在3DV, 2018.</p></div><p>[118] T. Alldieck, G. Pons-Moll, C. Theobalt, and M. Magnor, "Tex2Shape: Detailed Full Human Body Geometry from a Single Image," in IEEE ICCV, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[118] T. Alldieck, G. Pons-Moll, C. Theobalt, 和 M. Magnor, "Tex2Shape：从单张图像获取详细的完整人体几何," 在IEEE ICCV, 2019.</p></div><p>[119] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele, "Deepcut: Joint subset partition and labeling for multi person pose estimation," in IEEE CVPR, 2016, pp. 4929-4937.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[119] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, 和 B. Schiele, "Deepcut：用于多人体姿态估计的联合子集划分和标记," 在IEEE CVPR, 2016, pp. 4929-4937.</p></div><p>[120] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik, "End-to-end recovery of human shape and pose," CVPR, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[120] A. Kanazawa, M. J. Black, D. W. Jacobs, 和 J. Malik, "端到端恢复人体形状和姿态," CVPR, 2018.</p></div><p>[121] Z. Huang, T. Li, W. Chen, Y. Zhao, J. Xing, C. LeGendre, L. Luo, C. Ma, and H. Li, "Deep volumetric video from very sparse multiview performance capture," in ECCV, 2018, pp. 336-354.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[121] Z. Huang, T. Li, W. Chen, Y. Zhao, J. Xing, C. LeGendre, L. Luo, C. Ma, 和 H. Li, "从非常稀疏的多视角性能捕捉中生成深度体积视频," 在ECCV, 2018, pp. 336-354.</p></div><p>[122] G. Varol, D. Ceylan, B. Russell, J. Yang, E. Yumer, I. Laptev, and C. Schmid, "BodyNet: Volumetric Inference of 3D Human Body Shapes," in ECCV, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[122] G. Varol, D. Ceylan, B. Russell, J. Yang, E. Yumer, I. Laptev, 和 C. Schmid, "BodyNet：3D人体形状的体积推断," 在ECCV, 2018.</p></div><p>[123] T. F. Cootes, G. J. Edwards, and C. J. Taylor, "Active appearance models," IEEE PAMI, no. 6, pp. 681-685, 2001.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[123] T. F. Cootes, G. J. Edwards, 和 C. J. Taylor, "主动外观模型," IEEE PAMI, no. 6, pp. 681-685, 2001.</p></div><p>[124] T. Gerig, A. Morel-Forster, C. Blumer, B. Egger, M. Luthi, S. Schönborn, and T. Vetter, "Morphable face models-an open framework," in IEEE FG, 2018, pp. 75-82.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[124] T. Gerig, A. Morel-Forster, C. Blumer, B. Egger, M. Luthi, S. Schönborn, 和 T. Vetter, "可变形面部模型——一个开放框架," 在IEEE FG, 2018, pp. 75-82.</p></div><p>[125] O. M. Parkhi, A. Vedaldi, A. Zisserman et al., "Deep face recognition." in BMVC, vol. 1, no. 3, 2015, p. 6.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[125] O. M. Parkhi, A. Vedaldi, A. Zisserman 等, "深度人脸识别." 在BMVC, vol. 1, no. 3, 2015, p. 6.</p></div><p>[126] F. Schroff, D. Kalenichenko, and J. Philbin, "Facenet: A unified embedding for face recognition and clustering," in IEEE CVPR, 2015, pp. 815-823.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[126] F. Schroff, D. Kalenichenko, 和 J. Philbin, "Facenet：用于人脸识别和聚类的统一嵌入," 在IEEE CVPR, 2015, pp. 815-823.</p></div><p>[127] A. T. Tran, T. Hassner, I. Masi, and G. Medioni, "Regressing robust and discriminative \(3\mathrm{D}\) morphable models with a very deep neural network," in IEEE CVPR, 2017, pp. 1493-1502.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[127] A. T. Tran, T. Hassner, I. Masi, 和 G. Medioni, "使用非常深的神经网络回归鲁棒且具有区分性的\(3\mathrm{D}\)可变形模型," 在IEEE CVPR, 2017, pp. 1493-1502.</p></div><p>[128] E. Richardson, M. Sela, and R. Kimmel, "3D face reconstruction by learning from synthetic data," in 3D Vision, 2016, pp. 460-469.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[128] E. Richardson, M. Sela, 和 R. Kimmel, "通过学习合成数据进行3D人脸重建," 在3D Vision, 2016, pp. 460-469.</p></div><p>[129] E. Richardson, M. Sela, R. Or-El, and R. Kimmel, "Learning detailed face reconstruction from a single image," CoRR, vol. abs/1611.05053, 2016. [Online]. Available: <a href="http://arxiv.org/abs/">http://arxiv.org/abs/</a> 1611.05053</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[129] E. Richardson, M. Sela, R. Or-El, 和 R. Kimmel, "从单张图像学习详细的人脸重建," CoRR, vol. abs/1611.05053, 2016. [在线]. 可用： <a href="http://arxiv.org/abs/">http://arxiv.org/abs/</a> 1611.05053</p></div><p>[130] K. Genova, F. Cole, A. Maschinot, A. Sarna, D. Vlasic, and W. T. Freeman, "Unsupervised Training for 3D Morphable Model Regression," in IEEE CVPR, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[130] K. Genova, F. Cole, A. Maschinot, A. Sarna, D. Vlasic, 和 W. T. Freeman, "无监督训练用于3D可变形模型回归," 在IEEE CVPR, 2018.</p></div><p>[131] A. Tewari, M. Zollhofer, H. Kim, P. Garrido, F. Bernard, P. Perez, and C. Theobalt, "Mofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction," in IEEE CVPR, 2017, pp. 1274-1283.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[131] A. Tewari, M. Zollhofer, H. Kim, P. Garrido, F. Bernard, P. Perez, 和 C. Theobalt, "Mofa: 基于模型的深度卷积人脸自编码器用于无监督单目重建," 在IEEE CVPR, 2017, 第1274-1283页.</p></div><p>[132] A. S. Jackson, A. Bulat, V. Argyriou, and G. Tzimiropoulos, "Large pose 3d face reconstruction from a single image via direct volumetric cnn regression," in IEEE CVPR, 2017, pp. 1031-1039.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[132] A. S. Jackson, A. Bulat, V. Argyriou, 和 G. Tzimiropoulos, "通过直接体积卷积神经网络回归从单张图像重建大姿态3D人脸," 在IEEE CVPR, 2017, 第1031-1039页.</p></div><p>[133] M. Sela, E. Richardson, and R. Kimmel, "Unrestricted facial geometry reconstruction using image-to-image translation," in IEEE CVPR, 2017, pp. 1576-1585.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[133] M. Sela, E. Richardson, 和 R. Kimmel, "使用图像到图像转换进行无限制的面部几何重建," 在IEEE CVPR, 2017, 第1576-1585页.</p></div><p>[134] M. Feng, S. Zulqarnain Gilani, Y. Wang, and A. Mian, "3d face reconstruction from light field images: A model-free approach," in ECCV, 2018, pp. 501-518.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[134] M. Feng, S. Zulqarnain Gilani, Y. Wang, 和 A. Mian, "从光场图像重建3D人脸: 一种无模型的方法," 在ECCV, 2018, 第501-518页.</p></div><p>[135] J. D'Errico, "Surface fitting using gridfit," MATLAB central file exchange, vol. 643, 2005.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[135] J. D'Errico, "使用gridfit进行表面拟合," MATLAB中央文件交换, 第643卷, 2005.</p></div><p>[136] H. Izadinia, Q. Shan, and S. M. Seitz, "Im2cad," in IEEE CVPR, 2017, pp. 5134-5143.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[136] H. Izadinia, Q. Shan, 和 S. M. Seitz, "Im2cad," 在IEEE CVPR, 2017, 第5134-5143页.</p></div><p>[137] S. Ren, K. He, R. Girshick, and J. Sun, "Faster R-CNN: Towards real-time object detection with region proposal networks," in NIPS, 2015, pp. 91-99.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[137] S. Ren, K. He, R. Girshick, 和 J. Sun, "Faster R-CNN: 朝着实时物体检测与区域提议网络的方向," 在NIPS, 2015, 第91-99页.</p></div><p>[138] S. Tulsiani, S. Gupta, D. F. Fouhey, A. A. Efros, and J. Malik, "Factoring shape, pose, and layout from the 2D image of a 3D scene," in IEEE CVPR, 2018, pp. 302-310.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[138] S. Tulsiani, S. Gupta, D. F. Fouhey, A. A. Efros, 和 J. Malik, "从3D场景的2D图像中分解形状、姿态和布局," 在IEEE CVPR, 2018, 第302-310页.</p></div><p>[139] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su et al., "Shapenet: An information-rich 3D model repository," arXiv:1512.03012, 2015.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[139] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su 等, "Shapenet: 一个信息丰富的3D模型库," arXiv:1512.03012, 2015.</p></div><p>[140] J. J. Lim, H. Pirsiavash, and A. Torralba, "Parsing ikea objects: Fine pose estimation," in IEEE ICCV, 2013, pp. 2992-2999.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[140] J. J. Lim, H. Pirsiavash, 和 A. Torralba, "解析宜家物体: 精细姿态估计," 在IEEE ICCV, 2013, 第2992-2999页.</p></div><p>[141] Y. Xiang, R. Mottaghi, and S. Savarese, "Beyond pascal: A benchmark for 3D object detection in the wild," in IEEE WACV, 2014, pp. 75-82.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[141] Y. Xiang, R. Mottaghi, 和 S. Savarese, "超越Pascal: 一个野外3D物体检测基准," 在IEEE WACV, 2014, 第75-82页.</p></div><p>[142] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mottaghi, L. Guibas, and S. Savarese, "ObjectNet3D: A large scale database for 3D object recognition," in ECCV, 2016, pp. 160-176.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[142] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mottaghi, L. Guibas, 和 S. Savarese, "ObjectNet3D: 一个大规模3D物体识别数据库," 在ECCV, 2016, 第160-176页.</p></div><p>[143] A. Geiger, P. Lenz, and R. Urtasun, "Are we ready for autonomous driving? the kitti vision benchmark suite," in IEEE CVPR, 2012, pp. 3354-3361.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[143] A. Geiger, P. Lenz, 和 R. Urtasun, "我们准备好自动驾驶了吗? KITTI视觉基准套件," 在IEEE CVPR, 2012, 第3354-3361页.</p></div><p>[144] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner, "Scannet: Richly-annotated 3d reconstructions of indoor scenes," in IEEE CVPR, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[144] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, 和 M. Nießner, "ScanNet: 丰富注释的室内场景3D重建," 在IEEE CVPR, 2017.</p></div><p>[145] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, "3D object representations for fine-grained categorization," in IEEE CVPR Workshops, 2013, pp. 554-561.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[145] J. Krause, M. Stark, J. Deng, 和 L. Fei-Fei, "用于细粒度分类的3D物体表示," 在IEEE CVPR研讨会, 2013, 第554-561页.</p></div><p>[146] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona, "Caltech-UCSD Birds 200," no. Technical Report: CNS-TR-2010-001.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[146] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, 和 P. Perona, "加州理工学院-加州大学圣地亚哥分校鸟类200," no. 技术报告: CNS-TR-2010-001.</p></div><p>[147] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, "The Caltech-UCSD Birds-200-2011 Dataset," no. Technical Report: CNS-TR-2011-001, 2011.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[147] C. Wah, S. Branson, P. Welinder, P. Perona, 和 S. Belongie, "加州理工学院-加州大学圣地亚哥分校鸟类-200-2011 数据集," no. 技术报告: CNS-TR-2011-001, 2011.</p></div><p>[148] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser, "Semantic scene completion from a single depth image," in IEEE CVPR, 2017, pp. 1746-1754.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[148] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, 和 T. Funkhouser, "基于单一深度图像的语义场景补全," 在 IEEE CVPR, 2017, pp. 1746-1754.</p></div><p>[149] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and S. Savarese, "3d semantic parsing of large-scale indoor spaces," in IEEE CVPR, 2016.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[149] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, 和 S. Savarese, "大规模室内空间的3D语义解析," 在 IEEE CVPR, 2016.</p></div><p>[150] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, "Joint 2d-3d-semantic data for indoor scene understanding," arXiv preprint arXiv:1702.01105, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[150] I. Armeni, S. Sax, A. R. Zamir, 和 S. Savarese, "用于室内场景理解的联合2D-3D语义数据," arXiv 预印本 arXiv:1702.01105, 2017.</p></div><p>[151] H. Riemenschneider, A. Bódis-Szomorú, J. Weissenberg, and L. Van Gool, "Learning where to classify in multi-view semantic segmentation," in ECCV, 2014, pp. 516-532.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[151] H. Riemenschneider, A. Bódis-Szomorú, J. Weissenberg, 和 L. Van Gool, "在多视角语义分割中学习分类位置," 在 ECCV, 2014, pp. 516-532.</p></div><p>[152] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, "Indoor segmentation and support inference from rgbd images," ECCV, pp. 746-760, 2012.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[152] N. Silberman, D. Hoiem, P. Kohli, 和 R. Fergus, "基于RGBD图像的室内分割和支撑推断," ECCV, pp. 746-760, 2012.</p></div><p>[153] H. Laga, "A survey on deep learning architectures for image-based depth reconstruction," Under Review (paper available on ArXiv), 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[153] H. Laga, "关于基于图像的深度重建的深度学习架构的调查," 待审稿（论文可在ArXiv上获取），2019.</p></div><p>[154] I. R. Ward, H. Laga, and M. Bennamoun, "RGB-D image-based Object Detection: from Traditional Methods to Deep Learning Techniques," arXiv preprint arXiv:1907.09236, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[154] I. R. Ward, H. Laga, 和 M. Bennamoun, "基于RGB-D图像的物体检测：从传统方法到深度学习技术," arXiv 预印本 arXiv:1907.09236, 2019.</p></div><p>[155] H. Kato and T. Harada, "Learning view priors for single-view 3d reconstruction," IEEE CVPR, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[155] H. Kato 和 T. Harada, "为单视图3D重建学习视图先验," IEEE CVPR, 2019.</p></div><p>[156] J. Varley, C. DeChant, A. Richardson, J. Ruales, and P. Allen, "Shape completion enabled robotic grasping," in IEEE/RSJ IROS, 2017, pp. 2442-2447.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[156] J. Varley, C. DeChant, A. Richardson, J. Ruales, 和 P. Allen, "形状补全使机器人抓取成为可能," 在 IEEE/RSJ IROS, 2017, pp. 2442-2447.</p></div><p>[157] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger, "Occupancy Networks: Learning 3D Reconstruction in Function Space," IEEE CVPR, 2019.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[157] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, 和 A. Geiger, "占用网络：在函数空间中学习3D重建," IEEE CVPR, 2019.</p></div><p>[158] S. Kurtek, A. Srivastava, E. Klassen, and H. Laga, "Landmark-guided elastic shape analysis of spherically-parameterized surfaces," vol. 32, no. 2pt4, pp. 429-438, 2013.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[158] S. Kurtek, A. Srivastava, E. Klassen, 和 H. Laga, "基于地标的弹性形状分析的球面参数化表面," vol. 32, no. 2pt4, pp. 429-438, 2013.</p></div><p>[159] H. Laga, "A survey on nonrigid 3d shape analysis," in Academic Press Library in Signal Processing, Volume 6. Elsevier, 2018, pp. 261-304.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[159] H. Laga, "关于非刚性3D形状分析的调查," 在信号处理的学术出版社图书馆, 第6卷. Elsevier, 2018, pp. 261-304.</p></div><p>[160] J. Zelek and N. Lunscher, "Point cloud completion of foot shape from a single depth map for fit matching using deep learning view synthesis," in IEEE ICCV Workshop, 2017, pp. 2300-2305.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[160] J. Zelek 和 N. Lunscher, "基于单一深度图的足部形状点云补全以进行适配匹配，使用深度学习视图合成," 在 IEEE ICCV 研讨会, 2017, pp. 2300-2305.</p></div><p>[161] O. Litany, A. Bronstein, M. Bronstein, and A. Makadia, "Deformable shape completion with graph convolutional autoen-coders," arXiv:1712.00268, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[161] O. Litany, A. Bronstein, M. Bronstein, 和 A. Makadia, "使用图卷积自编码器的可变形形状补全," arXiv:1712.00268, 2017.</p></div><p>[162] Z. Lun, M. Gadelha, E. Kalogerakis, S. Maji, and R. Wang, "3D Shape Reconstruction from Sketches via Multi-view Convolutional Networks," 3D Vision, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[162] Z. Lun, M. Gadelha, E. Kalogerakis, S. Maji, 和 R. Wang, "通过多视角卷积网络从草图重建3D形状," 3D视觉, 2017.</p></div><p>[163] J. Delanoy, M. Aubry, P. Isola, A. A. Efros, and A. Bousseau, "3D Sketching using Multi-View Deep Volumetric Prediction," ACM ToG, vol. 1, no. 1, p. 21, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[163] J. Delanoy, M. Aubry, P. Isola, A. A. Efros, 和 A. Bousseau, "使用多视角深度体积预测进行3D草图绘制," ACM ToG, 第1卷，第1期，第21页, 2018.</p></div><p>[164] C. Li, M. Z. Zia, Q.-H. Tran, X. Yu, G. D. Hager, and M. Chan-draker, "Deep supervision with shape concepts for occlusion-aware 3D object parsing," in IEEE CVPR, vol. 1, 2017.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[164] C. Li, M. Z. Zia, Q.-H. Tran, X. Yu, G. D. Hager, 和 M. Chan-draker, "基于形状概念的深度监督用于考虑遮挡的3D物体解析," 在IEEE CVPR, 第1卷, 2017.</p></div><p>[165] C. Niu, J. Li, and K. Xu, "Im2Struct: Recovering 3D Shape Structure from a Single RGB Image," IEEE CVPR, vol. 4096, p. 80, 2018.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[165] C. Niu, J. Li, 和 K. Xu, "Im2Struct: 从单个RGB图像恢复3D形状结构," IEEE CVPR, 第4096卷，第80页, 2018.</p></div>
      </body>
    </html>
  