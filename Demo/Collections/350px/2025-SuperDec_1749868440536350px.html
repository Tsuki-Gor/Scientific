
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Doc2x-Export-Html-Result</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px 350px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <style>

</style>
      </head>
      <body style="padding: 40px;">
        <h1>SUPERDEC: 3D Scene Decomposition with Superquadric Primitives</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>SUPERDEC: 使用超二次体原语进行3D场景分解</h1></div><p>Elisabetta Fedele \({}^{1,2}\) Boyang \({\mathrm{{Sun}}}^{1}\) Leonidas Guibas \({}^{2}\) Marc Pollefeys \({}^{1,3}\) Francis Engelmann \({}^{2}\)</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>埃莉萨贝塔·费德尔 \({}^{1,2}\) 博扬 \({\mathrm{{Sun}}}^{1}\) 莱奥尼达斯·吉巴斯 \({}^{2}\) 马克·波勒费斯 \({}^{1,3}\) 弗朗西斯·恩格尔曼 \({}^{2}\)</p></div><p>\({}^{1}\) ETH Zurich \({}^{2}\) Stanford University \({}^{3}\) Microsoft</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>\({}^{1}\) 苏黎世联邦理工学院 \({}^{2}\) 斯坦福大学 \({}^{3}\) 微软</p></div><h2>Abstract</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>摘要</h2></div><p>We present SUPERDEC, an approach for creating compact 3D scene representations via decomposition into su-perquadric primitives. While most recent works leverage geometric primitives to obtain photorealistic \({3D}\) scene representations, we propose to leverage them to obtain a compact yet expressive representation. We propose to solve the problem locally on individual objects and leverage the capabilities of instance segmentation methods to scale our solution to full 3D scenes. In doing that, we design a new architecture which efficiently decompose point clouds of arbitrary objects in a compact set of superquadrics. We train our architecture on ShapeNet and we prove its generalization capabilities on object instances extracted from the ScanNet++ dataset as well as on full Replica scenes. Finally, we show how a compact representation based on superquadrics can be useful for a diverse range of downstream applications, including robotic tasks and controllable visual content generation and editing. Our project page is: <a href="https://super-dec.github.io">https://super-dec.github.io</a>.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们提出了SUPERDEC，这是一种通过分解为超二次体原语来创建紧凑的3D场景表示的方法。尽管最近的大多数工作利用几何原语来获得逼真的\({3D}\)场景表示，但我们建议利用它们来获得紧凑而富有表现力的表示。我们建议在单个对象上局部解决问题，并利用实例分割方法的能力将我们的解决方案扩展到完整的3D场景。在此过程中，我们设计了一种新架构，可以有效地将任意对象的点云分解为一组紧凑的超二次体。我们在ShapeNet上训练了我们的架构，并证明了其在从ScanNet++数据集中提取的对象实例以及完整Replica场景上的泛化能力。最后，我们展示了基于超二次体的紧凑表示如何对各种下游应用（包括机器人任务和可控视觉内容生成与编辑）有用。我们的项目页面是：<a href="https://super-dec.github.io%E3%80%82">https://super-dec.github.io。</a></p></div><h2>1. Introduction</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>1. 引言</h2></div><p>3D scene representations are essential for computer vision and robotics, serving as the foundation for tasks such as 3D scene understanding [33, 47], scene generation [41, 42], functional reasoning \(\left\lbrack  {{19},{57}}\right\rbrack\) ,and scene interaction \(\lbrack 9,{13}\) , 21, 38]. Recent work [18] employs 3D Gaussians as geometric primitives to achieve high-quality photorealistic reconstructions. However, these representations are typically memory-intensive. In contrast, we aim for a lightweight yet geometrically accurate 3D scene representation by decomposing the input point cloud into a compact set of explicit primitives, namely superquadrics (Fig. 1).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D场景表示对于计算机视觉和机器人技术至关重要，是3D场景理解[33, 47]、场景生成[41, 42]、功能推理\(\left\lbrack  {{19},{57}}\right\rbrack\)和场景交互\(\lbrack 9,{13}\)等任务的基础。最近的工作[18]采用3D高斯作为几何原语，以实现高质量的逼真重建。然而，这些表示通常占用大量内存。相比之下，我们的目标是通过将输入点云分解为一组紧凑的显式原语，即超二次体，来实现轻量且几何上准确的3D场景表示（图1）。</p></div><p>Representations for 3D scenes include well established formats such as point clouds, meshes, signed distance functions or voxel grids - each offering different tradeoff between geometric details, computational cost, resolution, performance, interpretability, and editability. Recently, multi-view approaches like Neural Radiance Fields (NeRF) [29] and Gaussian Splatting (GS) [18] have gained popularity as 3D scene representations. These methods optimize photometric losses to ensure that their implicit (NeRF) or explicit (GS) underlying representations align with observed images. While these representations excel in photorealism, none of them provides explicit control over compactness, often resulting in large, non-modular scene encodings - which are not suitable for tasks requiring explicit spatial reasoning.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>3D场景的表示包括点云、网格、带符号距离函数或体素网格等成熟格式——每种格式在几何细节、计算成本、分辨率、性能、可解释性和可编辑性之间提供不同的权衡。最近，多视角方法如神经辐射场（NeRF）[29]和高斯溅射（GS）[18]作为3D场景表示越来越受欢迎。这些方法优化光度损失，以确保其隐式（NeRF）或显式（GS）基础表示与观察到的图像对齐。尽管这些表示在逼真度上表现出色，但它们都未能提供对紧凑性的显式控制，通常导致大型、非模块化的场景编码——这不适合需要显式空间推理的任务。</p></div><!-- Media --><img src="https://cdn.noedgeai.com/bo_d16410bef24c73d1lfag_0.jpg?x=926&#x26;y=575&#x26;w=702&#x26;h=532&#x26;r=0"><p>Figure 1. 3D Scene Decomposition with Superquadrics. Given a 3D point cloud of an arbitrary scene, SUPERDEC decomposes all scene objects into a compact set of superquadric primitives.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图1. 使用超二次体进行3D场景分解。给定任意场景的3D点云，SUPERDEC将所有场景对象分解为一组紧凑的超二次体原语。</p></div><!-- Media --><p>While optimizing compactness on a scene level remains a challenging task, many approaches have shown that geometric primitives as cuboids \(\left\lbrack  {{49},{55}}\right\rbrack\) or superquadrics \(\lbrack 1\) , 31, 32] enable compact and interpretable decompositions of individual objects. Overall, these methods are either learning-based \(\left\lbrack  {{32},{55}}\right\rbrack\) ,prioritizing speed at the expense of accuracy,or optimization-based \(\left\lbrack  {1,{24},{31}}\right\rbrack\) ,achieving better accuracy but incurring higher computation times. While these methods can be accurate for specific object shapes, both approaches struggle to generalize across datasets containing diverse shapes; the former requires category-specific training, while the latter relies on hand-crafted heuristics that limit scalability in unconstrained settings.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>尽管在场景级别优化紧凑性仍然是一项具有挑战性的任务，但许多方法已经表明，几何原语如立方体\(\left\lbrack  {{49},{55}}\right\rbrack\)或超二次体\(\lbrack 1\)能够实现个别对象的紧凑和可解释的分解。总体而言，这些方法要么是基于学习的\(\left\lbrack  {{32},{55}}\right\rbrack\)，在牺牲准确性的情况下优先考虑速度，要么是基于优化的\(\left\lbrack  {1,{24},{31}}\right\rbrack\)，在实现更好准确性的同时导致更高的计算时间。虽然这些方法对于特定对象形状可能是准确的，但两种方法都难以在包含多样形状的数据集上进行泛化；前者需要特定类别的训练，而后者依赖于手工制作的启发式方法，这限制了在不受约束环境中的可扩展性。</p></div><p>Motivated by the abstraction capabilities of geometric primitives for individual objects categories, we propose to represent complex 3D scenes as a compact set of superquadrics. To this end, we learn general object-level shape priors to optimize compactness and leverage an off-the-shelf 3D instance segmentation method, Mask3D [40], to scale our approach to full 3D scenes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>受到几何原语对个别对象类别的抽象能力的启发，我们建议将复杂的3D场景表示为一组紧凑的超二次体。为此，我们学习一般的对象级形状先验，以优化紧凑性，并利用现成的3D实例分割方法Mask3D[40]将我们的方法扩展到完整的3D场景。</p></div><p>We choose superquadrics as building block as they offer more accurate shape modeling than cuboids while incurring minimal additional parameter costs ( 9 vs. 11, including 6-DoF pose parameters). To obtain a model which is able to generalize across different shapes, we draw inspiration from the literature of supervised segmentation and we look at the problem from the perspective of unsupervised geometric-based segmentation, using local point-based features to iteratively refine the predicted geometric primitives. We train our model on ShapeNet [4] and we evaluate it on three challenging and diverse 3D datasets: ShapeNet [4], ScanNet++ [56], and Replica [44]. On the 3D object dataset ShapeNet, our approach achieves a L2 error 6 times smaller compared to prior state-of-the-art work [32] while requiring only half the numbers of primitives. On ScanNet++ and Replica we demonstrate that our approach works well in the real-world scene-level setting, even if trained only on ShapeNet [4]. Finally, we demonstrate the practical use of our method as scene representation for robotic tasks including path planning and object grasping, as well as an editable 3D scene representation for controllable image generation. In summary, our contributions are the following:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们选择超二次体作为构建块，因为它们提供比立方体更准确的形状建模，同时额外的参数成本最小（9对11，包括6自由度姿态参数）。为了获得能够在不同形状之间进行泛化的模型，我们从监督分割的文献中汲取灵感，并从无监督几何基础分割的角度看待问题，使用基于局部点的特征迭代细化预测的几何原语。我们在ShapeNet [4]上训练我们的模型，并在三个具有挑战性和多样性的3D数据集上进行评估：ShapeNet [4]、ScanNet++ [56]和Replica [44]。在3D对象数据集ShapeNet上，我们的方法实现了比之前的最先进工作[32]小6倍的L2误差，同时只需要一半的原语数量。在ScanNet++和Replica上，我们证明了我们的方法在真实世界场景级设置中表现良好，即使仅在ShapeNet [4]上训练。最后，我们展示了我们的方法作为场景表示在机器人任务中的实际应用，包括路径规划和物体抓取，以及可编辑的3D场景表示用于可控图像生成。总之，我们的贡献如下：</p></div><ol>
<li>We introduce SUPERDEC, a novel method for decomposing 3D scenes using superquadric primitives.</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol>
<li>我们引入了SUPERDEC，一种使用超二次体原语分解3D场景的新方法。</li>
</ol></div><ol start="2">
<li>SUPERDEC achieves state-of-the-art object decomposition scores on ShapeNet trained jointly on multiple classes. 3) We demonstrate the effectiveness of \(3\mathrm{D}\) superquadric scene representations for robotic tasks and controllable generative content creation.</li>
</ol><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><ol start="2">
<li>SUPERDEC在ShapeNet上联合训练多个类别时，达到了最先进的对象分解得分。3) 我们展示了\(3\mathrm{D}\)超二次体场景表示在机器人任务和可控生成内容创作中的有效性。</li>
</ol></div><h2>2. Related Work</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>2. 相关工作</h2></div><p>Learning-based methods have shown that neural networks, when equipped with suitable reconstruction losses, can directly predict geometric primitive parameters to decompose point clouds into a minimal set of primitives for specific object categories. Tulsiani [49] introduced a CNN-based method for cuboid decomposition, which was later extended to more expressive primitives such as su-perquadrics by Paschalidou et al. [32]. CSA [55] further enhanced interpretability by employing a stronger point encoder and jointly predicting cuboid parameters and part segmentations. However, these methods remain constrained by their reliance on category-specific training. We attribute this limitation to their model design, which encodes only global shape features, sufficient for intra-category generalization but ineffective for decomposing out-of-category objects.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于学习的方法表明，当神经网络配备适当的重建损失时，可以直接预测几何原语参数，将点云分解为特定对象类别的最小原语集。Tulsiani [49] 引入了一种基于CNN的立方体分解方法，后来由Paschalidou等人[32]扩展到更具表现力的原语，如超二次体。CSA [55]通过采用更强的点编码器并联合预测立方体参数和部分分割，进一步增强了可解释性。然而，这些方法仍然受到依赖于类别特定训练的限制。我们将这一限制归因于它们的模型设计，该设计仅编码全局形状特征，足以进行类别内泛化，但对分解类别外对象无效。</p></div><p>Optimization-based methods largely originate from the literature on superquadric fitting. EMS [24] revisited this line of work by introducing a probabilistic formulation that enables the decomposition of arbitrary objects into multiple superquadrics. Given an input point cloud, the method first fits a superquadric to the main structure and identifies unfitted outlier clusters, which are then recursively processed in a hierarchical fashion up to a predefined depth level. However, as noted in their paper and confirmed by our experiments, this approach implicitly assumes that objects exhibit a hierarchical geometric structure, limiting its applicability to many real-world objects such as tables and chairs. Other methods, such as Marching Primitives [25], require Signed Distance Functions (SDFs) as input, which are generally unavailable in real-world scenes. More fundamentally, since these approaches optimize from scratch for each object, they cannot leverage generalizable point features or learned shape priors, both of which are critical for abstraction and robustness under partial observations, a common challenge in practical \(3\mathrm{D}\) capture scenarios.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>基于优化的方法主要源于超二次体拟合的文献。EMS [24]通过引入一种概率性公式重新审视了这一工作，能够将任意对象分解为多个超二次体。给定输入点云，该方法首先拟合一个超二次体到主要结构，并识别未拟合的离群簇，然后以递归方式在预定义的深度级别上进行处理。然而，正如他们的论文中所指出的并通过我们的实验确认的，这种方法隐含地假设对象表现出层次几何结构，限制了其在许多现实世界对象（如桌子和椅子）中的适用性。其他方法，如Marching Primitives [25]，需要有符号距离函数（SDF）作为输入，而这些在现实世界场景中通常不可用。更根本的是，由于这些方法为每个对象从头优化，因此无法利用可泛化的点特征或学习的形状先验，而这两者对于抽象和在部分观察下的鲁棒性至关重要，这是实际\(3\mathrm{D}\)捕获场景中的一个常见挑战。</p></div><p>Scene-level decomposition. With the emergence of 3DGS [18], an increasing number of works have explored representing \(3\mathrm{D}\) scenes using various geometric primitives \(\left\lbrack  {{14},{15}}\right\rbrack\) . While heuristics can control the number of Gaussians, achieving truly compact representations remains challenging. DBW [31] addresses this by fitting a small set of meshed superquadrics to \(3\mathrm{D}\) scenes,building on the principles of 3DGS. Given a set of scene images, it performs test-time optimization with a photoconsistency loss and renders the primitives using a differentiable ras-terizer such as SoftRas [22]. To model the environment, DBW adds a meshed ground plane and a meshed icosphere for the background. However, it is restricted to scenes with fewer than 10 primitives and requires objects to be aligned to a ground plane. Furthermore, the optimization is computationally expensive, taking around three hours even for simple DTU [17] scenes. In contrast, our method differs significantly in terms of input requirements, generality of application, and computational efficiency.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>场景级分解。随着3DGS [18]的出现，越来越多的工作探索使用各种几何原语\(\left\lbrack  {{14},{15}}\right\rbrack\)表示\(3\mathrm{D}\)场景。虽然启发式方法可以控制高斯的数量，但实现真正紧凑的表示仍然具有挑战性。DBW [31]通过拟合一小组网格超二次体到\(3\mathrm{D}\)场景来解决这个问题，基于3DGS的原则。给定一组场景图像，它使用光度一致性损失进行测试时优化，并使用可微分光栅化器（如SoftRas [22]）渲染原语。为了建模环境，DBW添加了一个网格地面平面和一个网格二十面体作为背景。然而，它仅限于原语少于10的场景，并要求对象与地面平面对齐。此外，优化计算开销大，即使对于简单的DTU [17]场景也需要大约三个小时。相比之下，我们的方法在输入要求、应用的通用性和计算效率方面有显著不同。</p></div><p>Superquadrics are a parametric family of shapes introduced by Barr et al. [2] in 1981 and have since been widely adopted in both computer vision and graphics [7, 34, 43]. Their popularity stems from their ability to represent a diverse range of shapes with a highly compact parameterization. A superquadric in its canonical pose is defined by just five parameters: \(\left( {{s}_{x},{s}_{y},{s}_{z}}\right)\) for the scales along the three principal semi-axes and \(\left( {{\epsilon }_{1},{\epsilon }_{2}}\right)\) for the shape-defining exponents. Given those parameters, their surface is described by the implicit equation:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>超二次体是一种由Barr等人于1981年引入的参数化形状家族，自那时以来在计算机视觉和图形学中得到了广泛应用。它们的流行源于其能够以高度紧凑的参数化表示多种形状。在其标准姿态下，超二次体仅由五个参数定义：\(\left( {{s}_{x},{s}_{y},{s}_{z}}\right)\)表示沿三个主半轴的缩放，\(\left( {{\epsilon }_{1},{\epsilon }_{2}}\right)\)表示形状定义指数。给定这些参数，其表面由隐式方程描述：</p></div><p></p>\[f\left( \mathbf{x}\right)  = {\left( {\left( \frac{x}{{s}_{x}}\right) }^{\frac{2}{{\epsilon }_{2}}} + {\left( \frac{y}{{s}_{y}}\right) }^{\frac{2}{{\epsilon }_{2}}}\right) }^{\frac{{\epsilon }_{2}}{{\epsilon }_{1}}} + {\left( \frac{z}{{s}_{z}}\right) }^{\frac{2}{{\epsilon }_{1}}} = 1 \tag{1}\]<p></p><p>Extending this representation to a global coordinate system requires 6 additional parameters ( 3 for translation and 3 for rotation), resulting in a total of 11 parameters. Another key property of superquadrics is the ability to compute the radial distance from any point in \(3\mathrm{D}\) space to the su-perquadric surface, i.e., the distance between a point and the superquadric's surface along the line connecting that point to the center of the superquadric. Specifically, given a point \(\mathbf{x} \in  {\mathbb{R}}^{3}\) ,its radial distance to the surface of a canonically oriented superquadric is defined as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>将这种表示扩展到全局坐标系统需要6个额外的参数（3个用于平移，3个用于旋转），总共需要11个参数。超二次体的另一个关键特性是能够计算\(3\mathrm{D}\)空间中任意点到超二次体表面的径向距离，即点与超二次体表面之间沿连接该点与超二次体中心的直线的距离。具体而言，给定一个点\(\mathbf{x} \in  {\mathbb{R}}^{3}\)，其到标准定向超二次体表面的径向距离定义为：</p></div><p></p>\[{d}_{r} = \left| \mathbf{x}\right|  \cdot  \left| {1 - f{\left( \mathbf{x}\right) }^{-{\epsilon }_{1}/2}}\right| , \tag{2}\]<p></p><p>where \(f\left( \mathbf{x}\right)\) is given in Eq. 1. We refer the reader to [51] for the derivation and to [16] for a more comprehensive overview on superquadrics.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\(f\left( \mathbf{x}\right)\)在方程1中给出。我们建议读者参考[51]以获取推导，并参考[16]以获得关于超二次体的更全面概述。</p></div><h2>3. Method</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>3. 方法</h2></div><p>Our ultimate goal is a 3D scene decomposition using su-perquadric primitives. To this end, we primarily focus on single-object decomposition and then show how our method, combined with 3D instance segmentation [40], can be applied to full 3D scenes. We detail the single-object approach in Sec. 3.1 and its extension to full scenes in Sec. 3.2.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的最终目标是使用超二次体原语进行3D场景分解。为此，我们主要关注单对象分解，然后展示我们的方法如何与3D实例分割结合应用于完整的3D场景。我们在第3.1节详细介绍单对象方法，并在第3.2节扩展到完整场景。</p></div><h3>3.1. Single Object Decomposition</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.1. 单对象分解</h3></div><p>Fig. 2 illustrates our model for single-object decomposition. It consists of two main components: a self-supervised feed-forward neural network that jointly predicts superquadric parameters and a segmentation matrix associating points to superquadrics, followed by a lightweight Levenberg-Marquardt (LM) optimization [20, 28].</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2展示了我们的单对象分解模型。它由两个主要组件组成：一个自监督的前馈神经网络，联合预测超二次体参数，以及一个将点与超二次体关联的分割矩阵，随后是一个轻量级的Levenberg-Marquardt（LM）优化。</p></div><h4>3.1.1. Feed-forward Neural Network</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.1.1. 前馈神经网络</h4></div><p>Our deep learning model draws inspiration from recent fully-supervised Transformer-based [52] segmentation models \(\left\lbrack  {5,6,{40}}\right\rbrack\) . These models iteratively decode a sequence of queries, each representing a segmentation mask, by cross-attending to input pixels or points. In our case, the queries represent superquadrics. Next, we show how such an architecture can be adapted to unsupervisedly segment superquadrics, instead of supervisedly segment objects.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的深度学习模型受到最近完全监督的基于Transformer的分割模型的启发。这些模型通过交叉关注输入像素或点，迭代解码一系列查询，每个查询代表一个分割掩码。在我们的案例中，查询代表超二次体。接下来，我们展示如何将这种架构调整为无监督地分割超二次体，而不是监督地分割对象。</p></div><p>Model Details. Given an input point cloud \(\mathcal{P} \in  {\mathbb{R}}^{N \times  3}\) , where each of the \(N\) points has a 3D coordinate,we first extract rich point features \({\mathcal{F}}_{PC} \in  {\mathbb{R}}^{N \times  H}\) using the PVCNN [26] point encoder. At the same time, we initialize the superquadrics features \({\mathcal{F}}_{SQ} \in  {\mathbb{R}}^{P \times  H}\) with sinusoidal positional encodings. We feed these features in a Transformer decoder [52] which leverages self-attention, cross-attention and feed-forward layers to refine them.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>模型细节。给定一个输入点云\(\mathcal{P} \in  {\mathbb{R}}^{N \times  3}\)，其中每个\(N\)点都有一个3D坐标，我们首先使用PVCNN点编码器提取丰富的点特征\({\mathcal{F}}_{PC} \in  {\mathbb{R}}^{N \times  H}\)。同时，我们用正弦位置编码初始化超二次体特征\({\mathcal{F}}_{SQ} \in  {\mathbb{R}}^{P \times  H}\)。我们将这些特征输入到一个Transformer解码器中，该解码器利用自注意力、交叉注意力和前馈层来细化它们。</p></div><p>Once refined,the superquadric features \({\mathcal{F}}_{SQ}\) and the point features \({\mathcal{F}}_{PC}\) are fed into two prediction heads: The segmentation head takes as input \({\mathcal{F}}_{SQ}\) and \({\mathcal{F}}_{PC}\) and predicts a soft assignment matrix \(M \in  {\mathbb{R}}^{N \times  P}\) associating points to superquadrics and whose elements are defined as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>一旦细化，超二次体特征\({\mathcal{F}}_{SQ}\)和点特征\({\mathcal{F}}_{PC}\)被输入到两个预测头中：分割头以\({\mathcal{F}}_{SQ}\)和\({\mathcal{F}}_{PC}\)为输入，预测一个软分配矩阵\(M \in  {\mathbb{R}}^{N \times  P}\)，将点与超二次体关联，其元素定义为：</p></div><p></p>\[{m}_{ij} = \sigma \left( {\phi \left( {\mathcal{F}}_{PC}\right)  \cdot  {\mathcal{F}}_{SQ}}\right) , \tag{3}\]<p></p><p>where \(\phi \left( {\mathcal{F}}_{PC}\right)  \in  {\mathbb{R}}^{N \times  H}\) is a learned projection of the point features to match the dimensionality of the superquadric features,and \(\sigma\) is the softmax function. The second head, the superquadric head,takes the superquadric features \({\mathcal{F}}_{SQ}\) as input and predicts 12 parameters for each superquadric: 11 encoding its 5-DoF shape and 6-DoF pose, and one modeling its existence probability \(\alpha\) ,enabling a variable number of superquadrics per object.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \(\phi \left( {\mathcal{F}}_{PC}\right)  \in  {\mathbb{R}}^{N \times  H}\) 是将点特征学习投影以匹配超二次特征的维度，\(\sigma\) 是softmax函数。第二个头，超二次头，以超二次特征 \({\mathcal{F}}_{SQ}\) 作为输入，并为每个超二次体预测12个参数：11个编码其5自由度形状和6自由度姿态，一个建模其存在概率 \(\alpha\)，使每个物体可以有可变数量的超二次体。</p></div><p>Losses. We train our model in a self-supervised manner, without requiring any ground truth annotation. Specifically, the total loss is defined as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>损失。我们以自监督的方式训练模型，无需任何真实标注。具体而言，总损失定义为：</p></div><p></p>\[\mathcal{L} = {\mathcal{L}}_{\text{rec }} + {\lambda }_{\text{par }}{\mathcal{L}}_{\text{par }} + {\lambda }_{\text{exist }}{\mathcal{L}}_{\text{exist }}, \tag{4}\]<p></p><p>where \({\mathcal{L}}_{\text{rec }}\) is the reconstruction loss aligning the predicted superquadrics to the input point cloud \(\mathcal{P},{\mathcal{L}}_{\text{par }}\) is the parsimony loss encouraging a small number of primitives, \({\mathcal{L}}_{\text{exist }}\) is the existence loss,and \({\lambda }_{\text{par }},{\lambda }_{\text{exist }}\) are weighting coefficients. The reconstruction loss \({\mathcal{L}}_{\text{rec }}\) consists of three terms:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中 \({\mathcal{L}}_{\text{rec }}\) 是重建损失，用于将预测的超二次体与输入点云对齐，\(\mathcal{P},{\mathcal{L}}_{\text{par }}\) 是鼓励使用少量原始体的简约损失，\({\mathcal{L}}_{\text{exist }}\) 是存在损失，\({\lambda }_{\text{par }},{\lambda }_{\text{exist }}\) 是权重系数。重建损失 \({\mathcal{L}}_{\text{rec }}\) 由三项组成：</p></div><p></p>\[{\mathcal{L}}_{\text{rec }} = {\mathcal{L}}_{\mathcal{P} \rightarrow  {SQ}} + {\mathcal{L}}_{{SQ} \rightarrow  \mathcal{P}} + {\mathcal{L}}_{N}. \tag{5}\]<p></p><p>The first two terms correspond to the bi-directional Chamfer distance between the input point cloud and the superquadric surfaces, while the third term serves as a regularizer incorporating normal information to improve convergence during training. To compute the Chamfer distance, we approximate each superquadric surface by uniformly sampling \(S\) points, following the method of Pilu et al. [35]. Denoting by \(d\left( {{\mathbf{x}}_{i},{\mathbf{x}}_{js}^{\prime }}\right)\) the euclidean distance between the \(i\) -th point in the input point cloud and the \(s\) -th point sampled on the surface of the \(j\) -th superquadric,we define \({\mathcal{L}}_{\mathcal{P} \rightarrow  {SQ}}\) as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>前两项对应于输入点云与超二次体表面之间的双向Chamfer距离，而第三项作为正则化项，结合法线信息以改善训练过程中的收敛性。为了计算Chamfer距离，我们通过均匀采样 \(S\) 个点来近似每个超二次体表面，遵循Pilu等人的方法[35]。用 \(d\left( {{\mathbf{x}}_{i},{\mathbf{x}}_{js}^{\prime }}\right)\) 表示输入点云中第 \(i\) 个点与第 \(s\) 个在第 \(j\) 个超二次体表面上采样的点之间的欧几里得距离，我们定义 \({\mathcal{L}}_{\mathcal{P} \rightarrow  {SQ}}\) 为：</p></div><p></p>\[{\mathcal{L}}_{\mathcal{P} \rightarrow  {SQ}} = \frac{1}{N}\mathop{\sum }\limits_{{i = 1}}^{N}\mathop{\sum }\limits_{{j = 0}}^{P}{m}_{ij}\mathop{\min }\limits_{{s \in  \left\lbrack  S\right\rbrack  }}d\left( {{\mathbf{x}}_{i},{\mathbf{x}}_{js}^{\prime }}\right) , \tag{6}\]<p></p><p>and \({\mathcal{L}}_{{SQ} \rightarrow  \mathcal{P}}\) as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>以及 \({\mathcal{L}}_{{SQ} \rightarrow  \mathcal{P}}\) 为：</p></div><p></p>\[{\mathcal{L}}_{{SQ} \rightarrow  \mathcal{P}} = \frac{1}{S\mathop{\sum }\limits_{{j = 1}}^{P}{\alpha }_{j}}\mathop{\sum }\limits_{{s = 1}}^{P}{\alpha }_{j}\mathop{\sum }\limits_{{s = 1}}^{S}\mathop{\min }\limits_{{i \in  \left\lbrack  N\right\rbrack  }}d\left( {{\mathbf{x}}_{i},{\mathbf{x}}_{js}^{\prime }}\right) . \tag{7}\]<p></p><p>The last term of Eq. 5,i.e., \({\mathcal{L}}_{N}\) is defined as the reconstruction loss from Yang et al. [55], and is used to incorporate normal information during training which leads to accelerated convergence. Additionally, since we seek not only accuracy but also compactness, we introduce a parsimony loss to encourage the use of fewer primitives. To do that, we optimize the 0.5-norm of \({m}_{j} \mathrel{\text{:=}} \mathop{\sum }\limits_{{i = 1}}^{N}\frac{{n}_{ij}}{N}\) and define the parsimony loss as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>方程5的最后一项，即 \({\mathcal{L}}_{N}\) 定义为Yang等人[55]的重建损失，并用于在训练过程中结合法线信息，从而加速收敛。此外，由于我们不仅追求准确性，还追求紧凑性，我们引入了简约损失以鼓励使用更少的原始体。为此，我们优化 \({m}_{j} \mathrel{\text{:=}} \mathop{\sum }\limits_{{i = 1}}^{N}\frac{{n}_{ij}}{N}\) 的0.5范数，并将简约损失定义为：</p></div><p></p>\[{\mathcal{L}}_{\text{par }} = {\left( \frac{1}{P}\mathop{\sum }\limits_{{j = 1}}^{P}\frac{\sqrt{{m}_{j}}}{P}\right) }^{2}. \tag{8}\]<p></p><p>Lastly,we employ an existence loss \({\mathcal{L}}_{\text{exist }}\) which uses the predicted segmentation as a teacher for the linear head in charge of predicting the existence probability. More specifically,given a threshold \({\epsilon }_{\text{exist }}\) ,we define the ground-truth existence of the \(j\) th superquadric as \({\widehat{\alpha }}_{j} \mathrel{\text{:=}} {m}_{j} > {\epsilon }_{\text{exist }}\) and define \({\mathcal{L}}_{\text{exist }}\) as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最后，我们采用存在损失 \({\mathcal{L}}_{\text{exist }}\)，该损失使用预测的分割作为负责预测存在概率的线性头的教师。更具体地说，给定一个阈值 \({\epsilon }_{\text{exist }}\)，我们将第 \(j\) 个超二次体的真实存在定义为 \({\widehat{\alpha }}_{j} \mathrel{\text{:=}} {m}_{j} > {\epsilon }_{\text{exist }}\)，并将 \({\mathcal{L}}_{\text{exist }}\) 定义为：</p></div><p></p>\[{\mathcal{L}}_{\text{exist }} = \mathop{\sum }\limits_{{j = 1}}^{P}\frac{{BCE}\left( {{\alpha }_{j},{\widehat{\alpha }}_{j}}\right) }{P}, \tag{9}\]<p></p><!-- Media --><!-- figureText: Input Neural Network Output Segmentation Matrix \( M \) LM Superquadric Optimization Parameters \( N \times  P \) Superquadric Parameters \( \Theta \) \( P \times  {12} \) Point Cloud \( \mathcal{P} \) \( {\mathcal{F}}_{SQ} \) softmax scale \( \mathbf{s} \in  {\mathbb{R}}^{3} \) Add & Norm shape \( \epsilon  \in  {\mathbb{R}}^{2} \) Feed Forward rotation \( \mathbf{r} \in  {\mathbb{R}}^{3} \) Add & Norm Cross Attention translate \( \mathbf{t} \in  {\mathbb{R}}^{3} \) \( {\mathcal{F}}_{PC} \) Add & Norm exist \( \alpha  \in  \mathbb{R} \) Self Attention CD \( {\mathcal{F}}_{SQ} \) --><img src="https://cdn.noedgeai.com/bo_d16410bef24c73d1lfag_3.jpg?x=207&#x26;y=210&#x26;w=1386&#x26;h=435&#x26;r=0"><p>Figure 2. Illustration of the SUPERDEC Model. Given a point cloud of an object with \(N\) points,a Transformer-based neural network predicts parameters for \(P\) superquadrics,as well as a soft segmentation matrix that assigns points to superquadrics. The predicted parameters include the 11 superquadric parameters and an objectness score. These predictions provide an effective initialization for the subsequent Levenberg-Marquardt (LM) optimization, which refines the superquadrics.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图2. SUPERDEC模型的示意图。给定一个包含 \(N\) 个点的物体点云，基于Transformer的神经网络预测 \(P\) 个超二次体的参数，以及一个将点分配给超二次体的软分割矩阵。预测的参数包括11个超二次体参数和一个物体性得分。这些预测为后续的Levenberg-Marquardt (LM) 优化提供了有效的初始化，从而细化超二次体。</p></div><!-- Media --><p>where BCE is the binary cross entropy and \({\alpha }_{j}\) is the predicted existence probability for the \(j\) th superquadric.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中BCE是二元交叉熵，\({\alpha }_{j}\) 是第 \(j\) 个超二次体的预测存在概率。</p></div><h4>3.1.2. Optimization</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>3.1.2. 优化</h4></div><p>Our optimization module takes as input the predicted soft segmentation matrix \(M\) as well as the superquadric parameters \(\Theta\) ,and further refines the superquadric parameters using the Levenberg-Marquardt (LM) [20, 28] algorithm. Specifically,given a point cloud of \(N\) points,it iteratively refines the parameters \({\Theta }_{j}\) of the \(j\) th superquadric,by computing two sets of residuals: The first set of residuals \({r}_{ij}\) with \(i \in  \left\lbrack  {1,N}\right\rbrack\) and \(j \in  \left\lbrack  {1,P}\right\rbrack\) is defined as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的优化模块以预测的软分割矩阵\(M\)和超二次参数\(\Theta\)作为输入，并进一步使用Levenberg-Marquardt (LM) [20, 28]算法细化超二次参数。具体而言，给定一个包含\(N\)个点的点云，它通过计算两组残差迭代地细化\({\Theta }_{j}\)个超二次体的参数\(j\)：第一组残差\({r}_{ij}\)与\(i \in  \left\lbrack  {1,N}\right\rbrack\)和\(j \in  \left\lbrack  {1,P}\right\rbrack\)的定义为：</p></div><p></p>\[{r}_{ij} = {m}_{ij}{\widetilde{d}}_{j}\left( {\mathbf{x}}_{i}\right) , \tag{10}\]<p></p><p>where \({\widetilde{d}}_{j}\left( {\mathbf{x}}_{i}\right)\) denotes the radial distance of point \({\mathbf{x}}_{i}\) from the \(j\) th superquadric,computed according to Eq. 2. The second set of residuals is used for normalization and is obtained by sampling a set of \(K\) points \({\mathbf{p}}_{1},\ldots ,{\mathbf{p}}_{\mathbf{K}}\) on the surface of the given superquadric and then computing the distance of each of them from the point cloud. Specifically, for \(i \in  \left\lbrack  {N + 1,N + K}\right\rbrack\) and \(j \in  \left\lbrack  {1,P}\right\rbrack\) we compute \({r}_{ij}\) as:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中\({\widetilde{d}}_{j}\left( {\mathbf{x}}_{i}\right)\)表示第\(j\)个超二次体中点\({\mathbf{x}}_{i}\)的径向距离，按照公式2计算。第二组残差用于归一化，通过在给定超二次体的表面上采样一组\(K\)个点\({\mathbf{p}}_{1},\ldots ,{\mathbf{p}}_{\mathbf{K}}\)，然后计算每个点与点云的距离来获得。具体而言，对于\(i \in  \left\lbrack  {N + 1,N + K}\right\rbrack\)和\(j \in  \left\lbrack  {1,P}\right\rbrack\)，我们计算\({r}_{ij}\)为：</p></div><p></p>\[{r}_{ij} = \mathop{\min }\limits_{k}{\begin{Vmatrix}{\mathbf{p}}_{\mathbf{i} - \mathbf{N}} - {\Pi }_{j}\left( {\mathbf{x}}_{\mathbf{k}}\right) \end{Vmatrix}}_{2},\;\text{ with }k \in  \left\lbrack  {1,N}\right\rbrack  . \tag{11}\]<p></p><h3>3.2. Decomposition of Full 3D Scenes</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>3.2. 完整3D场景的分解</h3></div><p>After training on single objects, extending SUPERDEC to full 3D scenes is straightforward. Given a scene-level point cloud, we extract 3D object instance masks using Mask3D [40]. Each object is centered and uniformly rescaled to the unit sphere. We then predict the superquadric primitives for each object individually using our model. We found our model trained on ShapeNet [4] to generalize well on real-world 3D scenes from ScanNet++ [56] and Replica [44] without additional fine-tuning.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在对单个物体进行训练后，将SUPERDEC扩展到完整的3D场景是直接的。给定一个场景级点云，我们使用Mask3D [40]提取3D物体实例掩膜。每个物体都被居中并均匀缩放到单位球体。然后，我们使用我们的模型单独预测每个物体的超二次原语。我们发现，在ShapeNet [4]上训练的模型在ScanNet++ [56]和Replica [44]的真实世界3D场景中表现良好，无需额外的微调。</p></div><h2>4. Experiments</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>4. 实验</h2></div><p>We first compare our SUPERDEC with previous state-of-the-art methods on individual objects and full 3D scenes (Sec. 4.1). We then demonstrate the usefulness of our representation on down-stream applications for robotics and controllable image generation (Sec. 4.2). Finally, in Sec. 4.3, we present additional analyses on part segmentation and the implicit learning of shape categories, followed by a study of the compactness-accuracy trade-off and runtime.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们首先在单个物体和完整3D场景上将我们的SUPERDEC与之前的最先进方法进行比较（第4.1节）。然后，我们展示了我们的表示在机器人技术和可控图像生成的下游应用中的实用性（第4.2节）。最后，在第4.3节中，我们呈现了关于部分分割和形状类别隐式学习的额外分析，随后研究了紧凑性-准确性权衡和运行时间。</p></div><h3>4.1. Comparing with State-of-the-art Methods</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.1. 与最先进方法的比较</h3></div><p>Datasets. We compare on three different datasets: ShapeNet [4]: We use the 13 and train-val-test splits as defined in Choy et al. [8]. For each object, we randomly sample 4096 points using Farthest Point Sampling (FPS) [36]. All objects are pre-aligned in a canonical orientation. ShapeNet is a widely used dataset and is therefore well-suited for comparison with existing baselines.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>数据集。我们在三个不同的数据集上进行比较：ShapeNet [4]：我们使用Choy等人[8]定义的13个训练-验证-测试划分。对于每个物体，我们使用最远点采样（FPS）[36]随机采样4096个点。所有物体都在规范方向上预对齐。ShapeNet是一个广泛使用的数据集，因此非常适合与现有基准进行比较。</p></div><p>ScanNet++ [56]: We further evaluate our model on real-world object scans from the ScanNet++ validation set. Each object is extracted using ground truth mask annotations, and 4,096 points are sampled per object. In contrast to ShapeNet, these object point clouds are noisier, partially observed, and subject to random orientation and translation, providing a more realistic and challenging evaluation setting for our method.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>ScanNet++ [56]：我们进一步在ScanNet++验证集的真实物体扫描上评估我们的模型。每个物体使用真实掩膜注释提取，每个物体采样4096个点。与ShapeNet相比，这些物体点云更嘈杂、部分可观察，并且受到随机方向和位移的影响，为我们的方法提供了更现实和具有挑战性的评估环境。</p></div><!-- Media --><table><tbody><tr><td rowspan="2">Model</td><td rowspan="2">Primitive Type</td><td rowspan="2">Segmentation</td><td colspan="3">In-category</td><td colspan="3">Out-of-category</td></tr><tr><td>L1 \( \downarrow \)</td><td>L2↓</td><td>#Prim. \( \downarrow \)</td><td>L1 \( \downarrow \)</td><td>L2 \( \downarrow \)</td><td>#Prim. \( \downarrow \)</td></tr><tr><td>EMS (Liu et al.) [24]</td><td>Superquadrics</td><td>✘</td><td>5.771</td><td>1.345</td><td>5.68</td><td>5.410</td><td>1.211</td><td>5.68</td></tr><tr><td>CSA (Yang et al.) [55]</td><td>Cuboids</td><td>✓</td><td>5.157</td><td>0.527</td><td>9.21</td><td>4.897</td><td>0.427</td><td>11.75</td></tr><tr><td>SQ (Paschalidou et al.) [32]</td><td>Superquadrics</td><td>✘</td><td>3.668</td><td>0.279</td><td>10</td><td>4.193</td><td>0.354</td><td>9</td></tr><tr><td>SUPERDEC (Ours)</td><td>Superquadrics</td><td>✓</td><td>1.698</td><td>0.051</td><td>5.8</td><td>1.847</td><td>0.061</td><td>5.26</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">模型</td><td rowspan="2">原始类型</td><td rowspan="2">分割</td><td colspan="3">类别内</td><td colspan="3">类别外</td></tr><tr><td>L1 \( \downarrow \)</td><td>L2↓</td><td>#原始. \( \downarrow \)</td><td>L1 \( \downarrow \)</td><td>L2 \( \downarrow \)</td><td>#原始. \( \downarrow \)</td></tr><tr><td>EMS (刘等) [24]</td><td>超四面体</td><td>✘</td><td>5.771</td><td>1.345</td><td>5.68</td><td>5.410</td><td>1.211</td><td>5.68</td></tr><tr><td>CSA (杨等) [55]</td><td>立方体</td><td>✓</td><td>5.157</td><td>0.527</td><td>9.21</td><td>4.897</td><td>0.427</td><td>11.75</td></tr><tr><td>SQ (帕斯卡利杜等) [32]</td><td>超四面体</td><td>✘</td><td>3.668</td><td>0.279</td><td>10</td><td>4.193</td><td>0.354</td><td>9</td></tr><tr><td>SUPERDEC (我们)</td><td>超四面体</td><td>✓</td><td>1.698</td><td>0.051</td><td>5.8</td><td>1.847</td><td>0.061</td><td>5.26</td></tr></tbody></table></div><p>Table 1. Quantitative Results on ShapeNet [4]. We show scores for in-category and out-of-category experiments and are scaled by \({10}^{3}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表1. ShapeNet [4] 的定量结果。我们展示了类别内和类别外实验的得分，并按 \({10}^{3}\) 进行缩放。</p></div><!-- figureText: In-category Classes Out-of-category Classes Point Cloud EMS [23] CSA [55] SQ [32] SUPERDEC (Ours) --><img src="https://cdn.noedgeai.com/bo_d16410bef24c73d1lfag_4.jpg?x=166&#x26;y=511&#x26;w=1448&#x26;h=858&#x26;r=0"><p>Figure 3. Qualitative Results on ShapeNet [4]. We show results on test samples for in-category (four first columns) classes and out-of-category classes (two last columns). The latter were not seen during training and illustrate how well models generalize to novel classes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图3. ShapeNet [4] 的定性结果。我们展示了类别内（前四列）和类别外（最后两列）测试样本的结果。后者在训练期间未见，展示了模型对新类别的泛化能力。</p></div><!-- Media --><p>Replica [44]: Lastly, we present qualitative results on full 3D scenes from Replica. Object instances are extracted using the pre-trained \(3\mathrm{D}\) instance segmentation model Mask3D [40], allowing us to demonstrate our approach in a fully realistic setting without relying on ground truth annotations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>Replica [44]：最后，我们展示了来自Replica的完整3D场景的定性结果。对象实例使用预训练的 \(3\mathrm{D}\) 实例分割模型Mask3D [40] 提取，使我们能够在完全真实的环境中展示我们的方法，而无需依赖真实标注。</p></div><p>Methods in comparison. We compare to a wide range of prior works using both cuboids and superquadrics. \({SQ}\left\lbrack  {32}\right\rbrack\) is a learning-based approach for object-level decomposition using superquadrics; it takes a voxel grid as input and predicts superquadric primitives via a CNN. CSA [55] is another learning-based method but uses cuboids as geometric primitives. It takes a point cloud as input and predicts cuboid parameters from a global latent code. Lastly, EMS [24] is an optimization-based approach that decomposes objects by hierarchically fitting superquadrics to parts of a point cloud until a maximum depth is reached.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>方法比较。我们比较了使用立方体和超立方体的广泛先前工作。\({SQ}\left\lbrack  {32}\right\rbrack\) 是一种基于学习的对象级分解方法，使用超立方体；它以体素网格作为输入，通过CNN预测超立方体原语。CSA [55] 是另一种基于学习的方法，但使用立方体作为几何原语。它以点云作为输入，从全局潜在编码中预测立方体参数。最后，EMS [24] 是一种基于优化的方法，通过分层拟合超立方体到点云的部分来分解对象，直到达到最大深度。</p></div><p>Training Details. Our goal is to develop a general-purpose, class-agnostic model capable of representing arbitrary objects as superquadrics. Existing methods typically train separate models for each object class, assuming that all classes are known in advance and that sufficient training data is available for each. These assumptions, however, often fail in real-world scenarios. To address this, we jointly train a single model on all 13 ShapeNet classes using the publicly available code of prior methods, moving towards a more realistic class-agnostic solution. In our model we set the following hyper-parameters \(P = {16},S = {4096},K = {25}\) , \({\lambda }_{\text{exist }} = {0.01},{\lambda }_{\text{par }} = {0.06}\) .</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>训练细节。我们的目标是开发一种通用的、与类别无关的模型，能够将任意对象表示为超立方体。现有方法通常为每个对象类别训练单独的模型，假设所有类别都是事先已知的，并且每个类别都有足够的训练数据。然而，这些假设在现实场景中往往失败。为了解决这个问题，我们在所有13个ShapeNet类别上联合训练一个单一模型，使用先前方法的公开代码，朝着更现实的与类别无关的解决方案迈进。在我们的模型中，我们设置以下超参数 \(P = {16},S = {4096},K = {25}\) , \({\lambda }_{\text{exist }} = {0.01},{\lambda }_{\text{par }} = {0.06}\)。</p></div><!-- Media --><table><tbody><tr><td>Model</td><td>L1 \( \downarrow \)</td><td>L2 \( \downarrow \)</td><td>#Prim. \( \downarrow \)</td></tr><tr><td>EMS (Liu et al.) [24]</td><td>5.51</td><td>2.11</td><td>4.25</td></tr><tr><td>SUPERDEC (Ours)</td><td>1.37</td><td>0.07</td><td>5.41</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>模型</td><td>L1 \( \downarrow \)</td><td>L2 \( \downarrow \)</td><td>#主要. \( \downarrow \)</td></tr><tr><td>EMS (刘等) [24]</td><td>5.51</td><td>2.11</td><td>4.25</td></tr><tr><td>SUPERDEC (我们的)</td><td>1.37</td><td>0.07</td><td>5.41</td></tr></tbody></table></div><p>Table 2. Object-Level Evaluation on ScanNet++ [56]</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表2. ScanNet++上的对象级评估 [56]</p></div><!-- Media --><p>Metrics. We assess reconstruction accuracy using L1 and L2 Chamfer distances and compactness by the average number of geometric primitives.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>指标。我们使用L1和L2 Chamfer距离评估重建精度，并通过几何原件的平均数量评估紧凑性。</p></div><h4>4.1.1. Results on ShapeNet</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.1.1. ShapeNet上的结果</h4></div><p>We show scores in Tab. 1 and qualitative results in Fig. 3. To evaluate both accuracy and generalization, we conduct two experiments: in-category and out-of-category. In the in-category setting, all learning-based methods are jointly trained on the 13 classes of the ShapeNet training set and evaluated on the corresponding test set. In the out-of-category setting, models are trained on half of the categories (airplane, bench, chair, lamp, rifle, table) and tested on the remaining ones (car, sofa, loudspeaker, cabinet, display, telephone, watercraft). Our SUPERDEC model significantly outperforms both learned and non-learned baselines. Compared to learned baselines, we reduce the L2 loss by a factor of six while using nearly half the number of primitives, supporting our hypothesis that leveraging local point features improves 3D decomposition in both accuracy and compactness. Compared to the non-learned baseline, we predict a similar number of primitives but achieve an L2 loss approximately 20 times smaller, validating the benefit of learning shape priors to avoid local minima that often hinder purely optimization-based approaches.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们在表1中展示了分数，在图3中展示了定性结果。为了评估精度和泛化能力，我们进行了两个实验：类别内和类别外。在类别内设置中，所有基于学习的方法在ShapeNet训练集的13个类别上共同训练，并在相应的测试集上进行评估。在类别外设置中，模型在一半的类别（飞机、长椅、椅子、灯、步枪、桌子）上进行训练，并在剩余类别（汽车、沙发、扬声器、橱柜、显示器、电话、水上交通工具）上进行测试。我们的SUPERDEC模型显著优于学习和非学习基线。与学习基线相比，我们在使用近一半的原件数量的情况下，将L2损失减少了六倍，支持了我们利用局部点特征提高3D分解精度和紧凑性的假设。与非学习基线相比，我们预测的原件数量相似，但L2损失约小20倍，验证了学习形状先验以避免常常阻碍纯优化方法的局部最小值的好处。</p></div><h4>4.1.2. Quantitative Results on ScanNet++ Instances</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.1.2. ScanNet++实例的定量结果</h4></div><p>In this section, models are evaluated on real-world, out-of-category objects, which appear in arbitrary orientations and often exhibit incomplete point clouds due to reconstruction artifacts. Tab. 2 shows the quantitative results. Despite never being trained on real-world objects, our method outperforms the optimization baseline by a large margin, achieving a 30-fold reduction in L2 loss.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本节中，模型在真实世界的类别外对象上进行评估，这些对象以任意方向出现，并且由于重建伪影通常表现出不完整的点云。表2显示了定量结果。尽管从未在真实世界对象上进行训练，我们的方法仍大幅超越了优化基线，实现了L2损失减少30倍。</p></div><h4>4.1.3. Qualitative Results on Full Replica Scenes</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.1.3. 完整复制场景的定性结果</h4></div><p>Lastly, we qualitatively evaluate our pipeline on full 3D scenes from Replica, where our object-level model is applied on top of class-agnostic instance segmentation predictions from Mask3D [40]. As shown in Fig. 1, our method effectively reconstructs object shapes, even under noisy segmentation masks and geometries that differ substantially from those seen during training.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>最后，我们在Replica的完整3D场景上定性评估我们的管道，其中我们的对象级模型应用于Mask3D [40]的类别无关实例分割预测之上。如图1所示，我们的方法有效重建了对象形状，即使在嘈杂的分割掩码和与训练期间看到的几何形状有显著差异的情况下。</p></div><h3>4.2. Down-stream Applications</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.2. 下游应用</h3></div><p>Next, we show the versatility of the SUPERDEC representation for downstream applications, including robotics tasks</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>接下来，我们展示了SUPERDEC表示在下游应用中的多功能性，包括机器人任务</p></div><!-- Media --><table><tbody><tr><td>Method</td><td>Time (ms)</td><td>Suc. (%)</td><td>\( \mathbf{{Mem}.\left( {MB}\right) } \)</td></tr><tr><td>Occupancy</td><td>0.056</td><td>100.00</td><td>0.873</td></tr><tr><td>PointCloud</td><td>0.063</td><td>89.57</td><td>19.286</td></tr><tr><td>Voxels</td><td>0.030</td><td>98.78</td><td>0.101</td></tr><tr><td>Cuboids [37]</td><td>0.120</td><td>61.23</td><td>0.024</td></tr><tr><td>SuperDEC</td><td>0.150</td><td>91.71</td><td>0.042</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td>方法</td><td>时间 (毫秒)</td><td>成功率 (%)</td><td>\( \mathbf{{Mem}.\left( {MB}\right) } \)</td></tr><tr><td>占用率</td><td>0.056</td><td>100.00</td><td>0.873</td></tr><tr><td>点云</td><td>0.063</td><td>89.57</td><td>19.286</td></tr><tr><td>体素</td><td>0.030</td><td>98.78</td><td>0.101</td></tr><tr><td>立方体 [37]</td><td>0.120</td><td>61.23</td><td>0.024</td></tr><tr><td>超级DEC</td><td>0.150</td><td>91.71</td><td>0.042</td></tr></tbody></table></div><p>Table 3. Path Planning Results. Values are averaged over 15 ScanNet++ scenes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表3. 路径规划结果。值是对15个ScanNet++场景的平均。</p></div><img src="https://cdn.noedgeai.com/bo_d16410bef24c73d1lfag_5.jpg?x=928&#x26;y=529&#x26;w=701&#x26;h=324&#x26;r=0"><p>Figure 4. Grasping Result. Visualization of computed grasp poses for a milk bottle, some flowers, a side table, and a plant.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图4. 抓取结果。对牛奶瓶、一些花、边桌和植物的计算抓取姿势的可视化。</p></div><!-- Media --><p>such as path planning and object grasping (Sec. 4.2.1), and controllable image generation (Sec. 4.2.2).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>例如路径规划和物体抓取（第4.2.1节），以及可控图像生成（第4.2.2节）。</p></div><h4>4.2.1. Robotics</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.2.1. 机器人技术</h4></div><p>Path planning seeks to compute a collision-free shortest path between a given start and end point in 3D space, enabling efficient robot navigation. Although essential for traversing large environments, it typically demands storing large-scale 3D representations. Here, we assess whether our compact representation can perform this task effectively while reducing memory requirements. We conduct experiments on 15 ScanNet++ [56] scenes, comparing SUPERDEC to common 3D representations, including dense occupancy grids, point clouds, voxel grids, and cuboids [37]. As shown in Tab. 4, SUPERDEC not only reduces memory consumption compared to traditional representations but also achieves a higher success rate than dense point clouds. Further details about experiment setup, metrics and analysis are provided in the Appendix.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>路径规划旨在计算给定起点和终点之间的无碰撞最短路径，从而实现高效的机器人导航。尽管在穿越大环境时至关重要，但通常需要存储大规模的3D表示。在这里，我们评估我们的紧凑表示是否能够有效地执行此任务，同时减少内存需求。我们在15个ScanNet++ [56]场景上进行实验，将SUPERDEC与常见的3D表示进行比较，包括密集占用网格、点云、体素网格和立方体 [37]。如表4所示，SUPERDEC不仅减少了与传统表示相比的内存消耗，还实现了比密集点云更高的成功率。关于实验设置、指标和分析的更多细节见附录。</p></div><p>Object Grasping enables robots to grasp real-world objects by computing suitable grasping poses. Existing methods fall into two categories, each with complementary limitations. Geometry-based approaches \(\left\lbrack  {3,{12},{30}}\right\rbrack\) require precise 3D object models, which are often unavailable in real-world scenarios. Learning-based approaches [27, 46, 54] operate directly on raw sensor data but tend to be biased towards training data, which typically consists of tabletop scenes with small, convex, or low-genus objects [11, 45]. To overcome these limitations, SuperQ-GRASP [48] explored decomposing objects into explicit primitives. However, its reliance on Marching Primitives [25] to obtain superquadrics from the object's Signed Distance Function (SDF) makes it unsuitable for most real-world cases</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>物体抓取使机器人能够通过计算合适的抓取姿势来抓取现实世界中的物体。现有方法分为两类，各有互补的局限性。基于几何的方法\(\left\lbrack  {3,{12},{30}}\right\rbrack\)需要精确的3D物体模型，而这些模型在现实场景中通常不可用。基于学习的方法 [27, 46, 54] 直接在原始传感器数据上操作，但往往对训练数据存在偏见，这些数据通常由小型、凸形或低基数物体的桌面场景组成 [11, 45]。为了克服这些局限性，SuperQ-GRASP [48] 探索了将物体分解为显式原语。然而，它依赖于Marching Primitives [25] 从物体的有符号距离函数（SDF）中获取超二次体，这使其不适用于大多数现实情况。</p></div><!-- Media --><!-- figureText: Input Point Cloud Superquadrics --><img src="https://cdn.noedgeai.com/bo_d16410bef24c73d1lfag_6.jpg?x=163&#x26;y=188&#x26;w=710&#x26;h=535&#x26;r=0"><p>Figure 5. Real-world robot experiment. The top row shows the input scan (left) and the representation from SUPERDEC with the computed path and grasping pose (right). The bottom row illustrates the robot following the planned path. We denote the starting point of the path with a green sphere, and the target location with a red sphere. The target object (a milk bottle) is circled in red.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图5. 现实世界的机器人实验。顶部行显示输入扫描（左）和来自SUPERDEC的表示，包含计算的路径和抓取姿势（右）。底部行展示了机器人沿着规划路径移动。我们用绿色球体表示路径的起点，用红色球体表示目标位置。目标物体（牛奶瓶）用红色圈出。</p></div><!-- Media --><p>where only point clouds are available. In contrast, our approach directly processes point clouds of entire scenes and, when combined with the class-agnostic segmentations from Mask3D [40], extracts superquadrics for all objects. Given the superquadric parameters, we employ a superquadric-based geometric method [53] to compute grasping poses for selected objects. Fig. 4 shows predicted grasping poses on objects from a real-world 3D scan of a room. In practice, our method eliminates the need for data-driven grasping models while remaining adaptable to diverse object shapes and producing high-quality grasping poses.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在仅有点云可用的情况下。相反，我们的方法直接处理整个场景的点云，并在与Mask3D [40] 的类无关分割相结合时，为所有物体提取超二次体。给定超二次体参数，我们采用基于超二次体的几何方法 [53] 为选定物体计算抓取姿势。图4展示了在现实世界房间的3D扫描中对物体的预测抓取姿势。在实践中，我们的方法消除了对数据驱动抓取模型的需求，同时仍然适应多样的物体形状，并生成高质量的抓取姿势。</p></div><p>Real-world Experiment. Finally, we demonstrate the real-world applicability of our superquadric-based representation by deploying it on a legged robot (Boston Dynamics Spot) equipped with an arm, supporting both motion planning and object grasping in an indoor environment. We scan a scene using a 3D scanning application on an iPad, extract a dense point cloud, and run SUPERDEC on it. Given the robot's starting position and a specified target object (a milk bottle), we compute both the path and the grasping pose as described earlier (see Fig. 4), enabling the robot to approach and successfully grasp the object. Fig. 5 shows the computed representation, the planned trajectory, the grasping pose, and a frame from the real-world demonstration. This experiment suggests that integrating SUPERDEC with open-vocabulary segmentation methods such as Open-Mask3D [47] could allow robots to navigate to and grasp arbitrary objects specified via natural-language prompts.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>现实世界实验。最后，我们通过在配备有手臂的腿式机器人（波士顿动力Spot）上部署超二次体表示，展示其在现实世界的适用性，支持室内环境中的运动规划和物体抓取。我们使用iPad上的3D扫描应用扫描场景，提取密集点云，并在其上运行SUPERDEC。给定机器人的起始位置和指定的目标物体（牛奶瓶），我们计算路径和抓取姿势，如前所述（见图4），使机器人能够接近并成功抓取物体。图5展示了计算的表示、规划的轨迹、抓取姿势以及现实世界演示中的一帧。该实验表明，将SUPERDEC与开放词汇分割方法（如Open-Mask3D [47]）结合，可以使机器人导航到并抓取通过自然语言提示指定的任意物体。</p></div><h4>4.2.2. Controllable Generation and Editing</h4><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h4>4.2.2. 可控生成与编辑</h4></div><p>We investigate how the SUPERDEC representation can be directly used to introduce joint spatial and semantic control in text-to-image diffusion models [39]. Specifically, we generate images by conditioning a ControlNet [58] on depth maps rendered from the superquadrics extracted from Replica [44] scenes. Qualitative results are shown in Fig. 6 and Fig. 7. Fig. 6 demonstrates spatial control: by moving, duplicating, or removing superquadrics corresponding to a plant, we coherently influence the generated images. Fig. 7 highlights semantic control: we can vary the room's style while preserving its semantic and geometric structure, and observe that object semantics naturally emerge from the spatial arrangement of superquadrics without explicit conditioning - e.g., pillows appear on couches, and a plant is placed on a central table, reflecting plausible real-world arrangements.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们研究SUPERDEC表示如何直接用于在文本到图像扩散模型 [39] 中引入联合空间和语义控制。具体而言，我们通过将ControlNet [58] 条件化于从超二次体提取的深度图来生成图像。定性结果见图6和图7。图6展示了空间控制：通过移动、复制或移除对应于植物的超二次体，我们一致地影响生成的图像。图7强调了语义控制：我们可以在保持语义和几何结构的同时改变房间的风格，并观察到物体语义自然地从超二次体的空间排列中出现，而无需显式条件 - 例如，枕头出现在沙发上，植物放在中央桌子上，反映出合理的现实世界排列。</p></div><!-- Media --><!-- figureText: Original Editing Addition Deletion --><img src="https://cdn.noedgeai.com/bo_d16410bef24c73d1lfag_6.jpg?x=924&#x26;y=188&#x26;w=707&#x26;h=444&#x26;r=0"><p>Figure 6. Spatial control using SUPERDEC. Top row shows su-perquadrics generated by SUPERDEC, bottom row shows generated images using the prompt 'A corner of a room with a plant'.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图6. 使用SUPERDEC进行空间控制。顶部行显示由SUPERDEC生成的超二次体，底部行显示使用提示“一个房间的角落里有一盆植物”生成的图像。</p></div><!-- figureText: Superquadrics Depth Prompt "Modern living room" "Pink living room" UNI --><img src="https://cdn.noedgeai.com/bo_d16410bef24c73d1lfag_6.jpg?x=924&#x26;y=751&#x26;w=707&#x26;h=541&#x26;r=0"><p>Figure 7. Semantic control using SUPERDEC. Top: su-perquadrics created by our SUPERDEC, and depth map to prompt the generation of text-to-image diffusion model []. Bottom row: generations with two different textual prompts.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图7. 使用SUPERDEC进行语义控制。顶部：由我们的SUPERDEC创建的超二次体，以及深度图以提示文本到图像扩散模型的生成[]。底部行：使用两个不同文本提示的生成结果。</p></div><!-- Media --><h3>4.3. Analysis Experiments</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>4.3. 分析实验</h3></div><p>Unsupervised part segmentation. Besides superquadric parameters, our method also predicts a segmentation matrix which segments the initial point cloud into parts that are fitted to the predicted superquadrics. In Fig. 8, we visualize the predicted segmentation masks for the same examples shown in Fig. 3. We observe that segmentation masks, especially in the in-category experiments, appear very sharp. This suggests that our method, especially if trained at a larger scale, can be leveraged for different applications as geometry-based part segmentation or as pretraining for supervised semantic part segmentation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>无监督部分分割。除了超二次体参数外，我们的方法还预测一个分割矩阵，将初始点云分割成与预测的超二次体相适应的部分。在图8中，我们可视化了与图3中相同示例的预测分割掩码。我们观察到，分割掩码，特别是在类别内实验中，显得非常清晰。这表明我们的方法，特别是在更大规模训练时，可以用于几何基础的部分分割或作为监督语义部分分割的预训练。</p></div><!-- Media --><!-- figureText: In-category Out-of-category --><img src="https://cdn.noedgeai.com/bo_d16410bef24c73d1lfag_7.jpg?x=168&#x26;y=234&#x26;w=686&#x26;h=180&#x26;r=0"><p>Figure 8. Qualitative Results on ShapeNet [4] segmentation. We show the resulting segmentation matrices on test samples for in-category (four first columns) classes and out-of-category classes (two last columns). The latter were not seen during training.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图8. ShapeNet [4] 分割的定性结果。我们展示了测试样本的分割矩阵，针对类别内（前四列）和类别外（最后两列）类。后者在训练期间未见过。</p></div><!-- figureText: table chair airplane rifle lamp cabinet display telephone watercraft --><img src="https://cdn.noedgeai.com/bo_d16410bef24c73d1lfag_7.jpg?x=244&#x26;y=549&#x26;w=557&#x26;h=440&#x26;r=0"><p>Figure 9. t-SNE Visualization of Primitive Embeddings across different ShapeNet classes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图9. 不同ShapeNet类的原始嵌入的t-SNE可视化。</p></div><!-- Media --><p>What does our network learn? Since our network performs unsupervised part segmentation, we analyze the features learned by the Transformer decoder across object classes. Inspired by BERT [10]'s [CLS] token, we append a learnable embedding to the sequence of embedded superquadrics; although never explicitly decoded, this embedding is refined through self- and cross-attention. After training, we extract and visualize these embeddings using t-SNE [50] for ShapeNet [4] categories (Fig. 9). We observe that categories with consistent shapes, such as chairs, airplanes, and cars, form clear clusters, while categories with high intra-class variability, such as watercraft, are more dispersed. This indicates that our model organizes objects by geometric structure without requiring class annotations.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的网络学习了什么？由于我们的网络执行无监督部分分割，我们分析了Transformer解码器在不同物体类别中学习的特征。受到BERT [10] 的[CLS]标记的启发，我们将一个可学习的嵌入附加到嵌入超二次体的序列中；尽管从未明确解码，但该嵌入通过自注意力和交叉注意力进行优化。训练后，我们使用t-SNE [50] 提取并可视化这些嵌入，针对ShapeNet [4] 类别（图9）。我们观察到，形状一致的类别，如椅子、飞机和汽车，形成了清晰的聚类，而具有高类内变异性的类别，如水上交通工具，则更加分散。这表明我们的模型通过几何结构组织物体，而无需类注释。</p></div><p>How fast is our method? Our model is highly paral-lelizable, allowing multiple objects to be batched and processed simultaneously in a single forward pass. On an RTX</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们的方法有多快？我们的模型高度可并行化，允许多个物体在单次前向传递中批量处理和同时处理。在RTX</p></div><!-- Media --><!-- figureText: Number of Primitives 8 0.09 - 0.08 0.07 0.06 0.05 0.6 0.8 4 \( {\lambda }_{\text{par }} \) --><img src="https://cdn.noedgeai.com/bo_d16410bef24c73d1lfag_7.jpg?x=1003&#x26;y=203&#x26;w=546&#x26;h=329&#x26;r=0"><p>Figure 10. Compactness vs. reconstruction accuracy tradeoff. We run experiment for different values of the parsimony weight \({\lambda }_{\text{par }}\) (x-axis) and we visualize the resulting number of primitives (y-axis, left) and the L2 Chamfer distance (y-axis, right).</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图10. 紧凑性与重建精度的权衡。我们对不同的简约权重\({\lambda }_{\text{par }}\)（x轴）进行实验，并可视化结果中原始体的数量（y轴，左）和L2 Chamfer距离（y轴，右）。</p></div><!-- Media --><p>4090 (24 GB), we can process up to 256 objects in parallel. On average,the forward pass takes \({0.13}\mathrm{\;s}\) for a complete Replica [44] scene, 3D instance segmentation with Mask3D [40] requires \({0.3}\mathrm{\;s}\) ,and each LM optimization step takes less than \(1\mathrm{\;s}\) ,see supplementary for more details.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>4090（24 GB），我们可以并行处理最多256个物体。平均而言，前向传递需要\({0.13}\mathrm{\;s}\)来完成一个完整的Replica [44]场景，使用Mask3D [40]进行3D实例分割需要\({0.3}\mathrm{\;s}\)，每个LM优化步骤需要少于\(1\mathrm{\;s}\)，更多细节见补充材料。</p></div><p>Compactness-Accuracy Trade-off. The hyperparameter \({\lambda }_{\text{par }}\) controls the trade-off between reconstruction accuracy and representation compactness (see Eq. 4). We evaluate this trade-off quantitatively by first training the model with \({\lambda }_{\text{par }} = {0.1}\) for 500 epochs,followed by fine-tuning for 100 epochs with varying \({\lambda }_{\text{par }}\) values. Fig. 10 shows the impact of \({\lambda }_{par}\) on Chamfer distance and the average number of predicted primitives. By adjusting \({\lambda }_{\text{par }}\) ,the model can smoothly balance compactness and accuracy, allowing for easy fine-tuning to meet target reconstruction quality. In our experiments,we use \({\lambda }_{\text{par }} = {0.6}\) ,which approximately corresponds to the intersection point of the two curves.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>紧凑性-精度权衡。超参数\({\lambda }_{\text{par }}\)控制重建精度与表示紧凑性之间的权衡（见公式4）。我们通过首先用\({\lambda }_{\text{par }} = {0.1}\)训练模型500个周期，然后用不同的\({\lambda }_{\text{par }}\)值微调100个周期，定量评估这一权衡。图10显示了\({\lambda }_{par}\)对Chamfer距离和预测原始体平均数量的影响。通过调整\({\lambda }_{\text{par }}\)，模型可以平滑地平衡紧凑性和精度，便于微调以满足目标重建质量。在我们的实验中，我们使用\({\lambda }_{\text{par }} = {0.6}\)，这大致对应于两条曲线的交点。</p></div><h2>5. Conclusion</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>5. 结论</h2></div><p>We proposed SUPERDEC, a method for deriving compact yet expressive 3D scene representations based on simple geometric primitives - specifically, superquadrics. Our model outperforms prior primitive-based methods and generalizes well to out-of-category classes. We further demonstrated the potential of the resulting \(3\mathrm{D}\) scene representation for various applications in robotics, and as a geometric prompt for diffusion-based image generation. While this is only a first step towards more compact, geometry-aware 3D scene representations, we anticipate broader applications and expect to see further research in this direction.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>我们提出了SUPERDEC，这是一种基于简单几何原始体（特别是超二次体）推导紧凑而富有表现力的3D场景表示的方法。我们的模型优于先前的基于原始体的方法，并且对类别外类具有良好的泛化能力。我们进一步展示了所得到的\(3\mathrm{D}\)场景表示在机器人技术等各种应用中的潜力，以及作为基于几何的扩散图像生成的提示。虽然这只是朝着更紧凑、几何感知的3D场景表示迈出的第一步，但我们预期会有更广泛的应用，并期待在这一方向上进行进一步研究。</p></div><p>Acknowledgemnts. Elisabetta Fedele is a doctoral research fellow at the ETH AI Center and is supported by the Swiss National Science Foundation (SNSF) Advanced Grant 216260 (Beyond Frozen Worlds: Capturing Functional 3D Digital Twins from the Real World), and an SNSF Mobility Grant. Francis Engelmann is supported by an SNSF PostDoc.mobilty grant. References</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>致谢。Elisabetta Fedele是ETH人工智能中心的博士研究员，获得瑞士国家科学基金会(SNSF)的高级资助216260（超越冰冻世界：从现实世界捕捉功能性3D数字双胞胎）和SNSF流动性资助的支持。Francis Engelmann获得SNSF博士后流动性资助的支持。参考文献</p></div><p>[1] Stephan Alaniz, Massimiliano Mancini, and Zeynep Akata. Iterative superquadric recomposition of \(3\mathrm{\;d}\) objects from multiple viewsd. In International Conference on Computer Vision (ICCV), 2023. 1</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[1] Stephan Alaniz, Massimiliano Mancini, 和 Zeynep Akata. 从多个视角对\(3\mathrm{\;d}\)对象进行迭代超四面体重组. 在国际计算机视觉会议(ICCV), 2023. 1</p></div><p>[2] Alan H. Barr. Superquadrics and angle-preserving transformations. IEEE Computer Graphics and Applications, 1981. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[2] Alan H. Barr. 超四面体和角保持变换. IEEE计算机图形与应用, 1981. 2</p></div><p>[3] Junhao Cai, Jingcheng Su, Zida Zhou, Hui Cheng, Qifeng Chen, and Michael Yu Wang. Volumetric-based contact point detection for 7-dof grasping. In Conference on Robot Learning (CoRL), 2022. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[3] Junhao Cai, Jingcheng Su, Zida Zhou, Hui Cheng, Qifeng Chen, 和 Michael Yu Wang. 基于体积的7自由度抓取接触点检测. 在机器人学习会议(CoRL), 2022. 6</p></div><p>[4] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical report, 2015. 2, 4, 5, 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[4] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, 和 Fisher Yu. ShapeNet: 一个信息丰富的3D模型库. 技术报告, 2015. 2, 4, 5, 8</p></div><p>[5] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention Mask Transformer for Universal Image Segmentation. International Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[5] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, 和 Rohit Girdhar. 用于通用图像分割的掩码注意力掩码变换器. 国际计算机视觉与模式识别会议(CVPR), 2021. 3</p></div><p>[6] Bowen Cheng, Alexander G. Schwing, and Alexander Kir-illov. Per-Pixel Classification is Not All You Need for Semantic Segmentation. In International Conference on Neural Information Processing Systems (NeurIPS), 2021. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[6] Bowen Cheng, Alexander G. Schwing, 和 Alexander Kirillov. 每像素分类并不是语义分割所需的一切. 在国际神经信息处理系统会议(NeurIPS), 2021. 3</p></div><p>[7] Laurent Chevalier, Fabrice Jaillet, and Atilla Baskurt. Segmentation and superquadric modeling of \(3\mathrm{\;d}\) objects. In International Conference in Central Europe on Computer Graphics and Visualization, 2003. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[7] Laurent Chevalier, Fabrice Jaillet, 和 Atilla Baskurt. 对\(3\mathrm{\;d}\)对象的分割和超四面体建模. 在中欧计算机图形与可视化国际会议, 2003. 2</p></div><p>[8] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction. In \({Eu}\) - ropean Conference on Computer Vision (ECCV), 2016. 4</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[8] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, 和 Silvio Savarese. 3D-R2N2: 单视图和多视图3D对象重建的统一方法. 在\({Eu}\) - 欧洲计算机视觉会议(ECCV), 2016. 4</p></div><p>[9] Alexandros Delitzas, Ayca Takmaz, Federico Tombari, Robert Sumner, Marc Pollefeys, and Francis Engelmann. SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2024.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[9] Alexandros Delitzas, Ayca Takmaz, Federico Tombari, Robert Sumner, Marc Pollefeys, 和 Francis Engelmann. SceneFun3D: 在3D场景中对功能性和可用性的细粒度理解. 在国际计算机视觉与模式识别会议(CVPR), 2024.</p></div><p>[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies., 2019. 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova. Bert: 用于语言理解的深度双向变换器的预训练. 在2019年北美计算语言学协会会议：人类语言技术的会议论文集, 2019. 8</p></div><p>[11] Hao-Shu Fang, Chenxi Wang, Hongjie Fang, Minghao Gou, Jirong Liu, Hengxu Yan, Wenhai Liu, Yichen Xie, and Cewu Lu. Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. IEEE Transactions on Robotics, 2023. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[11] Hao-Shu Fang, Chenxi Wang, Hongjie Fang, Minghao Gou, Jirong Liu, Hengxu Yan, Wenhai Liu, Yichen Xie, 和 Cewu Lu. Anygrasp: 在空间和时间域中进行稳健高效的抓取感知. IEEE机器人学报, 2023. 6</p></div><p>[12] B. Faverjon and J. Ponce. On Computing Two-finger Force-closure Grasps of Curved 2D Objects. In International Conference on Robotics and Automation (ICRA), 1991. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[12] B. Faverjon 和 J. Ponce. 计算曲线2D对象的两指力闭合抓取. 在国际机器人与自动化会议(ICRA), 1991. 6</p></div><p>[13] Qiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, et al. Conceptgraphs: Open-vocabulary \(3\mathrm{\;d}\) scene graphs for perception and planning. In International Conference on Robotics and Automation (ICRA), 2024. 1</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[13] Qiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, 等. Conceptgraphs: 用于感知和规划的开放词汇\(3\mathrm{\;d}\)场景图. 在国际机器人与自动化会议(ICRA), 2024. 1</p></div><p>[14] Abdullah Hamdi, Luke Melas-Kyriazi, Jinjie Mai, Guocheng Qian, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, and Andrea Vedaldi. Ges: Generalized exponential splatting for efficient radiance field rendering. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[14] Abdullah Hamdi, Luke Melas-Kyriazi, Jinjie Mai, Guocheng Qian, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, 和 Andrea Vedaldi. Ges: 用于高效辐射场渲染的广义指数溅射. 在国际计算机视觉与模式识别会议(CVPR), 2024. 2</p></div><p>[15] Jan Held, Renaud Vandeghen, Abdullah Hamdi, Adrien Deli'ege, Anthony Cioppa, Silvio Giancola, Andrea Vedaldi, Bernard Ghanem, and Marc Van Droogenbroeck. 3D convex splatting: Radiance field rendering with 3D smooth convexes. ArXiv, 2024. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[15] Jan Held, Renaud Vandeghen, Abdullah Hamdi, Adrien Deli'ege, Anthony Cioppa, Silvio Giancola, Andrea Vedaldi, Bernard Ghanem, 和 Marc Van Droogenbroeck. 3D 凸点云渲染：使用 3D 平滑凸体的辐射场渲染。ArXiv, 2024. 2</p></div><p>[16] Ales Jaklic, Ales Leonardis, and Franc Solina. Segmentation and recovery of superquadrics. Springer Science &#x26; Business Media, 2000. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[16] Ales Jaklic, Ales Leonardis, 和 Franc Solina. 超二次体的分割与恢复。Springer Science &#x26; Business Media, 2000. 3</p></div><p>[17] Rasmus Ramsbøl Jensen, A. Dahl, George Vogiatzis, Engil Tola, and Henrik Aanæs. Large scale multi-view stereopsis evaluation. International Conference on Computer Vision and Pattern Recognition (CVPR), 2014. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[17] Rasmus Ramsbøl Jensen, A. Dahl, George Vogiatzis, Engil Tola, 和 Henrik Aanæs. 大规模多视角立体视觉评估。国际计算机视觉与模式识别会议 (CVPR), 2014. 2</p></div><p>[18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. \(\;3\mathrm{\;d}\) gaussian splatting for real-time radiance field rendering. ACM Transactions On Graphics (TOG), 2023. 1, 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, 和 George Drettakis. \(\;3\mathrm{\;d}\) 高斯点云渲染用于实时辐射场渲染。ACM 图形学会会刊 (TOG), 2023. 1, 2</p></div><p>[19] Sebastian Koch, Narunas Vaskevicius, Mirco Colosi, Pedro Hermosilla, and Timo Ropinski. Open3DSG: Open-vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-set Relationships. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[19] Sebastian Koch, Narunas Vaskevicius, Mirco Colosi, Pedro Hermosilla, 和 Timo Ropinski. Open3DSG：来自点云的开放词汇 3D 场景图，具有可查询对象和开放集关系。在国际计算机视觉与模式识别会议 (CVPR), 2024. 1</p></div><p>[20] Kenneth Levenberg. A method for the solution of certain non-linear problems in least squares. Quarterly of Applied Mathematics, 1944. 3, 4</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[20] Kenneth Levenberg. 一种解决某些非线性最小二乘问题的方法。应用数学季刊, 1944. 3, 4</p></div><p>[21] Lei Li and Angela Dai. Genzi: Zero-shot 3d human-scene interaction generation. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[21] Lei Li 和 Angela Dai. Genzi：零样本 3D 人类场景交互生成。在国际计算机视觉与模式识别会议 (CVPR), 2024. 1</p></div><p>[22] Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft ras-terizer: A differentiable renderer for image-based 3d reasoning. International Conference on Computer Vision (ICCV), 2019. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[22] Shichen Liu, Tianye Li, Weikai Chen, 和 Hao Li. Soft ras-terizer：一种用于基于图像的 3D 推理的可微渲染器。国际计算机视觉会议 (ICCV), 2019. 2</p></div><p>[23] Weixiao Liu, Yuwei Wu, Sipu Ruan, and Gregory S. Chirikjian. Robust and accurate superquadric recovery: a probabilistic approach. International Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[23] Weixiao Liu, Yuwei Wu, Sipu Ruan, 和 Gregory S. Chirikjian. 鲁棒且准确的超二次体恢复：一种概率方法。国际计算机视觉与模式识别会议 (CVPR), 2022. 5</p></div><p>[24] Weixiao Liu, Yuwei Wu, Sipu Ruan, and Gregory S Chirikjian. Robust and Accurate Superquadric Recovery: A Probabilistic Approach. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 1, 2, 5, 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[24] Weixiao Liu, Yuwei Wu, Sipu Ruan, 和 Gregory S Chirikjian. 鲁棒且准确的超二次体恢复：一种概率方法。在国际计算机视觉与模式识别会议 (CVPR), 2022. 1, 2, 5, 6</p></div><p>[25] Weixiao Liu, Yuwei Wu, Sipu Ruan, and Gregory S Chirikjian. Marching-primitives: Shape abstraction from signed distance function. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[25] Weixiao Liu, Yuwei Wu, Sipu Ruan, 和 Gregory S Chirikjian. Marching-primitives：从符号距离函数中提取形状抽象。在国际计算机视觉与模式识别会议 (CVPR), 2023. 2, 6</p></div><p>[26] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-Voxel CNN for Efficient 3D Deep Learning. In International Conference on Neural Information Processing Systems (NeurIPS), 2019. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[26] Zhijian Liu, Haotian Tang, Yujun Lin, 和 Song Han. 点-体素 CNN 用于高效的 3D 深度学习。在神经信息处理系统国际会议 (NeurIPS), 2019. 3</p></div><p>[27] Jeffrey Mahler, Matthew Matl, Vishal Satish, Michael Danielczuk, Bill DeRose, Stephen McKinley, and Ken Goldberg. Learning ambidextrous robot grasping policies. Science Robotics, 2019. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[27] Jeffrey Mahler, Matthew Matl, Vishal Satish, Michael Danielczuk, Bill DeRose, Stephen McKinley, 和 Ken Goldberg. 学习双手机器人抓取策略。科学机器人, 2019. 6</p></div><p>[28] Donald W. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal of The Society for Industrial and Applied Mathematics, 1963. 3, 4</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[28] Donald W. Marquardt. 一种用于非线性参数最小二乘估计的算法。工业与应用数学学会杂志, 1963. 3, 4</p></div><p>[29] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision (ECCV), 2020. 1</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[29] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, 和 Ren Ng. Nerf：将场景表示为神经辐射场以进行视图合成。在欧洲计算机视觉会议 (ECCV), 2020. 1</p></div><p>[30] A.T. Miller, S. Knoop, H.I. Christensen, and P.K. Allen. Automatic Grasp Planning Using Shape Primitives. In International Conference on Robotics and Automation (ICRA), 2003. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[30] A.T. Miller, S. Knoop, H.I. Christensen, 和 P.K. Allen. 使用形状原语的自动抓取规划。在国际机器人与自动化会议 (ICRA), 2003. 6</p></div><p>[31] Tom Monnier, Jake Austin, Angjoo Kanazawa, Alexei A. Efros, and Mathieu Aubry. Differentiable blocks world: Qualitative 3d decomposition by rendering primitives. International Conference on Neural Information Processing Systems (NeurIPS), 2023. 1, 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[31] Tom Monnier, Jake Austin, Angjoo Kanazawa, Alexei A. Efros, 和 Mathieu Aubry. 可微分块世界：通过渲染原语进行定性3D分解。国际神经信息处理系统会议（NeurIPS），2023。1, 2</p></div><p>[32] Despoina Paschalidou, Ali Osman Ulusoy, and Andreas Geiger. Superquadrics revisited: Learning \(3\mathrm{\;d}\) shape parsing beyond cuboids. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 1, 2, 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[32] Despoina Paschalidou, Ali Osman Ulusoy, 和 Andreas Geiger. 超椭球体的再探讨：学习\(3\mathrm{\;d}\)形状解析超越立方体。在国际计算机视觉与模式识别会议（CVPR），2019。1, 2, 5</p></div><p>[33] Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, and Thomas Funkhouser. OpenScene: 3D Scene Understanding with Open Vocabularies. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[33] Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, 和 Thomas Funkhouser. OpenScene：使用开放词汇的3D场景理解。在国际计算机视觉与模式识别会议（CVPR），2023。1</p></div><p>[34] Alex P Pentland. Parts: structured descriptions of shape. In Proceedings of the Fifth AAAI National Conference on Artificial Intelligence, 1986. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[34] Alex P Pentland. 部件：形状的结构化描述。在第五届美国人工智能国家会议论文集，1986。2</p></div><p>[35] Maurizio Pilu and Robert B Fisher. Equal-distance sampling of superellipse models. In British Machine Vision Conference (BMVC), 1995. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[35] Maurizio Pilu 和 Robert B Fisher. 超椭圆模型的等距采样。在英国机器视觉会议（BMVC），1995。3</p></div><p>[36] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-net++: Deep hierarchical feature learning on point sets in a metric space. International Conference on Neural Information Processing Systems (NeurIPS), 2017. 4</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[36] Charles R Qi, Li Yi, Hao Su, 和 Leonidas J Guibas. Point-net++：在度量空间中的点集深度层次特征学习。国际神经信息处理系统会议（NeurIPS），2017。4</p></div><p>[37] Michaël Ramamonjisoa, Sinisa Stekovic, and Vincent Lep-etit. MonteBoxFinder: Detecting and Filtering Primitives to Fit a Noisy Point Cloud. In European Conference on Computer Vision (ECCV), 2022. 6, 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[37] Michaël Ramamonjisoa, Sinisa Stekovic, 和 Vincent Lep-etit. MonteBoxFinder：检测和过滤原语以适应噪声点云。在欧洲计算机视觉会议（ECCV），2022。6, 3</p></div><p>[38] Aaron Ray, Christopher Bradley, Luca Carlone, and Nicholas Roy. Task and motion planning in hierarchical 3d scene graphs. arXiv preprint arXiv:2403.08094, 2024. 1</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[38] Aaron Ray, Christopher Bradley, Luca Carlone, 和 Nicholas Roy. 在层次3D场景图中的任务和运动规划。arXiv预印本arXiv:2403.08094，2024。1</p></div><p>[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. International Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, 和 Björn Ommer. 使用潜在扩散模型进行高分辨率图像合成。国际计算机视觉与模式识别会议（CVPR），2022。7</p></div><p>[40] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d: Mask transformer for \(3\mathrm{\;d}\) semantic instance segmentation. International Conference on Robotics and Automation (ICRA), 2022. 2, 3, 4,5,6,7,8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[40] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, 和 Bastian Leibe. Mask3d：用于\(3\mathrm{\;d}\)语义实例分割的掩码变换器。国际机器人与自动化会议（ICRA），2022。2, 3, 4,5,6,7,8</p></div><p>[41] Jonas Schult, Sam Tsai, Lukas Höllein, Bichen Wu, Jialiang Wang, Chih-Yao Ma, Kunpeng Li, Xiaofang Wang, Felix Wimbauer, Zijian He, et al. Controlroom3d: Room generation using semantic proxy rooms. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[41] Jonas Schult, Sam Tsai, Lukas Höllein, Bichen Wu, Jialiang Wang, Chih-Yao Ma, Kunpeng Li, Xiaofang Wang, Felix Wimbauer, Zijian He, 等. Controlroom3d：使用语义代理房间生成房间。在国际计算机视觉与模式识别会议（CVPR），2024。1</p></div><p>[42] Jaidev Shriram, Alex Trevithick, Lingjie Liu, and Ravi Ra-mamoorthi. Realmdreamer: Text-driven 3d scene generation with inpainting and depth diffusion. arXiv preprint arXiv:2404.07199, 2024. 1</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[42] Jaidev Shriram, Alex Trevithick, Lingjie Liu, 和 Ravi Ra-mamoorthi. Realmdreamer：通过修复和深度扩散进行文本驱动的3D场景生成。arXiv预印本arXiv:2404.07199，2024。1</p></div><p>[43] Franc Solina and Ruzena Bajcsy. Recovery of parametric models from range images: The case for superquadrics with global deformations. Transactions on Pattern Analysis and Machine Intelligence (PAMI), 1990. 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[43] Franc Solina 和 Ruzena Bajcsy. 从范围图像恢复参数模型：超椭圆体与全局变形的案例。模式分析与机器智能汇刊（PAMI），1990。2</p></div><p>[44] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Yuheng Ren, Shobhit Verma, Anton Clarkson, Ming Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke Malte Strasdat, Renzo De Nardi, Michael Goesele, S. Lovegrove, and Richard A. Newcombe. The replica dataset: A digital replica of indoor spaces. ArXiv, 2019. 2, 4, 5, 7, 8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[44] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Yuheng Ren, Shobhit Verma, Anton Clarkson, Ming Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke Malte Strasdat, Renzo De Nardi, Michael Goesele, S. Lovegrove, 和 Richard A. Newcombe. 复制数据集：室内空间的数字复制品。ArXiv，2019。2, 4, 5, 7, 8</p></div><p>[45] Matan Sudry, Tom Jurgenson, Aviv Tamar, and Erez Karpas. Hierarchical planning for rope manipulation using knot theory and a learned inverse model. In Conference on Robot Learning, 2023. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[45] Matan Sudry, Tom Jurgenson, Aviv Tamar, 和 Erez Karpas. 使用结理论和学习的逆模型进行绳索操作的层次规划。在机器人学习会议，2023。6</p></div><p>[46] Martin Sundermeyer, Arsalan Mousavian, Rudolph Triebel, and Dieter Fox. Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes. In International Conference on Robotics and Automation (ICRA), 2021. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[46] 马丁·桑德迈耶（Martin Sundermeyer）、阿萨兰·穆萨维安（Arsalan Mousavian）、鲁道夫·特里贝尔（Rudolph Triebel）和迪特·福克斯（Dieter Fox）。接触抓取网：在杂乱场景中高效生成6自由度抓取。国际机器人与自动化会议（ICRA），2021年。6</p></div><p>[47] Ayça Takmaz, Elisabetta Fedele, Robert W. Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. Open-Mask3D: Open-Vocabulary 3D Instance Segmentation. In International Conference on Neural Information Processing Systems (NeurIPS), 2023. 1, 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[47] 艾查·塔克马兹（Ayça Takmaz）、埃莉萨贝塔·费德莱（Elisabetta Fedele）、罗伯特·W·萨姆纳（Robert W. Sumner）、马克·波莱费斯（Marc Pollefeys）、费德里科·汤巴里（Federico Tombari）和弗朗西斯·恩格尔曼（Francis Engelmann）。开放掩模3D：开放词汇3D实例分割。国际神经信息处理系统会议（NeurIPS），2023年。1, 7</p></div><p>[48] Xun Tu and Karthik Desingh. Superq-grasp: Superquadrics-based grasp pose estimation on larger objects for mobile-manipulation. arXiv, 2024. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[48] 许图（Xun Tu）和卡尔提克·德辛（Karthik Desingh）。超四面体抓取：基于超四面体的大型物体抓取姿态估计，用于移动操作。arXiv，2024年。6</p></div><p>[49] Shubham Tulsiani, Hao Su, Leonidas J. Guibas, Alexei A. Efros, and Jitendra Malik. Learning shape abstractions by assembling volumetric primitives. International Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 1, 2</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[49] 舒巴姆·图尔萨尼（Shubham Tulsiani）、郝苏（Hao Su）、利奥尼达斯·J·吉巴斯（Leonidas J. Guibas）、阿列克谢·A·埃夫罗斯（Alexei A. Efros）和吉滕德拉·马利克（Jitendra Malik）。通过组装体积原语学习形状抽象。计算机视觉与模式识别国际会议（CVPR），2017年。1, 2</p></div><p>[50] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine learning research, 2008.8</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[50] 劳伦斯·范德马滕（Laurens Van der Maaten）和杰弗里·辛顿（Geoffrey Hinton）。使用t-SNE可视化数据。机器学习研究杂志，2008年。8</p></div><p>[51] Erik Roeland van Dop and Paulus P.L. Regtien. Fitting undeformed superquadrics to range data: improving model recovery and classification. International Conference on Computer Vision and Pattern Recognition (CVPR), 1998. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[51] 埃里克·罗兰德·范·多普（Erik Roeland van Dop）和保罗斯·P·L·雷赫廷（Paulus P.L. Regtien）。将未变形的超四面体拟合到范围数据：改善模型恢复和分类。计算机视觉与模式识别国际会议（CVPR），1998年。3</p></div><p>[52] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In International Conference on Neural Information Processing Systems (NeurIPS), 2017. 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[52] 阿希什·瓦斯瓦尼（Ashish Vaswani）、诺亚·M·沙泽尔（Noam M. Shazeer）、尼基·帕尔马尔（Niki Parmar）、雅各布·乌斯科雷特（Jakob Uszkoreit）、利昂·琼斯（Llion Jones）、艾登·N·戈麦斯（Aidan N. Gomez）、卢卡斯·凯瑟（Lukasz Kaiser）和伊利亚·波洛苏金（Illia Polosukhin）。注意力机制是你所需要的一切。国际神经信息处理系统会议（NeurIPS），2017年。3</p></div><p>[53] Giulia Vezzani, Ugo Pattacini, and Lorenzo Natale. A grasping approach based on superquadric models. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 1579-1586. IEEE, 2017. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[53] 朱莉亚·维扎尼（Giulia Vezzani）、乌戈·帕塔奇尼（Ugo Pattacini）和洛伦佐·纳塔莱（Lorenzo Natale）。基于超四面体模型的抓取方法。2017年IEEE国际机器人与自动化会议（ICRA），第1579-1586页。IEEE，2017年。7</p></div><p>[54] Chenxi Wang, Hao-Shu Fang, Minghao Gou, Hongjie Fang, Jin Gao, and Cewu Lu. Graspness Discovery in Clutters for Fast and Accurate Grasp Detection. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 6</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[54] 陈曦·王（Chenxi Wang）、郝树·方（Hao-Shu Fang）、明浩·苟（Minghao Gou）、洪杰·方（Hongjie Fang）、金·高（Jin Gao）和策武·卢（Cewu Lu）。在杂乱中发现抓取性，以实现快速准确的抓取检测。计算机视觉与模式识别国际会议（CVPR），2021年。6</p></div><p>[55] Kaizhi Yang and Xuejin Chen. Unsupervised learning for cuboid shape abstraction via joint segmentation from point clouds. ACM Transactions On Graphics (TOG), 2021. 1, 2, 3, 5</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[55] 杨凯之（Kaizhi Yang）和陈雪金（Xuejin Chen）。通过从点云的联合分割进行无监督学习以实现立方体形状抽象。ACM图形学会会刊（TOG），2021年。1, 2, 3, 5</p></div><p>[56] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: A high-fidelity dataset of \(3\mathrm{\;d}\) indoor scenes. International Conference on Computer Vision (ICCV), 2023. 2, 4, 6, 1, 3</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[56] 钱丹·耶什万特（Chandan Yeshwanth）、刘月成（Yueh-Cheng Liu）、马蒂亚斯·尼斯纳（Matthias Nießner）和安吉拉·戴（Angela Dai）。ScanNet++：高保真度的室内场景数据集。计算机视觉国际会议（ICCV），2023年。2, 4, 6, 1, 3</p></div><p>[57] Chenyangguang Zhang, Alexandros Delitzas, Fangjinhua Wang, Ruida Zhang, Xiangyang Ji, Marc Pollefeys, and Francis Engelmann. Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 1</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[57] 张晨阳光（Chenyangguang Zhang）、亚历山德罗斯·德利扎斯（Alexandros Delitzas）、方金华（Fangjinhua Wang）、张瑞达（Ruida Zhang）、姜向阳（Xiangyang Ji）、马克·波莱费斯（Marc Pollefeys）和弗朗西斯·恩格尔曼（Francis Engelmann）。开放词汇功能性3D场景图，用于现实世界的室内空间。计算机视觉与模式识别国际会议（CVPR），2025年。1</p></div><p>[58] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. International Conference on Computer Vision (ICCV), 2023. 7</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>[58] 张吕敏（Lvmin Zhang）、饶安怡（Anyi Rao）和马尼什·阿格拉瓦拉（Maneesh Agrawala）。为文本到图像扩散模型添加条件控制。计算机视觉国际会议（ICCV），2023年。7</p></div><h1>SuperDEC: 3D Scene Decomposition with Superquadric Primitives</h1><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h1>SuperDEC：使用超四面体原语的3D场景分解</h1></div><p>Supplementary Material</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>补充材料</p></div><!-- Media --><!-- figureText: 0.08 Out-of-Category In-Category 6 8 10 Number of LM Optimization Rounds L2 Loss 0.07 0.06 2 --><img src="https://cdn.noedgeai.com/bo_d16410bef24c73d1lfag_11.jpg?x=262&#x26;y=366&#x26;w=511&#x26;h=339&#x26;r=0"><p>Figure 11. LM optimization experiment. We show how LM optimization improves results in terms of L2 Chamfer distance across a variable number of rounds. We report results both for in-category experiments and out-of-category ones.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>图11. LM优化实验。我们展示了LM优化如何在不同轮次中改善L2 Chamfer距离的结果。我们报告了类别内实验和类别外实验的结果。</p></div><!-- Media --><h2>6. Additional Results</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>6. 额外结果</h2></div><p>Does LM improve our final predictions? In our approach we use LM optimization as a post processing step. In this experiment (Fig. 11) we want to assess how a different number of LM optimization rounds affects the final predictions in terms of L2 Chamfer Distance. In order to evaluate this aspect, we report L2 loss after different numbers of LM optimization steps, evaluating both in-category and out-of-category. From this experiment we can notice two main aspects. Firstly, we see that it leads to larger improvements in the out-of-category rather than in the in-category one. This is probably due to the less accurate initial predictions of our feedforward model in this setting and it shows that our optimization step can be used to decrease the gap between in-category and out-category. Secondly, we see that even if LM optimization improves our final predictions, it does not lead to substantial improvements. This suggests that the solutions predicted by our method are located in local minima and that a diverse type of optimization should be resorted to improve the predictions further.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>语言模型（LM）是否改善了我们的最终预测？在我们的方法中，我们将语言模型优化作为后处理步骤。在这个实验中（图11），我们想评估不同数量的语言模型优化轮次如何影响最终预测的L2 Chamfer距离。为了评估这一方面，我们报告了在不同数量的语言模型优化步骤后的L2损失，评估了类别内和类别外的情况。从这个实验中我们可以注意到两个主要方面。首先，我们看到它在类别外的改进大于类别内的改进。这可能是由于我们前馈模型在这种设置下的初始预测不够准确，表明我们的优化步骤可以用来缩小类别内和类别外之间的差距。其次，我们看到即使语言模型优化改善了我们的最终预测，但并没有带来实质性的改进。这表明我们的方法预测的解决方案位于局部最小值，并且应该采用多样化的优化类型以进一步改善预测。</p></div><p>Why superquadrics? While our architecture can be easily adapted to segment and predict in an unsupervised manner other types of geometric primitives - in SUPERDEC we decided to use superquadrics. When looking for a suitable geometric primitive for our approach we were keeping in mind two main criteria. First, we wanted the primitive to be represented by a compact parameterization so that it can be described by only using a few parameters. Second, we wanted the representation to be expressive, in order to be able to describe real-world objects by only using a few primitives. Inspired by 3DGS [18], the first parameterization we took into consideration were the ellipsoids. Ellip-</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>为什么选择超二次体？虽然我们的架构可以很容易地适应以无监督的方式对其他类型的几何原语进行分割和预测，但在SUPERDEC中我们决定使用超二次体。在寻找适合我们方法的几何原语时，我们考虑了两个主要标准。首先，我们希望原语能够通过紧凑的参数化表示，以便仅使用少量参数进行描述。其次，我们希望表示能够富有表现力，以便能够仅使用少量原语描述现实世界的物体。受到3DGS [18]的启发，我们考虑的第一个参数化是椭球体。椭球体</p></div><p>soids have a very compact parameterization as their shape can be represented using the following implicit equation:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>的参数化非常紧凑，因为它们的形状可以通过以下隐式方程表示：</p></div><p></p>\[f\left( \mathbf{x}\right)  = {\left( \frac{x}{{s}_{x}}\right) }^{2} + {\left( \frac{y}{{s}_{y}}\right) }^{2} + {\left( \frac{z}{{s}_{z}}\right) }^{2} = 1,\]<p></p><p>where the only free variables are \({s}_{x},{s}_{y},{s}_{z}\) ,which are the lengths of the three main semi-axis. However, if we start thinking about which objects and object parts can be effectively fitted using a single ellipsoid, we realize that their representational capabilities are not enough. In order to obtain higher representational capabilities while still keeping a simple representation, a natural extension are generalized ellipsoids. In this representation, we not only allow the length of the semi-axis to be variable, but their roundness controlled by the three exponents, which previously were fixed to 2. In that way, we obtain the following implicit function:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>其中唯一的自由变量是\({s}_{x},{s}_{y},{s}_{z}\)，即三个主要半轴的长度。然而，如果我们开始思考哪些物体和物体部分可以有效地用单个椭球体拟合，我们会意识到它们的表示能力不足。为了获得更高的表示能力，同时保持简单的表示，自然的扩展是广义椭球体。在这种表示中，我们不仅允许半轴的长度可变，还允许通过三个指数控制其圆度，这些指数之前固定为2。这样，我们得到了以下隐式函数：</p></div><p></p>\[f\left( \mathbf{x}\right)  = {\left( \frac{\left| x\right| }{{s}_{x}}\right) }^{{e}_{1}} + {\left( \frac{\left| y\right| }{{s}_{y}}\right) }^{{e}_{2}} + {\left( \frac{\left| z\right| }{{s}_{z}}\right) }^{{e}_{3}} = 1.\]<p></p><p>Using generalized ellipsoids with high exponents it becomes possible to also represent cuboidal shapes. While having suitable representational capabilities, these primitives do not allow to compute distance to their surface in a closed form, a property which can be extremely useful for various downstream applications. This drawback is overcome by superquadrics, at the cost of one less degree of freedom, which however does not substantially impact expressivity. Unlike generalized ellipsoids, which assign a separate roundness parameter to each axis, superquadrics share the same roundness for the \(x\) and \(y\) axes while allowing a distinct parameter for the \(z\) axis. Their shape is represented in implicit form by the equation:</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>使用具有高指数的广义椭球体也可以表示立方体形状。虽然具有合适的表示能力，但这些原语不允许以封闭形式计算到其表面的距离，这一特性对于各种下游应用可能非常有用。这个缺点被超二次体克服，代价是减少一个自由度，但这并不会实质性影响表现力。与为每个轴分配单独圆度参数的广义椭球体不同，超二次体在\(x\)和\(y\)轴上共享相同的圆度，同时允许\(z\)轴有一个独特的参数。它们的形状通过以下隐式形式表示：</p></div><p></p>\[f\left( \mathbf{x}\right)  = {\left( {\left( \frac{x}{{s}_{x}}\right) }^{\frac{2}{{\epsilon }_{2}}} + {\left( \frac{y}{{s}_{y}}\right) }^{\frac{2}{{\epsilon }_{2}}}\right) }^{\frac{{\epsilon }_{2}}{{\epsilon }_{1}}} + {\left( \frac{z}{{s}_{z}}\right) }^{\frac{2}{{\epsilon }_{1}}} = 1,\]<p></p><p>and the euclidean radial distance to their surface can be computed in closed form, as shown in Eq. 2.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>并且可以以封闭形式计算到其表面的欧几里得径向距离，如公式2所示。</p></div><h2>7. Robot Experiment</h2><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h2>7. 机器人实验</h2></div><p>In this section we introduce the key methods and parameters used in our robot experiments. We also present more detailed qualitative and quantitative evaluation results.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在本节中，我们介绍了在机器人实验中使用的关键方法和参数。我们还呈现了更详细的定性和定量评估结果。</p></div><h3>7.1. Setup</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>7.1. 设置</h3></div><p>For path planning in both ScanNet++ [56] and real-world scenarios, we use the Python binding of the Open Motion Planning Library (OMPL). The state space is defined as a 3D RealVectorStateSpace, with boundaries extracted from the 3D bounding box of the input point cloud. We employ a sampling-based planner (RRT*), setting a maximum planning time of 2 seconds per start-goal pair.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在ScanNet++ [56]和现实场景中的路径规划中，我们使用Open Motion Planning Library (OMPL)的Python绑定。状态空间被定义为3D RealVectorStateSpace，其边界从输入点云的3D边界框中提取。我们采用基于采样的规划器（RRT*），为每对起点和目标设置最大规划时间为2秒。</p></div><p>In ScanNet++ scenes, the occupancy grid and voxel grid are both set to a \({10}\mathrm{\;{cm}}\) resolution,with voxels generated from the original point cloud. The collision radius is 25 \(\mathrm{{cm}}\) . For dense occupancy grid planning,we enforce an additional constraint in the validity checking to ensure that paths remain within \({25}\mathrm{\;{cm}}\) of free space,preventing them from extending outside the scene or penetrating walls. And the planned occupancy grid path serves as a reference for computing relative path optimality in our evaluation. Start and goal points are sampled within a \({0.4}\mathrm{\;m} - {0.6}\mathrm{\;m}\) height range in free space, as most furniture and objects are within this range. This allows for a fair evaluation of how different representations capture collisions for valid path planning. During evaluation, we further validate paths by interpolating them into \(5\mathrm{\;{cm}}\) waypoint intervals. Each way-point is checked against the occupancy grid to ensure that its nearest occupied grid is beyond \({25}\mathrm{\;{cm}}\) and its nearest free grid is within \({25}\mathrm{\;{cm}}\) . A path is considered unsuccessful if more than \({10}\%\) of waypoints fail this check. This soft constraint accounts for the sampling-based nature of RRT*, which does not enforce voxel-level validity but instead checks waypoints along the tree structure, leading to occasional minor violations. In the real-world path planning,we set the collision radius to \({60}\mathrm{\;{cm}}\) to approximate the size of the Boston Dynamics Spot robot. Spot follows the planned path using its Python API for execution.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在ScanNet++场景中，占用网格和体素网格均设置为\({10}\mathrm{\;{cm}}\)分辨率，体素由原始点云生成。碰撞半径为25 \(\mathrm{{cm}}\)。对于密集占用网格规划，我们在有效性检查中强制执行额外约束，以确保路径保持在\({25}\mathrm{\;{cm}}\)的自由空间内，防止其延伸到场景外或穿透墙壁。规划的占用网格路径作为我们评估中计算相对路径最优性的参考。起点和目标点在自由空间内的\({0.4}\mathrm{\;m} - {0.6}\mathrm{\;m}\)高度范围内进行采样，因为大多数家具和物体都在此范围内。这允许公平评估不同表示如何捕捉有效路径规划的碰撞。在评估过程中，我们通过将路径插值到\(5\mathrm{\;{cm}}\)航点间隔进一步验证路径。每个航点都与占用网格进行检查，以确保其最近的占用网格超出\({25}\mathrm{\;{cm}}\)，而其最近的自由网格在\({25}\mathrm{\;{cm}}\)之内。如果超过\({10}\%\)的航点未通过此检查，则认为路径不成功。这个软约束考虑了RRT*的基于采样的特性，它不强制执行体素级有效性，而是沿着树结构检查航点，导致偶尔出现轻微违规。在现实世界的路径规划中，我们将碰撞半径设置为\({60}\mathrm{\;{cm}}\)，以近似波士顿动力Spot机器人的大小。Spot使用其Python API执行规划路径。</p></div><p>For grasping in real-world experiments, we use the superquadric-library to compute single-hand grasping poses based on superquadric parameters. The process begins by identifying the object of interest and its corresponding su-perquadric decomposition. One of the superquadrics is selected and fed into the grasping estimator. To execute the grasp, the robot first navigates to the object's location during the planning stage. Then, using its built-in inverse kinematics planner and controller, the robot moves its end-effector to the estimated grasping pose for object manipulation.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在现实世界实验中进行抓取时，我们使用超椭球库根据超椭球参数计算单手抓取姿势。该过程首先识别感兴趣的物体及其相应的超椭球分解。选择其中一个超椭球并输入到抓取估计器中。为了执行抓取，机器人首先在规划阶段导航到物体的位置。然后，使用其内置的逆向运动学规划器和控制器，机器人将其末端执行器移动到估计的抓取姿势以进行物体操作。</p></div><h3>7.2. Planning Results</h3><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><h3>7.2. 规划结果</h3></div><p>In Tab. 4 we report the complete planning results on 15 Scannet++ [56] scenes.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>在表4中，我们报告了15个ScanNet++ [56]场景的完整规划结果。</p></div><!-- Media --><table><tbody><tr><td rowspan="2">Method</td><td colspan="4">0a76e06478</td><td colspan="4">0c6c7145ba</td><td colspan="4">0f0191b10b</td><td colspan="4">\( 1\mathrm{a}8\mathrm{e}0\mathrm{d}{78}\mathrm{c}0 \)</td><td colspan="4">1a130d092a</td></tr><tr><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td></tr><tr><td>Occupancy</td><td>0.05</td><td>100</td><td>1.00</td><td>960KB</td><td>0.06</td><td>100</td><td>1.00</td><td>667KB</td><td>0.06</td><td>100</td><td>1.00</td><td>1031KB</td><td>0.05</td><td>100</td><td>1.00</td><td>926KB</td><td>0.05</td><td>100</td><td>1.00</td><td>803KB</td></tr><tr><td>PointCloud</td><td>0.07</td><td>86</td><td>0.98</td><td>18MB</td><td>0.09</td><td>91</td><td>0.99</td><td>12MB</td><td>0.03</td><td>77</td><td>0.99</td><td>19MB</td><td>0.05</td><td>91</td><td>0.99</td><td>18MB</td><td>0.05</td><td>89</td><td>0.98</td><td>18MB</td></tr><tr><td>Voxels</td><td>0.03</td><td>100</td><td>0.97</td><td>91KB</td><td>0.03</td><td>100</td><td>1.00</td><td>65KB</td><td>0.03</td><td>100</td><td>0.99</td><td>99KB</td><td>0.03</td><td>100</td><td>1.01</td><td>91KB</td><td>0.03</td><td>100</td><td>1.09</td><td>99KB</td></tr><tr><td>Cuboids [37]</td><td>0.11</td><td>32</td><td>0.98</td><td>22KB</td><td>0.10</td><td>18</td><td>1.02</td><td>19KB</td><td>0.14</td><td>85</td><td>1.03</td><td>34KB</td><td>0.10</td><td>50</td><td>1.06</td><td>21KB</td><td>0.12</td><td>79</td><td>1.00</td><td>27KB</td></tr><tr><td>SUPERDEC</td><td>0.17</td><td>100</td><td>0.99</td><td>52KB</td><td>0.16</td><td>100</td><td>0.97</td><td>48KB</td><td>0.17</td><td>92</td><td>0.94</td><td>51KB</td><td>0.14</td><td>91</td><td>0.99</td><td>39KB</td><td>0.13</td><td>100</td><td>0.98</td><td>35KB</td></tr><tr><td rowspan="2">Method</td><td colspan="4">0a76e06478</td><td colspan="4">0b031f3119</td><td colspan="4">0dce89ab21</td><td colspan="4">0e350246d4</td><td colspan="4">0eba3981e9</td></tr><tr><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td></tr><tr><td>Occupancy</td><td>0.05</td><td>100</td><td>1.00</td><td>916KB</td><td>0.06</td><td>100</td><td>1.00</td><td>1760KB</td><td>0.05</td><td>100</td><td>1.00</td><td>1070KB</td><td>0.06</td><td>100</td><td>1.00</td><td>366KB</td><td>0.06</td><td>100</td><td>1.00</td><td>473KB</td></tr><tr><td>PointCloud</td><td>0.05</td><td>86</td><td>1.02</td><td>18MB</td><td>0.06</td><td>96</td><td>1.04</td><td>25MB</td><td>0.05</td><td>84</td><td>1.13</td><td>19MB</td><td>0.06</td><td>88</td><td>1.22</td><td>10MB</td><td>0.14</td><td>80</td><td>0.98</td><td>45MB</td></tr><tr><td>Voxels</td><td>0.03</td><td>100</td><td>1.01</td><td>99KB</td><td>0.03</td><td>100</td><td>1.00</td><td>160KB</td><td>0.03</td><td>100</td><td>1.19</td><td>104KB</td><td>0.03</td><td>100</td><td>1.00</td><td>51KB</td><td>0.03</td><td>100</td><td>1.12</td><td>199KB</td></tr><tr><td>Cuboid[37]</td><td>0.14</td><td>71</td><td>1.12</td><td>32KB</td><td>0.11</td><td>78</td><td>1.03</td><td>24KB</td><td>0.11</td><td>35</td><td>1.00</td><td>23KB</td><td>0.09</td><td>62</td><td>1.00</td><td>15KB</td><td>0.17</td><td>87</td><td>1.17</td><td>41KB</td></tr><tr><td>SuperDec</td><td>0.16</td><td>86</td><td>1.17</td><td>46KB</td><td>0.16</td><td>93</td><td>0.98</td><td>46KB</td><td>0.13</td><td>100</td><td>1.07</td><td>33KB</td><td>0.15</td><td>88</td><td>1.22</td><td>40KB</td><td>0.19</td><td>57</td><td>1.10</td><td>58KB</td></tr><tr><td rowspan="2">Method</td><td colspan="4">7cd2ac43b4</td><td colspan="4">1841a0b525</td><td colspan="4">25927bb04c</td><td colspan="4">e0abd740ba</td><td colspan="4">0f25f24a4f</td></tr><tr><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td><td>Time(ms)</td><td>Suc.(%)</td><td>Opt.</td><td>Mem.</td></tr><tr><td>Occupancy</td><td>0.06</td><td>100</td><td>1.00</td><td>1241KB</td><td>0.05</td><td>100</td><td>1.00</td><td>1053KB</td><td>0.06</td><td>100</td><td>1.00</td><td>407KB</td><td>0.06</td><td>100</td><td>1.00</td><td>554KB</td><td>0.05</td><td>100</td><td>1</td><td>7MB</td></tr><tr><td>PointCloud</td><td>0.05</td><td>100</td><td>1.09</td><td>25MB</td><td>0.04</td><td>89</td><td>0.98</td><td>16MB</td><td>0.06</td><td>100</td><td>1.01</td><td>11MB</td><td>0.06</td><td>97</td><td>0.93</td><td>16MB</td><td>0.07</td><td>61</td><td>0.97</td><td>99MB</td></tr><tr><td>Voxels</td><td>0.03</td><td>100</td><td>1.00</td><td>137KB</td><td>0.03</td><td>100</td><td>0.98</td><td>82KB</td><td>0.03</td><td>83</td><td>1.04</td><td>51KB</td><td>0.03</td><td>100</td><td>1.04</td><td>83KB</td><td>0.03</td><td>96</td><td>0.96</td><td>617KB</td></tr><tr><td>Cuboid[37]</td><td>0.21</td><td>80</td><td>1.04</td><td>57KB</td><td>x</td><td>X</td><td>✘</td><td>15KB</td><td>0.09</td><td>87</td><td>0.96</td><td>17KB</td><td>0.07</td><td>52</td><td>1.04</td><td>11KB</td><td>✘</td><td>✘</td><td>✘</td><td>✘</td></tr><tr><td>SuperDec</td><td>0.15</td><td>100</td><td>1.05</td><td>45KB</td><td>0.10</td><td>94</td><td>0.87</td><td>18KB</td><td>0.17</td><td>83</td><td>1.30</td><td>53KB</td><td>0.12</td><td>100</td><td>0.87</td><td>27KB</td><td>0.21</td><td>57</td><td>0.82</td><td>71KB</td></tr></tbody></table><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><table><tbody><tr><td rowspan="2">方法</td><td colspan="4">0a76e06478</td><td colspan="4">0c6c7145ba</td><td colspan="4">0f0191b10b</td><td colspan="4">\( 1\mathrm{a}8\mathrm{e}0\mathrm{d}{78}\mathrm{c}0 \)</td><td colspan="4">1a130d092a</td></tr><tr><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td></tr><tr><td>占用率</td><td>0.05</td><td>100</td><td>1.00</td><td>960KB</td><td>0.06</td><td>100</td><td>1.00</td><td>667KB</td><td>0.06</td><td>100</td><td>1.00</td><td>1031KB</td><td>0.05</td><td>100</td><td>1.00</td><td>926KB</td><td>0.05</td><td>100</td><td>1.00</td><td>803KB</td></tr><tr><td>点云</td><td>0.07</td><td>86</td><td>0.98</td><td>18MB</td><td>0.09</td><td>91</td><td>0.99</td><td>12MB</td><td>0.03</td><td>77</td><td>0.99</td><td>19MB</td><td>0.05</td><td>91</td><td>0.99</td><td>18MB</td><td>0.05</td><td>89</td><td>0.98</td><td>18MB</td></tr><tr><td>体素</td><td>0.03</td><td>100</td><td>0.97</td><td>91KB</td><td>0.03</td><td>100</td><td>1.00</td><td>65KB</td><td>0.03</td><td>100</td><td>0.99</td><td>99KB</td><td>0.03</td><td>100</td><td>1.01</td><td>91KB</td><td>0.03</td><td>100</td><td>1.09</td><td>99KB</td></tr><tr><td>立方体 [37]</td><td>0.11</td><td>32</td><td>0.98</td><td>22KB</td><td>0.10</td><td>18</td><td>1.02</td><td>19KB</td><td>0.14</td><td>85</td><td>1.03</td><td>34KB</td><td>0.10</td><td>50</td><td>1.06</td><td>21KB</td><td>0.12</td><td>79</td><td>1.00</td><td>27KB</td></tr><tr><td>超级解码</td><td>0.17</td><td>100</td><td>0.99</td><td>52KB</td><td>0.16</td><td>100</td><td>0.97</td><td>48KB</td><td>0.17</td><td>92</td><td>0.94</td><td>51KB</td><td>0.14</td><td>91</td><td>0.99</td><td>39KB</td><td>0.13</td><td>100</td><td>0.98</td><td>35KB</td></tr><tr><td rowspan="2">方法</td><td colspan="4">0a76e06478</td><td colspan="4">0b031f3119</td><td colspan="4">0dce89ab21</td><td colspan="4">0e350246d4</td><td colspan="4">0eba3981e9</td></tr><tr><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td></tr><tr><td>占用率</td><td>0.05</td><td>100</td><td>1.00</td><td>916KB</td><td>0.06</td><td>100</td><td>1.00</td><td>1760KB</td><td>0.05</td><td>100</td><td>1.00</td><td>1070KB</td><td>0.06</td><td>100</td><td>1.00</td><td>366KB</td><td>0.06</td><td>100</td><td>1.00</td><td>473KB</td></tr><tr><td>点云</td><td>0.05</td><td>86</td><td>1.02</td><td>18MB</td><td>0.06</td><td>96</td><td>1.04</td><td>25MB</td><td>0.05</td><td>84</td><td>1.13</td><td>19MB</td><td>0.06</td><td>88</td><td>1.22</td><td>10MB</td><td>0.14</td><td>80</td><td>0.98</td><td>45MB</td></tr><tr><td>体素</td><td>0.03</td><td>100</td><td>1.01</td><td>99KB</td><td>0.03</td><td>100</td><td>1.00</td><td>160KB</td><td>0.03</td><td>100</td><td>1.19</td><td>104KB</td><td>0.03</td><td>100</td><td>1.00</td><td>51KB</td><td>0.03</td><td>100</td><td>1.12</td><td>199KB</td></tr><tr><td>立方体[37]</td><td>0.14</td><td>71</td><td>1.12</td><td>32KB</td><td>0.11</td><td>78</td><td>1.03</td><td>24KB</td><td>0.11</td><td>35</td><td>1.00</td><td>23KB</td><td>0.09</td><td>62</td><td>1.00</td><td>15KB</td><td>0.17</td><td>87</td><td>1.17</td><td>41KB</td></tr><tr><td>超级解码</td><td>0.16</td><td>86</td><td>1.17</td><td>46KB</td><td>0.16</td><td>93</td><td>0.98</td><td>46KB</td><td>0.13</td><td>100</td><td>1.07</td><td>33KB</td><td>0.15</td><td>88</td><td>1.22</td><td>40KB</td><td>0.19</td><td>57</td><td>1.10</td><td>58KB</td></tr><tr><td rowspan="2">方法</td><td colspan="4">7cd2ac43b4</td><td colspan="4">1841a0b525</td><td colspan="4">25927bb04c</td><td colspan="4">e0abd740ba</td><td colspan="4">0f25f24a4f</td></tr><tr><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td><td>时间（毫秒）</td><td>成功率（%）</td><td>优化</td><td>内存</td></tr><tr><td>占用率</td><td>0.06</td><td>100</td><td>1.00</td><td>1241KB</td><td>0.05</td><td>100</td><td>1.00</td><td>1053KB</td><td>0.06</td><td>100</td><td>1.00</td><td>407KB</td><td>0.06</td><td>100</td><td>1.00</td><td>554KB</td><td>0.05</td><td>100</td><td>1</td><td>7MB</td></tr><tr><td>点云</td><td>0.05</td><td>100</td><td>1.09</td><td>25MB</td><td>0.04</td><td>89</td><td>0.98</td><td>16MB</td><td>0.06</td><td>100</td><td>1.01</td><td>11MB</td><td>0.06</td><td>97</td><td>0.93</td><td>16MB</td><td>0.07</td><td>61</td><td>0.97</td><td>99MB</td></tr><tr><td>体素</td><td>0.03</td><td>100</td><td>1.00</td><td>137KB</td><td>0.03</td><td>100</td><td>0.98</td><td>82KB</td><td>0.03</td><td>83</td><td>1.04</td><td>51KB</td><td>0.03</td><td>100</td><td>1.04</td><td>83KB</td><td>0.03</td><td>96</td><td>0.96</td><td>617KB</td></tr><tr><td>立方体[37]</td><td>0.21</td><td>80</td><td>1.04</td><td>57KB</td><td>x</td><td>X</td><td>✘</td><td>15KB</td><td>0.09</td><td>87</td><td>0.96</td><td>17KB</td><td>0.07</td><td>52</td><td>1.04</td><td>11KB</td><td>✘</td><td>✘</td><td>✘</td><td>✘</td></tr><tr><td>超级解码</td><td>0.15</td><td>100</td><td>1.05</td><td>45KB</td><td>0.10</td><td>94</td><td>0.87</td><td>18KB</td><td>0.17</td><td>83</td><td>1.30</td><td>53KB</td><td>0.12</td><td>100</td><td>0.87</td><td>27KB</td><td>0.21</td><td>57</td><td>0.82</td><td>71KB</td></tr></tbody></table></div><p>Table 4. Path Planning Results. We show results of path planning for different ScanNet++ [56] scenes, whose ids are reported on the top. PointCloud method uses dense point clouds from ScanNet++, all other methods process the same input point cloud. Time refers to average execution time of the validity-check function during the sampling stage of planning. Success rate (Suc.) is calculated after excluding trials where no representation could generate valid path due to randomness of start and goal sampling. The Cuboid method encounters an out-of-memory failure when fitting scene \({0f25f24a4f}\) due to its large scale,and fails to find any valid path in scene 1841aob525.</p><div style="background-color: rgb(240, 240, 240); padding: 2px 8px; border-radius: 4px; margin: 8px 0px; visibility: visible;"><p>表4. 路径规划结果。我们展示了不同ScanNet++ [56] 场景的路径规划结果，场景的ID在顶部列出。PointCloud方法使用来自ScanNet++的稠密点云，所有其他方法处理相同的输入点云。时间指的是在规划的采样阶段，验证检查函数的平均执行时间。成功率（Suc.）是在排除由于起始和目标采样的随机性而无法生成有效路径的试验后计算得出的。Cuboid方法在拟合场景\({0f25f24a4f}\)时由于其大规模而遇到内存不足的失败，并且在场景1841aob525中未能找到任何有效路径。</p></div><!-- Media -->
      </body>
    </html>
  