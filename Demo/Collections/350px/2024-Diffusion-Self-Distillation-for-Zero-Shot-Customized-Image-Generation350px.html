
    <!DOCTYPE html>
    <html lang="en">
      <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>2024-Diffusion Self-Distillation for Zero-Shot Customized Image Generation</title>
        <style>
            * {
              padding: 0;
              margin: 0;
              box-sizing: border-box;
            }
             html {
              line-height: 1.15; /* 1 */
              -webkit-text-size-adjust: 100%; /* 2 */
            }
            body {
              padding: 20px 350px;
              margin: 0;
            }
            main {
              display: block;
            }
            h1 {
              font-size: 2em;
              margin: 0.67em 0;
            }
            hr {
              box-sizing: content-box; /* 1 */
              height: 0; /* 1 */
              overflow: visible; /* 2 */
            }
            pre {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            a {
              background-color: transparent;
            }
            abbr[title] {
              border-bottom: none; /* 1 */
              text-decoration: underline; /* 2 */
              text-decoration: underline dotted; /* 2 */
            }
            b,
            strong {
              font-weight: bolder;
            }
            code,
            kbd,
            samp {
              font-family: monospace, monospace; /* 1 */
              font-size: 1em; /* 2 */
            }
            small {
              font-size: 80%;
            }
            sub,
            sup {
              font-size: 75%;
              line-height: 0;
              position: relative;
              vertical-align: baseline;
            }
            sub {
              bottom: -0.25em;
            }
            sup {
              top: -0.5em;
            }
            img {
              border-style: none;
            }
            button,
            input,
            optgroup,
            select,
            textarea {
              font-family: inherit; /* 1 */
              font-size: 100%; /* 1 */
              line-height: 1.15; /* 1 */
              margin: 0; /* 2 */
            }
            button,
            input { /* 1 */
              overflow: visible;
            }
            button,
            select { /* 1 */
              text-transform: none;
            }
            button,
            [type="button"],
            [type="reset"],
            [type="submit"] {
              -webkit-appearance: button;
            }
            button::-moz-focus-inner,
            [type="button"]::-moz-focus-inner,
            [type="reset"]::-moz-focus-inner,
            [type="submit"]::-moz-focus-inner {
              border-style: none;
              padding: 0;
            }
            button:-moz-focusring,
            [type="button"]:-moz-focusring,
            [type="reset"]:-moz-focusring,
            [type="submit"]:-moz-focusring {
              outline: 1px dotted ButtonText;
            }
            fieldset {
              padding: 0.35em 0.75em 0.625em;
            }
            legend {
              box-sizing: border-box; /* 1 */
              color: inherit; /* 2 */
              display: table; /* 1 */
              max-width: 100%; /* 1 */
              padding: 0; /* 3 */
              white-space: normal; /* 1 */
            }
            progress {
              vertical-align: baseline;
            }
            textarea {
              overflow: auto;
            }
            [type="checkbox"],
            [type="radio"] {
              box-sizing: border-box; /* 1 */
              padding: 0; /* 2 */
            }
            [type="number"]::-webkit-inner-spin-button,
            [type="number"]::-webkit-outer-spin-button {
              height: auto;
            }
            [type="search"] {
              -webkit-appearance: textfield; /* 1 */
              outline-offset: -2px; /* 2 */
            }
            [type="search"]::-webkit-search-decoration {
              -webkit-appearance: none;
            }
            ::-webkit-file-upload-button {
              -webkit-appearance: button; /* 1 */
              font: inherit; /* 2 */
            }
            details {
              display: block;
            }
            summary {
              display: list-item;
            }
            [hidden] {
              display: none;
            }
             table {
                border-collapse: collapse;
                width: 100%;
                margin-top: 20px;
                margin-bottom: 20px;
              }
              table thead {
                background-color: #e5e5e5;
              }
              table td {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              table th {
                padding: 8px;
                /*background-color: #e5e5e5;*/
                border: 1px solid #ccc; /* 可选，添加边框样式 */
              }
              h1, h2, h3, h4, h5, h6 {
                margin-bottom: 20px;
              }
              p {
                margin-top: 20px;
                text-indent: 2em;
                margin-bottom: 20px;
              }
        </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      </head>
      <body>
        <div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="1" id="mark-fedfb333-bc46-4c11-8685-13c20500f3cd" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-0="0,0"><div style="height: auto;"><span style="display: inline;"><h1><div><div>Diffusion Self-Distillation for Zero-Shot Customized Image Generation<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">用于零样本定制图像生成的扩散自蒸馏方法</div></div></div></h1><div><br></div><div><div><div>Shengqu Cai Eric Ryan Chan Yunzhi Zhang<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">蔡盛曲 埃里克·瑞安·陈 张云志</div></div></div></div><div><br></div><div><div><div>Leonidas Guibas Jiajun Wu Gordon Wetzstein<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">利奥尼达斯·吉巴斯 吴佳俊 戈登·韦茨斯坦</div></div></div></div><div><br></div><div><div><div>Stanford University<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">斯坦福大学</div></div></div></div><div><br><!-- Media --><br><!-- figureText: make the character make the man make the character make the man make the man lean the older man tend to a vibrant, against a weathered is giving overgrown garden wooden railing over a passionate speech looking a stormy sea in a dimly lit, worn by a draped over abandoned silver haired model a vintage warehouse in a studio wooden chair casually sip a cup of rest on a chair paint a vibrant coffee standing in a dusty room sunset on a canvas in the moonlight hangs in a standing in perched atop dark, medieval a grand, a towering castle hallway gothic cathedral castle wall --><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_0.jpg?x=165&amp;y=604&amp;w=1467&amp;h=507&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_0.jpg?x=165&amp;y=604&amp;w=1467&amp;h=507&amp;r=0"><div><br></div><div><div><div>Figure 1. Given an input image, Diffusion Self-Distillation is a novel diffusion-based approach that generates diverse images that maintain the input's identity across various contexts. Unlike prior approaches that require fine-tuning or are limited to specific domains, Diffusion Self-Distillation offers instant customization without any additional inference-stage training, enabling precise control and editability in text-to-image diffusion models. This ability makes Diffusion Self-Distillation a valuable tool for general AI content creation.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图1. 给定一张输入图像，扩散自蒸馏是一种基于扩散的新颖方法，它能生成多样化的图像，且在各种场景下都能保持输入图像的特征。与之前需要微调或局限于特定领域的方法不同，扩散自蒸馏无需任何额外的推理阶段训练即可实现即时定制，使文本到图像的扩散模型能够进行精确控制和编辑。这种能力使扩散自蒸馏成为通用人工智能内容创作的宝贵工具。</div></div></div></div><div><br><!-- Media --><br></div><h2><div><div>Abstract<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">摘要</div></div></div></h2><div><br></div><div><div><div>Text-to-image diffusion models produce impressive results but are frustrating tools for artists who desire fine-grained control. For example, a common use case is to create images of a specific concept in novel contexts, i.e., "identity-preserving generation". This setting, along with many other tasks (e.g., relighting), is a natural fit for image + text-conditional generative models. However, there is insufficient high-quality paired data to train such a model directly. We propose Diffusion Self-Distillation, a method for using a pre-trained text-to-image model to generate its own dataset for text-conditioned image-to-image tasks. We first leverage a text-to-image diffusion model's in-context generation ability to create grids of images and curate a large paired dataset with the help of a vision-language model. We then fine-tune the text-to-image model into a text+image-to-image model using the curated paired dataset. We demonstrate that Diffusion Self-Distillation outperforms existing zero-shot methods and is competitive with per-instance tuning techniques on a wide range of identity-preserving generation tasks, without requiring test-time optimization. Project page: primecai.github.io/dsd.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">文本到图像的扩散模型取得了令人印象深刻的成果，但对于希望进行细粒度控制的艺术家来说，这些模型使用起来却不尽人意。例如，一个常见的应用场景是在新的场景中创建特定概念的图像，即“保持特征的生成”。这种场景以及许多其他任务（如重新打光）非常适合图像 + 文本条件生成模型。然而，没有足够的高质量配对数据来直接训练这样的模型。我们提出了扩散自蒸馏方法，这是一种利用预训练的文本到图像模型为文本条件的图像到图像任务生成自己的数据集的方法。我们首先利用文本到图像扩散模型的上下文生成能力创建图像网格，并在视觉 - 语言模型的帮助下精心整理出一个大型配对数据集。然后，我们使用整理好的配对数据集将文本到图像模型微调为文本 + 图像到图像模型。我们证明了扩散自蒸馏方法在一系列保持特征的生成任务上优于现有的零样本方法，并且在不需要测试时优化的情况下，与每个实例的微调技术具有竞争力。项目页面：primecai.github.io/dsd。</div></div></div></div><div><br></div><h2><div><div>1. Introduction<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">1. 引言</div></div></div></h2><div><br></div><div><div><div>In recent years, text-to-image diffusion models [24, 28, 29, 32] have set new standards in image synthesis, generating high-quality and diverse images from textual prompts. However, while their ability to generate images from text is impressive, these models often fall short in offering precise control, editability, and consistency-key features that are crucial for real-world applications. Text input alone can be insufficient to convey specific details, leading to variations that may not fully align with the user's intent, especially in scenarios that require faithful adaptation of a character or asset's identity across different contexts.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">近年来，文本到图像的扩散模型 [24, 28, 29, 32] 在图像合成方面树立了新的标准，能够根据文本提示生成高质量且多样化的图像。然而，尽管这些模型从文本生成图像的能力令人印象深刻，但它们在提供精确控制、可编辑性和一致性方面往往有所不足，而这些关键特性对于实际应用至关重要。仅靠文本输入可能不足以传达特定细节，从而导致生成的图像可能与用户的意图不完全一致，特别是在需要在不同场景中忠实保留角色或资产特征的情况下。</div></div></div></div><div><br></div><div><div><div>Maintaining the instance's identity is challenging, however. We distinguish structure-preserving edits, in which the target and source image share the general layout, but may differ in style, texture, or other local features, and identity-preserving edits, where assets are recognizably the same across target and source images despite potentially large-scale changes in image structure (Fig. 3). The latter task is a superset of the former and requires the model to have a significantly more profound understanding of the input image and concepts to extract and customize the desired identity. For example,image editing <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13140" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mn>2</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>22</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>43</mn></mrow></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow></math></mjx-assistive-mml></mjx-container> ,such as local content editing, re-lighting, and semantic image synthesis, etc. are all structure-preserving and identity-preserving edits, but novel-view synthesis and character-consistent generation under pose variations, are identity-preserving but not structure-preserving. We aim to address the general case, maintaining identity without constraining structure.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">然而，保持实例的特征具有挑战性。我们区分了保留结构的编辑和保持特征的编辑，在保留结构的编辑中，目标图像和源图像具有相同的总体布局，但在风格、纹理或其他局部特征上可能有所不同；而在保持特征的编辑中，尽管图像结构可能发生大规模变化，但目标图像和源图像中的资产仍能被识别为相同（图3）。后一种任务是前一种任务的超集，要求模型对输入图像和概念有更深刻的理解，以提取和定制所需的特征。例如，图像编辑 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13141" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mn>2</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>22</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>43</mn></mrow></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow></math></mjx-assistive-mml></mjx-container> ，如局部内容编辑、重新打光和语义图像合成等，都是保留结构和保持特征的编辑，而新视角合成和姿态变化下的角色一致生成则是保持特征但不保留结构的编辑。我们的目标是解决一般情况，即在不限制结构的情况下保持特征。</div></div></div></div><div><div><div></div></div></div></span></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="2" id="mark-1bc12ab6-1f32-4717-b6ad-3c97a00df3be" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-1="0,0"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-1="0,0"></paragraphpositioning></div></div></div><div><!-- Media --><br><!-- figureText: Data Generation "a grid of four images, all depicting a Prompt Source Image Noisy Target Image "A man traversing through a floating musical notes and vibrant colors patchify patchify Uncurated Transformer Block norm & mod. MLP MLP MLP Dense Attention com um país pelo pelo había Generated Grids More transformer blocks.. unpatchify Curated Pairs bearded man wearing a kimono. In the arrow. In the top right, the man is writing calligraphy. In the bottom left, "a bearded man the man is on a sailboat. In the bottom right, the man is using a calculator." Teacher vearing a kimono" Image "rainbow-colored "a grid of four images, all depicting a Caption mushroom" rainbow-colored mushroom. In the Database top left, the woman is using a bow and arrow. In the top right, the Reference Captions mushroom is writing calligraphy. In sailboat. In the bottom right, the mushroom is using a calculator.” Generated Grid Sampling Prompts Data Curation & Paired up Do these image pairs depict identical mai subject? VLM 63 Uncurated Generated Pairs Classifying Generated Pairs --><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_1.jpg?x=168&amp;y=196&amp;w=1463&amp;h=607&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_1.jpg?x=168&amp;y=196&amp;w=1463&amp;h=607&amp;r=0"><div><br></div><div><div><div>Figure 2. Overview of our pipeline. Left: the top shows our vanilla paired data generation wheel (Sec. 3.1). We first sample reference image captions from the LAION [33] dataset. These reference captions are parsed through an LLM to be translated into identity-preserved grid generation prompts (Sec. 3.1.2). We feed these enhanced prompts to a pretrained text-to-image diffusion model to sample potentially identity-preserved grids of images, which are then cropped and composed into vanilla image pairs (Sec. 3.1.1). On the bottom, we show our data curation pipeline (Sec. 3.1.3), where the vanilla image paired are fed into a VLM to classify whether they depict identical main subjects. This process mimics a human annotation/curation process while being fully automatic; we use the curated data as our final training data. Right: we extend the diffusion transformer model into an image-conditioned framework by treating the input image as the first frame of a two-frame sequence. The model generates both frames simultaneously—the first reconstructs the input, while the second is the edited output—allowing effective information exchange between the conditioning image and the desired output.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图2. 我们的流程概述。左：上方展示了我们的原始配对数据生成流程（第3.1节）。我们首先从LAION [33] 数据集中采样参考图像描述。这些参考描述通过大语言模型（LLM）解析，转换为保持特征的网格生成提示（第3.1.2节）。我们将这些增强后的提示输入到预训练的文本到图像扩散模型中，采样可能保持特征的图像网格，然后将其裁剪并组合成原始图像对（第3.1.1节）。下方展示了我们的数据整理流程（第3.1.3节），将原始图像对输入到视觉 - 语言模型（VLM）中，以分类它们是否描绘了相同的主要对象。这个过程模拟了人工标注/整理过程，同时完全自动化；我们将整理后的数据用作最终的训练数据。右：我们将扩散变压器模型扩展为图像条件框架，将输入图像视为两帧序列的第一帧。模型同时生成两帧——第一帧重建输入图像，而第二帧是编辑后的输出——允许条件图像和所需输出之间进行有效的信息交换。</div></div></div></div><div><br><!-- Media --><br></div><div><div><div>For structure-preserving edits, adding layers, as in Con-trolNet [43], introduces spatial conditioning controls but is limited to structure guidance and does not address consistent identity adaptation across diverse contexts. For identity-preserving edits, fine-tuning methods such as Dream-Booth [31] and LoRA [13] can improve consistency using a few reference samples but are time consuming and computationally intensive, requiring training for each reference. Zero-shot alternatives like IP-Adapter [42] and Instan-tID [37] offer faster solutions without the need for retraining but fall short in providing the desired level of consistency and customization; IP-Adapter [42] lacks full customization capabilities, and InstantID [37] is restricted to facial identity.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">对于保留结构的编辑，像ControlNet [43]那样添加层，引入了空间条件控制，但仅限于结构引导，无法解决在不同上下文环境下一致的身份适配问题。对于保留身份的编辑，诸如DreamBooth [31]和LoRA [13]等微调方法可以利用少量参考样本提高一致性，但耗时且计算量大，每个参考样本都需要进行训练。像IP - Adapter [42]和InstantID [37]这类零样本替代方法无需重新训练，提供了更快的解决方案，但在提供所需的一致性和定制化水平方面有所不足；IP - Adapter [42]缺乏完全定制的能力，而InstantID [37]仅限于面部身份。</div></div></div></div><div><br></div><div><div><div>In this paper, we propose a novel approach called Diffusion Self-Distillation, designed to address the core challenge of zero-shot instant customization and adaptation of any character or asset in text-to-image diffusion models. We identify the primary obstacle that hinders prior methods, such as IP-Adapter [42] and InstantID [37], from achieving better identity preservation or generalizing beyond facial contexts: the absence of large-scale paired datasets and corresponding supervised identity-preserving training pipelines. With recent advancements in foundational model capabilities, we are now positioned to exploit these strengths further. Specifically, we can generate consistent grids of identical characters or assets, opening a new pathway for customization that eliminates the need for pre-existing, handcrafted paired datasets-which are expensive and time consuming to collect. The ability to generate these consistent grids likely emerged from foundational model training on diverse datasets, including photo albums, mangas, and comics. Our approach harnesses Vision-Language Models (VLMs) to automatically curate many generated grids, producing a diverse set of grid images with consistent identity features across various contexts. This curated synthetic dataset then serves as the foundation for fine-tuning and adapting any identity, transforming the task of zero-shot customized image generation from unsupervised to supervised. Diffusion Self-Distillation offers transformative potential for applications like consistent character generation, camera control, relighting, and asset customization in fields such as comics and digital art. This flexibility allows artists to rapidly iterate and adapt their work, reducing effort and enhancing creative freedom, making Diffusion Self-Distillation a valuable tool for AI-generated content.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">在本文中，我们提出了一种名为扩散自蒸馏（Diffusion Self - Distillation）的新方法，旨在解决文本到图像扩散模型中对任何角色或资产进行零样本即时定制和适配这一核心挑战。我们发现了阻碍诸如IP - Adapter [42]和InstantID [37]等先前方法实现更好的身份保留或在面部上下文之外进行泛化的主要障碍：缺乏大规模的配对数据集以及相应的有监督身份保留训练流程。随着基础模型能力的最新进展，我们现在能够进一步利用这些优势。具体而言，我们可以生成相同角色或资产的一致网格，为定制开辟了一条新途径，消除了对预先存在的、手工制作的配对数据集的需求，而收集这些数据集既昂贵又耗时。生成这些一致网格的能力可能源于基础模型在包括相册、漫画和连环画等多样化数据集上的训练。我们的方法利用视觉 - 语言模型（VLMs）自动整理许多生成的网格，生成一组在各种上下文环境中具有一致身份特征的多样化网格图像。这个经过整理的合成数据集随后成为微调并适配任何身份的基础，将零样本定制图像生成任务从无监督转变为有监督。扩散自蒸馏在漫画和数字艺术等领域的一致角色生成、相机控制、重新打光和资产定制等应用方面具有变革性潜力。这种灵活性使艺术家能够快速迭代和调整他们的作品，减少工作量并增强创作自由，使扩散自蒸馏成为人工智能生成内容的宝贵工具。</div></div></div></div><div><br></div><div><div><div>We summarize our contributions as follows:<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们将贡献总结如下：</div></div></div></div><div><br><ul><li>We propose Diffusion Self-Distillation, a zero-shot</li></ul><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><ul><li>我们提出了扩散自蒸馏（Diffusion Self - Distillation），一种零样本</li></ul></div></div><div><div><div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="3" id="mark-144c1c07-f43c-4e60-8403-770b9b829d03" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-2="0,0"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-2="0,0"></paragraphpositioning></div></div></div><div><!-- Media --><br><!-- figureText: Structure-preserving Edit Identity-preserving Edit input image perched atop a towering stack of books in a futuristic city (ours) input image make it a wizard (SeedEdit) --><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_2.jpg?x=165&amp;y=202&amp;w=703&amp;h=192&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_2.jpg?x=165&amp;y=202&amp;w=703&amp;h=192&amp;r=0"><div><br></div><div><div><div>Figure 3. Difference between structure-preserving and identity-preserving edits. In structure-preserving editing, the main structures of the image are preserved, and only local edits or stylizations are performed. In identity-preserving editing, the global structure of the image may change radically.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图3. 保留结构编辑和保留身份编辑的区别。在保留结构编辑中，图像的主要结构得以保留，仅进行局部编辑或风格化处理。在保留身份编辑中，图像的全局结构可能会发生根本性变化。</div></div></div></div><div><br><!-- Media --><br></div><div><div><div>identity-preserving customized image generation model that scales to any instance under any context, with performances on par with inference-stage tuning methods;<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">保留身份的定制图像生成模型，该模型可在任何上下文环境下扩展到任何实例，其性能与推理阶段的调优方法相当；</div></div></div></div><div><br><ul><li>We provide a self-distillation pipeline to obtain identity-preserving data pairs purely from pretrained text-to-image diffusion models, LLMs, and VLMs, without any human effort involved in the entire data creation wheel;</li></ul><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><ul><li>我们提供了一个自蒸馏流程，仅从预训练的文本到图像扩散模型、大语言模型（LLMs）和视觉 - 语言模型（VLMs）中获取保留身份的数据对，整个数据创建过程无需人工干预；</li></ul></div><br><ul><li>We correspondingly design a unified architecture for image-to-image translation tasks involving both identity-and structure-preserving edits, including personalization, relighting, depth controls, and instruction following.</li></ul><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><ul><li>我们相应地设计了一个统一架构，用于涉及保留身份和保留结构编辑的图像到图像翻译任务，包括个性化、重新打光、深度控制和指令遵循。</li></ul></div><br></div><h2><div><div>2. Related work<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">2. 相关工作</div></div></div></h2><div><br></div><div><div><div>Recent advancements in diffusion models have underscored the need for enhanced control and customization in image-generation tasks. Various methods have been proposed to address these challenges through additional conditioning mechanisms, personalization, and rapid adaptation [26].<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">扩散模型的最新进展凸显了在图像生成任务中增强控制和定制的必要性。已经提出了各种方法，通过额外的条件机制、个性化和快速适配来应对这些挑战 [26]。</div></div></div></div><div><br></div><div><div><div>Control Mechanisms in Diffusion Models. To move beyond purely text-based controls, approaches like Control-Net [43] introduce spatial conditioning via inputs such as sketches, depth maps, and segmentation masks, enabling fine-grained structure control. ControlNet++ [19] refines this by enhancing the integration of spatial inputs for more nuanced control. Uni-ControlNet [44] unifies various control types within a single framework, standardizing the handling of diverse signals. T2I-Adapter [23] employs lightweight adapters to align pretrained models with external control signals without altering the core architecture. While these methods offer increased flexibility, they often focus on structural conditioning types such as depths and lack capabilities for concept extraction or identity preservation.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">扩散模型中的控制机制。为了超越纯粹基于文本的控制，像ControlNet [43]这样的方法通过草图、深度图和分割掩码等输入引入空间条件控制，实现细粒度的结构控制。ControlNet++ [19]通过增强空间输入的集成来改进这一点，以实现更细致的控制。Uni - ControlNet [44]在单一框架内统一了各种控制类型，规范了对不同信号的处理。T2I - Adapter [23]采用轻量级适配器，使预训练模型与外部控制信号对齐，而不改变核心架构。虽然这些方法提供了更大的灵活性，但它们通常侧重于深度等结构条件类型，缺乏概念提取或身份保留的能力。</div></div></div></div><div><br></div><div><div><div>Personalization and Fine-Tuning. Techniques like Dream-Booth [31] and LoRA [13] enhance the consistency and relevance of generated images by fine-tuning models with small sets of reference images. DreamBooth [31] personalizes models to maintain a subject's identity across different contexts, while LoRA [13] provides an efficient approach to fine-tuning large models without extensive retraining. However, these methods require multiple images and test-time optimization for each reference, which can be computationally expensive-especially with the exponential growth in model sizes (12 billion parameters for FLUX).<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">个性化和微调。像DreamBooth [31]和LoRA [13]这样的技术通过使用少量参考图像对模型进行微调，提高了生成图像的一致性和相关性。DreamBooth [31]对模型进行个性化处理，以在不同上下文环境中保持主体的身份，而LoRA [13]提供了一种在无需大量重新训练的情况下微调大型模型的高效方法。然而，这些方法每个参考样本都需要多张图像和测试时优化，这在计算上可能成本很高，尤其是随着模型规模呈指数级增长（如FLUX有120亿个参数）。</div></div></div></div><div><br></div><div><div><div>Zero-Shot and Fast Adaptation. IP-Adapter [42] incorporates image prompts into diffusion models using image em-beddings, allowing for generations that align closely with reference visuals. InstantID [37] ensures zero-shot face preservation, maintaining a subject's key features across various contexts. While effective as zero-shot methods without user training, IP-Adapter [42] struggles to adapt specific targets like unique characters or assets, and InstantID [37] is limited to facial identity preservation. IPAdapter-Instruct [30] enhances image-based conditioning with instruct prompts but relies heavily on specific instructions and task-specific pretrained models. Other methods, such as SuTI [7] and GDT [15], handcrafted corresponding datasets, which are expensive and challenging to collect and scale. Another work along this line is Subject-Diffusion [21], which uses segmentation masks to create synthetic data for training but is bounded by achieving only simple attributes and accessories copying and editing within input images.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">零样本和快速适应。IP适配器（IP-Adapter）[42]利用图像嵌入将图像提示融入扩散模型，从而实现与参考视觉效果紧密匹配的图像生成。即时身份识别（InstantID）[37]确保零样本面部特征保留，能在各种场景下保持主体的关键特征。虽然作为无需用户训练的零样本方法很有效，但IP适配器（IP-Adapter）[42]难以适应特定目标，如独特的角色或资产，而即时身份识别（InstantID）[37]仅限于面部身份保留。IP适配器指令（IPAdapter-Instruct）[30]通过指令提示增强了基于图像的条件控制，但严重依赖特定指令和特定任务的预训练模型。其他方法，如SuTI [7]和GDT [15]，需要手工制作相应的数据集，这既昂贵又难以收集和扩展。沿着这一思路的另一项工作是主体扩散（Subject-Diffusion）[21]，它使用分割掩码创建合成数据进行训练，但仅限于复制和编辑输入图像中的简单属性和配饰。</div></div></div></div><div><br></div><div><div><div>Existing methods contribute valuable advancements but often target specific domains or require user-stage tuning. Diffusion Self-Distillation bridges these gaps by offering a unified, zero-shot approach for consistent customization of characters and assets using minimal input. By leveraging self-distillation assisted by vision-language models, Diffusion Self-Distillation provides a comprehensive and adaptable solution for a wide range of creative applications.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">现有方法取得了有价值的进展，但通常针对特定领域或需要用户阶段的调整。扩散自蒸馏（Diffusion Self-Distillation）通过提供一种统一的零样本方法，利用最少的输入实现对角色和资产的一致定制，弥补了这些差距。通过利用视觉语言模型辅助的自蒸馏，扩散自蒸馏（Diffusion Self-Distillation）为广泛的创意应用提供了全面且适应性强的解决方案。</div></div></div></div><div><br></div><h2><div><div>3. Diffusion Self-Distillation<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3. 扩散自蒸馏</div></div></div></h2><div><br></div><div><div><div>We discover that recent text-to-image generation models offer the surprising ability to generate in-context, consistent image grids (see Fig. 2, left). Motivated by this insight, we develop a zero-shot adaptation network that offers fast, diverse, high-quality, and identity-preserving, i.e., consistent image generation conditioned on a reference image. For this purpose, we first generate and curate sets of images that exhibit the desired consistency using pretrained text-to-image diffusion models, large language models (LLMs), and vision-language models (VLMs) (Sec. 3.1). Then, we finetune the same pretrained diffusion model with these consistent image sets, employing our newly proposed parallel processing architecture (Sec. 3.2) to create a conditional model. By this end, Diffusion Self-Distillation finetunes a pretrained text-to-image diffusion model into a zero-shot customized image generator in a supervised manner.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们发现，最近的文本到图像生成模型具备令人惊讶的上下文内生成一致图像网格的能力（见图2，左）。受这一发现的启发，我们开发了一个零样本适应网络，该网络能够基于参考图像进行快速、多样、高质量且保留身份特征（即一致）的图像生成。为此，我们首先使用预训练的文本到图像扩散模型、大语言模型（LLM）和视觉语言模型（VLM）生成并筛选出具有所需一致性的图像集（3.1节）。然后，我们使用这些一致的图像集对相同的预训练扩散模型进行微调，采用我们新提出的并行处理架构（3.2节）创建一个条件模型。最终，扩散自蒸馏（Diffusion Self-Distillation）以有监督的方式将预训练的文本到图像扩散模型微调为零样本定制图像生成器。</div></div></div></div><div><br></div><h3><div><div>3.1. Generating a Pairwise Dataset<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.1. 生成成对数据集</div></div></div></h3><div><br></div><div><div><div>To create a pairwise dataset for supervised Diffusion Self-Distillation training, we leverage the emerging multi-image generation capabilities of pretrained text-to-image diffusion models to produce potentially consistent vanilla images (Sec. 3.1.1) created by LLM-generated prompts (Sec. 3.1.2). We then use VLMs to curate these vanilla samples, obtaining clean sets of images that share the desired identity consistency (Sec. 3.1.3). The data generation and curation pipeline is shown in Fig. 2, left.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">为了创建用于有监督扩散自蒸馏训练的成对数据集，我们利用预训练文本到图像扩散模型新兴的多图像生成能力，生成由大语言模型生成的提示所创建的潜在一致的原始图像（3.1.1节）。然后，我们使用视觉语言模型对这些原始样本进行筛选，获得具有所需身份一致性的干净图像集（3.1.3节）。数据生成和筛选流程如图2左所示。</div></div></div></div><div><div><div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="4" id="mark-c0931ba5-b257-42c4-a2b2-f7773fee290b" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-3="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div><paragraphpositioning data-position-3="0,0"></paragraphpositioning></div></div></div><h4><div><div>3.1.1. Vanilla Data Generation via Teacher Model<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.1.1. 通过教师模型生成原始数据</div></div></div></h4><div><br></div><div><div><div>To generate sets of images that fulfill the desired identity preservation, we prompt the teacher pretrained text-to-image diffusion model to create images containing multiple panels featuring the same subject with variations in expression, pose, lighting conditions, and more, for training purposes. Such prompting can be as simple as specifying the desired identity preservation in the output, such as " a grid of 4 images representing the same <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13186" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&lt;</mo></math></mjx-assistive-mml></mjx-container> object/character/scene/etc. &gt;", "an evenly separated 4 panels, depicting identical &lt; object/character/scene/etc. &gt;", etc. We additionally specify the expected content in each sub-image/panel. The full set of prompts is provided in our supplemental material Sec. A. Our analysis shows that current state-of-the-art text-to-image diffusion models (e.g., SD3 [8], DALL-E 3, FLUX) demonstrate this identity-preserving capability, likely emerging from their training data, which includes comics, mangas, photo albums, and video frames. Such in-context generation ability is crucial to our data generation wheel.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">为了生成满足所需身份保留的图像集，我们向教师预训练文本到图像扩散模型发出提示，要求其创建包含多个面板的图像，这些面板以同一主体为特征，但在表情、姿势、光照条件等方面有所变化，用于训练。这样的提示可以很简单，只需在输出中指定所需的身份保留，例如“一个包含4张代表同一<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13187" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&lt;</mo></math></mjx-assistive-mml></mjx-container>对象/角色/场景等的图像网格”，“一个均匀分隔的4个面板，描绘相同的&lt;对象/角色/场景等&gt;”等。我们还会指定每个子图像/面板中的预期内容。完整的提示集在我们的补充材料A节中提供。我们的分析表明，当前最先进的文本到图像扩散模型（如SD3 [8]、DALL - E 3、FLUX）展示了这种身份保留能力，这可能源于它们的训练数据，其中包括漫画、动漫、相册和视频帧。这种上下文内生成能力对我们的数据生成流程至关重要。</div></div></div></div><div><br></div><h4><div><div>3.1.2. Prompt Generation via LLMs<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.1.2. 通过大语言模型生成提示</div></div></div></h4><div><br></div><div><div><div>We rely on an LLM to "brainstorm" a large dataset of diverse prompts, from which we derive our image grid dataset. By defining a prompt structure, we prompt the LLM to produce text prompts that describe image grids. A challenge we encountered is that when prompted to create large sets of prompts, LLMs tend to produce prompts of low diversity. For example, we noticed that without additional guidance, GPT-4o has a strong preference for prompts with cars and robots, resulting in highly repetitive outputs. To address this issue, we utilize the available image captions in the LAION [33] dataset, feeding them into the LLM as content references. These references from real image captions dramatically improve the diversity of generated prompts. Optionally, we also use the LLM to filter these reference captions, ensuring they contain a clear target for identity preservation. We find that this significantly improves the hit rate of generating consistent multi-image outputs.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们依靠大语言模型“集思广益”生成大量多样的提示数据集，从中衍生出我们的图像网格数据集。通过定义提示结构，我们促使大语言模型生成描述图像网格的文本提示。我们遇到的一个挑战是，当要求大语言模型创建大量提示集时，它们往往生成多样性较低的提示。例如，我们注意到，如果没有额外的指导，GPT - 4o强烈偏好包含汽车和机器人的提示，导致输出高度重复。为了解决这个问题，我们利用LAION [33]数据集中现有的图像描述，将它们作为内容参考输入到大语言模型中。这些来自真实图像描述的参考显著提高了生成提示的多样性。可选地，我们还使用大语言模型对这些参考描述进行过滤，确保它们包含明确的身份保留目标。我们发现，这显著提高了生成一致多图像输出的成功率。</div></div></div></div><div><br></div><h4><div><div>3.1.3. Dataset Curation and Caption with VLMs<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.1.3. 数据集整理与视觉语言模型标注</div></div></div></h4><div><br></div><div><div><div>While the aforementioned data generation scheme provides identity-preserving multi-image samples of decent quality and quantity, these initial "uncurated" images tend to be noisy and unsuitable for direct use. Therefore, we leverage the strong capabilities of VLMs to curate a clean dataset. We extract pairs of images from the generated samples intended to preserve the identity and ask the VLM whether the two images depict the same object, character, scene, etc. We find that employing Chain-of-Thought prompting [38] is particularly helpful in this context. Specifically, we first prompt the VLM to identify the common object, character, or scene present in both images, then have it describe each one in detail, and finally analyze whether they are identical, providing a conclusive response. This process yields pairs of images that share the same identity.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">虽然上述数据生成方案提供了质量和数量都不错的保留身份信息的多图像样本，但这些初始的“未整理”图像往往存在噪声，不适合直接使用。因此，我们利用视觉语言模型（VLM）的强大能力来整理出一个干净的数据集。我们从生成的样本中提取旨在保留身份信息的图像对，并询问视觉语言模型这两张图像是否描绘了相同的物体、人物、场景等。我们发现，在这种情况下采用思维链提示法 [38] 特别有用。具体来说，我们首先提示视觉语言模型识别两张图像中共同存在的物体、人物或场景，然后让它详细描述每一张图像，最后分析它们是否相同，并给出明确的回答。这个过程会得到具有相同身份信息的图像对。</div></div></div></div><div><br></div><h3><div><div>3.2. Parallel Processing Architecture<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">3.2. 并行处理架构</div></div></div></h3><div><br></div><div><div><div>We desire a conditional architecture suitable for general image-to-image tasks, including transformations in which structure is preserved, and transformations in which concepts/identities are preserved but image structure is not. This is a challenging problem because it may necessitate the transfer of fine details without guaranteeing spatial correspondences. While the ControlNet [43] architecture is excellent at structure-preserving edits, such as depth-to-image or segmentation-map-to-image, it struggles to preserve details under more complex identity-preserving edits, where the source and target images are not pixel-aligned. On the other hand, IP-Adapter [42] can extract certain concepts, such as styles, from the input image. Still, it strongly relies on a task-specific image encoder and often fails to preserve more complex concepts and identities. Drawing inspiration from the success of multi-view and video diffusion models <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13188" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo fence="false" stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>3</mn><mo>−</mo></math></mjx-assistive-mml></mjx-container> <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13189" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>10</mn></mrow><mo>−</mo><mrow data-mjx-texclass="ORD"><mn>12</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>14</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>16</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>18</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>34</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>35</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>39</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>41</mn></mrow><mo fence="false" stretchy="false">]</mo></math></mjx-assistive-mml></mjx-container> ,we propose a simple yet effective method to extend the vanilla diffusion transformer model into an image-conditioned diffusion model. Specifically, we treat the input image as the first frame of a video and produce a two-frame video as output. The final loss is computed over the two-frame video, establishing an identity mapping for the first frame and a conditionally editing target for the second frame. Our architecture design allows generality for generic image-to-image translation tasks, since it enables effective information exchange between the two frames, allowing the model to capture complex semantics and perform sophisticated edits, as shown in Fig. 2, right.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">我们需要一种适用于通用图像到图像任务的条件架构，包括保留结构的转换，以及保留概念/身份但不保留图像结构的转换。这是一个具有挑战性的问题，因为它可能需要在不保证空间对应关系的情况下传递精细细节。虽然ControlNet [43] 架构在保留结构的编辑方面表现出色，例如深度图到图像或分割图到图像的转换，但在更复杂的保留身份信息的编辑中，当源图像和目标图像的像素不对齐时，它很难保留细节。另一方面，IP - 适配器 [42] 可以从输入图像中提取某些概念，如风格，但它严重依赖于特定任务的图像编码器，并且往往无法保留更复杂的概念和身份信息。受多视图和视频扩散模型 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13190" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="2"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo fence="false" stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>3</mn><mo>−</mo></math></mjx-assistive-mml></mjx-container> <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13191" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-texatom space="3" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>10</mn></mrow><mo>−</mo><mrow data-mjx-texclass="ORD"><mn>12</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>14</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>16</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>18</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>34</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>35</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>39</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>41</mn></mrow><mo fence="false" stretchy="false">]</mo></math></mjx-assistive-mml></mjx-container> 成功的启发，我们提出了一种简单而有效的方法，将普通的扩散变压器模型扩展为图像条件扩散模型。具体来说，我们将输入图像视为视频的第一帧，并生成一个两帧的视频作为输出。最终的损失是在两帧视频上计算的，为第一帧建立身份映射，为第二帧建立条件编辑目标。我们的架构设计使通用图像到图像的翻译任务具有通用性，因为它能够实现两帧之间的有效信息交换，使模型能够捕捉复杂的语义并进行精细的编辑，如图2右侧所示。</div></div></div></div><div><br></div><h2><div><div>4. Experiments<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">4. 实验</div></div></div></h2><div><br></div><div><div><div>Implementation details. We use FLUX1.0 DEV as both our teacher and student models, achieving self-distillation. For prompt generation, we use GPT-4o; for dataset curation and captioning, we use Gemini-1.5. We train all models on 8 NVIDIA H100 80GB GPUs with an effective batch size of 160 for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13192" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6B"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>100</mn></mrow><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">k</mi></mrow></math></mjx-assistive-mml></mjx-container> iterations,using AdamW optimizer [20] with a learning rate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13193" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-script style="vertical-align: 0.393em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mn>10</mn></mrow><mrow data-mjx-texclass="ORD"><mo>−</mo><mn>4</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> . Our parallel processing architecture uses LoRAs with rank 512 on the base model.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">实现细节。我们使用FLUX1.0 DEV作为教师模型和学生模型，实现自蒸馏。对于提示生成，我们使用GPT - 4o；对于数据集整理和标注，我们使用Gemini - 1.5。我们在8块NVIDIA H100 80GB GPU上训练所有模型，有效批量大小为160，进行 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13194" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6B"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mn>100</mn></mrow><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">k</mi></mrow></math></mjx-assistive-mml></mjx-container> 次迭代，使用AdamW优化器 [20]，学习率为 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13195" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-script style="vertical-align: 0.393em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow data-mjx-texclass="ORD"><mn>10</mn></mrow><mrow data-mjx-texclass="ORD"><mo>−</mo><mn>4</mn></mrow></msup></math></mjx-assistive-mml></mjx-container>。我们的并行处理架构在基础模型上使用秩为512的低秩自适应（LoRA）。</div></div></div></div><div><br></div><div><div><div>Datasets. Our final training dataset contains <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13196" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6B"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo><mrow data-mjx-texclass="ORD"><mn>400</mn></mrow><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">k</mi></mrow></math></mjx-assistive-mml></mjx-container> subject-consistent image pairs generated from our teacher model, FLUX1.0 DEV. Generating and curating the dataset is<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">数据集。我们最终的训练数据集包含从我们的教师模型FLUX1.0 DEV生成的 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13197" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6B"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo><mrow data-mjx-texclass="ORD"><mn>400</mn></mrow><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">k</mi></mrow></math></mjx-assistive-mml></mjx-container> 个主体一致的图像对。数据集的生成和整理</div></div></div></div><div><div><div></div></div></div></span></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="5" id="mark-7d409e95-2c81-43cb-8068-fe7335c4f129" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-4="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div><paragraphpositioning data-position-4="0,0"></paragraphpositioning></div></div></div><div><!-- Media --><br><!-- figureText: reference prompt Textual Inversion DreamBooth DreamBooth-LoRA BLIP-Diffusion Emu2 IP-Adapter IP-Adapter+ surreal landscape made of floating musical notes and vibrant colors A teddy bear conducting an orchestra of animated toys in a depicts the night sky, each star c shimmering thread A corgi navigating a tiny sailboat in a teacup sea A photograph of a motorcycle street at night, illuminated by street lights A photograph of a robot helping an elderly woman cross the street A photo capturing a leather handbag hanging on the shoulder of a woman walking through a bustling marketplace --><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_4.jpg?x=165&amp;y=194&amp;w=1469&amp;h=957&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_4.jpg?x=165&amp;y=194&amp;w=1469&amp;h=957&amp;r=0"><div><br></div><div><div><div>Figure 4. Qualitative comparison. Overall, our method achieves high subject identity preservation and prompt-aligned diversity while not suffering from a "copy-paste" effect, such as the results of IP-Adapter+ [42]. This is largely thanks to our supervised training pipeline, which alleviates the base model's in-context generation ability.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图4. 定性比较。总体而言，我们的方法在实现高主体身份保留和与提示对齐的多样性的同时，不会出现“复制粘贴”效应，例如IP - 适配器+ [42] 的结果。这在很大程度上归功于我们的有监督训练流程，它缓解了基础模型的上下文生成能力问题。</div></div></div></div><div><br><!-- Media --><br></div><div><div><div>fully automated and requires no human effort, so its size could be further scaled. We use the publicly available DreamBench++ [25] dataset and follow their protocols for evaluation. DreamBench++ [25] is a comprehensive and diverse dataset for evaluating personalized image generation, consisting of 150 high-quality images and 1,350 prompts-significantly more than previous benchmarks like DreamBench [31]. The dataset covers various categories such as animals, humans, objects, etc., including photoreal-istic and non-photorealistic images, with prompts designed to span different difficulty levels (simple/imaginative). In contrast, prompts are generated using GPT-4o and refined by human annotators to ensure diversity and ethical compliance.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">是完全自动化的，不需要人工干预，因此其规模可以进一步扩大。我们使用公开可用的DreamBench++ [25] 数据集，并遵循其评估协议。DreamBench++ [25] 是一个用于评估个性化图像生成的全面且多样化的数据集，由150张高质量图像和1350个提示组成，比之前的基准数据集如DreamBench [31] 要多得多。该数据集涵盖了各种类别，如动物、人类、物体等，包括逼真和非逼真的图像，提示设计涵盖了不同的难度级别（简单/富有想象力）。相比之下，提示是使用GPT - 4o生成并由人工标注员进行优化的，以确保多样性和符合道德规范。</div></div></div></div><div><br></div><div><div><div>Baselines. We follow the setups in DreamBench++ [25] and compare our model with two classes of baselines: inference-stage tuning models and zero-shot models. For inference-stage models, we compare against Textual Inversion [9], DreamBooth [31] and its LoRA [13] version. For zero-shot models, we compare with BLIP-Diffusion [17], Emu2 [36], IP-Adapter [42], IP-Adapter+ [42].<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">基线模型。我们遵循DreamBench++ [25] 中的设置，将我们的模型与两类基线模型进行比较：推理阶段调优模型和零样本模型。对于推理阶段模型，我们与文本反转 [9]、DreamBooth [31] 及其低秩自适应（LoRA）版本 [13] 进行比较。对于零样本模型，我们与BLIP - 扩散 [17]、Emu2 [36]、IP - 适配器 [42]、IP - 适配器+ [42] 进行比较。</div></div></div></div><div><br></div><div><div><div>Evaluation metrics. The evaluation protocol of prior works <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13198" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mn>7</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>30</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>31</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>42</mn></mrow></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow></math></mjx-assistive-mml></mjx-container> typically involves comparing the CLIP [27] and DINO [6] feature similarities. However, we note that the metrics mentioned above capture only global semantic similarity, are extremely noisy, and are biased towards "copy-pasting" the input image. This is especially troublesome when the input image or the prompt is complex. We refer to DreamBench++ [25] for a detailed analysis of their limitations. Therefore, we follow the metrics designed in DreamBench++ [25] and report GPT-4o scores on the more diverse DreamBench++ [25] benchmark for both concept preservation (CP) with different categories of subjects and prompt following (PF) with photorealistic (Real.) and Imaginative (Imag.) prompts, then use their product as a final evaluation score. This evaluation protocol emulates a human user study using VLMs. We additionally slightly modify the GPT evaluation prompts so that penalization can be applied if the generated contents show no internal understanding and creative output but instead naively copy over components from the reference image. The modified metrics are named "de-biased concept preservation (Debiased CP)" and "de-biased prompt following (Debiased PF)". The full set of GPT evaluation prompts will be provided in our supplementary Sec. B.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">评估指标。先前工作<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13199" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mn>7</mn><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>30</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>31</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>42</mn></mrow></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow></math></mjx-assistive-mml></mjx-container>的评估协议通常涉及比较CLIP [27]和DINO [6]的特征相似度。然而，我们注意到上述指标仅能捕捉全局语义相似度，存在极大的噪声，并且倾向于“复制粘贴”输入图像。当输入图像或提示词较为复杂时，这一问题尤为棘手。我们参考了DreamBench++ [25]对其局限性的详细分析。因此，我们采用DreamBench++ [25]中设计的指标，并在更多样化的DreamBench++ [25]基准测试中报告GPT - 4o分数，包括针对不同类别主体的概念保留（CP）以及具有逼真（真实）和富有想象力（想象）提示词的提示遵循（PF），然后将两者的乘积作为最终评估分数。该评估协议模拟了使用视觉语言模型（VLM）进行的人类用户研究。此外，我们对GPT评估提示词进行了轻微修改，以便在生成内容没有展现出内在理解和创造性输出，而是简单地从参考图像中复制元素时进行惩罚。修改后的指标被命名为“去偏概念保留（去偏CP）”和“去偏提示遵循（去偏PF）”。完整的GPT评估提示词将在我们的补充材料B节中提供。</div></div></div></div><div><div><div></div></div></div></span></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="6" id="mark-25097faf-a3b4-4db2-94c9-2baa357eae97" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-5="0,0"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-5="0,0"></paragraphpositioning></div></div></div><div><!-- Media --><br><!-- figureText: Character Preservation a man a man painting a meticulously still life of a bowl ( vintage map under at a weathered repairs a vintage the flickering light wooden table telescope in his fruit, bathed in of a gas lamp in a sipping c light coffee robot performing robot holding a robot holding a robot looking at a a complex dance vintage record glowing blue miniature routine player, against a orb in a futuristic ityscape made of backdrop of a cityscape at night intricate bustling city gears and cogs. resting on a white resting on a worn resting on a resting on a leather journal pristine white velvet cushion cushion, bathed with a faded marble countertop inside a vintage in golden sunlight inscription. bathed in soft jewelry box streaming sunlight. through a window. being held by put the bag on a put it next to a put the black a stylish woman weathered vibrant flower leather bag in a bustling city wooden crate in a arrangement on on a dusty antique marketplace vintage Parisia chest in a dimly balcony Iit antique shop make the girl make the coat put the bag make the girl sitting in a cafe hang in a dusty in a dusty shelf wonder around a antique room holographic map of a futuristic Relighting cityscape a dramatic a harsh, blue a soft, overcast a moody lighting stormy sky toned sky sky a female anime a female anime a female anime a female anime character character soaring character gazing kneeling on a cliff through a stormy thoughtfully standing amidst sky, illuminated towards a blooming city. orchard. mushroom at a mushroom with a mushroom mushroom comes table menacing frown, on a table out of a vintage centerpiece perched atop a at a fancy dinner towering, with a checkered party. crumbling stone tablecloth. Item Preservation shirt, lying on a rainbow is love" shirt, is love" shirt bed of colorful hanging in a worn wildflowers, "love is love", on a bu a bustling Parisian by a lovely bathed in warm rustic wooden market. blonde girl. sunlight. chair in a vintage clothing store. Instruction Prompts make the man gracefully tips a make the man in make the man against a sun- hat to a beautiful a vintage library stand in a neon-lit drenched woman in a illuminated by cityscape, while a backdrop of a vibrant, blooming warm, soft bustling city flower garden lighting vintage car races street in the background make the make the man make the make the character play skateboard, character explore character a girl, standing in rest on a chair on an empty street in a park a vast, neon-lit a busy city street alien landscape - golden cool, blue-toned blue dusk light dramatic, hour lighting lighting cinematic lighting --><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_5.jpg?x=155&amp;y=203&amp;w=1487&amp;h=1467&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_5.jpg?x=155&amp;y=203&amp;w=1487&amp;h=1467&amp;r=0"><div><br></div><div><div><div>Figure 5. Qualitative result. Our Diffusion Self-Distillation is capable of various customization targets across different tasks and styles, for instance, characters or objects, photorealistic or animated. Diffusion Self-Distillation can also take instruction types of prompts as input, similar to InstructPix2Pix [2]. Further, our model exhibits relighting capabilities without significantly altering the scene's content.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图5. 定性结果。我们的扩散自蒸馏方法能够针对不同任务和风格实现各种定制目标，例如人物或物体、逼真风格或动画风格。扩散自蒸馏方法还可以将指令类型的提示词作为输入，类似于InstructPix2Pix [2]。此外，我们的模型在不显著改变场景内容的情况下具备重新打光的能力。</div></div></div></div><div><br><!-- Media --><br></div><div><div><div>Qualitative results. Fig. 4 presents our qualitative comparison results, demonstrating that our model significantly outperforms all baselines in subject adaptation and concept consistency while exhibiting excellent prompt alignment and diversity in the outputs. Textual Inversion [9], as an early concept extraction method, captures only vague semantics from the input image, making it unsuitable for zero-shot customization tasks that require precise subject adaptation. DreamBooth [31] and DreamBooth-LoRA [13, 31] face challenges in maintaining consistency, primarily because they perform better with multiple input images. This dependency limits their effectiveness when only a single reference image is available. In contrast, our method achieves robust results even with just one input image, highlighting its efficiency and practicality. BLIP-Diffusion [17], operating as a self-supervised representation learning framework, can extract concepts from the input in a zero-shot manner but is confined to capturing overall semantic concepts without the ability to customize specific subjects. Similarly, Emu2 [36], a multimodal foundation model, excels at extracting semantic concepts but lacks mechanisms for specific subject customization, limiting its utility in personalized image generation. IP-Adapter [42] and IP-Adapter+ [42] employ self-supervised learning schemes aimed at reconstructing the input from encoded signals. While effective in extracting global concepts, they suffer from a pronounced "copy-paste" effect, where the generated images closely resemble the input without meaningful transformation. Notably, IP-Adapter+ [42], which utilizes a stronger input image encoder, exacerbates this issue, leading to less diversity and adaptability in the outputs. In contrast, our approach effectively preserves the subject's core identity while enabling diverse and contextually appropriate transformations. As illustrated in Fig. 5, our Diffusion Self-Distillation demonstrates remarkable versatility, adeptly handling various customization targets across different targets (characters, objects, etc.) and styles (photorealistic, animated, etc.). Moreover, Diffusion Self-Distillation generalizes well to a wide range of prompts, including instructions similar to InstructPix2Pix [2], underscoring its robustness and adaptability in diverse customization tasks.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">定性结果。图4展示了我们的定性比较结果，表明我们的模型在主体适配和概念一致性方面显著优于所有基线模型，同时在输出结果中展现出出色的提示对齐和多样性。文本反转[9]作为一种早期的概念提取方法，只能从输入图像中捕捉模糊的语义，因此不适用于需要精确主体适配的零样本定制任务。DreamBooth [31]和DreamBooth - LoRA [13, 31]在保持一致性方面面临挑战，主要是因为它们在使用多个输入图像时表现更好。这种对多图像的依赖限制了它们在仅有单张参考图像时的有效性。相比之下，我们的方法即使仅使用一张输入图像也能取得稳健的结果，凸显了其高效性和实用性。BLIP - 扩散[17]作为一种自监督表示学习框架，能够以零样本方式从输入中提取概念，但仅限于捕捉整体语义概念，无法对特定主体进行定制。同样，多模态基础模型Emu2 [36]擅长提取语义概念，但缺乏对特定主体进行定制的机制，限制了其在个性化图像生成中的实用性。IP - 适配器[42]和IP - 适配器+ [42]采用自监督学习方案，旨在从编码信号中重建输入。虽然它们在提取全局概念方面有效，但存在明显的“复制粘贴”效应，即生成的图像与输入图像极为相似，没有有意义的变换。值得注意的是，使用更强输入图像编码器的IP - 适配器+ [42]加剧了这一问题，导致输出结果的多样性和适应性降低。相比之下，我们的方法能够有效保留主体的核心特征，同时实现多样化且符合上下文的变换。如图5所示，我们的扩散自蒸馏方法展现出卓越的通用性，能够熟练处理不同目标（人物、物体等）和风格（逼真风格、动画风格等）的各种定制目标。此外，扩散自蒸馏方法能够很好地泛化到各种提示词，包括类似于InstructPix2Pix [2]的指令，凸显了其在多样化定制任务中的稳健性和适应性。</div></div></div></div><div><div><div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="7" id="mark-c15b1f99-e911-42b4-a6b0-1b36f22195cb" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-6="0,0"><div style="height: auto;"><span style="display: inline;"><div><div><div><paragraphpositioning data-position-6="0,0"></paragraphpositioning></div></div></div><div><!-- Media --><br><div class="table-container"><table class="fixed-table"><tbody><tr><td rowspan="2">Method</td><td rowspan="2"></td><td colspan="4">Concept Preservation</td><td colspan="3">Prompt Following</td><td rowspan="2">CP-PF↑</td><td colspan="4">Debiased Concept Preservation</td><td colspan="4">Debiased PromptFollowingDebiased</td></tr><tr><td>Animal</td><td>Human↑</td><td>Object↑</td><td>Overall↑</td><td>Real.↑</td><td>Imag.↑</td><td>Overall↑</td><td>Animal↑</td><td>Human↑</td><td>Object↑</td><td>Overall↑</td><td>Real.↑</td><td>Imag.↑</td><td>Overall↑</td><td>CP-PF↑</td></tr><tr><td>Textual Inversion</td><td>✘</td><td>0.502</td><td>0.358</td><td>0.305</td><td>0.388</td><td>0.671</td><td>0.437</td><td>0.598</td><td>0.232</td><td>0.741</td><td>0.694</td><td>0.717</td><td>0.722</td><td>0.619</td><td>0.385</td><td>0.541</td><td>0.391</td></tr><tr><td>DreamBooth</td><td>✘</td><td>0.640</td><td>0.199</td><td>0.488</td><td>0.442</td><td>0.798</td><td>0.504</td><td>0.692</td><td>0.306</td><td>0.670</td><td>0.362</td><td>0.676</td><td>0.626</td><td>0.750</td><td>0.467</td><td>0.656</td><td>0.411</td></tr><tr><td>DreamBooth LoRA</td><td>✘</td><td>0.751</td><td>0.311</td><td>0.543</td><td>0.535</td><td>0.898</td><td>0.754</td><td>0.849</td><td>0.450</td><td>0.681</td><td>0.675</td><td>0.761</td><td>0.720</td><td>0.865</td><td>0.718</td><td>0.816</td><td>0.588</td></tr><tr><td>BLIP-Diffusion</td><td>✓</td><td>0.637</td><td>0.557</td><td>0.469</td><td>0.554</td><td>0.581</td><td>0.303</td><td>0.464</td><td>0.257</td><td>0.771</td><td>0.733</td><td>0.745</td><td>0.750</td><td>0.529</td><td>0.266</td><td>0.442</td><td>0.332</td></tr><tr><td>Emu2</td><td>✓</td><td>0.670</td><td>0.546</td><td>0.447</td><td>0.554</td><td>0.732</td><td>0.560</td><td>0.670</td><td>0.371</td><td>0.652</td><td>0.683</td><td>0.701</td><td>0.681</td><td>0.686</td><td>0.494</td><td>0.622</td><td>0.424</td></tr><tr><td>IP-Adapter</td><td>✓</td><td>0.667</td><td>0.558</td><td>0.504</td><td>0.576</td><td>0.743</td><td>0.446</td><td>0.607</td><td>0.350</td><td>0.790</td><td>0.764</td><td>0.743</td><td>0.766</td><td>0.695</td><td>0.377</td><td>0.589</td><td>0.451</td></tr><tr><td>IP-Adapter+</td><td>✓</td><td>0.900</td><td>0.845</td><td>0.759</td><td>0.834</td><td>0.502</td><td>0.279</td><td>0.388</td><td>0.324</td><td>0.481</td><td>0.473</td><td>0.530</td><td>0.504</td><td>0.442</td><td>0.229</td><td>0.371</td><td>0.187</td></tr><tr><td>Ours</td><td>✓</td><td>0.647</td><td>0.567</td><td>0.640</td><td>0.631</td><td>0.777</td><td>0.625</td><td>0.726</td><td>0.458</td><td>0.852</td><td>0.774</td><td>0.750</td><td>0.789</td><td>0.808</td><td>0.681</td><td>0.757</td><td>0.597</td></tr></tbody></table></div><div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;"><div class="table-container"><table class="fixed-table"><tbody><tr><td rowspan="2">方法</td><td rowspan="2"></td><td colspan="4">概念保留</td><td colspan="3">提示遵循</td><td rowspan="2">概念保留 - 提示遵循（CP - PF）↑</td><td colspan="4">去偏概念保留</td><td colspan="4">去偏提示遵循</td></tr><tr><td>动物</td><td>人类↑</td><td>物体↑</td><td>总体↑</td><td>真实↑</td><td>虚构↑</td><td>总体↑</td><td>动物↑</td><td>人类↑</td><td>物体↑</td><td>总体↑</td><td>真实↑</td><td>虚构↑</td><td>总体↑</td><td>概念保留 - 提示遵循（CP - PF）↑</td></tr><tr><td>文本反转</td><td>✘</td><td>0.502</td><td>0.358</td><td>0.305</td><td>0.388</td><td>0.671</td><td>0.437</td><td>0.598</td><td>0.232</td><td>0.741</td><td>0.694</td><td>0.717</td><td>0.722</td><td>0.619</td><td>0.385</td><td>0.541</td><td>0.391</td></tr><tr><td>梦境展位（DreamBooth）</td><td>✘</td><td>0.640</td><td>0.199</td><td>0.488</td><td>0.442</td><td>0.798</td><td>0.504</td><td>0.692</td><td>0.306</td><td>0.670</td><td>0.362</td><td>0.676</td><td>0.626</td><td>0.750</td><td>0.467</td><td>0.656</td><td>0.411</td></tr><tr><td>梦境展位低秩自适应（DreamBooth LoRA）</td><td>✘</td><td>0.751</td><td>0.311</td><td>0.543</td><td>0.535</td><td>0.898</td><td>0.754</td><td>0.849</td><td>0.450</td><td>0.681</td><td>0.675</td><td>0.761</td><td>0.720</td><td>0.865</td><td>0.718</td><td>0.816</td><td>0.588</td></tr><tr><td>BLIP - 扩散模型</td><td>✓</td><td>0.637</td><td>0.557</td><td>0.469</td><td>0.554</td><td>0.581</td><td>0.303</td><td>0.464</td><td>0.257</td><td>0.771</td><td>0.733</td><td>0.745</td><td>0.750</td><td>0.529</td><td>0.266</td><td>0.442</td><td>0.332</td></tr><tr><td>鸸鹋2（Emu2）</td><td>✓</td><td>0.670</td><td>0.546</td><td>0.447</td><td>0.554</td><td>0.732</td><td>0.560</td><td>0.670</td><td>0.371</td><td>0.652</td><td>0.683</td><td>0.701</td><td>0.681</td><td>0.686</td><td>0.494</td><td>0.622</td><td>0.424</td></tr><tr><td>图像提示适配器（IP - Adapter）</td><td>✓</td><td>0.667</td><td>0.558</td><td>0.504</td><td>0.576</td><td>0.743</td><td>0.446</td><td>0.607</td><td>0.350</td><td>0.790</td><td>0.764</td><td>0.743</td><td>0.766</td><td>0.695</td><td>0.377</td><td>0.589</td><td>0.451</td></tr><tr><td>图像提示适配器增强版（IP - Adapter+）</td><td>✓</td><td>0.900</td><td>0.845</td><td>0.759</td><td>0.834</td><td>0.502</td><td>0.279</td><td>0.388</td><td>0.324</td><td>0.481</td><td>0.473</td><td>0.530</td><td>0.504</td><td>0.442</td><td>0.229</td><td>0.371</td><td>0.187</td></tr><tr><td>我们的方法</td><td>✓</td><td>0.647</td><td>0.567</td><td>0.640</td><td>0.631</td><td>0.777</td><td>0.625</td><td>0.726</td><td>0.458</td><td>0.852</td><td>0.774</td><td>0.750</td><td>0.789</td><td>0.808</td><td>0.681</td><td>0.757</td><td>0.597</td></tr></tbody></table></div></div><br></div><div><div><div>Table 1. Quantitative result. On the human-aligned GPT score metrics, our method is only inferior to IP-Adapter+ [42] for concept preservation (largely because of IP-Adapter families' "copy-pasting" effect) and the tuning-base DreamBooth-LoRA [13, 31] for prompt following, but outperforms every other baseline, achieving the best overall performance considering both concept preservation and prompt following. We also note that on the de-biased GPT evaluation, which penalizes "copy-pasting" the reference image without significant creative interpretation or transformation, the advantages of IP-Adaper+ [42] no longer hold. This can also be partly observed by their bad prompt following scores, meaning they are biased towards the reference input and are not accommodating the input prompt. The first, second, and third values are highlighted, where Diffusion Self-Distillation is the best overall performing model.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">表1. 定量结果。在与人对齐的GPT评分指标上，我们的方法仅在概念保留方面逊色于IP-Adapter+ [42]（主要是因为IP-Adapter系列的“复制粘贴”效果），在遵循提示方面逊色于微调基础的DreamBooth-LoRA [13, 31]，但优于其他所有基线方法。综合考虑概念保留和遵循提示这两个方面，我们的方法取得了最佳的整体性能。我们还注意到，在去偏的GPT评估中（该评估会对没有显著创意解读或转换而“复制粘贴”参考图像的情况进行惩罚），IP-Adaper+ [42]的优势不再存在。从它们较差的遵循提示得分也可以部分观察到这一点，这意味着它们偏向于参考输入，而不能很好地适应输入提示。我们突出显示了排名第一、第二和第三的值，其中扩散自蒸馏是整体表现最佳的模型。</div></div></div></div><div><br><!-- Media --><br></div><div><div><div>Quantitative results. Quantitative comparison with the baselines are shown in Tab. 1, where we report GPT evaluation following DreamBench++ [25]. Such an evaluation protocol is similar to human score but uses automatic multimodal LLMs. Our method achieves the best overall performances accommodating both concept preservation and prompt following, while only being inferior to IP-Adapter+ [42] for the former (mainly because of the "copy-paste" effect again), and the per-instance tuning DreamBooth-LoRA <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13200" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mn>13</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>31</mn></mrow></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow></math></mjx-assistive-mml></mjx-container> for the latter. We note that the concept preservation evaluation of DreamBench++ [25] is still biased towards favoring a "copy-paste" effect, especially on more challenging and diverse prompts. For instance, the outstanding concept preservation performances of the IP-Adapter family [42] are primarily because of their strong "copy-paste" effect, which copies over the input image without considering relevant essential changes in the prompts. This can also be partly observed by their underperforming prompt following scores, which means they are biased towards the reference input and do not accommodate the input prompt. Therefore, we also present our "de-biased" version of GPT scores, which are as simple as telling GPT to penalize if the generated image resembles a direct copy of the reference image. We observe that the advantages of IP-Adaper+ [42] no longer hold. Overall, Diffusion Self-Distillation is the best-performing model.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">定量结果。与基线方法的定量比较见表1，我们按照DreamBench++ [25]的方法进行了GPT评估。这种评估协议与人工评分类似，但使用了自动多模态大语言模型。我们的方法在兼顾概念保留和遵循提示方面取得了最佳的整体性能，仅在概念保留方面逊色于IP-Adapter+ [42]（主要还是因为“复制粘贴”效果），在遵循提示方面逊色于逐实例微调的DreamBooth-LoRA <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13201" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mrow><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-texatom texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">[</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mn>13</mn></mrow><mo>,</mo><mrow data-mjx-texclass="ORD"><mn>31</mn></mrow></mrow><mo data-mjx-texclass="CLOSE">]</mo></mrow></math></mjx-assistive-mml></mjx-container>。我们注意到，DreamBench++ [25]的概念保留评估仍然偏向于“复制粘贴”效果，尤其是在更具挑战性和多样性的提示下。例如，IP-Adapter系列 [42]出色的概念保留性能主要是因为它们强大的“复制粘贴”效果，即直接复制输入图像而不考虑提示中的相关关键变化。从它们较差的遵循提示得分也可以部分观察到这一点，这意味着它们偏向于参考输入，而不能很好地适应输入提示。因此，我们还给出了“去偏”版的GPT得分，其方法很简单，就是告诉GPT如果生成的图像与参考图像直接复制相似就进行惩罚。我们发现，IP-Adaper+ [42]的优势不再存在。总体而言，扩散自蒸馏是表现最佳的模型。</div></div></div></div><div><br></div><div><div><div>Ablation studies. (1) Data curation: During dataset generation, we first synthesize grids using a frozen pre-trained FLUX model and then filter the images via VLM curation. Why not fine-tune the FLUX model on image grids to improve the hit rate? To study this, we fit a LoRA [13] using <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13202" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3E"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&gt;</mo><mrow data-mjx-texclass="ORD"><mn>7000</mn></mrow></math></mjx-assistive-mml></mjx-container> consistent grids (Fig. 6,left). Though a more significant proportion of samples are consistent grids, we find that the teacher model loses diversity in its output. Therefore, we choose to rely entirely on VLMs to help us curate from large numbers of diverse but potentially noisy grids. (2) Parallel processing architecture: We compare the parallel processing architecture to three alternative image-to-image architectures: 1) concatenating the source image to the noise image ("concatenation"); 2) a ControlNet [43]- based design, and 3) an IP-Adapter [42]-based design. We train each architecture using the same data as our parallel processing model (Fig. 6, middle). For ControlNet [43], we draw the same conclusion as prior work [14], in that it works best for structure-aligned edits, but generally struggles to preserve details when the source image and target image differ in camera pose. IP-Adapter [42] struggles to effectively transfer details and styles from the source image due to the limited capacity of its image encoder. (3) Other image-to-image tasks: Although not "self-distillation", since<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">消融研究。(1) 数据筛选：在数据集生成过程中，我们首先使用冻结的预训练FLUX模型合成网格，然后通过视觉语言模型（VLM）筛选图像。为什么不直接在图像网格上微调FLUX模型以提高命中率呢？为了研究这个问题，我们使用<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13203" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c3E"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&gt;</mo><mrow data-mjx-texclass="ORD"><mn>7000</mn></mrow></math></mjx-assistive-mml></mjx-container>一致网格拟合了一个LoRA [13]（图6，左）。虽然有更大比例的样本是一致网格，但我们发现教师模型的输出失去了多样性。因此，我们选择完全依靠视觉语言模型（VLM）从大量多样但可能有噪声的网格中进行筛选。(2) 并行处理架构：我们将并行处理架构与三种替代的图像到图像架构进行了比较：1) 将源图像与噪声图像拼接（“拼接”）；2) 基于ControlNet [43]的设计；3) 基于IP-Adapter [42]的设计。我们使用与并行处理模型相同的数据训练了每种架构（图6，中间）。对于ControlNet [43]，我们得出了与先前工作 [14]相同的结论，即它在结构对齐编辑方面效果最佳，但当源图像和目标图像的相机姿态不同时，通常难以保留细节。由于其图像编码器的容量有限，IP-Adapter [42]难以有效地从源图像中传递细节和风格。(3) 其他图像到图像任务：虽然不是“自蒸馏”，因为</div></div></div></div><div><div><div></div></div></div></span></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="8" id="mark-787bdcd1-a6c2-43b3-b016-ff838a5e884e" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-7="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="9" id="mark-ea0ae962-d482-4587-b85f-806c67f25b64" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-8="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="10" id="mark-7849de28-cb63-465b-9c23-b3eeb49c1ae6" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-9="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="11" id="mark-86691fb3-9997-45d9-b662-c24ca3c88ffc" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-10="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="12" id="mark-9a1b357b-183d-40ce-84e1-18c18d541c02" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-11="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="13" id="mark-8741b142-6519-4f68-b381-838ecee687c9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-12="0,0"><div style="height: auto;"></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="14" id="mark-17315bc8-4619-486b-9a46-43299e250665" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-13="0,0"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-13="0,0"></paragraphpositioning></div></div></div><div><!-- Media --><br><!-- figureText: reference prompt Ours Textual Inversion DreamBooth DreamBooth-LoRA BLIP-Diffusion Emu2 IP-Adapter IP-Adapter-Plus A photograph of a kitten napping curled up in a cozy basket A basketball shoe transforming free-throw line towards the hoop A man ascending a staircase made of falling autumn leaves, each step vanishing into a mist A photograph of a man reading a newspaper on a park bench A samurai walking through a portal that leads to a futuristic cityscape A photograph of a guitar hanging on a brick wall adorned with vintage posters A photo of a minion in a bustling city street, trying to hail a taxi A photograph of a street lamp covered in snow during a quiet winter night A photo of a girl reading a book under a large oak tree A French bulldog wearing a tiny jetpack, zooming playfully around the Eiffel Tower A photo of a pixelated warrior standing guard at the entrance of a digital castle --><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_13.jpg?x=158&amp;y=294&amp;w=1477&amp;h=1620&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_13.jpg?x=158&amp;y=294&amp;w=1477&amp;h=1620&amp;r=0"><div><br></div><div><div><div>Figure 8. Additional qualitative comparison.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 8. 额外的定性比较。</div></div></div></div><div><br></div><div><div><div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="15" id="mark-16c3426d-42ea-4280-8b58-d2ecedbc2c05" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-14="0,0"><div style="height: auto;"><div><paragraphpositioning data-position-14="0,0"><!-- figureText: tied up with golden ornaments, wearing dark hair adorned with golden ornaments, tied up with golden ornaments, wearing a white dark hair adorned with golden ornaments, tied up with golden ornaments, wearing a white dress with gold accents and large white clad in a flowing white dress with gold accents dress with gold accents and large white feathered wings, gozing thoughtfully toward: standing omidst a blooming cherry blosson feathered wings, kneeling on a clift a distant sunlit city, holding a closed red book orchard, sunlight filtering through overlooking a stormy sea, holding c soft lighting, medium close-up. the delicate petals, camera close-up. closed red book in her hands, wind whipping her hair, dramatic lighting a close-up of a cartoon mushroom with a miniature, brown-capped mushroom with a photorealistic render of a brown, spotted brown cap and tan spots, sitting on a tan spots sits on a vintage typewriter, its mushroom cap with an angry expression, with a checkered tablecloth a man with a long beard and mustache, wearing a dark green beref and brown coat on a a man with a long beard and mustache, a dark green beref and brown coat, meticulous weathered wooden table beside wearing a dark green beret and brown coat, examines a vintage map under the a steaming cup of coffee meticulously paints a still life of a bowl flickering light of a gas lamp in a cozy bookstore of fruit on a weathered wooden table, bathed in warm afternoon light and articulated hands, holding a glowing blue robot with headphones, large circular eyes, large circular eyes, standing proudly on a stage orb in a futuristic cityscape at night. and articulated hands, holding a miniature spotlight shining on its articulated hands cityscape mode of intricate gears and cogs holding a microphone illuminated by soft, warm lighting a man with a long beard and mustache, wearing a cartoon cat with orange and white fur a man with a long beard and mustache a dark green beret and brown coat, meticulously wearing a blue pirate jacket and a black wearing a dark green beret and brown coat, flickering light of a gas lamp in a cozy bookstore in a magical forest, with vibrant colors of fruit on a weathered wooden table and whimsical lighting bathed in warm afternoon light a white dress with gold accents and large wearing a flowing white dress with gold accen: white feathered wings, standing in and large white feathered wings, gracefull a meadow of wildflowers under a vibrant soaring through a stormy sky, illuminated sunset sky, soft lighting, camera low angle. by a brilliant flash of lightning, camera view from below. a close-up of a cartoon mushroom with a a detailed, intricate illustration of a brown, brown cap, tan spots, and an angry spotted mushroom with a menacing frown at a fancy dinner party. crumbling stone archway a man with a long beard and mustache, a man with a long beard and mustache, wearing a dark green beret and brown wearing a dark green beret and brown coc coat, meticulously repairs a vintage meticulously crafting a wooden boat in his telescope in his dimly lit workshop. workshop, bathed in the warm glow of afternoon sunlight circular eyes, and articulated hands performir circular eyes, and articulated hands holding a complex dance routine on a brightly lit stage a vintage record player, against a backdrop of a bustling city skyline at sunset a man with a long beard and mustache a man with a long beard and mustache, wearing a dark green beret and brown wearing a dark green beret and brown cor telescope in his dimly lit workshop. workshop, bathed in the warm glow of afternoon sunlight --><br></paragraphpositioning></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_14.jpg?x=156&amp;y=278&amp;w=1483&amp;h=1660&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_14.jpg?x=156&amp;y=278&amp;w=1483&amp;h=1660&amp;r=0"><div><br></div><div><div><div>Figure 9. Additional character identity preserving results.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 9. 额外的角色身份保留结果。</div></div></div></div><div><br></div><div><div><div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="16" id="mark-3d079ce4-09a1-4ad9-ba9e-f2cd563c1463" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-15="0,0"><div style="height: auto;"><div><paragraphpositioning data-position-15="0,0"><!-- figureText: teal and white gown, stands gracefully wearing a teal and white gown, is standing in : wearing a flowing teal dress, sits thoughtfull wearing a teal and white outfit, gracefully holds on a park bench, holding a book in her lap. a book in her hand, her eyes focused intently on the pages, bathed in the warm glow of a sunset a sleek, blue holographic laptop displaying wearing a blue sports bra, performing a illuminated by neon lights bathed in soft, ethereal light. a photorealistic anthropomorphic gazelle with azelle with orange fur, large ears, and a blac orange fur, large ears, and a black nose, orange fur, large ears, and a black nose, nose, wearing a red jacket, white shirt, blue wearing a red jacket, white shirt, blue shorts wearing a red jacket, white shirt, blue shorts, horts, and red sneakers, standing in a bustling and red sneakers, gracefully standing on a and red sneakers, conducting a symphony marketplace, surrounded by vendors and stage under bright spotlights, holding a orchestra with a baton, surrounded by colorful stalls, holding a basket of fresh fruit microphone, singing with a confident smile musicians in a grand concert hall white robe and red sash, gazes intensely at a glowing orb in a futuristic laboratory, perched on a rooftop overlooking a bustling surrounded by complex machinery. a small, black nose, and large, expressive eyes resting on a fallen log a sun-dappled forest clearing in a dappled sunlit forest clearing dimly lit, ornate ballroom, holding a silver goblet, her eyes gazing thoughtfully at a flickering candle. gazelle with orange fur, large ears, and a black gazelle with orange fur, large ears, and a block nose, wearing a red jacket, white shirt, blue nose, wearing a red jacket, white shirt, blue shorts, and red sneakers, holding a paintbrusi shorts, and red sneakers, standing proudly on c and palette, painting a vibrant sunset mountain peak, holding a telescope, looking at landscape. the majestic sunrise light of a full moon. the warm glow of a setting sur --><br></paragraphpositioning></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_15.jpg?x=159&amp;y=281&amp;w=1475&amp;h=1651&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_15.jpg?x=159&amp;y=281&amp;w=1475&amp;h=1651&amp;r=0"><div><br></div><div><div><div>Figure 10. Additional character identity preserving results.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 10. 额外的角色身份保留结果。</div></div></div></div><div><br></div><div><div><div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="17" id="mark-e72c6d2c-4501-490b-8a79-d2cfd5b29ad9" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-16="0,0"><div style="height: auto;"><div><paragraphpositioning data-position-16="0,0"><!-- figureText: Love is Love a vibrant rainbow "love is love" shirt, hanging a vibrant, rainbow "love is love" shirt wearing a close-up shot of a white shirt with a rainbow on a clothesline in a bustling parision market, by a lovely blonde girl standing in "love is love" design, droped over a rustic sunlight dappling through the fabric. a bustling, sunny marketplace. wooden chair in a dimly lit vintage clothing store. a white pillow with a circular beach print print depicting a sun-drenched sandy beach, featuring a sandy beach, a body featuring a sandy beach, a body of water amongst plush white linens on a wicker chair on a sunny patio. table next to a steaming cup of coffee a luxurious king-size bed. a close-up shot of a rose gold ring a rose gold ring with a large oval-shaped pini a close-up of a rose gold ring with an oval with a large oval-shaped pink stone and a halt stone and a halo of white diamonds, pink stone and diamond halo, the band spli of white diamonds, the band splitting and the band splits and wraps into a flower shape and forming a flower, resting on forming a flower, resting on a pristine white resting on a velvet cushion inside a vintage a white velvet cushion, bathed in warm marble countertop bathed in jewelry box illuminated by golden sunlight streaming through a window soft, warm sunlight. a single, warm spotlight. bottle labeled "vip" with pink liquid, resting or "vip" with shimmering pink liquid, resting with "vip" in gold, filled with vibrant pink a velvety crimson cushion within a grand, on a pristine white marble pedestal liquid, resting on a smooth, polished ornately decorated jewelry box bathed in soft, warm light. wooden table in a dimly lit library. a detailed, intricate engraving of a perfume a glass christmas ornament with two black a glass christmas ornament with two black bottle labeled "vip" with pink liquid, resting or and white penguins kissing under a clear and white penguins kissing under a clear a velvety crimson cushion within a grand, globe, standing on a white base that reads globe, standing on a white base that read: ornately decorated jewelry box. 'our first couple', resting on a worn wooder 'our first couple', nestled in a snowy fores: candlelight a detailed close-up of a vibrant rainbow a vibrant rainbow "love is love" shirt hanging "love is love" design printed on a white on a clothesline, fluttering in cotton t-shirt, lying on a bed of colorful a gentle summer breeze. wildflowers, bathed in warm sunlight a white pillow with a circular beach print a pristine white pillow with a circular beach featuring a sandy beach, a body of water print of a serene sandy beach, azure water, with a cup of coffee and a book beside it porch swing bathed in warm afternoon sunlight JW50S a dazzling rose gold ring with an oval pink stor a close-up, macro shot of a rose gold ring surrounded by a halo of diamonds, its split with a large oval pink stone surrounded band forming a delicate flower, displayed on by a halo of white diamonds, the split band a pristine white velvet cushion within a forming a delicate flower design, resting on gleaming mahogany ring box, captured in a a worn leather journal with a foded inscription. close-up shot with soft, warm lighting. labeled "vip" with pink liquid, resting on labeled "vip" on a pristine white marble a velvet cushion inside a gilded antique vanity pedestal, surrounded by blooming orchids in a sunlit greenhouse. our first couple a glass christmas ornament with two black a glass christmas ornament with two black and white penguins kissing under a clear and white penguins kissing under a clear globe, standing on a white base that reads globe, standing on a white base that reads 'our first couple', hanging from a vintage- --><br></paragraphpositioning></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_16.jpg?x=161&amp;y=297&amp;w=1473&amp;h=1625&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_16.jpg?x=161&amp;y=297&amp;w=1473&amp;h=1625&amp;r=0"><div><br></div><div><div><div>Figure 11. Additional object/item identity preserving results.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 11. 额外的物体/物品身份保留结果。</div></div></div></div><div><br></div><div><div><div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="18" id="mark-0a3d37f5-edad-4376-b009-4599f841d219" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-17="0,0"><div style="height: auto;"><div><paragraphpositioning data-position-17="0,0"><!-- figureText: a weathered brown, bipedal dinosaur toy a weathered brown, bipedal dinosaur toy a dusty, worn brown dinosaur toy with a a close-up photorealistic shot of a brown, a brown, bipedal dinosaur foy with c prominent crest, sharp teeth, and three- bipedal dinosqur toy with a prominent crest prominent crest, sharp teeth, and three fingered claws, perched precariously on the sharp teeth, and three-fingered claws, fingered claws, perched atop a dusty edge of a child's cluttered desk, with a blurn clutched in a child's hand, against a sunlit. vintage record player, its needle gently background of a messy bedroom. blue sky background stroking a scratched vinyl record a vintage gramophone with a large, rusty a steampunk robot, perched on a crumbling a vintage gramophone with a large, rusty metallic horn, angled left, intricate gold base, brick wall, delicately adjusting the needle of a metallic horn, angled left, intricate gold surrounded by swirling, turquoise waters. metallic horn, against a backdrop of swirling drenched parisian cafe, with a young woma neon-lit clouds in a beref lost in thought as she listens to the a golden heart-shaped pendant necklace a gleaming gold heart-shaped pendant a gold heart-shaped pendant necklace with with sparkling crystals hangs from the necklace, adorned with sparkling crystals sparkling crystals, hanging on a dusty rearview mirror of a vintage convertible lies nestled in the palm of a weathered windowsill bathed in the warm glow of c woman. weekender bag with leather accents. weekender bag with worn leather accents weekender bag with leather accents, nestlec perched atop a mountain peak, overlooking hanging from the hook of a rusty swing set amongst the tools in a bustling artisan's a sprawling city skyline bathed in the golder swaying gently in the breeze workshop a black toy bear with jointed arms and legs a black toy bear with jointed arms and legs wearing a square pendant, stands on a wearing a square pendant, standing or wearing a square pendant, perched on dusty shelf in a dimly lit attic, illuminated by a wooden table in a vintage toy store a dusty shelf illuminated by a single, a single ray of sunlight streaming with warm lighting. flickering candle in a dimly lit attic with a prominent crest, sharp teeth, and with a prominent crest, sharp teeth, and three-fingered claws, stands forgotten on a three-fingered claws, nestled amongst the dusty shelf in a cluttered antique shop antique trinkets in a dusty, forgotten attic a vintage gramophone with a large, rusty a vintage gramophone with a large, rust) metallic horn, angled left, intricate gold metallic horn, angled left, intricate gold archway overlooking a bustling medieval overlooking a bustling cityscape at sunset marketplace a gold heart-shaped pendant necklace with a golden heart-shaped pendant necklace sparkling crystals, dangling from a broken adorned with glittering crystals, resting atop robot hand reaching into a futuristic city a weathered, moss-covered stone in a sunlight. weekender bag with leather accents, tucked weekender bag with leather accents. inside a dusty, cobweb-draped antique strapped to the back of a robot exploring ( trunk in a forgotten attic. desolate martian landscape. a black toy bear with jointed arms and legs a black toy bear with jointed arms and legs. wearing a square pendant, next to a red wearing a square pendant, perched on a dusty balloon and walking on a tightrope windowsill overlooking a cityscape above a swirling galaxy. bathed in a warm, golden sunset. --><br></paragraphpositioning></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_17.jpg?x=158&amp;y=302&amp;w=1478&amp;h=1612&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_17.jpg?x=158&amp;y=302&amp;w=1478&amp;h=1612&amp;r=0"><div><br></div><div><div><div>Figure 12. Additional object/item identity preserving results.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 12. 额外的物体/物品身份保留结果。</div></div></div></div><div><br></div><div><div><div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="19" id="mark-ce8ce3f1-d0cc-4ece-9653-cbdc7e4272d2" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-18="0,0"><div style="height: auto;"><div><paragraphpositioning data-position-18="0,0"><!-- figureText: nake the boot rest on a moonlit cobblestone put the boot on a dusty, moonlit windowsill in put the boot atop a dusty old tome put the boot on on a dusty shelf in a forgotter attic, lighted by a single, flickering lightbulb make the pony stand on a polished woods make the pony stand against a shimmerir make the pony leap over a rainbow-colored podium under the bright stage lights rainbow over a bustling city skyline hurdle in a vibrant circus tent make the character silhouette against a fiery make the warrior stride through make the character poise for battle amidst a sunset with a raised sword a sun-dappled meadow fiery sunset illuminating a desolate battlefield make the pug dance on a stage bathed in a make the pug perch on a giant, glowing intently at a flickering gramophone spotlight with a microphone in its paw mushroom in a surreal, psychedelic forest make the old man hold a glowing orb aloft put the book on a worn wooden table in a dimly bustling city street holding a cup of coffee with a dramatic pose against a stormy sky. lit library, illuminated by soft, warm light from a nearby window street, beside a crumpled map make the pony gallop across a field of put the pony through a neon-lit cityscape sunflowers under a vibrant sunset make the character stand amidst ancient make the character strides confidently ruins bathed in soft, diffused light through a misty, ancient forest goggles, sit on a control panel in the cockpit in space surrounded by stars of a vintage biplane make the old man stand on a rocky cliff make the old man read a few pages from a holding a golden chalice, overlooking a vast worn leather-bound book, weathered and swirling ocean under a stormy sky aged, lying open on a dusty wooden table --><br></paragraphpositioning></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_18.jpg?x=163&amp;y=423&amp;w=1468&amp;h=1376&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_18.jpg?x=163&amp;y=423&amp;w=1468&amp;h=1376&amp;r=0"><div><br></div><div><div><div>Figure 13. Additional instruction prompting results.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 13. 额外的指令提示结果。</div></div></div></div><div><br></div><div><div><div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="20" id="mark-178c47c6-6fe9-4936-ac60-997ddfe47648" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-19="0,0"><div style="height: auto;"><div><paragraphpositioning data-position-19="0,0"><!-- figureText: warm, late afternoon lighting a night with a full moon dramatic crimson lighting a cloudy evening light eerie, green glow of an aurora boreali a vibrant, post-rain morning soft, warm glow of sunrise a high-contrast, chiaroscuro lighting a dark lighting soft, diffused, natural lighting a harsh, blue-toned sky cool, blue and white lighting --><br></paragraphpositioning></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_19.jpg?x=165&amp;y=458&amp;w=1463&amp;h=1302&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_19.jpg?x=165&amp;y=458&amp;w=1463&amp;h=1302&amp;r=0"><div><br></div><div><div><div>Figure 14. Additional relighting results.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图 14. 额外的重新照明结果。</div></div></div></div><div><br></div><div><div><div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="21" id="mark-370f40aa-f749-4b5f-ae34-d4d14663b52a" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-20="0,0"><div style="height: auto;"><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=186&amp;y=203&amp;w=473&amp;h=415&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=186&amp;y=203&amp;w=473&amp;h=415&amp;r=0"><div><br></div><div><div><div>In a room dimly lit by a single lamp, a serious man in a dark suit sat at a wooden table, reading an old book intently. Framed portraits adorned the green walls, and shadows shifted subtly under the soft, directional lighting. The man's expression was deeply focused, as if the secrets of the universe lay within the pages.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">在一盏孤灯昏暗的光线下，一个身着深色西装的严肃男子坐在木桌旁，专注地读着一本旧书。绿色的墙壁上挂着镶框的肖像，柔和的定向灯光下，影子在微微移动。男子的神情极为专注，仿佛宇宙的奥秘就藏在书页之中。</div></div></div></div><div><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=679&amp;y=205&amp;w=460&amp;h=413&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=679&amp;y=205&amp;w=460&amp;h=413&amp;r=0"><div><br></div><div><div><div>Suddenly, he realized something, his intense gaze locked onto a passage he had just read. The warm lamplight threw his shadow across the room, making it loom large against the walls and highlighting his furrowed brow. Something he had discovered in the book seemed urgent-almost alarming.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">突然，他意识到了什么，锐利的目光锁定在刚刚读过的一段文字上。温暖的灯光将他的影子投射在房间里，影子在墙壁上显得格外巨大，凸显出他紧皱的眉头。他在书中发现的某些东西似乎很紧急——几乎令人警觉。</div></div></div></div><div><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=1177&amp;y=203&amp;w=448&amp;h=416&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=1177&amp;y=203&amp;w=448&amp;h=416&amp;r=0"><div><br></div><div><div><div>He leaned forward over the table, urgently flipping through the pages. His hands trembled slightly, and the golden light from the lamp illuminated his tense features, deepening the lines of concentration etched into his face. Whatever he sought, he was desperate to find it.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">他探身向前，急切地翻动着书页。他的手微微颤抖，灯光的金色光芒照亮了他紧绷的面容，加深了他脸上专注的皱纹。无论他在寻找什么，他都不顾一切地想要找到它。</div></div></div></div><div><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=175&amp;y=819&amp;w=483&amp;h=415&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=175&amp;y=819&amp;w=483&amp;h=415&amp;r=0"><div><br></div><div><div><div>Raising his head, the man's eyes fixed on the wall of portraits. Holding the old book open in one hand, he approached the paintings, his eyes narrowing with focus. The faces in the frames seemed to stare back at him, and he scanned each one carefully, as if hoping to find something-some connection-that only he<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">男子抬起头，目光落在挂满肖像的墙上。他一只手拿着翻开的旧书，朝那些画作走去，眼睛眯成一条缝，全神贯注。画框里的面孔似乎在回望着他，他仔细地扫视着每一幅画，仿佛希望找到某种只有他</div></div></div></div><div><br></div><div><div><div>could see.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">能看到的联系。</div></div></div></div><div><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=671&amp;y=821&amp;w=469&amp;h=412&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=671&amp;y=821&amp;w=469&amp;h=412&amp;r=0"><div><br></div><div><div><div>With a sense of determination, he reached out to touch a specific portrait, the old book tucked under his arm. His fingers lightly brushed the frame, and his expression grew thoughtful, curious. The soft light emphasized his focused gaze, as if the touch itself might reveal a hidden truth.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">他带着坚定的神情，伸出手去触摸一幅特定的肖像，旧书夹在腋下。他的手指轻轻拂过画框，表情变得若有所思，充满好奇。柔和的灯光凸显出他专注的目光，仿佛触摸本身就能揭示一个隐藏的真相。</div></div></div></div><div><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=1153&amp;y=821&amp;w=471&amp;h=412&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=1153&amp;y=821&amp;w=471&amp;h=412&amp;r=0"><div><br></div><div><div><div>Just then, a creaking sound broke the silence. A hidden door began to open in the wall, and the man stepped back in shock, his eyes wide. The old book clutched in his hands, he stared at the widening gap, where light from a secret passage spilled into the room, creating eerie, shifting shadows.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">就在这时，一阵嘎吱声打破了寂静。墙上一扇隐藏的门开始打开，男子震惊地往后退了一步，眼睛睁得大大的。他紧紧抓着手中的旧书，盯着逐渐扩大的缝隙，从秘密通道透进来的光线洒进房间，形成了诡异、摇曳的影子。</div></div></div></div><div><br><!-- figureText: Steeling himself, he cautiously stepped into the Midway through the corridor, he stopped Finally, he reached the end of the passage, abruptly. Glowing symbols began to appear on where a grand, ancient door loomed before the walls, casting an ethereal light that danced him. It was adorned with intricate, glowing around him. The man's face was illuminated, a runes that pulsed with a life of their own. The look of wonder mingling with his focused man held the old book tightly against his chest determination, as if he was on the brink of awe and anticipation mixing in his expression, understanding a great mystery. while radiant light seeped through the cracks. narrow corridor. The passage was lined with dusty bookshelves, and the faint, flickering light barely illuminated the space. He held the old book close, his expression a mix of wariness and determination, ready to face whatever lay ahead. --><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=161&amp;y=1454&amp;w=1468&amp;h=600&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_20.jpg?x=161&amp;y=1454&amp;w=1468&amp;h=600&amp;r=0"><div><br><!-- Media --><br></div><div><div><div>Figure 15. Comic generation example 1. The conditioned image is the first panel.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图15. 漫画生成示例1。条件图像为第一格漫画。</div></div></div></div><div><div><div></div></div></div></div></div></div></div></div></div><div><div class="relative cursor-pointer md-tranlate-menu-layout"><div><div data-page="22" id="mark-2b50c223-a41d-4f73-8429-a240a9e7a408" class="markdown-parser-view mb-5 relative cursor-pointer"><div class="locator-translate" data-positiontag-21="0,0"><div style="height: auto;"><div><div><div><paragraphpositioning data-position-21="0,0"></paragraphpositioning></div></div></div><div><!-- Media --><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=191&amp;y=206&amp;w=473&amp;h=414&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=191&amp;y=206&amp;w=473&amp;h=414&amp;r=0"><div><br></div><div><div><div>In a room filled with vibrant colors and energy, a focused man with a shaved head and a gold chain sat at a table. He was deeply engrossed in drawing on a sheet of paper, his pencil moving with purpose, while warm light spilled over graffiti-like murals painted on the walls around him. His expression was determined, as if every line he sketched carried deep meaning.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">在一个色彩斑斓、充满活力的房间里，一个剃着光头、戴着金链子的专注男子坐在桌前。他全神贯注地在一张纸上作画，铅笔有目的地移动着，温暖的光线洒在他周围墙壁上类似涂鸦的壁画上。他神情坚定，仿佛每一笔勾勒都蕴含着深刻的意义。</div></div></div></div><div><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=723&amp;y=205&amp;w=424&amp;h=411&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=723&amp;y=205&amp;w=424&amp;h=411&amp;r=0"><div><br></div><div><div><div>After a moment, he held up his sketchpad in his hands. His eyes scanned the drawings he had created, and a look of resolve crossed his face. The colorful murals behind him seemed to mirror the intensity in his gaze, the warm lighting accentuating the passion that had sparked within him.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">过了一会儿，他双手举起素描本。他的目光扫视着自己创作的画作，脸上闪过一丝坚定。他身后色彩鲜艳的壁画似乎映照出他目光中的专注，温暖的灯光凸显出他内心燃起的激情。</div></div></div></div><div><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=1208&amp;y=203&amp;w=414&amp;h=415&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=1208&amp;y=203&amp;w=414&amp;h=415&amp;r=0"><div><br></div><div><div><div>Suddenly inspired, he approached one of the large murals. He began to draw directly onto the wall. His movements were precise and intentional, as colors and patterns flowed from his imagination to the surface. The warm light bathed his intense expression, as if illuminating the raw energy of his creativity.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">突然灵感涌现，他走向其中一幅大型壁画，开始直接在墙上作画。他的动作精准而有意图，色彩和图案从他的想象中流淌到墙面上。温暖的灯光笼罩着他专注的神情，仿佛照亮了他创造力的原始能量。</div></div></div></div><div><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=177&amp;y=822&amp;w=487&amp;h=411&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=177&amp;y=822&amp;w=487&amp;h=411&amp;r=0"><div><br></div><div><div><div>When he was finished, he stepped back to admire his work, arms crossed over his chest. The mural was alive with vivid, swirling graffiti, and his face lit up with pride. The warm light<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">画完后，他双臂交叉抱在胸前，往后退了几步欣赏自己的作品。壁画上满是生动、回旋的涂鸦，他的脸上洋溢着自豪的神情。温暖的灯光</div></div></div></div><div><br></div><div><div><div>glowed over the artwork, and for a moment, he stood there, content, knowing he had given life to his vision.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">洒在这幅艺术作品上，有那么一刻，他心满意足地站在那里，知道自己让心中的愿景变成了现实。</div></div></div></div><div><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=725&amp;y=821&amp;w=415&amp;h=412&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=725&amp;y=821&amp;w=415&amp;h=412&amp;r=0"><div><br></div><div><div><div>But he was not done yet. He returned to the table, sketchpad in hand, and began drawing again. His pencil moved even faster now, capturing new ideas that poured into his mind. The room was a swirl of vibrant murals and soft, warm shadows, the energy of creation pulsing through the space.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">但他还没画完。他回到桌旁，手里拿着素描本，又开始画了起来。此刻他的铅笔移动得更快了，捕捉着涌入他脑海的新想法。房间里满是色彩鲜艳的壁画和柔和温暖的阴影，创作的活力在这个空间中涌动。</div></div></div></div><div><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=1166&amp;y=819&amp;w=456&amp;h=415&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=1166&amp;y=819&amp;w=456&amp;h=415&amp;r=0"><div><br></div><div><div><div>Suddenly, he turned his head slightly, as a noise from outside broke his concentration. He set down his pencil, his expression one of curiosity and intrigue. The warm light reflected off his dark jacket, and the graffiti walls behind him seemed to whisper with a story yet to be discovered.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">突然，他微微转过头，外面的一声响动打断了他的专注。他放下铅笔，脸上露出好奇和兴致勃勃的神情。温暖的灯光从他深色的夹克上反射出来，他身后的涂鸦墙似乎在低语着一个尚未被发现的故事。</div></div></div></div><div><br><!-- figureText: He walked to the doorway, peering out into the Outside, the man found himself bathed in the Finally, he paused at a street corner and started golden light of the setting sun. He walked with sketching again. The warm sunset light purpose, the colors of the urban world around enveloped him, and he realized that his art had him just as vibrant as the murals he had become a part of something larger-a story created. He felt a sense of unity with the woven into the very fabric of the city. His graffiti-covered walls that stretched along the journey of creativity had led him here, and he city streets. knew there were still many more stories to tell. distance. The warm light of the room spilled out into the world beyond, casting long shadows on the floor. Something had drawn his attention, and he knew he had to explore it. The spark of adventure lit his eyes, and he stepped forward. --><br></div><img src="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=170&amp;y=1454&amp;w=1461&amp;h=597&amp;r=0" alt="https://cdn.noedgeai.com/0195d6ef-e939-7abd-9b6d-dae30f972af7_21.jpg?x=170&amp;y=1454&amp;w=1461&amp;h=597&amp;r=0"><div><br><!-- Media --><br></div><div><div><div>Figure 16. Comic generation example 2. The conditioned image is the first panel.<div style="background-color: #e3e4e588;border-radius: 4px;margin: 8px 0;padding: 12px;">图16. 漫画生成示例2。条件图像是第一格。</div></div></div></div><div><div><div></div></div></div></div></div></div></div></div></div>
      </body>
    </html>
  