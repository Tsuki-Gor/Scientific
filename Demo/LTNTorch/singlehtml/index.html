<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LTNtorch 0.9 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            LTNtorch
          </a>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-grounding">Grounding in Logic Tensor Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-learningltn">Introduction to Learning in Logic Tensor Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-ltnobjects">LTN objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-broadcasting">LTN broadcasting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#ltn-predicate-case">LTN predicate case</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#ltn-function-case">LTN function case</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#ltn-connective-case">LTN connective case</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#document-quantification">Quantification in Logic Tensor Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#base-quantification">Base quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#diagonal-quantification">Diagonal quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#guarded-quantification">Guarded quantification</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#document-stableconf">Stable Fuzzy Semantics</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Modules</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-core">ltn.core</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#members">Members</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.core.LTNObject"><code class="docutils literal notranslate"><span class="pre">LTNObject</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.core.LTNObject.shape"><code class="docutils literal notranslate"><span class="pre">LTNObject.shape()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.core.Constant"><code class="docutils literal notranslate"><span class="pre">Constant</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.core.Variable"><code class="docutils literal notranslate"><span class="pre">Variable</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.core.Predicate"><code class="docutils literal notranslate"><span class="pre">Predicate</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.core.Predicate.forward"><code class="docutils literal notranslate"><span class="pre">Predicate.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.core.Function"><code class="docutils literal notranslate"><span class="pre">Function</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.core.Function.forward"><code class="docutils literal notranslate"><span class="pre">Function.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.core.diag"><code class="docutils literal notranslate"><span class="pre">diag()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.core.undiag"><code class="docutils literal notranslate"><span class="pre">undiag()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.core.Connective"><code class="docutils literal notranslate"><span class="pre">Connective</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.core.Connective.__call__"><code class="docutils literal notranslate"><span class="pre">Connective.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.core.Quantifier"><code class="docutils literal notranslate"><span class="pre">Quantifier</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.core.Quantifier.__call__"><code class="docutils literal notranslate"><span class="pre">Quantifier.__call__()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#document-fuzzy_ops">ltn.fuzzy_ops</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#members">Members</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.ConnectiveOperator"><code class="docutils literal notranslate"><span class="pre">ConnectiveOperator</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.UnaryConnectiveOperator"><code class="docutils literal notranslate"><span class="pre">UnaryConnectiveOperator</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="docutils literal notranslate"><span class="pre">BinaryConnectiveOperator</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.NotStandard"><code class="docutils literal notranslate"><span class="pre">NotStandard</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.NotStandard.__call__"><code class="docutils literal notranslate"><span class="pre">NotStandard.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.NotGodel"><code class="docutils literal notranslate"><span class="pre">NotGodel</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.NotGodel.__call__"><code class="docutils literal notranslate"><span class="pre">NotGodel.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AndMin"><code class="docutils literal notranslate"><span class="pre">AndMin</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AndMin.__call__"><code class="docutils literal notranslate"><span class="pre">AndMin.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AndProd"><code class="docutils literal notranslate"><span class="pre">AndProd</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AndProd.__call__"><code class="docutils literal notranslate"><span class="pre">AndProd.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AndLuk"><code class="docutils literal notranslate"><span class="pre">AndLuk</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AndLuk.__call__"><code class="docutils literal notranslate"><span class="pre">AndLuk.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.OrMax"><code class="docutils literal notranslate"><span class="pre">OrMax</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.OrMax.__call__"><code class="docutils literal notranslate"><span class="pre">OrMax.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.OrProbSum"><code class="docutils literal notranslate"><span class="pre">OrProbSum</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.OrProbSum.__call__"><code class="docutils literal notranslate"><span class="pre">OrProbSum.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.OrLuk"><code class="docutils literal notranslate"><span class="pre">OrLuk</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.OrLuk.__call__"><code class="docutils literal notranslate"><span class="pre">OrLuk.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.ImpliesKleeneDienes"><code class="docutils literal notranslate"><span class="pre">ImpliesKleeneDienes</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.ImpliesKleeneDienes.__call__"><code class="docutils literal notranslate"><span class="pre">ImpliesKleeneDienes.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.ImpliesGodel"><code class="docutils literal notranslate"><span class="pre">ImpliesGodel</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.ImpliesGodel.__call__"><code class="docutils literal notranslate"><span class="pre">ImpliesGodel.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.ImpliesReichenbach"><code class="docutils literal notranslate"><span class="pre">ImpliesReichenbach</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.ImpliesReichenbach.__call__"><code class="docutils literal notranslate"><span class="pre">ImpliesReichenbach.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.ImpliesGoguen"><code class="docutils literal notranslate"><span class="pre">ImpliesGoguen</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.ImpliesGoguen.__call__"><code class="docutils literal notranslate"><span class="pre">ImpliesGoguen.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.Equiv"><code class="docutils literal notranslate"><span class="pre">Equiv</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.Equiv.__call__"><code class="docutils literal notranslate"><span class="pre">Equiv.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AggregationOperator"><code class="docutils literal notranslate"><span class="pre">AggregationOperator</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AggregMin"><code class="docutils literal notranslate"><span class="pre">AggregMin</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AggregMin.__call__"><code class="docutils literal notranslate"><span class="pre">AggregMin.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AggregMean"><code class="docutils literal notranslate"><span class="pre">AggregMean</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AggregMean.__call__"><code class="docutils literal notranslate"><span class="pre">AggregMean.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AggregPMean"><code class="docutils literal notranslate"><span class="pre">AggregPMean</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AggregPMean.__call__"><code class="docutils literal notranslate"><span class="pre">AggregPMean.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AggregPMeanError"><code class="docutils literal notranslate"><span class="pre">AggregPMeanError</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AggregPMeanError.__call__"><code class="docutils literal notranslate"><span class="pre">AggregPMeanError.__call__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ltn.fuzzy_ops.SatAgg"><code class="docutils literal notranslate"><span class="pre">SatAgg</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ltn.fuzzy_ops.SatAgg.__call__"><code class="docutils literal notranslate"><span class="pre">SatAgg.__call__()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">LTNtorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">LTNtorch 0.9 documentation</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="ltntorch-s-documentation">
<h1>LTNtorch’s documentation<a class="headerlink" href="#ltntorch-s-documentation" title="Link to this heading"></a></h1>
<p>Welcome to the LTNtorch’s documentation!</p>
<p><a class="reference external" href="https://github.com/bmxitalia/LTNtorch">LTNtorch</a> is a <strong>fully tested</strong> and <strong>well documented</strong> <a class="reference external" href="https://pytorch.org/">PyTorch</a> implementation of <a class="reference external" href="https://arxiv.org/abs/2012.13635">Logic Tensor Networks (LTNs)</a>, a
Neural-Symbolic approach which allows learning neural networks using the satisfaction of a First-Order
Logic (FOL) knowledge base as an objective.</p>
<p>The documentation is organized as follows:</p>
<ul>
<li><p><strong>Notes</strong>: contains some information that may be useful for those unfamiliar with the LTNtorch framework;</p></li>
<li><p><strong>LTNtorch’s modules</strong>:</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>ltn.core</cite>, which contains the definition of constants, variables, predicates, functions, connectives, and quantifiers;</p></li>
<li><p><cite>ltn.fuzzy_ops</cite>, which contains the definition of some of the most common fuzzy semantics (connective operators and aggregators).</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="toctree-wrapper compound">
<span id="document-grounding"></span><section id="grounding-in-logic-tensor-networks">
<h2>Grounding in Logic Tensor Networks<a class="headerlink" href="#grounding-in-logic-tensor-networks" title="Link to this heading"></a></h2>
<p id="notegrounding">To make learning possible, LTN uses a differentiable first-order logic language, called Real Logic, which enables
the incorporation of data and logic.</p>
<p>Real Logic defines the concept of <cite>grounding</cite> (different from the grounding of logic), which is a mapping from the
logical domain (i.e., constants, variables, and logical symbols) to tensors in the Real field or operations based on
tensors. These operations could be, for instance, mathematical functions or learnable neural networks. In other words,
a <cite>grounding</cite>, denoted as <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>, is a function which maps a logical symbol into a real tensor or an
operation on tensors.</p>
<p>In particular, the grounding is defined as follows. Let us assume that <span class="math notranslate nohighlight">\(c\)</span> is a constant, <span class="math notranslate nohighlight">\(x\)</span> is a logical
variable, <span class="math notranslate nohighlight">\(P\)</span> is a predicate, and <span class="math notranslate nohighlight">\(f\)</span> is a logical function:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{G}(c) = \mathbb{R}^{d_1 \times \dots \times d_n}\)</span>: a logical constant is grounded as a tensor (individual) of <strong>any order</strong> (e.g., <span class="math notranslate nohighlight">\(\mathbb{R}^4$ or $\mathbb{R}^{5 \times 4 \times 4}\)</span>);</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{G}(x) = \mathbb{R}^{m \times d}\)</span>: a logical variable is grounded as a <strong>sequence</strong> of <span class="math notranslate nohighlight">\(m\)</span> tensors (individuals) of the same shape <span class="math notranslate nohighlight">\(d\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{G}(f \mid \theta) = x \mapsto MLP_{\theta}(x)\)</span>: a logical function is grounded as a (learnable) <strong>mathematical function</strong> which takes as input some tensors (individuals) and returns a tensor. In this definition, <span class="math notranslate nohighlight">\(\theta\)</span> are the learnable parameters of the function, while <span class="math notranslate nohighlight">\(MLP_{\theta}\)</span> is the neural network representing the function, parametrized by <span class="math notranslate nohighlight">\(\theta\)</span>. Note that the given definition has one input <span class="math notranslate nohighlight">\(x\)</span>, however, an LTN function can take multiple inputs;</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{G}(P \mid \theta) = x \mapsto \sigma (MLP_{\theta}(x))\)</span>: a logical formula (atomic or not) is grounded as a mathematical function which takes as input some tensors (individuals) and returns a value in <strong>[0., 1.]</strong>. In this case, the logistic function <span class="math notranslate nohighlight">\(\sigma\)</span> assures the output to be in the range [0., 1.], resulting in a value which can be interpreted as a fuzzy truth value. Note that the given definition has one input <span class="math notranslate nohighlight">\(x\)</span>, however, an LTN predicate (or formula) can take multiple inputs.</p></li>
</ul>
<p>The <cite>grounding</cite> defines also how the logical connectives (<span class="math notranslate nohighlight">\(\land, \lor, \lnot, \implies, \leftrightarrow\)</span>) and quantifiers
(<span class="math notranslate nohighlight">\(\forall, \exists\)</span>) are mapped in the Real domain. In particular, logical connectives are grounded using fuzzy logic semantics, while
quantifiers are grounded using fuzzy aggregators. Please, <strong>carefully</strong> read the <a class="reference external" href="https://arxiv.org/abs/2012.13635">paper</a> if you have some doubts on these notions.</p>
<p>Examples of possible groundings are showed in the figure below. In particular, <span class="math notranslate nohighlight">\(friend(Mary, John)\)</span> is an
atomic formula (predicate), while <span class="math notranslate nohighlight">\(\forall x (friend(John, x) \implies friend(Mary, x))\)</span> is a closed formula (all the variables are
quantified). The letter <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>, again, is the grounding, the function which maps the logical domain into the Real domain.</p>
<img alt="_images/framework_grounding.png" src="_images/framework_grounding.png" />
</section>
<span id="document-learningltn"></span><section id="introduction-to-learning-in-logic-tensor-networks">
<h2>Introduction to Learning in Logic Tensor Networks<a class="headerlink" href="#introduction-to-learning-in-logic-tensor-networks" title="Link to this heading"></a></h2>
<p id="notelearning">In order to train a Logic Tensor Network, one has to define:</p>
<ol class="arabic simple">
<li><p>a First-Order Logic knowledge base containing some logical axioms;</p></li>
<li><p>some learnable predicates, functions, and/or logical constants appearing in the axioms;</p></li>
<li><p>some data.</p></li>
</ol>
<p>Given these three components, the LTN workflow is the following:</p>
<ol class="arabic simple">
<li><p><strong>grounding phase</strong>: data is used to ground (instantiate) the logical axioms included in the knowledge base;</p></li>
<li><p><strong>forward phase</strong>: the truth values of the logical axioms are computed based on the given grounding (instantiation);</p></li>
<li><p><strong>aggregation phase</strong>: the truth values of the axioms are aggregated to compute the overall satisfaction level of the knowledge base;</p></li>
<li><p><strong>loss function computation</strong>: the gap between the overall satisfaction level and the truth (1) has to be minimized;</p></li>
<li><p><strong>backward phase</strong>: the parameters of the learnable predicates, functions, and/or constants are changed in such a way to maximize the overall satisfaction level.</p></li>
</ol>
<p>The training ends up with a solution which maximally satisfies all the logical axioms in the knowledge base. This
<a class="reference external" href="https://nbviewer.jupyter.org/github/bmxitalia/LTNtorch/blob/main/tutorials/3-knowledgebase-and-learning.ipynb">tutorial</a>
shows how to use the satisfaction of a First-Order Logic knowledge base as an objective to learn a Logic Tensor Network.</p>
<p>In this documentation, you will find how to create a First-Order Logic knowledge base containing learnable predicates (<a class="reference internal" href="index.html#ltn.core.Predicate" title="ltn.core.Predicate"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Predicate</span></code></a>),
functions (<a class="reference internal" href="index.html#ltn.core.Function" title="ltn.core.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Function</span></code></a>), and/or constants (<a class="reference internal" href="index.html#ltn.core.Constant" title="ltn.core.Constant"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Constant</span></code></a>) using LTNtorch.</p>
</section>
<span id="document-ltnobjects"></span><section id="ltn-objects">
<h2>LTN objects<a class="headerlink" href="#ltn-objects" title="Link to this heading"></a></h2>
<p id="noteltnobject">In LTNtorch, non-logical symbols (constants and variables) and the output of logical symbols (predicates, functions,
formulas, connectives, and quantifiers) are wrapped inside <a class="reference internal" href="index.html#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a> instances.</p>
<p>An <cite>LTNObject</cite> represents a generic symbol (non-logical or logical) used by LTNtorch. Every <cite>LTNObject</cite> instance is defined by two important attributes:</p>
<ol class="arabic simple">
<li><p><cite>value</cite>, which contains the grounding of the symbol (<cite>LTNObject</cite>). For example, if the grounding of variable <span class="math notranslate nohighlight">\(x\)</span> is <span class="math notranslate nohighlight">\(\mathcal{G}(x) = [1., 2., 3.]\)</span>, then the <cite>value</cite> attribute for variable <span class="math notranslate nohighlight">\(x\)</span> will contain the vector <span class="math notranslate nohighlight">\([1., 2., 3.]\)</span>;</p></li>
<li><p><cite>free_vars</cite>, which contains the list of the labels of the free variables contained in the <cite>LTNObject</cite> instance. For example, if we have the formula <span class="math notranslate nohighlight">\(\forall x P(x, y)\)</span>, the <cite>free_vars</cite> attribute for this formula will be <cite>[‘y’]</cite>. In fact, <span class="math notranslate nohighlight">\(x\)</span> is quantified by <span class="math notranslate nohighlight">\(\forall\)</span>, while <span class="math notranslate nohighlight">\(y\)</span> is not quantified, namely it is a free variable.</p></li>
</ol>
<p>For those unfamiliar with logic, a free variable is a variable which is not quantified by a universal (<span class="math notranslate nohighlight">\(\forall\)</span>) or
existential (<span class="math notranslate nohighlight">\(\exists\)</span>) quantifier.</p>
</section>
<span id="document-broadcasting"></span><section id="ltn-broadcasting">
<h2>LTN broadcasting<a class="headerlink" href="#ltn-broadcasting" title="Link to this heading"></a></h2>
<section id="ltn-predicate-case">
<span id="broadcasting"></span><h3>LTN predicate case<a class="headerlink" href="#ltn-predicate-case" title="Link to this heading"></a></h3>
<p>In LTNtorch, when a predicate (<a class="reference internal" href="index.html#ltn.core.Predicate" title="ltn.core.Predicate"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Predicate</span></code></a>), function (<a class="reference internal" href="index.html#ltn.core.Function" title="ltn.core.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Function</span></code></a>), or connective (<a class="reference internal" href="index.html#ltn.core.Connective" title="ltn.core.Connective"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Connective</span></code></a>) is
called, the framework automatically performs the broadcasting of the inputs.</p>
<p>To make a simple example, assume that we have two variables, <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, with the following <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">groundings</span></a>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{G}(x)=[[1.6, 1.8, 2.3], [9.3, 4.5, 3.4]] \in \mathbb{R}^{2 \times 3}\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{G}(y)=[[1.2, 1.3, 2.7, 10.4], [4.3, 5.6, 9.5, 1.3], [5.4, 1.5, 9.5, 8.4]] \in \mathbb{R}^{3 \times 4}`\)</span>.</p></li>
</ul>
<p>Variable <span class="math notranslate nohighlight">\(x\)</span> has two individuals with three features each, while variable <span class="math notranslate nohighlight">\(y\)</span> has three individuals with four
features each.</p>
<p>Now, let us assume that we have a binary predicate <span class="math notranslate nohighlight">\(P(a, b)\)</span>, <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounded</span></a> as
<span class="math notranslate nohighlight">\(\mathcal{G}P(a, b | \theta) = a, b \mapsto \sigma(MLP_{\theta}(a, b))\)</span>. <span class="math notranslate nohighlight">\(P(a, b)\)</span> is a learnable
predicate which maps from <span class="math notranslate nohighlight">\(\mathbb{R}^7\)</span> to <span class="math notranslate nohighlight">\([0., 1.]\)</span>. In the notation, <span class="math notranslate nohighlight">\(MLP_{\theta}\)</span> is a neural network,
parametrized by <span class="math notranslate nohighlight">\(\theta\)</span>, with 7 input neurons, some hidden layers, and 1 output neuron. At the last layer has been applied
a logistic function to assure the output to be in the range <span class="math notranslate nohighlight">\([0., 1.]\)</span>. By doing so, the output of <span class="math notranslate nohighlight">\(P(a, b)\)</span>
can be interpreted as fuzzy truth value.</p>
<p>Now, suppose that we want to compute <span class="math notranslate nohighlight">\(P(x, y)\)</span>. LTNtorch automatically broadcasts the two variables before
computing the predicate. After the broadcasting, we will have the following inputs for our predicate:</p>
<p><span class="math notranslate nohighlight">\(\begin{bmatrix} 1.6 &amp; 1.8 &amp; 2.3\\ 1.6 &amp; 1.8 &amp; 2.3\\ 1.6 &amp; 1.8 &amp; 2.3\\ 9.3 &amp; 4.5 &amp; 3.4\\ 9.3 &amp; 4.5 &amp; 3.4\\ 9.3 &amp; 4.5 &amp; 3.4 \end{bmatrix} \in \mathbb{R}^{6 \times 3}\)</span>
for <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(\begin{bmatrix} 1.2 &amp; 1.3 &amp; 2.7 &amp; 10.4\\ 4.3 &amp; 5.6 &amp; 9.5 &amp; 1.3\\ 5.4 &amp; 1.5 &amp; 9.5 &amp; 8.4\\ 1.2 &amp; 1.3 &amp; 2.7 &amp; 10.4\\ 4.3 &amp; 5.6 &amp; 9.5 &amp; 1.3\\ 5.4 &amp; 1.5 &amp; 9.5 &amp; 8.4 \end{bmatrix} \in \mathbb{R}^{6 \times 4}\)</span> for <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>Now, it is possible to observe that if we concatenate these two tensors on the first dimension (<cite>torch.cat([x, y], dim=1)</cite>), we obtain the following input for our predicate:</p>
<p><span class="math notranslate nohighlight">\(\begin{bmatrix} 1.6 &amp; 1.8 &amp; 2.3 &amp; 1.2 &amp; 1.3 &amp; 2.7 &amp; 10.4\\ 1.6 &amp; 1.8 &amp; 2.3 &amp; 4.3 &amp; 5.6 &amp; 9.5 &amp; 1.3\\ 1.6 &amp; 1.8 &amp; 2.3 &amp; 5.4 &amp; 1.5 &amp; 9.5 &amp; 8.4\\ 9.3 &amp; 4.5 &amp; 3.4 &amp; 1.2 &amp; 1.3 &amp; 2.7 &amp; 10.4\\ 9.3 &amp; 4.5 &amp; 3.4 &amp; 4.3 &amp; 5.6 &amp; 9.5 &amp; 1.3\\ 9.3 &amp; 4.5 &amp; 3.4 &amp; 5.4 &amp; 1.5 &amp; 9.5 &amp; 8.4 \end{bmatrix} \in \mathbb{R}^{6 \times 7}\)</span>.</p>
<p>This tensor contains all the possible combinations of the individuals of
the two variables, that are 6. After the computation of the predicate, LTNtorch organizes the output in a tensor <span class="math notranslate nohighlight">\(\mathbf{out} \in [0., 1.]^{2 \times 3}\)</span>, where
the first dimension is related with variable <span class="math notranslate nohighlight">\(x\)</span>, while the second dimension with variable <span class="math notranslate nohighlight">\(y\)</span>.
In <span class="math notranslate nohighlight">\(\mathbf{out}[0, 0]\)</span> there will be the result of the evaluation of <span class="math notranslate nohighlight">\(P(x, y)\)</span> on the first individual of
<span class="math notranslate nohighlight">\(x\)</span>, namely <span class="math notranslate nohighlight">\([1.6, 1.8, 2.3]\)</span>, and first individual of <span class="math notranslate nohighlight">\(y\)</span>, namely <span class="math notranslate nohighlight">\([1.2, 1.3, 2.7, 10.4]\)</span>, in <span class="math notranslate nohighlight">\(\mathbf{out}[0, 1]\)</span> there will be the result of the evaluation of <span class="math notranslate nohighlight">\(P(x, y)\)</span> on the first individual of
<span class="math notranslate nohighlight">\(x\)</span>, namely <span class="math notranslate nohighlight">\([1.6, 1.8, 2.3]\)</span>, and second individual of <span class="math notranslate nohighlight">\(y\)</span>, namely <span class="math notranslate nohighlight">\([4.3, 5.6, 9.5, 1.3]\)</span>, and so forth.</p>
<p>To conclude this note, in LTNtorch, the output of predicates, functions, connectives, and quantifiers are
<a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTNObject</span></a> instances. In the case of our example, the output of predicate <span class="math notranslate nohighlight">\(P\)</span> is
an <cite>LTNObject</cite> with the following attributes:</p>
<ul class="simple">
<li><p><cite>value</cite> <span class="math notranslate nohighlight">\(\in [0., 1.]^{2 \times 3}\)</span>;</p></li>
<li><p><cite>free_vars</cite> = <cite>[‘x’, ‘y’]</cite>.</p></li>
</ul>
<p>Note that we have analyzed just an atomic formula (predicate) in this scenario. Since the variables appearing in the formula are not quantified, the
free variables in the output are both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. If instead of <span class="math notranslate nohighlight">\(P(x, y)\)</span> we had to compute <span class="math notranslate nohighlight">\(\forall x P(x, y)\)</span>,
the <cite>free_vars</cite> attribute would have been equal to <cite>[‘y’]</cite>. Finally, if we had to compute <span class="math notranslate nohighlight">\(\forall x \forall y P(x, y)\)</span>,
the <cite>free_vars</cite> attribute would have been an empty list.</p>
</section>
<section id="ltn-function-case">
<h3>LTN function case<a class="headerlink" href="#ltn-function-case" title="Link to this heading"></a></h3>
<p>The same scenario explained above can be applied to an LTN function (<a class="reference internal" href="index.html#ltn.core.Function" title="ltn.core.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Function</span></code></a>) instead of an LTN predicate (<a class="reference internal" href="index.html#ltn.core.Predicate" title="ltn.core.Predicate"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Predicate</span></code></a>). Suppose we have the same
variables, <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, with the same <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">groundings</span></a>, <span class="math notranslate nohighlight">\(\mathcal{G}(x)\)</span> and <span class="math notranslate nohighlight">\(\mathcal{G}(y)\)</span>.
Then, suppose we have a 2-ary (2 inputs) logical function <span class="math notranslate nohighlight">\(f\)</span>, <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounded</span></a> as
<span class="math notranslate nohighlight">\(\mathcal{G}f(a, b | \theta) = a, b \mapsto MLP_{\theta}(a, b)\)</span>.</p>
<p>In this case, <span class="math notranslate nohighlight">\(MLP_{\theta}\)</span> is a neural network, parametrized by <span class="math notranslate nohighlight">\(\theta\)</span>, with 7 input neurons, some hidden layers, and
five output neurons. In other words, <span class="math notranslate nohighlight">\(f\)</span> is a learnable function which maps from individuals in <span class="math notranslate nohighlight">\(\mathbb{R}^7\)</span> to individuals in <span class="math notranslate nohighlight">\(\mathbb{R}^5\)</span>.
Note that, in this case, we do not have applied a logistic function to the output. In fact, logical functions do not have
the constraint of having outputs in the range <span class="math notranslate nohighlight">\([0., 1.]\)</span>.</p>
<p>LTNtorch applies the same broadcasting that we have seen above to the inputs of function <span class="math notranslate nohighlight">\(f\)</span>. The only difference is
on how the output is organized. In the case of an LTN function, the output is organized in a tensor where the first <span class="math notranslate nohighlight">\(k\)</span>
dimensions are related with the variables given in input, while the remaining dimensions are related with the features of the individuals in output.</p>
<p>In our scenario, the output of <span class="math notranslate nohighlight">\(f(x, y)\)</span> is a tensor <span class="math notranslate nohighlight">\(\mathbf{out} \in \mathbb{R}^{2 \times 3 \times 5}\)</span>. The first
dimension is related with variable <span class="math notranslate nohighlight">\(x\)</span>, the second dimension with variable <span class="math notranslate nohighlight">\(y\)</span>, while the third dimension with the
features of the individuals in output. In <span class="math notranslate nohighlight">\(\mathbf{out}[0, 0]\)</span> there will be the result of the evaluation of <span class="math notranslate nohighlight">\(f(x, y)\)</span> on the first individual of
<span class="math notranslate nohighlight">\(x\)</span>, namely <span class="math notranslate nohighlight">\([1.6, 1.8, 2.3]\)</span>, and first individual of <span class="math notranslate nohighlight">\(y\)</span>, namely <span class="math notranslate nohighlight">\([1.2, 1.3, 2.7, 10.4]\)</span>, in <span class="math notranslate nohighlight">\(\mathbf{out}[0, 1]\)</span> there will be the result of the evaluation of <span class="math notranslate nohighlight">\(f(x, y)\)</span> on the first individual of
<span class="math notranslate nohighlight">\(x\)</span>, namely <span class="math notranslate nohighlight">\([1.6, 1.8, 2.3]\)</span>, and second individual of <span class="math notranslate nohighlight">\(y\)</span>, namely <span class="math notranslate nohighlight">\([4.3, 5.6, 9.5, 1.3]\)</span>, and so forth.</p>
</section>
<section id="ltn-connective-case">
<h3>LTN connective case<a class="headerlink" href="#ltn-connective-case" title="Link to this heading"></a></h3>
<p>LTNtorch applies the LTN broadcasting also before computing a logical connective. To make the concept clear, let us make
a simple example.</p>
<p>Suppose that we have variables <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(y\)</span>, <span class="math notranslate nohighlight">\(z\)</span>, and <span class="math notranslate nohighlight">\(u\)</span>, with <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">groundings</span></a>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(mathcal{G}(x) \in \mathbb{R}^{2 \times 3}\)</span>, namely <span class="math notranslate nohighlight">\(x\)</span> contains two individuals in <span class="math notranslate nohighlight">\(\mathbb{R}^3\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(mathcal{G}(y) \in \mathbb{R}^{4 \times 3 \times 5}\)</span>, namely <span class="math notranslate nohighlight">\(y\)</span> contains four individuals in <span class="math notranslate nohighlight">\(\mathbb{R}^{3 \times 5}\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(mathcal{G}(z) \in \mathbb{R}^{3 \times 5}\)</span>, namely <span class="math notranslate nohighlight">\(z\)</span> contains three individuals in <span class="math notranslate nohighlight">\(\mathbb{R}^5\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(mathcal{G}(u) \in \mathbb{R}^{6 \times 2}\)</span>, namely <span class="math notranslate nohighlight">\(u\)</span> contains six individuals in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>.</p></li>
</ul>
<p>Then, suppose that we have two binary predicates, <span class="math notranslate nohighlight">\(P(a, b)\)</span> and <span class="math notranslate nohighlight">\(Q(a, b)\)</span>. <span class="math notranslate nohighlight">\(P\)</span> maps from <span class="math notranslate nohighlight">\(\mathbb{R}^18\)</span> to
<span class="math notranslate nohighlight">\([0., 1.]\)</span>, while <span class="math notranslate nohighlight">\(Q\)</span> maps from <span class="math notranslate nohighlight">\(\mathbb{R}^7\)</span> to <span class="math notranslate nohighlight">\([0., 1.]\)</span>.</p>
<p>Suppose now that we want to compute the formula: <span class="math notranslate nohighlight">\(P(x, y) \land Q(z, u)\)</span>. In order to evaluate this formula, LTNtorch
follows the following procedure:</p>
<ol class="arabic simple">
<li><p>it computes the result of the atomic formula <span class="math notranslate nohighlight">\(P(x, y)\)</span>, which is a tensor <span class="math notranslate nohighlight">\(\mathbf{out_1} \in [0., 1.]^{2 \times 4}\)</span>. Note that before the computation of <span class="math notranslate nohighlight">\(P(x, y)\)</span>, LTNtorch performed the LTN broadcasting of variables <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>;</p></li>
<li><p>it computes the result of the atomic formula <span class="math notranslate nohighlight">\(Q(z, u)\)</span>, which is a tensor <span class="math notranslate nohighlight">\(\mathbf{out_2} \in [0., 1.]^{3 \times 6}\)</span>. Note that before the computation of <span class="math notranslate nohighlight">\(P(z, u)\)</span>, LTNtorch performed the LTN broadcasting of variables <span class="math notranslate nohighlight">\(z\)</span> and <span class="math notranslate nohighlight">\(u\)</span>;</p></li>
<li><p>it performs the LTN broadcasting of <span class="math notranslate nohighlight">\(\mathbf{out_1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{out_2}\)</span>;</p></li>
<li><p>it applies the fuzzy conjunction <span class="math notranslate nohighlight">\(\mathbf{out_1} \land_{fuzzy} \mathbf{out_2}\)</span>. The result is a tensor <span class="math notranslate nohighlight">\(\mathbf{out} \in [0., 1.]^{2 \times 4 \times 3 \times 6}\)</span>.</p></li>
</ol>
<p>Notice that the output of a logical connective is always wrapped into an <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN object</span></a>, like it happens for predicates and functions.
In this simple example, the <cite>LTNObject</cite> produced by the fuzzy conjunction has the following attributes:</p>
<ul class="simple">
<li><p><cite>value</cite> = <span class="math notranslate nohighlight">\(\mathbf{out}\)</span>;</p></li>
<li><p><cite>free_vars = [‘x’, ‘y’, ‘z’, ‘u’]</cite>.</p></li>
</ul>
<p>Notice that <cite>free_vars</cite> contains the labels of all the variables appearing in <span class="math notranslate nohighlight">\(P(x, y) \land Q(z, u)\)</span>. This is due
to the fact that all the variables are free in the formula since are not quantified by any logical quantifier. Notice also
that <span class="math notranslate nohighlight">\(\mathbf{out}\)</span> has four dimensions, one for each variable appearing in the formula. These dimensions can be
indexed to retrieve the evaluation of <span class="math notranslate nohighlight">\(P(x, y) \land Q(z, u)\)</span> on a specific combination of individuals of <span class="math notranslate nohighlight">\(x,y,z,u\)</span>.
For example, <span class="math notranslate nohighlight">\(\mathbf{out}[0, 0, 0, 0]\)</span> contains the evaluation of <span class="math notranslate nohighlight">\(P(x, y) \land Q(z, u)\)</span> on the first individuals
of all the variables, while <span class="math notranslate nohighlight">\(\mathbf{out}[0, 1, 0, 0]\)</span> contains the evaluation of <span class="math notranslate nohighlight">\(P(x, y) \land Q(z, u)\)</span> on the first individuals
of <span class="math notranslate nohighlight">\(x,z,u\)</span> and second individual of <span class="math notranslate nohighlight">\(y\)</span>.</p>
</section>
</section>
<span id="document-quantification"></span><section id="quantification-in-logic-tensor-networks">
<h2>Quantification in Logic Tensor Networks<a class="headerlink" href="#quantification-in-logic-tensor-networks" title="Link to this heading"></a></h2>
<section id="base-quantification">
<span id="quantification"></span><h3>Base quantification<a class="headerlink" href="#base-quantification" title="Link to this heading"></a></h3>
<p>The logical quantifiers, namely <span class="math notranslate nohighlight">\(\forall\)</span> and <span class="math notranslate nohighlight">\(\exists\)</span>, are implemented in LTN using fuzzy aggregators. An
example of fuzzy aggregator is the mean. The <a class="reference internal" href="index.html#module-ltn.fuzzy_ops" title="ltn.fuzzy_ops"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops</span></code></a> module contains the implementation
of the most common fuzzy aggregators.</p>
<p>In the note regarding the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>, we have seen that the output of logical predicates, for
example <span class="math notranslate nohighlight">\(P(x, y)\)</span>, is organized in a tensor where the dimensions are related with the free variables appearing in the
predicate. In the case of the atomic formula <span class="math notranslate nohighlight">\(P(x, y)\)</span>, the result is a tensor <span class="math notranslate nohighlight">\(\mathbf{out} \in [0., 1.]^{m \times n}\)</span>,
where <span class="math notranslate nohighlight">\(m\)</span> is the number of individuals of variable <span class="math notranslate nohighlight">\(x\)</span>, while <span class="math notranslate nohighlight">\(n\)</span> is the number of individuals of variable <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>Notice that:</p>
<ol class="arabic simple">
<li><p>the tensor <span class="math notranslate nohighlight">\(\mathbf{out}\)</span> has two dimensions because the atomic formula <span class="math notranslate nohighlight">\(P(x, y)\)</span> has two free variables, namely <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>;</p></li>
<li><p>variable <span class="math notranslate nohighlight">\(x\)</span> has been placed on the first dimension, while variable <span class="math notranslate nohighlight">\(y\)</span> in the second dimension, according to their order of appearance in the atomic formula;</p></li>
<li><p>the output of the predicate is wrapped inside an <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN object</span></a> where the <cite>value</cite> attribute contains the tensor <span class="math notranslate nohighlight">\(\mathbf{out}\)</span>, while the <cite>free_vars</cite> attribute contains the list <cite>[‘x’, ‘y’]</cite>.</p></li>
</ol>
<p>Now, we can extend this example with the formula: <span class="math notranslate nohighlight">\(\forall x P(x, y)\)</span>. In order to compute the result of this formula,
LTNtorch must compute the result of the atomic formula <span class="math notranslate nohighlight">\(P(x, y)\)</span> first, and then apply a fuzzy aggregator to this
result for performing the desired quantification on variable <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>We know that the output of <span class="math notranslate nohighlight">\(P(x, y)\)</span> is the tensor <span class="math notranslate nohighlight">\(\mathbf{out} \in [0., 1.]^{m \times n}\)</span>, and that the
first dimension is related to variable <span class="math notranslate nohighlight">\(x\)</span> while second dimension to variable <span class="math notranslate nohighlight">\(y\)</span>. In order to perform the
quantification of <span class="math notranslate nohighlight">\(P(x, y)\)</span> on variable <span class="math notranslate nohighlight">\(x\)</span>, LTNtorch simply performs an aggregation of tensor
<span class="math notranslate nohighlight">\(\mathbf{out}\)</span> on the first dimension. Let us assume that the selected fuzzy aggregator for <span class="math notranslate nohighlight">\(\forall\)</span> is the mean.
LTNtorch simply computes <cite>torch.mean(out, dim=0)</cite>, where <cite>out</cite> is the tensor <span class="math notranslate nohighlight">\(\mathbf{out}\)</span> and <cite>dim=0</cite> means that
we aggregate on the first dimension of <cite>out</cite>, namely the dimension related to variable <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>After the application of the quantifier, a new <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN object</span></a> is created. The attribute <cite>value</cite> will contain
the result of <span class="math notranslate nohighlight">\(\forall x P(x, y)\)</span>, that will be the tensor <span class="math notranslate nohighlight">\(\mathbf{out}' \in [0., 1.]^n\)</span>, while the attribute
<cite>free_vars</cite> will contain the list <cite>[‘y’]</cite>.</p>
<p>Notice that:</p>
<ol class="arabic simple">
<li><p>the result has one less dimension within respect to the result of <span class="math notranslate nohighlight">\(P(x, y)\)</span>. This is due to the fact that we have aggregated one of the two dimensions, namely the dimension of <span class="math notranslate nohighlight">\(x\)</span>. The only dimension left is the dimension related with <span class="math notranslate nohighlight">\(y\)</span>, and it is for this reason that <span class="math notranslate nohighlight">\(\mathbf{out}\)</span> is now a vector in <span class="math notranslate nohighlight">\([0., 1.]^n\)</span>;</p></li>
<li><p>the attribute <cite>free_vars</cite> contains now only variable <span class="math notranslate nohighlight">\(y\)</span>. This is due to the fact that variable <span class="math notranslate nohighlight">\(x\)</span> is not free anymore since it has been quantified by <span class="math notranslate nohighlight">\(\forall\)</span>.</p></li>
</ol>
<p>Finally, notice that if the formula would have been <span class="math notranslate nohighlight">\(\forall x \forall y P(x, y)\)</span>, the quantification would have been
<cite>torch.mean(out, dim=(0,1))</cite>, namely both dimensions of <span class="math notranslate nohighlight">\(\mathbf{out}\)</span> would have been aggregated. Also, the output
would have been a scalar in <span class="math notranslate nohighlight">\([0., 1.]\)</span> and no more free variables would have been in the formula, since both variables
have been quantified.</p>
</section>
<section id="diagonal-quantification">
<h3>Diagonal quantification<a class="headerlink" href="#diagonal-quantification" title="Link to this heading"></a></h3>
<p id="diagonal">Given 2 (or more) variables, there are scenarios where one wants to express statements about specific pairs (or tuples)
of values only, instead of all the possible combinations of values of the variables. This is allowed in LTN thanks to the concept
of diagonal quantification.</p>
<p>To make the concept clear, let us make a simple example. Suppose that we have the formula <span class="math notranslate nohighlight">\(\forall x \forall y P(x, y)\)</span>.
Suppose also that variables <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> have the same number of individuals, and that this number is <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p>With the usual <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>, the predicate <span class="math notranslate nohighlight">\(P(x, y)\)</span> is computed on all the possible
combinations of individuals of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. In other words, the result of <span class="math notranslate nohighlight">\(P(x, y)\)</span> would be a tensor
<span class="math notranslate nohighlight">\(\mathbf{out} \in [0., 1.]^{n \times n}\)</span>. Then, after the quantification on both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, we obtain
a scalar in <span class="math notranslate nohighlight">\([0., 1.]\)</span>.</p>
<p>With diagonal quantification (<span class="math notranslate nohighlight">\(\forall Diag(x, y) P(x, y)\)</span>), instead, predicate <span class="math notranslate nohighlight">\(P(x, y)\)</span> is computed only on the tuples <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>, where
<span class="math notranslate nohighlight">\(x_i\)</span> is the <span class="math notranslate nohighlight">\(i_{th}\)</span> individual of <span class="math notranslate nohighlight">\(x\)</span>, while <span class="math notranslate nohighlight">\(y_i\)</span> is the <span class="math notranslate nohighlight">\(i_{th}\)</span> individual of <span class="math notranslate nohighlight">\(y\)</span>.
In other words, the output would be a tensor <span class="math notranslate nohighlight">\(\mathbf{out} \in [0., 1.]^n\)</span>. Notice that the output has one single dimension
since the predicate has been computed on <span class="math notranslate nohighlight">\(n\)</span> tuples only, namely the tuples created constructing a one-to-one correspondence
between the individuals of <span class="math notranslate nohighlight">\(x\)</span> and the individuals <span class="math notranslate nohighlight">\(y\)</span>. At the end, after the quantification, a scalar is obtained like in the
case with the previous case.</p>
<p>The advantages of diagonal quantification are mani-fold:</p>
<ol class="arabic simple">
<li><p>it is a way to disable the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>. Even if it has been designed to work with quantified statements, it could serve as a way to temporarily disable the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> when computing some formulas. In fact, sometimes it is not necessary to compute a predicate on all the possible combinations of individuals of the variables in input;</p></li>
<li><p>it is more efficient compared to the usual quantification since it allows to avoid to compute a predicate on all the possible combinations of individuals of the variables appearing in the predicate;</p></li>
<li><p>it is useful when dealing with variables which represent machine learning examples. In many tasks, the dataset comes with some labelled examples. One variable could contain the examples, while another variable could contain the labels of the examples. With diagonal quantification we are able to force LTNtorch to use these variables with a one-to-one correspondence. This allows to avoid to compute formulas on combination of individuals which do not make any sense, for example a data sample labelled with a wrong label.</p></li>
</ol>
<p>Notice that diagonal quantification expects the variables to have the same number of individuals, since a one-to-one correspondence has to be created.</p>
<p>In order to use diagonal quantification in LTNtorch, it is possible to use <a class="reference internal" href="index.html#ltn.core.diag" title="ltn.core.diag"><code class="xref py py-func docutils literal notranslate"><span class="pre">ltn.core.diag()</span></code></a>.</p>
</section>
<section id="guarded-quantification">
<h3>Guarded quantification<a class="headerlink" href="#guarded-quantification" title="Link to this heading"></a></h3>
<p id="guarded">In some cases, it could be useful to quantify formulas only on variables’ individuals which satisfy a given condition.
This is allowed in LTN by using the guarded quantification.</p>
<p>To make the concept clear, let us make a simple example. Suppose that we have a binary predicate <span class="math notranslate nohighlight">\(P(a, b)\)</span>. Then, we have two variables,
<span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, containing sequences of points in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>. Specifically,
<span class="math notranslate nohighlight">\(\mathcal{G}(x) \in \mathbb{R}^{m_1 \times 2}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{G}(y) \in \mathbb{R}^{m_2 \times 2}\)</span>. So, <span class="math notranslate nohighlight">\(x\)</span> contains
<span class="math notranslate nohighlight">\(m_1\)</span> points, while <span class="math notranslate nohighlight">\(m_2\)</span> contains <span class="math notranslate nohighlight">\(m_2\)</span> points.</p>
<p>We have already seen that LTN allows to compute the formula: <span class="math notranslate nohighlight">\(\forall x \forall y P(x, y)\)</span>. Also,
we know that LTNtorch firstly computes <span class="math notranslate nohighlight">\(P(x, y)\)</span> and then aggregates on the dimensions specified by the quantified variables.</p>
<p>Suppose now that we want to compute the same formula <span class="math notranslate nohighlight">\(\forall x \forall y P(x, y)\)</span> but quantifying only on the pairs of
points whose distance is lower than a certain threshold. We represent this threshold with the constant <span class="math notranslate nohighlight">\(th\)</span>. In Real Logic,
it is possible to formalize this statement as <span class="math notranslate nohighlight">\(\forall x \forall y : (dist(x, y) &lt; th) \text{ } P(x, y)\)</span>, where <span class="math notranslate nohighlight">\(dist\)</span> is
a function which computes the Euclidean distance between two points in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>.</p>
<p>In order to compute this formula, LTNtorch follows this procedure:</p>
<ol class="arabic simple">
<li><p>it computes the result of the atomic formula <span class="math notranslate nohighlight">\(P(x, y)\)</span>, which is a tensor <span class="math notranslate nohighlight">\(\mathbf{out} \in [0., 1.]^{m_1 \times m_2}\)</span>;</p></li>
<li><p>it masks the truth values in the tensor <span class="math notranslate nohighlight">\(\mathbf{out}\)</span> which do not satisfy the given condition (<span class="math notranslate nohighlight">\(dist(x, y) &lt; th\)</span>);</p></li>
<li><p>it aggregates the tensor <span class="math notranslate nohighlight">\(mathbf{out}\)</span> on both dimensions, since both variables have been quantified. In this aggregation, the truth values masked by the previous step are not considered. Since both variables have been quantified, the result is a scalar in <span class="math notranslate nohighlight">\([0, 1]\)</span>.</p></li>
</ol>
<p>For applying guarded quantification in LTNtorch, see <a class="reference internal" href="index.html#ltn.core.Quantifier" title="ltn.core.Quantifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Quantifier</span></code></a>. In particular, see <cite>mask_fn</cite>
and <cite>cond_vars</cite> parameters.</p>
</section>
</section>
<span id="document-stableconf"></span><section id="stable-fuzzy-semantics">
<h2>Stable Fuzzy Semantics<a class="headerlink" href="#stable-fuzzy-semantics" title="Link to this heading"></a></h2>
<p>In LTNtorch, connectives and quantifiers are <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounded</span></a> using fuzzy semantics.
Despite fuzzy logic enables the incorporation of logic and learning, not all fuzzy semantics are equally
suited for gradient-descent optimization. Many fuzzy logic operators can lead to vanishing or exploding gradients.
Some operators are also <em>single-passing</em>, in that they propagate gradients to only one input at a time.</p>
<p>If you are
interested in an analysis of differentiable fuzzy semantics and gradient problems in LTN, see this
<a class="reference external" href="https://nbviewer.jupyter.org/github/bmxitalia/LTNtorch/blob/main/tutorials/2b-operators-and-gradients.ipynb">tutorial</a>.</p>
<p>In the following, there are some examples of gradient problems in LTN:</p>
<ul class="simple">
<li><p>the Gougen fuzzy conjunction (<a class="reference internal" href="index.html#ltn.fuzzy_ops.AndProd" title="ltn.fuzzy_ops.AndProd"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AndProd</span></code></a>) has vanishing gradients on the edge case <span class="math notranslate nohighlight">\(x = y = 0\)</span>;</p></li>
<li><p>the Gougen fuzzy disjunction (<a class="reference internal" href="index.html#ltn.fuzzy_ops.OrProbSum" title="ltn.fuzzy_ops.OrProbSum"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.OrProbSum</span></code></a>) has vanishing gradients on the edge case <span class="math notranslate nohighlight">\(x = y = 1\)</span>;</p></li>
<li><p>the Gougen fuzzy implication (<a class="reference internal" href="index.html#ltn.fuzzy_ops.ImpliesGoguen" title="ltn.fuzzy_ops.ImpliesGoguen"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.ImpliesGoguen</span></code></a>) has vanishing gradients on the edge case <span class="math notranslate nohighlight">\(x = 0, y = 1\)</span>;</p></li>
<li><p>for other examples, refer to the appendix of the <a class="reference external" href="https://arxiv.org/abs/2012.13635">LTN paper</a>.</p></li>
</ul>
<p>To address these problems, LTNtorch provides additional stable versions for the fuzzy operators and aggregators with
gradient problems (unstable operators). In particular, the stable version can be accessed by setting the boolean
parameter <cite>stable</cite> of the constructor method of the operator to <cite>True</cite>. If the parameter <cite>stable</cite> does not appear in the
signature of the constructor, it means that the selected operator does not have gradient problems, so a stable version
for that specific operator is not required. See <a class="reference internal" href="index.html#ltn.fuzzy_ops.AndProd" title="ltn.fuzzy_ops.AndProd"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AndProd</span></code></a> for an example of
unstable operator. Notice the parameter <cite>stable</cite> appears in the signature of the constructor.</p>
<p>The stable versions are obtained in LTNtorch by applying the following projection functions to the inputs of the operators:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\pi_0:[0,1] \rightarrow ]0,1]: x \rightarrow(1-\epsilon) x+\epsilon\)</span>, to avoid having zeros in input to the operator;</p></li>
<li><p><span class="math notranslate nohighlight">\(\pi_1:[0,1] \rightarrow [0,1[: x \rightarrow(1-\epsilon) x\)</span>, to avoid having ones in input to the operator.</p></li>
</ul>
<p>In LTNtorch, <span class="math notranslate nohighlight">\(\epsilon\)</span> is set to 0.0001.</p>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-core"></span><section id="ltn-core">
<h2>ltn.core<a class="headerlink" href="#ltn-core" title="Link to this heading"></a></h2>
<section id="members">
<h3>Members<a class="headerlink" href="#members" title="Link to this heading"></a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a>(value, var_labels)</p></td>
<td><p>Class representing a generic <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN object</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.core.Constant" title="ltn.core.Constant"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.core.Constant</span></code></a>(value[, trainable])</p></td>
<td><p>Class representing an LTN constant.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.core.Variable" title="ltn.core.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.core.Variable</span></code></a>(var_label, individuals[, ...])</p></td>
<td><p>Class representing an LTN variable.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.core.Predicate" title="ltn.core.Predicate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.core.Predicate</span></code></a>([model, func])</p></td>
<td><p>Class representing an LTN predicate.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.core.Function" title="ltn.core.Function"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.core.Function</span></code></a>([model, func])</p></td>
<td><p>Class representing LTN functions.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.core.diag" title="ltn.core.diag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.core.diag</span></code></a>(*vars)</p></td>
<td><p>Sets the given LTN variables for <a class="reference internal" href="index.html#diagonal"><span class="std std-ref">diagonal quantification</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.core.undiag" title="ltn.core.undiag"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.core.undiag</span></code></a>(*vars)</p></td>
<td><p>Resets the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> for the given LTN variables.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.core.Connective" title="ltn.core.Connective"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.core.Connective</span></code></a>(connective_op)</p></td>
<td><p>Class representing an LTN connective.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.core.Quantifier" title="ltn.core.Quantifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.core.Quantifier</span></code></a>(agg_op, quantifier)</p></td>
<td><p>Class representing an LTN quantifier.</p></td>
</tr>
</tbody>
</table>
<p id="module-ltn.core">The <cite>ltn.core</cite> module contains the main functionalities of LTNtorch. In particular, it contains the definitions of
constants, variables, predicates, functions, connectives, and quantifiers.</p>
<dl class="py class">
<dt class="sig sig-object py" id="ltn.core.LTNObject">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.core.</span></span><span class="sig-name descname"><span class="pre">LTNObject</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">var_labels</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.LTNObject" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Class representing a generic <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN object</span></a>.</p>
<p>In LTNtorch, LTN objects are constants, variables, and outputs of predicates, formulas, functions, connectives,
and quantifiers.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>value</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>The <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> (value) of the LTN object.</p>
</dd>
<dt><strong>var_labels</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code></a> of <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></a></span></dt><dd><p>The labels of the free variables contained in the LTN object.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the types of the input parameters are incorrect.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>in LTNtorch, the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">groundings</span></a> of the LTN objects (symbols) are represented using PyTorch tensors, namely <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> instances;</p></li>
<li><p><cite>LTNObject</cite> is used by LTNtorch internally. The user should not create <cite>LTNObject</cite> instances by his/her own, unless strictly necessary.</p></li>
</ul>
<dl class="field-list">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>value</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>See <cite>value</cite> parameter.</p>
</dd>
<dt><strong>free_vars</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code></a> of <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></a></span></dt><dd><p>See <cite>var_labels</cite> parameter.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.core.LTNObject.shape">
<span class="sig-name descname"><span class="pre">shape</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.LTNObject.shape" title="Link to this definition"></a></dt>
<dd><p>Returns the shape of the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the LTN object.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code></a></dt><dd><p>The shape of the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the LTN object.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.core.Constant">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.core.</span></span><span class="sig-name descname"><span class="pre">Constant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.Constant" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">LTNObject</span></code></a></p>
<p>Class representing an LTN constant.</p>
<p>An LTN constant denotes an individual <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounded</span></a> as a tensor in the Real field.
The individual can be pre-defined (fixed data point) or learnable (embedding).</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>value</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>The <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the LTN constant. It can be a tensor of any order.</p>
</dd>
<dt><strong>trainable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=False</span></dt><dd><p>Flag indicating whether the LTN constant is trainable (embedding) or not.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>LTN constants are <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN objects</span></a>. <a class="reference internal" href="#ltn.core.Constant" title="ltn.core.Constant"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Constant</span></code></a> is a subclass of <a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a>;</p></li>
<li><p>the attribute <cite>free_vars</cite> for LTN constants is an empty list, since a constant does not have variables by definition;</p></li>
<li><p>if parameter <cite>trainable</cite> is set to <cite>True</cite>, the LTN constant becomes trainable, namely an embedding;</p></li>
<li><p>if parameter <cite>trainable</cite> is set to <cite>True</cite>, then the <cite>value</cite> attribute of the LTN constant will be used as an initialization for the embedding of the constant.</p></li>
</ul>
<p class="rubric">Examples</p>
<p>Non-trainable constant</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.4</span><span class="p">,</span> <span class="mf">5.4</span><span class="p">,</span> <span class="mf">4.3</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="go">Constant(value=tensor([3.4000, 5.4000, 4.3000]), free_vars=[])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([3.4000, 5.4000, 4.3000])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([3])</span>
</pre></div>
</div>
<p>Trainable constant</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t_c</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">3.4</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">5.6</span><span class="p">],</span>
<span class="gp">... </span>                                 <span class="p">[</span><span class="mf">6.7</span><span class="p">,</span> <span class="mf">5.6</span><span class="p">,</span> <span class="mf">4.3</span><span class="p">]]),</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">t_c</span><span class="p">)</span>
<span class="go">Constant(value=tensor([[3.4000, 2.3000, 5.6000],</span>
<span class="go">        [6.7000, 5.6000, 4.3000]], requires_grad=True), free_vars=[])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">t_c</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[3.4000, 2.3000, 5.6000],</span>
<span class="go">        [6.7000, 5.6000, 4.3000]], requires_grad=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">t_c</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">t_c</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.core.Variable">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.core.</span></span><span class="sig-name descname"><span class="pre">Variable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">var_label</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">individuals</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_batch_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.Variable" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">LTNObject</span></code></a></p>
<p>Class representing an LTN variable.</p>
<p>An LTN variable denotes a sequence of individuals. It is <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounded</span></a> as a sequence of
tensors (<a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">groundings</span></a> of individuals) in the real field.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>var_label</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></a></span></dt><dd><p>Name of the variable.</p>
</dd>
<dt><strong>individuals</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Sequence of individuals (tensors) that becomes the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> the LTN variable.</p>
</dd>
<dt><strong>add_batch_dim</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=True</span></dt><dd><p>Flag indicating whether a batch dimension (first dimension) has to be added to the
<cite>vale</cite> of the variable or not. If <cite>True</cite>, a dimension will be added only if the
<cite>value</cite> attribute of the LTN variable has one single dimension. In all the other cases, the first dimension
will be considered as batch dimension, so no dimension will be added.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the types of the input parameters are not correct.</p>
</dd>
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a></dt><dd><p>Raises when the value of the <cite>var_label</cite> parameter is not correct.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>LTN variables are <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN objects</span></a>. <a class="reference internal" href="#ltn.core.Variable" title="ltn.core.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Variable</span></code></a> is a subclass of <a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a>;</p></li>
<li><p>the first dimension of an LTN variable is associated with the number of individuals in the variable, while the other dimensions are associated with the features of the individuals;</p></li>
<li><p>setting <cite>add_batch_dim</cite> to <cite>False</cite> is useful, for instance, when an LTN variable is used to denote a sequence of indexes (for example indexes for retrieving values in tensors);</p></li>
<li><p>variable labels starting with ‘_diag’ are reserved for diagonal quantification (<a class="reference internal" href="#ltn.core.diag" title="ltn.core.diag"><code class="xref py py-func docutils literal notranslate"><span class="pre">ltn.core.diag()</span></code></a>).</p></li>
</ul>
<p class="rubric">Examples</p>
<p><cite>add_batch_dim=True</cite> has no effects on the variable since its <cite>value</cite> has more than one dimension, namely there is
already a batch dimension.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">3.4</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">6.7</span><span class="p">,</span> <span class="mf">9.6</span><span class="p">]]),</span> <span class="n">add_batch_dim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">Variable(value=tensor([[3.4000, 4.5000],</span>
<span class="go">        [6.7000, 9.6000]]), free_vars=[&#39;x&#39;])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[3.4000, 4.5000],</span>
<span class="go">        [6.7000, 9.6000]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;x&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2, 2])</span>
</pre></div>
</div>
<p><cite>add_bath_dim=True</cite> adds a batch dimension to the <cite>value</cite> of the variable since it has only one dimension.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.4</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mf">8.9</span><span class="p">]),</span> <span class="n">add_batch_dim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="go">Variable(value=tensor([[3.4000],</span>
<span class="go">        [4.5000],</span>
<span class="go">        [8.9000]]), free_vars=[&#39;y&#39;])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[3.4000],</span>
<span class="go">        [4.5000],</span>
<span class="go">        [8.9000]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;y&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([3, 1])</span>
</pre></div>
</div>
<p><cite>add_batch_dim=False</cite> tells to LTNtorch to not add a batch dimension to the <cite>value</cite> of the variable. This is useful
when a variable contains a sequence of indexes.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;z&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">add_batch_dim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="go">Variable(value=tensor([1, 2, 3]), free_vars=[&#39;z&#39;])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([1, 2, 3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;z&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.core.Predicate">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.core.</span></span><span class="sig-name descname"><span class="pre">Predicate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">func</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.Predicate" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Class representing an LTN predicate.</p>
<p>An LTN predicate is <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounded</span></a> as a mathematical function (either pre-defined or learnable)
that maps from some n-ary domain of individuals to a real number in [0,1] (fuzzy), which can be interpreted as a
truth value.</p>
<p>In LTNtorch, the inputs of a predicate are automatically broadcasted before the computation of the predicate,
if necessary. Moreover, the output is organized in a tensor where each dimension is related to
one variable given in input. See <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> for more information.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>model</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>, default=None</span></dt><dd><p>PyTorch model that becomes the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the LTN predicate.</p>
</dd>
<dt><strong>func</strong><span class="classifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">function</span></code>, default=None</span></dt><dd><p>Function that becomes the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the LTN predicate.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the types of the input parameters are incorrect.</p>
</dd>
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a></dt><dd><p>Raises when the values of the input parameters are incorrect.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>the output of an LTN predicate is always an <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN object</span></a> (<a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a>);</p></li>
<li><p>LTNtorch allows to define a predicate using a trainable model <strong>or</strong> a python function, not both;</p></li>
<li><p>defining a predicate using a python function is suggested only for simple and non-learnable mathematical operations;</p></li>
<li><p>examples of LTN predicates could be similarity measures, classifiers, etc;</p></li>
<li><p>the output of an LTN predicate must be always in the range [0., 1.]. Outputs outside of this range are not allowed;</p></li>
<li><p>evaluating a predicate with one variable of <span class="math notranslate nohighlight">\(n\)</span> individuals yields <span class="math notranslate nohighlight">\(n\)</span> output values, where the <span class="math notranslate nohighlight">\(i_{th}\)</span> output value corresponds to the predicate calculated with the <span class="math notranslate nohighlight">\(i_{th}\)</span> individual;</p></li>
<li><p>evaluating a predicate with <span class="math notranslate nohighlight">\(k\)</span> variables <span class="math notranslate nohighlight">\((x_1, \dots, x_k)\)</span> with respectively <span class="math notranslate nohighlight">\(n_1, \dots, n_k\)</span> individuals each, yields a result with <span class="math notranslate nohighlight">\(n_1 * \dots * n_k\)</span> values. The result is organized in a tensor where the first <span class="math notranslate nohighlight">\(k\)</span> dimensions can be indexed to retrieve the outcome(s) that correspond to each variable;</p></li>
<li><p>the attribute <cite>free_vars</cite> of the <cite>LTNobject</cite> output by the predicate tells which dimension corresponds to which variable in the <cite>value</cite> of the <cite>LTNObject</cite>. See <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> for more information;</p></li>
<li><p>to disable the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>, see <a class="reference internal" href="#ltn.core.diag" title="ltn.core.diag"><code class="xref py py-func docutils literal notranslate"><span class="pre">ltn.core.diag()</span></code></a>.</p></li>
</ul>
<p class="rubric">Examples</p>
<p>Unary predicate defined using a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Sequential</span></code></a>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predicate_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
<span class="gp">... </span>                        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
<span class="gp">... </span>                        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">(),</span>
<span class="gp">... </span>                        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>                        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">... </span>                  <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">predicate_model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="go">Predicate(model=Sequential(</span>
<span class="go">  (0): Linear(in_features=4, out_features=2, bias=True)</span>
<span class="go">  (1): ELU(alpha=1.0)</span>
<span class="go">  (2): Linear(in_features=2, out_features=1, bias=True)</span>
<span class="go">  (3): Sigmoid()</span>
<span class="go">))</span>
</pre></div>
</div>
<p>Unary predicate defined using a function. Note that <cite>torch.sum</cite> is performed on <cite>dim=1</cite>. This is because in LTNtorch
the first dimension (<cite>dim=0</cite>) is related to the batch dimension, while other dimensions are related to the features
of the individuals. Notice that the output of the print is <cite>Predicate(model=LambdaModel())</cite>. This indicates that the
LTN predicate has been defined using a function, through the <cite>func</cite> parameter of the constructor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">p_f</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p_f</span><span class="p">)</span>
<span class="go">Predicate(model=LambdaModel())</span>
</pre></div>
</div>
<p>Binary predicate defined using a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Note the call to <cite>torch.cat</cite> to merge
the two inputs of the binary predicate.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">PredicateModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">PredicateModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="n">elu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>
<span class="gp">... </span>        <span class="n">sigmoid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">... </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predicate_model</span> <span class="o">=</span> <span class="n">PredicateModel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b_p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">predicate_model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">b_p</span><span class="p">)</span>
<span class="go">Predicate(model=PredicateModel(</span>
<span class="go">  (dense1): Linear(in_features=4, out_features=5, bias=True)</span>
<span class="go">  (dense2): Linear(in_features=5, out_features=1, bias=True)</span>
<span class="go">))</span>
</pre></div>
</div>
<p>Binary predicate defined using a function. Note the call to <cite>torch.cat</cite> to merge the two inputs of the
binary predicate.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">b_p_f</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                            <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                        <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">b_p_f</span><span class="p">)</span>
<span class="go">Predicate(model=LambdaModel())</span>
</pre></div>
</div>
<p>Evaluation of a unary predicate on a constant. Note that:</p>
<ul class="simple">
<li><p>the predicate returns a <a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a> instance;</p></li>
<li><p>since a constant has been given, the <cite>LTNObject</cite> in output does not have free variables;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is empty since the predicate has been evaluated on a constant, namely on one single individual;</p></li>
<li><p>the attribute <cite>value</cite> of the <cite>LTNObject</cite> in output contains the result of the evaluation of the predicate;</p></li>
<li><p>the <cite>value</cite> is in the range [0., 1.] since it has to be interpreted as a truth value. This is assured thanks to the <em>sigmoid function</em> in the definition of the predicate.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.34</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">p_f</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
<span class="go">&lt;class &#39;ltn.core.LTNObject&#39;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor(0.7008), free_vars=[])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor(0.7008)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([])</span>
</pre></div>
</div>
<p>Evaluation of a unary predicate on a variable. Note that:</p>
<ul class="simple">
<li><p>since a variable has been given, the <cite>LTNObject</cite> in output has one free variable;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is 2 since the predicate has been evaluated on a variable with two individuls.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.32</span><span class="p">,</span> <span class="mf">0.043</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">p_f</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor([0.6682, 0.5898]), free_vars=[&#39;v&#39;])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6682, 0.5898])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;v&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2])</span>
</pre></div>
</div>
<p>Evaluation of a binary predicate on a variable and a constant. Note that:</p>
<ul class="simple">
<li><p>like in the previous example, the <cite>LTNObject</cite> in output has just one free variable, since only one variable has been given to the predicate;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is 2 since the predicate has been evaluated on a variable with two individuals. The constant does not add dimensions to the output.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.32</span><span class="p">,</span> <span class="mf">0.043</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.23</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">b_p_f</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor([0.8581, 0.8120]), free_vars=[&#39;v&#39;])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.8581, 0.8120])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;v&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2])</span>
</pre></div>
</div>
<p>Evaluation of a binary predicate on two variables. Note that:</p>
<ul class="simple">
<li><p>since two variables have been given, the <cite>LTNObject</cite> in output has two free variables;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is <cite>(2, 3)</cite> since the predicate has been evaluated on a variable with two individuals and a variable with three individuals;</p></li>
<li><p>the first dimension is dedicated to variable <cite>x</cite>, which is also the first one appearing in <cite>free_vars</cite>, while the second dimension is dedicated to variable <cite>y</cite>, which is the second one appearing in <cite>free_vars</cite>;</p></li>
<li><p>it is possible to access the <cite>value</cite> attribute for getting the results of the predicate. For example, at position <cite>(1, 2)</cite> there is the evaluation of the predicate on the second individual of <cite>x</cite> and third individuals of <cite>y</cite>.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.32</span><span class="p">,</span> <span class="mf">0.043</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.23</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.32</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">b_p_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor([[0.7974, 0.7790, 0.7577],</span>
<span class="go">        [0.7375, 0.7157, 0.6906]]), free_vars=[&#39;x&#39;, &#39;y&#39;])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[0.7974, 0.7790, 0.7577],</span>
<span class="go">        [0.7375, 0.7157, 0.6906]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;x&#39;, &#39;y&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2, 3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">tensor(0.6906)</span>
</pre></div>
</div>
<dl class="field-list">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>model</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">function</span></code></span></dt><dd><p>The <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the LTN predicate.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.core.Predicate.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.Predicate.forward" title="Link to this definition"></a></dt>
<dd><p>It computes the output of the predicate given some <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN objects</span></a> in input.</p>
<p>Before computing the predicate, it performs the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> of the inputs.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>inputs</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a></span></dt><dd><p>Tuple of <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN objects</span></a> for which the predicate has to be computed.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a></dt><dd><p>An <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTNObject</span></a> whose <cite>value</cite> attribute contains the truth values representing the result of the
predicate, while <cite>free_vars</cite> attribute contains the labels of the free variables contained in the result.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the types of the inputs are incorrect.</p>
</dd>
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a></dt><dd><p>Raises when the values of the output are not in the range [0., 1.].</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.core.Function">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.core.</span></span><span class="sig-name descname"><span class="pre">Function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">func</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.Function" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Class representing LTN functions.</p>
<p>An LTN function is <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounded</span></a> as a mathematical function (either pre-defined or learnable)
that maps from some n-ary domain of individuals to a tensor (individual) in the Real field.</p>
<p>In LTNtorch, the inputs of a function are automatically broadcasted before the computation of the function,
if necessary. Moreover, the output is organized in a tensor where the first <span class="math notranslate nohighlight">\(k\)</span> dimensions are related
with the <span class="math notranslate nohighlight">\(k\)</span> variables given in input, while the last dimensions are related with the features of the
individual in output. See <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> for more information.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>model</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>, default=None</span></dt><dd><p>PyTorch model that becomes the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the LTN function.</p>
</dd>
<dt><strong>func</strong><span class="classifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">function</span></code>, default=None</span></dt><dd><p>Function that becomes the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the LTN function.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the types of the input parameters are incorrect.</p>
</dd>
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a></dt><dd><p>Raises when the values of the input parameters are incorrect.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>the output of an LTN function is always an <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN object</span></a> (<a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a>);</p></li>
<li><p>LTNtorch allows to define a function using a trainable model <strong>or</strong> a python function, not both;</p></li>
<li><p>defining an LTN function using a python function is suggested only for simple and non-learnable mathematical operations;</p></li>
<li><p>examples of LTN functions could be distance functions, regressors, etc;</p></li>
<li><p>differently from LTN predicates, the output of an LTN function has no constraints;</p></li>
<li><p>evaluating a function with one variable of <span class="math notranslate nohighlight">\(n\)</span> individuals yields <span class="math notranslate nohighlight">\(n\)</span> output values, where the <span class="math notranslate nohighlight">\(i_{th}\)</span> output value corresponds to the function calculated with the <span class="math notranslate nohighlight">\(i_{th}\)</span> individual;</p></li>
<li><p>evaluating a function with <span class="math notranslate nohighlight">\(k\)</span> variables <span class="math notranslate nohighlight">\((x_1, \dots, x_k)\)</span> with respectively <span class="math notranslate nohighlight">\(n_1, \dots, n_k\)</span> individuals each, yields a result with <span class="math notranslate nohighlight">\(n_1 * \dots * n_k\)</span> values. The result is organized in a tensor where the first <span class="math notranslate nohighlight">\(k\)</span> dimensions can be indexed to retrieve the outcome(s) that correspond to each variable;</p></li>
<li><p>the attribute <cite>free_vars</cite> of the <cite>LTNobject</cite> output by the function tells which dimension corresponds to which variable in the <cite>value</cite> of the <cite>LTNObject</cite>. See <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> for more information;</p></li>
<li><p>to disable the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>, see <a class="reference internal" href="#ltn.core.diag" title="ltn.core.diag"><code class="xref py py-func docutils literal notranslate"><span class="pre">ltn.core.diag()</span></code></a>.</p></li>
</ul>
<p class="rubric">Examples</p>
<p>Unary function defined using a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Sequential</span></code></a>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">function_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
<span class="gp">... </span>                        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
<span class="gp">... </span>                        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">(),</span>
<span class="gp">... </span>                        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">... </span>                  <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Function</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">function_model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="go">Function(model=Sequential(</span>
<span class="go">  (0): Linear(in_features=4, out_features=3, bias=True)</span>
<span class="go">  (1): ELU(alpha=1.0)</span>
<span class="go">  (2): Linear(in_features=3, out_features=2, bias=True)</span>
<span class="go">))</span>
</pre></div>
</div>
<p>Unary function defined using a function. Note that <cite>torch.sum</cite> is performed on <cite>dim=1</cite>. This is because in LTNtorch
the first dimension (<cite>dim=0</cite>) is related to the batch dimension, while other dimensions are related to the features
of the individuals. Notice that the output of the print is <cite>Function(model=LambdaModel())</cite>. This indicates that the
LTN function has been defined using a function, through the <cite>func</cite> parameter of the constructor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">f_f</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Function</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
<span class="gp">... </span>                                             <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                        <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">f_f</span><span class="p">)</span>
<span class="go">Function(model=LambdaModel())</span>
</pre></div>
</div>
<p>Binary function defined using a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Note the call to <cite>torch.cat</cite> to merge
the two inputs of the binary function.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">FunctionModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">FunctionModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="n">elu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">dense2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">... </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">function_model</span> <span class="o">=</span> <span class="n">FunctionModel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b_f</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Function</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">function_model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">b_f</span><span class="p">)</span>
<span class="go">Function(model=FunctionModel(</span>
<span class="go">  (dense1): Linear(in_features=4, out_features=5, bias=True)</span>
<span class="go">))</span>
</pre></div>
</div>
<p>Binary function defined using a function. Note the call to <cite>torch.cat</cite> to merge the two inputs of the
binary function.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">b_f_f</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Function</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span>
<span class="gp">... </span>                                <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                                    <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">b_f_f</span><span class="p">)</span>
<span class="go">Function(model=LambdaModel())</span>
</pre></div>
</div>
<p>Evaluation of a unary function on a constant. Note that:</p>
<ul class="simple">
<li><p>the function returns a <a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a> instance;</p></li>
<li><p>since a constant has been given, the <cite>LTNObject</cite> in output does not have free variables;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is <cite>(2)</cite> since the function has been evaluated on a constant, namely on one single individual, and returns individuals in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>;</p></li>
<li><p>the attribute <cite>value</cite> of the <cite>LTNObject</cite> in output contains the result of the evaluation of the function.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.34</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">f_f</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
<span class="go">&lt;class &#39;ltn.core.LTNObject&#39;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor([0.8510, 0.8510]), free_vars=[])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.8510, 0.8510])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2])</span>
</pre></div>
</div>
<p>Evaluation of a unary function on a variable. Note that:</p>
<ul class="simple">
<li><p>since a variable has been given, the <cite>LTNObject</cite> in output has one free variable;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is <cite>(2, 2)</cite> since the function has been evaluated on a variable with two individuls and returns individuals in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.32</span><span class="p">,</span> <span class="mf">0.043</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">f_f</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor([[0.7000, 0.7000],</span>
<span class="go">        [0.3630, 0.3630]]), free_vars=[&#39;v&#39;])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[0.7000, 0.7000],</span>
<span class="go">        [0.3630, 0.3630]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;v&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2, 2])</span>
</pre></div>
</div>
<p>Evaluation of a binary function on a variable and a constant. Note that:</p>
<ul class="simple">
<li><p>like in the previous example, the <cite>LTNObject</cite> in output has just one free variable, since only one variable has been given to the predicate;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is <cite>(2, 2)</cite> since the function has been evaluated on a variable with two individuals and returns individuals in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>. The constant does not add dimensions to the output.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.32</span><span class="p">,</span> <span class="mf">0.043</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.23</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">b_f_f</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor([[1.8000, 1.8000],</span>
<span class="go">        [1.4630, 1.4630]]), free_vars=[&#39;v&#39;])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[1.8000, 1.8000],</span>
<span class="go">        [1.4630, 1.4630]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;v&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2, 2])</span>
</pre></div>
</div>
<p>Evaluation of a binary function on two variables. Note that:</p>
<ul class="simple">
<li><p>since two variables have been given, the <cite>LTNObject</cite> in output has two free variables;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is <cite>(2, 3, 2)</cite> since the function has been evaluated on a variable with two individuals, a variable with three individuals, and returns individuals in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>;</p></li>
<li><p>the first dimension is dedicated to variable <cite>x</cite>, which is also the first one appearing in <cite>free_vars</cite>, the second dimension is dedicated to variable <cite>y</cite>, which is the second one appearing in <cite>free_vars</cite>, while the last dimensions is dedicated to the features of the individuals in output;</p></li>
<li><p>it is possible to access the <cite>value</cite> attribute for getting the results of the function. For example, at position <cite>(1, 2)</cite> there is the evaluation of the function on the second individual of <cite>x</cite> and third individuals of <cite>y</cite>.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.32</span><span class="p">,</span> <span class="mf">0.043</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.23</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.32</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">b_f_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor([[[1.3700, 1.3700],</span>
<span class="go">         [1.2600, 1.2600],</span>
<span class="go">         [1.1400, 1.1400]],</span>

<span class="go">        [[1.0330, 1.0330],</span>
<span class="go">         [0.9230, 0.9230],</span>
<span class="go">         [0.8030, 0.8030]]]), free_vars=[&#39;x&#39;, &#39;y&#39;])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[[1.3700, 1.3700],</span>
<span class="go">         [1.2600, 1.2600],</span>
<span class="go">         [1.1400, 1.1400]],</span>

<span class="go">        [[1.0330, 1.0330],</span>
<span class="go">         [0.9230, 0.9230],</span>
<span class="go">         [0.8030, 0.8030]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;x&#39;, &#39;y&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2, 3, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">tensor([0.8030, 0.8030])</span>
</pre></div>
</div>
<dl class="field-list">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>model</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">function</span></code></span></dt><dd><p>The <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the LTN function.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.core.Function.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.Function.forward" title="Link to this definition"></a></dt>
<dd><p>It computes the output of the function given some <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN objects</span></a> in input.</p>
<p>Before computing the function, it performs the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> of the inputs.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>inputs</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a></span></dt><dd><p>Tuple of <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN objects</span></a> for which the function has to be computed.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a></dt><dd><p>An <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTNObject</span></a> whose <cite>value</cite> attribute contains the result of the
function, while <cite>free_vars</cite> attribute contains the labels of the free variables contained in the result.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the types of the inputs are incorrect.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ltn.core.diag">
<span class="sig-prename descclassname"><span class="pre">ltn.core.</span></span><span class="sig-name descname"><span class="pre">diag</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">vars</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.diag" title="Link to this definition"></a></dt>
<dd><p>Sets the given LTN variables for <a class="reference internal" href="index.html#diagonal"><span class="std std-ref">diagonal quantification</span></a>.</p>
<p>The diagonal quantification disables the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> for the given variables.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>vars</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference internal" href="#ltn.core.Variable" title="ltn.core.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Variable</span></code></a></span></dt><dd><p>Tuple of LTN variables for which the diagonal quantification has to be set.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code></a> of <a class="reference internal" href="#ltn.core.Variable" title="ltn.core.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Variable</span></code></a></dt><dd><p>List of the same LTN variables given in input, prepared for the use of <a class="reference internal" href="index.html#diagonal"><span class="std std-ref">diagonal quantification</span></a>.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the types of the input parameters are incorrect.</p>
</dd>
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a></dt><dd><p>Raises when the values of the input parameters are incorrect.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#ltn.core.undiag" title="ltn.core.undiag"><code class="xref py py-func docutils literal notranslate"><span class="pre">ltn.core.undiag()</span></code></a></dt><dd><p>It allows to disable the diagonal quantification for the given variables.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>diagonal quantification has been designed to work with quantified statements, however, it could be used also to reduce the combinations of individuals for which a predicate has to be computed, making the computation more efficient;</p></li>
<li><p>diagonal quantification is particularly useful when we need to compute a predicate, or function, on specific tuples of variables’ individuals only;</p></li>
<li><p>diagonal quantification expects the given variables to have the same number of individuals.</p></li>
</ul>
<p class="rubric">Examples</p>
<p>Behavior of a predicate without diagonal quantification. Note that:</p>
<ul class="simple">
<li><p>if diagonal quantification is not used, LTNtorch applies the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> to the variables before computing the predicate;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is <cite>(2, 2)</cite> since the predicate has been computed on two variables with two individuals each;</p></li>
<li><p>the <cite>free_vars</cite> attribute of the <cite>LTNObject</cite> in output contains two variables, namely the variables on which the predicate has been computed.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                        <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                    <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.56</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.004</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.32</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[0.8447, 0.8710],</span>
<span class="go">        [0.7763, 0.8115]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;x&#39;, &#39;y&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2, 2])</span>
</pre></div>
</div>
<p>Behavior of the same predicate with diagonal quantification. Note that:</p>
<ul class="simple">
<li><p>diagonal quantification requires the two variables to have the same number of individuals;</p></li>
<li><p>diagonal quantification has disabled the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>, namely the predicate is not computed on all the possible combinations of individuals of the two variables (that are 2x2). Instead, it is computed only on the given tuples of individuals (that are 2), namely on the first individual of <cite>x</cite> and first individual of <cite>y</cite>, and on the second individual of <cite>x</cite> and second individual of <cite>y</cite>;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is <cite>(2)</cite> since diagonal quantification has been set and the variables have two individuals;</p></li>
<li><p>the <cite>free_vars</cite> attribute of the <cite>LTNObject</cite> in output has just one variable, even if two variables have been given to the predicate. This is due to diagonal quantification;</p></li>
<li><p>when diagonal quantification is set, you will se a variable label starting with <cite>diag_</cite> in the <cite>free_Vars</cite> attribute.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.8447, 0.8115])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;diag_x_y&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2])</span>
</pre></div>
</div>
<p>See the examples under <a class="reference internal" href="#ltn.core.Quantifier" title="ltn.core.Quantifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Quantifier</span></code></a> to see how to use <a class="reference internal" href="#ltn.core.diag" title="ltn.core.diag"><code class="xref py py-func docutils literal notranslate"><span class="pre">ltn.core.diag()</span></code></a> with quantifiers.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ltn.core.undiag">
<span class="sig-prename descclassname"><span class="pre">ltn.core.</span></span><span class="sig-name descname"><span class="pre">undiag</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">vars</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.undiag" title="Link to this definition"></a></dt>
<dd><p>Resets the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> for the given LTN variables.</p>
<p>In other words, it removes the <a class="reference internal" href="index.html#diagonal"><span class="std std-ref">diagonal quantification</span></a> setting from the given variables.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>vars</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference internal" href="#ltn.core.Variable" title="ltn.core.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Variable</span></code></a></span></dt><dd><p>Tuple of LTN variables for which the <a class="reference internal" href="index.html#diagonal"><span class="std std-ref">diagonal quantification</span></a> setting has to be removed.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code></a></dt><dd><p>List of the same LTN variables given in input, with the <a class="reference internal" href="index.html#diagonal"><span class="std std-ref">diagonal quantification</span></a> setting removed.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the types of the input parameters are incorrect.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#ltn.core.diag" title="ltn.core.diag"><code class="xref py py-func docutils literal notranslate"><span class="pre">ltn.core.diag()</span></code></a></dt><dd><p>It allows to set the <a class="reference internal" href="index.html#diagonal"><span class="std std-ref">diagonal quantification</span></a> for the given variables.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<p>Behavior of predicate with diagonal quantification. Note that:</p>
<ul class="simple">
<li><p>diagonal quantification requires the two variables to have the same number of individuals;</p></li>
<li><p>diagonal quantification has disabled the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>, namely the predicate is not computed on all the possible combinations of individuals of the two variables (that are 2x2). Instead, it is computed only on the given tuples of individuals (that are 2), namely on the first individual of <cite>x</cite> and first individual of <cite>y</cite>, and on the second individual of <cite>x</cite> and second individual of <cite>y</cite>;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is <cite>(2)</cite> since diagonal quantification has been set and the variables have two individuals;</p></li>
<li><p>the <cite>free_vars</cite> attribute of the <cite>LTNObject</cite> in output has just one variable, even if two variables have been given to the predicate. This is due to diagonal quantification;</p></li>
<li><p>when diagonal quantification is set, you will se a variable label starting with <cite>diag_</cite> in the <cite>free_Vars</cite> attribute.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                        <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                    <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.56</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.004</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.32</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.8447, 0.8115])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;diag_x_y&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2])</span>
</pre></div>
</div>
<p><a class="reference internal" href="#ltn.core.undiag" title="ltn.core.undiag"><code class="xref py py-func docutils literal notranslate"><span class="pre">ltn.core.undiag()</span></code></a> can be used to restore the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> for the two variables. In
the following, it is shown the behavior of the same predicate without diagonal quantification. Note that:</p>
<ul class="simple">
<li><p>since diagonal quantification has been disabled, LTNtorch applies the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> to the variables before computing the predicate;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is <cite>(2, 2)</cite> since the predicate has been computed on two variables with two individuals each;</p></li>
<li><p>the <cite>free_vars</cite> attribute of the <cite>LTNObject</cite> in output contains two variables, namely the variables on which the predicate has been computed.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">undiag</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[0.8447, 0.8710],</span>
<span class="go">        [0.7763, 0.8115]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;x&#39;, &#39;y&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2, 2])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.core.Connective">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.core.</span></span><span class="sig-name descname"><span class="pre">Connective</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">connective_op</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.Connective" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Class representing an LTN connective.</p>
<p>An LTN connective is <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounded</span></a> as a fuzzy connective operator.</p>
<p>In LTNtorch, the inputs of a connective are automatically broadcasted before the computation of the connective,
if necessary. Moreover, the output is organized in a tensor where each dimension is related to
one variable appearing in the inputs. See <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> for more information.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>connective_op</strong><span class="classifier"><a class="reference internal" href="index.html#ltn.fuzzy_ops.ConnectiveOperator" title="ltn.fuzzy_ops.ConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.ConnectiveOperator</span></code></a></span></dt><dd><p>The unary/binary fuzzy connective operator that becomes the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the LTN connective.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the type of the input parameter is incorrect.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="index.html#module-ltn.fuzzy_ops" title="ltn.fuzzy_ops"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops</span></code></a></dt><dd><p>The <cite>ltn.fuzzy_ops</cite> module contains the definition of common fuzzy connective operators that can be used with LTN connectives.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>the LTN connective supports various fuzzy connective operators. They can be found in <a class="reference internal" href="index.html#fuzzyop"><span class="std std-ref">ltn.fuzzy_ops</span></a>;</p></li>
<li><p>the LTN connective allows to use these fuzzy operators with LTN formulas. It takes care of combining sub-formulas which have different variables appearing in them (<a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>).</p></li>
<li><p>an LTN connective can be applied only to <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN objects</span></a> containing truth values, namely values in <span class="math notranslate nohighlight">\([0., 1.]\)</span>;</p></li>
<li><p>the output of an LTN connective is always an <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN object</span></a> (<a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a>).</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.core.Connective.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">operands</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.Connective.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the selected fuzzy connective operator (<cite>connective_op</cite> attribute) to the operands
(<a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN objects</span></a>) given in input.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>operands</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a></span></dt><dd><p>Tuple of <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN objects</span></a> representing the operands to which the fuzzy connective
operator has to be applied.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a></dt><dd><p>The <cite>LTNObject</cite> that is the result of the application of the fuzzy connective operator to the given
<a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN objects</span></a>.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the types of the input parameters are incorrect.</p>
</dd>
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a></dt><dd><p>Raises when the values of the input parameters are incorrect.
Raises when the truth values of the operands given in input are not in the range [0., 1.].</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<p>Use of <span class="math notranslate nohighlight">\(\land\)</span> to create a formula which is the conjunction of two predicates. Note that:</p>
<ul class="simple">
<li><p>a connective operator can be applied only to inputs which represent truth values. In this case with have two predicates;</p></li>
<li><p>LTNtorch provides various semantics for the conjunction, here we use the Goguen conjunction (<a class="reference internal" href="index.html#ltn.fuzzy_ops.AndProd" title="ltn.fuzzy_ops.AndProd"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AndProd</span></code></a>);</p></li>
<li><p>LTNtorch applies the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> to the variables before computing the predicates;</p></li>
<li><p>LTNtorch applies the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN brodcasting</span></a> to the operands before applying the selected conjunction operator;</p></li>
<li><p>the result of a connective operator is a <a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a> instance containing truth values in [0., 1.];</p></li>
<li><p>the attribute <cite>value</cite> of the <cite>LTNObject</cite> in output contains the result of the connective operator;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is <cite>(2, 3, 4)</cite>. The first dimension is associated with variable <cite>x</cite>, which has two individuals, the second dimension with variable <cite>y</cite>, which has three individuals, while the last dimension with variable <cite>z</cite>, which has four individuals;</p></li>
<li><p>it is possible to access to specific results by indexing the attribute <cite>value</cite>. For example, at index <cite>(0, 1, 2)</cite> there is the evaluation of the formula on the first individual of <cite>x</cite>, second individual of <cite>y</cite>, and third individual of <cite>z</cite>;</p></li>
<li><p>the attribute <cite>free_vars</cite> of the <cite>LTNObject</cite> in output contains the labels of the three variables appearing in the formula.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                        <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>                                    <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.23</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">4.3</span><span class="p">,</span> <span class="mf">9.3</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">4.3</span><span class="p">,</span> <span class="mf">0.32</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;z&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">4.3</span><span class="p">,</span> <span class="mf">5.1</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">4.3</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">And</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AndProd</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">And</span><span class="p">)</span>
<span class="go">Connective(connective_op=AndProd(stable=True))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">And</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">q</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor([[[0.5971, 0.6900, 0.6899, 0.6391],</span>
<span class="go">         [0.6900, 0.6900, 0.6900, 0.6900],</span>
<span class="go">         [0.6878, 0.6900, 0.6900, 0.6889]],</span>

<span class="go">        [[0.5325, 0.6154, 0.6153, 0.5700],</span>
<span class="go">         [0.6154, 0.6154, 0.6154, 0.6154],</span>
<span class="go">         [0.6135, 0.6154, 0.6154, 0.6144]]]), free_vars=[&#39;x&#39;, &#39;y&#39;, &#39;z&#39;])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[[0.5971, 0.6900, 0.6899, 0.6391],</span>
<span class="go">         [0.6900, 0.6900, 0.6900, 0.6900],</span>
<span class="go">         [0.6878, 0.6900, 0.6900, 0.6889]],</span>

<span class="go">        [[0.5325, 0.6154, 0.6153, 0.5700],</span>
<span class="go">         [0.6154, 0.6154, 0.6154, 0.6154],</span>
<span class="go">         [0.6135, 0.6154, 0.6154, 0.6144]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;x&#39;, &#39;y&#39;, &#39;z&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2, 3, 4])</span>
</pre></div>
</div>
<dl class="field-list">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>connective_op</strong><span class="classifier"><a class="reference internal" href="index.html#ltn.fuzzy_ops.ConnectiveOperator" title="ltn.fuzzy_ops.ConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.ConnectiveOperator</span></code></a></span></dt><dd><p>See <cite>connective_op</cite> parameter.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.core.Quantifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.core.</span></span><span class="sig-name descname"><span class="pre">Quantifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">agg_op</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantifier</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.Quantifier" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Class representing an LTN quantifier.</p>
<p>An LTN quantifier is <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounded</span></a> as a fuzzy aggregation operator. See <a class="reference internal" href="index.html#quantification"><span class="std std-ref">quantification in LTN</span></a>
for more information about quantification.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>agg_op</strong><span class="classifier"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AggregationOperator" title="ltn.fuzzy_ops.AggregationOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AggregationOperator</span></code></a></span></dt><dd><p>The fuzzy aggregation operator that becomes the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the LTN quantifier.</p>
</dd>
<dt><strong>quantifier</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></a></span></dt><dd><p>String indicating the quantification that has to be performed (‘e’ for ∃, or ‘f’ for ∀).</p>
</dd>
</dl>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the type of the <cite>agg_op</cite> parameter is incorrect.</p>
</dd>
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a></dt><dd><p>Raises when the value of the <cite>quantifier</cite> parameter is incorrect.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="index.html#module-ltn.fuzzy_ops" title="ltn.fuzzy_ops"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops</span></code></a></dt><dd><p>The <cite>ltn.fuzzy_ops</cite> module contains the definition of common fuzzy aggregation operators that can be used with LTN quantifiers.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>the LTN quantifier supports various fuzzy aggregation operators, which can be found in <a class="reference internal" href="index.html#module-ltn.fuzzy_ops" title="ltn.fuzzy_ops"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops</span></code></a>;</p></li>
<li><p>the LTN quantifier allows to use these fuzzy aggregators with LTN formulas. It takes care of selecting the formula (<cite>LTNObject</cite>) dimensions to aggregate, given some LTN variables in arguments.</p></li>
<li><p>boolean conditions (by setting parameters <cite>mask_fn</cite> and <cite>mask_vars</cite>) can be used for <a class="reference internal" href="index.html#guarded"><span class="std std-ref">guarded quantification</span></a>;</p></li>
<li><p>an LTN quantifier can be applied only to <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN objects</span></a> containing truth values, namely values in <span class="math notranslate nohighlight">\([0., 1.]\)</span>;</p></li>
<li><p>the output of an LTN quantifier is always an <a class="reference internal" href="index.html#noteltnobject"><span class="std std-ref">LTN object</span></a> (<a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a>).</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.core.Quantifier.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vars</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formula</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cond_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cond_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.core.Quantifier.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the selected aggregation operator (<cite>agg_op</cite> attribute) to the formula given in input based on the
selected variables.</p>
<p>It allows also to perform a <a class="reference internal" href="index.html#guarded"><span class="std std-ref">guarded quantification</span></a> by setting <cite>cond_vars</cite> and <cite>cond_fn</cite>
parameters.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>vars</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code></a> of <a class="reference internal" href="#ltn.core.Variable" title="ltn.core.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Variable</span></code></a></span></dt><dd><p>List of LTN variables on which the quantification has to be performed.</p>
</dd>
<dt><strong>formula</strong><span class="classifier"><a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a></span></dt><dd><p>Formula on which the quantification has to be performed.</p>
</dd>
<dt><strong>cond_vars</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code></a> of <a class="reference internal" href="#ltn.core.Variable" title="ltn.core.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Variable</span></code></a>, default=None</span></dt><dd><p>List of LTN variables that appear in the <a class="reference internal" href="index.html#guarded"><span class="std std-ref">guarded quantification</span></a> condition.</p>
</dd>
<dt><strong>cond_fn</strong><span class="classifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">function</span></code>, default=None</span></dt><dd><p>Function representing the <a class="reference internal" href="index.html#guarded"><span class="std std-ref">guarded quantification</span></a> condition.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the types of the input parameters are incorrect.</p>
</dd>
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a></dt><dd><p>Raises when the values of the input parameters are incorrect.
Raises when the truth values of the formula given in input are not in the range [0., 1.].</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<p>Behavior of a binary predicate evaluated on two variables. Note that:</p>
<ul class="simple">
<li><p>the shape of the <cite>LTNObject</cite> in output is <cite>(2, 3)</cite> since the predicate has been computed on a variable with two individuals and a variable with three individuals;</p></li>
<li><p>the attribute <cite>free_vars</cite> of the <cite>LTNObject</cite> in output contains the labels of the two variables given in input to the predicate.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                        <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>                                    <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor([[0.9900, 0.9994, 0.9988],</span>
<span class="go">        [0.9734, 0.9985, 0.9967]]), free_vars=[&#39;x&#39;, &#39;y&#39;])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[0.9900, 0.9994, 0.9988],</span>
<span class="go">        [0.9734, 0.9985, 0.9967]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;x&#39;, &#39;y&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([2, 3])</span>
</pre></div>
</div>
<p>Universal quantification on one single variable of the same predicate. Note that:</p>
<ul class="simple">
<li><p><cite>quantifier=’f’</cite> means that we are defining the fuzzy semantics for the universal quantifier;</p></li>
<li><p>the result of a quantification operator is always a <a class="reference internal" href="#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a> instance;</p></li>
<li><p>LTNtorch supports various sematics for quantifiers, here we use <a class="reference internal" href="index.html#ltn.fuzzy_ops.AggregPMeanError" title="ltn.fuzzy_ops.AggregPMeanError"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AggregPMeanError</span></code></a> for <span class="math notranslate nohighlight">\(\forall\)</span>;</p></li>
<li><p>the shape of the <cite>LTNObject</cite> in output is <cite>(3)</cite> since the quantification has been performed on variable <cite>x</cite>. Only the dimension associated with variable <cite>y</cite> has left since the quantification has been computed by LTNtorch as an aggregation on the dimension related with variable <cite>x</cite>;</p></li>
<li><p>the attribute <cite>free_vars</cite> of the <cite>LTNObject</cite> in output contains only the label of variable <cite>y</cite>. This is because variable <cite>x</cite> has been quantified, namely it is not a free variable anymore;</p></li>
<li><p>in LTNtorch, the quantification is performed by computing the value of the predicate first and then by aggregating on the selected dimensions, specified by the quantified variables.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Forall</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Quantifier</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AggregPMeanError</span><span class="p">(),</span> <span class="n">quantifier</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Forall</span><span class="p">)</span>
<span class="go">Quantifier(agg_op=AggregPMeanError(p=2, stable=True), quantifier=&#39;f&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">Forall</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor([0.9798, 0.9988, 0.9974]), free_vars=[&#39;y&#39;])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.9798, 0.9988, 0.9974])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[&#39;y&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([3])</span>
</pre></div>
</div>
<p>Universal quantification on both variables of the same predicate. Note that:</p>
<ul class="simple">
<li><p>the shape of the <cite>LTNObject</cite> in output is empty since the quantification has been performed on both variables. No dimension has left since the quantification has been computed by LTNtorch as an aggregation on both dimensions of the <cite>value</cite> of the predicate;</p></li>
<li><p>the attribute <cite>free_vars</cite> of the <cite>LTNObject</cite> in output contains no labels of variables. This is because both variables have been quantified, namely they are not free variables anymore.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">Forall</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor(0.9882), free_vars=[])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor(0.9882)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([])</span>
</pre></div>
</div>
<p>Universal quantification on one variable, and existential quantification on the other variable, of the same predicate.
Note that:</p>
<ul class="simple">
<li><p>the only way in LTNtorch to apply two different quantifiers to the same formula is a nested syntax;</p></li>
<li><p><cite>quantifier=’e’</cite> means that we are defining the fuzzy semantics for the existential quantifier;</p></li>
<li><p>LTNtorch supports various sematics for quantifiers, here we use <a class="reference internal" href="index.html#ltn.fuzzy_ops.AggregPMean" title="ltn.fuzzy_ops.AggregPMean"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AggregPMean</span></code></a> for <span class="math notranslate nohighlight">\(\exists\)</span>.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Exists</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Quantifier</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AggregPMean</span><span class="p">(),</span> <span class="n">quantifier</span><span class="o">=</span><span class="s1">&#39;e&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Exists</span><span class="p">)</span>
<span class="go">Quantifier(agg_op=AggregPMean(p=2, stable=True), quantifier=&#39;e&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">Forall</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Exists</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor(0.9920), free_vars=[])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor(0.9920)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([])</span>
</pre></div>
</div>
<p>Guarded quantification. We perform a universal quantification on both variables of the same predicate, considering
only the individuals of variable <cite>x</cite> whose sum of features is lower than a certain threshold. Note that:</p>
<ul class="simple">
<li><p>guarded quantification requires the parameters <cite>cond_vars</cite> and <cite>cond_fn</cite> to be set;</p></li>
<li><p><cite>cond_vars</cite> contains the variables on which the guarded condition is based on. In this case, we have decided to create a condition on <cite>x</cite>;</p></li>
<li><p><cite>cond_fn</cite> contains the function which is the guarded condition. In this case, it verifies if the sum of features of the individuals of <cite>x</cite> is lower than 1. (our threshold);</p></li>
<li><p>the second individual of <cite>x</cite>, which is <cite>[0.3, 0.3]</cite>, satisfies the condition, namely it will not be considered when the aggregation has to be performed. In other words, all the results of the predicate computed using the second individual of <cite>x</cite> will not be considered in the aggregation;</p></li>
<li><p>notice the result changes compared to the previous example (<span class="math notranslate nohighlight">\(\forall x \forall y P(x, y)\)</span>). This is due to the fact that some truth values of the result of the predicate are not considered in the aggregation due to guarded quantification. These values are at positions <cite>(1, 0)</cite>, <cite>(1, 1)</cite>, and <cite>(1, 2)</cite>, namely all the positions related with the second individual of <cite>x</cite> in the result of the predicate;</p></li>
<li><p>notice that the shape of the <cite>LTNObject</cite> in output and its attribute <cite>free_vars</cite> remain the same compared to the previous example. This is because the quantification is still on both variables, namely it is perfomed on both dimensions of the result of the predicate.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">Forall</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
<span class="gp">... </span>            <span class="n">cond_vars</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">],</span>
<span class="gp">... </span>            <span class="n">cond_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="mf">1.</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor(0.9844, dtype=torch.float64), free_vars=[])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor(0.9844, dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([])</span>
</pre></div>
</div>
<p>Universal quantification of both variables of the same predicate using diagonal quantification
(<a class="reference internal" href="#ltn.core.diag" title="ltn.core.diag"><code class="xref py py-func docutils literal notranslate"><span class="pre">ltn.core.diag()</span></code></a>). Note that:</p>
<ul class="simple">
<li><p>the variables have the same number of individuals since it is a constraint for applying diagonal quantification;</p></li>
<li><p>since diagonal quantification has been set, the predicate will not be computed on all the possible combinations of individuals of the two variables (that are 4), namely the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a> is disabled;</p></li>
<li><p>the predicate is computed only on the given tuples of individuals in a one-to-one correspondence, namely on the first individual of <cite>x</cite> and <cite>y</cite>, and second individual of <cite>x</cite> and <cite>y</cite>;</p></li>
<li><p>the result changes compared to the case without diagonal quantification. This is due to the fact that we are aggregating a smaller number of truth values since the predicate has been computed only two times.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
<span class="gp">... </span>                                   <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">Forall</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span> <span class="c1"># with diagonal quantification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_without_diag</span> <span class="o">=</span> <span class="n">Forall</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span> <span class="c1"># without diagonal quantification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out_without_diag</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor(0.9788), free_vars=[])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out_without_diag</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor(0.9788)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">LTNObject(value=tensor(0.9888), free_vars=[])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor(0.9888)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">free_vars</span><span class="p">)</span>
<span class="go">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">())</span>
<span class="go">torch.Size([])</span>
</pre></div>
</div>
<dl class="field-list">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>agg_op</strong><span class="classifier"><a class="reference internal" href="index.html#ltn.fuzzy_ops.AggregationOperator" title="ltn.fuzzy_ops.AggregationOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AggregationOperator</span></code></a></span></dt><dd><p>See <cite>agg_op</cite> parameter.</p>
</dd>
<dt><strong>quantifier</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></a></span></dt><dd><p>See <cite>quantifier</cite> parameter.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</section>
</section>
<span id="document-fuzzy_ops"></span><section id="ltn-fuzzy-ops">
<h2>ltn.fuzzy_ops<a class="headerlink" href="#ltn-fuzzy-ops" title="Link to this heading"></a></h2>
<section id="members">
<h3>Members<a class="headerlink" href="#members" title="Link to this heading"></a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.ConnectiveOperator" title="ltn.fuzzy_ops.ConnectiveOperator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.ConnectiveOperator</span></code></a>()</p></td>
<td><p>Abstract class for connective operators.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.UnaryConnectiveOperator" title="ltn.fuzzy_ops.UnaryConnectiveOperator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.UnaryConnectiveOperator</span></code></a>()</p></td>
<td><p>Abstract class for unary connective operators.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.BinaryConnectiveOperator</span></code></a>()</p></td>
<td><p>Abstract class for binary connective operators.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.NotStandard" title="ltn.fuzzy_ops.NotStandard"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.NotStandard</span></code></a>()</p></td>
<td><p>Standard fuzzy negation operator.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.NotGodel" title="ltn.fuzzy_ops.NotGodel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.NotGodel</span></code></a>()</p></td>
<td><p>Godel fuzzy negation operator.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.AndMin" title="ltn.fuzzy_ops.AndMin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AndMin</span></code></a>()</p></td>
<td><p>Godel fuzzy conjunction operator (min operator).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.AndProd" title="ltn.fuzzy_ops.AndProd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AndProd</span></code></a>([stable])</p></td>
<td><p>Goguen fuzzy conjunction operator (product operator).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.AndLuk" title="ltn.fuzzy_ops.AndLuk"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AndLuk</span></code></a>()</p></td>
<td><p>Lukasiewicz fuzzy conjunction operator.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.OrMax" title="ltn.fuzzy_ops.OrMax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.OrMax</span></code></a>()</p></td>
<td><p>Godel fuzzy disjunction operator (max operator).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.OrProbSum" title="ltn.fuzzy_ops.OrProbSum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.OrProbSum</span></code></a>([stable])</p></td>
<td><p>Goguen fuzzy disjunction operator (probabilistic sum).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.OrLuk" title="ltn.fuzzy_ops.OrLuk"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.OrLuk</span></code></a>()</p></td>
<td><p>Lukasiewicz fuzzy disjunction operator.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.ImpliesKleeneDienes" title="ltn.fuzzy_ops.ImpliesKleeneDienes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.ImpliesKleeneDienes</span></code></a>()</p></td>
<td><p>Kleene Dienes fuzzy implication operator.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.ImpliesGodel" title="ltn.fuzzy_ops.ImpliesGodel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.ImpliesGodel</span></code></a>()</p></td>
<td><p>Godel fuzzy implication operand.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.ImpliesReichenbach" title="ltn.fuzzy_ops.ImpliesReichenbach"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.ImpliesReichenbach</span></code></a>([stable])</p></td>
<td><p>Reichenbach fuzzy implication operator.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.ImpliesGoguen" title="ltn.fuzzy_ops.ImpliesGoguen"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.ImpliesGoguen</span></code></a>([stable])</p></td>
<td><p>Goguen fuzzy implication operator.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.Equiv" title="ltn.fuzzy_ops.Equiv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.Equiv</span></code></a>(and_op, implies_op)</p></td>
<td><p>Equivalence (<span class="math notranslate nohighlight">\(\leftrightarrow\)</span>) fuzzy operator.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.AggregationOperator" title="ltn.fuzzy_ops.AggregationOperator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AggregationOperator</span></code></a>()</p></td>
<td><p>Abstract class for aggregation operators.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.AggregMin" title="ltn.fuzzy_ops.AggregMin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AggregMin</span></code></a>()</p></td>
<td><p>Min fuzzy aggregation operator.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.AggregMean" title="ltn.fuzzy_ops.AggregMean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AggregMean</span></code></a>()</p></td>
<td><p>Mean fuzzy aggregation operator.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.AggregPMean" title="ltn.fuzzy_ops.AggregPMean"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AggregPMean</span></code></a>([p, stable])</p></td>
<td><p><cite>pMean</cite> fuzzy aggregation operator.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.AggregPMeanError" title="ltn.fuzzy_ops.AggregPMeanError"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AggregPMeanError</span></code></a>([p, stable])</p></td>
<td><p><cite>pMeanError</cite> fuzzy aggregation operator.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#ltn.fuzzy_ops.SatAgg" title="ltn.fuzzy_ops.SatAgg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.SatAgg</span></code></a>([agg_op])</p></td>
<td><p><cite>SatAgg</cite> aggregation operator.</p></td>
</tr>
</tbody>
</table>
<p id="fuzzyop"><span id="module-ltn.fuzzy_ops"></span>The <cite>ltn.fuzzy_ops</cite> module contains the PyTorch implementation of some common fuzzy logic operators and aggregators.
Refer to the <a class="reference external" href="https://arxiv.org/abs/2012.13635">LTN paper</a> for a detailed description of these operators
(see the Appendix).</p>
<p>All the operators included in this module support the traditional NumPy/PyTorch broadcasting.</p>
<p>The operators have been designed to be used with <a class="reference internal" href="index.html#ltn.core.Connective" title="ltn.core.Connective"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Connective</span></code></a> or <a class="reference internal" href="index.html#ltn.core.Quantifier" title="ltn.core.Quantifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.Quantifier</span></code></a>.</p>
<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.ConnectiveOperator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">ConnectiveOperator</span></span><a class="headerlink" href="#ltn.fuzzy_ops.ConnectiveOperator" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Abstract class for connective operators.</p>
<p>Every connective operator implemented in LTNtorch must inherit from this class and implements
the <cite>__call__()</cite> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#NotImplementedError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">NotImplementedError</span></code></a></dt><dd><p>Raised when <cite>__call__()</cite> is not implemented in the sub-class.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.UnaryConnectiveOperator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">UnaryConnectiveOperator</span></span><a class="headerlink" href="#ltn.fuzzy_ops.UnaryConnectiveOperator" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.ConnectiveOperator" title="ltn.fuzzy_ops.ConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConnectiveOperator</span></code></a></p>
<p>Abstract class for unary connective operators.</p>
<p>Every unary connective operator implemented in LTNtorch must inherit from this class and
implement the <cite>__call__()</cite> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#NotImplementedError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">NotImplementedError</span></code></a></dt><dd><p>Raised when <cite>__call__()</cite> is not implemented in the sub-class.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.BinaryConnectiveOperator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">BinaryConnectiveOperator</span></span><a class="headerlink" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.ConnectiveOperator" title="ltn.fuzzy_ops.ConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConnectiveOperator</span></code></a></p>
<p>Abstract class for binary connective operators.</p>
<p>Every binary connective operator implemented in LTNtorch must inherit from this class
and implement the <cite>__call__()</cite> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#NotImplementedError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">NotImplementedError</span></code></a></dt><dd><p>Raised when <cite>__call__()</cite> is not implemented in the sub-class.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.NotStandard">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">NotStandard</span></span><a class="headerlink" href="#ltn.fuzzy_ops.NotStandard" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.UnaryConnectiveOperator" title="ltn.fuzzy_ops.UnaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">UnaryConnectiveOperator</span></code></a></p>
<p>Standard fuzzy negation operator.</p>
<p><span class="math notranslate nohighlight">\(\lnot_{standard}(x) = 1 - x\)</span></p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Not</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">NotStandard</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Not</span><span class="p">)</span>
<span class="go">Connective(connective_op=NotStandard())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Not</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.3635, 0.2891])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.NotStandard.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.NotStandard.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the standard fuzzy negation operator to the given operand.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Operand on which the operator has to be applied.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The standard fuzzy negation of the given operand.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.NotGodel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">NotGodel</span></span><a class="headerlink" href="#ltn.fuzzy_ops.NotGodel" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.UnaryConnectiveOperator" title="ltn.fuzzy_ops.UnaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">UnaryConnectiveOperator</span></code></a></p>
<p>Godel fuzzy negation operator.</p>
<p><span class="math notranslate nohighlight">\(\lnot_{Godel}(x) = \left\{\begin{array}{ c l }1 &amp; \quad \textrm{if } x = 0 \\ 0 &amp; \quad \textrm{otherwise} \end{array} \right.\)</span></p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Not</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">NotGodel</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Not</span><span class="p">)</span>
<span class="go">Connective(connective_op=NotGodel())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Not</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0., 0.])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.NotGodel.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.NotGodel.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the Godel fuzzy negation operator to the given operand.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Operand on which the operator has to be applied.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The Godel fuzzy negation of the given operand.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AndMin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">AndMin</span></span><a class="headerlink" href="#ltn.fuzzy_ops.AndMin" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BinaryConnectiveOperator</span></code></a></p>
<p>Godel fuzzy conjunction operator (min operator).</p>
<p><span class="math notranslate nohighlight">\(\land_{Godel}(x, y) = \operatorname{min}(x, y)\)</span></p>
<p class="rubric">Examples</p>
<p>Note that:</p>
<ul class="simple">
<li><p>variable <cite>x</cite> has two individuals;</p></li>
<li><p>variable <cite>y</cite> has three individuals;</p></li>
<li><p>the shape of the result of the conjunction is <cite>(2, 3)</cite> due to the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>. The first dimension is dedicated two variable <cite>x</cite>, while the second dimension to variable <cite>y</cite>;</p></li>
<li><p>at index <cite>(0, 0)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and first individual of <cite>y</cite>, at index <cite>(0, 1)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and second individual of <cite>y</cite>, and so forth.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">And</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AndMin</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">And</span><span class="p">)</span>
<span class="go">Connective(connective_op=AndMin())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6682, 0.5498, 0.5250])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">And</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[0.6365, 0.5498, 0.5250],</span>
<span class="go">        [0.6682, 0.5498, 0.5250]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AndMin.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.AndMin.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the Godel fuzzy conjunction operator to the given operands.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>First operand on which the operator has to be applied.</p>
</dd>
<dt><strong>y</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Second operand on which the operator has to be applied.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The Godel fuzzy conjunction of the two operands.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AndProd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">AndProd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.AndProd" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BinaryConnectiveOperator</span></code></a></p>
<p>Goguen fuzzy conjunction operator (product operator).</p>
<p><span class="math notranslate nohighlight">\(\land_{Goguen}(x, y) = xy\)</span></p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=True</span></dt><dd><p>Flag indicating whether to use the <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable version</span></a> of the operator or not.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The Gougen fuzzy conjunction could have vanishing gradients if not used in its <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable</span></a> version.</p>
<p class="rubric">Examples</p>
<p>Note that:</p>
<ul class="simple">
<li><p>variable <cite>x</cite> has two individuals;</p></li>
<li><p>variable <cite>y</cite> has three individuals;</p></li>
<li><p>the shape of the result of the conjunction is <cite>(2, 3)</cite> due to the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>. The first dimension is dedicated two variable <cite>x</cite>, while the second dimension to variable <cite>y</cite>;</p></li>
<li><p>at index <cite>(0, 0)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and first individual of <cite>y</cite>, at index <cite>(0, 1)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and second individual of <cite>y</cite>, and so forth.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">And</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AndProd</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">And</span><span class="p">)</span>
<span class="go">Connective(connective_op=AndProd(stable=True))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6682, 0.5498, 0.5250])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">And</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[0.4253, 0.3500, 0.3342],</span>
<span class="go">        [0.4751, 0.3910, 0.3733]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AndProd.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.AndProd.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the Goguen fuzzy conjunction operator to the given operands.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>First operand on which the operator has to be applied.</p>
</dd>
<dt><strong>y</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Second operand on which the operator has to be applied.</p>
</dd>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=None</span></dt><dd><p>Flag indicating whether to use the <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable version</span></a> of the operator or not.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The Goguen fuzzy conjunction of the two operands.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="field-list">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a></span></dt><dd><p>See <cite>stable</cite> parameter.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AndLuk">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">AndLuk</span></span><a class="headerlink" href="#ltn.fuzzy_ops.AndLuk" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BinaryConnectiveOperator</span></code></a></p>
<p>Lukasiewicz fuzzy conjunction operator.</p>
<p><span class="math notranslate nohighlight">\(\land_{Lukasiewicz}(x, y) = \operatorname{max}(x + y - 1, 0)\)</span></p>
<p class="rubric">Examples</p>
<p>Note that:</p>
<ul class="simple">
<li><p>variable <cite>x</cite> has two individuals;</p></li>
<li><p>variable <cite>y</cite> has three individuals;</p></li>
<li><p>the shape of the result of the conjunction is <cite>(2, 3)</cite> due to the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>. The first dimension is dedicated two variable <cite>x</cite>, while the second dimension to variable <cite>y</cite>;</p></li>
<li><p>at index <cite>(0, 0)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and first individual of <cite>y</cite>, at index <cite>(0, 1)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and second individual of <cite>y</cite>, and so forth.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">And</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AndLuk</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">And</span><span class="p">)</span>
<span class="go">Connective(connective_op=AndLuk())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6682, 0.5498, 0.5250])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">And</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[0.3046, 0.1863, 0.1614],</span>
<span class="go">        [0.3791, 0.2608, 0.2359]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AndLuk.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.AndLuk.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the Lukasiewicz fuzzy conjunction operator to the given operands.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>First operand on which the operator has to be applied.</p>
</dd>
<dt><strong>y</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Second operand on which the operator has to be applied.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The Lukasiewicz fuzzy conjunction of the two operands.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.OrMax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">OrMax</span></span><a class="headerlink" href="#ltn.fuzzy_ops.OrMax" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BinaryConnectiveOperator</span></code></a></p>
<p>Godel fuzzy disjunction operator (max operator).</p>
<p><span class="math notranslate nohighlight">\(\lor_{Godel}(x, y) = \operatorname{max}(x, y)\)</span></p>
<p class="rubric">Examples</p>
<p>Note that:</p>
<ul class="simple">
<li><p>variable <cite>x</cite> has two individuals;</p></li>
<li><p>variable <cite>y</cite> has three individuals;</p></li>
<li><p>the shape of the result of the conjunction is <cite>(2, 3)</cite> due to the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>. The first dimension is dedicated two variable <cite>x</cite>, while the second dimension to variable <cite>y</cite>;</p></li>
<li><p>at index <cite>(0, 0)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and first individual of <cite>y</cite>, at index <cite>(0, 1)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and second individual of <cite>y</cite>, and so forth.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Or</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">OrMax</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Or</span><span class="p">)</span>
<span class="go">Connective(connective_op=OrMax())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6682, 0.5498, 0.5250])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Or</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[0.6682, 0.6365, 0.6365],</span>
<span class="go">        [0.7109, 0.7109, 0.7109]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.OrMax.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.OrMax.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the Godel fuzzy disjunction operator to the given operands.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>First operand on which the operator has to be applied.</p>
</dd>
<dt><strong>y</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Second operand on which the operator has to be applied.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The Godel fuzzy disjunction of the two operands.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.OrProbSum">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">OrProbSum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.OrProbSum" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BinaryConnectiveOperator</span></code></a></p>
<p>Goguen fuzzy disjunction operator (probabilistic sum).</p>
<p><span class="math notranslate nohighlight">\(\lor_{Goguen}(x, y) = x + y - xy\)</span></p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=True</span></dt><dd><p>Flag indicating whether to use the <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable version</span></a> of the operator or not.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The Gougen fuzzy disjunction could have vanishing gradients if not used in its <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable</span></a> version.</p>
<p class="rubric">Examples</p>
<p>Note that:</p>
<ul class="simple">
<li><p>variable <cite>x</cite> has two individuals;</p></li>
<li><p>variable <cite>y</cite> has three individuals;</p></li>
<li><p>the shape of the result of the conjunction is <cite>(2, 3)</cite> due to the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>. The first dimension is dedicated two variable <cite>x</cite>, while the second dimension to variable <cite>y</cite>;</p></li>
<li><p>at index <cite>(0, 0)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and first individual of <cite>y</cite>, at index <cite>(0, 1)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and second individual of <cite>y</cite>, and so forth.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Or</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">OrProbSum</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Or</span><span class="p">)</span>
<span class="go">Connective(connective_op=OrProbSum(stable=True))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6682, 0.5498, 0.5250])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Or</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[0.8793, 0.8363, 0.8273],</span>
<span class="go">        [0.9040, 0.8698, 0.8626]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.OrProbSum.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.OrProbSum.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the Goguen fuzzy disjunction operator to the given operands.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>First operand on which the operator has to be applied.</p>
</dd>
<dt><strong>y</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Second operand on which the operator has to be applied.</p>
</dd>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=None</span></dt><dd><p>Flag indicating whether to use the <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable version</span></a> of the operator or not.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The Goguen fuzzy disjunction of the two operands.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="field-list">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a></span></dt><dd><p>See <cite>stable</cite> parameter.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.OrLuk">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">OrLuk</span></span><a class="headerlink" href="#ltn.fuzzy_ops.OrLuk" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BinaryConnectiveOperator</span></code></a></p>
<p>Lukasiewicz fuzzy disjunction operator.</p>
<p><span class="math notranslate nohighlight">\(\lor_{Lukasiewicz}(x, y) = \operatorname{min}(x + y, 1)\)</span></p>
<p class="rubric">Examples</p>
<p>Note that:</p>
<ul class="simple">
<li><p>variable <cite>x</cite> has two individuals;</p></li>
<li><p>variable <cite>y</cite> has three individuals;</p></li>
<li><p>the shape of the result of the conjunction is <cite>(2, 3)</cite> due to the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>. The first dimension is dedicated two variable <cite>x</cite>, while the second dimension to variable <cite>y</cite>;</p></li>
<li><p>at index <cite>(0, 0)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and first individual of <cite>y</cite>, at index <cite>(0, 1)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and second individual of <cite>y</cite>, and so forth.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Or</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">OrLuk</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Or</span><span class="p">)</span>
<span class="go">Connective(connective_op=OrLuk())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6682, 0.5498, 0.5250])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Or</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[1., 1., 1.],</span>
<span class="go">        [1., 1., 1.]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.OrLuk.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.OrLuk.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the Lukasiewicz fuzzy disjunction operator to the given operands.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>First operand on which the operator has to be applied.</p>
</dd>
<dt><strong>y</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Second operand on which the operator has to be applied.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The Lukasiewicz fuzzy disjunction of the two operands.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.ImpliesKleeneDienes">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">ImpliesKleeneDienes</span></span><a class="headerlink" href="#ltn.fuzzy_ops.ImpliesKleeneDienes" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BinaryConnectiveOperator</span></code></a></p>
<p>Kleene Dienes fuzzy implication operator.</p>
<p><span class="math notranslate nohighlight">\(\rightarrow_{KleeneDienes}(x, y) = \operatorname{max}(1 - x, y)\)</span></p>
<p class="rubric">Examples</p>
<p>Note that:</p>
<ul class="simple">
<li><p>variable <cite>x</cite> has two individuals;</p></li>
<li><p>variable <cite>y</cite> has three individuals;</p></li>
<li><p>the shape of the result of the conjunction is <cite>(2, 3)</cite> due to the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>. The first dimension is dedicated two variable <cite>x</cite>, while the second dimension to variable <cite>y</cite>;</p></li>
<li><p>at index <cite>(0, 0)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and first individual of <cite>y</cite>, at index <cite>(0, 1)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and second individual of <cite>y</cite>, and so forth.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Implies</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">ImpliesKleeneDienes</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Implies</span><span class="p">)</span>
<span class="go">Connective(connective_op=ImpliesKleeneDienes())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6682, 0.5498, 0.5250])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Implies</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[0.6682, 0.5498, 0.5250],</span>
<span class="go">        [0.6682, 0.5498, 0.5250]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.ImpliesKleeneDienes.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.ImpliesKleeneDienes.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the Kleene Dienes fuzzy implication operator to the given operands.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>First operand on which the operator has to be applied.</p>
</dd>
<dt><strong>y</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Second operand on which the operator has to be applied.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The Kleene Dienes fuzzy implication of the two operands.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.ImpliesGodel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">ImpliesGodel</span></span><a class="headerlink" href="#ltn.fuzzy_ops.ImpliesGodel" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BinaryConnectiveOperator</span></code></a></p>
<p>Godel fuzzy implication operand.</p>
<p><span class="math notranslate nohighlight">\(\rightarrow_{Godel}(x, y) = \left\{\begin{array}{ c l }1 &amp; \quad \textrm{if } x \le y \\ y &amp; \quad \textrm{otherwise} \end{array} \right.\)</span></p>
<p class="rubric">Examples</p>
<p>Note that:</p>
<ul class="simple">
<li><p>variable <cite>x</cite> has two individuals;</p></li>
<li><p>variable <cite>y</cite> has three individuals;</p></li>
<li><p>the shape of the result of the conjunction is <cite>(2, 3)</cite> due to the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>. The first dimension is dedicated two variable <cite>x</cite>, while the second dimension to variable <cite>y</cite>;</p></li>
<li><p>at index <cite>(0, 0)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and first individual of <cite>y</cite>, at index <cite>(0, 1)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and second individual of <cite>y</cite>, and so forth.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Implies</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">ImpliesGodel</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Implies</span><span class="p">)</span>
<span class="go">Connective(connective_op=ImpliesGodel())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6682, 0.5498, 0.5250])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Implies</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[1.0000, 0.5498, 0.5250],</span>
<span class="go">        [0.6682, 0.5498, 0.5250]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.ImpliesGodel.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.ImpliesGodel.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the Godel fuzzy implication operator to the given operands.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>First operand on which the operator has to be applied.</p>
</dd>
<dt><strong>y</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Second operand on which the operator has to be applied.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The Godel fuzzy implication of the two operands.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.ImpliesReichenbach">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">ImpliesReichenbach</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.ImpliesReichenbach" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BinaryConnectiveOperator</span></code></a></p>
<p>Reichenbach fuzzy implication operator.</p>
<p><span class="math notranslate nohighlight">\(\rightarrow_{Reichenbach}(x, y) = 1 - x + xy\)</span></p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=True</span></dt><dd><p>Flag indicating whether to use the <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable version</span></a> of the operator or not.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The Reichenbach fuzzy implication could have vanishing gradients if not used in its <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable</span></a> version.</p>
<p class="rubric">Examples</p>
<p>Note that:</p>
<ul class="simple">
<li><p>variable <cite>x</cite> has two individuals;</p></li>
<li><p>variable <cite>y</cite> has three individuals;</p></li>
<li><p>the shape of the result of the conjunction is <cite>(2, 3)</cite> due to the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>. The first dimension is dedicated two variable <cite>x</cite>, while the second dimension to variable <cite>y</cite>;</p></li>
<li><p>at index <cite>(0, 0)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and first individual of <cite>y</cite>, at index <cite>(0, 1)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and second individual of <cite>y</cite>, and so forth.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Implies</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">ImpliesReichenbach</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Implies</span><span class="p">)</span>
<span class="go">Connective(connective_op=ImpliesReichenbach(stable=True))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6682, 0.5498, 0.5250])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Implies</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[0.7888, 0.7134, 0.6976],</span>
<span class="go">        [0.7640, 0.6799, 0.6622]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.ImpliesReichenbach.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.ImpliesReichenbach.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the Reichenbach fuzzy implication operator to the given operands.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>First operand on which the operator has to be applied.</p>
</dd>
<dt><strong>y</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Second operand on which the operator has to be applied.</p>
</dd>
<dt><strong>stable: :obj:`bool`, default=None</strong></dt><dd><p>Flag indicating whether to use the <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable version</span></a> of the operator or not.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The Reichenbach fuzzy implication of the two operands.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="field-list">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a></span></dt><dd><p>See <cite>stable</cite> parameter.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.ImpliesGoguen">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">ImpliesGoguen</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.ImpliesGoguen" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BinaryConnectiveOperator</span></code></a></p>
<p>Goguen fuzzy implication operator.</p>
<p><span class="math notranslate nohighlight">\(\rightarrow_{Goguen}(x, y) = \left\{\begin{array}{ c l }1 &amp; \quad \textrm{if } x \le y \\ \frac{y}{x} &amp; \quad \textrm{otherwise} \end{array} \right.\)</span></p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=True</span></dt><dd><p>Flag indicating whether to use the <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable version</span></a> of the operator or not.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The Goguen fuzzy implication could have vanishing gradients if not used in its <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable</span></a> version.</p>
<p class="rubric">Examples</p>
<p>Note that:</p>
<ul class="simple">
<li><p>variable <cite>x</cite> has two individuals;</p></li>
<li><p>variable <cite>y</cite> has three individuals;</p></li>
<li><p>the shape of the result of the conjunction is <cite>(2, 3)</cite> due to the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>. The first dimension is dedicated two variable <cite>x</cite>, while the second dimension to variable <cite>y</cite>;</p></li>
<li><p>at index <cite>(0, 0)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and first individual of <cite>y</cite>, at index <cite>(0, 1)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and second individual of <cite>y</cite>, and so forth.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Implies</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">ImpliesGoguen</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Implies</span><span class="p">)</span>
<span class="go">Connective(connective_op=ImpliesGoguen(stable=True))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6682, 0.5498, 0.5250])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Implies</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[1.0000, 0.8639, 0.8248],</span>
<span class="go">        [0.9398, 0.7733, 0.7384]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.ImpliesGoguen.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.ImpliesGoguen.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the Goguen fuzzy implication operator to the given operands.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>First operand on which the operator has to be applied.</p>
</dd>
<dt><strong>y</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Second operand on which the operator has to be applied.</p>
</dd>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=None</span></dt><dd><p>Flag indicating whether to use the <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable version</span></a> of the operator or not.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The Goguen fuzzy implication of the two operands.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="field-list">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a></span></dt><dd><p>See <cite>stable</cite> parameter.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.Equiv">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">Equiv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">and_op</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">implies_op</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.Equiv" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">BinaryConnectiveOperator</span></code></a></p>
<p>Equivalence (<span class="math notranslate nohighlight">\(\leftrightarrow\)</span>) fuzzy operator.</p>
<p><span class="math notranslate nohighlight">\(x \leftrightarrow y \equiv x \rightarrow y \land y \rightarrow x\)</span></p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>and_op</strong><span class="classifier"><a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.BinaryConnectiveOperator</span></code></a></span></dt><dd><p>Fuzzy conjunction operator to use for the equivalence operator.</p>
</dd>
<dt><strong>implies_op</strong><span class="classifier"><a class="reference internal" href="#ltn.fuzzy_ops.BinaryConnectiveOperator" title="ltn.fuzzy_ops.BinaryConnectiveOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.BinaryConnectiveOperator</span></code></a></span></dt><dd><p>Fuzzy implication operator to use for the implication operator.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>the equivalence operator (<span class="math notranslate nohighlight">\(\leftrightarrow\)</span>) is implemented in LTNtorch as an operator which computes: <span class="math notranslate nohighlight">\(x \rightarrow y \land y \rightarrow x\)</span>;</p></li>
<li><p>the <cite>and_op</cite> parameter defines the operator for <span class="math notranslate nohighlight">\(\land\)</span>;</p></li>
<li><p>the <cite>implies_op</cite> parameter defines the operator for <span class="math notranslate nohighlight">\(\rightarrow\)</span>.</p></li>
</ul>
<p class="rubric">Examples</p>
<p>Note that:</p>
<ul class="simple">
<li><p>we have selected <a class="reference internal" href="#ltn.fuzzy_ops.AndProd" title="ltn.fuzzy_ops.AndProd"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AndProd()</span></code></a> as an operator for the conjunction of the equivalence, and <a class="reference internal" href="#ltn.fuzzy_ops.ImpliesReichenbach" title="ltn.fuzzy_ops.ImpliesReichenbach"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.ImpliesReichenbach</span></code></a> as an operator for the implication;</p></li>
<li><p>variable <cite>x</cite> has two individuals;</p></li>
<li><p>variable <cite>y</cite> has three individuals;</p></li>
<li><p>the shape of the result of the conjunction is <cite>(2, 3)</cite> due to the <a class="reference internal" href="index.html#broadcasting"><span class="std std-ref">LTN broadcasting</span></a>. The first dimension is dedicated two variable <cite>x</cite>, while the second dimension to variable <cite>y</cite>;</p></li>
<li><p>at index <cite>(0, 0)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and first individual of <cite>y</cite>, at index <cite>(0, 1)</cite> there is the evaluation of the formula on first individual of <cite>x</cite> and second individual of <cite>y</cite>, and so forth.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Equiv</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">Equiv</span><span class="p">(</span>
<span class="gp">... </span>                            <span class="n">and_op</span><span class="o">=</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AndProd</span><span class="p">(),</span>
<span class="gp">... </span>                            <span class="n">implies_op</span><span class="o">=</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">ImpliesReichenbach</span><span class="p">()</span>
<span class="gp">... </span>                        <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Equiv</span><span class="p">)</span>
<span class="go">Connective(connective_op=Equiv(and_op=AndProd(stable=True), implies_op=ImpliesReichenbach(stable=True)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6682, 0.5498, 0.5250])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Equiv</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">p</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([[0.5972, 0.5708, 0.5645],</span>
<span class="go">        [0.6165, 0.5718, 0.5617]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.Equiv.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.Equiv.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the fuzzy equivalence operator to the given operands.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>First operand on which the operator has to be applied.</p>
</dd>
<dt><strong>y</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Second operand on which the operator has to be applied.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The fuzzy equivalence of the two operands.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>and_op: :class:`ltn.fuzzy_ops.BinaryConnectiveOperator`</strong></dt><dd><p>See <cite>and_op</cite> parameter.</p>
</dd>
<dt><strong>implies_op: :class:`ltn.fuzzy_ops.BinaryConnectiveOperator`</strong></dt><dd><p>See <cite>implies_op</cite> parameter.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AggregationOperator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">AggregationOperator</span></span><a class="headerlink" href="#ltn.fuzzy_ops.AggregationOperator" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Abstract class for aggregation operators.</p>
<p>Every aggregation operator implemented in LTNtorch must inherit from this class
and implement the <cite>__call__()</cite> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#NotImplementedError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">NotImplementedError</span></code></a></dt><dd><p>Raised when <cite>__call__()</cite> is not implemented in the sub-class.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AggregMin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">AggregMin</span></span><a class="headerlink" href="#ltn.fuzzy_ops.AggregMin" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.AggregationOperator" title="ltn.fuzzy_ops.AggregationOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">AggregationOperator</span></code></a></p>
<p>Min fuzzy aggregation operator.</p>
<p><span class="math notranslate nohighlight">\(A_{T_{M}}(x_1, \dots, x_n) = \operatorname{min}(x_1, \dots, x_n)\)</span></p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Forall</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Quantifier</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AggregMin</span><span class="p">(),</span> <span class="n">quantifier</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Forall</span><span class="p">)</span>
<span class="go">Quantifier(agg_op=AggregMin(), quantifier=&#39;f&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109, 0.6682])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Forall</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor(0.6365)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AggregMin.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.AggregMin.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the min fuzzy aggregation operator to the given formula’s <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> on
the selected dimensions.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>xs</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p><a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">Grounding</span></a> of formula on which the aggregation has to be performed.</p>
</dd>
<dt><strong>dim</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>, default=None</span></dt><dd><p>Tuple containing the indexes of dimensions on which the aggregation has to be performed.</p>
</dd>
<dt><strong>keepdim</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=False</span></dt><dd><p>Flag indicating whether the output has to keep the same dimensions as the input after
the aggregation.</p>
</dd>
<dt><strong>mask</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>, default=None</span></dt><dd><p>Boolean mask for excluding values of ‘xs’ from the aggregation. It is internally used for guarded
quantification. The mask must have the same shape of ‘xs’. <cite>False</cite> means exclusion, <cite>True</cite> means inclusion.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>Min fuzzy aggregation of the formula.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a></dt><dd><p>Raises when the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the formula (‘xs’) and the mask do not have the same
shape.
Raises when the ‘mask’ is not boolean.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AggregMean">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">AggregMean</span></span><a class="headerlink" href="#ltn.fuzzy_ops.AggregMean" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.AggregationOperator" title="ltn.fuzzy_ops.AggregationOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">AggregationOperator</span></code></a></p>
<p>Mean fuzzy aggregation operator.</p>
<p><span class="math notranslate nohighlight">\(A_{M}(x_1, \dots, x_n) = \frac{1}{n} \sum_{i = 1}^n x_i\)</span></p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Forall</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Quantifier</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AggregMean</span><span class="p">(),</span> <span class="n">quantifier</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Forall</span><span class="p">)</span>
<span class="go">Quantifier(agg_op=AggregMean(), quantifier=&#39;f&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109, 0.6682])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Forall</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor(0.6719)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AggregMean.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.AggregMean.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the mean fuzzy aggregation operator to the given formula’s <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> on
the selected dimensions.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>xs</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p><a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">Grounding</span></a> of formula on which the aggregation has to be performed.</p>
</dd>
<dt><strong>dim</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>, default=None</span></dt><dd><p>Tuple containing the indexes of dimensions on which the aggregation has to be performed.</p>
</dd>
<dt><strong>keepdim</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=False</span></dt><dd><p>Flag indicating whether the output has to keep the same dimensions as the input after
the aggregation.</p>
</dd>
<dt><strong>mask</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>, default=None</span></dt><dd><p>Boolean mask for excluding values of ‘xs’ from the aggregation. It is internally used for guarded
quantification. The mask must have the same shape of ‘xs’. <cite>False</cite> means exclusion, <cite>True</cite> means inclusion.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>Mean fuzzy aggregation of the formula.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a></dt><dd><p>Raises when the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the formula (‘xs’) and the mask do not have the same
shape.
Raises when the ‘mask’ is not boolean.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AggregPMean">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">AggregPMean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.AggregPMean" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.AggregationOperator" title="ltn.fuzzy_ops.AggregationOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">AggregationOperator</span></code></a></p>
<p><cite>pMean</cite> fuzzy aggregation operator.</p>
<p><span class="math notranslate nohighlight">\(A_{pM}(x_1, \dots, x_n) = (\frac{1}{n} \sum_{i = 1}^n x_i^p)^{\frac{1}{p}}\)</span></p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>p</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>, default=2</span></dt><dd><p>Value of hyper-parameter <cite>p</cite> of the <cite>pMean</cite> fuzzy aggregation operator.</p>
</dd>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=True</span></dt><dd><p>Flag indicating whether to use the <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable version</span></a> of the operator or not.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The <cite>pMean</cite> aggregation operator has been selected as an approximation of
<span class="math notranslate nohighlight">\(\exists\)</span> with <span class="math notranslate nohighlight">\(p \geq 1\)</span>.
If <span class="math notranslate nohighlight">\(p \to \infty\)</span>, then the <cite>pMean</cite> operator tends to the
maximum of the input values (classical behavior of <span class="math notranslate nohighlight">\(\exists\)</span>).</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Exists</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Quantifier</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AggregPMean</span><span class="p">(),</span> <span class="n">quantifier</span><span class="o">=</span><span class="s1">&#39;e&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Exists</span><span class="p">)</span>
<span class="go">Quantifier(agg_op=AggregPMean(p=2, stable=True), quantifier=&#39;e&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109, 0.6682])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Exists</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor(0.6726)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AggregPMean.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.AggregPMean.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the <cite>pMean</cite> aggregation operator to the given formula’s <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a>
on the selected dimensions.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>xs</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p><a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">Grounding</span></a> of formula on which the aggregation has to be performed.</p>
</dd>
<dt><strong>dim</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>, default=None</span></dt><dd><p>Tuple containing the indexes of dimensions on which the aggregation has to be performed.</p>
</dd>
<dt><strong>keepdim</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=False</span></dt><dd><p>Flag indicating whether the output has to keep the same dimensions as the input after
the aggregation.</p>
</dd>
<dt><strong>mask</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>, default=None</span></dt><dd><p>Boolean mask for excluding values of ‘xs’ from the aggregation. It is internally used for guarded
quantification. The mask must have the same shape of ‘xs’. <cite>False</cite> means exclusion, <cite>True</cite> means inclusion.</p>
</dd>
<dt><strong>p</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>, default=None</span></dt><dd><p>Value of hyper-parameter <cite>p</cite> of the <cite>pMean</cite> fuzzy aggregation operator.</p>
</dd>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=None</span></dt><dd><p>Flag indicating whether to use the <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable version</span></a> of the operator or not.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p><cite>pMean</cite> fuzzy aggregation of the formula.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a></dt><dd><p>Raises when the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the formula (‘xs’) and the mask do not have the same
shape.
Raises when the ‘mask’ is not boolean.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="field-list">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>p</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a></span></dt><dd><p>See <cite>p</cite> parameter.</p>
</dd>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a></span></dt><dd><p>See <cite>stable</cite> parameter.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AggregPMeanError">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">AggregPMeanError</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.AggregPMeanError" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#ltn.fuzzy_ops.AggregationOperator" title="ltn.fuzzy_ops.AggregationOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">AggregationOperator</span></code></a></p>
<p><cite>pMeanError</cite> fuzzy aggregation operator.</p>
<p><span class="math notranslate nohighlight">\(A_{pME}(x_1, \dots, x_n) = 1 - (\frac{1}{n} \sum_{i = 1}^n (1 - x_i)^p)^{\frac{1}{p}}\)</span></p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>p</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>, default=2</span></dt><dd><p>Value of hyper-parameter <cite>p</cite> of the <cite>pMeanError</cite> fuzzy aggregation operator.</p>
</dd>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=True</span></dt><dd><p>Flag indicating whether to use the <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable version</span></a> of the operator or not.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The <cite>pMeanError</cite> aggregation operator has been selected as an approximation of
<span class="math notranslate nohighlight">\(\forall\)</span> with <span class="math notranslate nohighlight">\(p \geq 1\)</span>. If <span class="math notranslate nohighlight">\(p \to \infty\)</span>, then the <cite>pMeanError</cite> operator tends to the
minimum of the input values (classical behavior of <span class="math notranslate nohighlight">\(\forall\)</span>).</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Forall</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Quantifier</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AggregPMeanError</span><span class="p">(),</span> <span class="n">quantifier</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Forall</span><span class="p">)</span>
<span class="go">Quantifier(agg_op=AggregPMeanError(p=2, stable=True), quantifier=&#39;f&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.56</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor([0.6365, 0.7109, 0.6682])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">Forall</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="go">tensor(0.6704)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.AggregPMeanError.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.AggregPMeanError.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the <cite>pMeanError</cite> aggregation operator to the given formula’s <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a>
on the selected dimensions.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>xs</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p><a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">Grounding</span></a> of formula on which the aggregation has to be performed.</p>
</dd>
<dt><strong>dim</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>, default=None</span></dt><dd><p>Tuple containing the indexes of dimensions on which the aggregation has to be performed.</p>
</dd>
<dt><strong>keepdim</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a>, default=False</span></dt><dd><p>Flag indicating whether the output has to keep the same dimensions as the input after
the aggregation.</p>
</dd>
<dt><strong>mask</strong><span class="classifier"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>, default=None</span></dt><dd><p>Boolean mask for excluding values of ‘xs’ from the aggregation. It is internally used for guarded
quantification. The mask must have the same shape of ‘xs’. <cite>False</cite> means exclusion, <cite>True</cite> means inclusion.</p>
</dd>
<dt><strong>p</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>, default=None</span></dt><dd><p>Value of hyper-parameter <cite>p</cite> of the <cite>pMeanError</cite> fuzzy aggregation operator.</p>
</dd>
<dt><strong>stable: :obj:`bool`, default=None</strong></dt><dd><p>Flag indicating whether to use the <a class="reference external" href="https://docs.python.org/3/c-api/stable.html#stable" title="(in Python v3.12)"><span class="xref std std-ref">stable version</span></a> of the operator or not.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p><cite>pMeanError</cite> fuzzy aggregation of the formula.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a></dt><dd><p>Raises when the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> of the formula (‘xs’) and the mask do not have the same
shape.
Raises when the ‘mask’ is not boolean.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="field-list">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>p</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a></span></dt><dd><p>See <cite>p</cite> parameter.</p>
</dd>
<dt><strong>stable</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code></a></span></dt><dd><p>See <cite>stable</cite> parameter.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.SatAgg">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">ltn.fuzzy_ops.</span></span><span class="sig-name descname"><span class="pre">SatAgg</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">agg_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">AggregPMeanError(p=2,</span> <span class="pre">stable=True)</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.SatAgg" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p><cite>SatAgg</cite> aggregation operator.</p>
<p><span class="math notranslate nohighlight">\(\operatorname{SatAgg}_{\phi \in \mathcal{K}} \mathcal{G}_{\theta} (\phi)\)</span></p>
<p>It aggregates the truth values of the closed formulas given in input, namely the formulas
<span class="math notranslate nohighlight">\(\phi_1, \dots, \phi_n\)</span> contained in the knowledge base <span class="math notranslate nohighlight">\(\mathcal{K}\)</span>. In the notation,
<span class="math notranslate nohighlight">\(\mathcal{G}_{\theta}\)</span> is the <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">grounding</span></a> function, parametrized by <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>agg_op</strong><span class="classifier"><a class="reference internal" href="#ltn.fuzzy_ops.AggregationOperator" title="ltn.fuzzy_ops.AggregationOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AggregationOperator</span></code></a>, default=AggregPMeanError(p=2)</span></dt><dd><p>Fuzzy aggregation operator used by the <cite>SatAgg</cite> operator to perform the aggregation.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the type of the input parameter is not correct.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p><cite>SatAgg</cite> is particularly useful for computing the overall satisfaction level of a knowledge base when <a class="reference internal" href="index.html#notelearning"><span class="std std-ref">learning</span></a> a Logic Tensor Network;</p></li>
<li><p>the result of the <cite>SatAgg</cite> aggregation is a scalar. It is the satisfaction level of the knowledge based composed of the closed formulas given in input.</p></li>
</ul>
<p class="rubric">Examples</p>
<p><cite>SatAgg</cite> can be used to aggregate the truth values of formulas contained in a knowledge base. Note that:</p>
<ul class="simple">
<li><p><cite>SatAgg</cite> takes as input a tuple of <a class="reference internal" href="index.html#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a> and/or <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>;</p></li>
<li><p>when some <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> are given to <cite>SatAgg</cite>, they have to be scalars in [0., 1.] since <cite>SatAgg</cite> is designed to work with closed formulas;</p></li>
<li><p>in this example, our knowledge base is composed of closed formulas <cite>f1</cite>, <cite>f2</cite>, and <cite>f3</cite>;</p></li>
<li><p><cite>SatAgg</cite> applies the <cite>pMeanError</cite> aggregation operator to the truth values of these formulas. The result is a new truth value which can be interpreted as a satisfaction level of the entire knowledge base;</p></li>
<li><p>the result of <cite>SatAgg</cite> is a <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> since it has been designed for learning in PyTorch. The idea is to put the result of the operator directly inside the loss function of the LTN. See this <a class="reference external" href="https://nbviewer.jupyter.org/github/bmxitalia/LTNtorch/blob/main/tutorials/3-knowledgebase-and-learning.ipynb">tutorial</a> for a detailed example.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">ltn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">4.3</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">3.4</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                        <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>                                    <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Forall</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Quantifier</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AggregPMeanError</span><span class="p">(),</span> <span class="n">quantifier</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">And</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Connective</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AndProd</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1</span> <span class="o">=</span> <span class="n">Forall</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f2</span> <span class="o">=</span> <span class="n">Forall</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">q</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f3</span> <span class="o">=</span> <span class="n">And</span><span class="p">(</span><span class="n">Forall</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">q</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)),</span> <span class="n">Forall</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sat_agg</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">SatAgg</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AggregPMeanError</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">sat_agg</span><span class="p">)</span>
<span class="go">SatAgg(agg_op=AggregPMeanError(p=2, stable=True))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">sat_agg</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span> <span class="n">f2</span><span class="p">,</span> <span class="n">f3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">tensor(0.7294)</span>
</pre></div>
</div>
<p>In the previous example, some closed formulas (<a class="reference internal" href="index.html#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a>) have been given to the <cite>SatAgg</cite>
operator.
In this example, we show that <cite>SatAgg</cite> can take as input also <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> containing the result of some
closed formulas, namely scalars in [0., 1.]. Note that:</p>
<ul class="simple">
<li><p><cite>f2</cite> is just a <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>;</p></li>
<li><p>since <cite>f2</cite> contains a scalar in [0., 1.], its value can be interpreted as a truth value of a closed formula. For this reason, it is possible to give <cite>f2</cite> to the <cite>SatAgg</cite> operator to get the aggregation of <cite>f1</cite> (<a class="reference internal" href="index.html#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a>) and <cite>f2</cite> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>).</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">],</span>
<span class="gp">... </span>                                    <span class="p">[</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">4.3</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Predicate</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span>
<span class="gp">... </span>                                    <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>                                 <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Forall</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">Quantifier</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AggregPMeanError</span><span class="p">(),</span> <span class="n">quantifier</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1</span> <span class="o">=</span> <span class="n">Forall</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sat_agg</span> <span class="o">=</span> <span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">SatAgg</span><span class="p">(</span><span class="n">ltn</span><span class="o">.</span><span class="n">fuzzy_ops</span><span class="o">.</span><span class="n">AggregPMeanError</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">sat_agg</span><span class="p">)</span>
<span class="go">SatAgg(agg_op=AggregPMeanError(p=2, stable=True))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">sat_agg</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span> <span class="n">f2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="go">tensor(0.6842)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="ltn.fuzzy_ops.SatAgg.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">closed_formulas</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ltn.fuzzy_ops.SatAgg.__call__" title="Link to this definition"></a></dt>
<dd><p>It applies the <cite>SatAgg</cite> aggregation operator to the given closed formula’s <a class="reference internal" href="index.html#notegrounding"><span class="std std-ref">groundings</span></a>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>closed_formulas</strong><span class="classifier"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference internal" href="index.html#ltn.core.LTNObject" title="ltn.core.LTNObject"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.core.LTNObject</span></code></a> and/or <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></span></dt><dd><p>Tuple of closed formulas (<cite>LTNObject</cite> and/or tensors) for which the aggregation has to be computed.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a></dt><dd><p>The result of the <cite>SatAgg</cite> aggregation.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a></dt><dd><p>Raises when the type of the input parameter is not correct.</p>
</dd>
<dt><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a></dt><dd><p>Raises when the truth values of the formulas/tensors given in input are not in the range [0., 1.].
Raises when the truth values of the formulas/tensors given in input are not scalars, namely some formulas
are not closed formulas.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="field-list">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>agg_op</strong><span class="classifier"><a class="reference internal" href="#ltn.fuzzy_ops.AggregationOperator" title="ltn.fuzzy_ops.AggregationOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">ltn.fuzzy_ops.AggregationOperator</span></code></a>, default=AggregPMeanError(p=2)</span></dt><dd><p>See <cite>agg_op</cite> parameter.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</section>
</section>
</div>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Tommaso Carraro.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>